This article was downloaded by: [149.169.132.70] On: 25 May 2017, At: 16:44
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA

Interfaces
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org

America West Airlines Develops Efficient Boarding
Strategies
Menkes H. L. van den Briel, J. René Villalobos, Gary L. Hogg, Tim Lindemann, Anthony V.
Mulé,

To cite this article:
Menkes H. L. van den Briel, J. René Villalobos, Gary L. Hogg, Tim Lindemann, Anthony V. Mulé, (2005) America West Airlines
Develops Efficient Boarding Strategies. Interfaces 35(3):191-201. http://dx.doi.org/10.1287/inte.1050.0135
Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions
This article may be used only for the purposes of research, teaching, and/or private study. Commercial use
or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher
approval, unless otherwise noted. For more information, contact permissions@informs.org.
The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness
for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or
inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or
support of claims made of that product, publication, or service.
© 2005 INFORMS
Please scroll down for article—it is on subsequent pages

INFORMS is the largest professional society in the world for professionals in the fields of operations research, management
science, and analytics.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org

informs

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

Vol. 35, No. 3, May–June 2005, pp. 191–201
issn 0092-2102  eissn 1526-551X  05  3503  0191

®

doi 10.1287/inte.1050.0135
© 2005 INFORMS

America West Airlines Develops
Efﬁcient Boarding Strategies
Menkes H. L. van den Briel, J. René Villalobos, Gary L. Hogg

Department of Industrial Engineering, Arizona State University, PO Box 875906, Tempe, Arizona 85287-5906
{menkes@asu.edu, rene.villalobos@asu.edu, ghogg@asu.edu}

Tim Lindemann, Anthony V. Mulé

Airport Services, America West Airlines, 111 West Rio Salado Parkway, Tempe, Arizona 85281
{tim.lindemann@americawest.com, anthony.mule@americawest.com}

In September 2003, America West Airlines implemented a new aircraft boarding strategy that reduces the airline’s average passenger boarding time by over two minutes, or approximately 20 percent, for full and nearly
full ﬂights. The strategy, developed by a team of Arizona State University and America West Airline’s personnel,
is a hybrid between traditional back-to-front boarding and outside-inside boarding used by other airlines. Field
observations, numerical results of analytical models, and simulation studies provided information that resulted
in an improved aircraft-boarding strategy termed reverse pyramid. With the new boarding strategy, passengers
still have personal seat assignments, but rather than boarding by rows from the back to the front of the airplane,
they board in groups minimizing expected passenger interference in the airplane. The analytical, simulation,
and implementation results obtained show that the method represents a signiﬁcant improvement in terms of
boarding time over traditional pure back-to-front, outside-inside boarding strategies.
Key words: transportation: travel; programming: integer, applications.
History: This paper was refereed.

A

turnaround time, a factor that is particularly difﬁcult
to shorten is passenger-boarding time. Airlines have
little control over passenger-boarding time because
they have limited control over passengers. Furthermore, passengers expect levels of service corresponding to the airline and the class of service they pay
for, from no preassigned seats on a discount airline
to boarding preference in ﬁrst class on a full-service
airline. Therefore, while airlines want to speed up
the passengers boarding airplanes, they have been
cautious in making changes to increase operational
efﬁciency.
America West Airlines has made efforts to improve
its turnaround performance. We worked on a joint
project between America West and Arizona State University to cut passenger-boarding times for America
West’s narrow-body passenger airplanes, such as the
Airbus A320 and the Boeing 737, which have a central aisle and rows of three seats on both sides of
the aisle. The project included gathering data, developing and solving mathematical programming and

traditional metric used by commercial airlines to
measure the efﬁciency of their operations is airplane turnaround time. Usually turnaround time (or
turn time) is measured by the time between an airplane’s arrival and its departure. Recently, commercial airlines have paid a great deal of attention to
turnaround time because they believe it affects the
overall success of an airline. One of the main factors cited for the success of discount (or no-frills) airlines is the quick turnaround of their airplanes, which
helps them achieve high airplane utilization (Allen
2000, Michaels 2003). Thus, they make efﬁcient use
of their primary capital investment, the aircraft. Long
turnaround times decrease revenue-producing ﬂying
time, while short turnaround times please customers
and can increase airlines’ revenues.
Some factors that determine turnaround time
include passenger deplaning, baggage unloading,
fueling, cargo unloading, airplane maintenance, cargo
loading, baggage loading, and passenger boarding.
While improving any of these factors can decrease
191

192

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

simulation models, and validating and implementing
the results.

America West Airlines
America West Airlines is a major US carrier based in
Phoenix, Arizona, from which it serves more destinations nonstop than any other carrier. America West
has hubs at Phoenix Sky Harbor International Airport
in Phoenix and McCarran International Airport in Las
Vegas, Nevada. The airline’s modern, fuel-efﬁcient
ﬂeet consists of Airbus A320s, Airbus A319s, Boeing
757s, and Boeing 737s. By the end of 2006, America
West expects to take delivery of new 110-seat Airbus A318s. The average age of the planes in America
West’s ﬂeet is about 10 years.
America West is the only major airline not only
to survive, but also to thrive since the US airline
industry was deregulated in 1978. The carrier began
service on August 1, 1983, with three airplanes and
280 employees. It grew rapidly and, by 1990, had
become a major airline, with annual revenues of over
$1 billion.
Today, America West is a low-fare, full-service airline. Its coast-to-coast route system includes 90+ destinations across the United States, Mexico, Canada,
and Central America, with more than 800 daily
departures. It uses its Phoenix and Las Vegas hubs
as gateways for travel throughout its route network. America West Express provides regional service through code-sharing agreements with Mesa
Airlines, and Air Midwest, which are wholly owned
subsidiaries of Mesa Air Group, one of the largest
regional airlines in the world. These regional carriers channel trafﬁc to America West’s hubs. Through
the agreement with Mesa Air Group, America West
plans to extend its route system and enhance its
ﬂight schedule as America West Express increases its
regional jet ﬂeet to 77 airplanes by 2005.

Project Objectives
America West Airlines asked members of the industrial engineering department of Arizona State University to take a critical look at the existing boarding
procedures and to propose new strategies.
Our task consisted of recommending a boarding
strategy that would minimize the average boarding

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

time, thus improving turnaround times and utilization of the aircraft ﬂeet.
Like most commercial airlines, America West traditionally boarded passengers in groups of those sitting in contiguous rows, ordering these groups from
the back to the front of the airplane (back-to-front
approach), after boarding special groups (usually
ﬁrst-class and special-needs passengers). The logic
behind this boarding procedure was that freeing the
passengers making the journey to the back of the airplane from aisle obstacles would minimize congestion in the aircraft aisle. However, the back-to-front
approach created congestion in a reduced area of the
aisle among passengers of the same group, impeding their access to overhead bins for stowing carryon luggage and making it difﬁcult for them to reach
their assigned seats. We conjectured that a different
boarding approach, with the groups composed of passengers dispersed throughout the airplane, might perform better. This conjecture was the basis for our
recommendation that America West should replace
the existing back-to-front groups with groups made
up of a widespread cross-section of the plane. Other
airlines have experimented with such alternatives as
the outside-in approach; but we found no formal and
comprehensive analysis of this approach or of the
back-to-front approach in the open literature.

Previous Works and Project Strategy
Marelli et al. (1998) described a simulation-based
analysis performed for Boeing. They designed the
passenger enplane/deplane simulation (PEDS) to
test different boarding strategies and different interior conﬁgurations on a Boeing 757 airplane. PEDS
showed that by boarding from the outside in, that
is, window-seats ﬁrst, middle seats second, and
aisle seats last, airlines could reduce boarding times
signiﬁcantly.
Shuttle by United was one of the ﬁrst airlines to
actually employ the outside-in strategy. While Kimes
and Young (1997) reported that the airline implemented the method with a good degree of success, United Airlines later discontinued the method
and replaced it with its current approach: boarding
all premium-class customers ﬁrst, economy-plus customers second, customers seated in the last 10 rows of
economy third, and all remaining customers fourth.

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

Van Landeghem and Beuselinck (2002) conducted
another simulation-based study on airplane boarding
that showed that the fastest way to get people on an
airplane would be to board them individually by their
row and seat number by calling each one of the passengers individually to board the aircraft. Although
this approach seems impractical, the authors claimed
that it could halve total boarding time. In their study,
they analyzed many alternative boarding patterns.
One pattern that seemed practical and efﬁcient was
boarding passengers by half-row, that is, by splitting
each row into a starboard-side group and a port-side
group and then boarding the half-rows one by one.
While traditional computer-based simulation studies are good tools for testing the performance of already identiﬁed alternatives, they do not provide efﬁcient mechanisms for constructing the most promising
alternatives. For this reason, we decided to use analytical models to analyze the problem. Surprisingly, in
the airline industry, which has a very rich background
in operations research applications, we found only
simulation-based solutions for analyzing and improving passenger airplane boarding. One exception is a
study by Bachmat et al. (2005) that approaches the
airplane-boarding problem from a physicist’s point
of view. They constructed a model based on spacetime geometry and random matrix theory that captures the asymptotic behavior of airplane boarding.
Their results are qualitatively in line with ours.
Our analysis of the problem consisted of six phases:
(1) We developed a simple integer-programming
model to understand the problem and to create general patterns for efﬁcient boarding strategies. To make
the problem more tractable, we used the minimization of passenger interferences as our objective in lieu
of the minimization of boarding time. (2) We tested
these patterns using simulation models that incorporated more details of an actual airplane-boarding procedure. (3) We improved and reﬁned these models
to accommodate practical factors and implementation
limits, such as several passengers traveling together
and the processing speed of the gate agent. (4) We
analyzed the results of the simulation models and
the analytical models to determine the best boarding procedures to recommend. (5) We tested and ﬁnetuned the recommended procedures. (6) Finally, we
implemented and validated the proposed boarding
procedures.

193

Problem Analysis
Boarding interference is deﬁned as an instance of a
passenger blocking another passenger’s access to his
or her seat. We assume that there is a correspondence between minimizing the expected number of
passenger interferences and minimizing the boarding time. A passenger blocked by another passenger takes longer to reach his or her seat than one
who has free access. Therefore, as the number of passengers facing interference during the boarding process increases, the total boarding time increases. Thus,
by minimizing passenger interferences, we shorten
individual passengers’ seating times, which will ultimately shorten overall boarding times.
We deﬁned two types of interferences: seat interferences and aisle interferences. Seat interferences occur
when passengers seated close to the aisle block other
passengers seated in the same row. Consider, for
example, an aircraft with rows progressively numbered from front to back and seats labeled A to F from
left to right. A passenger sitting in seat 7C (the aisle
seat in row 7) could block the passenger seeking seat
7A (the window seat) and will have to stand in the
aisle for the passenger in 7A to be seated. The interference is even worse when passenger 7A arrives and
passengers 7B and 7C are seated.
Aisle interferences occur when passengers stowing luggage in overhead bins block other passengers’
access to seats. For example, if passenger 9A boards
the airplane just before passenger 14C, passenger 9A
will block passenger 14C’s progress down the aisle as
he or she stores luggage.
We developed a model to minimize expected boarding interferences. The decision is to assign each passenger boarding the airplane to a boarding group, to
minimize boarding interferences. The objective function includes all the different interferences that could
possibly occur during boarding. Each of these interferences has a certain penalty, and the sum of all the
penalties related to a particular seat assignment determines the objective value. The constraints guarantee
that every seat is assigned to exactly one group and
that every group is assigned a particular number or
range of numbers of total seats.
The interference model is a nonlinear assignment
problem with quadratic and cubic terms in the objective function. Such assignment problems belong to the

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

194

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

BF3

1
1
1
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2

1
1
1
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2

1
1
1
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2

1
1
1
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2

BF4

1
1
1
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2

4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2

5
5
5
5
5
4
4
4
4
4
4
3
3
3
3
3
3
2
2
2
2
2
2

1
1
1
5
5
5
5
5
4
4
4
4
4
4
3
3
3
3
3
3
2
2
2
2
2
2

1
1
1
5
5
5
5
5
4
4
4
4
4
4
3
3
3
3
3
3
2
2
2
2
2
2

1
1
1
5
5
5
5
5
4
4
4
4
4
4
3
3
3
3
3
3
2
2
2
2
2
2

BF5

1
1
1
5
5
5
5
5
4
4
4
4
4
4
3
3
3
3
3
3
2
2
2
2
2
2

5
5
5
5
5
4
4
4
4
4
4
3
3
3
3
3
3
2
2
2
2
2
2

6
6
6
6
5
5
5
5
4
4
4
4
4
3
3
3
3
3
2
2
2
2
2

1
1
1
6
6
6
6
5
5
5
5
4
4
4
4
4
3
3
3
3
3
2
2
2
2
2

1
1
1
6
6
6
6
5
5
5
5
4
4
4
4
4
3
3
3
3
3
2
2
2
2
2

1
1
1
6
6
6
6
5
5
5
5
4
4
4
4
4
3
3
3
3
3
2
2
2
2
2

1
1
1
6
6
6
6
5
5
5
5
4
4
4
4
4
3
3
3
3
3
2
2
2
2
2

6
6
6
6
5
5
5
5
4
4
4
4
4
3
3
3
3
3
2
2
2
2
2

BF6

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

1
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

OI3

1
1
1
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

1
1
1
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4

1
1
1
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4

OI4

1
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3
3

1
1
1
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
4
4
4
4
4
4

1
1
1
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
4
4
4
4
4
4

1
1
1
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3
3

3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

OI5

4
4
4
4
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2

1
1
1
5
5
5
5
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3

1
1
1
6
6
6
6
6
6
6
6
6
6
6
6
6
5
5
5
5
5
5
5
5
5
5

1
1
1
6
6
6
6
6
6
6
6
6
6
6
6
6
5
5
5
5
5
5
5
5
5
5

1
1
1
5
5
5
5
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3

4
4
4
4
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2

OI6

Figure 1: The eight boarding patterns analyzed are presented in this ﬁgure. Each schematic shows the seat layout
of an Airbus A320 airplane. The number shown on each seat is the boarding group to which the seat is assigned.
Back-to-front boarding strategies are identiﬁed by BF, and the boarding strategies found by MINLP that minimize
expected passenger interference and tend to board passengers from the outside in are identiﬁed by OI. The
number following the boarding-strategy identiﬁcation indicates the number of boarding groups. The OI5 and OI6
patterns are also referred to as reverse-pyramid boarding strategies.

NP-hard complexity class. In fact, if the model contained only quadratic terms, it would be a quadratic
assignment problem, a type of problem generally
known to be NP-hard. We used MINLP, a mixedinteger nonlinearly constrained optimization solver
(Leyffer 1999). MINLP uses a branch-and-bound algorithm in which each node corresponds to a continuous
nonlinearly constrained optimization problem. This
algorithm is effective in solving nonconvex MINLP
problems, but being a heuristic approach, it does not
give any guarantees that a global solution will be
found.
We compared boarding patterns for different numbers of boarding groups; four using the traditional
back-to-front (BF) boarding pattern and four using the
boarding patterns obtained from MINLP (Figure 1,
Table 1). The MINLP solutions have a tendency to
board outside-inside (OI). For both the BF and OI
strategies, we ﬁxed the ﬁrst boarding group to contain ﬁrst-class seats only. We divided the economy
class section in the BF strategies into groups of similar size; for the OI strategies, we let MINLP decide
on the most efﬁcient size for each group.
The OI boarding strategies are also referred to
as the reverse-pyramid strategies. The reverse-pyramid
boarding patterns actually take on characteristics of

multiple strategies. Interestingly, as the number of
groups increases, the way in which boarding passengers are distributed changes its form. That is, it moves
from a mixture of window/middle and middle/aisle
seats contained in the same groups with three groups,
to a strictly laminar form with four groups, to patterns that mix back-to-front with outside-in strategies
with ﬁve or more groups.
Boarding window seats before middle seats and
middle seats before aisle seats as in the reversepyramid strategies reduced the number of expected
seat interferences signiﬁcantly. Additionally, the number of expected aisle interferences in the reversepyramid strategies is below the number expected in
the BF strategies (Table 1). The numbers favor the
reverse-pyramid strategies over the BF strategies, but
the numbers can be a little misleading. In our model,
we used unit weights for each interference type. That
is, each type of interference was assumed to be of
equal importance. It might be the case, however, that
aisle interferences should be weighted more heavily
than seat interferences, and maybe aisle interferences
that occur within groups should be weighted more
heavily than aisle interferences that occur between
groups. It is very difﬁcult to estimate these weights

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

195

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

Seat interferences
First class [xx]
First class [x] → [x]
Economy class [xxx]
Economy class [xx] → [x]
Economy class [x] → [xx]
Economy class [x] → [x] → [x]
Aisle interferences
Within groups
Same row same side
Same row different side
Different rows
Between groups
Same row same side
Same row different side
Different rows
Total seat interferences
Total aisle interferences
Total interferences

BF3

BF4

BF5

BF6

OI3

OI4

OI5

3
0
69
0
0
0

3
0
69
0
0
0

3
0
69
0
0
0

5
8
67

7
11
64

0
0
1
72

0
0
1
72

OI6

3
0
69
0
0
0

3
0
0
12
11
0

3
0
0
0
0
0

3
0
0
0
0
0

3
0
0
0
0
0

9
14
61

11
17
58

233
533
6967

1
5
70

1
6
69

1
7
68

0
0
1
72

0
0
1
72

002
002
131
26

004
004
196
3

006
006
229
3

006
006
256
3

81

83

85

87

7868

7804

7840

7868

153

155

157

159

10469

8104

8140

8168

Table 1: We show the number of expected passenger interferences by boarding strategy. We indicate the type
of seat interferences by the boarding order of the passengers. We use squared brackets, [ ], to group together
the passengers boarding in the same group, that is, [xxx] indicates that three passengers in the same half-row
board in the same group. We use an arrow to indicate a precedence relation between groups. For instance, the
notation [x] → [xx] means that a single passenger sitting in a half-row boards alone in a group followed by the
other two passengers who board together in a subsequent group. We divide aisle interferences into interferences
that occur within groups and those that occur between groups.

and determine how important each interference type
is compared to other interference types.
We used simulation to validate the analytical model
and to obtain a ﬁner level of detail. America West
personnel ﬁlmed a number of actual airplane boardings. They used two cameras, one inside the airplane
and one inside the jet bridge leading to the plane.
We retrieved data on the time between passengers,
walking speed, interference time, and time to store
luggage in the overhead bins by analyzing the tapes.

Average seat interferences
Average aisle
Interferences
Average total
Interferences
Average boarding
time (seconds)

We built the simulation model in ProModel
2001 and simulated each of the boarding strategies
(Figure 1) 100 times (Table 2). Then, we obtained
enough replicates for all the strategies we tested to
give 95 percent conﬁdence intervals of less than 60
seconds. The model showed that the strategies based
on the solutions of the interference model are better
than the back-to-front approach. The average number of seat interferences matches well with the number produced by the interference model. The number

BF3

BF4

BF5

BF6

OI3

OI4

OI5

OI6

7076
5341

7211
5336

7336
5274

7222
5227

2605
4695

294
4202

294
4292

294
4264

12417

12547

12610

12449

7300

4496

4586

4558

143676

146068

147369

149168

141279

137607

138271

138780

Table 2: We show the simulation results by boarding strategy. The data represents the average of 100 runs. This
number of runs is sufﬁcient to obtain a 95 percent conﬁdence interval of at most 60 seconds on total boarding
time. We computed the number of aisle interferences by taking into account only those aisle interferences caused
by the passenger immediately ahead in the boarding process of any passenger boarding the aircraft.

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies
Interfaces 35(3), pp. 191–201, © 2005 INFORMS

of aisle interferences in the simulation, however, is
signiﬁcantly lower than that produced by the interference model. The reason for this difference is that the
way we deﬁned aisle interferences in the analytical
model does not always result in an aisle interference
in the simulation model. For example, when two passengers are expected to interfere in the aisle, in the
simulation model, they might enter the airplane with
enough of a gap that the ﬁrst clears the aisle before the
second arrives. As the time between passenger boardings decreases, the chance of the passengers interfering with each other increases (Figure 2). As the time
between passengers decreases, the outside-in strategy
with six groups tends to perform increasingly better than the back-to-front strategy with six groups.
A plateau seems to exist where further reductions
in the time between passengers boarding stops being
beneﬁcial in terms of reducing the overall boarding time. The bottleneck for the outside-in strategy
appears at a much lower time between passengers
(that is, at a higher passenger throughput rate) than
for the back-to-front strategy. Because the gate agents
determine the time between passenger boardings, the
airline can shorten boarding times by expediting the
process at the gate.
From the videotapes, we determined that a single
gate agent processed about 6.7 passengers per minute
(or one passenger every nine seconds). By using two
or more gate agents or having an extra agent to check
passengers’ identiﬁcation documents, the airline can
2,500
OI6

Total boarding time (sec.)

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

196

speed up the boarding process without changing
much. Based on the information provided by the data
and the analytical and simulation models, we estimated that America West could cut its boarding times
by as much as 37 percent by using two agents and
the OI strategy with six groups. With only one agent,
we estimated that it could reduce times by around
25 percent.

Implementation
We implemented the results of the project ﬁrst in a
pilot and then systemwide. In the pilot implementation, our main objective was to validate and ﬁne tune
the results of our analysis. In the pilot implementation, we used the reverse-pyramid boarding strategy
with six boarding groups (OI6) in all the America
West boarding gates at the Los Angeles International
(LAX) airport (Tables 3 and 4).
The average savings obtained by using the new
strategy were 26 percent using one gate agent and
39 percent using two agents, almost exactly matching
those predicted by the simulation model.
America West management decided to implement
the new boarding strategies systemwide. It implemented reverse-pyramid group boarding patterns in
80 percent of its airports in September 2003. It has
not yet employed this method in Las Vegas McCarran
International Airport (LAS) because of that airport’s
unique network infrastructure, which prints boarding
passes. Las Vegas is the largest airport in the America
West system that still boards passengers using the traditional back-to-front method.

BF6

2,000

Average number of
passengers
Average total boarding
time (seconds)
Average time between
passengers

1,500

1,000

500

0
1

2

3

4

5

6

7

8

9

10 11 12

13

14 15

Average time between boarding passengers (sec.)

Figure 2: We show boarding time performance of the OI6 and BF6 boarding strategies for various average times between boarding passengers. The average time between passengers may decrease if passenger
throughput at the gate increases.

1BF6

2BF6

1OI6

2OI6

12370

13190

12120

13560

146270

147660

146000

102500

1182

1119

1205

756

Table 3: We show the average results of a ﬁeld study using the BF6 and OI6
boarding strategies. The total boarding time is the time it takes for all passengers to get seated. The headings 1BF6 (1OI6) and 2BF6 (2OI6) denote
the BF6 (OI6) boarding strategy using one or two boarding agents, respectively. The numbers shown represent the average of 10 (eight for 1OI6)
airplane boardings. All data points for the BF6 strategy were obtained
at Phoenix Sky Harbor International Airport (PHX), and those for the OI6
strategy were obtained at Los Angeles International Airport (LAX).

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

197

Average total boarding time
Average time between
passengers

1BF6

2BF6

1OI6

2OI6

160

121770
984

124670
945

88770
732

78810
581

140

2002
2003
120
100

300
2002
250

2003

200
150
100
50

AUG

JUL

JUN

MAY

APR

MAR

FEB

JAN

DEC

NOV

OCT

0

Figure 3: This ﬁgure shows total departure delays at America West Airlines in hours per month. The data does not include Las Vegas McCarran
International Airport (LAS).

60
40
20

AUG

JUL

JUN

MAY

APR

MAR

FEB

JAN

DEC

NOV

0
OCT

Departure delays have decreased signiﬁcantly since
the implementation of the reverse-pyramid boarding
strategy. An average decrease of 21.0 percent in departure delays (Figure 3) was observed in the ﬁrst three
months after incorporating the new boarding strategy at all of America West’s airports except Las Vegas
McCarran International Airport. At Phoenix’s Sky
Harbor Airport, the largest hub in the America West
system, the average decrease in boarding time delays
was 60.1 percent (Figure 4).
In addition to the quantiﬁable beneﬁts, America
West has realized nonquantiﬁable beneﬁts with this
boarding method:
(1) The airline’s customers can easily understand
when to queue up for boarding, and
(2) The airport agents can easily remember where
they are in the boarding process because they are calling group numbers instead of row sections.
Also, agents can preferentially board passengers
for special seats on the airplane, such as the bulkhead seats. Passengers in the bulkhead seats have

80

SEP

Table 4: We show the average results of a ﬁeld study using the BF6 and
OI6 boarding strategies. The numbers are from the same data set as those
in Table 3 but exclude the passengers that do not board with the bulk of
the passengers, such as preboarding and late passengers.

SEP

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

Figure 4: We present total departure delays at America West Airlines in
hours per month. The data includes only Phoenix Sky Harbor International
Airport (PHX).

traditionally been last in boarding. These seats have
no under-seat storage for carry-on luggage. When
passengers assigned to the bulkhead seats board last,
they often ﬁnd the overhead bins full. Giving these
passengers boarding priority grants them ﬁrst access
to the overhead-bin storage space.

Conclusions
The models and implementation results show that
outside-in boarding outperforms traditional back-tofront boarding. Although the models make several
simplifying assumptions, they provide a good analysis of the factors affecting the boarding process. Based
on the results provided by the integer programming
and simulation models, we developed a new boarding strategy; a hybrid between the traditional backto-front and outside-in boarding strategies termed the
‘reverse-pyramid’ approach. This approach was implemented in pilot form in the America West ﬂights
departing Los Angeles international airport. During
this pilot implementation, America West estimated
that boarding times were reduced by up to 39 percent. After the success of this pilot implementation,
America West management decided to implement the
reverse-pyramid boarding strategy systemwide. While
it is too early to draw general conclusions, preliminary information indicates that the implementation
has been successful. For instance, America West has
observed a signiﬁcant increase in the number of
on-time departures from its Phoenix hub.

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

198

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

While the results available show that our method
is an excellent alternative for reducing passengerboarding time, additional research is needed to ﬁne
tune and improve its implementation. For instance,
we did not consider the saturation of storage space
for carry-on luggage explicitly in the analytical model.
This and other factors should be explored further to determine the best possible boarding strategies. We hypothesize that the best boarding strategy
depends on such factors as airplane design and the
proﬁle of the passengers boarding the plane. These
factors could be analyzed in future projects.

Appendix
In our mathematical programming model for the
airplane-boarding problem, we considered an airplane with one aisle and three seats on either side.
For brevity, we considered only the economy-class
section, but the model could easily be extended to
include the business or ﬁrst-class section.
Let N represent the set of rows and M = A B
C D E F represent the set of seat positions in the
airplane. In addition, let the seats on the left side
of the aisle be represented by L = A B C ∈ M and
those on the right side by R = D E F ∈ M, such that
A and F are window seats, B and E are middle seats,
and C and D are aisle seats. Now, given a row number i ∈ N and a seat position j ∈ M, we can uniquely
identify and represent all seat locations in the airplane
by the pair 
i j.
By assigning seats to groups (with ﬁxed seat assignments, this is similar to assigning passengers to
groups), we can create different boarding patterns.
For the airplane-boarding problem, we want to assign
each seat 
i j to a boarding group k, k ∈ G, with
G representing the set of groups. Let us deﬁne the
decision variable xijk = 1 if seat 
i j is assigned to
group k and xijk = 0 otherwise, for all i ∈ N , j ∈ M, and
k ∈ G.
In our formulation of the airplane-boarding problem, the equation numbers indicate the purpose of
each set of expressions. In the objective function, we
have different penalties for each type of interference.
We represent seat-interference penalties by s and
aisle-interference penalties by a . The penalties associated with the different types of interferences capture

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

their relative contributions to the total delay of the
boarding procedure.
Minimize
 
 
xiAk xiBk xiCk +s1
xiF k xiEk xiDk
z = s1
i∈N k∈G

+s2
+s3
+s4
+s2
+s4
+s6





+s9



i∈N kl∈Gk<l





i∈N kl∈Gk<l





i∈N kl∈Gk<l





i∈N kl∈Gk<l





i∈N kl∈Gk<l





i∈N kl∈Gk<l



+s7
+s8



i∈N kl∈Gk<l

+s7
+s5



i∈N kl∈Gk<l





i∈N kl∈Gk<l



xiAk xiBk xiCl
xiAk xiBl xiCk
xiAl xiBk xiCk
xiF k xiEk xiDl +s3
xiF l xiEk xiDk +s5





i∈N kl∈Gk<l



xiDk xiEl xiF l +s6

+ s8
+ s9



(1c)


i∈N kl∈Gk<l



+ s11
+ s12
+ a1






i∈N k l m∈G k<l<m

+ s10



xiF k xiEl xiDm





i∈N k l m∈G k<l<m





i∈N k l m∈G k<l<m







i∈N u v∈L v=u k∈G

xiAm xiBk xiCl

xiF l xiEm xiDk

i∈N k l m∈G k<l<m





xiF l xiEk xiDl

xiAk xiBl xiCm

i∈N k l m∈G k<l<m



xiAk xiBl xiCl

xiAl xiBm xiCk

i∈N k l m∈G k<l<m



xiF k xiEl xiDk

xiF l xiEl xiDk







i∈N kl∈Gk<l

xiAm xiBl xiCk


s i∈N klm∈Gk<l<m
+ 11
xiAk xiBm xiCl
i∈N k l m∈G k<l<m

+ s12



xiAl xiBl xiCk

i∈N klm∈Gk<l<m

+s10

(1b)

xiAl xiBk xiCl

i∈N klm∈Gk<l<m



(1a)

i∈N k∈G

xiF m xiEl xiDk
xiF k xiEm xiDl
xiF m xiEk xiDl

xiuk xivk

(1d)

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

199

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

+ a1



Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

+ a4
+ a4
+ a5
+ a5
+ a6



i∈N u v∈R u=v k∈G

+ 2a2
+ a3





xiuk xivk





i∈N u v∈M u∈L v∈R k∈G







a b∈N  a<b u v∈M k∈G

 



i∈N u v∈R k l∈G k<l

 



i∈N u v∈L k l∈G k<l





xauk xbvk

xiuk xivl







i∈N u∈R v∈L k l∈G k<l





(2b)
(2c)

xiuk xivl

i∈N u∈L v∈R k l∈G k<l



xiuk xivk

(2a)

(2d)

xiuk xivl
xivl xiuk



a b∈N  a<b u v∈M k l∈G k<l

xauk xbvl

(2e)
(2f)

subject to

k∈G

xijk = 1



i∈N j∈M



i∈N j∈M

xijk ∈ 0 1

for all i ∈ N  j ∈ M

(3)

xijk ≥ Cmin

for all k ∈ G

(4)

xijk ≤ Cmax

for all k ∈ G

(5)

for all i ∈ N  j ∈ M k ∈ G

(6)

Expressions (1a)–(1d) are associated with seat interferences. Seat interferences may occur when agents
assign all seats on the same side in one row to the
same boarding group (1a); when they assign two seats
on the same side and row to one boarding group and
the third seat to a later (1b) or earlier (1c) boarding
group; or when they assign all seats on the same side
and row to different boarding groups (1d).
We represent aisle interferences by expressions
(2a)–(2f). Where (2a)–(2c) represent the aisle interferences that take place within a group; (2d)–(2f) are the
aisle interferences that take place between two consecutive groups. Aisle interferences can occur when
the two passengers are seated in the same row and on
the same side (2a) and (2d), in the same row and on
different sides (2b) and (2e), and in different rows (2c)
and (2f) (in this case, the side does not matter). We
could have gathered expressions (2a)–(2c), similarly
(2d)–(2f), into one expression. However, we could not

then have applied different penalties to these seemingly different interferences.
The constraints grouped under (3) represent the
assignment restrictions. They ensure that each seat is
assigned to only one boarding group. Constraints (4)
and (5) restrict the group size to at least Cmin and at
most Cmax seats assigned to each group. Finally, (6)
are the binary constraints.
Determination of Penalties
We could use various procedures to determine penalty
values; for example, we could use historical data and
estimate the contributions of each type of interference to the total delay. However, we used probabilistic
expectations (Table 5). Our main assumption in determining these expectations was that all boarding positions for a particular passenger within the group are
equally likely. That is, a particular passenger within a
group can take any of the different boarding positions
in that group with the same probability. Based on
these assumptions, we computed the expected number of interferences and used it as the penalty value.
For example, suppose that three passengers seated
in the same row on the same side of the aisle at
positions A (window), B (middle), and C (aisle) are
assigned to the same boarding group. These passengers could board the plane in six ways (ABC, ACB,
BAC, BCA, CAB, CBA). Based on our assumptions,
these boarding patterns are equally likely; however,
Penalty













s
1
s
2
s
3
s
4
s
5
s
6
s
7
s
8
s
9
s
10
s
11
s
12

Passenger order

E (No. of interferences)

[window, middle, aisle]
[window, middle] → [aisle]
[window, aisle] → [middle]
[middle, aisle] → [window]
[window] → [middle, aisle]
[middle] → [window, aisle]
[aisle] → [window, middle]
[window] → [aisle] → [middle]
[middle] → [window] → [aisle]
[middle] → [aisle] → [window]
[aisle] → [window] → [middle]
[aisle] → [middle] → [window]

15
05
15
25
05
15
25
1
1
2
2
3

Table 5: We present the expected seat interferences for different passenger boardings. Squared brackets, [ ], in the second column indicate passengers boarding in the same group. The order in which passengers board
the airplane within a group is arbitrary. An arrow indicates a precedence
relation between groups. The ﬁrst column gives the corresponding seat
interference penalty.

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

200

Interfaces 35(3), pp. 191–201, © 2005 INFORMS

the interferences they cause are different. For example, in the boarding pattern ABC, the window passenger (A) boards the plane ﬁrst, then the passenger
sitting in the middle seat (B) boards, and then the passenger sitting in the aisle seat (C). This represents the
best-case scenario (zero penalties) because none of the
three would have to get up once seated.
In the boarding pattern ACB, the window-seat passenger again boards ﬁrst, then the aisle-seat passenger, and then the middle-seat passenger. In this case,
there is one interference, the aisle-seat passenger must
get up for the middle-seat passenger. Because all of
the boarding patterns are equally likely (1/6 probability), we can determine the interferences associated with each pattern to obtain its expected value.
We use the same procedure when the three passengers
on the same side of a row are assigned to different
groups. For instance, if the window-seat and the aisleseat passengers are assigned to one boarding group
and the middle-seat passenger is assigned to another
boarding group that boards after the ﬁrst group, we
will have one interference if the aisle-seat passenger
boards before the window-seat passenger and none if
the window-seat passenger boards ﬁrst. Because the
probability of each of the two boarding patterns is 0.5
and because we know that the middle-seat passenger will be assigned to a different group, causing an
interference of one with probability of one, the total
expected interferences, and the penalty, for this case
is 1.5.
In a similar way, we can calculate the expected
aisle interferences (Table 6). For aisle interferences
within a boarding group, we look for the number of
ways two passengers can interfere with each other.
Consider a boarding group of size s1 . One type of
aisle interference occurs when a passenger seated in
a higher row number enters the airplane right behind
a lower-row-number passenger. If we assume the
Penalty
a
1
a
4

a
2
a
5

a
3
a
6

  
  

Description

E (No. of interferences)

Within groups
Between groups

1/s1
1/s1 s2 

Table 6: We present the expected aisle interferences for two passengers
within the same and between two consecutive groups. We use s1 and s2
to represent the sizes of the groups in which the two passengers board.
The ﬁrst column gives the corresponding aisle interference penalty.

size of their boarding group to be equal to s1 , then
there are 
s1 − 1
s1 − 2! out of s1 ! ways the passengers could have boarded the airplane such that these
two passengers would interfere with each other. To
be more speciﬁc, there are 
s1 − 1 positions for the
two passengers to board one after another, leaving

s1 − 2! ways for the remaining passengers in this
group to board, with s1 ! total ways to board s1 passengers. Hence, the probability that two passengers
with different row numbers will interfere is equal to
1/s1 
= 
s1 − 1
s1 − 2!/s1 !. Thus, if m passengers have
a lower row number than the higher-row-number
passenger, the expected value for this type of aisle
interference is m/s1 .
For aisle interferences between groups, we look at
the probability that the ﬁrst passenger in one group
has a higher row number than the last passenger in
the preceding group. The probability that a passenger will board ﬁrst or last in a group of size s1 is
equal to 1/s1 . Because boarding groups can be of different sizes, the probability that a passenger will be
last in a group of size s1 is equal to 1/s1 , and the
probability that a passenger will be ﬁrst in a group
of size s2 is equal to 1/s2 . Hence, the probability of
the two passengers boarding ﬁrst and last in their
groups is 1/
s1 s2 . Hence, if m passengers have lower
row numbers than the higher-row-number passenger,
the expected value for this type of aisle interference is
equal to m/
s1 s2 . Table 6 summarizes the aisle interference penalties.
References
Allen, M. 2000. Dual bridges may shorten Southwest’s turnaround. Dallas Bus. J. (May 22). Retrieved September 2003
from http://dallas.bizjournals.com/dallas/stories/2000/05/
22/story5.html.
Bachmat, E., D. Berend, L. Sapir, S. Skiena. 2005. Airplane boarding, disk scheduling, and space-time geometry. N. Megiddo, Y.
Yu, N. Alonstioti, B. Zhu, eds. Algorithmic Appl. Management.
Lecture Notes in Science, Springer, 192–202.
Kimes, S. E., F. Young. 1997. The shuttle by United. Interfaces 27(3)
1–13.
Leyffer, S. 1999. User manual for MINLP_BB. Numerical Analysis
Report NA/XXX. Dundee University, Dundee, Scotland.
Marelli, S., G. Mattocks, R. Merry. 1998. The role of computer
simulation in reducing airplane turn time. AERO Magazine 1.
Retrieved November 2002 from http://www.boeing.com/
commercial/aeromagazine.
Michaels, D. 2003. Economy class: As airlines suffer, British Air tries
takeoff strategy—carrier sheds jets, suppliers and old habits in
a bid for greater efﬁciency. Wall Street J., Eastern ed. (May 22).

van den Briel et al.: America West Airlines Develops Efﬁcient Boarding Strategies
Interfaces 35(3), pp. 191–201, © 2005 INFORMS

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:44 . For personal use only, all rights reserved.

Van Landeghem, H., A. Beuselinck. 2002. Reducing passenger
boarding times in airplanes: A simulation based approach. Eur.
J. Oper. Res. 142(2) 294–308.

Anthony V. Mulé, Senior Vice President, Customer
Service, America West Airlines, 4000 E. Sky Harbor
Boulevard, Phoenix, Arizona 85034, writes: “For over
two years we have worked with Dr. Villalobos and
his team to improve the aircraft boarding procedures
at America West Airlines. Some of the results of these
efforts are presented in the paper “America West Airlines Develops Efﬁcient Boarding Strategies.” In particular, the boarding strategies described in the paper
have been implemented successfully at America West
Airlines. Currently, we board 800+ of our daily

201

departures using group-boarding patterns that are
very similar to the reverse-pyramid pattern described
in the paper. We are currently working at using the
reverse-pyramid boarding method at the Las Vegas
International Airport, which is the only major airport
in the America West system not using it.
“We estimate that the implementation of the boarding methods described in the paper has been a complete success. For instance, we estimate a reduction in
boarding time of two minutes has been achieved.
“Summarizing, we are very pleased with the results
obtained from the aircraft boarding project and we
look forward to continue partnering with Arizona
State University to solve additional problems faced by
the aviation industry.”

394

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

An Automated Feature Selection Method
for Visual Inspection Systems
Hugo C. Garcia, J. René Villalobos, and George C. Runger

Abstract—Automated visual inspection (AVI) systems these
days are considered essential in the assembly of surface-mounted
devices (SMDs) in the electronics industry. This industry has faced
the problem of rapid introduction and retirement of SMD-based
products with the consequent obsolescence of the inspection
systems already in the assembly lines. The constant introduction
of new products has caused AVI systems to become rapidly obsolete. The general goal of this research centers on developing
self-training AVI systems for the inspection of SMD components.
The premise is that these systems would be less prone to obsolescence. In this paper, the authors describe the methodology
being used for automatically selecting the features to inspect new
components. In particular, this paper explores the use of multivariate stepwise discriminant analysis techniques, such as Wilks’
Lambda, in order to automate the feature selection process. All of
these techniques are applied to a case study of the inspection of
SMD components.
Note to Practitioners—In this paper, we present a methodology
that would allow the automation of the tedious task of exploring
and selecting features to develop algorithms for automated visual
inspection systems. In particular, the proposed method selects a
subset of features among all of the known features. The chosen
subset seeks to minimize inspection errors while keeping the algorithmic development time to a minimum. This is particularly useful
for adapting pre-existing systems to inspect new components, especially when the characteristics of the new components are similar
to those of components already inspected by the inspection system.
We applied this methodology to a case of study of the inspection of
surface-mounted devices.
Index Terms—Automated visual inspection (AVI), feature selection, quadratic vector classifier.

I. INTRODUCTION

T

HE continuous miniaturization of surface-mounted device
(SMD) components has made automated visual inspection (AVI) systems an indispensable tool in the assembly of
electronic products. However, the full potential of AVI systems
has not yet been exploited. For instance, to date, the main use
of these systems involves performing a screening inspection
(that is, the identification and removal of defective components)
rather than as a tool for the continuous improvement of the underlying manufacturing system. One reason for the absence of
AVI systems with continuous improvement capabilities is the
Manuscript received November 15, 2005. This work was supported by the
National Science Foundation under Grant DMI-0300361. This paper was recommended for publication by Associate Editor Y. Ding and Editor P. Ferreira
upon evaluation of the reviewers’ comments.
The authors are with the Department of Industrial Engineering at Arizona
State University, Tempe, AZ 85281 USA (e-mail: hugo.garcia@asu.edu;
rene.villalobos@asu.edu; George.Runger@asu.edu).
Digital Object Identifier 10.1109/TASE.2006.877399

lack of flexibility in the existing AVI systems to adapt its inspection algorithms to new products. This problem of reconfiguration, combined with the rapid introduction and retirement of
electronic products, has deterred equipment manufacturers and
the electronic-assembly industry from investing in the development of AVI systems for continuous process improvement.
Currently, when new components are introduced into an existing assembly line, the AVI system requires reprogramming
to incorporate the new characteristics of the new components
into the inspection algorithms. Often, this process of reconfiguration requires the involvement of the original equipment manufacturer (OEM) of the AVI system to develop the new inspection
algorithms and adjust the existing AVI system to the new components. The cost of this adjustment period is frequently one of
the main deterrents for the appropriate use of AVI systems on the
factory floor. Therefore, it is necessary to develop the tools and
methodologies that would allow the rapid adaptation of existing
AVI systems to new electronics products so that the AVI systems are not rendered obsolete by incremental changes in SMD
component technology.
One of the crucial and most time-consuming processes in
the training phase of AVI systems is the selection of inspection features. A feature, in our context, is usually a prominent
or distinctive characteristic that can be extracted from a digital image of an SMD component. Ideally, the features used
share common characteristics in that they are computationally
inexpensive, and simple enough to accommodate new components without major modifications to the actual system. However, a downside of the features simplicity is that they alone do
not provide an error-free classification between the populations.
In the training phase, the developer of the AVI system needs
to identify, among known features, a subset of them that provides an adequate level of discrimination between defective and
nondefective components. If no subset of features provides the
needed discrimination, it is necessary to develop new features
to attain the desired level of discrimination. The feature selection process requires a great deal of time and a knowledgeable
human developer. This paper addresses the problem of accelerating, feature selection for SMD inspection by introducing a
general methodology whose aim is to accelerate and eventually
automate the generation of SMD inspection routines. In particular, this paper focuses on solving the problem of automatically
selecting a subset of features among the larger set of features
known to provide at least reasonable discrimination for similar
components.
This paper focuses on using multivariate statistical techniques
to automate the feature selection process. When a large number
of features to select from is available, for instance in a database,

1545-5955/$20.00 © 2006 IEEE

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

395

Fig. 2. Side view of the AVI system.

Fig. 1. SMD defects proportions.

it is impossible to develop an exhaustive search of all the possible subset combinations to find the subset that gives the best
discrimination. For a discussion in the underlying complexity
of an exhaustive search, the reader is referred to [27]. In particular, in order to identify a subset of features to be used for
the inspection of new components, we explore the use of multivariate stepwise discriminant methods (MSDM) such as Wilks’
Lamba, unexplained variance, Mahalanobis Distance, smallest
distance, and Rao’s V. The subset selected by the MSDM in the
training phase of the classifier rule is termed “the best subset.”
In this phase, data are acquired to determine the features and
their corresponding parameters that provide an adequate level
of discrimination. When this phase is concluded, a classification rule is identified to be used for the actual inspection of the
components.
This paper is organized as follows. Sections II-A and B describe the experimental setting and the features used in this research. Section II introduces the background of the problem and
the platform to be used in the experiments. Section III describes
the feature selection problem. Section IV gives the framework
to deal with the classification problem as a multiple linear regression problem. Section V introduces the feature selection
problem as a multistep discrimination problem. Section VI illustrates the application of the methodology to SMD inspection
and presents some numerical results. Finally, Section VII offers
the conclusions of the work presented.
II. SMD INSPECTION PROBLEM
According to Feld [8], the two most common defects in electronics assembly are missing and misaligned components. He
reports (Fig. 1) that these two defects represent 40% of all the
defects found in SMD assembly.
Therefore, the detection and the correction of the underlying
causes of these defects would represent a significant improvement for the current state of the art of the electronics assembly.
In theory, human inspectors could be utilized to detect
missing components. However, in practice, the continuous
miniaturization and the increasing speed of assembly of these
components make, for all practical purposes, human inspection
obsolete and the use of AVI systems a necessity. However,
because of their lack of flexibility, these systems have been
used mostly as a way to detect defects rather than to use the

information generated by these systems to improve the underlying manufacturing systems. We believe that a key to provide
the flexibility needed in the AVI systems for SMD inspection
is the automation of the development and reconfiguration of
inspection routines. Because of the time it requires, a very
important element in achieving this goal is the automation of
the selection of the features to use for the inspection of SMD
components. The introduction of a methodology to automate
this part of the process is the main purpose of this paper.
In our discussion, we use an experimental platform available
at the Electronics Assembly Laboratory at Arizona State University. The platform used consists of a mechanical positioning
system, an image-acquisition system, an illumination system, a
Matrox® board, and a personal computer (PC). The image-acquisition system consists of four Pulnix™ cameras each with a
25-mm lens. The resolution of the system is 0.0703 mm/pixel.
The lighting system consists of 8 light-emitting diode (LED)
panels. Fig. 2 presents a picture of the experimental platform.
For a more detailed description of the experimental platform
and the image-acquisition methodology, the reader is referred
to [35]. In order to acquire the images of the components to be
inspected, the board is divided into “component windows” that
correspond to those areas of the board where a component is
supposed to be present. Once these component windows are defined, the system only examines the images of these regions.
A. Description of the Classification Features
One of the underlying practical requirements for the development of an automated feature selection process is the computational simplicity of the features. Another assumption is that
there are many features to select from. In order to build a system
that is reliable and easy to implement, the potential inspection
features should be computationally inexpensive, provide a good,
yet not perfect, level of discrimination between defective and
nondefective components, and present an appropriate level of
cross-correlation with the other considered. For illustration purposes, we introduce six representative features to classify a component. These features are termed energy (E), correlation (C),
diffusion (D), fenergy (F), blob (B), and texture (T). While these
features were carefully selected to illustrate the feature selection
method to be introduced in this paper, it must be mentioned that
the validity of the methodology described in this paper is independent of the particular features selected for this discussion. A
brief description of the features is given next. A more detailed
description of these features can be found in [35] and [36].

396

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

Energy is a measure of the “brightness” of an image and it
is given by the number of pixels whose value is above a certain threshold. If no pixels exist above this threshold, a value of
zero is assigned as the energy of the image. Ideally, a threshold
value is defined such that the perfect discrimination between
the population of present components and absent components is
achieved. Diffusion is the finite-difference diffusion equation. It
is a measure of the definition of local features. It can be thought
of as an iterative, local correlation operation, in which a mask
of a particular feature of interest is used to highlight all occurrences of the feature in a signal. This feature is computed on
the one-dimensional (1-D) projection obtained in the first step
of the computation of the correlation feature. The fenergy feature, which derives its name from its close relationship to the
energy feature, is a weighted sum of all the white pixels from
a projected image of an SMD component. The blob feature is
simply the average area of homogeneous regions defined by a
certain value of pixel brightness. In the case of the SMD inspection application being described, the value for this feature
is typically much higher for a nondefective image. The texture
feature is based on a comparison of the image under inspection
to an ideal image, which is prepared offline, similar to the case
of correlation. In order to show the computationally simplicity
of features used in this paper, the correlation feature is explained
next in more detail.
Correlation is a measure of the likeness of a signature obtained from the image when compared against a syntactic signal
representing the corresponding signature of a defect-free image.
The correlation feature is defined as the maximum value returned by the convolution operation between a signal of the
image under inspection and an ideal signal. In order to perform this operation in real time, a simplification of the process
is necessary. The simplification consists of taking the projection of the two-dimensional (2-D) image on its major axis. This
way, the convolution can be performed using two 1-D signals.
In order to add more stability to the resulting signal, its discrete first derivative is obtained. This process is illustrated in
Fig. 3. Fig. 3(a) presents the original image when a component
is present, Fig. 3(b) presents the projection of the 2-D image on
its major axis. Fig. 3(c) presents the derivative of this signal. In
order to apply the convolution operation, it is necessary to have a
syntactic signal representing a signal of an image with a component present. This signal is currently prepared offline for each
different type of component under inspection and it is totally
defined by three parameters: width of positive signal, width of
negative signal, and distance between the signal’s peaks. An example of this signal is given in Fig. 3(d) (and repeated for illustration purposes in Fig. 3(i). Once the projected signal and the
syntactic signal are available, the convolution operation takes
place. Fig. 3(e) presents the results of the convolution operation for an image with a component present. Figs. 3(f)–(j) show
the correlation process applied to an image with a component
missing. The peak or maximum value produced by this correlation will be the value of the correlation feature for the particular component under inspection. Ideally, if the component is
present, the maximum value returned by this operation is high,
and if the component is not present, the value is low. This can
be observed in Fig. 3(e) and (j).

B. Discrimination of Selected Features
Ideally, for each feature, a threshold value could be defined
to separate the values generated by defective and nondefective
images perfectly, thus rendering perfect discrimination between
the populations. In practice, this perfect discrimination is rarely
achieved, having to rely on several features to obtain a low-error
classification. For instance, the six features previously described
were applied to the images of a type of component (“805 V”)
present in a preselected printed-circuit board (PCB).
Fig. 4 presents a histogram of each feature for the nondefective and defective populations. As the reader can see, none of the
features used independently can result in free misclassification
errors. A usual approach is to use more than one feature, very
often in a sequential fashion, to classify the component under inspection as “nondefective (or present)” or “defective (absent).”
A problem with using a sequential approach is that it can result in inflating the type II error (classifying a nondefective component as defective). In the approach that we propose, a vectorbased approach is used. This vector approach is described in
Section III-A. However, the question that still remains is what
features to use to perform the classification of components. In
order to answer this question, we need to look beyond the levels
of discrimination provided by each feature independently.
First, we need to establish the level of discrimination desired.
Second, we need to determine the level of correlation among
the features being considered. The consideration of the level of
correlation among the features being considered is extremely
important in the development of an automated feature selection
process. For instance, from the information provided in the histograms of Fig. 4, if correlation is not considered and only two
features were to be selected, the obvious approach would be to
select the “C” and “D” features using the misclassification error
rate (MER) information from Table I. However, we will see later
that this is a particularly poor choice. A better option is to select features “C” and “B.” The way that the latter combination
was deemed better than the former will be explained later in this
paper. However, something that is worth mentioning is that this
is a surprising result since feature “B” would seem to be a poor
choice because of the high number of potential misclassification
errors rendered by this feature.
A possible explanation is given by the low level of cross correlation exhibited by these two features as presented in Table II.
This table shows the values of the cross correlation between
the six features analyzed. However, the question still remains
of what constitutes a good set of features and how to construct
these set of features. The methodology to be presented in Section V of this paper is an approach that we propose to answer
these questions.
III. FEATURE SELECTION PROBLEM
DEFINITION AND BACKGROUND
A classification function, or classifier, is a rule that decides
which population each point under analysis should be assigned.
One of the main issues in classification is determining the
number of features used in the classification function. In practical problems, usually a large number of features are initially
considered, only some are developed, and few are used in the
final implementation of the classification system. Hence, a

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

Fig. 3.

397

Steps of correlation feature.

problem that often arises is the way to accelerate the selection
process of the features.
The problem of feature selection has been addressed extensively for applications other than AVI systems, such as classification and diagnostic problems in engineering, social, and

medical sciences (see, for instance, [2], [5], and [24], respectively). The literature concerned with feature selection for discriminant analysis is vast; numerous references of applications
of this analysis appear in the literature; some examples include
[10], [12], [21], and [33].

398

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

Fig. 4. Defective and nondefective population histograms.
TABLE I
LEVEL OF DISCRIMINATION

TABLE II
CROSS-CORRELATION BETWEEN FEATURES

Reference [15] defines the problem of feature selection in
inspection systems as follows: “Given a set of features, sethat leads to the smallest classification
lect a subset of size
error.” Feature selection methods are often employed to determine which features are critical in order to reduce the classification error or the amount of data needed to train the classification
system.
A new area of application and development of the feature selection methods is called sensor selection. In this case, instead
of searching for features, the problem is to select the set of sensors that will result in the small classification error. Some in-

formative papers that address this topic are as follows: [4], [7],
[9], [11], and [32]. Therefore, the sensor or feature selection
problem can also be stated as an optimization problem.
A possible definition of the optimization problem is as follows: given feature sets of size , select the subset of size that
optimizes the measure subject to a set of constraints . The
measure can assume a multitude of forms. However, some
measures are commonly used to assess the performance of the
classification system. For instance, a measure that is commonly
used is the expected number of classification errors or the proportion of attempted classifications that result in an error, this
measure is called the MER. Common examples of constraints
imposed on the feature selection process include the maximum
number of features as part of the final classification set, a certain maximum level of probability misclassification, the total inspection time available, the discrimination power, and so forth.
In our case, in addition to the traditional constraints such as the
ones previously listed, the authors are also concerned with the
overall time needed to identify the set of features that will render
an acceptable classification. Therefore, it is not only important
to obtain a certain level of discrimination but also to reduce the
time and data as much as possible to identify an acceptable set of
features to perform the inspection phase. Since the data needed
for the discriminant analysis are directly related to the number
of features used, it is important to reduce this number as much
as possible [13]. As a consequence, if two different subsets of
features render the same level of discrimination, the one using
the least number of features is to be preferred in order to perform the inspection of the components.
The reduction in the number of features can be accomplished
by taking advantage of the cross-correlation or redundancies
among the features [19]. For instance, [20] states that increasing
the number of features does not necessarily result in an increase

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

in the discriminatory power and that it may indeed decrease if
too many unnecessary features are used. These authors also contend that the greater the number of features are, the larger
the sample needed to achieve the same level of discrimination.
However, the feature selection to be used for discrimination purposes is only part of the discrimination problem: in order to
detect the effectiveness of a discriminatory features subset, it
is necessary to know the way that this subset behaves in the
training and inspection phases.
Similar comments were made by [22]. Additionally, if the
sample size remains the same as more irrelevant features are
added, the power of the discriminant test decreases and the
variability of the estimates increases. As a result, decreasing
the number of features is useful in many cases [28]. Reference
[34] presents an argument for the reduction of features from
the perspective of time; they state that if the number of features
is ten or more, the statistical computation becomes an extremely time-consuming process, even with the use of modern
computers. Therefore, one of our goals centers on reducing
the number of features used in the classification process. The
purpose in feature selection involves minimizing the complexity of the methodology by including features that are as
computationally simple as possible while providing acceptable
levels of discrimination.
A. Feature Selection in the Context of the
SMD Inspection Problem
The AVI of SMDs depends on an efficient classification
system to determine if a component is defective or nondefective. Under this approach, each feature acts as a function whose
domain is the digital image and whose range is the real line.
It is hoped that each function will tend to map the images of
defective and nondefective components to distinct regions of
the real line for each population to make an easy and defect-free
classification process possible. When more than two features
are used in the inspection, the AVI classification problem
becomes one of discrimination between two multivariate populations. One of the problems in multivariate discrimination is
how to use the data rendered by different features. A frequent
approach uses classification functions or classifiers that take as
input the information provided by each feature and deliver a
single value to classify the image under analysis.
We base the classification methodology on a vector discrimination approach for the identification of defective components
whose basic characteristics have already been developed [35].
Although multiple classification functions exist (see, for instance, [7] and [14]), one function that is particularly well
suited for SMDs’ classification is known as the quadratic
classification function (QCF). The QCF has advantages over
the traditional sequential decision classification because the
probability of type I error does not increase when the number
of features used in the analysis increases. Another characteristic of this classifier is the simplicity of the features used in
it, which make the application of the classification method
possible for real-time automated inspection systems. The QCF
is the classifier that the authors use throughout this paper and
is briefly described next.

399

The QCF is a multivariate method used to discriminate different populations of observation and allocates new observations to the population previously formed. The QCF assigns an
observation under consideration to the most likely of popudenoted as
“defect free” and
lations (in our case,
“defective”). Since the real population parameters are not
known in advance, it is necessary to obtain their estimates from
samples obtained in a training phase. For more details about the
development of the QCF equation and the way that it is used in
the inspection of SMD components, see [18] and [35]. The QCF
for two populations using training data becomes:
(new observation) to
if
Allocate

(1)
to , where
Otherwise, allocate
value of multivariate classifier;

vector of feature values of the new inspected component;
sample covariance matrix for population ;
sample covariance matrix for population ;
sample mean vector of the features for population
;
sample mean vector of the features for population
;
cost of misclassification (observation from
is incorrectly classified as );
cost of misclassification (observation from
is incorrectly classified as );
prior probability ratio.
It is important to highlight two issues: first, the multivariate
classifier just described relies on assumptions of normality and
which is a
second, the classifier renders a single value of
measure of the closeness of the point under consideration to
a targeted population which determines the final classification
of the component under consideration. Regarding the normality
assumptions, commonly the data rendered by features used in
SMD inspection depart from these assumptions. However, the
QCF can accommodate well small departures from normality.
If the departures from normality are significant, the data can
be transformed to obtain better results. Reference [1] discusses
ways to deal with departures from normality.
One requirement for the use of the QCF is the availability
of a data set of features. We define the problem as finding the
methodology that automatically renders the minimal set of features that will meet the constraints of a minimum level of discrimination. Fig. 5 depicts the general organization of an envisioned environment for the automated generation of inspection
routines for SMD. In this paper, the authors focus on the feature selection module of this environment. The explanation to
be presented relies on the techniques used to select independent
variables (or regressors) used in regression analysis. For an introduction to these techniques, the reader is referred to [6] and
[26].

400

Fig. 5.

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

feature selection problem as follows: to create a multiple linear
regression equation for the particular response in terms of the
. For instance, assume
basic regressor or features
that
are all functions of one or more of the
’s and the ’s represent the complete set of features from
which the final equation is to be chosen, using the best subset
of features. The subset selected can include any functions of
the original set of features, such as linear, squares, cross-products, and so forth. Therefore, the feature selection process
will consist of identifying the best function. However, this
approach is problematic because the feature selection problem
has opposing criteria to determine the best subset, namely the
following two:
1) For discrimination purposes, the model should include as
many ’s as are necessary to keep the bias of the error
small, and to determine reliable MER values.
2) The cost and time involved in obtaining information on
a large number of ’s and subsequently monitoring the
features increment the variance of the predictor variable;
therefore, it is desired that one include as few ’s as possible in the model.
Notice that the feature selection problem in the multivariate
regression analysis and in the discriminant analysis is similar.
In fact, the mathematical background is almost the same for the
two problems. In multivariate regression, the principal objective centers on minimizing the square error between the model
and the experimental data while in multivariate discriminant
analysis, the principal goal involves minimizing the MER. For
instance, [13] mentions that the MER in discriminant analysis
(the coefficient of mulis inversely analogous to the statistic
tiple determination) of regression analysis. The MER reveals
how well the discriminant function classifies the data under
indicates how much variance of data
analysis while the
under analysis can be explained by the regression equation. In
the rest of the paper, we use discriminant analysis exclusively.

SMD inspection problem.

IV. CLASSIFICATION AS A MULTIPLE LINEAR
REGRESSION PROBLEM
In general, in a linear regression model, the response variable
is related to the predictor regressor (features) through the
following mathematical model:
(2)
Equation (2) is called a multiple linear regression model with
regressors. The parameters , for
, are called
the regression coefficients or partial regression coefficients. The
random error is denoted by . The model describes a hyperplane
in the -dimensional space defined by the regressor variables .
This formulation can also be a representation of the feature selection problem provided the response variable is the one used
to perform the discrimination between the defective and nondefective populations or the one that provides a mapping to the
appropriate region. Notice that in this case, would be a binary
variable, taking values of 0 or 1, accordingly. This representation will be used in the discrimination problem.
Different authors have researched the feature selection
problem under different contexts; for instance, [6] defined the

V. FEATURE SELECTION AS A MULTISTEP
DISCRIMINANT PROBLEM
In this paper, the authors focus on the use of multivariate stepwise discriminant methods (MSDM) to solve the feature selection problem that arises in the development of AVI algorithms.
The use of a stepwise procedure is a widely accepted practice
for feature selection when the comparison of all possible subsets is unfeasible. A typical procedure used by MSDM for the
selection of features is in following steps.
Step 1) Start with a discriminant function with no features
in it and a list of potential features candidates to be
used in the discriminant function.
Step 2) From the list of features, select the feature in the
list that maximizes a given criterion, add that feature to the model.
Step 3) The marginal contribution, as measured by the
given criterion, is calculated for each remaining
potential feature in the list. This marginal contribution is calculated over the feature(s) already in
the model.

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

Step 4)

The feature that in combination with the included
feature(s) maximizes the criterion is selected.
Step 5) If the selected feature meets a given condition it is
introduced into the model.
Step 6) The partial statistic for each feature in the model
is re-examined. If a feature does not meet a given
condition, it is removed from the model.
Step 7) Steps 2)–5) are repeated until no more features are
added to the model.
A pseudocode for the previous procedure is given by:
Let
; Initialize the Best current
Subset of features used by the classifier as empty
; Set of all potenLet
tial features
; Define AF as the set of
Let
features to be considered
Examine all the potential
For
features
; Select feature
from AF that maximizes a chosen figure
of merit FM given BS
threshold for inclusion
If
; update AF
if it is not
Continue; discard
promising

401

The previous description of a general MSDM relied on a partial statistic, or figure of merit, to guide the inclusion and exclusion of features. Numerous approaches are available to arrive at
this partial statistic. The authors tested the following five different methods in this research: Wilks’ Lambda, unexplained
variance, Mahalanobis Distance, smallest f ratio, and Rao’s V.
The principal difference between these methods is that each has
different criteria to select and remove the features that will be
included in the final subset. Since similar results were obtained
with the five methods, we only describe the Wilks’ Lambda
method in this paper because this method is the most commonly
used in practice. For details in the other four methods considered, the reader is referred to [25] and [28].
as the th observation taken from the th popuWe define
lation during the training phase of the development of the clas,
,
, repsifier. Where each
.
resents independently sample points observed from
Where is the total number of populations, is the number of
observations of the training data set, and represents the observation number. In matrix notation, the data obtained during the
training phase is given next

..
.

..
.

..
.

..
.

..
.

..
.

..
.

..
.

We use a short-hand notation for the sample totals and means as
follows:

else
; add

to the current best

subset
; update AF
For
Marginal contribution
;
marginal contribution of
in the new
subset
threshold
If (marginal Contribution
for removal)
; remove feature y from current best subset

For additional technical details about the previous procedure,
the reader is referred to [3]. The conditions discussed in Steps
4) and 5) above are parameters that may be specified by the
user, and the most typical ones are referred to as -to-enter and
-to-stay ( means the type I error). This parameter, together
with the criterion defined in Step 3), gives the decision thresholds to include or eliminate the features from a given subset.
Some recommended values of have been given in [23] in the
–
whereas [20] differs from this recommendarange of
tion, giving a value of 0.15.

Wilks’ Lambda method is based on Wilks’ –criterion, which is
a ratio expressing the total variability not explained by population differences [16]. This criterion is expressed in the following
equation:
(3)
where
is the determinant of the estimated generalized vari“deance within the two populations ( “defect free” and
fective”), represents the variation between populations, and
is the determinant of the estimated generalized total
variance of the training data. The definitions of these matrices
are given in (4) and (5), respectively
(4)

(5)

402

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

The
“hypothesis” matrix has, on its diagonal, the between sum of squares (BSS) for each of feature. The offdiagonal elements of are analogous sums of products for all pairs
of variables. The
“error” matrix
has on its diagonal the
within sum of squares (WSS) for each variable with the analogous sum of products offdiagonal. The has a Wilks’ Lambda
,
distribution with degrees of freedom (DOF) ,
and
. Reference [29] shows the values of this
distribution.
, in which larger
The value lambda is between
values indicate a poor separation between populations, and
smaller values indicate good separation between groups. The
reader is referred to [16] and [18] for more computational
details of this ratio.
Using Wilks’ statistic as a criterion for finding a good
subset is analogous in multiple regression analysis to finding
the set of variables that maximizes the -ratio. In fact, for
stepwise discriminant analysis, the partial -statistic is used to
estimate the corresponding -statistic. The partial -statistic,
denoted as
, is a ratio that measures the change in
that results from the addition of variable to the subset
.
is defined by the following
equation:
(6)
This equation has a Wilks’ Lambda distribution , ,
and is independent of
. Since
, this partial -statistic
has an exact -transformation given by the following equation:
(7)
Equation (7) becomes a partial statistics with the DOF of
and
. Recall that is the number of features in the model.
Therefore, this method selects for inclusion in the model the
variable that minimizes the overall Wilks’ value.
A. Measuring the Performance of the Selected Subset
Once a subset of features is identified, it is necessary to identify the metrics to be used to determine the effectiveness of the
selection. Several useful statistics can be used to measure the
performance of the selected subset by the MSDM; because of
simplicity, however, the authors focus on three nonparametric
statistics. These measures are based on the MER and are called
the following: the apparent error rate (APER), the expected actual error rate (EAER), and the inspection error rate (IER). The
first two methods are based on the training phase of the classifier
and the last one is based on the inspection or validation phase
of the AVI system. The three statistics represent the proportion
of errors that result from the use of selected features in the classifier. Each of these statistics is described next.
The APER is applied to data gathered during the training
phase and it is simply the proportion of components that are
misclassified for the given subset of features
(8)

the total number of defect-free observations,
where
the total number of defective observations, and
is the
total number of defect-free observations that are misclassified
is the total number of defecas being defective, and
tive observations that are misclassified as being defect free. The
APER statistic is a biased estimator of the true MER of the systems, because it is evaluated on the training data. This procedure is often misleading because it provides optimistic values
for MER, because the same observations that are used to compute the discrimination function in the training phase are used
to evaluate its performance in the validation phase [30].
The EAER is an indication of the way that a classification
function will perform in future samples of observations. The
statistic cannot be directly calculated because it depends on the
and . Howunknown density functions of the populations
ever, an estimated value can be projected using the method of
Lachenbruch’s holdout, which is also commonly referred to as
the cross-validation method [18]. The EAER is performed by
omitting one observation from the populations and developing
the QCF function based on the remaining observations. The absent observation is then classified using the QCF. The EAER is
a nearly unbiased estimate of the AER and EAER [33].
The inspection error rate (IER) is the statistic of the overall
errors percentages obtained from the inspection data set of observations. The IER is an unbiased and consistent estimator of
the MER because it uses two independent data samples, one
(training data) to build the QCF and the other (inspection data)
to evaluate it [15]. While the APER is an error percentage calculated on the training sample, the IER is calculated on the inspection sample. The IER is calculated in the same way as the
APER, but the difference lies in the use of the inspection sample
data instead of the training data.

VI. APPLICATION OF THE FEATURE SELECTION METHODOLOGY
The results to be presented are based on SMD images obtained with the experimental platform introduced in Section II.
These images are classified based on multiple features considered simultaneously in the QCF previously described. This
vector-based decision approach is used to perform the classification of an image as defective (SMD component absent)
or nondefective (SMD component present). The six features
described in Section II are used as candidates for inclusion in
the QCF to evaluate the MSDM methods previously described.
The first step consisted of getting data from a particular type
of component present in the PCB inspected (i.e., the 805-V component). This component was selected because it was the most
common in the PCB inspected. The parameters of the six features were fine-tuned for the inspection of this component. The
frequency distributions of the six features for the defective and
nondefective populations were obtained next. The resulting distributions were tested for normality. Although some of the distributions did not pass the normality tests (Kolmogorov–Smirnov
and Chi–Square), the decision was made to not transform the
data. The reason behind this decision was that our aim was to
test the overall performance of the feature selection methods and
not the performance of the QCF.

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

In order to conduct additional stepwise discriminant experiments, it was assumed that the full data set was composed
of only five potential features instead of six. For example,
the full set of features (energy, correlation, diffusion, fenergy,
, which represents
blob, and texture) is labeled
the original set of features to be examined in the experiment.
Under the new assumption, the following new feature sets of
size five may be denoted as new six independent experiments
,
,
,
,
,
. These six subsets and the original subset made
and
the seven groups of features used in the experiments compare
the efficiency of the five stepwise discriminant methods based
in the level of discrimination between the populations and the
number of features selected on the final subset.
Two statistics were created to measure the efficiency of the
stepwise methods. The statistics evaluate the performance of
the subset selected by the MSDM. The objective of these statistics centers on ranking each subset of features and identifying
the position of the subset selected by the MSDM. The statistics
are based on the three MER statistics previously described. The
first statistic is called MER criteria (MERc). MERc uses IER to
rank the subsets. When two or more subsets have the same IER
value, then the other two error rates—EAER and APER—are
used as primary and secondary tiebreakers, respectively. Then
each subset is ranked from 1 to , where is equal to the total
number of subsets, based on their respective MERc.
The second statistic is the error-type ratio (ETR). Generally,
in the manufacturing field, it is considerably more serious to
incur in a type II error (whose probability is denoted by ) in
the inspection phase than a type I error (denoted as ). In AVI
systems, type II errors are also considered more serious and usually have higher associated costs. Therefore, the proportion of
the expected number of type II errors to the overall expected errors should be examined in order to evaluate the performance
of a particular subset of features. The and values are estimated using the number of type I and type II errors obtained in
is computed
the training phase of the classifier. The ratio
and is referred to as the ETR. A large ETR value indicates that
the likelihood of type II errors is much greater than type I errors
and, therefore, a value less than one is desirable.
A. Numerical Results
The PCB used in the experiment is comprised of 281 elements of the 805-V component. The AVI system inspected 40
printed-circuit boards PCB for the two populations: present and
absent components. The data set obtained for the training phase
was based on the average of 30 runs and the remaining ten
runs were assigned to the inspection phase. Therefore, 281 data
points were available for the training phase and 2810 data points
for the inspection phase for each population.
The five multivariate stepwise discriminant methods mentioned in Section IV were tested with the data obtained by the
AVI system, this with the objective to compare the performance
of each method. The objective of this comparison involved determining which method gave the best subset: the subset with the
minimum MER using the minimum number of features. In hypothesis testing, this value is equivalent to (i.e., not including
a feature into the model when, in fact, the feature is relevant

403

TABLE III
MULTIVARIATE STEPWISE DISCRIMINANT SUMMARY

to discriminate between two populations). A common value of
used in hypothesis testing is 0.05. However, in our case, we
use higher levels of in each MSDM step to include or remove
one feature from the subset because we want to have more certainty that a feature is really relevant when it is included in the
subset or irrelevant when it is removed from the subset. Table III
presents the progression of the variable selection process of the
MSDM along with its associated Wilks’ Lambda and values
for the particular case of
. The five methods gave the
same subset of features; correlation, diffusion, fenergy and texfor any value of in the range of
–
. In
ture,
the example presented in Table III, no variables were removed
from the set of features once they entered the set.
In addition, the MER was calculated for the selected subset
and was compared to the full set of six features
. Table IV represents the ranking system created
to evaluate the performance of each subset. This table illustrates
the results of using the six features, providing a total of 63
different combinations of subsets. The measure used to rank
the different subsets was the MERc previously above. The ETR
was used as a tiebreaker when two subsets had the same MERc
values.
The previous table shows that the selected subset from the
performs better than the full set of
stepwise methods
features
because it is ranked in position 6 instead
of position 28 of the complete set. The significance of obtaining
is the decrease in the MER and the reducthe subset
with
tion in time and cost of acquiring two less features
respect to the complete set.
The summary of the results of the stepwise experiments
conducted on the other six groups of five features previously
,
,
,
,
described
, and
are presented in Table V. The five
methods of stepwise resulted in the same suggested subset for
each experiment.
Table VI shows the ranked positions for the complete set and
subset determined by the MSDM procedure for the seven experiments. The second column presents the number of features
for each complete set. The third column gives the number of
different subsets derived from the number of features given in
column 2. Rank 1 displays the position of each full set for all
possible combinations. Rank 2 reveals the location of the best
subset selected by all five stepwise methods. The last column
illustrates the difference between the ranks; this number represents the number of ranking error positions that the best subset
incremented or decreased regarding the full set.
Table VII illustrates that six of the seven subsets selected by
the stepwise methods resulted in less classification errors than

404

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

TABLE IV
SUBSET RANK

TABLE V
SUMMARY OF MSDM

TABLE VI
RANK POSITIONS USING MSDM

using the original set of features. It can be concluded that in
general, the proposed techniques use less features and render
less classification errors than when using the full set of features

TABLE VII
REDUCTION OF THE IER

available. This conclusion means that using fewer features resulted in lower IER values in the experiments. The values of the
IER are presented in Table VII.
Although Table VI shows that the MSDM renders a subset
of features providing better discrimination than the full set of
features, it also shows that the MSDM did not result in the best
subset. There are several reasons to explain this situation. The
first reason is the MSDM are based on the assumption that the
features values are from a multinormal population with equal
covariance matrices. In our case, the features do not meet this
assumption. The other reason is that the partial statistics used for
the inclusion or exclusion of features is based in maximizing the
statistical distance between the populations, not in minimizing
the MER. However, if there is a departure from the normality
assumptions, the relationship between statistical distance and
misclassification errors is not necessarily linear.

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

In this case, we decided to rank the performance of the subsets based in the MER instead of the statistical distance because of its practical importance in the inspection process. Finally, there is the issue of correlation; the SMDM may not reach
the “best” subset in any sense, except when the correlation between the features is zero, a situation that, in practice, hardly
exists. Thus, it is important to highlight that any feature selection process needs to consider the correlation between features
to obtain better results as shown in the previous examples. The
feature selection procedure introduced, while not perfect, represents an efficient way to handle the cross correlation exhibited
from the features being considered. For instance, Table II shows
that the values of the correlation between features are between
0.27 and 0.93.
VII. CONCLUSION
In this paper, the authors showed that using multivariate stepwise discriminant analysis methods can result in the selection of
reduced subsets of decision features for the inspection of SMD
components. These subsets of features perform usually better
than the complete set of decision features. The authors have also
shown that a careful selection of ranking metrics is essential for
the selection of features. In particular, our findings reveal that
the use of the Wilks’ Lambda method is a good alternative for
the selection of features. One of the main findings is that the
multivariate stepwise discriminant methods present a good option to automate the feature selection process in the development
and reconfiguration of automated visual inspection systems. The
authors believe that the feature selection method presented can
be extended to the sensor selection problem when the aim is to
recognize or discriminate between populations.
Although this research is still in its preliminary stages, the
results reveal the importance of having an efficient feature selection methodology for automating the generation of inspection algorithms. Obtaining an efficient feature selection methodology is the starting point for additional development in the
automated generation of inspection algorithms. While the results obtained in this paper support the proposed methodology
as a way to automate feature selection, these results also show
that there is much room for improvement. For instance, with a
methodology that takes more explicitly into account, the individual contributions of a single or of a combination of features
under different levels of correlation should be explored in the
future. Additional work should also include the refinement of
the methodology to explicitly include the time needed for feature selection as well as the refinement of the figure of merit for
the inclusion/exclusion of features in the resulting feature set.
Another fertile area for research is the development of classifiers that depart from the normality assumptions usually made
by the classifiers.
REFERENCES
[1] M. Arellano, “Analysis and development of a vector classification technique for SMD components,” M.S. thesis, Dept. Elect. Comput. Eng.,
Univ. Texas at El Paso, El Paso, 1999.
[2] M. J. Baxter, “Stepwise discriminant analysis in archaeometry: A Critique,” J. Archaeol. Sci., vol. 21, pp. 659–699, 1994.
[3] K. N. Benk, “Forward and backward stepping in variable selection,” J.
Stat. Comput. Simul., pp. 177–185, 1980.

405

[4] R. Debouk, S. Lafortune, and D. Teneketzis, “On an optimization
problem in sensor selection,” Discrete Event Dynamic Syst.: Theory
Appl., vol. 12, pp. 417–445, 2002.
[5] Y. Ding, P. Kim, D. Ceglarek, and J. Ji, “Optimal sensor distribution
for variable diagnosis in multistation assembly processes,” IEEE Trans.
Robot. Autom., vol. 19, no. 4, pp. 543–554, Aug. 2003.
[6] N. Draper and H. Smith, Applied Regression Analysis, 3rd ed. New
York: Wiley, 1998.
[7] R. Duda, P. Hart, and D. Stork, Pattern Classification, 2nd
ed. Hoboken, NJ: Wiley, 2001.
[8] M. Feld, “Simplification of automated optical inspection systems for
assembled circuit boards,” SMT Expr., vol. 3, no. 4, 2001.
[9] L. M. Fraleigh, M. Guay, and J. F. Forbes, “Sensor selection for modelbased real-time optimization: Relating design of experiments and design
cost,” J. Process Control, vol. 13, pp. 667–678, 2003.
[10] J. J. Glenn, “Integer programming methods for normalization and
variable selection in mathematical programming discriminant analysis
models,” J. Oper. Res. Soc., vol. 50, no. 10, pp. 1043–1053, Oct. 1999.
[11] I. Gunyon, S. Gunn, M. Nikravesh, and L. Zadeh, Variable/Feature Selection and Ensemble Learning: A Dual View. New York: SpringerVerlag, 2005, ch. Feature Extraction, Foundations and Applications.
[12] A. Gupta, T. Logan, and J. Chen, “A variable selection technique in
discriminant analysis with application in marketing data,” J. Statist.
Comput. Simulation, vol. 63, no. 2, pp. 187–199, 1999.
[13] J. Hair, R. Anderson, R. Tatham, and W. Black, Multivariate Data Analysis With Readings, 4th ed. Englewood Cliffs, NJ: Prentice-Hall, 1995.
[14] T. Hastie, R. Tibshirani, and J. Firedman, The Elements of Statistical Learning, Data Mining, Inference and Prediction. New York:
Springer, 2001.
[15] A. Jain, R. Duin, and J. Mao, “Statistical pattern recognition: A review,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 1, pp. 4–37, Jan.
2000.
[16] R. I. Jenrich, “Stepwise discriminant analysis,” in Statistical Methods
for Digital Computers, 1997, ch. 8, pp. 576–95.
[17] S. Jiang, R. Kumar, and H. Garcia, “Optimal sensor selection for discrete-event systems with partial observation,” IEEE Trans. Autom. Control, vol. 48, no. 3, pp. 369–381, Mar. 2003.
[18] R. A. Johnson and D. W. Wichern, Applied Multivariate Statistical Analysis, 3rd ed. Upper Saddle River, NJ: Prentice-Hall, 1996.
[19] S. Kachigan, Multivariate Statistical Analysis. New York: Radius
Press, 1982.
[20] R. Khattree and D. Naik, Multivariate Data Reduction and Discrimination With SAS Software. New York: Wiley, 2000.
[21] E. Kinney and D. Murphy, “Comparison of the ID3 algorithm versus discriminant analysis for performing feature selection,” Comput. Biomed.
Res., vol. 20, no. 5, pp. 467–476, Oct. 1987.
[22] W. J. Krzanowski, Principles of Multivariate Analysis, a User’s Perspective. Oxford, U.K.: Oxford Science, 1998.
[23] C. Le, Applied Categorical Data Analysis. New York: Wiley, 1998, pp.
127–147.
[24] K. Lee, N. Sha, E. Dougherty, M. Vannucci, and B. Mallick, “Gene selection: A Bayesian variable selection approach,” Bioinformatics, vol.
19, no. 1, pp. 90–97, 2003.
[25] G. McCabe, “Computations for variable selection in discriminant analysis,” Technometrics, vol. 17, no. 1, Feb. 1975.
[26] D. Montgomery, E. Peck, and V. Geoffrey, Introduction to Linear Regression Analysis, 3rd ed. New York: Wiley, 2001.
[27] T. Pavlenko, “On feature selection, curse-of-dimensionality and error
probability in discriminant analysis,” J. Statist. Planning Inference, vol.
115, no. 2, pp. 565–584, Aug. 2003.
[28] A. Rencher, “The contribution of individual variables to hotelling T ,
wilks and R ,” Biometrics, vol. 49, pp. 479–489, Jun. 1993.
[29]
, Methods of Multivariate Analysis, 2nd ed. New York: Wiley,
2002.
[30] A. Rshirsagar, Multivariate Analysis. New York: Marcel Dekker,
1972.
[31] C. Rutter, V. Flack, and P. Lanchenbruch, “Bias in error rate estimates
in discriminant analysis when stepwise variable selection is employed,”
Commun. Statistics–Simulation Comput., vol. 20, no. 1, pp. 1–22, 1991.
[32] S. Sampatraj, K. Abhishek, and Y. Ding, “A survey of inspection
strategy and sensor distribution studies in discrete-part manufacturing
processes,” Ph.D. dissertation, Dept. Ind. Eng., Texas A&M Univ.,
College Station, 2005.

1

406

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

[33] O. Snorrason and F. Garber, “Evaluation of nonparametric discriminant
analysis techniques for radar signal feature and extraction,” Opt. Eng.,
vol. 31, no. 12, pp. 2608–2617, Dec. 1992.
[34] M. Tatsuoka and P. Lohnes, Multivariate Analysis, 2nd ed. New York:
Macmillan, 1998.
[35] J. R. Villalobos, M. Arellano, A. Medina, and F. Aguirre, “Vector classification of SMD images,” J. Manuf. Syst., vol. 22, no. 4, pp. 265–282,
2004.
[36] L. Williams, “Development of a feature selection methodology for automated visual inspection systems,” Dept. Thesis Coursework-Ind. Eng.,
Arizona State Univ., Tempe, 2004.

J. René Villalobos received the B.S. degree in industrial–mechanical engineering from the Chihuahua
Technological Institute in Mexico, Chihuahua,
Mexico, in 1982, the M.S. degree in industrial engineering from the University of Texas at El Paso in
1987, and the Ph.D. degree in industrial engineering
from Texas A&M University, College Station, in
1991.
Currently, he is an Associate Professor in the Department of Industrial Engineering with the Ira A.
Fulton School of Engineering at Arizona State University, Tempe. His research interests are in the areas of logistics, automated
quality systems, manufacturing systems, and applied operations research.

Hugo C. Garcia received the B.S. degree in industrial and systems engineering from the Instituto
Tecnológico y de Estudios Superiores de Monterrey,
Campus Ciudad Juárez, Juarez, Mexico, in 2000; the
M.B.A. degree from the Universidad Autónoma de
Ciudad Juárez, Juarez, in 2002; and the M.S. degree
in statistics from The University of Texas at El Paso
in 2003. He is currently pursing the Ph.D. degree in
industrial engineering at Arizona State University,
Tempe, and the M.S. degree in industrial engineering
at Instituto Tecnológico de Ciudad Juárez, Juarez.
His main research interest is in automated visual inspection systems.

George C. Runger received the B.S. degree in industrial engineering from Cornell University, Ithaca, NY,
and the Ph.D. degree in statistics from the University
of Minnesota, Minneapolis.
Currently, he is a Professor in the Department of
Industrial Engineering at Arizona State University,
Tempe. His research is on real-time monitoring and
control, data mining, and other data-analysis methods
with a focus on large, complex, and multivariate data
streams. His work is funded by grants from the National Science Foundation and corporations. He was
a Senior Engineer with IBM, Tucson, AZ.

European Journal of Operational Research 254 (2016) 398–409

Contents lists available at ScienceDirect

European Journal of Operational Research
journal homepage: www.elsevier.com/locate/ejor

Discrete Optimization

Robust eﬃciency measures for linear knapsack problem variants
Christopher Wishon, J. Rene Villalobos∗
International Logistics and Productivity Improvement Laboratory, Arizona State University, Tempe, AZ 85287-8809, United States

a r t i c l e

i n f o

Article history:
Received 29 June 2015
Accepted 13 April 2016
Available online 21 April 2016
Keywords:
Combinatorial optimization
Heuristic
Genetic Algorithm
Knapsack problem

a b s t r a c t
Numerous combinatorial optimization applications, such as the mobile retailer product mix problem, utilize the multidemand, multidimensional knapsack problem variant in which there are multiple knapsack
constraints requiring a weighted summation of the variables to be less than or equal to a nonnegative
value and multiple demand constraints requiring a weighted summation of the variables to be greater
than or equal to a nonnegative value. The purpose of this paper is to demonstrate that core variables
and eﬃciency measures, concepts used in the most eﬃcient solvers to-date for binary knapsack problems and some of its variants, can be extended to the multidemand, multidimensional knapsack problem
variant. Speciﬁcally, new eﬃciency measure calculations are provided and their properties are mathematically proven and experimentally demonstrated. The contribution of such measures to knapsack problem research is that these measures are applicable to all knapsack problem variants with a single linear objective function and linear constraints of any quantity. The applicability of these new measures is
demonstrated through the development of three heuristic procedures: a Fixed-Core heuristic, a Genetic
Algorithm, and a Kernel Search heuristic. The results from these tests are compared with the results from
a commercial solver and an existing heuristic. The ﬁndings from these tests demonstrate that the FixedCore and Kernel Search heuristics developed for this paper are the most eﬃcient solvers to-date for hard
multidemand, multidimensional knapsack problems.
© 2016 Elsevier B.V. All rights reserved.

1. Introduction
Recent advances in solving Knapsack Problems (KPs) have
utilized the concepts of core variables and eﬃciency measures.
The concept of core variables for KPs was ﬁrst introduced by Balas
and Zemel (1980) who observed that only a small percentage of
the decision variable values in a KP change between the optimal
binary solution and the optimal linear relaxation solution. The
variables whose values differ between these two solutions were
then deﬁned as the ‘core variables’. Further experimentation
indicated that the quantity of core variables increases at a less
than linear rate with respect to the number of decision variables
(Pisinger, 1999). Hence, identifying these variables can greatly
reduce the computational effort when solving the binary problem,
especially for large problem instances.
The obvious challenge with identifying these core variables
is that it requires knowledge of the solution to the binary KP
which defeats the purpose of exactly identifying this set. However,
research has identiﬁed closed form ‘eﬃciency measures’ which use
∗

Corresponding author. Tel.: +1 480 965 0437.
E-mail address: rene.villalobos@asu.edu, rene.villalobos@tisconsulting.org (J.R.
Villalobos).
http://dx.doi.org/10.1016/j.ejor.2016.04.025
0377-2217/© 2016 Elsevier B.V. All rights reserved.

the problem-speciﬁc coeﬃcients to rank the variables such that
those variables most likely to be in the core are clustered together.
A majority of modern KP techniques utilize these measures to
create extremely eﬃcient solution procedures, both exact and
heuristic, for solving the binary KP and some of its variants.
To date, eﬃciency measures have been developed for the basic
KP as well as for the multidimensional KP (MKPs) variant which
has multiple knapsack constraints. These measures are typically a
ratio of the objective coeﬃcient for the variable over the weighted
sum of the constraint coeﬃcients (Puchinger, Raidl, & Pferschy,
2010). One of the preferred weighting techniques in these measures is to use the optimal dual variable values from the linear
relaxation solution as they serve as a dependent measure of each
constraint’s importance with respect to the objective function.
The disadvantage of these measures is that they have strict
assumptions with regards to problem structure. With respect to
this research, the ﬁrst and most important assumption in current
eﬃciency measures is that there are no demand constraints in
the problem. Within KP terminology, a demand constraint is
any constraint which has a weighted sum of decision variables
which must be greater than or equal to a given threshold. The
most universal KP variant which contains these constraints is the
multidemand MKP (MDMKP) which has multiple knapsack and

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

multiple demand constraints. The second assumption for current
eﬃciency measures is that the objective and constraint coeﬃcients
are non-negative. Such an assumption is typically valid for MKPs,
but it may no longer be valid for MDMKPs as including items
with a negative objective coeﬃcient may be necessary to satisfy
demand constraints and including items with a negative knapsack
constraint coeﬃcient may not be automatic depending on the
values of the variable’s other coeﬃcients. Hence, new measures
are needed if more complex KP variants are to be solved using
recent KP solution techniques.
Solving the MDMKP is motivated by its applicability to other KP
problems as well as its applications to documented problems in
the literature. The principal application of interest for this research
is in determining the ideal product mix for a mobile retailer which
is constrained by a traditional KP constraint modeling the space of
the retailer as well as by a constraint for requiring the product mix
to meet or exceed a given revenue threshold (Wishon & Villalobos,
2016). Additional applications include selecting an optimal project
portfolio (Beaujon, Marin, & McDonald, 2001), locating and routing
for obnoxious facilities (Cappanera, Gallo, & Maﬃoli, 2004), and
the sea cargo mix problem (Ang, Cao, & Ye, 2007).
The principal goal of this article is to demonstrate that closedform eﬃciency measures exist for the MDMKP. The beneﬁt of
such measures is that they are robust eﬃciency measures for
all KP variants with a single linear objective function and linear
constraints with any type of equality or inequality since these
formulations can be transformed into a MDMKP. The developed
robust eﬃciency measures will then be employed in three solution techniques to demonstrate the eﬃciency measures’ utility
in solving MDMKPs. These solution techniques were selected as
they demonstrate that the new measures can be used to extend
existing KP solution techniques to solve MDMKPs.
The remainder of the article is as follows. Section 2 summarizes
the most relevant MDMKP solution methodologies as well as the
most recent solution methods for all KP variants that utilize eﬃciency measures. Section 3 introduces the new eﬃciency measures
and mathematically proves their properties. Section 4 experimentally demonstrates that the robust eﬃciency measures provide
the same beneﬁt for MDMKPs as the traditional measures do for
simpler KPs while Section 5 introduces three heuristic solution
procedures which utilize the new measures. Finally, Section 6 concludes with a summary of all completed work along with a
discussion of algorithmic shortcomings and future research goals.

2. Background
The literature in this section covers two subsections of modern
KP research. First, a summary is provided on the use of core
variables and eﬃciency measures in KPs with speciﬁc emphasis
on the use of eﬃciency measures for KP variants. This is followed
by a review of the current literature on solving MDMKPs. Those
techniques ranking variables with pseudo-eﬃciency measures will
be given special attention.
The concept of core variables for traditional, binary KPs was
ﬁrst introduced by Balas and Zemel (1980) who observed that
dynamic programming solutions for KPs relied on a complete
sorting and branch and bound algorithm for all of the decision
variables even though only a small interval of the sorted variables
differed between the linear relaxation solution and the optimal
binary solution. Balas and Zemel used this observation to create
a partial sorting methodology and heuristic which was able to
quickly develop good solutions for the binary KP. This sorting was
completed by using the ratio of the objective coeﬃcient over the
constraint coeﬃcient for each variable, a technique ﬁrst employed
by Dantzig (1957) to solve the linear KP. These ratios have since

399

been employed as the standard eﬃciency measures for traditional,
binary KPs.
Martello and Toth (1988) then used this partial sorting to
create an eﬃcient, exact solution procedure which ﬁrst assumed
an approximate core prior to reducing the problem and solving
the remainder through a depth-ﬁrst branch and bound procedure.
The next advancement was from Pisinger (1995b) whose algorithm
completed a sorting-as-needed, depth-ﬁrst, branch and bound that
prioritized branching based on the variables most likely to be in
the core as determined from their eﬃciency measures. Pisinger
(1997) then updated this algorithm to use a breadth-ﬁrst approach
which proved to be superior. At the same time, Martello and Toth
(1997) developed their own algorithm for solving hard KPs using
strong bounding rules. These last two works were then combined
by Martello, Pisinger, and Toth (1999) to create the fastest exact
solution method for binary KPs to date.
Other recent advances have focused on expanding the core variable and eﬃciency measure concepts to KP variants. The greatest
contribution of such research is in the development of eﬃciency
measures for problems with multiple knapsack constraints. These
types of measures were ﬁrst introduced by Dobson (1982) who
used a measure which was the ratio of the objective coeﬃcient
over the sum of the constraint coeﬃcients. These measures have
since been updated to feature a weighted sum of the constraint
coeﬃcients, typically weighted by the optimal dual variables, as
demonstrated by Angelelli, Mansini, and Grazia Speranza (2010),
Puchinger et al. (2010), and Della Croce and Grosso (2012). In addition, either the eﬃciency measure for MKPs or the measure for
standard KPs has been used to solve other KP variants including an
equality KP (Volgenant & Marsman, 1998), bounded KP (Pisinger,
20 0 0), unbounded KP (Martello & Toth, 1990), multiple-choice KP
(Pisinger, 1995a), multiple-choice MKP (Ghasemi & Razzazi, 2011),
and multi-objective KP (Gomes da Silva, Clímaco, & Rui Figueira,
2008; Lust & Teghem, 2012; Mavrotas, Rui Figueira, & Florios,
2009). For those interested in more information, concise reviews
exist for solving KPs or their variants using core approaches, either
exactly (Dudziński & Walukiewicz, 1987; Martello, Pisinger, & Toth,
20 0 0) or heuristically (Wilbaut, Hanaﬁ, & Salhi, 2008).
While techniques for solving traditional KPs is substantial,
research into solving MDMKPs is limited in comparison. The
ﬁrst focused research into a solution method for the MDMKP is
from Cappanera and Trubian (2005) who developed a tabu-search
heuristic which searches the near-infeasible solution space for
feasible solutions which are then used as seeds for local searches
within the feasible region. A tabu-search procedure was later
developed by Arntzen, Hvattum, and Løkketangen (2006) with
greater focus on exploring the infeasible solution space compared
with Cappanera and Trubian. Following this research, Hvattum
and Løkketangen (2007) developed a scatter search method which
found feasible solutions by performing various mathematical
combinations of previously identiﬁed feasible solutions. Later,
Hvattum, Arntzen, Løkketangen, and Glover (2010) developed an
alternating control tree procedure which uses the linear relaxation
solution to create subproblems of the initial MDMKP. The research
by Hvattum et al. is unique as it is the sole solution method
which can determine the optimal binary solution so long as an
optimal solution procedure is applied to the subproblem. Another
solution procedure is from Balachandar and Kannan (2011) whose
dominance-based heuristic ranks variables based on the value
of constraint coeﬃcients such that eﬃcient additions/removals
of variables are made from the ﬁnal solution set. While such an
approach does rank the variables, it does not utilize all problem
information such as the objective coeﬃcients or the relative
importance of each constraint. Hence, none of the aforementioned
research applies eﬃciency measures or core variables to improve
their solution procedures.

400

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

3. Eﬃciency measures
To provide the MDMKP eﬃciency measures, consider the formulation for the general bounded MDMKP given below. Within
KP terminology, a bounded problem is one in which the decision
variables are no longer binary but can be any integer number
between zero and a given upper bound.

(MDMKP ) Maximize z =

n


c jx j

(1)

ej =

j=1
n


∀i ∈ {1, . . . , m}

ai j x j ≤ bi

constraints while the ﬁnal q variables are associated with the
demand constraints. The other set of dual variables is denoted as
vLR
for all i ∈ {1, . . . , m} which are associated with constraint set
j
(5). All other coeﬃcients are the same as the LMDMKP.
Given these formulations, the eﬃciency measures e j for any
decision variable j can be calculated as

(2)

∀i ∈ {m + 1, . . . , m + q}

ai j x j ≥ bi

(3)

j=1



x j ∈ 0, . . . , d j



∀ j ∈ {1, . . . , n}

(4)

In total, there are n decision variables denoted by x j and the
objective given by (1) is to maximize the summation of these variables weighted by c j . The MDMKP is constrained by m knapsack
constraints given as constraint set (2) and q demand constraints
given as constraint set (3). Finally, each variable is bounded such
that it can have any integer value between 0 and d j as denoted
by (4). The bounded form is used in this formulation since it is
a more robust variant and the binary MDMKP is a special case
where d j = 1 for all j.
In this formulation, the only assumption is that bi ≥ 0 for
all i ∈ {1, . . . , m + q}. Hence, c j and ai j for all i and j are unrestricted in sign which differs from prior knapsack problem
research. If some bi is negative, the constraint can be negated and
substituted to the other constraint set. Furthermore, well-stated
n
MDMKPs assume that
j=1 d j ai j > bi ∀i ∈ {1, . . . , m + q} since a
violation for any i ∈ {1, . . . , m} would imply that some knapsack
constraint would never be violated and that a violation for any
i ∈ {m + 1, . . . , m + q} would indicate that the problem is infeasible.
To develop the eﬃciency measures, the LP relaxation of the
bounded MDMKP (LMDMKP) is required. The LMDMKP formulation is the same as the MDMKP except all integer variables are
replaced with their linear counterparts, xLR
, and constraint set (4)
j
is replaced with the set of constraints given in (5) and (6).

xLR
j ≤ dj
xLR
j

∈ R+

∀ j ∈ {1, . . . , n}

(5)

∀ j ∈ {1, . . . , n}

(DLMDMKP ) Minimize zDLR =

i=1

m+q

bi uLR
i −



bi uLR
i +

i=m+1

n


i=1



LR
ai j uLR
i + vj ≥ cj

∀ j ∈ {1, . . . , n}

(iii) If 0 < xˆLR
< d j , then e j = 1
j
Proof. Consider any variable j such that c j < 0. The other possible
values for c j will not be explicitly proven but can be easily demonstrated using similar reasoning. To prove Theorem 1, the following
properties from complementary slackness of the LMDMKP and its
dual formulation are required:
m


∀i ∈ {1, . . . , m + q}

vLR
j ∈ R+

∀ j ∈ {1, . . . , n}

ai j uˆLR
i −

i=1





m+q



ˆ LR
ˆLR
ai j uˆLR
i +v
j − cj x
j = 0,

(12)

i=m



ˆ LR
xˆLR
j − dj v
j = 0.

(13)

In the case of (i), xˆLR
= d j and (12) implies that
j

j=1

(8)

i=m

uLR
i ∈ R+

cj < 0

(ii) If xˆLR
= 0, then e j ≥ 1
j

m

i=1

m+q

(11)

(i) If xˆLR
= d j , then e j ≤ 1
j

d j vLR
j
(7)

ai j uLR
i −

i=1

cj = 0

Theorem 1. For any j ∈ {1, . . . , n}, let xˆLR
represent the optimal
j
LMDMKP solution value. The following facts hold:

(6)

m


ai j uˆLR + 1

cj > 0

where uˆLR
is the optimal solution value for the dual variable uLR
.
i
i
Prior to demonstrating that these measures rank the decision
variables according to their likelihood of being a core variable,
other properties will ﬁrst be demonstrated. First and foremost, the
new robust eﬃciency measures provide an equivalent ranking of
the decision variables as compared to existing measures for both
standard KPs and MKPs. To demonstrate this equivalence, note
that for either of these problems it is commonly assumed that
all objective coeﬃcients are positive in which only the ﬁrst case
in (11) must be considered. In addition, the second summation
in the numerator of this calculation can be removed as there are
no demand constraints in these formulations. This makes the new
eﬃciency measures the inverse of the measures presented by
Puchinger et al. (2010) which therefore provide the same rankings
but in the reversed order.
Furthermore, observe that the measures partition the variables
according to their optimal solution values for the LMDMKP. This
property is given in Theorem 1.



Finally, the following formulation represents the dual formulation of the LMDMKP.

m


ai j uˆLR −

i
i
⎪
i

=1
i=m+1

⎪
⎪
m
+
q
m
⎪


⎪
LR
⎩
ˆ
ai j uˆLR
−
a
u
/c j + 2
ij i
i
i=m+1

j=1
n



⎧
 m
m
+q


LR
LR
⎪
ˆ
ˆ
a
u
−
a
u
/c j
⎪
ij i
ij i
⎪
⎪
i=1
i=m+1
⎪
⎨
m
+q
m


(9)
(10)

The DLMDMKP is deﬁned by two sets of decision variables. The
ﬁrst set is denoted as uLR
for all i ∈ {1, . . . , m, m + 1, . . . , m + q}
i
where the ﬁrst m variables are associated with the knapsack

m+q

ai j uˆLR
i −



ˆ LR
ai j uˆLR
i +v
j − c j = 0.

(14)

i=m

Since c j < 0, careful algebraic manipulation and vˆ LR
≥ 0 demonj
strates that e j ≤ 1. In the case of (ii), xˆLR
= 0 and (13) implies that
j

vˆ LR
= 0. By substituting vˆ LR
in (8) along with c j < 0, careful algej
j

braic manipulation demonstrates that e j ≥ 1. Finally in the case of
(iii), xˆLR
= d j and (13) implies that vˆ LR
= 0. Since xˆLR
= 0, then (14)
j
j
j
must also hold in this case. After substituting vˆ LR
in (14), algebraic
j
manipulation along with c j < 0 demonstrates that e j = 1. 
Note that there is no biconditional equivalent to Theorem 1 due
to the possible scenarios in which e j = 1. However, it is possible to

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

prove that if e j > 1 then xˆLR
= d j and that if e j < 1 then xˆLR
= 0 by
j
j
using similar construction methods as Theorem 1 and statements
(7) through (13).
The purpose of Theorem 1 is to demonstrate that (11) partitions
the variables into three categories based on the solution to the
LMDMKP. Observe that this is the same approach used by Dantzig
(1957) for solving the linear KP. The most critical of these categories are those variables for which e j = 1 as these variables are
nearly guaranteed to be in the set of core variables as they are typically non-integer for the solution of LMDMKP. In KP terminology,
these values are called the ‘break items’ and serve as the starting
point for many core-based solution methodologies.
Besides these break items, the other two sets from
Theorem 1 partition the variables of a MDMKP based on their
solution values. Within these partitions, the variables are ordered
such that the further the variable’s eﬃciency measure is from
one, the more likely that variable will not be in the set of core
variables. This can be practically demonstrated by varying the
parameters in each eﬃciency measure such that the measure
becomes smaller or larger. In every possible case, the changes
coincide with making that variable more desirable in the solution
(larger objective coeﬃcient, smaller knapsack weights, and larger
demand weights) if the measure value decreases or it makes
the variable less desirable in the solution if the measure value
increases. The clear advantage of this behavior is that more effort
should be spent investigating variables whose measure value is
near one as opposed to those which are further away. This concept
serves as the fundamental basis for the conceptual tests that
follow.
These measures can also be used as a simple test when there
are new variables to consider in the formulation. For instance, assume an MDMKP has been solved but a new item is potentially introduced. Without solving the problem again, the item’s eﬃciency
measure can be estimated using the optimal multipliers from the
prior tests. Since introducing this item, assuming the MDMKP instance is at least of moderate size, will not drastically change these
multipliers, this estimate will indicate whether the item will heavily be considered for inclusion or exclusion or whether it will be
similar to the break items. In the case it is likely to be excluded,
the MDMKP instance does not have to be solved again as the solution will likely not change. In the case it is likely to be included, it
is clearly recommended to solve the MDMKP instance again to observe the new solution. In the case where the item is similar to the
break items, it is recommended the practitioners decide whether
or not the instance should be re-solved since the item’s inclusion
may not even be optimal. Even in the situation where including
the item is optimal, it will not provide a large improvement to the
solution value. Hence, these measures can be used as a screening
mechanism for possible future items.
4. Conceptual tests
To demonstrate that the newly developed robust eﬃciency
measures perform at least as well as the existing eﬃciency measures for KPs and MKPs, a set of small MDMKPs were solved
such that the core variables could be identiﬁed. The results from
similar tests can be seen in the eﬃciency measure tests outlined by Puchinger et al. (2010) for MKPs and by Pisinger (1997)
for KPs.
To test these measures, 10 0 0 binary MDMKPs were solved to
obtain both the linear and integer solution for each test instance.
Each instance included 200 variables (n), 10 knapsack constraints
(m), and 10 demand constraints (q). The objective coeﬃcients (c j )
were randomly sampled from an integer uniform distribution with
range [−10, 10] for all j. These coeﬃcients were chosen such
that all three cases of (11) would be well represented in the re-

401

Fig. 1. Core variable frequency by eﬃciency measure value.

sults. The constraint coeﬃcients (ai j ) were randomly sampled from
[1, . . . , 10] for all i and j. The constraint thresholds were calculated
as

bi = α ∗

n

j=1

ai j

(15)

with α = 0.50 for all i. Finally, d j = 1 for all j as each instance is
a binary MDMKP. This random generation procedure extends similar uncorrelated test instance generation methods from Chu and
Beasley (1998) who developed random MKP test instances. All instances were solved to optimality in CPLEX version 12.6. Identifying the optimal solution was possible due to the problem’s small
size and limited coeﬃcient ranges.
The principal result from these tests is shown in Fig. 1 which
plots the frequency of observing a variable being in the core based
on the variable’s eﬃciency measure. The plot demonstrates that as
a variable’s eﬃciency measure deviates from one, it is less likely to
be included in the core of the problem. Hence, these measures are
an effective tool for clustering likely core variables around the set
of break items. The same pattern occurs in all prior eﬃciency measure techniques for KP (Pisinger, 1997) and MKP variants demonstrating that the new measures provide the same utility as the
measures for traditional KPs.
The advantage of the robust eﬃciency measures is further
strengthened by Fig. 2 which shows the frequency of observing
a speciﬁc eﬃciency measure in the test problems based on the
sign of c j . In Fig. 1, the eﬃciency measures of the core variables
were clustered tightly around one and Fig. 2 shows that the distribution of observed eﬃciency measures is bimodal with peaks at
zero or two. Note there is a large spike at the eﬃciency measures
associated with the break items (1.0), but this is to be expected
as these are the only measures which are guaranteed to be observed in every problem. Hence, Fig. 1 shows that nearly all of the
observed core variables have eﬃciency measures between 0.5 and
1.5 while Fig. 2 shows that observing such measures is not overly
common with respect to all of the variables in the problem. This
indicates that search techniques starting at the break items are
likely to be eﬃcient as there is not a signiﬁcant quantity of variables with these measures. It should also be noted that the results
shown in Fig. 2 are problem-dependent due to the nature of the
random data generation and the simplicity of the problem. Hence,
there is no guarantee that all MDMKPs will display this property
but these tests indicate that such instances are possible.
5. Application
To demonstrate the utility of the new eﬃciency measures,
they have been applied to three solution algorithms for solving
MDMKPs. Speciﬁcally, the measures will be employed in a Fixed-

402

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

Fig. 2. Eﬃciency measure frequency by objective coeﬃcient sign.

Core procedure, a Kernel Search procedure, and a Genetic Algorithm procedure. These solution methodologies were selected for
two primary reasons. Principally, all three solution techniques were
selected as they are heuristic procedures which have been applied to solve traditional KPs, but never MDMKPs. Secondly, the
measures will be employed in novel ways thereby expanding the
state of the art for some of these heuristics. Speciﬁcally, the Kernel
Search procedure, as far as the authors are aware, has never previously utilized eﬃciency measures for solving KP variants while a
Genetic Algorithm has never had a mutation rate which is dependent on a variable’s eﬃciency measure. Details of these modiﬁcations will be given in the appropriate sections.
Each of these techniques will be tested using a more robust data set than the data used in Section 4. Speciﬁcally, randomly generated binary MDMKPs test instances were created with
n ∈ {250, 500, 1000} and (m, q ) ∈ {(5, 5 ), (10, 10 ), (25, 25 )}. All
possible combinations of these values were used to generate
test instances except for n = 10 0 0 and (m, q ) = (25, 25 ) which
was excluded due to the complexity of the problem and the
time required to solve the instances. For each instance, ai j was
randomly sampled from an integer uniform distribution over
the range [1, 100] for all i and j. Likewise, c j was randomly
sampled from an integer uniform distribution over the range
m+q
m+q
[ i=1 ai j /(m + q ) − 50, i=1 ai j /(m + q ) + 50] for all j. This data
generation implies the objective coeﬃcients will be slightly correlated with the constraint coeﬃcients. Each bi for i ∈ {1, . . . , m + q}
is calculated according to (15). For each i ∈ {m + 1, , m + q}, α =
0.50 in (15) while α = 0.40 for each i ∈ {1, .., m} when (m, q ) =
(5, 5) and α = 0.47 for each i ∈ {1, . . . , m} for all other values of
(m, q ). These values for α differ based on the number of constraints due to feasibility challenges when m and q increase. In
total, ten test instances were created for each of the parameter
settings. This data generation methodology is similar to those employed by Puchinger et al. (2010) but future research and tests
are recommended to test application-based data as well as to test
other data generation techniques such as negative values for ai j ,
differing levels of coeﬃcient correlation, and larger test instances
with respect to both n and (m, q ).
To establish a benchmark, each test instance will be solved using two existing methodologies. The ﬁrst is CPLEX which was selected as it is assumed commercial software is available to most
practitioners. Hence, the results from CPLEX are hereafter referred
to as the ‘base case’ results. CPLEX was given the whole instance to
solve and terminated once the difference between the best global
integer solution and the linear relaxation of the best remaining
node was less than 0.15 percent or until 8 hours had elapsed.
The second benchmark methodology is the Alternating Control
Tree (ACT) procedure from Hvattum et al. (2010). The ACT methodology was selected as it is currently the best approach for solv-

ing MDMKPs in the literature. For summary, the ACT procedure
is an iterative process which continually solves the MDMKP linear relaxation and then solves a reduced binary subproblem based
on this solution. This binary problem updates the current lower
bound if possible and introduces cuts to the linear relaxation. This
process is continued until the linear relaxation solution is less
than the current lower bound. For this implementation, CPLEX was
used as the technique to solve the binary subproblem. The algorithm was terminated after 8 hours if the terminating condition
was not met and all other parameters were set as recommended
by Hvattum et al. It should be noted that the results from Hvattum et al. identiﬁed that the best subproblem solution method is
to use a combination of CPLEX and their Scatter Search heuristic. CPLEX was selected in this paper as it is easier to implement
and provided results which are only slightly worse than the Scatter Search methodology as reported by Hvattum et al. We do not
believe this shortcoming drastically alters the conclusions found in
Section 6 but future research may seek to compare the methods in
this section with the improved subproblem solver. The full results
of this methodology are shown in Section 6 as the remainder of
the current section will introduce the heuristics and compare their
results solely to the base case.
Finally, the computational tests performed in the following subsections were not conducted with the aim to exhaustively study
all tuning parameters in each of the presented heuristics. Speciﬁcally, no tuning parameters are studied for the Genetic Algorithm,
the impact of varying the Fixed-Core size is tested in the FixedCore algorithm, and the number of buckets is tested in the Kernel Search algorithm. This is a clear shortcoming of the presented
work, but the impact of varying tuning parameters is discussed
when appropriate. Since varying most of these tuning parameters
will have an obvious impact on the solution procedure (i.e. higher
solution quality at the cost of longer solution times) and problems
will have to be re-tuned if any of the data parameters are changed
(n, m, q, coeﬃcient correlations, α , etc.), we believe the impact of
this shortcoming on the discussed conclusions is minimal since future researchers would have to perform computational tests to set
tuning parameters based on their problem instances regardless of
the values recommended in this research.
5.1. Fixed-Core algorithm
The ﬁrst application of the new measures is to the Fixed-Core
solution methodology. The Fixed-Core solution method is motivated by the observation that knowing the true core variables
of a knapsack prior to solving the binary/integer model is impossible, but sorting the variables according to their eﬃciency
measures (as was done in Fig. 1) groups the most likely core
variables together. By assuming that the core variables are within

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409
Table 1
Count of instances where the ﬁxed-core procedure equals or outperforms the
base case (count of feasible ﬁxed-core test instances if less than 10).
(n, m, q )

δA

δB

δC

δD

δE

δF

δG

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

2
7
1 (1)
8
6
6 (9)
4
4

5
7
3 (3)
6
9
7
5
3

6
9
2 (4)
9
5
8
3
6

2
6
2 (5)
10
7
6 (8)
3
5

6
8
2 (7)
9
6
6
7
4

3
8
3 (3)
8
8
6
4
7

6
8
2 (7)
6
4
7
6
4

Table 2
Average ratio of the ﬁxed-core solution value over the base case solution value.
(n, m, q )

δA

δB

δC

δD

δE

δF

δG

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

0.999
1.0 0 0
1.002
1.0 0 0
1.0 0 0
1.002
1.0 0 0
1.0 0 0

1.0 0 0
1.0 0 0
1.006
1.0 0 0
1.0 0 0
1.001
1.0 0 0
1.0 0 0

1.0 0 0
1.0 0 0
1.005
1.0 0 0
1.0 0 0
1.003
1.0 0 0
1.0 0 0

0.999
1.0 0 0
1.002
1.0 0 0
1.0 0 0
1.002
1.0 0 0
1.0 0 0

1.0 0 0
1.0 0 0
0.997
1.0 0 0
1.0 0 0
1.001
1.0 0 0
1.0 0 0

0.999
1.0 0 0
1.004
1.0 0 0
1.0 0 0
1.002
1.0 0 0
1.0 0 0

1.0 0 0
1.0 0 0
0.996
1.0 0 0
1.0 0 0
1.002
1.0 0 0
1.0 0 0

a speciﬁcally selected subset of the sorted variables, the problem
can be reduced by setting all variables outside of this subset to
their linear solution values. If the true core is within this subset,
then an optimal solution for the reduced problem is optimal for
the full problem. Otherwise, the solution to the reduced problem
is near-optimal for the full problem.
To outline the Fixed-Core algorithm, assume that there are b
break items and the size of the desired ﬁxed core is δ . After solving the linear relaxation of the MDMKP and calculating all of the
eﬃciency measures for each variable, sort the variables according to these measures. The ﬁxed core is then created from the b
break items and the sets of (δ − b)/2 items to the left and right
of the break items. All other variables are ﬁxed to their linear solution values and the reduced MDMKP is solved via CPLEX using
the same stopping criteria as the base case. In this application,
7 core sizes were tested: δA = 0.1n, δB = 0.15n, δC = 0.2n, δD =
0 . 1 n + 0 . 1 ( m + q ) , δE = 0 . 2 n + 0 . 1 ( m + q ) , δF = 0 . 1 n + 0 . 2 ( m + q ) ,
and δG = 0.2n + 0.2(m + q ). This procedure is similar to techniques
applied to KPs and MKPs as demonstrated by Puchinger et al.
(2010).
The results with respect to solution quality from the Fixed-Core
experiments are shown in Tables 1 and 2. Speciﬁcally, both tables
show two quality measures aggregated over the 10 test instances
for each value of n, m, q, and δ . Table 1 displays the count of test
cases in which the Fixed-Core approach found an equal or better
solution than the base case. If any of the test instances were not
solvable for the Fixed-Core test instance, the number of instances
that were solved is given in parenthesis. An instance being unsolvable could either be a function of having no feasible region
or could be a result of CPLEX not identifying any feasible solution
within the 8 hour limit. Table 2 shows the ratio of the Fixed-Core
objective value over the base case objective value averaged over all
of the solvable Fixed-Core and base case instances.
The results from the Fixed-Core experiments with respect to solution time are shown in Table 3. The measures are reported as
the average time ratio to solve the Fixed-Core test instances over
the time required to solve the base case instance. Hence, values
less than 100 percent indicate that the time required to solve the
Fixed-Core problem were less on average than the time required to
solve the base case.
The results from Tables 1 and 2 demonstrate that the solutions
obtained from the Fixed-Core experiments are extremely compet-

403

Table 3
Average ratio of the ﬁxed-core solution time over the base case solution time.
(n, m, q )

δA

δB

δC

δD

δE

δF

δG

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

0.116
0.120
1.0 0 0
0.424
0.622
1.0 0 0
0.564
0.659

0.233
0.334
1.0 0 0
0.657
0.564
1.0 0 0
0.542
0.851

0.631
0.354
1.0 0 0
0.719
0.877
1.0 0 0
0.488
0.713

0.153
0.117
1.0 0 0
0.398
0.633
1.0 0 0
0.460
0.669

0.719
0.411
1.0 0 0
0.654
0.778
1.0 0 0
0.747
0.732

0.150
0.172
1.0 0 0
0.369
0.782
1.0 0 0
0.413
0.713

0.660
0.399
1.0 0 0
0.859
0.822
1.0 0 0
0.595
0.828

itive when compared with the base case. With respect to Table 1,
a vast majority of the problem instances and core sizes found
that the Fixed-Core algorithm identiﬁed an equal or better solution than the base case as shown by all of the entries which are 5
or greater. Also observed in Table 1 is that the quality of the identiﬁed solution increased as the core size grew which is expected as
larger core sizes place less restrictive assumptions on the feasible
region of the full problem.
With respect to speciﬁc test instances, when n = 250 and there
are 10 total constraints, the procedure did not perform well with
small core sizes according to Table 1. This is likely because these
core sizes contained the smallest count of variables across all
tested instances and therefore had the least ﬂexibility if all of the
true core variables were not within the ﬁxed core. The only other
instances which performed poorly in Table 1 are those which had
50 combined constraints. This is clearly because the ﬁxed core
was too restrictive in some of these instances and made the problem infeasible. However, the results from Table 2 demonstrate that
when feasible solutions were identiﬁed in these cases, the FixedCore methodology greatly outperformed the base case solution.
Hence, so long as the highly constrained problem is not rendered
infeasible by a smaller core size, reducing the problem is advantageous since it focuses the solution effort. It should also be noted
that the base case methodology had feasibility issues on some of
the more constrained instances. For instance, when n = 250 and
there are 50 total constraints, the base case only identiﬁed feasible solutions in 4 out of the 10 instances. Hence, the largest core
sizes for this test combination were able ﬁnd feasible solutions in
instances which were deemed infeasible in the base case.
The only other cases which provided poor results in Table 2 are
when n = 250 and there were only 10 total constraints. This is
likely because the problem was relatively easier to solve in the
base case (i.e. the time limit stopping criteria was not reached) and
a small ﬁxed core has a higher likelihood of missing key core variables. Hence, if the problem is smaller, a larger ﬁxed core may be
advisable to avoid such issues.
While the results from Tables 1 and 2 demonstrate that there
are not drastic quality differences between the base case and the
Fixed-Core algorithm outside of the feasibility challenges from the
(250, 5, 5) instances, Table 3 demonstrates that the Fixed-Core
methodology reports signiﬁcant time savings in comparison to the
base case. Speciﬁcally, other than for the most highly constrained
instances, the average Fixed-Core time savings is 55 percent over
the base case. The only instances where time savings were not
observed are during the most constrained problems as all of the
instances with 50 combined constraints terminated at 8 hours
for both the base case and for the Fixed-Core algorithm. Even
though this does not represent a time savings, the results from
Table 2 show that the Fixed-Core methodology is more eﬃcient
at ﬁnding high quality solutions in these situations.
Overall, the Fixed-Core algorithm can ﬁnd equivalent or better solutions compared with solving the full problem using commercial software and frequently in a shorter amount of time as it
can more eﬃciently focus the solution procedure. With respect to

404

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

problems with a small amount of constraints and variables, it is
recommended that a larger ﬁxed core is employed as these small
problems may be too constrained by a smaller core. For all larger
problems which are not highly constrained, the smallest ﬁxed core
size is highly competitive with respect to solution quality. Finally,
highly constrained problem should ﬁrst be approached with small
ﬁxed core sizes and if preprocessing identiﬁes the problem is infeasible, the ﬁxed core size should be increased until the problem
can be solved.
5.2. Kernel Search algorithm
The second application of the robust eﬃciency measures is to
expand the work of Angelelli et al. (2010) to be applicable to solving MDMKPs. In their work, Angelelli, Mansini, and Speranza developed a procedure, called the Kernel Search, which is used to
solve MKPs. The solution procedure starts by identifying a set of
promising decision variables, analogous to the set of core variables,
which are referred to as the kernel. This kernel is then expanded
based on the results from small, integer programming subproblems. Once all subproblems have been completed, a ﬁnal heuristic
solution is obtained.
The adaption of Angelelli, Mansini, and Speranza’s algorithm
to solve MDMKPs is straightforward as the initial algorithm only
needs minor changes. The adapted Kernel Search procedure for
MDMKPs is outlined below and the same notation will be used
to maintain consistency between the approaches. The differences
between the Kernel Search for the MDMKP and the algorithm
presented by Angelelli, Mansini, and Speranza are discussed later.
Those interested in a more in-depth discussion of the original approach are referred to the original publication by Angelelli et al.
(2010).
To deﬁne the Kernel Search algorithm, let N represent the set
of decision variables and let Tmax represent the user-deﬁned maximum computational time. Let x∗ and z∗ store the best identiﬁed
solution and solution value respectively. Let {Bi } represent a set
of pairwise independent buckets containing all variables excluding
the break items. The construction methodology for the buckets is
explained shortly. Finally, let  (hereafter referred to as the kernel)
represent a subset of N and let MDMKP() represent solving the
MDMKP instance assuming that all N ∈
/  are ﬁxed to their linear
relaxation solution values and all variables in  are constrained to
be binary.
Using these deﬁnitions, the Kernel Search algorithm for solving
the MDMKP is as follows:
MDMKP Kernel Search
Solve LMDMKP
Sort N according to e j
Construct the following:
If e j = 1, then x j ∈ 
Split N\ into a sequence of pairwise independent buckets {Bi }
Let t = Tmax /(|{Bi }| + 1)
Solve MDMKP() with time limit t and update x∗ and z∗ if feasible
For all Bi in {Bi }
Let i =  ∪ Bi
Solve MDMKP(i ) with time limit t and added constraints:

If e j > 1 for all j ∈ Bi , then
xj ≥ 1
If e j < 1 for all j ∈ Bi , then

j∈Bi



j∈Bi

x j ≤ |Bi | − 1

z ≥ z∗
If a feasible solution to MDMKP(i ) has been identiﬁed, then
Update x∗ and z∗ according to MDMKP(i )
¯ ⊆ B represent any items whose solution differs between MDMKP( )
Let 
i
i
i
and LDMKP
¯

 = ∪ i
End if
End for

This procedure has two key differences from the algorithm employed by Angelelli et al. (2010). Most importantly, the Kernel

Search for the MDMKP sorts the variables according to their efﬁciency measures while the algorithm used by Angelelli, Mansini
and Speranza uses the reduced cost of the variable. The other difference is that  was initialized by Angelelli, Mansini and Speranza
such that it contained all the items which were included in the
solution to the linear relaxation. Testing the impact of these differences is outside of the scope of this work and future research could
be conducted to understand the repercussions of these changes.
To construct {Bi }, let NB be a user-deﬁned even number that
represents the total number of buckets. Since all j ∈ N are already
sorted according to e j at this phase of the Kernel Search procedure, assign all j ∈ N such that e j < 1 into NB/2 buckets where
each bucket contains pairwise independent groupings of variables
which are adjacent in the ordering. The same should be performed
for all j ∈ N such that e j > 1 for the other NB/2 buckets. Within the
Kernel Search procedure, these buckets can be investigated in any
order, but this implementation analyzed buckets by selecting the
next bucket such that its elements had eﬃciency measures closest
to one compared with the remaining buckets which have yet to be
analyzed.
In this paper, two methodologies were tested for the size of
each Bi ∈ {Bi }. The ﬁrst methodology is that all buckets had a uniform size which is similar to the initial algorithm by Angelelli,
Mansini and Speranza. The other is that the buckets whose variables had measures closest to one were smaller than those buckets whose variables had measures which were further from one.
Speciﬁcally, an exponential approach was taken such that the buckets whose variables had measures which were one step further
from one were twice the size as compared with the buckets whose
variables had measures which were one step closer to one. For example, if there were only seven items whose measures exceeded
one and six buckets were desired in total (three buckets would
be allocated to contain these items), then the ﬁrst tested bucket
would contain the one item whose measure is closest to one, the
next bucket would contain the two items whose measures are next
closest to one, and the ﬁnal bucket would contain the last four
items whose measures are the furthest from one. This strategy was
hypothesized to be better than the uniform approach as buckets
which had variables whose eﬃciency measures were further from
one were less likely to contain core variables. Hence, making these
buckets larger could improve the eﬃciency of the Kernel Search
approach as more of these unlikely core variables would be investigated at one time.
The Kernel Search was implemented in MATLAB 2013, all subproblems were solved via CPLEX, and all test instances described
at the start of Section 5 were tested. Each instance was solved for
NB ∈ {12, 14, . . . , 28, 30} in order to draw conclusions on ideal
bucket sizes and Tmax was set to 2880 seconds for all test instances. This setting was used as the total computational time to
solve all ten bucket sizes for one test instance would be 8 hours.
Hence if there is not clear dominance with respect to bucket size
and all buckets must be investigated for the best solution, the computational burden of the Kernel Search procedure is equivalent to
the time limit for the Fixed-Core and base case experiments.
The results from these tests instances are summarized in
Table 4. Speciﬁcally, the results are reported for all tested combinations of n, m, and q and bucket construction methodologies (uniform vs. exponential). For each test instance, the objective value as
a percentage of the base case objective value was calculated and
then averaged over all values of NB. The grand average of these
values for all ten test instances at each combination of (n, m, q )
is listed in Table 4. In addition, the maximum ratio over all values of NB was identiﬁed for each test instance and the grand average of these ratios over all test ten instances at each combination
of (n, m, q ) is presented in the parentheses. The average best NB
to use over all ten instances is also reported. Finally, the time to

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

405

Table 4
Kernel search solution objective and time results.
(n, m, q )

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

Uniform buckets

Exponential buckets

Exp. over

Ave obj ratio (max obj ratio)

Best NB

Ave obj ratio (max obj ratio)

Best NB

Unif. time ratio

0.999 (1.0 0 0)
1.0 0 0 (1.0 0 0)
0.962 (0.986)
1.0 0 0 (1.0 0 0)
1.0 0 0 (1.0 0 0)
0.991 (1.0 0 0)
1.0 0 0 (1.0 01)
1.0 0 0 (1.0 0 0)

18
20
24
22
22
20
22
26

0.999 (0.999)
0.999 (1.0 0 0)
0.970 (0.987)
0.999 (1.0 0 0)
1.0 0 0 (1.0 0 0)
0.991 (1.0 0 0)
1.0 0 0 (1.0 01)
1.0 0 0 (1.0 0 0)

20
20
24
20
20
22
18
22

0.651
0.495
0.766
0.331
0.545
0.822
0.650
0.757

solve the exponential bucket implementation divided by the time
to solve the uniform bucket implementation averaged over all instances and values for NB is provided in the last column of Table 4.
The results in Table 4 demonstrate that the Kernel Search
method applied to MDMKPs is an effective solution technique.
Speciﬁcally, all instances except those with ﬁfty combined constraints were solvable with a high level of accuracy on average.
Furthermore, if only the best bucket sizes are considered as shown
in the parenthesis in Table 4, the average solution quality is equal
to or better than the base case in a majority of the test instances
regardless of the bucket strategy. In addition, eight out of the ten
(250, 25, 25) instances were solved for at least one value of NB.
This is better than the base case where only four of these instances
were solved and better than the Fixed-Core approach where the
largest core sizes only solved seven of these instances. Hence, the
Kernel Search methodology appears to be a better approach for
ﬁnding feasible solutions.
With respect to the ideal value for NB, the results in Table 4 do
not permit clear conclusions. In general, if the value for n is constant, the best value for NB increases as the number of constraints
increases. This pattern is observed for nearly all values of n and
bucket construction methodologies. Furthermore, as n increases,
the best value for NB increases if the number of constraints and
bucket construction methodologies are kept constant except for
the sole outlier using exponential buckets and 10 total constraints.
Outside of these trends, knowing the best bucket size prior to solving the problem would be challenging to identify without extensive computational testing on a variety of problem types and parameters. Since such testing is not the singular objective of this
research, it is advised that the implemented approach (using multiple values for NB and retaining the best) be followed until such
work can be performed. Therefore, the discussion of the results to
follow focuses solely on the numbers in parentheses from Table 4.
With respect to solution quality, either bucket construction
methodology is preferred as they each provide competitive results.
However, it should be noted that if another digit were to be shown
in Table 4, the results favor the uniform bucket strategy in a majority of the cases. With respect to computational time, the last
column of Table 4 clearly shows that the exponential bucket strategy is preferred for all test instances. Hence, unless solution quality
is the sole factor in selecting a solution method, the exponential
bucket strategy is recommended as it is signiﬁcantly faster than
using uniform buckets and ﬁnds the same or nearly the same results. The uniform bucket approach is therefore only recommended
in those situations when a 0.01 percent or less improvement in the
ﬁnal solution value is more important than a 50 percent average
time improvement.
5.3. Genetic Algorithm
The ﬁnal demonstration of the applicability of the new eﬃciency measures is within a Genetic Algorithm (GA). GAs are a

common methodology for solving both KPs and MKPs, but they
have never been applied to the MDMKP. GAs for solving MKPs have
speciﬁcally been well studied in recent years with the most noteworthy advancement from Chu and Beasley (1998) who utilized efﬁciency measures within the repair phase to encourage better evolution towards optimality.
The GA which follows, hereafter referred to as the EﬃciencyWeighted GA (EWGA), is developed as evidence that GAs can be
applied to MDMKPs as well as proof of concept that eﬃciency
measures can be applied to the mutation phase of GAs. To outline the EWGA, let K l represent the population at the lth iteration
of the EWGA. Assume that there are p individuals during each iteration. Let k ∈ K l represent an individual of this population which
is deﬁned by the binary array xk . This array represents a solution
vector for the MDMKP which is not necessarily feasible.
The most unique component of the EWGA is that each gene
j ∈ N (i.e. each decision variable) has a speciﬁc mutation rate r j
which represents the probability that a gene will mutate whenever the mutation procedure is performed. Speciﬁcally, the mutation procedure generates a continuous random number between
zero and one for each gene. If this random number is less than r j ,
the binary value for that gene is ﬂipped. This mutation operation
occurs at two points within the EWGA.
The ﬁrst occurrence is after the initial population is created.
To demonstrate the mutation rates at this phase, we will assume there is an MDMKP instance with 100 variables which are
sorted according to their eﬃciency measures. Furthermore, assume there is only one break item which is the 50th item in this
sorting. Within the EWGA, an initial population of p individuals
is created by rounding the optimal linear solution value for the
MDMKP instance. Hence, all p individuals represent the same solution to the binary MDMKP prior to mutation. With respect to
the aforementioned example, let r50 = 0.50. Any break item has
this mutation rate which represents an equal likelihood of being
a zero or one in the initial population. Next, let r49 = r51 = 0.05 +
exp(−1 − 1 ∗ (1/16 ) ), r48 = r52 = 0.05 + exp(−1 − 2 ∗ (1/16 )), and
so forth. In general, r j = 0.05 + exp(−1 − jˆ ∗ (1/16 ) ) where jˆ represents the sorted distance to the break item set for variable j ∈ N
and r j = 0.50 if j is a break item. This methodology makes the mutation rate for a variable exponentially decrease as the variable becomes less likely to be in the set of core variables until the mutation rates asymptotically approach 0.05 for those items furthest
from the break items. The speciﬁc approach was implemented as
early computational tests found it outperformed other techniques
and parameters.
Note that these rates only apply when mutating the starting
population. The other mutation procedure occurs after a new offspring is created. In this case, the mutation rate is scaled such that
one gene is ﬂipped on average in each offspring. Speciﬁcally, let

R = nj=1 r j and then the new mutation rate for any variable j is


r j = r j /R. By using the same mutation methodology as before (i.e.
generating random variables for each variable), the new mutation

406

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

rate will ensure that one gene is ﬂipped on average for each offspring.
Outside of this mutation procedure, the EWGA is similar to existing Genetic Algorithms seen in literature. Speciﬁcally, each individual k is scored based on its ﬁtness measure

fk =

n




c j xkj

+M

m

i=1

j=1
m+q

+





uˆLR
i



uˆLR
i min 0,

i=m+1

min 0, bi −

n




ai j xkj

j=1

n




ai j xkj − bi

(16)

j=1

which is a summation of the objective function value for k penalized by the weighted violation of all knapsack and demand constraints respectively. These violations are individually weighted by
the optimal dual solution values corresponding to each constraint
and globally weighted by a large value M. This penalty is required
as there is no simple repair procedure possible for individuals who
are infeasible for MDMKP problems.
For the parent selection methodology, a tournament method is
employed which randomly selects two pairs of individuals from
the current population. The best individuals, as measured by their
ﬁtness, from each of these pairs are then selected as the parents.
The offspring is created by randomly selecting a parent to pass
along their information for each gene. Once this is completed, the
offspring is mutated by utilizing r 
j as described previously. This
process is completed to create p offspring. If an offspring is created
which is feasible and better than the current best feasible solution
identiﬁed thus far, z∗ and x∗ are updated. Then, the best p
 offspring are selected and joined with the best p − p
 parents to create a new population and the process is repeated. This is continued
until I populations have been created without an improvement in
z∗ . The full EWGA procedure is given next.
Eﬃciency-Weighted Genetic Algorithm
Solve LMDMKP
Let l = 0, z∗ = −∞, and x∗ = ∅
Initialize K 0 of p individuals by rounding solution of linear MDMKP
relaxation
Mutate K 0 using initial mutation rate r j
n

rj
For each j ∈ N, let r 
j = r j /
j=1

While l ≤ I
For h = 1 to p
Select two pairs of individual from K l
Set the best individuals from each pair as the parents
Create the hth offspring by randomly selecting genes from each parent
Mutate the hth offspring according to r j
End for
Create K l+1 by selecting the best p − p
 individual from K l and selecting
the best p
 individuals from the offspring population
n

Let z
 = max { c j xkj |xk is a feasible solution}
k∈K l+1 j=1
∗

If z
 > z , then l = 0 and update z∗ and x∗
End while

The EWGA was implemented in MATLAB 2013 and was tested
on each of the test instances described at the start of Section 5.
For this implementation p = 100 and p
 = 30 which implies that
30 percent of each generation is comprised of offspring of the
prior generation with the remainder being the best parents from
the prior generation. Additionally, I = 10 0 0 which implies that the
EWGA will terminate after an improved feasible offspring is not
found after 10 0 0 consecutive generations. For each test instance,
the EWGA was conducted 100 times to avoid initialization bias.
These values were employed because they performed well in preliminary testing and they provided some computational parity
with the other heuristics. This parity is demonstrated in Section 6.
Finally, a comparison methodology was developed which is a
Genetic Algorithm that assumed r j = 0.50 for each gene. Hence,

this represents traditional GAs which start with completely random individuals at the start of the procedure and assume that each
gene has an equal likelihood to mutate throughout the entire algorithm. This procedure will hereafter be referred to as the ‘Standard
GA’ and is subject to the same parameters as the EWGA except for
the differences in r j for each j ∈ N.
The results from these tests are shown in Table 5. For each of
the 100 GA procedures conducted on each test instance, the best
feasible solution was recorded. The ratio of this value over the base
case objective value was calculated and the average of these ratios
is shown in Table 5. Additionally, the maximum feasible solution
over all of the 100 GA procedures was identiﬁed and the ratio of
this value over the base case objective value is given in the parentheses. Also provided in Table 5 is the average percentage of the
GA tests which found a feasible solution for each test case along
with the value of M in (16) used for that test. The values of M
were determined through experimentation and are a function of n,
m, and q. Finally, the last column in Table 5 shows the average ratio of the computational time required to solve the EWGA over the
computational time to solve the uniform weighted mutation rate
GA.
The results in Table 5 clearly demonstrate that across all measures and instances, the EWGA methodology is preferred in comparison to the Standard GA. With respect to solution quality, initializing the GA with the linear solution and making mutation less
likely for those variables whose eﬃciency measures most deviate
from one clearly improves the solution quality. As this is the ﬁrst
instance of such a mutation procedure, it is hypothesized that similar results would also occur for other KP variants, but additional
research is needed to test such a theory. With respect to solution
time, the EWGA again performs signiﬁcantly better. These results
demonstrate that the EWGA is able to ﬁnd better, feasible solutions
in a shorter amount of time than the Standard GA.
With respect to the different problem instances, the EWGA
clearly performs better for less constrained problems. This result
is hypothesized to be a result of the EWGA not featuring a repair operation which can turn infeasible offspring into feasible
MDMKP solutions. While such operators are present in GAs applied to KPs and MKPs, there is no simple mechanism to guarantee
feasible MDMKP solutions. Hence, the EWGA is not recommended
in highly constrained instances. However, assuming the number of
constraints is held constant, the results in Table 5 show that EWGA
performance increases as the problem size grows. Future research
is therefore recommended to test if larger instances continue to
result in better performance for the EWGA.
6. Discussion
The discussion of each technique in Section 5 solely compared
the computational results against the test options within the algorithm and against the base case (solving the problems solely
with CPLEX). However, this discussion did not compare the results
against one another or against other developed MDMKP solution
methods. The purpose of this section is to present and discuss such
results as well as to discuss future research opportunities for each
of the developed techniques.
In addition to using CPLEX as a comparison solution methodology, the ACT method developed by Hvattum et al. (2010) was
employed to solve each test instance. As previously stated, the
subproblem solver used in this ACT implementation was CPLEX
which is reported to be outperformed by a combination of CPLEX
and Scatter Search. CPLEX alone was chosen for this implementation as it is easier to implement and provided equivalent or only
slightly worse results than the problems solved with both methodologies. In this implementation, all parameters were kept the same
as those recommended by Hvattum et al. and a total of 8 hours was

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409

407

Table 5
EWGA solution objective and time results.
(n, m, q )

M

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

30 0 0
60 0 0
15,0 0 0
60 0 0
12,0 0 0
30,0 0 0
12,0 0 0
24,0 0 0

EWGA

Standard GA

EWGA over

Ave obj ratio (max obj ratio)

Feas. ratio

Ave obj ratio (max obj ratio)

Feas. ratio

Stand. time ratio

0.983
0.982
0.942
0.990
0.990
0.965
0.993
0.993

1.0 0 0
0.963
0.070
1.0 0 0
0.985
0.204
1.0 0 0
0.997

0.948
0.941
0.833
0.946
0.949
0.850
0.934
0.941

0.984
0.781
0.022
1.0 0 0
0.696
0.046
1.0 0 0
0.631

0.624
0.823
0.901
0.521
0.638
0.792
0.401
0.576

(0.993)
(0.995)
(0.973)
(0.995)
(0.996)
(0.987)
(0.997)
(0.997)

(0.978)
(0.983)
(0.851)
(0.971)
(0.981)
(0.884)
(0.958)
(0.976)

Table 6
Average ratio of the eﬃciency measures based solution values over the ACT solution values.
(n, m, q )

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

EWGA

0.993
0.994
0.980
0.995
0.996
0.985
0.996
0.997

Kernel

Fixed-Core

Unif.

Exp.

δA

δB

δC

δD

δE

δF

δG

1.0 0 0
1.0 0 0
0.997
1.0 0 0
1.0 0 0
1.003
1.0 0 0
1.0 0 0

0.999
1.0 0 0
0.987
1.0 0 0
1.0 0 0
1.006
1.0 0 0
1.0 0 0

0.999
1.0 0 0
1.008
1.0 0 0
1.0 0 0
1.008
0.999
0.999

1.0 0 0
1.0 0 0
1.013
1.0 0 0
1.0 0 0
1.007
1.0 0 0
0.999

1.0 0 0
1.0 0 0
1.020
1.0 0 0
1.0 0 0
1.008
0.999
1.0 0 0

0.999
1.0 0 0
1.019
1.0 0 0
1.0 0 0
1.009
0.999
1.0 0 0

1.0 0 0
1.0 0 0
1.020
1.0 0 0
1.0 0 0
1.007
1.0 0 0
1.0 0 0

0.999
1.0 0 0
1.011
1.0 0 0
1.0 0 0
1.008
0.999
1.0 0 0

1.0 0 0
1.0 0 0
1.018
1.0 0 0
1.0 0 0
1.007
1.0 0 0
1.0 0 0

Table 7
Average ratio of the eﬃciency measures based solution times over the ACT solution times.
(n, m, q )

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

EWGA

0.056
0.265
0.044
0.087
0.086
0.040
0.156
0.161

Kernel

Fixed-Core

Unif.

Exp.

δA

δB

δC

δD

δE

δF

δG

0.042
0.083
0.183
0.189
0.241
0.291
0.289
0.294

0.016
0.030
0.111
0.041
0.109
0.239
0.186
0.221

0.006
0.014
0.995
0.334
0.419
0.996
0.001
0.002

0.018
0.097
0.995
0.479
0.502
0.996
0.002
0.016

0.507
0.108
0.995
0.512
0.568
0.996
0.001
0.035

0.009
0.017
0.995
0.290
0.382
0.996
0.001
0.002

0.636
0.139
0.995
0.467
0.612
0.996
0.003
0.005

0.009
0.024
0.995
0.190
0.477
0.997
0.001
0.007

0.415
0.148
0.995
0.640
0.616
0.996
0.002
0.041

given as the maximum processing time. Note that it is also possible for the ACT algorithm to terminate early if the linear problem
(which has cuts continually added to it by the subproblem) returns
an answer less than best feasible solution identiﬁed thus far.
The results in Tables 6 through 8 compare the ACT solutions
against the techniques shown in Section 5. Note that the Standard
GA is omitted in each table since it was dominated by the EWGA
for each test instance and the results for the Kernel Search assume
each bucket size was tested for each instance and the ﬁnal solution
was selected as the maximum value over all of the buckets as the
results from Table 4 demonstrate there is no ideal bucket strategy. Each column in Tables 6 through 8 lists a different solution
method and each row shows a different test combination. The values presented in Table 6 are the average objective value ratios for
the eﬃciency measure techniques over the ACT methodology while
the values in Table 7 are the average solution time ratios for the
eﬃciency measure techniques over the ACT solution method. The
values in Table 8 show the count of instances in which the solution method identiﬁed the best known solution. In the case where
multiple techniques identiﬁed the best solution, they are both included in Table 8. Shown in parenthesis in Table 8 is the count
of instances in which the ACT solution method identiﬁed a feasible solution when the indicated eﬃciency measure based solution
method could not. Those instances where they both identiﬁed the
same number of feasible solutions are omitted.

First, one of the key results from Tables 6 and 8 is that the
EWGA is not competitive when compared to the other solution
methods. While this may be partly explained by the non-time
based stopping criteria, the results in Table 7 show that the EWGA
had solution times where were comparable on average with the
other methods. It is hypothesized that the EWGA (and Genetic Algorithms in general) are not well suited for solving MDMKP instances due to the lack of a repair operation. As previously stated,
this operation was not included as there is no easy procedure
which can guarantee feasibility for the generic MDMKP. Hence, this
often results in instances in which long stretches of populations
have no feasible individuals. The EWGA is therefore not recommended for future research unless a suitable repair operation is
developed.
With respect to all of the other solution methods from
Section 5, the results in Table 6 demonstrate that the eﬃciency
measure based solution methods are highly competitive on average compared with the ACT solution methodology, but the recommended solution method differs based on the test instance
type. For example, the ACT generally performs better on average
compared with the other solution methods for the (250, 5, 5) instances. Note that the Kernel Search with uniformly sized buckets and some of the ﬁxed core sizes are equally as competitive
in these instances. Otherwise, the most constrained problem (e.g.
those with 50 total knapsack and demand constraints) demonstrate

408

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409
Table 8
Count of instances each solution method identiﬁed the best solution (Count of instances not solved by eﬃciency
based methods which were solved by ACT if greater than 0 in parentheses).
(n, m, q )

(250, 5, 5)
(250, 10, 10)
(250, 25, 25)
(500, 5, 5)
(500, 10, 10)
(500, 25, 25)
(10 0 0, 5, 5)
(10 0 0, 10, 10)

ACT

9
9
2
8
2
0
4
6

EWGA

0
0
2 (1)
0
0
0 (2)
0
0

Kernel

Fixed-Core

Unif.

Exp.

δA

δB

δC

δD

δE

δF

δG

6
9
1
7
2
0
5
3

3
7
1
6
2
0
3
2

3
7
0 (7)
5
1
0 (1)
0
0

6
7
1 (5)
2
4
1
0
0

7
9
1 (4)
4
1
5
1
0

3
6
0 (3)
6
4
3 (2)
0
0

7
8
4 (1)
6
3
0
0
0

4
8
0 (5)
6
2
1
0
0

7
8
1 (1)
2
1
0
0
0

the advantages of the eﬃciency-based techniques compared with
the ACT procedure. During these instances, all ﬁxed cores sizes
outperform the ACT method for n = 250 and all ﬁxed core sizes
and both Kernel Search techniques outperform the ACT method
for n = 500. Hence, the results in Table 6 appear to favor the ACT
method for simpler, less constrained problems, but larger and more
constrained problems are better solved using either the Kernel
Search or Fixed-Core methods.
The results from Table 8 further conﬁrm these ﬁndings. With
respect to smaller and simpler problems, the ACT method is the
superior solution method as a majority of the highest quality solutions are a result of this methodology. However, it should be noted
that Fixed-Core and Kernel Search methods also ﬁnd a high percentage of the same solutions for these instances. With respect
to the larger and most constrained problems, the ACT does not
perform as well. In these instances, the Fixed-Core tests often are
the sole solution methods which ﬁnd the highest quality solutions,
even in comparison to the Kernel Search methods which did report promising ﬁndings in Table 6. The clear disadvantage of the
Fixed-Core methods is that they frequently result in infeasible solutions in these instances, especially compared with the ACT procedure and the Kernel Search procedures, as observed by the values
in parentheses.
Finally, the results from Table 7 demonstrate the true advantages of the eﬃciency measure based solution methods. For nearly
all of the solution methods and test instances, the eﬃciency measure based solution techniques terminated signiﬁcantly faster on
average than the ACT solution methodology. This is especially true
for the Kernel Search method which reported high quality average solutions in typically 20 percent or less of the computational
time. While this technique did not frequently determine the best
solution as shown in Table 8, it is highly recommended if solution
time is of major importance. In comparison, the Fixed-Core solution method frequently solved problems much faster for all but the
most constrained instances in which both the ACT method and the
Fixed-Core solution methods needed the complete allotted time.
In summary, the developed eﬃciency measures can be applied
to numerous solution methods such that MDMKP test instances
can be solved in an eﬃcient manner. Three such solution methods
were created and compared with a commercial solver and an existing MDMKP solution heuristic. The results demonstrated that for
simpler and less constrained problems, the ACT solution method
is only slightly preferred with respect to solution quality, but is
greatly outperformed with respect to solution time by the FixedCore and Kernel Search methods. Therefore the ACT method is recommended only if solution time is not a factor. For the larger problems, the best technique is to use the Fixed-Core methodology, but
this may result in feasibility issues. Hence, it is recommended that
the Fixed-Core be tested ﬁrst and if a feasible solution is not identiﬁed quickly, the ACT procedure can be performed. If time is a

factor in such solution methods, than the Kernel Search method
should be substituted for the ACT procedure.
In regards to the methodologies themselves, the EWGA technique is not currently recommended unless future research can
identify a reliable repair operator for infeasible solutions. For the
Kernel Search, the exponential approach is recommended for constructing buckets unless slight improvements in solution quality
are more important than drastic time savings. At this moment,
no strategy is recommended for the number of buckets to employ
other than to test a range of buckets and retain the best solution
from those tests. Future research for the Kernel Search could be
conducted to make the procedure iterative as similarly completed
by Angelelli et al. (2010). Also, it could be possible to solve the Kernel Search subproblems with solution methods other than CPLEX
to observe the impact on both solution quality and time. Finally,
the Fixed-Core technique is highly recommended to solve MDMKPs
heuristically due to its ease of implementation and high quality
results for most problems. The recommended core size based on
this research is δB = 0.15n due to the results shown in Tables 1,
6, and 8, but other problem instances may ﬁnd other core sizes
provide better results. Ultimately, it is recommended that a small
core size is ﬁrst tested and if the results are not satisfactory, the
core size is increased. Future research into the Fixed-Core technique includes testing new core sizes as well as testing either exact or heuristic solvers in lieu of using CPLEX. Those seeking to
implement any of the developed techniques for either research or
other non-commercial purposes are highly encouraged to contact
the authors for access to the current implementation code as well
as copies of all tests instances used in this research.
References
Ang, J., Cao, C., & Ye, H.-Q. (2007). Model and algorithms for multi-period sea cargo
problem. European Journal of Operational Research, 180(3), 1381–1393.
Angelelli, E., Mansini, R., & Grazia Speranza, M. (2010). Kernel search: A general
heuristic for the multi-dimensional knapsack problem. Computers & Operations
Research, 37(11), 2017–2026.
Arntzen, H., Hvattum, L. M., & Løkketangen, A. (2006). Adaptive memory
search for multidemand multidimensional knapsack problems. Computer, 33(9),
2508–2525.
Balachandar, S., & Kannan, K. (2011). A new heuristic approach for knapsack/covering problem. International Journal of Mathematical Sciences and Applications, 1(2), 593–606.
Balas, E., & Zemel, E. (1980). An algorithm for large zero-one knapsack problems.
Operations Research, 28(5), 1130–1154.
Beaujon, G., Marin, S., & McDonald, G. (2001). Balancing and optimizing a portfolio
of R & D projects. Naval Research Logistics, 48(1), 18–40.
Cappanera, P., Gallo, G., & Maﬃoli, F. (2004). Discrete facility location and routing
of obnoxious activities. Discrete Applied Mathematics, 133(1), 3–28.
Cappanera, P., & Trubian, M. (2005). A local-search-based heuristic for the demand–
constrained multidimensional knapsack problem. INFORMS Journal on Computing, 17(1), 82–98.
Chu, P. C., & Beasley, J. E. (1998). A genetic algorithm for the multidimensional knapsack problem. Journal of Heuristics, 4(1), 63–86.
Dantzig, G. (1957). Discrete-variable extremum problems. Operations Research, 5(2),
266–288.

C. Wishon, J.R. Villalobos / European Journal of Operational Research 254 (2016) 398–409
Della Croce, F., & Grosso, A. (2012). Improved core problem based heuristics for
the 0/1 multi-dimensional knapsack problem. Computers & Operations Research,
39(1), 27–31.
Dobson, G. (1982). Worst-case analysis of greedy heuristics for integer programming
with nonnegative data. Mathematics of Operations Research, 7(4), 515–531.
Dudziński, K., & Walukiewicz, S. (1987). Exact methods for the knapsack problem
and its generalizations. European Journal of Operational Research, 28(1), 3–21.
Ghasemi, T., & Razzazi, M. (2011). Development of core to solve the multidimensional multiple-choice knapsack problem. Computers & Industrial Engineering,
60(2), 349–360.
Gomes da Silva, C., Clímaco, J., & Rui Figueira, J. (2008). Core problems in bi-criteria
{0, 1}-knapsack problems. Computers & Operations Research, 35(7), 2292–2306.
Hvattum, L., Arntzen, H., Løkketangen, A., & Glover, F. (2010). Alternating control tree search for knapsack/covering problems. Journal of Heuristics, 16(3),
239–258.
Hvattum, L. M., & Løkketangen, A. (2007). Experiments using scatter search for
the multidemand multidimensional knapsack problem. In K. Doerner, M. Gendreau, P. Greistorfer, W. Gutjahr, R. Hartl, & M. Reimann (Eds.), Metaheuristics: Progress in complex system optimization (pp. 3–24). New York, NY: Springer
Science+Business Media.
Lust, T., & Teghem, J. (2012). The multiobjective multidimensional knapsack problem: A survey and a new approach. International Transactions in Operational Research, 19(4), 495–520.
Martello, S., Pisinger, D., & Toth, P. (1999). Dynamic programming and strong bounds
for the 0-1 knapsack problem. Management Science, 45(3), 414–424.
Martello, S., Pisinger, D., & Toth, P. (20 0 0). New trends in exact algorithms for
the 0-1 knapsack problem. European Journal of Operational Research, 123(2),
325–332.
Martello, S., & Toth, P. (1988). A new algorithm for the 0-1 knapsack problem. Management Science, 34(5), 633–644.

409

Martello, S., & Toth, P. (1990). An exact algorithm for large unbounded knapsack
problems. Operations Research Letters, 9(1), 15–20.
Martello, S., & Toth, P. (1997). Upper bound and algorithms for hard 0-1 knapsack
problems. Operations Research, 45(5), 768–778.
Mavrotas, G., Rui Figueira, J., & Florios, K. (2009). Solving the bi-objective multi-dimensional knapsack problem exploiting the concept of core. Applied Mathematics and Computation, 215(7), 2502–2514.
Pisinger, D. (1995). A minimal algorithm for the multiple-choice knapsack problem.
European Journal of Operational Research, 83(2), 394–410.
Pisinger, D. (1995). An expanding-core algorithm for the exact 0-1 knapsack problem. European Journal of Operational Research, 87(1), 175–187.
Pisinger, D. (1997). A minimal algorithm for the 0-1 knapsack problem. Operations
Research, 45(5), 758–767.
Pisinger, D. (1999). Core problems in knapsack algorithms. Operations Research,
47(4), 570–575.
Pisinger, D. (20 0 0). A minimal algorithm for the bounded knapsack problem. INFORMS Journal on Computing, 12(1), 75–82.
Puchinger, J., Raidl, G. R., & Pferschy, U. (2010). The multidimensional knapsack problem: Structure and algorithms. INFORMS Journal on Computing, 22(2),
250–265.
Volgenant, A., & Marsman, S. (1998). A core approach to the 0-1 equality knapsack
problem. Journal of the Operational Research Society, 49(1), 86–92.
Wilbaut, C., Hanaﬁ, S., & Salhi, S. (2008). A survey of effective heuristics and their
application to a variety of knapsack problems. IMA Journal of Management Mathematics, 19(3), 227–244.
Wishon, C., & Villalobos, J. R. (2016). Alleviating food disparities with mobile retailers: Dissecting the problem from an OR perspective. Computers & Industrial
Engineering, 91, 154–164.

,

,I/

IEEE TRANSACTIONS ON SYSTEMS, MAN, A N D CYBERNETICS,VOL. 22, NO. 3, MAY/JUNE 1992

441

Process Capability of Automated Visual
Inspection Systems
Paul M. Griffin, Member, IEEE, and J. RenC Villalobos, Member, IEEE

including positional enor, changes in the lighting conditions
and image quantization. From these errors, a process capability
zone is determined that may be compared to the design
specifications. If the process capability zone falls completely
within the design specifications,then the AV1 system is capable
of inspecting the part to the design requirements. If not, then
either a different inspection system will have to be used or the
design specifications loosened.
I. INTRODUCITON
It is assumed that the parts to be inspected are flat, or at
UTOMATED VISUAL INSPECTION (AVI) has become
least that the features to be inspected for are 2-D, and that the
commonplace in the manufacturing arena, especially in
parts are stationary. There are many examples of this such as
the area of dimension verification [l]. An important issue
automated visual inspection of circuit boards, machined parts,
for the use of AV1 is the process capability of an inspection
integrated circuits and textiles.
system. That is, given a part type, it must be determined if
In the next section the sources of error inherent in an
this part can be inspected to the design specifications by the
automated visual inspection system are analyzed. It is also
AV1 system. If in fact the part can be inspected, it is also
determined how to derive the process capability zone based
important to determine the inspection parameters of the AV1
on these errors. The basics of tolerance specifications are
system such as resolution.
discussed in Section 111. Comparison of the process capability
There has been some previous work done on the analysis
zone with the tolerance specifications is presented in Section
of error of digital vision systems [2]-[8]. Ho [2] examined
IV. Section V presents experimental data used for determinathe digitization error of various geometric shapes such as
tion of the error distributions, and examples. Conclusions and
lines and squares in terms of the perimeter of the object (a
further research are presented in Section VI.
dimensionless parameter) for both two-dimensional (2-D) and
three-dimensional (3-D) systems. Blostein and Huang [3], [4]
11. SOURCES OF ERROR
examine the relationship between the setup of a stereo camera
geometry and the accuracy in obtaining 3-D point positions.
There are three principal sources of error that are evaluated
Kamgar-Parsi and Kamgar-Parsi [5] analyze quantization error in this paper: spatial quantization error, illumination error due
for a computer vision system and developed a computational to changes in the lighting conditions and positional inaccuracy.
method for the determination of the average error due to Each of these error sources is modeled below. It must be
digitization and the error distribution function. Related work mentioned that many other sources of error that are not
to the aforementioned can be found in Havelock [6], Moon considered may influence the capability of an automated visual
[7] and Tappan et al. [SI.
inspection system such as lens distortion and electrical noise.
One question that has not been adequately addressed is However, we feel that in most cases the three sources modeled
the determination of how the error of an automated visual in this paper are the dominant sources, and so the results
inspection system effects the performance. In particular, given obtained from this analysis are a good approximation.
the dimensions and tolerances for a part, is the inspection
system capable of inspecting the part to these requirements.
A. Spatial Quantization Error
In this paper, a methodology is developed to determine the
In computer vision, an image is approximated by a matrix of
process capability of a given AV1 system. From the process
capability, it is possible to determine whether or not a specified discrete intensity values called pixels. This image digitization
part can be inspected on the AV1 system. The methodology is introduces spatial quantization error. That is, a point in the
developed by modeling the error inherent in the AV1 system image can only be located to within a pixel accuracy. This is
illustrated in Fig. 1.
Manuscript received January 10, 1990; revised April 21, 1991, and July 16,
For a boundary, the range of error from the actual to the
1991.
P. M. Griffin is with the School of Industrial and Systems Engineering, approximated boundary along the x-axis is one pixel on either
Georgia Institute of Technology, Atlanta, GA 30332.
side of the boundary. This error is uniformly distributed over
J. R. Vilalobos is with the Department of Mechanical and Industrial
the range. This error can be transformed into real terms by
Engineering, University of Texas at El Paso, El Paso, TX 79968.
IEEE Log Number 9104557.
multiplying by the ratio of distance to pixels. This ratio, rZ, is

Abstruct- The problem of determining if a given twodimensional (2-D)automated visual inspection system using
binary images is capable of inspecting a specified part to
the design specifications is addressed. This is accomplished by
modeling the error inherent in the visual inspection system. The
error defines a process capability zone that is compared to the
part design specifications.

A

0018-9472/92$03.00 0 1992 IEEE

_

_

-

-

~-

~

~~

I

-

-

-

-

-

-

442

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. 22, NO. 3, MAY/JUNE 1992

Fig. 1. An ellipse and its corresponding digitized image.

Fig. 2. Region containing the bound for the digitized ellipse determined by
sweeping R about the ellipse.

called the aspect ratio. The probability density function @df)
This change in the lighting conditions can be approximated as
of spatial quantization error along the x-axis is described by
being Gaussian distributed about the boundary. If it assumed
1
(1) that change along the x and y axes are independent, then this
f ( x ) = L'r ,.I[-Td=I(X)
illumination error can be described by the bivariate normal
where I is the indicator function (i.e., I[-r5,Tz1(x) = 1 if pdf with p = 0. That is
-T,
5 x 5 T,, and 0 otherwise). Similarly, along the y-axis:

If the pixels are square, then T, = ry.Since the pixels are on There is no theoretical justification for assuming that there is
a grid, then the quantization error is described as a rectangle, independence. between the x and y axes. However, experimenR, with height equal to 2r, and width equal to 2r,. If the tal evidence has shown that the independence assumption is
rectangle is swept along the actual boundary then a region valid [2].
is obtained in which the digitized boundary will fall. This is
C. Positional Error
illustrated in Fig. 2 for the figure shown in Fig. 1.
It should be noted that there are many methods to decrease
Most positioning devices are some variation of an x-y table
the error introduced by spatial quantization by obtaining driven by a leadscrew along each axis. Because movement
subpixel resolution (for example Lyvers et al. [9]). In order along the x and y axes are independent, then the error along
to keep the analysis presented in this paper as general as each axis may also be assumed to be independent. The error
possible, we do not choose a particular subpixel resolution along each axis is given by the manufacturers specifications.
method. Instead, we assume that if a method is used, that The error along the 5 axis is described by the random variable
spatial quantization will be limited to a rectangle of size s, X with distribution f(x). The expected value of X in general
by sy. From this point on, we assume that s, and sy will be is equal to zero. The same is true for error in the Y axis. Since
absorbed into the constants rx and rY.
X and Y are independent, then the joint positional error pdf
is given by
B. Illumination Error
f ( x , 9 ) = f ( x ) . f(Y).
(4)
Whenever an automated visual inspection system is used
in an industrial environment, the lighting conditions are sus- We assume that f(x) and f ( y ) are both normally distributed,
ceptible to change over time. A change in illumination can and so f(x, y) has the bivariate normal distribution with p = 0.
cause a change in the boundary of the observed part. If
There are different types of control available for positional
backlighting is used, then if the illumination increases, the devices. If open loop control is used, then the positional error
observed boundary size decreases. On the other hand, if the must be convolved for each movement of the table. On the
illumination decreases, the observed boundary size increases. other hand, if closed loop control is used, feedback information

443

GRIFFIN AND VILLALOBOS: PROCESS CAPABILITY OF AUTOMMED VISUAL INSPECTION SYSTEMS

is available. For this reason, f ( x , y ) would be the same for
each table movement.
111. PROCESS CAPABILITY ZONE
Spatial quantization error, illumination error and positional
error are all independent error sources. Because of this, the
density function of the sum of the three error sources may
be computed in a straightforward manner. Recall that for all
three error sources, there is independence between the x and y
axes. The individual distributions, therefore, may be convolved
along each axis, that is the sum of the independent continuous
random variable may be computed by the convolution formula
[lo]. Let X1 be the random variable for spatial quantization
error along the x-axis, X p the random variable for illumination error along the x-axis and X3 the random variable for
positional error along the x-axis. Further assume that there is
closed loop control for the positioning device.
In order to get the convolution of Xp and X3, let 21 =
X2+X3. Both X2 and X3 are normally distributed. Therefore,
the pdf of 21 is also normal [lo] with
Pz, = Pzz

+ Px,

oZ1 = o:z

+ oz3.

(5)

and X3 are independent of XI, and so the pdf
f(xl,zl) is defined as the product of the pdf for X1 and
the pdf of 21. In order to get the convolution of XI and 21,
let "1 = XI + 21. The distribution of 1" is given by the
convolution formula
Both

X2

1,
CO

f(wd =

f ( w l - ZlrZl) dZl.

(6)

Computing the pdf for f(w1) yields

where T = rxl and $0is the cumulative distribution function
(cdf) of the standard normal. The same can be done along the
y-axis. Letting 2 2 = Yp Y3 and 2" = 2 2 Y1 :

+

+

(b)

where s = ry,. Since independence along the x and y axes is
assumed, the the joint pdf is formed by the product of f(w1)
and f(w2) :
f(Wl,WZ)=

f(w1). f ( w z )

Fig. 3. (a) An image of a binary object. (b) Process capability zone for the
object shown in (a).

The level set L, for f(w1, wp) is found from Appendix to
be an ellipse. The equation of the ellipse is

(9)
where:
The process capability zone can be found by determining a
level set L,, where

L, = ((W1,W) : f ( w 1 , w ) 2 7)

~ dwp
l

= 1 - a.

+

(2) c

(12)

=

C = -2 1n(2rsnozloz27).

(13)

(10)

such that for a lOO(1 - a)%confidence level:
f ( w 1 , ~ 2d
)

):(

(11)

Note that z1 and z2 are defined on the x and y axes respectively. If the level set L, is swept around the object boundary,
then this defines the process capability zone. Fig. 3 illustrates
this process.

Ill

I

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. 22 NO. 3, M A Y I N 1992

444

2 x 0 1.o f 0.2

T-

.

'f'
1

3.0f0.5

1

Fig. 4. Sample part with assigned tolerances.

The process capability of the vision system can be determined by checking to see if all of the tolerance specifications
fall within the process capability zone defined by the level set.
This step is discussed in the Section IV.

by the level set shown in Fig. 6(a). From Fig. 6(a) it is seen
that the system is capable of inspecting the outer boundary of
the part to the required tolerances since the process capability
zone lies entirely within the zone defined by the tolerances.
However, the system is not capable of inspecting the holes
to the required tolerances since the process capability zone
exceeds part of the tolerance zone. In order for the vision
system to have the capability to inspect the part, one of two
things needs to be done: 1) the tolerances for the holes must
be loosened to plus or minus 0.3 in, or 2) the size of the level
set must be reduced by either increasing the camera resolution
or buying a more accurate positioning device or both.
The previous discussion has assumed an ideal surface geometry along each feature. For most manufacturing processes,
however, variation will occur along a surface. Many reasons
may account for this including positional error, material imperfections, thermal effects and so on. This is very dependent
on the process used to produce the part. The idea of a process
capability zone for an AV1 system is still valid in this case.
However, a zone should be determined for the manufacturing
process and added to the capability zone for the AV1 system.
This new zone may then be compared to the tolerance zone.
The problem of determining the manufacturing process zone,
however, is extremely process dependent and hence is not
addressed here.

V. EXPERIMENTAL
DATA

In this section, we present a method for determining the
parameters needed to compute the process capability zone.
Further, justification of the error assumptions made in Section
CAPABILITY
FOR AV1 SYSTEM
IV. PROCESS
I1 is provided experimentally to show the validity of (9).
The process capability of the vision system for a particular Finally, an example is given to show how to compute the
part type is defined by the process capability zone discussed in process capability zone from experimental data.
the previous section. In order to determine the process capability of the system, the part specificationsare compared with the
process capability zone. This comparison is dependent upon A. Parameter Estimation
In order to obtain the parameters to be used in the equation
how the part dimensions are specified. There are many (and
at times inconsistent) ways in which parts can be toleranced of the ellipse developed in (13) in Section 11, an experiment
[111-[ 131. These included form control tolerances, profile must be set up for the vision system under consideration. The
control tolerances, orientation tolerances, location tolerances four parameters needed for the ellipse are T , s, uzl , and u Z 2 .
The parameters T and s are from the spatial quantization
and runout tolerances.
In tolerancing a part, it may be that only some of these error. For a given camera, the size of a pixel on the CCD
different tolerances are of importance. As this paper is general, array as well as the focal length can be found from the owners
we are not concerned with how a part is dimensioned and manual. For a camera with pixel height h, width w, and focal
toleranced. What is of concern, however, is if the vision system length f at a distance d from the object, the values for T and
will be able to inspect for the specified tolerances. If the s are
process capability zone lies entirely within the part tolerances,
then the system is capable of inspecting the part. For instance,
Fig. 4 shows a part with with tolerances assigned. For this
part, the length and the width most both be within plus or
minus 0.5 in. The part also contains two holes that are located
with respect to surface A and surface B. The centers of the The parameters
are defined by the standard
and
holes are located absolutely from surfaces A and B, but the deviation of the positional emor and illumination
hole radii are allowed to vary by plus or minus 0.2 in.
shown previously:
The variation allowed for the part defined by its tolerances
is shown in Fig. 5. Fig. 5(a) shows the outer boundary and Fig.
5(b) shows the hole boundaries. Assume the vision system to
be used to inspect the part has a process capability zone defined

~

111

,

GRIFFIN AND VILLALOBOS: PROCESS CAPABILITY OF AUTOMATED VISUAL INSPECTION SYSTEMS

445

(b)
Fig. 5. (a) Allowable part variation for the outer boundary for the sample
part defined by the specified tolerances. @) Allowable part variation for the
inner circles of the same part.

The standard deviation of the positional error along the z
and y axes, a;, and ai,, can be found either by the owners
manual of the positioning device or by experimentation. The
experiment would involve repeated measures of the positional
error of moving the camera to the same location in both the
2 and y axes. These measurements may then be used in the
and
maximum likelihood estimator (MLE) [lo] for
The standard deviation of the illumination error along the 2
and y axes, a;, and ai2,is harder to determine since the range
of lighting conditions of the lighting table must be determined,
or at least data must be collected over this range. Further,
the data collected will also include the quantization error.
: may be developed
However, a point estimator for az1 and a
from (9). Using the method of moments ?MOM):

02~

-2

azl

CiX?
=r s .n

Fig. 6. (a) The area in black shows the process capability zone defined by
the level set. Since this areas falls entirely within the allowable part variation,
the outer boundary of the part satisfies the specifications. @) The area in black
shows the process capability zone defined by the level set for the inner holes.
Notice that this area falls partly outside of the allowable part variation, and
so the specifications are not satisfied in this case.

If the positioning table is stationary during the data collection,
then 8: and 62, are estimates for a;, and ai,.

ai3.

B. Justification of Error Assumptions
In Section 11, we assume that illumination and positioning
error are normally distributed along each axis. In this section,
we show some experimental justification for this assumption
with the use of a vision system. The vision system used by the
authors consists of an Imaging Technology image processing
board (FG100) in a SUN 4/110 workstation with a Javelin
digital camera. The positioning table was made in house, and
fluorescent lighting was used for illumination.
An experiment was conducted in which the intensity level of
a uniform grey sheet was measured at a single pixel over an 8-

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. 22, NO. 3, MAYIJUNE 1992

446

-4.5
94

95

96

97

98

99

100

101 102 103

104

105

-3 5 -2.5 -1.5 -0.5

0.5

1.5

2.5

3.5

4.5

106

Fig. 7 . Frequency histogram for intensity.

Fig. 8. Frequency histogram for positional error of the .I-y

table.

h period at random points in time. Fig. 7 shows the frequency
histogram for intensity for this experiment using the vision
system mentioned previously. Using the MLEs [lo] for the
mean and variance for this data, we get that ji = 100.05
and that U = 2.524. We wish to test the hypothesis that
the observed data are IID random variables from the normal
distribution with mean ji and variance c2 by using the chisquare test. From the data, we get x2 = 0.876. Law and
Kelton [14] show that the hypothesis being tested for should
~ ,IC is~ the- number
~
not be rejected if x2 < ~ i - ~ -where
of intervals, m is the number of estimated parameters and cy
is the significance level of the test. For our case, IC = 9 and
m = 2. For cy = 0.05,
- x6,0.95
2
= 1.635. Since
this number is greater than 0.876, then we do not reject the
hypothesis of normality.
A second experiment was conducted to check the assumption of normality for the 2-9 positioning table with closed loop
control. The table was instructed to move along the z-axis to
a specified point. At each trial, the deviation from this point
was measured. Fig. 8 shows the frequency histogram for the
positional error of the x-y table (units are in hundredths of an
inch). The MLEs for the mean and variance for this data is
jiz3 = 0.021 and that cZ3
= 2.053. Using the chi-square test
Fig. 9. Frequency histogram for system error
to check normality, from the data, we get x2 = 0.522. For
IC = 6, m = 1 and a = 0.05,
- x4,0,95
2
= 0.711.
for data for this experiment.
Since this number is greater than 0.522, then we do not reject
For the camera used in the experiment, the aspect ratio
the hypothesis of normality. Further, notice that ji = 0.021,
is
T = s = 115 pixeldin = 1.15 pixels/hundredth. Using
which is very close to the hypothesized mean of zero. A similar
the
estimator in (15), ez1 = ez2 = 2.500. Using the chiexperiment was conducted along the y-axis, and the MLEs for
the mean and variance for this data was ,Gy3 = -0.028 and square test to check to see if the data follows the hypothesized
= 1.852. Further, applying the chi-square test on this data, distribution, from the data, we get x2 = 0.552. For k = 8,
2
m = 1 and CY = 0 . 0 5 , ~ i - ~ - ~ --, ~xs,0.95
-~
= 1.635.
we did not reject the hypothesis of normality at a = 0.05.
A final experiment was conducted to check the validity of Since this number is greater than 0.552, then we do not
(9) for modeling the system error. In this experiment, the reject the hypothesized distribution. Further, notice that jiz, =
positioning table was held stationary, and the deviation of fizz = -0.037, which is very close to the hypothesized
the imaged boundary from the actual boundary was measured mean of zero. A similar experiment was conducted along the
along the x-axis over an 8-h period to account for the variation y-axis, and using the MOM estimators, it was found that
in lighting conditions. Fig. 9 shows the frequency histogram eZz = cYz= 2.563. Further, using the chi-square test, it

~ i - ~ - ~ , ~ - ~

~ i - ~ - ~ , ~ - ~

111

,
447

GRIFFIN AND VILLALOBOS:PROCESS CAPABILITY OF AUTOMATED VISUAL INSPECTION SYSTEMS

z2= w2 +1

was found that we do not reject the hypothesized distribution
from (9).
C. Computing Process Capability Zone
Based on the data collected in Section V-B, we will compute
the process capability zone. For Section V-B, we found that
T = s = 1.15, 1 3 ~= 2.500, 13yz = 2.563, SX3 = 2.053,
and 13.y3 = 1.852.'From (5), we get
= ((2.500)'
(2.053)2)1/2 = 3.247 and ez2= ((2.563)'
(1.852)2)1/2 =
3.162.
Recall that from Section I11 that the level set L, is an ellipse,
and that the equation of the ellipse is given in (12). Suppose
we wish to have CY = 0.05 (a 95% confidence level) for the
process capability zone, The first step is to find the value of
7.This value may be found by specifying a value of y, and
then numerically integrating (11). We used a golden section
search on gamma until the value of the integral converged to
0.95. The value of gamma was found to be 0.000335. Using
this value of gamma in (13), it was found that C = 9.399.
Using this value of C in (12) to solve for the major axis a and
minor axis b, we found that a = 9.955 hundredths (0.09955
in) and b = 9.694 hundredths (0.0964 in).
The values of a and b define the ellipse used to define
the process capability zone, The values of a and b could be
reduced in a number of ways. First, a more precise positioning
table could be used to reduce the positional error. Second, a
more controlled illumination environment could be used to
reduce the illumination error. However, the easiest form of
control would be to increase the camera resolution, thereby
reducing the spatial quantization error.

+

+

Z1

Fig. 10. Definition of area@).

such that for a 100(1 - a)% confidence level,
f ( ~ 1~, 2 dwl
) dw2 = 1 - CY.
The distribution f(w1,wg) was found to be
1

Define g(w1,wz) as
VI. CONCLUSION
This paper has presented a methodology to determine if an
automated visual inspection system is capable of inspecting
a part within the design specifications. The method involves
determining the error inherent in the computer vision system,
and then comparing a process capability zone based on this
error with the design specifications of the part.
There are a number of interesting problems that still need
to be addressed for process capability analysis. In this paper such that
1
it was assumed that the vision parameters such as resolution
f(w1, W2) = qg(z1, ,z2,) . Area(R)
were specified. An important question is what would be the
best parameters to use in order to inspect a part. This would be
where Area(R) is illustrated in Fig. 10.
based on a cost analysis where the trade off between inspection
Since Area(R) is equal to 4, then
time and type I and type I1 errors would be examined. It was
also assumed in this paper that the image is binary and that the
part to be inspected is flat. Useful extensions would account
for grey-scale images and for 3-D parts. The authors plan on
there
', exists an ( a , , 22,) such
That is, for all (wl, WZ)E !I?
addressing these problems in the future.
that f(wl,w2) = g(z1,,zz0). Therefore, by the definition of
the level set L,:
APPENDIX

To find the level set L,, recall that

L, = ((W1,WP) : f(Wl,W2) 2 7 )
which implies that there is (z1,
g(z1, ,Z',)

,z2, ) where
2 7.

Ill

I

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. 22, NO. 3, MAY/JUNE 1992

448

Therefore

and so

where

C = -2 ln(2rs.lruzl nZ2y).
From this, then, L, is elliptical.

[8] J. H. Tappan, M. E. Wright, and F. E. Sistler, “Error sources in a digital
image analysis system,” Computers and Electronics in Agriculture, vol.
2, pp. 109-118, 1987.
[9] E. P. Lyvers, 0. R. Mitchell, M. L. Akey, and A. P. Reeves, “Subpixel
measurements using a moment-based edge operator,” IEEE Trans.
Pattern Anal. Machine Intell., vol. 11, pp. 1293-1309, 1989.
[lo] W. Feller, An Introduction to Probability Theory and Its Applications,
vol. I, third ed. New York: Wiley, 1968.
[ l l ] A. A. G. Requicha, “toward a theory of geometric tolerancing,” Int. J.
Robotics Res., vol. 2, pp. 45-60, 1983.
[12] C. Wick and R. F. Veilleux, Dimensional Metrology and Geometric
Conformance. Dearbom, M I Society of Manufacturing Engineers,
1988.
[13] F. Etesami, “Tolerance verification through manufactured part modeling,” J. Manufacturing Systems, vol. 7, pp. 223-232, 1988.
[141 A. M. Law and W. D.Kelton, SimulationModeling and Analysis. New
York: McGraw-Hill, 1982.

ACKNOWLEDGMENT

The authors would like to acknowledge Faiz AI-Khayyal,
Chan-Jun Su and the anonymous referees for their helpful
comments.

REFERENCES
[ l ] R. T. Chin, “Automated visual inspection: 1981 to 1987,” Computer
Ksion, Graphics and Image Processing, vol. 41, pp. 346-381, 1988.
[2] C. S . Ho, “Precision of digital vision systems,” IEEE Trans. Pattern
Anal. Machine IntelL, vol. 5, pp. 593-601, 1983.
131 S . D. Blostein and T. S . Huane. “Error analysis in stereo determination
of 3-Dpoint positions,’’ IEEE Tans. Pattern Anal. Machine Intell., vol.
9, pp. 752-765, 1987.
[41 S. D. Blostein. and T. S. Huang, “Correction to ‘Error analysis in
stereo determination of 3-D point positions,’ ” IEEE Trans. Pattern Anal.
Machine Intell., vol. 10, pp. 765, 1988.
151 B. Kamgar-Parsi and B. Kamgar-Parsi, “Evaluation of quantization error
in computer vision,” IEEE Trans. Pattern Anal. Machine Intell., vol. 11,
pp. 929-940, 1989.
D. I. Havelock, “Geometricprecision in noise-free digital images,”IEEE
Trans. Pattern Anal. Machine Intell., vol. 11, pp. 1065-1075, 1989.
C. W. Moon, “Error analysis for dimensional measurement using
computer vision techniques,” IEEE Instrumentation and Measurement
Technol. Conf., San Diego, CA, pp. 372-376, 1988.

Paul M. Griffin (S’86-M’88) received the Ph. D.
degree in industrial engineering in 1988 from Texas
A&M University, College Station, TX.
He is currently an Assistant Professor in the
School of Industrial and Systems Engineering at
the Georgia Institute of Technology, Atlanta. His
research interests include computer vision and tolerance verification.

J. R e d Villalobos (S’86-M’92) received the B. S.
degree in mechanicalhndustrial engineering from
the Tecnologico de Chihuahua in Mexico, the M.S.
degree in industrial engineeringfrom the University
of Texas at El Paso, and the Ph. D. degree in
industrial engineering from Texas A&M University.
He is an Assistant Professor at the University
of Texas at El Paso. His research interests include
quality control, automated visual inspection, and
applied operations research.
Dr. Villalobos is a member of ORSA, IIE, and
Alpha Pi Mu.

514

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 6, NO. 3, JULY 2009

Automated Refinement of Automated Visual
Inspection Algorithms
Hugo C. Garcia and J. Rene Villalobos

Abstract—One of the challenges faced by the users of automated
visual inspection (AVI) systems is how to efficiently upgrade the
legacy systems to inspect the new components introduced into the
assembly lines. If the AVI systems are not flexible enough to accommodate new components, they will be rendered obsolete even by
small changes in the product being inspected. The overall objective
of the research presented in this paper is to produce the methodological basis that will result in the development of highly reconfigurable AVI systems. In this paper, we focus on part of this overall
development, the adaptation of preexisting inspection algorithms
to inspect similar components introduced into the assembly line.
While this paper bases its development and discussion on the inspection of surface mounted devices (SMDs), the proposed methodology is general enough to be applicable to a broad range of inspection problems.
Note to Practitioners—In this paper, we present a methodology
that would allow the automation of the refinement of AVI algorithms. In particular, the proposed method identifies a set of components, or cluster of components, for which a particular set of inspection features or algorithms, renders a certain level of inspection reliability. This is particularly useful for adapting preexisting
systems to inspect new components, especially when the characteristics of the new components are similar to those of components already inspected by the inspection system. We applied this methodology to a case of study of the inspection of SMDs.
Index Terms—Automated visual inspection (AVI) system,
quadratic classification function (QCF), surface mounted devices.

I. INTRODUCTION
NE of the main problems with automated visual inspection (AVI) systems is how to upgrade the legacy system
to be capable of inspecting new components introduced into the
assembly line. A major problem of the existing inspection systems is the high reconfiguration cost, resulting from hardware
and software development, and labor-maintenance costs [1] and
[2]. Many of the current AVI systems are custom-designed for a
specific task and often are very hard to adapt to new applications
[3]. Moreover, the reconfiguration process is a difficult task because it requires knowledge in many technical areas including

O

Manuscript received October 04, 2007. First published May 15, 2009; current version published July 01, 2009. This paper was recommended for publication by Associate Editor Y. F. Li and Editor M. Wang upon evaluation of
the reviewers’ comments. This work was supported in part by the National Science Foundation under Grant DMI-0300361 for the realization of this research
project.
H. C. Garcia was with the Department of Industrial Engineering, Arizona
State University, Tempe, AZ 85281 USA. His is now with L3, Electro-Optical
Systems, Tempe, AZ 85281 USA (e-mail: Hugo.Garcia@L-3com.com).
J. R. Villalobos is with the Department of Industrial Engineering, Arizona
State University, Tempe, AZ 85281 USA (e-mail: Rene.Villalobos@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TASE.2009.2021354

illumination, optics, image processing, and real-time programming [4]–[6]. Finding the personnel with expertise in all these
areas is a difficult and expensive task [6]. As a consequence if,
the AVI systems do not have enough flexibility to accommodate
new products, these systems will be rendered obsolete even by
small changes in the product being inspected. This problem is
particularly intense in the electronics industry where the introduction of new types of components or products is almost a daily
occurrence. Thus, it is necessary to develop reconfigurable AVI
systems that are easily adaptable to new products to extend their
economic life and to reduce the per-unit inspection cost. Ideally,
the adaptation or reconfigurability of the inspection algorithms
to inspect new components should be automated, built into the
inspection system, and transparent to the human user.
The overall objective of the research presented in this paper
is to produce a methodological basis that will result in the emergence of highly reconfigurable AVI systems. In this paper, we
focus on part of this overall development, the adaptation of preexisting inspection algorithms to inspect similar components introduced into the assembly line. In the first part of this paper, we
present a methodology that will allow the automatic formation
of clusters for which a particular inspection algorithm renders
the desired inspection performance. This is important because
this methodology will speed up the development of inspection
algorithms by allowing the incremental adaptation of the inspection algorithms to different cluster of components by changing
the appropriate algorithms’ parameters. In the second part of
this paper, we introduce a method to estimate the performance
of the inspection algorithms being developed during the actual
inspection process. This method has the potential of shortening
the process of adapting the inspection algorithms by screening
out those algorithms that may have performed well in the development phase but may not do so in the actual inspection.
We believe that while this paper bases its development
and discussion on the inspection of surface mounted devices
(SMDs), the methodology is general enough to be applicable
to a broad range of inspection problems.
In a previous paper, [7], the authors laid out the foundations
for the inspection methodology used in this research. In particular, they introduced an iterative vector-based approach for
the inspection of SMD components. In this approach, for each
component being inspected, a fixed number of inspection features are extracted. Then, a real value is associated with each
feature; these values form a vector that is used for the classification of the SMD under inspection as present or absent using
the quadratic classification function (QCF). Using this previous
work as a starting point, we present an iterative methodology to
facilitate the automated adaptation of preexisting inspection systems. The general steps of the methodology are given in Fig. 1.

1545-5955/$25.00 © 2009 IEEE

GARCIA AND VILLALOBOS: AUTOMATED REFINEMENT OF AUTOMATED VISUAL INSPECTION ALGORITHMS

515

work to classify the components; this algorithm is called the
quadratic classification function (QCF). Section IV presents
the methodologies to identify and remove outliers from the
main cluster of elements. Section V shows the methods to
predict the misclassification error rate (MER) during the inspection process. Section VI explains the experimental settings
of the platform used to validate the proposed methodologies.
Section VII presents the experimental results. Section VIII
offers the conclusions of the results obtained.
II. RELATED WORKS

Fig. 1. General steps of the methodology.

This paper focuses on the steps highlighted in gray color in
Fig. 1. Our focus is on determining what elements of components under inspection can be clustered together to be reliably
inspected with the same inspection algorithm and also on the
estimation of the performance of the resulting inspection algorithm in the assembly line. These two problems are briefly described next.
The first issue focuses on identifying a methodology to
determine a main cluster (subset) of those SMD instances
for which the preexisting inspection algorithm provides the
desired level of discrimination. We explore the issue of setting
a reliable boundary to identify the components belonging to
the main cluster. The main idea is that all the components
whose values fall within the boundaries will be inspected using
the same inspection algorithm; otherwise, those components
will be tagged as “outliers” and inspected using alternative
inspection algorithms. It is important that the outliers are not
inspected as part of the main cluster since they can seriously
distort the performance of the inspection algorithm, retarding
the reconfiguration of the preexisting inspection system.
The second issue addressed is the estimation, or prediction,
of the performance of the resulting inspection algorithms in the
assembly line. The methodology to be presented uses the historical performance of the AVI system, as well as data from
the training phase of the development of the inspection algorithm. This issue is important because of shortcomings of the
usual approach of minimizing the inspection errors during the
training phase hoping that this will result in the minimization
of inspection errors in the actual inspection phase. The goal of
this process is to make sure that the level of discrimination provided by the new algorithms is adequate when the system is performing the actual inspection of components.
This paper is organized as follows. Section II gives a brief
review of previous works related to the problem at hand.
Section III introduces the statistical algorithm used in this

While the literature on each of the individual problems addressed in this paper is abundant, literature on the integration
of these problems is scarce. For instance, numerous works in
the area of development of AVI algorithms are documented in
surveys by [8]–[13]. Reference [14] documents the work in the
area of inspection of the printed circuit board (PCB). However,
none of these surveys report any methodology to address the automatic reconfiguration of AVI systems.
The problem of feature selection, while not directly addressed
in this paper, is part of the overall automated reconfiguration
methodology being proposed. For this reason, we give a brief
literature review of this topic. Feature selection has been addressed extensively for applications other than AVI systems,
such as classification and diagnostic problems in engineering,
social and medical sciences (see, for instance, [15]–[17], respectively). A new area of application and development of the feature
selection methods is called sensor selection. Some informative
papers that address this topic are [18]–[21]. The literature concerned with feature selection for discriminant analysis is vast;
some examples include [22]–[26]. For a case of study of feature
selection applied to SMD components, the lector is referred to
[27].
Outliers are observations with a unique combination of characteristics identifiable as distinctly from the other observations
[28]. Many statistical techniques have been proposed to detect
outliers, compressive texts, and articles on this topic are those
by [29]–[36]. In our case, the problem is to decide if the corresponding component should be inspected as part of a main
cluster of components or inspected individually as an exception.
Regarding the issue of estimating the performance inspection algorithms, the papers available in the open literature are
very limited. For instance, one of the most common ways to
measure the capability of an inspection system is by using a
non-parametric statistic known as the MER. The MER consists
of counting the number of misclassified components and dividing this number by the total number of components inspected
[37]. A goal of this paper is to determine an appropriate function to establish the inspection capability in terms of the MER.
III. THE QUADRATIC CLASSIFICATION FUNCTION (QCF)
The inspection approach presented in this paper is based on
a vector-based classifier known as the QCF. The advantage
of using a vector-based classification approach over the more
traditional sequential approaches is documented, among other
places, in [7]. We briefly summarize this classifier function
next.

516

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 6, NO. 3, JULY 2009

The QCF assigns an observation under consideration to the
most likely of two predefined populations; in our case denoted
“present” and
“absent” components. Since the actual
as
population parameters are usually not known in advance, it is
necessary to obtain their estimates from data samples obtained
in a training phase. Therefore, the QCF is usually expressed as
follows.
(new observation) to , if
Allocate

(1)
otherwise, allocate

to

, whereby

Fig. 2. Schematic approach to eliminate outliers.

Value of multivariate classifier.
.
The vector of feature values of the new
inspected component.
The sample covariance matrix for population
.
The sample covariance matrix for population
.
The sample mean vector of the features for
population .
The sample mean vector of the features for
population .
The cost of misclassification (observation from
is incorrectly classified as ).
The cost of misclassification (observation from
is incorrectly classified as ).
The prior probability ratio.
For more details about the development of the QCF equation, the reader is referred to [37]. However, it is important to
highlight that the multivariate classifier just described relies on
assumptions of normality. Commonly the data rendered by features used in SMD inspection depart from these assumptions.
However, the QCF can accommodate small departures from normality. If the departures from normality are significant the data
can be transformed to obtain better results [38]–[41].

outliers. Once a main cluster is obtained, the appropriate parameters for the QCF are determined. In addition, for each outlier,
a QCF is created to classify them individually. Fig. 2 presents a
schematic view of the approach used to identify and eliminate
outliers.
The proposed approach is iterative in nature. The first step
consists in obtaining training data from present and absent components on PCBs. Using this data, a function to eliminate the
outliers is created. This process includes the estimation of the
mean and covariance matrices for the present and absent populations. Once the function is defined, all the elements are evaluated using this function to determine if they belong to one of
these populations (or main cluster) or are classified as outliers.
After the first set of outliers is eliminated from the main
cluster, the parameters of the modified main cluster are recalculated. The outlier identification process continues until
no additional elements are eliminated. With the elements remaining in the main cluster, the final parameters of the QDF
are calculated. We explored three different methods for the
identification of outliers: setting a classification boundary (CB)
based on the statistical distance, the use of the Chi-Square
distribution, and the use of the Mahalanobis distance. These
methods are presented in the next subsection of this paper.
From the perspective of the overall reconfiguration problem,
the process of cluster formation will help accelerate the adaptation of preexisting algorithms to new components by allowing
the efficient reuse of the old inspection algorithms to inspect the
new components.
A. Classification Boundary (CB)

IV. CLUSTER FORMATION AND ELIMINATION OF OUTLIERS
An underlying assumption of the QCF is that all the components being inspected belong to a single homogeneous population or “main cluster”. Because of slight differences between the
components, this supposition is rarely strictly met in practical
applications. This phenomenon may be the result of different
factors, for instance components in different locations of a PCB
may produce different images because of lighting. We propose a
refinement strategy by which a priori, seemingly homogenous,
group for each type of component is formed and subsequently
through several iterations this group is refined by eliminating

The CB method was proposed by [7]. In this method a CB is
created to delimit the statistical region within which it is safe to
make the assignment of a particular element to one of the two
populations under consideration. This CB is given by
(2)
where
are the mean vectors and
matrices of the populations and

are the inverse covariance
is the number of features.

GARCIA AND VILLALOBOS: AUTOMATED REFINEMENT OF AUTOMATED VISUAL INSPECTION ALGORITHMS

517

Once the CB is determined, the next step is to determine the
statistical distance of the element under consideration in the two
populations. These distances are given by
(3)
(4)
or
is greater than the threshold CB
If the distance
given by (2), the element is considered an outlier.
B. Chi-Square
The second method is a variation of the Chi-Square multivariate test. This test is used as a multivariate vector mean test
is known. Using this test as a
when the matrix covariance
base, two equations were designed to eliminate the outliers of
the populations. These equations are
(5)
(6)
Fig. 3. Methodology to predict the AVI system performance.

is the sample inverse covariance matrix, is the
where
sample mean vector of the each population, and is the vector
of the values of the component .
or
, the
The decision rule of this method is: if
is the upper
component is considered an outlier, where
percentile of the
distribution, is the sample size and is
the probability of a Type I error.
C. Mahalanobis Distance
The last method used is similar to the previous one; the difference is that this method uses the Mahalanobis distance, which
distribution. The equations are defined as
has a Hotelling’s
follows:
(7)
(8)
, where
is the number of instances.
for all
is the sample inverse covariance matrix, is the sample
or
mean vector of the population. The decision rule is: if
, the component is considered an outlier, where
is a Hotelling distribution with and
degrees of
freedom.
The previous three methods for outlier elimination have as
common characteristics their simplicity and efficiency. From
the point-of-view of the user of the AVI systems, these methods
are easy to understand because they are based on simple mean
vectors and covariance matrices of the populations, which are
also used in the QCF previously introduced.

the aim to meet the required performance level during the actual inspection rather than during the training phase of the algorithmic development. This is the aim of the procedure discussed
next.
When a new component is introduced into the assembly line,
it is necessary to choose the appropriate set of features to inspect
it. Once a set of features is selected, it is necessary to evaluate
the effectiveness of the resulting inspection algorithm. Usually,
this evaluation is based on misclassification error data obtained
during the training phase of the development of the algorithm,
not the actual performance of the inspection system in the factory floor. In this paper we aim to fill this gap by using the information of the training phase and the historical behavior of the
system to predict the actual MER for the new component.
A general methodology for the prediction of the MER is given
in Fig. 3. In this methodology, a regression analysis equation is
used to predict the discrimination performance of the system on
the factory floor in terms of the expected error rate.
With this methodology, it is necessary to determine the independent variable to be used in the regression function. The dependent variable is given by the MER. In the next subsections,
we describe three variables that we explored as candidates for
independent variables. These variables are based on methods
such as Wilks’ Lambda Value, Hotelling–Lawley Trace Value,
and the statistical distance between populations. These methods
are briefly explained next.
A. MANOVA Tests

V. PREDICTION OF THE AVI SYSTEM PERFORMANCE
When a new inspection algorithm is developed, it is usually
judged on whether it meets a certain required level of performance rather than on whether it is optimal with respect to a certain measure of performance. Thus, the redesigned inspection
algorithm should be constructed accordingly, in particular, with

The first two methods, Wilks’ Lambda Value and
Hotelling–Lawley Trace Value are based on multiple analysis of variance (MANOVA) tests. MANOVA is a procedure
for testing the equality of mean vectors of more than two populations [42]. In a one-way MANOVA, it is assumed that random
samples of size with -features observations are available

518

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 6, NO. 3, JULY 2009

from each of
multivariate normal populations with equal
[43]. The model for each observation
covariance matrices
vector is

Wilks’ Lambda Value: The Wilks’ Lambda value is a measure
of the difference between groups or populations of the vector of
means on the independent variables. The likelihood ratio test
statistics can be expressed as

(9)
(12)
and

, where
and
provide a mean and “treatment” effect, respectively, for all
features. The hypothesis of interest in one-way MANOVA is
versus
: at least two ’s are
unequal.
We use MANOVA tests to obtain the values of Wilks’
Lambda and Hotelling–Lawley Trace and identify the correlation between these values and the MER. Before introducing
the Wilks’ Lambda and Hotelling–Lawley Trace methods, we
define some terminology used in the remainder of this paper.
The definitions of sample totals and means of the training data
in matrix form are given as follows:
for all

The statistic has a Wilks’ Lambda distribution with
and
degrees of freedom [43]. The Lambda
, in which larger values indicate a
value is between
poor separation between populations, and smaller values indicate good separation between groups.
Hotelling-Lawley Trace Value: The Hotelling–Lawley trace
-test. This test is
test is also called Hotelling’s generalized
given by the following equation:
(13)
For more details about this test, the reader is referred to [43].
B. Statistical Distance between Populations

..
.

..
.

..
.

..
.

..
.

..
.

..
.

..
.

The approach presented in [7] was to use the
distribution
to determine the classification boundaries of the populations. A
measure of the distance between the populations is given by

where
should be independently observed from a multivariate population
is the total number of populations, is the number of observations of the training data set, and represents an individual
observation.
represent the between and within variThe matrices and
ation of the training data, respectively

where
is the estimate of the statistical distance between
the means of the populations. Reference [7] established a decision rule as follows: the populations are determined to be classifiable if
(14)
otherwise, the populations are termed unclassifiable. Reasonable values for included numbers equal or greater than five. In
as the independent variable of a
the current paper, we use
regression analysis equation.
VI. THE EXPERIMENTAL SETTING

(10)

(11)

The eigenvalues of matrix
are defined as follows:
, where
is the
number of features and
. The methods to determine
the independent variable using the MANOVA tests are described
next.

We used data collected in an experimental platform available at the Electronics Assembly Laboratory (EAL) at Arizona
State University (ASU) to test the different methods previously
described. The platform consists of a mechanical positioning
system, an image acquisition system, an illumination system, a
Matrox® board, and a personal computer (PC). The image acquisition system consists of four Pulnix™ cameras each with a
25 mm lens. The resolution of the system is 0.0703 mm/pixel
and the size of a typical board to inspect is 10 13 inches. The
positioning devices consist of a chain conveyor, an – table,
and the controllers needed to position the four cameras with
respect to the board being inspected. The lighting system consists of eight LED panels. Four of the panels provide coplanar
(sideways) illumination and the other four panels provide frontal
(top) illumination to the board. The Matrox® board is in charge
of digitizing and storing the images, as well as performing the

GARCIA AND VILLALOBOS: AUTOMATED REFINEMENT OF AUTOMATED VISUAL INSPECTION ALGORITHMS

519

Fig. 4. Side view of AVI system.

Fig. 7. Histogram of the correlation feature.

Fig. 5. Frontal illumination image.

Fig. 8. Histogram of the blob feature.

Fig. 6. Coplanar illumination.

different image processing algorithm computations. The PC is
in charge of the overall control of the system, performing data
preprocessing, inspection control, statistics gathering, and data
management. Fig. 4 presents a picture of the experimental platform.
Figs. 5 and 6 present images acquired of SMD using frontal
and coplanar illumination, respectively. In order to acquire the
images of the components to be inspected, the board is divided
into “component windows” that correspond to those areas of
the board where a component should be present. Once these
component windows are defined, the AVI system only examines
the images of these regions.
The experimental results are based on a single class of SMD
component, the 805 V component. There are 281 elements of
this type of component in the PCB used in the experiments. The
AVI system inspected 11 PCB sets. Each set was composed of a
PCB with all the SMD components present and another one with
all components absent. The data used for the training phase of
the classifier was based on the first data set and the remaining ten
data sets were designated to simulate the real inspection phase.
Therefore, there were 281 elements for the training phase and

2810 elements for the inspection phase for each population. Six
inspection features were used: Energy (E), Correlation (C), Diffusion (D), Fenergy (F), Texture (T), and Blob (B). The details
of these features are discussed in [44]. The features used in the
experiment were selected to highlight the application presented
in this paper and not necessarily for their discrimination power.
For instance, Figs. 7 and 8 depict, through the use of the histograms, the discrimination provided by the features of Correlation and Blob, respectively, to highlight the degree of separation between the absent (population 1) and present (population
0) components.
In addition, Fig. 9 presents the T values of the QCF using
Correlation and Blob. In the following section, it will be shown,
in Table III, that this subset of features has the lowest MER
of all the combination of subsets, despite the underwhelming
performance of each feature when used independently.
VII. EXPERIMENTAL RESULTS
To simulate new data sets and conduct more outlier elimination experiments, an assumption was made that the full
data set was composed of only five features instead of
six. This assumption gives six extra combinations of feature sets that can be analyzed as separate experiments to

520

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 6, NO. 3, JULY 2009

Fig. 9. The T value using the correlation and blob.

TABLE I
DATA SET SCENARIOS

elements used for training and inspection phases. In scenarios
B and C, the first column represents the number of elements
excluding the outliers found by applying the methods of elimination of outliers. Column 2 in B and C displays the names of
the method used to eliminate the outliers. Columns 2 in A and
3 in B and C, show the statistics used to measure the MER. The
final column in each scenario represents the value of the statistics obtained from the training and inspection data sets.
From Table II, it can be concluded that it is better to use any of
the three methods to eliminate the outliers than to use all the elements as one cluster. This deduction is derived from the observation that lower values of MER are obtained when the outliers
are eliminated. Furthermore, we conclude, based on the reduction of the MERs, that the CB was the best of the three methods
used. For instance, the MER obtained by the CB method in Scenario C was lower than the MER for Scenario A for any of the
seven experiments. In particular, using this method the MER in
Scenario C was reduced to zero in five of the seven experiments
conducted.
B. AVI System Performance Results

determine the best outlier elimination method. For example, the full set of features (Energy, Correlation, Diffu,
sion, Fenergy, Blob, and Texture) is labeled
which represents the original set of features. Under the
new assumption, the new six feature sets are denoted as:
and

.

A. Outlier Elimination
Three different scenarios were used to validate the outlier
elimination methods. The scenarios are displayed in Table I.
For example, Scenario A means that all outliers are included in
the training and inspection data. Scenario B means that only the
outliers were removed from the training data. Lastly, Scenario
C means that there are no outliers in the training and inspection data sets. Scenarios B and C were created to evaluate the
methods and also to compare the results with respect to Scenario A.
For each scenario, we used three nonparametric statistics to
measure the level of discrimination of the seven experiments
through the MER. These statistics are: the apparent error rate
(APER), the expected actual error rate (EAER), and the inspection error rate (IER).
The three statistics represent the proportion of errors that result from applying the QCF to the training or inspection data
sets, the first two methods are based on the training phase and
the last one is based on the inspection phase. These statistics are
described with more details in [27] and [37].
Table II shows the results of the experiments. The first column
of the Table II shows the features used in each particular experiment. The first column in Scenario A represents the number of

Table III shows the results of the three methods used to predict
the AVI system performance. The values of these statistics were
calculated based on the total number of possible subsets (63)
using the six features considered. For example, for the subset
number four (CFBT), Table III presents the value of the MER
and the values of the statistics calculated using the training data.
The subsets are shown in ascending order according to the MER.
In order to summarize the previous information, Table IV
presents the values of the correlation coefficients between the
MER and each of the statistical methods whose results are
shown in Table III. The method with the greatest correlation
coefficient with respect to the MER is Wilks Lambda. This
result supports the idea that the best method to predict the MER
based on the correlation between these two measurements is the
Wilks Lambda method. In addition to this analysis, a regression
analysis was performed to identify the best method to predict
the MER and to estimate the values of the linear equation.
The simple linear regression model with a single independent
variable means that it has a relationship with a dependent variable in form of a straight line. A linear model is described as
follows:
(15)
where
and
are the intercept and slope of the linear equation, respectively.
Using the least squares method, we obtain the fitted simple
linear model as follows:
(15)
where
and
. Equation (15) gives an estimation point of
the mean of for a particular point of . For more details about
this technique, the reader is referred to [45]–[49].
Table V presents the summary of the regression analysis performed with the information from Table III. The dependent variable was the MER and the independent variable was the value

GARCIA AND VILLALOBOS: AUTOMATED REFINEMENT OF AUTOMATED VISUAL INSPECTION ALGORITHMS

521

TABLE II
SUMMARY OF THE OUTLIERS ELIMINATION PROCESSES

of each statistical method. Three different regression analysis
equations (one for each method) were created. The values of
these equations are given in Table V.

According to the model adequacy measures and the analysis
of variance tests shown in Table V, the best function used to
predict the MER was the one obtained using the Wilks Lambda

522

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 6, NO. 3, JULY 2009

TABLE III
SUMMARY OF THE EXPERIMENTAL RESULTS

TABLE IV
CORRELATION COEFFICIENTS VALUES

TABLE V
SUMMARY OF THE REGRESSION ANALYSIS

method. To validate this result, we created ten prediction equausing the Wilks
tions with the confidence intervals
Lambda values of 20 random subsets. Using the equation of
these subsets, the MER was predicted for the other 43 subsets.
The average of the subsets inside of the confidence interval, of
the ten experiments, was 95.91%. With this result, we can conclude that this method is a good predictor of the MER of a new
component. This method will be very useful to AVI developers
because they will have a statistical tool to measure the capacity
of the system and decide if it has the necessary capability to inspect the new components.

VIII. CONCLUSION
This paper tackles two critical problems that need to be
solved to make feasible the development of reconfigurable
inspection systems; namely, the identification of those components for which the direct transfer of the preexisting algorithms
is not possible and the prediction of the reconfigured inspection
algorithm in the actual inspection of components. The former
problem was addressed through outlier elimination, while the
latter was addressed through the use of linear regression models
based on discrimination measures. In particular, three methods
to eliminate the outlier of the training data of the populations
were introduced. The results showed that the use of any of
the three methods to eliminate the outliers in the training data
renders lower MERs than using the data as a single cluster. We
concluded that the best method, of the three analyzed, was the
method based on setting a CB.
The problem of predicting the performance of the new inspection algorithms was solved using a linear regression analysis approach. The linear regression equation rendering the best prediction of the AVI system performance used the Wilks Lambda
value as an independent variable. This method has the potential
of considerably shortening the algorithmic development time
by providing a more precise measure of the capability of the
resulting inspection algorithms. In case that the algorithms do
not have the desired capability, the developer can decide to improve the systems’ capability by modifying the preexisting feature characteristics, creating more features, or acquiring more

GARCIA AND VILLALOBOS: AUTOMATED REFINEMENT OF AUTOMATED VISUAL INSPECTION ALGORITHMS

sophisticated hardware. It should be mentioned that while the
linear models explored provide acceptable results in terms of
their prediction power the underlying problem is clearly nonlinear. The exploration of nonlinear prediction models is something that is left as future research.
REFERENCES
[1] S. Cuenca, A. Camara, J. Suardiaz, and A. Toledo, “Domain-specific
codesign for automated visual inspection systems,” Lecture Notes of
Computer Science, vol. 3522, pp. 683–690, 2005.
[2] R. Sablatning, “Increasing flexibility for automatic visual inspection:
The general analysis graph,” Machine Vision and Applications, vol. 12,
pp. 158–169, 2000.
[3] D. Kang, Y. Chung, W. Doh, W. Jung, and S. Park, “Applying object modeling technique to automated visual inspection of automated
compressor parts omission,” Int. J. Mach. Tools Manuf., vol. 39, pp.
1779–1792, 1999.
[4] R. Rosandich, Intelligent Visual Inspection. London, U.K.: Chapman
& Hall, 1997.
[5] S. Seida and M. Magee, D. P. Casasent, Ed., “Real time model based
vision for industrial domains,” in Proc. SPIE Intell. Robot. Comput.
Vision XII: Algorithms and Tech., 1993, pp. 17–27.
[6] W. Chen and G. Libert, “Real-time automatic visual inspection of highspeed plane products by means of parallelism,” Real-Time Imaging,
vol. 4, pp. 379–388, 1998.
[7] J. R. Villalobos, M. Arellano, A. Medina, and F. Aguirre, “Vector classification of SMD images,” J. Manuf. Syst., vol. 22, no. 4, pp. 265–282,
2004.
[8] E. Malamas, G. Petrakis, M. Zervakis, and L. Petit, “A survey on industrial vision systems, applications and tools,” Image and Vision Computing, vol. 21, pp. 171–188, 2003.
[9] R. Chin and C. Harlow, “Automated visual inspection: Survey,” IEEE
Trans. Pattern Anal. Mach. Intell., no. 6, pp. 557–573, 1982.
[10] A. Thomas, M. Rodd, J. Holt, and C. Neill, “Real-time industrial visual
inspection: A review,” Real-Time Imaging, vol. 1, pp. 139–158, 1995.
[11] E. Bayro, D. P. Casasent, Ed., “Review of automated visual inspection:
Part I conventional approaches,” in Proc. SPIE Intell. Robot. Comput.
Vision XII: Algorithms and Tech., 1993a, pp. 128–158.
[12] E. Bayro, D. P. Casasent, Ed., “Review of automated visual inspection:
Part II approaches to intelligent systems,” in Proc. SPIE Intell. Robot.
Comput. Vision XII: Algorithms and Tech., 1993b, pp. 159–169.
[13] R. T. Chin, “Automated visual inspection: 1981 to 1987: Survey,” Computer Vision, Graphics, and Image Processing, vol. 41, pp. 346–381,
1988.
[14] M. Moganti, F. Ercaal, C. Dagli, and S. Tsunekawa, “Automatic PCB
inspection algorithms,” Computer Vision and Image Understanding,
vol. 63, no. 2, pp. 287–313, Mar. 1996.
[15] Y. Ding, P. Kim, D. Ceglarek, and J. Jin, “Optimal sensor distribution
for variable diagnosis in multistation assembly processes,” IEEE Trans.
Robot. Autom., vol. 19, no. 4, pp. 543–554, Aug. 2003.
[16] M. J. Baxter, “Stepwise discriminant analysis in archaeometry: A critique,” J. Archaeological Sci., vol. 21, pp. 659–699, 1994.
[17] K. Lee, N. Sha, E. Dougherty, M. Vannucci, and B. Mallick, “Gene selection: A Bayesian variable selection approach,” Bioinformatics, vol.
19, no. 1, pp. 90–97, 2003.
[18] Y. Ding, S. Sampatraj, and K. Abhishek, “A survey of inspection
strategy and sensor distribution studies in discrete-part manufacturing
processes,” IIE Trans., 2006.
[19] L. Fraleigh, M. Guay, and J. Forbes, “Sensor selection for model-based
real-time optimization: Relating design of experiments and design
cost,” J. Process Control, vol. 13, pp. 667–678, 2003.
[20] S. Jiang, R. Kumar, and H. Garcia, “Optimal sensor selection for
discrete-event systems with partial observation,” IEEE Trans. Autom.
Control, vol. 48, no. 3, pp. 369–381, Mar. 2003.
[21] R. Debouk, S. Lafortune, and D. Teneketzis, “On an optimization
problem in sensor selection,” Discrete Event Dynamic Systems: Theory
and Applications, vol. 12, pp. 417–445, 2002.
[22] A. Gupta, T. Logan, and J. Chen, “A variable selection technique in discriminant analysis with application in marketing data,” J. Stat. Comput.
Simulation, vol. 63, no. 2, pp. 187–199, 1999.
[23] A. Jain, R. Duin, and J. Mao, “Statistical pattern recognition: A review,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 1, Jan. 2000.
[24] J. Glenn, “Integer programming methods for normalization and variable selection in mathematical programming discriminant analysis
models,” J. Oper. Res. Soc., vol. 50, no. 10, pp. 1043–1053, Oct. 1999.
[25] O. Snorrason and F. Garber, “Evaluation of nonparametric discriminant
analysis techniques for radar signal feature and extraction,” Opt. Eng.,
vol. 31, no. 12, pp. 2608–2617, Dec. 1992.

523

[26] E. Kinney and D. Murphy, “Comparison of the ID3 algorithm versus
discriminant analysis for performing feature selection,” Comput.
Biomed. Res., vol. 20, no. 5, pp. 467–476, Oct. 1987.
[27] H. C. Garcia, J. R. Villalobos, and G. Runger, “Automated feature selection for visual inspection systems,” IEEE Trans. Autom. Sci. Eng.,
vol. 3, no. 4, pp. 394–406, 2006.
[28] J. Hair, R. Anderson, R. Tatham, and W. Black, Multivariate Data
Analysis With Readings, 4th ed. Englewood Cliffs, NJ: Prentice-Hall,
1995.
[29] V. Barnet and T. Lewis, Outliers in Statistical Data, 3rd ed. : Wiley,
1994.
[30] R. Gnanadesikan and J. Kettenring, “Robust estimates, residual, and
outlier detection with multiresponse data,” Biometrics, vol. 28, pp.
81–124, Mar. 1972.
[31] F. E. Grubbs, “Procedures for detecting outlying observations in samples,” Technometrics, vol. 11, pp. 1–21, 1969.
[32] D. Hawkins, Identification of Outliers. London, U.K.: Chapman &
Hall, 1980.
[33] J. Hodge and J. Austin, “A survey of outlier detection methodologies,”
Artif. Intell. Rev., vol. 22, no. 2, pp. 85–126, Oct. 2004.
[34] T. Hu and S. Sung, “Detecting pattern-based outliers,” Pattern Recogn.
Lett., vol. 24, pp. 3059–3068, 2003.
[35] X. Liu, C. Gongxian, and X. John, “Analyzing outliers cautiously,”
IEEE Trans. Knowl. Data Eng., vol. 14, no. 2, pp. 432–437, Mar./Apr.
2002.
[36] W. Woodwarda and S. Sainb, “Testing for outliers from a mixture distribution when some data are missing,” Comput. Stat. Data Anal., vol.
44, pp. 193–210, 2003.
[37] R. A. Johnson and D. W. Wichern, Applied Multivariate Statistical
Analysis, 3rd ed. Upper Saddle River, NJ: Prentice-Hall, 1996.
[38] R. Gnanadesikan, Methods for Statistical Data Analysis of Multivariate
Observations, 2nd ed. New York: Wiley, 1997.
[39] K. Mardia, , P. Krishnaiah, Ed., “Tests of univariate and multivariate
normality,” in Handbook of Statistics. Amsterdam, The Netherlands:
North-Holland, 1980, vol. 1, ch. 9, pp. 279–320.
[40] J. Romeu and A. Ozturk, “A comparative-study of goodness-of-fit tests
for multivariate normality,” J. Multivariate Anal., vol. 46, pp. 309–334,
1993.
[41] K. Mardia, “Measures of multivariate skewness and kurtosis with applications,” Biometrika, vol. 57, no. 3, pp. 519–530, 1970.
[42] E. W. Weisstein, “MANOVA,” in From MathWorld-A Wolfram Web
Resource. Champaign, IL: Wolfram Research. [Online]. Available:
http://mathworld.wolfram.com/MANOVA.html
[43] A. Rencher, Multivariate Statistical Inference and Applications. New
York: Wiley, 1998.
[44] L. Williams, “Development of a feature selection methodology for automated visual inspection systems” Honor thesis, Dept. Industrial Engineering (Thesis Coursework), Arizona State Univ., Tempe, AZ, 2004
[Online]. Available: http://library.lib.asu.edu/record=b4759349
[45] J. Devore, Probability and Statistics for Engineering and the Sciences,
5th ed. Duxbury Press: Pacific Grove, CA, 2000.
[46] S. Weisberg, Applied Linear Regression, 3rd ed. New York: Wiley,
2005.
[47] D. Montgomery, E. Peck, and G. Vining, Introduction to Linear Regression Analysis, 3rd ed. New York: Wiley, 2001.
[48] R. Myers and D. Montgomery, Response Surface Methodology, 2nd
ed. New York: Wiley, 2002.
[49] N. Draper and H. Smith, Applied Regression Analysis. New York:
Wiley, 1998.
[50] V. Barnet, “The ordering of multivariate data (with discussion),” J.
Royal Stat. Soc. A, vol. 139, pp. 318–354, 1976.

Hugo C. Garcia received the B.S. degree in industrial and systems engineering from the Instituto
Tecnológico y de Estudios Superiores de Monterrey,
Campus Ciudad Juárez, México, in 2000, the M.B.A.
degree from the Universidad Autónoma de Ciudad
Juárez in 2002, the M.S. degree in statistics from
the University of Texas, El Paso, in 2003, the M.S.
degree in industrial engineering from the Instituto
Tecnológico de Ciudad Juárez in 2005, and the Ph.D.
degree in industrial engineering at Arizona State
University, Tempe, in 2007.
His main research interest lies in automated quality, applied statistics, and
inspection systems.

524

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 6, NO. 3, JULY 2009

René Villalobos received the B.S. degree in industrial-mechanical engineering from the Chihuahua
Technological Institute, Mexico, in 1982, the M.S.
degree in industrial engineering from the University
of Texas, El Paso, in 1987, and the Ph.D. degree in
industrial engineering from Texas A&M University,
College Station, in 1991.
He is an Associate Professor in the Department
of Industrial Engineering, Ira A. Fulton School
of Engineering, Arizona State University, Tempe.
His research interests are in the areas of logistics,
automated quality systems, manufacturing systems, and applied operations
research.

1338

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

A Novel Feature Selection Methodology for
Automated Inspection Systems
Hugo C. Garcia, Jesus Rene Villalobos, Rong Pan,
and George C. Runger
Abstract—This paper proposes a new feature selection methodology. The
methodology is based on the stepwise variable selection procedure, but, instead of
using the traditional discriminant metrics such as Wilks’ Lambda, it uses an
estimation of the misclassification error as the figure of merit to evaluate the
introduction of new features. The expected misclassification error rate (MER) is
obtained by using the densities of a constructed function of random variables,
which is the stochastic representation of the conditional distribution of the
quadratic discriminant function estimate. The application of the proposed
methodology results in significant savings of computational time in the estimation
of classification error over the traditional simulation and cross-validation methods.
One of the main advantages of the proposed method is that it provides a direct
estimation of the expected misclassification error at the time of feature selection,
which provides an immediate assessment of the benefits of introducing an
additional feature into an inspection/classification algorithm.
Index Terms—Feature selection, misclassification error rate, quadratic
discriminant function.

Ç
1

INTRODUCTION

ONE of the key steps in the development of any inspection or
classification system is the feature selection process. This process
often considers all possible subsets of features. The result is
combinatorial in nature that makes it a long and tedious process,
especially when the selection is performed by humans and the
features being considered present complex cross-correlation
patterns. A process that is particularly tedious is testing the results
rendered by the different subsets of features used during the
algorithm development process. During this process, each time a
new subset is evaluated, the resulting inspection algorithm needs
to be implemented and tested to assess its performance in terms of
misclassification error rate (MER). This paper addresses this
problem by introducing a novel methodology that speeds up the
identification of the appropriate subset of features. We estimate the
misclassification error rate of the resulting inspection algorithm
even before it is implemented. The methodology uses the MER as
the guiding evaluation criterion of a stepwise feature selection that
has as its main goals the minimization of MER, while minimizing
the number of features. This is important because the larger the
feature set used, the more expensive the development of the
inspection system.
The proposed methodology is based on the Quadratic
Discriminant Function. This discriminant function has several
advantages with respect to other classifiers. For example, for
small sets of training samples, it is preferred over complicated
classifiers when class separation is not complex [1]. This classifier
also tends to decrease the design error while improving the
. H.C. Garcia is with L3, Electro-Optical Systems, 1215 S. 52nd Street,
Tempe, AZ 85281. E-mail: Hugo.Garcia@L-3com.com.
. J.R. Villalobos, R. Pan, and G.C. Runger are with the Department of
Industrial Engineering, Arizona State University, PO Box 875906, Tempe,
AZ 85287-8692. E-mail: {rene.villalobos, runger, rong.pan}@asu.edu.
Manuscript received 26 Mar. 2008; revised 29 Sept. 2008; accepted 30 Oct.
2008; published online 10 Nov. 2008.
Recommended for acceptance by S. Li.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number
TPAMI-2008-03-0166.
Digital Object Identifier no. 10.1109/TPAMI.2008.276.
0162-8828/09/$25.00 ß 2009 IEEE

Published by the IEEE Computer Society

VOL. 31,

NO. 7, JULY 2009

understandability of the classification properties [2]. The methodology is based on previous research in the areas of applied and
theoretical statistics, in particular, the works by McFarland and
Richard [3] and Hua et al. [4].
One of the main motivations of the proposed feature selection
methodology is to develop tools needed for the emergence of selfreconfigurable automated inspection systems [5]. One of the main
conditions for the emergence of these systems is the automation of
decisions that are traditionally performed by human developers
such as feature generation, selection, and testing of the resulting
algorithms. The methodology presented in this paper contributes
to this goal by speeding up the feature selection process and by
providing an accurate method to estimate the MER associated with
a particular feature subset, even before the inspection algorithm is
fully developed.
This paper is organized as follows: Section 2 gives a summary of
the literature review. Section 3 presents the new methodology for
the selection of features including the new method to estimate the
MER. In Section 4, the experimental results are presented. Section 5
presents the conclusions of this research and the recommendations
for future research followed by the list of references.

2

PROBLEM BACKGROUND

One of the problems faced by the developer of an automated
inspection system is the issue of what features to use to inspect a
particular component or product. The optimal feature selection
process is a very important step of any development process
because it has an impact on the overall performance of the resulting
inspection algorithm. For instance, inspection features need to be
carefully chosen to avoid using redundant features that may cause
system instability and require additional costs and training effort.
In fact, reducing the number of features can be accomplished by
taking advantage of cross correlation or redundancies among the
features [6]. The problem of optimal feature selection is typically
defined as follows: “Given a set of p features, select a subset of
size m that leads to the smallest classification error” [7].
The topic of optimal feature selection has been addressed
extensively for applications other than automated inspection
systems, for instance, see [8], [9], [10], [11], and [12]. Furthermore,
a new area of application closely related to feature selection is
called sensor selection. Some papers that address this topic are [13],
[14], and [15]. The literature associated with feature selection for
classification purpose is vast. Some examples include [16], [17],
[18], and [19]. However, the existing feature selection methods
present serious shortcomings for their use in reconfigurable
environments. For instance, one shortcoming is the amount of
time required to determine the performance of the resulting
inspection algorithms. This procedure is usually very long, because
once the feature selection process is performed, it is necessary to
asses the performance of the resulting subset, usually using some
of the cross-validation techniques, to estimate the MER. For
example, the “leave-one-out” cross-validation method consists of
using n  1 observations or data points to compute the classification rule and then classifying the omitted observation. This
procedure is repeated for each observation to estimate the MER.
A second problem is that the evaluation criteria used during the
feature selection process do not always correspond to the metrics or
statistics used to assess the performance of the resulting inspection
algorithm. For instance, when metrics such as Wilks Lambda are
used to guide the feature selection process, the objective is to
optimize the value of the metric. However, the optimization of such
indirect metrics does not guarantee the selection of a subset of
features that renders the smallest MER in the assembly line [17].
Thus, it is important to use a metric that would give a direct measure
of the expected MER during the selection of the inspection features.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

This paper addresses this gap by proposing a metric, and the
method to compute it, to make the use of stepwise feature selection
methods significantly more efficient and accurate in the development and adaptation of inspection and classification algorithms.
The proposed method is based on an indirect estimation of the
MER that is used to guide the feature selection process. That is, the
MER is estimated through a distribution function instead of relying
on direct observations. The advantage of this approach is the
indirect computation of the MER from the multivariate distributions of the components’ features. The proposed method is
applicable when a Quadratic Discriminant Function (QDF) is used
as the algorithm to discriminate between the classes of components, or in a broader context, between two populations. The
proposed method of calculating MER consists of using conditional
distribution functions of the QDF (to be presented in (3) and (4)),
but, instead of calculating MER using simulation as proposed in [3]
or by doing cross-validation, as is commonly done, an approximation of the distribution, based on the normal distribution, is used.
Once this MER is obtained, it is used to guide the search for an
optimal feature subset.
Because of the inherent complexity of feature selection
problems, heuristics methods such as stepwise selection of
variables have been developed. A typical procedure for this
heuristic method is as follows:
Start with an empty subset and a list of potential feature
candidates to be included into this subset.
2. Add a feature to the subset from the list of available
features that maximizes (minimizes) a given evaluation
criterion.
3. Calculate the marginal contribution to the evaluation
criterion for each remaining potential feature in the list.
This marginal contribution is calculated over the feature(s)
already in the subset.
4. Select the feature that in combination with the included
feature(s) maximizes (minimizes) the evaluation criterion.
5. Include the selected feature into the subset if it meets the
constraints.
6. Reexamine the marginal contribution for each feature in
the subset. If a feature does not meet the constraints, it is
removed from the subset.
7. Repeat steps 3-6 until no more features can be added into
or removed from the subset.
For additional details about the previous procedure, the
reader is referred to [20], [21], and [22]. The previous
description of the stepwise methodology relied on an evaluation
criterion to guide the inclusion and exclusion of features. One
issue is which evaluation criterion to use in the heuristic
method. There are numerous criteria to conduct the feature
selection process. These include: Wilks’ Lambda, Unexplained
Variance, Mahalanobis Distance, Smallest F Ratio, and Rao’s V.
The reader is referred to [23] and [24] for more computational
details of these evaluation criteria.
For example, Wilks’ Lambda method is one of the most
commonly used in practice [25]. This method is based on Wilks’
-criterion, which is a ratio expressing the total variability not
explained by the population differences [24]. This criterion is
1.

¼

jWj
;
jB þ Wj

ð1Þ

where jWj is the determinant of the estimated generalized
variance within the two populations, B represents the variation
between populations, and jB þ Wj is the determinant of the
estimated generalized total variance of the training data. The
Lambda value is between 0    1, and larger values indicate a

VOL. 31, NO. 7,

JULY 2009

1339

poor separation between populations, while smaller values denote
good separation between populations.
Although these methods are the most commonly used to
conduct the selection of the features, they exhibit an important
disadvantage. That is, they do not provide a good indication of the
expected misclassification results of the underlying feature/
variable subset. That is, the principal assumption is that optimizing
the evaluation criterion results in the minimization of MER.
However, this assumption is not always true. For instance,
heuristic methods directly using the MER as the evaluation
criterion (i.e., the classifier is the evaluation function) render
superior performance than the other criteria, but they also tend to
be the most computationally expensive because every new subset
that is explored requires the acquisition of actual misclassification
data [17]. Thus, using the MER as an evaluation criterion for a large
number of features is not practical.
Ideally, we would like to have a method that is accurate, as
given by applying the MER evaluation criteria, and computationally efficient. This is the objective of the feature selection procedure
proposed in this paper. The proposed method seeks to estimate the
MER based on the stochastic representation of the conditional
distribution of the quadratic classifier. These misclassification rates
are estimated through the use of a “plug-in” QDF originally
proposed in [3], later expanded in [4], and modified to fit our
needs in this paper and [26]. A brief explanation of these works is
presented in the next section.

2.1

Stochastic Representation of the “Plug-In”
Quadratic Discriminant Function

Equation (2) presents what is called the “Plug-in” QDF for the
observation x. It is the representation of the QDF when the mean
vectors 1 and 2 , and the covariance matrices 1 and 2 of the
two competing normal populations (1 and 2 ) involved in the
classification are unknown and replaced by their unbiased
1 , x
1 , S1 , and S2 [27] and [28]:
estimators, x

 0 1
 1
1 
1
02 S1
 1 S1  x
 x0 S1
1  S2 x þ x
2 x
C
B 2
^¼B
C:
Q
A
@ 1 jS1 j 1 

0 1
0 1
 2 S2 x
1  x
2
 S x
 x
 log
2
2 1 1
jS2 j
0

ð2Þ

^ is a random variable, in theory, the appropriate
Since Q
distribution could be derived to determine misclassification
probabilities for the observation x. However, since x is also a
random variable that follows a particular normal distribution, the
^ is too complex to be obtained directly.
exact distribution of Q
McFarland and Richard [3] addressed this problem by deriving
instead approximations to the conditional distributions of the
^ i.e., PfQ
^  kjx 2 1 g and PfQ
^ < kjx 2 2 g to get the
variable Q,
expected probability of misclassifying an observation to one of the
two competing populations of multivariate normal distributions.
Through the study of its characteristic function, these authors
^ can be expressed as a
showed that the conditional distribution of Q
series of independent univariate random variables, which they
^ They used this result to
called the stochastic representation of Q.
^ through the use of Monte Carlo
estimate the distribution of Q
simulations based on the individual components of the stochastic
representation. In order to present this stochastic representation, it
is necessary to introduce some terminology.
Let H be an orthogonal matrix that diagonalizes the matrix
12
1
1
1
2 1 2 2 such that 2 2 1 2 2 ¼ HH0 , where  ¼ diagð1 ;
2 ; . . . ; p Þ is a diagonal matrix. The entities 1 ; 2 ; . . . ; p on the
column
main diagonal are the eigenvalues of 1
2 1 . The auxiliary
1
1  2 Þ.
vector  ¼ ð1 ; 2 ; . . . ; p Þ0 is defined such that  ¼ H0 2 2 ð

1340

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Let k be the threshold value classifying x to belong to 1 or 2 .
Thus, the misclassification probabilities are defined as P ð2j1Þ ¼
^  kjx 2 1 g and P ð1j2Þ ¼ 1  PfQ
^  kjx 2 2 g, where
P  fQ
P ð2j1Þ is the probability of misclassifying an observation x into
2 when x belongs to 1 , and otherwise, is denoted by P ð1j2Þ. Let
Q1 and Q2 be the stochastic representations of P ð2j1Þ and P ð1j2Þ
defined by
1
0
p


2

1X 
2 1=2
2
Z2j þ j  1 Z1j C
2j !3j Z1j þ 1  !3j
B
C
B 2 j¼1
B
!C
Q1 ¼ B
C;




p1
C
B 1

 X
np T2
n2  j
A
@þ

log 1p
þ
F
 log1

log
1
j
2
2
n
n2 T1

j
1
j¼1
ð3Þ
0
B
B
B
Q2 ¼ B
B
@

1
p



2 
1X
2
2 1=2
~3j
~3j Z1j þ 1  !
Z2j þ ~j
~1 Z1j  ~2j !
C
2 j¼1
C
!C
C;
 p 


p1
C

 X
1
n T2
n2  j
A
þ
log 1p
F
 log1

log
þ
1
j
2
2
n1  j
n2 T1
j¼1
ð4Þ
2ni p

where, for i ¼ 1; 2 and j ¼ 1; . . . ; p, Ti ¼
is a chi-square
distribution with ni  p degrees of freedom, Zij ¼ Nð0; 1Þ is a
standard normal random variable, and Fj is an F distribution with
ðn2  j; n1  jÞ degrees of freedom
1 ¼

ðn1  1Þðn1 þ 1Þ
;
ðn1 T1 Þ

2j ¼

ðn2  1Þðj þ n1
2 Þ
;
T2

ðn2  1Þðn2 þ 1Þ
;
ðn2 T2 Þ
1
ðn1  1Þð1
j þ n1 Þ
~2j ¼
:
T1
~1 ¼

3

VOL. 31,

NO. 7, JULY 2009

DESCRIPTION OF THE FEATURE
SELECTION METHODOLOGY

The proposed methodology, in this paper called the Approximation Method (AM), is based on the indirect estimation of the MER
as the evaluation criterion in the stepwise feature selection method.
That is, MER is approximated by multivariate normal distribution
functions of Q1 and Q2 instead of relying on Monte Carlo
simulation experiments. The outline of the proposed method is
as follows:
Based on sample information, calculate the means
and variances of the stochastic representations, Q1
and Q2 .
2. Once the estimates of the parameters of the stochastic
representations are available, the calculation of MER is
based on a normal approximation of the stochastic
distributions.
These steps are explained in the following two sections.
1.

3.1

Mean and Variance Estimations

Q1 is assumed to follow normal distributions whose mean and
variance are given by (5) and (6):
11
0
0
n1 n2 j þ ðn1 þ n2 j þ 1Þ
p B
CC
B 1 ðn  1Þ X
n2 ðn1 þ 1Þ
CC
B
2
B
CC
B
B
AC
B 2 ðn2  p  2Þ j¼1 @
ðn

p

2Þðn

1Þðn
þ
1Þ
2
1
1
2
C
B
þ

j
C
B
ðn

p

2Þðn

1Þn
1
2
1
C
B

1C;
0 
 n  p
 n  p
Q1 ¼ B
C
B
n1  1 p
1
2
C
B

CC
B log n2  1 þ
B
2
2
CC
B
B
C
B þ1B


 C C
p
p1  
C
B
X
2B
n2  j
n1  j
AA
@ X
@


log j þ
2
2
j¼1
j¼1

The remaining random variables and constants are defined as


n1 n2 j
ðn1 þ 1Þðj n2 þ 1Þ

 1
1 2
;
j ¼ j j þ
n2

!3j ¼

12



;

n1 n2
ðn2 þ 1Þðj þ n1 Þ

 1
j 2
~j ¼ j 1 þ
:
n1

~3j ¼
!

12
;

Once Q1 and Q2 are available, then the calculation of the
misclassification probabilities can be performed through the use of
Monte Carlo simulation based on each one of the independent
stochastic components. However, in order to estimate these
probabilities, a large number of time-consuming simulations runs
need to be performed.
The results obtained in [3] were extended in [4] to find an
analytic method to produce a misclassification error curve as a
function of the number of features in the classifier so that the
optimal number of features could be determined. Two shortcomings of the work in [4] are that it assumed that all of the
features had normal distributions with identical parameters and
that samples of the same sizes (n1 ¼ n2 ) were drawn from each
population to estimate these parameters. These two shortcomings
were addressed by [26]. This work considered the case of
populations that had different parameters and unequal sample
sizes to accommodate the inspection and classification situations
in which there is a difference in number of samples available
from the targeted populations. The results of this latter work are
the basis for the feature selection methodology presented in the
next section. In particular, the proposed method extends the work
in [4] to directly estimate the MER associated with a particular
feature subset.

ð5Þ
0

ðn2  1Þ2

6
X

1

k þ c C
B
C n1 > p þ 2;
B ðn2  p  2Þ2 ðn2  p  4Þ k¼1
2Q1 ¼ B




 C
C; n > p þ 4;
B
p
X
n2  j
A 2
@ 1
0 n1  j
þ 0
þ
4 j¼1
2
2

ð6Þ

0
where
ðxÞ is the Digamma function and
ðxÞ is the
derivative of ðxÞ. The parameters k and c are defined as
follows:
!

2
p
p X
p
X
X
1
n1
1 ¼
ðn2  p  1Þ
2j þ
i j ;
2 n1 þ 1
j¼1
i¼1 j¼1

1
p
X
2
ðn

p

1Þ
ðn
þ
n

þ
1Þ
1
2 j
C

2 B 2
C
B
j¼1
1
1
C;
B
2 ¼
C
B
p
p
X
X
2 n2 ðn1 þ 1Þ @
A
þ
ðn1 þ n2 i þ 1Þðn1 þ n2 j þ 1Þ
0

i¼1 j¼1

1
p
X
j ðn1 þ n2 j þ 1Þ C
B ðn2  p  1Þ
C
B
j¼1
n1
1
C;
B
3 ¼
p
p
C
n2 ðn1 þ 1Þ2 B
A
@ XX
þ
j ðn1 þ n2 i þ 1Þ
0

i¼1 j¼1

!
p
p X
p
X
X
n1
2
2
4 ¼
ðn2  p  1Þ
j j þ
i j ;
n1 þ 1
j¼1
i¼1 j¼1

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31, NO. 7,

1
p
X
2j ðn1 þ n2 j þ 1Þ C
B ðn2  p  1Þ
C
B
j¼1
1
C;
B
5 ¼
p
p
C
n2 ðn1 þ 1Þ B
A
@ XX 2
þ
i ðn1 þ n2 j þ 1Þ

JULY 2009

1341

0

TABLE 1
Wilks Lambda Steps with Correlation 0

i¼1 j¼1

0

p
X

!2

ðn1  1Þðn2  p  2Þ
þ
ðn2  1Þðn1  p  2Þ

1

C
B
2j
C
B
C
B
j¼1
C
B 0
1
p
C
B
X
C
B

p

4Þ

ðn
6 ¼ B B
2
j
C C;
C
B B
C
i¼1
B B
CC
B B
!
CC

C
B B
pðn1  1Þ2 ðn2  p  2Þðn2  p  4Þ
n1 þ 1 2 C
@ @
AA
þ
2ðn2  1Þðn1  p  2Þðn1  p  4Þ
n1
1
0 0

1
p 
ðn2  1Þ X
1
2

þ

þ
C
B B
j
j
C
C
B B 2ðn2  p  2Þ j¼1
n2
C
C
B B
C
C
B B
C


C
B B 0
1


C
n

p
2
p
C
B B
C
log 2n1 þ
C
B B
C
2
C
C
B B B
C
C
C
B B B
C
C
C
B B B
C
 p C
C
B B B
2
C
C
C
B B B 

log
2n
C
2
C
C
B B B ðn2  p  2Þ
C
C
C
B B B
C
C
C
B B B
C
A
C
B B @
n  p
C
C
B B
1
C
C
B B
þ
C
C
B B
2
C
C
B B
C


C
B B
C
C
B B
pðn1  1Þ
n1 þ 1
C
C
B Bþ
C
C
B B 2ðn1  p  2Þ
n
C
1
C
B B
C
c¼B B 0
C:
1C
 n  p
C
B B
C
 p
2
1
C
B B
C

þ
log
2n
C
B B B
2
C
2
ðn1  p  2Þ C
C
B B B
CC
C
B B B
C
C
C
B @ @
A
A


C
B
 p
n

p
2
C
B
 log 2n1 þ
C
B
2
C
B
B 0
11C
0
C
B
n1 n2 j þ ðn1 þ n2 j þ 1Þ
2
B B
þ j C C C
BX
C
B B
p
n
ðn
þ
1Þ
2
1
CCC
B B ðn2  1Þ B
CCC
B
B B
CCC
B B 2ðn2  p  2Þ B
@ j¼1 ðn2  p  2Þðn1  1Þðn1 þ 1Þ A C C
B B
CC
B B

CC
B B
ðn1  p  2Þðn2  1Þn1
CC
B B
CC
B B 

p
CC
B @




AA
@
n1  1
n1  p
n2  p
log

þ
n2  1
2
2
The derivation and computational details of the previous
equations are presented in [26]. Although the parameters of Q2
can be estimated in a similar way to Q1 , a more efficient method
originally proposed by Hua et al. [4], consists of representing Q2 as
a function of Q1 through the following transformation:
Q2 ð
;  Þ  Q1 ð
1=2  ; 1 Þ:

ð7Þ

Therefore, to obtain the mean and variance for Q2 using this
transformation,
it is necessary to replace the values of j and j by
1
j 2 j and 1
j , respectively, in (5) and (6).
Once the mean and variance equations are developed for the
general case of unequal sample size, the next step in our
proposed methodology is to approximate Q1 and Q2 with normal
distributions.

3.2

Approximation of the MER

The approximation of MER is based on the assumption that
the distributions of the random variables, Q1 and Q2 ,
follow normal distributions as NðQ1 ; 2Q1 Þ and NðQ2 ; 2Q2 Þ.
Thus, the proposed equation to approximate MER is
!
!
k  Q1
k  Q2
~ 
~
MER ¼ 
ð8Þ
p1 þ 
p2 ;
Q1
Q 2

R 1 u2 =2
~
e
du is the upper tail area of the standard
where ðxÞ
¼ p1ﬃﬃﬃﬃ
2 x
normal distribution and k is defined as follows: k ¼
logðp2 Cð1j2Þ=p1 Cð2j1ÞÞ, where p1 and p2 are known as a priori
probabilities for 1 and 2 . Further, Cð2j1Þ denotes the cost of
misclassifying x into 2 and the other misclassification cost is
denoted by Cð1j2Þ.
The results provided by the approximation method were
validated by using simulation data sets. The reader is referred to
[26] for a detailed discussion of the validation results. The next
section provides the results of the feature selection examples
using MER as the evaluation criterion in the stepwise feature
selection method.

4

EXPERIMENTAL RESULTS

The performance of the proposed evaluation criterion is compared against some of the most common existing evaluation
criteria [23] and [24]. These evaluation criteria are: Wilk’s
Lambda, Unexplained Variance, Mahalanobis Distance, Smallest
F Ratio, and Rao’s V. The five methods selected the same feature
subset in all our experiments; thus, these methods are grouped in
single class and referred to as Conventional Methods (CM). The
first part of this section gives one example to validate using
simulation data, while the second part presents one example
using real inspection data.

4.1

Simulated Data

This experiment was performed using 1,000 simulated pseudorandom data points for two multivariate normal distributions
with 15 features (A to O) for two populations: 1 and 2 . The
mean vector for 1 was set to 1 ¼ 1;000 and mean vector for 2
to  2 ¼ 0. The diagonals of the covariance matrices for 1 and
2 were set to 1j ¼ 2j ¼ 400 þ 100j, for j ¼ 1 to 15. Five data
sets were generated using different levels of correlation between
the features. The levels of correlation used were 0.0, 0.25, 0.5,
0.75, and 0.99.
One example sequence of the feature selection process (with the
level of correlation equal to 0) is presented in Table 1. This table
also presents, in the third column, the values of the Wilks Lambda
associated with each subset. The last column is the value of MER
for each subset computed using the Approximation Method
(MER_AM).
From Table 1, it can be seen that one of the disadvantages of the
conventional methods is that the statistic used as the main
evaluation criteria does not convey meaningful absolute information about the resulting MER. For example, when the Wilks
Lambda statistic is used as the evaluation criterion, the values of

1342

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. 7, JULY 2009

TABLE 2
Summary of the Feature Selection Methods

this statistic do not provide information in terms of the expected
MER. Therefore, the users of the Conventional Methods do not
obtain information to interpret the values of the evaluation
criterion in terms of MER. This disadvantage is eliminated when
using the MER estimated by the proposed approximation method.
The summary of the feature selection experiments using the five
different levels of correlation and three methods to estimate MER is
presented in Table 2. The methods are the Conventional Methods,
the Approximation Method, and the Resubstitution Method. The
first column presents the level of correlation. In order to obtain the
different number of subsets, the MER for the 32,767 subsets was
calculated by using the resubstitution method. The number of
different subsets based on the MER is presented in the second
column for each correlation of the section Data. The MER of the
full subset (using the 15 features) and the position (rank) of this
subset with respect to the optimal subset based on the MER
obtained for each level of correlation are presented under the
section labeled “Full.” The summary of the feature selection
methods is presented in three sections in this table. These
sections are called: Conventional, Approximation, and Resubstitution. The selected subset of features is presented in the first
column (Subset) of each section. The second column (MER) of
each section presents the corresponding MER using the
resubstitution method. The next column (Size) shows the
number of features in the selected subset. The final column
(Rank) of each section gives the position of this subset with
respect to the optimal subset of features based on the MER.
Additionally, the last section of the table under the heading
“Optimal” shows the optimal subset with its corresponding MER
and a number of features. For the last two rows of the optimal
subset column, there are multiple subsets with the target MER
equal to zero; the size for these examples represents the number of
features in the largest subset.
The previous results support the hypothesis that the approximation method renders similar performance to those of the
conventional methods. The values obtained in the three factors
analyzed on the previous experiment are very similar. However, it
is necessary to explaIn that, in the case of the approximation
method, no threshold value was used for the inclusion or
elimination of features. This means that any reduction of the
MER was enough to include a feature in the final subset. For
instance, see Table 3, which shows the progression of the feature
selection process for the example with correlation equal to 0. For
instance, this table presents the feature that was added in each step
of the feature selection process. The first column gives the step
number of the stepwise process. The second column shows the
subset for each step of the process. Columns 3 and 4 present MER

estimated from the Approximation Method (denoted as MER_AM)
and the Resubstitution Method (denoted as MER_R), respectively.
The fifth column shows the value differential between consecutive
estimates of MER.
As an example, consider step 4 of Table 3. Feature D was
added/included to the best subset of step 3 {A, B, C}. This feature
was added/included because it provided the maximum reduction
to MER_AM. The marginal reduction of the MER_AM from feature
D was 0:0804  0:0646 ¼ 0:0158. As was explained before, the
threshold used to include a feature was 0. Therefore, any reduction
in the MER_AM of feature being considered is enough to be
included in the best subset. This threshold was used to understand
the experimental behavior of the MER_AM versus the sample size
of the best subset. However, this approach may not be useful from
a practical point of view. For example, the reduction in MER_AM
from step 11 to step 12 in Table 3 is 0.000461. This reduction was
enough to include feature L although the reduction of MER_AM
was minimal. In practice, this reduction may not justify including
this feature in the resulting subset.
Given that the MER_AM statistic used in the feature selection
process is very similar to MER obtained using the resubstitution
method (MER_R), it can be used to determine a stopping point in
the feature selection progression according to the needs of the
users. The similarity between the MER_AM and the MER_R can be
analyzed in Table 3. For instance, the information provided in this
table can be used to determine the optimal number of features

TABLE 3
Feature Selection Summary

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31, NO. 7,

JULY 2009

1343

TABLE 4
Feature Selection Summary

Fig. 1. Histograms of the EAL features.

from the cost/benefit trade-off including features in the final
subset. For example, a user can determine that an error of 0.05 is
acceptable for the classification of the populations. Therefore, in
the first experiment, the feature selection process should be
stopped at step 6 with the features {A, B, C, D, O, E} instead of
step 12 with the features {A, B, C, D, O, E, N, F, G, M, H, L}
because, in step 6, the level of discrimination determined by the
user was achieved. Another example is provided by assuming that
the user cannot afford more than seven features due to the
inspection cost/time constraints; therefore, the process should stop
with the subset {A, B, C, D, O, E, N} with an MER AM ¼ 0:0438050
and the MER R ¼ 0:0435. It is important to note that this process of
setting a prespecified stopping MER could not be achieved by
using conventional methods alone.
Regarding the time the AM takes to estimate an MER, it is
significantly lower than the alternative of running simulations as
proposed by [3]. For instance, an experiment that consisted of
running 100,000 simulation points in a personal computer with an
Intel Core 2CPU T5600 at 1.83 GHz to estimate the MER took 190 sec,
while the same estimation based on the AM took 2 seconds.

4.2

Automated Visual Inspection Data

This example is applied to the inspection of Surface-Mounted
Devices (SMD) to determine if these electronic components are
present or absent on a Printed Circuit Board (PCB). The
experimental data used to validate the feature construction
methodology were collected in an experimental AVI system
available at the Electronics Assembly Laboratory (EAL) at Arizona
State University. For a description of the experimental setting, the
reader is referred to [29].
The experimental results are based on a single class of SMD.
There are 281 components on the PCB used in this experiment. The
AVI system inspected one PCB with all the SMD components
present and another PCB with each one of the components absent.
Therefore, there were 281 elements for the training phase for each
population. Six inspection features were utilized in this analysis.
These features are called Energy (E), Correlation (C), Diffusion (D),
Fenergy (F), Texture (T), and Blob (B). The details of these features
were discussed in [29]. In order to conduct more feature selection
experiments, an assumption was made that the full data set was
composed of only five features instead of six. This assumption
gives six extra combinations of feature sets that can be analyzed as
separate experiments. For example, the full set of features is
labeled {ECDFBT}, under the new assumption, and the six new

feature sets are: {ECDFB}, {ECDFT}, {ECDBT}, {ECFBT}, {EDFBT},
and {CDFBT}.
The features used were selected to highlight the application of
the selection methodology and not for their discrimination power.
Fig. 1 depicts, with histograms, the discrimination provided by the
features to highlight the degree of separation between the Present
(1 ) and Absent (2 ) populations of components.
In order to evaluate the performance of the new evaluation
criterion, its performance is compared again to the Conventional
Methods (CM).

4.3

Feature Selection Summary

The summary of the seven experiments with two methods to
estimate MER is presented in Table 4. The first column
presents the number of features. The total number of different
subsets using the original number of features in each experiment is presented in the second column (Sets) of the section
Data. The MER of the full subset and the position of this
subset with respect to the optimal subset for each combination
of features are presented under the section labeled “Full.”
The results of the feature selection process using the two
methods to estimate MER are presented in the sections of the table
under the headings “Conventional” and “Approximation,” respectively. The selected subset of features is presented in the first
column of each section. The second column of each section
presents the number of features in the selected subset. The next
column shows the change of the position of this subset with respect
to the optimal subset of features based on MER using the
resubstitution method.
The previous results support the hypothesis that the approximation method renders similar or better performance as compared to those provided by conventional methods.

5

CONCLUSIONS

The main contribution of this research was the design of a new
feature selection methodology based on the “plug-in” Quadratic
Discriminant Function. One of the key contributions of the paper is
to conduct the feature selection process through an estimate of the
misclassification error rate obtained from the densities of the
stochastic representations of the conditional quadratic discriminant function.
The application of the proposed methodology will result in
significant savings of computational time in the estimation of MER
over the traditional simulation and cross-validation methods. This
will significantly shorten the time needed to reconfigure a
preexisting inspection system, a key feature for the emergence of
self-reconfigurable inspection systems; systems that will be better
capable to deal with the rapid introduction and retirement of
products.
The proposed methodology for feature selection was developed
assuming that the density functions of the stochastic representations of the conditional probabilities of the “plug-in” QDF follow
normal distributions. While this assumption was numerically
supported in this paper, it is necessary to further validate it.

1344

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Another interesting topic for future research is to consider the
strategy to include or exclude more than one feature at a time.
Also, addressing marginal feature inclusion from an economical
point of view is an interesting area of future research. For example,
it could be useful to include economical factors such as the cost
and time required for including/excluding preexisting features.
This is something that is also left for future research.

ACKNOWLEDGMENTS
The authors would like to acknowledge the support provided by
the US National Science Foundation through grant DMI-0300361
for the realization of this research. They would also like to
acknowledge the helpful suggestions of the anonymous reviewers
of this paper.

REFERENCES
[1]
[2]
[3]

[4]

[5]

[6]
[7]

[8]

[9]

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]
[21]
[22]

[23]

R. Duda, P. Hart, and D. Stork, Pattern Classification, second ed. John Wiley
& Sons, 2003.
L. Devroye, L. Gyorfi, and G. Lugosi, A Probabilistic Theory of Pattern
Recognition. Springer, 1996.
R. McFarland and D. Richard, “Exact Misclassification Probabilities for
Plug-In Normal Quadratic Discriminant Function—The Heterogeneous
Case,” J. Multivariate Analysis, vol. 82, pp. 299-330, 2002.
J. Hua, Z. Xiong, and E. Dougherty, “Determination of the Optimal
Number of Features for Quadratic Discriminant Analysis via the Normal
Approximation to the Discriminant Distribution,” Pattern Recognition,
vol. 38, pp. 403-421, 2005.
H.C. Garcia and J.R Villalobos, “Development of a Methodological
Framework for the Self Reconfiguration of Automated Visual Inspection
Systems,” Proc. Fifth Int’l Conf. Industrial Informatics, July 2007.
S. Kachigan, Multivariate Statistical Analysis. Radius Press, 1982.
A. Jain, R. Duin, and J. Mao, “Statistical Pattern Recognition: A Review,”
IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 1, pp. 4-37,
Jan. 2000.
J.A. Zongker, “Feature Selection: Evaluation, Application, and Small
Sample Performance,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 19, no. 2, pp. 153-158, Feb. 1997.
P. Mitra, C. Murthy, and S. Pal, “Unsupervised Feature Selection Using
Feature Similarity,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 24, no. 3, pp. 285-297, Mar. 2002.
Y. Saeys, I. Inza, and P. Larranaga, “A Review of Feature Selection
Techniques in Bioinformatics,” Bioinformatics, vol. 23, no. 19, pp. 2507-2517,
2007.
P. Liu, N. Wu, and J. Zhu, “A Unified Strategy of Feature Selection,” Proc.
Int’l Conf. Advanced Data Mining and Applications, pp. 457-464, 2006.
H. Wei and S. Billings, “Feature Subset Selection and Ranking for Data
Dimensionality Reduction,” IEEE Trans. Pattern Analysis and Machine
Intelligence, vol. 29, no. 1, pp. 162-166, Jan. 2007.
S. Sampatraj, K. Abhishek, and Y. Ding, “A Survey of Inspection Strategy
and Sensor Distribution Studies in Discrete-Part Manufacturing Processes,”
IIE Trans., vol. 38, no. 4, pp. 309-328, 2005.
I. Gunyon, S. Gunn, M. Nikravesh, and L. Zadeh, “Variable-Feature
Selection and Ensemble Learning: A Dual View,” Feature Extraction,
Foundations and Applications, I. Guyon, L. Zadeh, and M. Nikravesh, eds.,
Springer-Verlag, 2005.
S. Jiang, R. Kumar, and H. Garcia, “Optimal Sensor Selection for DiscreteEvent Systems with Partial Observation,” IEEE Trans. Automatic Control,
vol. 48, no. 3, pp. 369-381, Mar. 2003.
H.C. Garcia, J.R. Villalobos, and G. Runger, “Automated Feature Selection
for Visual Inspection Systems,” IEEE Trans. Automation Science and Eng.,
vol. 3, no. 4, pp. 394-406, Oct. 2006.
H. Liu and L. Yu, “Toward Integrating Feature Selection Algorithms for
Classification and Clustering,” IEEE Trans. Knowledge and Data Eng., vol. 17,
no. 4, pp. 491-502, Apr. 2005.
N. Louw and S. Steel, “Variable Selection in Kernel Fisher Discriminant
Analysis by Means of Recursive Feature Elimination,” Computational
Statistics and Data Analysis, vol. 51, no. 3, pp. 2043-2055, 2006.
M. Ashihara and S. Abe, “Feature Selection Based on Kernel Discriminant
Analysis,” Proc. Int’l Conf. Artificial Neural Networks, Part 2, pp. 282-291,
2006.
C. Le, Applied Categorical Data Analysis. Wiley, 1998.
R. Khattree and D. Naik, Multivariate Data Reduction and Discrimination with
SAS Software. Wiley-Interscience, 2000.
C. Park, J. Koo, and P. Kim, “Stepwise Feature Selection Using Generalized
Logistic loss,” Computational Statistics and Data Analysis, vol. 52, no. 7,
pp. 3709-3718, 2008.
A. Rencher, “The Contribution of Individual Variables to Hotelling T2 ,
Wilks and R2 ,” Biometrics, vol. 49, pp. 479-489, 1993.

[24]
[25]
[26]

[27]
[28]

[29]

VOL. 31,

NO. 7, JULY 2009

R. Jenrich, “Stepwise Discriminant Analysis,” Statistical Methods for Digital
Computers, K. Enslein, ed., John Wiley & Sons, 1997.
A. Rencher, Methods of Multivariate Analysis, second ed. John Wiley & Sons,
2002.
H.C. Garcia, “A Framework for the Self Reconfiguration of Automated
Visual Inspection Systems,” PhD dissertation, Dept. of Industrial Eng.,
Arizona State Univ., 2008.
T.W. Anderson, An Introduction to Multivariate Statistical Analysis, third ed.
Wiley, 1984.
A. Wald, “On a Statistical Problem Arising in the Classification of an
Individual into One of Two Groups,” The Annals of Math. Statistics, vol. 15,
no. 2, pp. 145-162, 1944.
J.R. Villalobos, M. Arellano, A. Medina, and F. Aguirre, “Vector
Classification of SMD Images,” J. Manufacturing Systems, vol. 22, no. 4,
pp. 265-282, 2004.

. For more information on this or any other computing topic, please visit our
Digital Library at www.computer.org/publications/dlib.

This article was downloaded by: [149.169.132.70] On: 25 May 2017, At: 16:43
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA

INFORMS Transactions on Education
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org

The Stock Portfolio Game
J. Rene Villalobos,

To cite this article:
J. Rene Villalobos, (2007) The Stock Portfolio Game. INFORMS Transactions on Education 8(1):41-48. http://
dx.doi.org/10.1287/ited.8.1.41
Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions
This article may be used only for the purposes of research, teaching, and/or private study. Commercial use
or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher
approval, unless otherwise noted. For more information, contact permissions@informs.org.
The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness
for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or
inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or
support of claims made of that product, publication, or service.
Copyright © 2007, INFORMS
Please scroll down for article—it is on subsequent pages

INFORMS is the largest professional society in the world for professionals in the fields of operations research, management
science, and analytics.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org

VILLALOBOS
The Stock Portfolio Game

The Stock Portfolio Game
J. Rene Villalobos

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

Arizona State University
Department of Industrial Engineering
PO Box 875906 Tempe, AZ 85287-5906
rene.villalobos@asu.edu

Abstract
One of the objectives of the stock portfolio game is to introduce undergraduate engineering students to the
application of optimization techniques for stock portfolio selection. Ideally students playing the game would
have basic background in linear algebra, calculus, and statistics. The general idea of the game is to present
teams of students with real historical performance data for stocks with different levels of returns and variability
so that they make decisions concerning the allocation of limited funds among these stocks. The game is played
in four rounds within an hour-long session. The teams, comprised of no more than four students, are provided
with an initial virtual investment fund, which they have to allocate or reallocate based on the information presented. The team with the most money in its investment account at the end of the game is declared the winner.
Once the results of the game are known, one or more teams are asked to share their investment strategy with
the rest of the group. This discussion forms the starting point for the last part of the session when the lecturer
puts the investment game in the perspective of an optimization problem. At the end of the session basic principles
of Markowitz's portfolio theory are presented and applied to the data given to the students.
Editor's note: This is a pdf copy of an html document which resides at http://ite.pubs.informs.org/Vo8No1/
Villalobos/ (Volume 8, Number 1, September 2007)
presented in this paper is to introduce undergraduate
students to some of the issues and tools of financial
engineering to make them aware of financial engineering related career paths. Another objective of the game
is to introduce these students to applications of optimization techniques to financial investment problems.

1. Introduction
The use of optimization techniques in financial applications is not new. For instance, Markowitz developed
his now famous portfolio optimization theory in the
1950s (Markowitz, 1952). However, most of the development in the areas of financial optimization remained
generally within economics and finance. Nevertheless,
in the last fifteen years the operations research (OR)
and industrial engineering communities have begun
to develop and use decision-support tools for financial
applications such as portfolio selection and risk management. This has been driven, in part, by increasing
numbers of graduates entering the financial sector
from OR and industrial engineering programs. In fact,
entire programs dedicated to financial engineering or
financial mathematics have been created throughout
the world; currently there are more than 30 such programs in the United States (International Association
of Financial Engineers(1), 2007). However, the majority
of these programs are offered only at the graduate
level. Thus, one of the main objectives of the game
(1)

Evidence of the benefits in using games and simulation
approaches to business and other fields has been extensively discussed in the literature. For instance, Lean
et al. (2006) present an overview of the current practices of using games in higher education and what
they perceive as the barriers to the adoption of games
in the classroom. Wolfe and Crookal (1998) present a
brief review of the history of simulation/gaming and
discuss the maturity and the challenges of the field.
Although the practice of teaching games has been
adopted in practically every field of higher education,
game applications in management have a long and
unique history. For instance, Faria and Wellington
(2004) claim that the first use of business-related games
can be traced back to China as far as 5,000 years ago;

http://www.iafe.org/resources_acad.html

INFORMS Transactions on Education 8:1(41-48)

41

© INFORMS ISSN: 1532-0545

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

VILLALOBOS
The Stock Portfolio Game

business simulation games can be traced back to at
least 50 years. Based on surveys, Faria and Wellington
found that the use of simulation games in business
schools is widespread with more than 95% of these
schools using some type of business simulation games
at the time of their study. Surprisingly, however,
among the different disciplines of the business schools,
finance reported one of the lowest utilization rates of
business simulation games.

techniques. In Section 2, the game and its rules are
presented; in this section, the different phases of the
game are discussed, and the material used to play the
game is also introduced. Section 3 gives general recommendations about the organization and what the students will need in order to play the game. Section 4
discusses the stock data set used to play the game and
also gives recommendations related to alternative stock
data. Section 5 presents some results and lessons
learned in using the stock portfolio game. Finally,
Section 6 presents some conclusions.

Regarding the commercial market for business simulation games, Summers (2004) reports that the annual
revenue of the business simulation gaming industry
was approximately $500 million at the time of the
study and has considerable growth potential. There
are also several commercially available stock market
simulators. The characteristics, use, and pedagogical
benefits of these games have been reported in literature
as well (Wood et. al, 1992, Maier, 2002). A classic example of these commercial simulators is the National
SMS®, a trademark of Stock-Trak, Inc. (2007). In this
game the user is given a $100,000 virtual account to
invest over several weeks using real stock market information. Students compete against other students
based on the performance of their investments. This
simulator is particularly popular among high schools
in the United States.

2. The Game
The Stock Market Game introduces financial engineering and mathematics students to models used to support financial investment decisions. Although the game
has been played at the graduate level, it is assumed
that the players are sophomore or junior level students
who have taken basic courses in statistics and calculus.
The minimum knowledge required to play the game
includes the ability to compute average and standard
deviations as well as the ability to obtain simple
derivatives of quadratic functions.
The game is designed to be played during a 1-hour
class session, and it is recommended that each session
be followed by an in-depth discussion of the optimization problems derived from the game. The session is
divided into three phases: introducing the game to
students, playing the game, and putting the stock
portfolio formation in the context of an optimization
problem. A general explanation of the game is given
next.

The Stock Market Game  , sponsored by the Foundation for Investor Education (2007), is perhaps the most
popular stock simulator game and has been used by
millions of students (Maier, 2002). Common characteristics of these games include that they are played over
several weeks and that they aim to introduce the participants to stock picking strategies. Unlike stock simulators, the Stock Market Game is meant to be played
in a one-hour long session and its main objective is to
introduce students to the decision making process
behind investment strategies rather than to develop a
successful investment strategy.

The game begins when a virtual investment account
with a balance of $100,000 is created for each team of
students. See Section 3 for recommendations on how
the teams can be organized. These funds are considered "loans" that must be repaid, along with accrued
interest, during the game in two installments of $55,000
each. The funds in the virtual account must be invested
in a combination of seven stocks (A through G) as well
as a "risk-free" investment that yields an interest of 2%
per game period. The total balance in the investment
account can be invested among the seven stocks and
the risk-free investment in any way the team decides.
Throughout the game, each team will make three investment decisions: one initial allocation of funds and
two reallocations. Before every allocation/reallocation

The stock game to be presented was developed to be
played as part of an undergraduate engineering
economy course as an introduction to the principles
of portfolio selection and optimization. The game has
also been used to showcase potential graduate students
some of the non-traditional fields in which industrial
engineers currently work.
In the remainder of this paper we discuss in detail this
game and its use to introduce portfolio optimization
INFORMS Transactions on Education 8:1(41-48)

42

© INFORMS ISSN: 1532-0545

VILLALOBOS
The Stock Portfolio Game

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

decision, the teams are given historical information
on the adjusted daily closing price of the seven stocks
for the previous several months. Thus, the value of
the investment account is determined by the total

funds allocated to each stock in the previous decision
point (DP) and the performance of the stocks since
then. Figure 1 shows the timeline corresponding to
the DPs of the stock game.

Figure 1: Decision Points Timeline
In DPs two and three, the teams need to make an installment payment of $55,000. If the value of the investment account for a team is less than $55,000 that team
is declared "bankrupt" and not allowed to continue
playing the game. Due to the selection of the stocks,
it is impossible for the teams to become bankrupt at
the time of the first reallocation (DP2). However, it is
common for one or two teams to be unable to make
the second installment payment (DP3), thus resulting
in bankruptcy. At the discretion of the instructor,
bankrupt teams can continue playing by borrowing
additional funds or by joining some of the other teams
still in the competition.

is given at the beginning of the session or during a
prior session. As part of the introduction, a graph with
the adjusted closing prices of two stocks is presented
to make observations on the erratic behavior of the
stock prices and the difficulty of predicting the performance of the stocks.
The next part of the session consists of describing the
game to the students and discussing the rules, which
are then illustrated by playing a reduced version of
the game involving two investment periods and two
stocks. The student version of the PowerPoint file used
in this part of the session can be downloaded from J.
Rene Villalobos's Stock Game website(2).

After the students are given the rules, they may use
any tools they consider appropriate to make the best
possible investment decisions. No formal objective is
given to the students to force debate among the
members of the team on which objective is best to
pursue. For instance, an obvious objective is to maximize profits; a less obvious objective is to minimize
the probability of bankruptcy.

2.2. Second Phase: Playing the Game
In the second part of the session the actual game is
played. Before making the first investment decision
the teams are provided with information reflecting
the adjusted daily closing prices of the seven stocks
for approximately 2½ years of actual trading. The last
closing price is assumed to be the current price of each
stock. This means that the teams can buy a share of a
stock at the last closing price. It is also assumed that
no commission is charged on any of the transactions.
Figure 2 presents the information given to the students.
This information is based on the performance of real
stocks with different levels of average returns and
volatility. Once the information is available, the teams
have approximately seven minutes to make the allocation of funds among the different investment opportunities. This allocation takes the form of percentages of
the total investment fund that each team wants to allo-

2.1. First Phase: Organization and Introduction
to the Game
The first activity consists of organizing the students
into teams. See Section 3 for recommendations on how
the teams can be organized. Ideally, the students
would have elementary knowledge of stocks and stock
markets. However, to ensure that the students have
the information needed to play the game an explanation of stocks, stock markets, and stock closing prices

(2)

http://eal.asu.edu/stockgame

INFORMS Transactions on Education 8:1(41-48)

43

© INFORMS ISSN: 1532-0545

VILLALOBOS
The Stock Portfolio Game

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

cate to each investment option. Because shorting stocks
is not allowed, only non-negative percentages are reported. Using the last closing price given, the percentages are translated into shares of stocks to determine
the resulting investment portfolio. The purchase of
fractions of shares is allowed.

team from going bankrupt at this point; thus, the teams
will have approximately seven minutes to discuss investment strategies and turn in their new allocation
of funds (DP2). However, if a different set of stocks is
used and the result of bankruptcy is then possible, it
is up to the instructor's discretion to decide if bankrupt
teams can continue playing the game by borrowing
additional funds or joining with another team. It is
highly suggested that all students continue to participate in the game at this point, as the main objective of
the game is to keep students actively involved in the
decision making process.
In the third round, once again the teams are given
data on the performance of the different stocks for
approximately the next two years (Figure 4), the resulting value of the current portfolio is calculated, and the
second installment payment of $55,000 is deducted.
Those teams with negative account balances leave the
game; the rest of the teams are given seven minutes
to adjust their strategy and make the final investment
allocation (DP3). As shown in Figure 4, some of the
stocks register negative performance during the period. Thus, there is a high possibility that one or more
teams will be forced into bankruptcy because they are
unable too make the second payment. This is particularly true for investment portfolios that had large positions in Stocks B and C. The recommendations previously given for the bankrupt teams also apply this
time.

Figure 2: First data set
After the initial allocation, the teams will have two
opportunities to change the original investment before
the game ends.
Once the teams turn in their original investment allocation (DP1) the second round of the game starts. In
the second round the teams are presented with the
performance of each of the stocks for approximately
two additional years. This information is depicted in
Figure 3 below. Using the last stock closing the current
value of the portfolio is calculated. From this value
the first installment payment of $55,000 is deducted
to determine the current value of the investment account. The balance of the investment accounts is shared
with the rest of teams; this way, each team knows its
overall position in the game and can assess how effective their strategy is and whether it needs to be
changed.

Figure 4: Third data set
In the final round of the game the teams receive data
on the performance of the stocks for a last time period
(Figure 5). The value of the last closing price for the
stocks is used to compute the final value of the investment account for each team. The final value is used to
determine the ranking of the teams in terms of their
investment performance. The team with the highest

Figure 3: Second data set
As previously explained, the stock information that
has been used so far to play the game precludes any
INFORMS Transactions on Education 8:1(41-48)

44

© INFORMS ISSN: 1532-0545

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

VILLALOBOS
The Stock Portfolio Game

value investment account is the overall winner of the
game.

The semi annual average return and standard deviation are computed and presented to the students in
tabular form and in a mean standard deviation format
(Figure 7). This information introduces basic concepts
of efficient frontier for the selection of stocks and the
stock selection problem as Markowitz's

Figure 5: Last data set

Figure 7: Average returns and standard deviations for
the different stocks

2.3. Third Phase: The Game as an Optimization
Problem

stock optimization problem. The return on the portfolio concept is introduced using a 2-asset portfolio and
developing its expected return and standard deviation,
namely.

The objective of this phase is to put stock selection in
the context of an optimization problem. After the results of the game are announced the instructor asks
the winning team and an additional team, which has
used an innovative approach, to share with the rest of
the participants the strategies and tools, if any, used
to identify investment stocks. Although the strategies
presented may vary, the winning team usually cites
strategies based on investing in stocks offering the best
price improvement combined with stocks prices that
did not show significant losses over time. Many teams
compute some expected price change per period while
other teams compute the variance of the stocks prices.
The instructor will use the presentation as a starting
point for a discussion to make the connection between
higher returns and higher variability of the stocks. To
do this the instructor will unveil the overall data used
and the corresponding stocks and ticker symbols.
Figure 6 presents the closing price data for each stock
used, their corresponding ticker symbols, and the average daily prices.

where ρ represents the correlation between the two
assets, α represents the weight given to asset 2, and σ
represents the standard deviation of each of the assets.
Equation 1 highlights two issues: the reduced variability of a portfolio when compared to that of individual
assets and the existence of an optimal weight allocation
to minimize this standard deviation. Figure 8 shows
three assets in a mean standard deviation graph of
different daily returns and standard deviations as the
weights of the assets in a 2-asset portfolio are varied
from 0 to 1. This figure also introduces the topic of efficient frontier of the Markowitz portfolio theory.

Figure 8: 2-asset portfolio returns and standard deviations

Figure 6: Complete data set with proper ticker symbols
and chronology
INFORMS Transactions on Education 8:1(41-48)

45

© INFORMS ISSN: 1532-0545

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

VILLALOBOS
The Stock Portfolio Game

Finally, if there is enough time left in the class session,
the optimization problem is simplified so that it can
be solved by Lagrangian relaxation. The simplification
consists of allowing negative values (or shortage of
stocks) to set up the problem. If any negative value is
obtained, it is set to zero, thus rebalancing the portfolio
with the positive values. Although this is not strictly
valid, it is done to keep the formulation as simple as
possible. Otherwise the solution of a quadratic objective function would be required. This simplification
is explained to the students so that they are aware that
the model can be improved to better reflect the real
life problem.

more than five minutes should be spent in the organization of teams.
Different types of classroom infrastructure have been
used to give the data sets to the teams, to receive fund
allocation, and to communicate current results. The
instructor must have a personal computer for the instructor connected to a projector, and a computer
should be setup for each team. Each computer must
have access to an electronic worksheet such as Excel.
Although not strictly necessary, a computer-mediated
classroom enabled by instructional software such as
Blackboard is useful to communicate information between the instructor and the teams and vice versa.
However, if access to this type of infrastructure is not
available communication can also be done through
other file transfer mechanisms such as Flash memory
drives or floppy disks.

Once the problem is expressed as a Lagrangian relaxation problem, derivatives are taken to obtain a set of
linear equations that can be easily solved. The unknowns in the equations are the percentage to invest
in each of the stocks at each stage of the game. This
technique is applied to the stock information provided
to the students at each stage to obtain the portfolio
composition. The results so obtained are then compared to those obtained by the students. If there is no
class time remaining, a follow-up session can be held
to review the overall optimization. A PowerPoint
presentation including all the details of game, including the mathematical development, can be downloaded from J. Rene Villalobos's Stock Game website(3).

It is also recommended that a teaching assistant be
present in the classroom to receive teams allocations
and make computations on the current value of each
team's investment accounts. This leaves the instructor
free to interact with the students and conduct the
game.
Before the class begins, it is also important to prepare
forms that will be used for the teams to communicate
their allocations in each round of the game. A set of
sample forms can be downloaded from J. Rene Villalobos's Stock Game website(4).

3. Session Organization
To ensure an orderly and successful session, material
to be used should be prepared and organized ahead
of time according to the class size and classroom type.
We have played the game under different settings with
class sizes ranging from small (around 13 students) to
large (around 70 students) and with different classroom layouts and infrastructure. To ensure effective
participation of all students, we recommend that each
team be limited to no more than four students. For
small class sizes the membership in the teams can be
left to the students' discretion and determined at the
beginning of the class. For large class sizes it is recommended that teams be organized prior to the beginning
of the class. A strategy that has worked well in the
past is to form the teams according to seating assignments. For instance, four students seated side-by-side
in the same row would form a team. In any case no
(3)
(4)

4. Data Set Used and Other Relevant Information
The stock performance data set that we have used is
based on the adjusted closing prices for seven different
stocks from April 1996 to March 2003. This data set
was partitioned into four files and given to the students as the stock performance information for each
round. However, the data were slightly modified so
that the students did not know the actual time period
for that date. For instance, the first data set shows an
initial date of April 12, 2001, when in fact the real initial
date was April 12, 1996. This was done so that the
students were not able to guess in retrospect how the
market would perform in a particular time period.

http://eal.asu.edu/stockgame/
http://eal.asu.edu/stockgame/

INFORMS Transactions on Education 8:1(41-48)

46

© INFORMS ISSN: 1532-0545

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

VILLALOBOS
The Stock Portfolio Game

The data set chosen for this game corresponds to
stocks: American Airlines (AA), International Business
Machines (IBM), Intel (INTC), Johnson & Johnson
(JNJ), McDonald's (MCD), Yahoo (YHOO), and Tenet
Healthcare (THC). These stocks were selected to obtain
a diverse sample of stock average performance and
volatility. The names of the stocks were withheld from
the students to ensure they would base their selections
strictly on stock performance for the periods. The
stocks were identified by the letters A through G. At
the end of the game, the names of the stocks and the
timing information should be revealed to the students.
This will allow them to review their investments and
determine whether they would have changed their
decisions based on the new information given.

instructor should encourage discussion of these conflicting choices by commenting on the performance of
certain stocks. For instance, the instructor could mention the high level of volatility exhibited by the data
series corresponding to Yahoo!. Ideally, the level of
interest present in the game is maintained in the second part of the session when basic models for portfolio
formation are introduced.
Because the primary objectives of the game are to introduce students to the application of optimization
techniques for stock portfolio selection and to entice
them to consider pursuing a career in financial engineering, the success of the game should be judged by
the interest produced among the students rather than
by the performance of the stock selection strategies of
the teams. While a metric to measure this success is
difficult to establish, we have noted an increasing
number of engineering students (beyond those in industrial engineering) inquiring about advanced
courses dealing with portfolio optimization and financial modeling.

The data and all of the material used in the game can
be accessed on J. Rene Villalobos's Stock Game website.
The set of stocks and time periods used can be easily
changed if the game is used frequently or in different
sections of a class during the same semester. However,
it is recommended that stocks from different industry
segments be used to highlight the point that different
returns are expected for different levels of volatility.

The game is played under a strict time schedule. For
instance, if the game is played in a 50-minute session,
the introduction takes about 10 minutes, playing the
game takes about 28 minutes, and the discussion/modeling phase takes about 12 minutes. This time schedule
can be modified to meet the specific objectives of the
instructor. For instance, the current three DPs could
be reduced to two in order to allow additional time
for discussions amongst the teams or the discussion
of model building. Also, as previously mentioned, an
additional session could be held to discuss the details
of the models and to apply these models to the stock
information given to the teams. Students are usually
interested in the comparison between the performances
of the strategies used in the game against those resulting from applying the analytical models; a discussion
of this could also be done in an additional session.

5. Results and Lessons Learned
The stock portfolio game has been played for about
four years. One of the main results from playing the
game at an undergraduate level has been a significant
increase of interest in financial decision analysis. Before
playing the game, the students were asked to assess
their knowledge of stocks, portfolios, and markets.
Although most knew something about markets, stock
prices, and indices, few had any knowledge of stock
selection techniques, and even fewer knew about
portfolio information. At the end of the game the students have greater knowledge in these areas, but more
importantly they have wrestled with conflicting objectives such as maximizing portfolio value while not
falling into bankruptcy. Throughout the game the
teams address these issues on an ad hoc basis.

6. Conclusions

Students also show a very high level of intra-team
engagement, particularly as they learn their overall
position (based on stock allocations results) and try to
maintain or reach the top position. The game forces
students to tackle issues related to making choices
between higher stock returns with higher levels of
variability or lower return and lower variability. The
INFORMS Transactions on Education 8:1(41-48)

The use of stock simulation games is valuable for introducing engineering students with limited knowledge of finance and financial markets to the principles
of portfolio optimization and financial risk management. The stock portfolio game is easy to play and
amenable to the intuitive introduction of more advanced optimization topics.
47

© INFORMS ISSN: 1532-0545

VILLALOBOS
The Stock Portfolio Game

Downloaded from informs.org by [149.169.132.70] on 25 May 2017, at 16:43 . For personal use only, all rights reserved.

The game has been played for about four years under
different classroom settings, group sizes, and class
levels. While the game has had a good level of acceptance among students and much success, according
to the objectives set for the game there is room for
improvement in terms of tailoring the game to different audiences, class settings, and time available.

Wolfe, J., D. Crookal, (1998) "Developing a scientific
knowledge of simulation/gaming," Simulation
Gaming, Vol. 29, No. 1, pp. 7-19.
Wood, W. C., S. L. O'Hare, R. L. Andrews, (1992) "The
Stock Market Game: Classroom use and strategy," J. Econom. Ed, Vol. 23, No. 3, pp. 236-246.

7. Acknowledgments
The author would like to thank Joel Polanco and
Marco A. Gutierrez for their assistance in the development of the original version of the game.

References
Faria, A. J., W. J. Wellington, (2004) "A survey of simulation game users, former-users, and neverusers," Simulation Gaming, Vol. 35, No. 2, pp.
178-207.
Foundation for Investor Education. "The Stock Market
Game,"
http://www.stockmarketgame.org/
International Association of Financial Engineers, "Resources: Academic Programs,"
http://www.iafe.org/resources_acad.html
(accessed on October 17, 2007).
Lean, J., J. Moizer, M. Towler, and C. Abbey, (2006)
"Simulation and games: Use and barriers in
higher education," Active Learn. Higher Ed., Vol.
7, No. 3, pp. 227-242.
Maier, M., (2002) "A critical review of learning from
the market: Integrating the Stock Market Game
across the curriculum," J. Econom. Ed., Vol. 33,
No. 1, pp. 83-88.
Markowitz, M., (1952) "Portfolio selection," J. Finance,
Vol. 7, No. 1, pp. 77-91.
Stock-Trak, Inc. "National SMS ® Stock Market Simulation.",
http://www.nationalsms.com/na_about2.php
Summers, G. J., (2004) "Today's business simulation
industry," Simulation Gaming, Vol. 35, No. 2,
pp. 208-241.
Villalobos, J. R. "Stock Game,"
http://eal.asu.edu/stockgame/
(accessed on October 18, 2007)
INFORMS Transactions on Education 8:1(41-48)

48

© INFORMS ISSN: 1532-0545

Computers & Industrial Engineering 91 (2016) 154–164

Contents lists available at ScienceDirect

Computers & Industrial Engineering
journal homepage: www.elsevier.com/locate/caie

Alleviating food disparities with mobile retailers: Dissecting the problem
from an OR perspective
Christopher Wishon, J. Rene Villalobos ⇑
International Logistics and Productivity Improvement Laboratory, School of Computing, Informatics and Decisions Systems Engineering, Arizona State University, P.O.
Box 878809, Tempe, AZ 85287-8809, United States

a r t i c l e

i n f o

Article history:
Received 25 June 2015
Received in revised form 16 November 2015
Accepted 19 November 2015
Available online 2 December 2015
Keywords:
OR in service industries
Decision support systems
OR in agriculture
Knapsack problem
Vehicle routing problem

a b s t r a c t
Within the past 20 years, disparities in access to healthy and affordable foods have been observed within
urban communities leading to significant negative health and diet related repercussions. One recent
technique to alleviate such conditions is to introduce a mobile food retailer within the community but
preliminary evidence raises concerns regarding this retail format. While extensive literature exists
regarding other alleviation strategies, research on mobile food retailers has been sparse. To facilitate
new research into the efficacy and deployment of mobile food retailers, operations research tools are outlined and discussed. These tools specifically address the product mix and the routing plan of the retailer
as these decisions represent the greatest barriers to success for current retailers. A set of potential tools is
developed and employed within a preliminary case study in a Phoenix, Arizona community and the
results demonstrate the utility of the methodology by identifying changes to both the product mix and
routing plan which could result in greater earnings potential.
Ó 2015 Elsevier Ltd. All rights reserved.

1. Introduction
One of the more recent social inequalities identified within the
US is the existence of urban communities which lack easy and
affordable access to healthy foods. Such communities are now
commonly referred to as ‘food deserts’ and have been defined by
the United States Department of Agriculture (USDA) in the 2008
farm bill as an ‘‘area in the United States with limited access to
affordable and nutritious food, particularly such an area composed
of predominantly lower income neighborhoods and communities”
(Food, Conservation, and Energy Act of 2008, 2008). To accompany
this definition, the USDA developed threshold criteria to establish
if an urban census tract should be classified as a food desert. The
initial criteria developed by the USDA identified an urban food
desert as any tract where at least 500 people or 33% of the residents live more than a mile from the nearest supermarket and
the census tract’s poverty rate is greater than 20% or meets similar
poverty requirements. Currently using these definitions, 13.6% of
all urban US census tracts are classified as food deserts. In total,
this encompasses 33 million residents. Since these initial definitions, the USDA has added varying criteria on distance and poverty,
but even the most conservative definitions indicate that 19 million
⇑ Corresponding author.
E-mail address: rene.villalobos@asu.edu (J. Rene Villalobos).
http://dx.doi.org/10.1016/j.cie.2015.11.014
0360-8352/Ó 2015 Elsevier Ltd. All rights reserved.

people reside in the 7.7% of urban census tracts that are
categorized as food deserts.
To address this issue, numerous alleviation strategies have been
proposed. One of the most popular approaches is to introduce a
new, traditional food retailer into the community such as a supermarket or grocery store. However, it has been identified that some
of these communities do not have the requisite purchasing power
to support such a retailer (Califano, Gross, Loethen, Haag, &
Goldstein, 2012). In these instances, common alleviation strategies
include improving existing small retailers, promoting community
developed initiatives, and improving the existing urban infrastructure (Rose et al., 2009).
While these traditional remedies have shown some success in
certain communities, a novel vehicle-based retail format has
emerged which is specifically designed to alleviate urban food
desert conditions. This mobile retailer is typically a large vehicle
(e.g. a repurposed bus or a large trailer pulled by a truck) which
is stocked with healthy food items that are sold within food desert
communities at specific locations according to a predetermined
schedule. Even though this retail format for healthy foods did not
exist 15 years ago, examples of these retailers can currently be
found in over a dozen US cities.
In spite of the growing popularity of this retail format, mobile
retailers have yet to demonstrate that they can be an effective
alleviation technique since they have had minimal success at

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

becoming economically sustainable. Most, if not all, existing
mobile retailers receive a significant portion of their funding from
federal or local grants and even long serving mobile retailers experience difficulties in obtaining sufficient income to offset their
operational costs. Such a strategy is clearly unsustainable for any
permanent alleviation remedy.
This initial inability for mobile retailers to reach economic sustainability has created significant concerns regarding the efficacy
and longevity of the mobile retail format in alleviating food desert
conditions. To address these concerns, it is proposed that research
be conducted into the necessary community conditions (i.e. population, purchasing power, population density, road network, etc.)
and into the mobile retailer decision making process to understand
the factors that impact the economic sustainability of mobile
retailers. To date, there is no research specifically addressing the
community conditions and only limited research exists on the
decision making process but none has specifically focused on all
of the nuances of the mobile retailer.
Given this gap in research, the purpose of this article is to outline
the proposed analysis methodologies to investigate the true economic feasibility criteria for mobile retailers. Specifically, the motivation for using operations research (OR) modeling techniques,
which have rarely been applied to the food desert problem, will
be presented and possible research methodologies will be suggested and outlined. The utility of these methods will then be
demonstrated through a preliminary case study. This discussion
and study will serve as a starting benchmark on possible OR techniques for current researchers who are seeking to address food
deserts or similar issues.
To accomplish this goal, the scale and effect of the food desert
problem will be summarized to motivate the need for research into
alleviating food deserts with mobile retailers. Following this summary, key decision points for the mobile retailer will be determined
and possible OR techniques will be identified which can assist
researchers in understanding the feasibility of employing mobile
retailers within food deserts. The advantage of this approach is that
a carefully developed tool can be employed by researchers to study
the ramifications of mobile retailer decisions as well as by mobile
retailer coordinators who seek decision support assistance in their
operational decisions. An initial case study will then be presented
using operational data from the Phoenix metropolitan area.
2. Food desert background and alleviation
To motivate the need for research into alleviating food deserts,
the extent and effect of food deserts must be documented. Hence, a
summary of the most recent statistical research into access disparities and the effect these disparities have on community and individual health will be provided. This will ultimately demonstrate
alleviating food deserts is a justifiable use of time and economic
resources within underserved communities. Common approaches
to alleviating food deserts will then be presented. These
approaches will conclude with a discussion regarding the advantages and disadvantages of the mobile healthy food retailer.
2.1. Extent and effect of food deserts
Considerable effort has been expended to determine the extent
of food desert communities by researchers across numerous disciplines and organizations. Since summarizing all of this research is
outside of the scope of this discussion, the following section presents the most recent research into statistically significant disparities in food access. For readers interested in more food desert
research, summaries of existing literature are provided by Caspi,
Sorensen, Subramanian, and Kawachi (2012), Walker, Keane, and

155

Burke (2010), McKinnon, Reedy, Morrissette, Lytle, and Yaroch
(2009), and Beaulac, Kristjansson, and Cummins (2009).
The key issue with current research into the existence of food
deserts is that there is little consistency when defining the community, healthy food, and access criteria required for an area to be
classified as a food desert. For instance, Ver Ploeg et al. (2009),
the study accompanying the USDA’s definition of a food desert,
used 1 km. grids as proxies for communities while defining poor
access to healthy foods as living more than a half mile (or full mile)
away from the nearest supermarket with additional requirements
including the resident being low-income and possibly not having
access to personal transportation. In comparison, Rose et al.
(2009) used census tracts to define a community and measured
disparity based on access to all food retailer types within a 1 (or
2) km. radius while considering the actual shelf space dedicated
to fruits and vegetables in these stores. While it is not the focus
of this discussion, this important shortcoming is mentioned
because these differences in parameters are pervasive throughout
food desert literature and hinder a concise conclusion on the existence and extent of food deserts. Readers interested in a thorough
discussion of this and other issues within food desert literature are
recommended to read Bitler and Haider (2011).
With this shortcoming in mind, existing food desert studies are
still useful in identifying local statistical disparities in access. For
instance, numerous studies into the existence of food deserts indicate that at-risk demographic groups have statistically less access
to supermarkets then their reference groups. This applies to lowincome versus high-income (Baker, Schootman, Barnidge, & Kelly,
2006; Block & Kouba, 2007; Larsen & Gilliland, 2008), Black versus
non-Hispanic White (Bader, Purciel, Yousefzadeh, & Neckerman,
2010; Baker et al., 2006; Powell, Slater, Mirtcheva, Bao, &
Chaloupka, 2007), and Hispanic versus non-Hispanic White
(Bader et al., 2010; Moore & Diez Roux, 2006). However, the opposite is true for access to smaller or independent food retailers since
it appears that most at-risk groups have better access to these
types of stores (Moore & Diez Roux, 2006; Morland, Wing, Diez
Roux, & Poole, 2002; Powell, Slater et al., 2007). For the purposes
of this summary, smaller stores include independent grocery
stores, fruit and vegetable markets, meat markets, farmers’ markets, or similar independent food retailers.
These results demonstrate that even if urban, at-risk groups
may be underserved by traditional retailers, smaller retailers could
fill this gap. For instance, when the proportion of Mexican–Americans started to increase in the southwest US, small ethnic grocery
stores called Carnicerias naturally arose as a method to address the
need for community-specific foods (Duran, 2007). Hence, it
appears that natural economic development favors smaller food
retailers within the urban food desert environment. Such a result
is not surprising, but it is often overlooked by community developers seeking to alleviate food desert conditions.
Given these disparities, significant research has been undertaken to quantify the potential effects of having poor access to
healthy foods. With respect to supermarket access, numerous studies have investigated if having poor access to supermarkets implies
that at-risk communities pay more for their healthy food purchases.
These studies have unanimously found that low-income citizens do
not spend more on food items. In many instances, low-income
urban populations have statistically less food expenditures than
high-income shoppers even though studies have demonstrated that
the stores more frequently located nearby low-income populations
tend to have higher food prices (Andreyeva, Blumenthal, Schwartz,
Long, & Brownell, 2008; Kaufman, MacDonald, Lutz, & Smallwood,
1997).
One theory for this phenomenon is that lower-income shoppers
are more likely to rely on lower quality food items as measured by
the look and freshness of the food. In support of this theory, Block

156

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

and Kouba (2007) and Andreyeva et al. (2008) found that lowincome shoppers have worse access to high-quality fruits and vegetables as opposed to high-income shoppers. Another factor is that
low-income shoppers are extremely price sensitive with respect to
food purchases (Alkon et al., 2013; Caraher, Dixon, Lang, & CarrHill, 1998; Clifton, 2004; Haynes-Maslow, Parsons, Wheeler, &
Leone, 2013; Ohls, Ponza, Moreno, Zambrowski, & Cohen, 1999)
and will therefore be more likely to travel further for bigger cost
savings. Both of these theories demonstrate that any intervention
must be very cost-conscious if the goal is to serve low-income
communities.
Besides the financial effects, research has been conducted on
the health implications of living within a food desert community.
Numerous studies have identified that having statistically better
access to supermarkets is either positively associated or has no
effect on healthy eating habits (Bonanno & Goetz, 2012; Rose &
Richards, 2004; Zenk et al., 2009) or on health outcomes such as
obesity and disease (Bodor, Rice, Farley, Swalm, & Rose, 2010;
Morland, Wing, & Diez Roux, 2002; Zenk et al., 2009). While these
results are not definitive, no statistically-based study was identified which indicated better access was associated with decreases
in healthy food consumption or health outcomes.
Likewise, better access to smaller food retailers, applying the
same definition used for identifying disparities in access, showed
the same trend for diet quality/consumption (Gustafson, Lewis,
Moore, & Jilcott, 2013; Lopez, 2007) but had mixed correlations
with obesity. Three out of the five studies which investigated the
correlation between small store access and obesity found no relationship (Bodor, Rose, Farley, Swalm, & Scott, 2008; Morland, Wing,
& Diez Roux, 2002; Zenk et al., 2009) while two others found that
better access was correlated with increased obesity (Bodor et al.,
2010; Powell, Auld, Chaloupka, O’Malley, & Johnston, 2007). While
this could be a causal relationship, these results can also be
explained within the context of the prior findings since lowincome and minority populations (especially Black Americans)
tend to have better access to smaller food retailers and these
groups are much more likely to be obese (Flegal, Carroll, Kit, &
Ogden, 2012).
It should also be noted that most of the studies investigating the
effects of food deserts suffer from the same inconsistencies as the
research which studies the existence of disparities since there is no
standard definition of a food desert. Therefore, comparing the
results between two studies is challenging since different measures may have been employed. This disparity also prohibits precise, global measurements on the effects of food deserts since the
definition differences between studies imply that the scale of the
effects is incomparable.
2.2. Alleviating food deserts
Numerous alleviation strategies have been suggested for
improving access to healthy foods. Some of the more popular
strategies are to introduce new retailers, to improve physical or
financial access to healthy foods in existing retailers, or to improve
the community infrastructure. Thorough discussions of the applicability of each of these approaches is provided by Ver Ploeg
et al. (2009) and Rose et al. (2009).
In spite of the quantity of research into the existence and effect
of food deserts, limited formal evidence exists on how to alleviate
food desert conditions. With respect to introducing new retailers,
only two studies have investigated the full effect of such an intervention. Cummins, Petticrew, Higgins, Findlay, and Sparks (2005)
studied the introduction of a new supermarket into an underserved Scottish neighborhood and found no evidence that the
new supermarket increased consumption of healthy foods or
improved physiological health. However, they did find that the

community self-reported an improved psychological health after
the retailer was introduced. Wrigley, Warm, and Margetts (2003)
also studied the introduction of a supermarket in Leeds, England
and noted that residents who had poor diets prior to the retailer’s
introduction increased their fruit and vegetable intake by a half
portion while those with the worst diets increased their intake
by a full portion on average. They also reported that residents
who reported walking to their food retailer of choice doubled as
a result of the new supermarket’s introduction which is important
given that low-income individuals may lack access to personal
transportation (Clifton, 2004).
The results from these studies are far from conclusive and findings from other studies indicate that introducing new, large traditional retailers (such as supermarkets) will not guarantee an
alleviation of food desert conditions. For instance, one requirement
for a new food retailer to be sustainable is that there is sufficient
purchasing power within the community. Alwitt and Donley
(1997) found that supermarkets in Chicago, IL, a city in which disparities in access are typically found, are already evenly distributed
with respect to purchasing power indicating that a new supermarket may not be able to aggregate enough revenue to become sustainable. Further supporting this finding is the limited
supermarket access (LSA) areas identified by The Reinvestment
Fund who found that over half of the LSA areas do not have the sufficient income to support a new food retailer (Califano et al., 2012).
The study by Rose et al. also supports this observation as they
state, ‘‘all neighborhoods cannot support a supermarket, nor are
supermarkets the only way to assure access to healthy food”
(Rose et al., 2009).
For other common alleviation strategies, the effect of interventions has been more exhaustively studied due to their ease of
implementation. For instance, Sigurdsson, Larsen, and
Gunnarsson (2014) found that changing the placement of foods
and their advertisements in smaller stores could induce an
increase in the sales of healthy food items and a decrease in sales
for less healthy items. Similarly, Weatherspoon et al. (2013) analyzed transaction data and found that when low-income food
desert residents in Detroit, MI were given access to affordable,
higher quality, healthy food items from small retailers, they will
increase their purchases of such items. For community-level programs such as farmers’ markets, incentivizing Supplemental Nutrition Assistance Program (SNAP) purchases was found to increase
both SNAP and general purchases (Bertmann, Ohri-Vachaspati,
Buman, & Wharton, 2012) and the establishment of a farmers’ market in London, Ontario, Canada lowered food basket prices within
the entire community (Larsen & Gilliland, 2009).
While these approaches have shown tremendous success
within certain communities, significant barriers still remain. Principally, low-income shoppers perceive farmers’ market and similar
efforts as higher priced than supermarkets (Balsam, Webber, &
Oehlke, 1994; Flamm, 2011; Zepeda, 2009) even though a difference typically does not exist (Flamm, 2011; McGuirt, Jilcott, Liu,
& Ammerman, 2011). This is further complicated if the retailer
does not accept nutritional assistance vouchers, as administered
through SNAP or similar programs, as low-income shoppers frequently cite this issue as a common barrier (Flamm, 2011; Leone
et al., 2012). Additionally, many low-income shoppers state they
have poor physical access to local markets (Grace, Grace, Becker,
& Lyden, 2007; Racine, Smith Vaughn, & Laditka, 2010; Ruelas,
Iverson, Kiekel, & Peters, 2012) or they do not even know a market/healthy food retailer exists in their area (Leone et al., 2012;
Racine et al., 2010).
Ultimately, this evidence shows that there is no standard
approach to alleviating food desert conditions within an urban
community. While introducing new, traditional food retailers
would immediately solve the food disparity, the community may

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

not have the requisite purchasing power to support such a retailer.
In addition, there is conflicting evidence as to the effect of such a
retailer on the surrounding community. Likewise, improving existing retailers has shown success, but may not be cost effective for
smaller stores. Finally, community-driven efforts such as farmers’
markets have been well studied and show promise, but there are
still significant barriers which could undermine the effectiveness
of the retailer in serving low-income residents.
2.3. Mobile food retailers
A new retail format has emerged which could also satisfy the
demand for fresh and healthy foods within food deserts. This format utilizes a mobile retailer which stocks nutritious foods that
it sells at various locations throughout the community. This type
of retailer is similar in concept to food trucks which have risen in
popularity in numerous US cities, but mobile retailers tend to be
larger and do not typically or solely stock prepared food items.
This retailer format was first piloted in Oakland, CA in 2003 by a
local organization that modified a used postal truck to stock
healthy food items (Windmoeller, 2012). This truck was then driven to different locations within an underserved West Oakland
community where residents were able to purchase food items
not normally found within their local stores. Since 2003, this business model is currently or was previously operating in over 10
North American cities including Chicago, IL; Madison, WI; Chattanooga, TN; Boston, MA; Hartford, CT; Toronto, Ontario; and Phoenix, AZ. These retailers all operate as non-profit businesses and are
typically organized by a community outreach program with the
support of the city. Less frequently, these retailers are organized
by a local food retailer which has expanded their business model
to include this new mobile format.
While the objective of all of these retailers is similar, not all of
them make the same decisions to meet these goals. First, not all
of the retailers use the same vehicle as the basis for their operations. The most common vehicle format is a city bus, as demonstrated by the retailer in Phoenix, AZ, where the interior of the
vehicle is replaced with shelving and bins in which fresh fruits
and vegetables are stocked. The popularity of this format is likely
because these buses are donated or purchased at a discount from
the city in which the retailer operates. Other vehicles include the
aforementioned postal truck or a racing-car trailer pulled by a
truck.
Additionally, the food items stocked by the retailer can vary.
Many of the existing mobile food retailers solely stock fruits and
vegetables. The justification for this limited product mix is that
these food items are typically non-existent in food desert communities. However, some mobile retailers have an expanded product
mix which includes whole grain bread and even low-fat milk.
While stocking such food items increases the operating cost of
the retailer, especially for refrigerated goods, a broader product
mix may be more appealing to food desert residents as it more closely replicates the robust offerings from traditional food retailers
such as supermarkets.
The final operational detail for mobile food retailers is the locations and times when the retailer stops to sell its goods. In general,
the retail stops are locations in which it is presumed that lowincome, underserved people visit with relative frequency at the
time of service. An additional consideration is that the mobile
retailer may be limited in its possible service locations based on
the size of the vehicle. Many mobile retailers have commented that
there are both physical and legislative limitations on where the
vehicle can sell its goods. Given these requirements, popular examples of service locations include community centers, schools,
senior living centers, housing centers, and community shelters.
Other locations may exist, but the novelty of the format does not

157

permit insight into a preferred location strategy. Furthermore,
the best locations likely differ between cities and even between
neighborhoods.
This retail format has numerous advantages with respect to servicing a food desert population. As mentioned prior, it was identified that supermarkets can be evenly distributed with respect to
purchasing power. This implies underserved communities have
purchasing power which is dispersed within the community.
Therefore, a mobile market has a unique advantage in alleviating
food desert conditions since it is the sole retail format which is able
to capture dispersed demand.
Another advantage of the mobile retailer is that it is insensitive
to the traditional strategic and tactical decisions of normal food
retailers. For example, a traditional food retailer’s success is often
dictated by its location (typically a strategic decision) whereas a
mobile retailer is able to easily change its retail location if the incorrect decision is made. Furthermore, this unique ability permits the
mobile retailer to adapt and change within its environment.
Similarly, the mobile retailers operated by community outreach
entities have complete flexibility with respect to their agricultural
supply chain which can be utilized to obtain better prices for their
customers. For instance, a mobile retailer can source its product
from traditional food wholesalers, local food hubs (aggregation
points for local farmers), or even directly from growers. This is
advantageous for the mobile retailer since they would be able to
circumvent an increasingly larger agricultural supply chain
(Cook, 2011) thereby reducing the cost of their produce. This
would also offer growers an attractive buyer of their goods since
the mobile retailer would not enforce the services and fees typically introduced by larger retailers (Calvin et al., 2001; Dimitri,
Tegene, & Kaufman, 2003).
This retail format also has numerous intangible benefits with
respect to servicing food desert communities. Due to its small size,
a mobile retailer would be able to specify its product offerings to
target specific nutritional or cultural requirements without feeling
compelled to offer unhealthy food items. Additionally, the retailer
is able to provide employment for the low-income population it
services. While the driver must have specific qualifications,
employees who stock the vehicle or serve as cashiers can be hired
from within the community. The retailer is also able to provide
complimentary services such as wellness screenings or nutritional
education by partnering with other community outreach groups at
the visited sites.
Even with these advantages, numerous issues exist with this
retail format. For instance, one of the major challenges for all existing mobile retailers is that most, if not all, rely heavily on local and
federal sources of funding to support their operations. This presents a clear sustainability challenge. This is demonstrated in the
case of the mobile retailer in Chicago, IL as they have currently
stopped operations due to a lack of funds. Given that Chicago, IL
has a large population of low-income, minority residents, such a
retailer ceasing its distribution of healthy foods poses serious
health concerns for residents who became reliant on the retailer.
One of the other key challenges is that a mobile retailer’s limited shelving space doesn’t permit a wide variety of products to
be offered. This differs from one of the key advantages of a supermarket since the average supermarket can stock over 40,000 different items. Clearly a mobile retailer could not offer the same
breadth of selection which may deter those shoppers who prefer
to obtain all possible food items in one trip.
In spite of the aforementioned advantages, the documented
challenges that mobile retailers have encountered introduce concerns regarding the impact and feasibility of this retail format.
Since this format has only increased in popularity, insight and
research into the efficacy of mobile retailers is greatly needed. To
date, research into mobile retailers is sparse and no research exists

158

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

which examines the community requirements or mobile retailer
decisions which are needed for the retailer to become a sustainable
intervention within its community.
3. Mobile food retailer operational planning
To study the efficacy of alleviating food deserts using mobile
food retailers, investigators must be able to quantify the potential
sustainability of mobile retailers based on possible combinations of
community demographics such as demand, population, and density. Since mobile retailers have only existed for a short amount
of time, thoroughly investigating the capabilities of these retailers
using historical data is impossible. Thus, it is recommended that
mathematical models, specifically designed for mobile retailer, be
developed such that the efficacy of this retailer format can be
investigated through sensitivity analyses.
Two problems were identified as the most significant to current
and future mobile retailers: the healthy food product mix and vehicle routing. These operational level decisions were identified as
they are the decisions which are the most within the control of current mobile retailer. Other decisions include the strategic level distribution, operating region, and warehouse selection problems and
the tactical level vehicle selection and vehicle quantity decisions. A
graphical summary of these decisions, their dependencies, and the
possible tools which can be used to solve these decisions is shown
in Fig. 1. The applicability of the tools in Fig. 1 are apparent except
for utilizing warehouse selection to solve the product mix problem.
In such a methodology, it would be possible to model the vehicle as
a fast-pick area (an area in the warehouse where the most popular
items are kept close to the main picking stations) and the closest
supermarket as the rest of the warehouse. By assigning costs to travel, it is possible to model which foods should be stocked on the
retailer. The issue is that many of these costs may be subjective
so the knapsack approach recommended in Section 3.1 is preferred.
Ultimately, all but the operational decisions are outside of the control of current mobile retailers so the following subsections will
detail both the product mix and vehicle routing problems giving
possible formulations and motivations for each. As the retailer format continues to expand in popularity, future research is recommended for the strategic and tactical decisions.
3.1. Mobile food retailer product mix
To date, the most common approach for stocking healthy
mobile retailers is to exclusively sell fresh fruits and vegetables.

The advantage of this approach is that these items are always considered to be healthy and they are generally the foods which are
lacking in food desert communities. In comparison, some mobile
retailers decide to offer a more diverse array of foods including
fruits, vegetables, low-fat milks, and whole grain breads. This
mix requires more space and infrastructure, but the diverse offerings more closely match the product types found in traditional
food retailers. To investigate if either of these mixes are the correct
approach, an optimization model is developed to test the sensitivity of various mix requirements.
The product mix model is conceptually simple to define. With
respect to the model’s goals, there are typically two objectives.
The first is that the healthiness of the stocked food items should
be maximized. This presents a clear challenge as it can be difficult
to accurately measure the health effects of foods and some items
have both negative and positive effects. The other objective is that
the stocked product mix should have a minimum consumer cost
since stocking high cost items would alienate low-income shoppers. This goal is often at odds with maximizing the health of the
product mix since healthier food items can be more expensive.
Restricting these objectives are two principal constraints. The first
is that the stocked mix must not exceed the volume limitations of
the vehicle while the second is that the mix must be profitable
enough for the retailer to become economically sustainable.
Hence, the following bi-criteria, demand constrained knapsack
problem was developed as it is relatively simple and can be
expanded as needed to model any additional details deemed necessary by either the researcher or mobile retailer.

Maximize

X
h i xi ;
i2I

X
c i xi ;
Minimize
i2I

Subject to :

X

v i xi 6 V;

i2I

X
pi xi P P;
i2I

xi 2 f0; 1g 8 i 2 I:
The goal of the bi-objective problem is to select the food item i
for inclusion in the retailer only if the summation of the healthiness ratings, hi , for the selected subset is maximal and the summation of the consumer costs, ci , for the selected subset is minimal.
These objectives are subject to three constraints. The first ensures
that the summation of the selected item volumes, v i , does not

Fig. 1. Mobile retailer decisions.

159

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

exceed a given threshold V. The second ensures that the summation of the selected item profits, pi , exceeds a given threshold P.
The final constraint requires the decision variables to be binary.
There are three key details motivating this formulation. First, it
is a relatively simple formulation but further complexity could be
added based on the needs of the mobile retailer. For instance, additional constraints could be added if other restrictions are required
such as multiple-choice constraints that restrict the quantity of
food items which can be selected from any larger food category.
Also, a robust solution to the problem can be obtained by making
the profit parameters stochastic as this more closely models uncertainty in the actual system. The second motivating factor is that
solving such a problem using commercial software is simple. However, it is unreasonable to assume mobile retailers will have the
financial or technical access to such software. Therefore, researchers are cautioned to solve the product mix problem using dynamic
programs with basic languages such that they can be easily implemented by basic computer users. Such solution methods have yet
to be developed for demand constrained knapsack problems.
The final motivation is that there are numerous simple methodologies to obtain hi for all of the food items. The most likely candidate is to use one of the existing nutritional profiling schemes
which exist to score the healthiness of various food items. Numerous such systems have been developed and a discussion of different systems can be found in Drewnowski (2005), Garsetti, de
Vries, Smith, Amosse, and Rolf-Pedersen (2007), and Drewnowski
and Fulgoni (2008). Nearly all nutritional profiling schemes use a
ratio with the healthy measures of foods in the numerator and
the unhealthy measures of foods in the denominator. Hence, the
higher the score, the healthier the food item. In lieu of using such
measures, it is also possible to independently score food items
based on their nutritional content by using basic statistical analysis. Such a technique will be presented in Section 4.

3.2. Mobile food retailer routing plan
The other operational decision which would need to be
addressed is the vehicle routing and scheduling problem. As discussed previously, a mobile food retailer visits a predetermined
set of locations and can service any demand within a specified distance since it is assumed that customers would be willing to travel
small to moderate distances to visit the retailer. Hence, the goal of
this operational decision is to locate in such a way that maximal
demand is captured while minimizing the cost of logistically routing through the visited locations.
So far, this is the sole aspect of the mobile food retailer problem
to be addressed. Algert, Agrawal, and Lewis (2006) identified
potential areas to service by clustering demand, but provided no
discussion to the routing aspect of the problem. Additionally, only
the residential location of the population was considered and
which subset of clustered locations should be served was never
addressed. The other research is from Widener, Metcalf, and BarYam (2012, 2013) who identified which customers lack access to
supermarkets and which of these groups should be served in an
optimal solution. Again, no discussion is given to routing the vehicle and only a subset of the visitable locations is considered in their
formulation. Hence, more research is needed to assist a mobile
food retailer coordinator in making the optimal routing decision.
This unique problem combines elements of two classical problems: the Vehicle Routing Problem (VRP) and the Maximal Coverage problem. The combination of these two problems has only
been addressed twice. The first instance is from Akinc and
Srikanth (1992) who sought to schedule and route a generic mobile
facility. In their formulation, the authors assigned a cost for servicing nodes from a distance and assumed that all points must be cov-

ered. The other instance is by Halper and Raghavan (2011) who
developed a mathematical model to locate mobile radio towers
to provide service at dispersed events. In their formulation,
demand is modeled as a continuous variable which is satisfied so
long as the tower is at that location. Either of these studies is not
directly applicable since they use different mathematical models
or make different assumptions than those required for the mobile
food retailer. Hence, novel research is needed.
Given this motivation, the following sets, parameters, and variables are required.
Indexes and sets
J: set of mobile vehicles.
M: set of demand locations and the vehicle depot (indexed
with 0).
P: set of different products stocked on the mobile retailer.
Parameters

qmjp : profit generated by servicing one unit of demand of
product p at location m.
/mj : fixed cost for serving location m with vehicle j.
smnj : fixed cost for vehicle j traveling from location m to
location n.
bmn : indicator for the percentage of location m demand which
can be captured by a vehicle servicing location n.
C jp : total stocked units of demand on vehicle j for product p.
Dmp : maximum units of demand at location m for product p.
Variables
ymnj : binary variable indicating if vehicle j travels from demand
location m to demand location n.
zmjp : non-negative units of demand satisfied for product p at
location m by vehicle j.
Given these parameters and variables, the mathematical formulation is as follows.

2
3
XX X
X
X
4 qmjp zmjp  /mj ymnj 
Maximize :
smnj ymnj 5
j2J m2M

Subject To :

X

p2P

n2M;
n–m

n2M;
n–m

X
ymnj 
ynoj ¼ 0 8 j 2 J; n 2 M

m2M;
m–n

X

o2M;
o–n

y0nj ¼ 1 8 j 2 J

n2Mnf0g

XX
ymnj 6 jSj  1 8 j 2 J; S # M n f0g; jSj P 2
m2S

n2S;
n–m

zmjp 6

X
X
bmn Dmp ynpj
n2M
n–m

X
zmjp 6 Dmp

8 j 2 J; m 2 M

p2M

8 m 2 M; p 2 P

j2J

X

zmjp 6 C jp

8 j 2 J; p 2 P

m2M

The objective of this formulation is to locate and route the set of
vehicles such that summation of the revenues minus the costs
from locating and routing are minimized. This is subject to six primary sets of constraints. The first three sets of constraints is the
standard vehicle routing constraints which ensure conservation
of flow, that all vehicles leave the depot, and that there are no subtours. The fourth constraint is used to link the VRP and maximal
coverage aspects of the formulation by ensuring that demand is
only satisfied at a location if a vehicle stops at a location within

160

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

that service radius. The final two constraints ensure the demand
serviced at a location does not exceed the given threshold and that
all the demand serviced by a truck does not exceed its capacity. As
was stated for the product mix formulation, researchers should
develop solutions to this problem using dynamic programming
techniques which do not require commercial software. Such a solution procedure will allow any research to be applicable to mobile
retailers who are seeking standalone methods to improve their
operations.
In addition to this baseline formulation, it is possible to add
additional elements to the formulation which will make the developed routes more robust. For instance, time window constraints
can be added such that the retailer is servicing a point only during
a feasible time when potential customers would be present. This is
an extremely important consideration as one of the preferred stops
of existing retailers is elementary schools where the only serviceable windows are when parents are dropping off and/or picking
up their child. Another consideration is that it is unreasonable to
assume that the community demand for food will ever be known
with certainty. Hence, a stochastic variant of the problem can be
formulated with respect to food item demand.

4. Phoenix case study
To demonstrate the capabilities of the suggested approach, an
initial investigative case study was conducted for the Phoenix
metropolitan area. Specifically, operational data from the existing
mobile food retailer Fresh Express was analyzed and broadened
to study their existing efficiencies and operating network. The
study that follows is the first in a series of research efforts and
the conclusions are therefore preliminary. The future work to
expand upon this case study will be summarized in Section 5.
The first investigated operational decision for this case study is
the product mix which should be stocked by Fresh Express. Fresh
Express currently operates out of a retrofitted city bus with capacity to hold at most 45 different items in separate bins. Within the
current system, they exclusively stock fresh fruits and vegetables.
To support this investigative study, two months of operational data
regarding sales quantity, revenues, and costs were obtained which
comprised of 6978 units sold from the 44 most consistently
stocked items. These items cover a diverse offering including but
not limited to pineapples, zucchinis, cilantro, cucumbers, and red
grapes.
To expand on this product offering, total fresh fruit and vegetable usage for an entire calendar year was provided from a different Phoenix-based supermarket chain. By selecting the most
popular food items which were not currently offered by Fresh
Express, another 51 items were included in the potential product
mix. These items were mostly fresh fruits and vegetables but additional items such as rice, croutons, and fruit juice were added as
they were offered in the supermarket’s produce section and they
would provide significantly different options for Fresh Express in
comparison to their current offerings.
Using these items, the historical Fresh Express data was analyzed to determine the customer cost of an item (ci ) and the
expected profit per week from stocking that item (pi ) for any food
items that Fresh Express already stocks. For the additional 51 items
not currently offered by Fresh Express, the supermarket data was
analyzed and scaled to represent potential profits/week if the item
were to be offered on the mobile retailer. This was completed by
comparing the overlapping foods (i.e. those sold by both Fresh
Express and the supermarket) and then determining a scaling ratio
to apply to other 51 food items. The customer costs of these items
were based on their current price in traditional supermarkets as
they compare favorably with Fresh Express’ pricing structure.

Finally, v i ¼ 1 for each possible item and V ¼ 45 as Fresh Express
currently stocks their bus by using 45 bins which store only one
item type each.
To determine the healthiness scores of each item (i.e. hi ), a
nutritional profiling system, such as the commercially available
NUVAL system, was initially considered. However, these systems
will normally score fresh fruits and vegetables at the highest level
which allows for little differentiation for the items in the current
study. Instead, nutritional data per serving was acquired for each
of the food items. The nutritional data points were selected such
that they match those utilized by the USDA to create the Thrifty
Food Plan (Carlson, Lino, Juan, Hanson, & Basiotis, 2007). These categories include protein, thiamin, riboflavin, niacin, folate, calcium,
phosphorus, potassium, magnesium, iron, zinc, fiber, cholesterol,
sodium, and fat as well as vitamins A, B6, B12, C, and E. Each food
item’s nutritional data was normalized and summed to form one
score. Any nutrient which has a positive health benefit (e.g. calcium) had a positive influence on the score and any nutrient which
has a negative health benefit (e.g. fat) had a negative influence.
In total, 7 different profit requirements (P) were tested which
represent the total range of feasible knapsack problem solutions.
For each level, convex combinations of the two objective functions
were solved in order to obtain an efficient frontier for that profit
level. The graphical representation of these frontiers is shown in
Fig. 2 where each labeled line represents a different profit requirement. The vertical axis represents the consumer cost if a customer
were to purchase one of each item while the horizontal axis is the
sum of the health scores for each selected item. Clearly as the profit
requirement becomes more restrictive, the flexibility in food offerings with respect to consumer cost and health decreases. This flexibility decrease is initially minimal but quickly increases once the
profit requirement exceeds $225. Additionally, there are no feasible solutions when P ¼ $325 or more.
The results shown in Fig. 2 clearly demonstrate that Fresh
Express has significant variety with respect to product offering,
especially when profit requirements are minimal. Currently, Fresh
Express’ weekly profits are in the lower range of the values tested.
This not only provides validation of the data, but also demonstrates
that Fresh Express may be able to become economically stronger if
they were to alter their product mix. In addition, Fresh Express
clearly has significantly different options with respect to the cost
and health of the food items. For instance, out of the 95 possible
items, 81 of the items were included in at least one of the product
mixes identified indicating that a diverse set of mixes are possible.
This shows that this methodology will allow Fresh Express to select
the offerings which best suits their needs or best satisfies any nonmodeled criteria.
Finally, it is worth observing that this methodology also returns
practical product mixes. For example, when P ¼ $250, the product
list shown in Table 1 is along the efficient frontier. This mix’s total

Fig. 2. Product mix efficient frontiers for all profit requirements.

161

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164
Table 1
Sample product mix for P ¼ $250.
Gala apples
Plantains
Bananas
Green beans
Pinto beans
Snap peas
Blackberries
Raspberries
Strawberries
Broccoli florets

Broccoli crowns
Green cabbage
Regular carrots
Bi-color corn
White corn
Cucumber
Red grapes
Collard greens
Iceberg lettuce
Kale

Coconut
Kiwi
Papaya
Artichoke
Asparagus
Bean sprouts
Brussel sprouts
White mushrooms
Grapefruit
Lemon

health offering is 287.4 (average over all optimal offerings in this
frontier is 270.9) and the total cost to customers is $43.26 (average
over all optimal offerings in this frontier is $40.69). This mix is particularly practical as it does not feature many substitutable products (i.e. products where if one item is not offered, then the
other product would be considered a viable substitute) and it represents an even balance between health and consumer costs.
Hence, the employed formulation, even though it is relatively simple, is useful without adding additional constraints which could
complicate simple solution methodologies. This is important since
mobile retailers would not have access to sophisticated software
packages.
Given a selected product offering from Fresh Express, the second operational decision, the routes of the vehicle, can be
addressed. For this case study, the set P (representing different
products) was not utilized in the routing formulation as item
demand per location was not available from Fresh Express. However, Fresh Express visits schools with strict time windows for service and the formulation was expanded to include such
restrictions. A Tabu Search methodology, similar to Cordeau,
Laporte, and Mercier (2001), was developed to obtain heuristic
solutions to this covering VRP with time windows.
The data for this problem was provided by Fresh Express for the
same two month period as the product mix data. Specifically, the
set of visitable locations, the average revenues per location, and
the times of service for the locations were provided. These locations served as the set of possible sites for this case study and
the service durations were kept the same as the current schedule
provided by Fresh Express. The time windows were determined
as either the morning period (7 AM to 12 PM) or afternoon period
(1 PM to 5 PM) for any non-school location and 7:30 AM to
9:30 AM or 2 PM to 4 PM for each school location. The only outlier
is the location which represented downtown Phoenix which is
always served during the lunch time period. These windows were
selected to minimize disruption from the current morning and
afternoon scheduling of Fresh Express. The cost of traveling
between any pair of locations was determined based on the network distance between all locations multiplied by the average
operational cost/mile reported by US municipal bus networks.
The time to travel between any pair of locations is again based
on the network distance and is rounded up to the nearest 30 min
interval to account for the time required to setup/breakdown the
retailer at each stop.
In addition to the locations currently visited by Fresh Express,
an additional 20 points were added as potential stops. These points
represent the distance between each serviceable location and its
closest neighbor. These were added such that these intermediate
locations could be visited by Fresh Express and hopefully satisfy
both points at a distance. Future research could expand upon these
points by finding other meeting sites and possible service locations
within Fresh Express’ operating region. It was assumed that there
are three vehicles which needed routes as Fresh Express currently
operates three days per week.

Sunflower seeds
Green onion
Navel oranges
Valencia oranges
Gold bell pepper
Red bell pepper
Anaheim chili
Red plums
Russet potatoes
Sweet potatoes

Yukon gold potatoes
Spinach salad mix
Butternut squash
Grey squash
Yellow squash

Finally, various service radii were tested. In total, 7 separate distances were tested between 0 and 1600 m which represent the
spectrum of customers not willing to travel any distance for service
to customers willing to drive or travel a mile for service. If the
retailer was within this distance it could satisfy 100% of that location’s demand and 0% otherwise. The optimal solution from these
cases is shown in Table 2 and a graphical representation of the network and a selection of the solutions are shown in Fig. 3. Specifically, the four panels in Fig. 3 show the routes for the best week
from the two month historic data compared with the heuristic
solutions with 0 m, 800 m, and 1600 m covering permissions,
respectively. Within each panel are the depot (single large circle),
the stops with demand (black circles), and the intermediate stops
with no demands (grey circles) as well as the heuristic routes
showing the specific stop times and any covered stops indicated
by lines without arrows.
The results from Table 2 compare favorably with the current
routing plan from Fresh Express which averages approximately
$700 per week without including the cost of transit. Hence, the
retailer can again modify their operational decisions to increase
their revenues if desired. For instance, the first panel of Fig. 3 only
shows two routes since the third route of that week solely visits
locations which are never visited in any heuristic solution. These
are never selected as they are much further from the depot than
all the other points and they provide half of the revenue on average
compared with the other points shown in Fig. 3. If Fresh Express
wanted to increase their revenues, omitting these stops from the
schedule would be advised.
Table 2 also demonstrates that given the current network, there
is not much differentiation between the solution routes at the various covering levels. This is likely since the solution method is a
heuristic and the potential stops are only a small subset of the possible stops available in the Fresh Express operational community.
Hence, future research should seek to identify new locations and
measure demand potentials at these stops.
Finally, Fig. 3 shows the various route possibilities. It is recommended that Fresh Express modify their routes to match to second
panel solution as either of the other solutions assumes customers
would be willing to travel too far for service. In total, the best week
from Fresh Express visits nine stops within the region shown in
Fig. 3 while the improved routes in the second panel earns $170

Table 2
Operational results of vehicle route solutions by covering radius.
Radius

Travel cost

Revenue earned

Net reward

0m
200 m
400 m
600 m
800 m
1200 m
1600 m

$135.91
$135.91
$145.49
$139.03
$134.94
$135.45
$135.49

$872.05
$872.05
$875.35
$873.77
$869.41
$872.38
$879.26

$736.14
$736.14
$729.86
$734.73
$734.47
$736.93
$743.76

162

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

Fig. 3. Current and heuristic vehicle routes.

more per week and still visit six of the same stops. Hence, the solution is plausible as it won’t drastically deviate from the current set
of customers and the time windows ensure that all current customers at each location will still visit the retailer.

5. Discussion
One of the most recent alleviation strategies for improving food
desert conditions is to introduce a mobile retailer into the community. This type of food retailer has recently increased in popularity
with examples in over a dozen North American cities. The growing
support for this retail format is attributable to the unique benefits
a mobile retailer can provide an urban food desert community.
These benefits include the mobile retailer’s unique ability to aggregate community demand, to easily modify its routing and stocked
product mix in response to changes in demand, and to serve as a
platform for other community-level interventions. Even with these
advantages, existing mobile retailers have faced challenges with
respect to reaching economical sustainability. Hence, research is
needed to determine if mobile retailers are a feasible alleviation
technique. The advantage of such research is that any tools used
in the research effort can then be employed to aid the decision
making of existing and future mobile retailers.

In total, it was observed that there are decision making points at
each of the three traditional decision making epochs but the two at
the operational level were identified as the most critical to current
mobile retailer operations. The first of these two operational decisions is the stocked product mix of the retailer. It was identified
that the objective of this decision is to offer the healthiest, lowcost food items while meeting a specified profit margin and not
exceeding the space of the retailer. The second of the operational
decisions is the routing/scheduling plan for the mobile food retailer. This type of problem is a combination of two classical problems: maximal coverage and vehicle routing. No applicable
formulation and solution to these problems exists which indicates
novel research is needed to address these issues. Research into
such problems should ensure any developed tools are usable by
mobile food retailers. Therefore, solution methods must not feature expensive software, complicated data input requirements,
and convoluted output reports.
A preliminary case study focusing on an underserved Phoenix
community was conducted. This study expanded on both the product mix and routing plan of the retailer and identified that different operational decisions can result in higher chances of economic
stability. Specifically, by varying the focus between consumer cost
and product health, a wide array of product mixes were developed
which have daily profits between $150 and $300. Between these

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

mixes, 81 out of the 95 products were included and many plausible
product offerings were created. With respect to the routing plan of
the retailer, a covering vehicle routing problem was solved to create seven different plans. Even for the most conservative assumption when no covering was allowed, a routing plan was still
developed which has a higher average revenue than the current
best weekly revenue found in practice.
There is significant research still necessary for both the problems discussed in this article. With respect to the product mix
problem, the biggest algorithmic improvement is to develop a
basic solution method which does not require commercial software. For this preliminary work, CPLEX was used to solve the formulation and it is assumed mobile food retailers will not have
access to such tools. Hence, heuristic or exact solution methods
are needed which do not utilize commercial software. Furthermore, it is recommended that a broader set of products be considered to expand upon the case study. Specifically, products which
are not fruits and vegetables should be included for consideration
to study the economic possibilities from including less healthy
food items. Such items were not included at this time as the data
was not currently available and a thorough study would be
required to find all possible items and measure their demands.
For the routing problem, other algorithmic solution methods
could be investigated to determine if other techniques exist which
provide better results than the Tabu Search methodology
employed in this research. With respect to studying mobile retailer
feasibility, more locations would have to be identified for service.
For this case study, only the current demand points of Fresh
Express were considered. However, there are other schools, community centers, commercial areas, and housing developments
which are not currently visited by the retailer but meet the qualities of being underserved and low-income. Future work would be
recommended to not only identify such locations, but to also estimate how much demand could be captured at such locations. By
creating a denser network, the covering formulation and its solution becomes more viable and will likely identify significantly
improved routes.
Finally, once more thorough data is obtained, a sensitivity analysis can be performed to understand the community conditions
necessary for mobile food retailers in that particular area to reach
economic sustainability. For instance, product demand and population density can be studied to estimate how dense the urban network must be for the retailer to have access to sufficient revenue
streams. Furthermore, a better defined network and more food
items will allow a study to be conducted on the feasibility of only
stocking healthy food items such as fresh fruits and vegetables.
Such research is needed to not only guide alleviation efforts within
urban communities, but to also aid the current planning framework of mobile retailers.
Acknowledgments
The authors would like to acknowledge Dr. Ed Mendez, Elyse
Guidas, and Don Keuth whose support made this research possible.
We also acknowledge the comments and suggestions from two
anonymous referees which greatly improved the quality of this
research.
References
Akinc, U., & Srikanth, K. (1992). Optimal routing and process scheduling for a mobile
service facility. Networks, 22(2), 163–183.
Algert, S., Agrawal, A., & Lewis, D. (2006). Disparities in access to fresh produce in
low-income neighborhoods in Los Angeles. American Journal of Preventive
Medicine, 30(5), 365–370.
Alkon, A. H., Block, D., Moore, K., Gillis, C., DiNuccio, N., & Chavez, N. (2013).
Foodways of the urban poor. Geoforum, 48(1), 126–135.

163

Alwitt, L., & Donley, T. (1997). Retail stores in poor urban neighborhoods. The Journal
of Consumer Affairs, 31(1), 139–164.
Andreyeva, T., Blumenthal, D., Schwartz, M., Long, M., & Brownell, K. (2008).
Availability and prices of foods across stores and neighborhoods: The Case of
New Haven Connecticut. Health Affairs, 27(5), 1381–1388.
Bader, M., Purciel, M., Yousefzadeh, P., & Neckerman, K. (2010). Disparities in
neighborhood food environments: Implications of measurement strategies.
Economic Geography, 86(4), 409–430.
Baker, E., Schootman, M., Barnidge, E., & Kelly, C. (2006). The role of race and
poverty in access to foods that enable individuals to adhere to dietary
guidelines. Preventing Chronic Disease, 3(3), A76.
Balsam, A., Webber, D., & Oehlke, B. (1994). The Farmers’ market coupon program
for low-income elders. Journal of Nutrition for the Elderly, 13(4), 35–42.
Beaulac, J., Kristjansson, E., & Cummins, S. (2009). A systematic review of food
deserts, 1966–2007. Preventing Chronic Disease, 6(3), 1–10.
Bertmann, F. M. W., Ohri-Vachaspati, P., Buman, M. P., & Wharton, C. M. (2012).
Implementation of wireless terminals at Farmers’ markets: Impact on SNAP
redemption and overall sales. American Journal of Public Health, 102(7), e53–e55.
Bitler, M., & Haider, S. J. (2011). An economic view of food deserts in the United
States. Policy Retrospectives, 30(1), 153–176.
Block, D., & Kouba, J. (2007). A comparison of the availability and affordability of a
market basket in two communities in the Chicago Area. Public Health Nutrition,
9(7), 837–845.
Bodor, J. N., Rice, J. C., Farley, T. A., Swalm, C. M., & Rose, D. (2010). The association
between obesity and urban food environments. Journal of Urban Health, 87(5),
771–781.
Bodor, J. N., Rose, D., Farley, T., Swalm, C., & Scott, S. (2008). Neighbourhood fruit
and vegetable availability and consumption: The role of small food stores in an
urban environment. Public Health Nutrition, 11(4), 413–420.
Bonanno, A., & Goetz, S. (2012). Food store density, nutrition education, eating
habits and obesity. International Food and Agribusiness Management Review, 15
(4), 1–26.
Califano, C., Gross, K., Loethen, L., Haag, S., & Goldstein, I. (2012). Searching for
markets: The geography of inequitable access to healthy & affordable food in the
United States. The Reinvestment Fund and Opportunity Finance Network.
Calvin, L., Cook, R., Dimitri, C., Glaser, L., Handy, C., Jekanowski, M., . . . Thornsbury, S.
(2001). U.S. Fresh Fruit and Vegetable Marketing: Emerging trade practices, trends,
and issues. US Department of Agriculture, Economic Research Service.
Caraher, M., Dixon, P., Lang, T., & Carr-Hill, R. (1998). Access of healthy foods: Part I.
Barriers to accessing healthy foods: Differentials by gender, social class, income
and mode of transport. Health Education Journal, 57(3), 191–201.
Carlson, A., Lino, M., Juan, W., Hanson, K., & Basiotis, P. P. (2007). Thrifty Food Plan,
2006. United States Department of Agriculture, Center for Nutrition Policy and
Promotion.
Caspi, C., Sorensen, G., Subramanian, S. V., & Kawachi, I. (2012). The local
food environment and diet: A systematic review. Health & Place, 18(5),
1172–1187.
Clifton, K. (2004). Mobility strategies and food shopping for low-income families: A
case study. Journal of Planning Education and Research, 23(4), 402–413.
Cook, R. (2011). Fundamental forces affecting U.S. fresh produce growers and
marketers. Choices, 26(4).
Cordeau, J. F., Laporte, G., & Mercier, A. (2001). A unified tabu search heuristic for
vehicle routing problems with time windows. The Journal of the Operational
Research Society, 52(8), 928–936.
Cummins, S., Petticrew, M., Higgins, C., Findlay, A., & Sparks, L. (2005). Large scale
food retailing as an intervention for diet and health: Quasi-experimental
evaluation of a natural experiment. Journal of Epidemiology and Community
Health, 59(12), 1035–1040.
Dimitri, C., Tegene, A., & Kaufman, P. R. (2003). U.S. Fresh Produce Markets: Marketing
channels, trade practices, and retail pricing behavior. United States Department of
Agriculture, Economic Research Service.
Drewnowski, A. (2005). Concept of a nutritious food: Toward a nutrient density
score. The American Journal of Clinical Nutrition, 82(4), 721–732.
Drewnowski, A., & Fulgoni, V. III, (2008). Nutrient profiling of foods: Creating a
nutrient-rich food index. Nutrition Reviews, 66(1), 23–39.
Duran, C. (2007). Panaderias, Peluquerias, y Carnicerias: Re-Mexicanizing the Urban
Landscapes of a Southwest City. The University of New Mexico.
Flamm, L. (2011). Barriers to EBT use at Farmers’ markets: Lessons in empowerment
evaluation from Rural Ohio. Journal of Hunger & Environmental Nutrition, 6(1),
54–63.
Flegal, K., Carroll, M., Kit, B., & Ogden, C. (2012). Prevalence of obesity and trends in
the distribution of body mass index among US Adults, 1999–2010. Journal of the
American Medical Association, 307(5), 491–497.
Food, Conservation, and Energy Act of 2008 (2008). H.R. 2419.
Garsetti, M., de Vries, J., Smith, M., Amosse, A., & Rolf-Pedersen, N. (2007). Nutrient
profiling schemes: Overview and comparative analysis. European Journal of
Nutrition, 46(2), 15–28.
Grace, C., Grace, T., Becker, N., & Lyden, J. (2007). Barriers to using urban Farmers’
markets: An investigation of food stamp clients’ perceptions. Journal of Hunger
& Environmental Nutrition, 2(1), 55–75.
Gustafson, A., Lewis, S., Moore, K., & Jilcott, S. (2013). Food venue choice, consumer
food environment, but not food venue availability within daily travel patterns
are associated with dietary intake among adults, Lexington Kentucky 2011.
Nutrition Journal, 12(1), 17.
Halper, R., & Raghavan, S. (2011). The mobile facility routing problem.
Transportation Science, 45(3), 413–434.

164

C. Wishon, J. Rene Villalobos / Computers & Industrial Engineering 91 (2016) 154–164

Haynes-Maslow, L., Parsons, S., Wheeler, S., & Leone, L. (2013). A qualitative study of
perceived barriers to fruit and vegetable consumption among low-income
population, North Carolina, 2011. Preventing Chronic Disease, 10(E34).
Kaufman, P., MacDonald, J., Lutz, S., & Smallwood, D. (1997). Do the Poor Pay More
for Food? Item selection and price differences affect low-income household food
costs. Monographs of the society for research in child development. Food and Rural
Economics Division, Economic Research Service.
Larsen, K., & Gilliland, J. (2008). Mapping the evolution of ‘‘Food Deserts” in a
Canadian City: Supermarket Accessibility in London, Ontario, 1961–2005.
International Journal of Health Geographics, 7(16).
Larsen, K., & Gilliland, J. (2009). A Farmers’ market in a food desert: Evaluating
impacts on the price and availability of healthy food. Health & Place, 15(4),
1158–1162.
Leone, L. A., Beth, D., Ickes, S. B., Macguire, K., Smith, R. A., Tate, D. F., & Ammerman,
A. S. (2012). Attitudes toward fruit and vegetable consumption and Farmers’
market usage among low-income north Carolinians. Journal of Hunger &
Environmental Nutrition, 7(1), 64–76.
Lopez, R. (2007). Neighborhood risk factors for obesity. Obesity, 15(8), 2111–2119.
McGuirt, J. T., Jilcott, S. B., Liu, H., & Ammerman, A. S. (2011). Produce price savings
for consumers at Farmers’ markets compared to supermarkets in North
Carolina. Journal of Hunger & Environmental Nutrition, 6(1), 86–98.
McKinnon, R., Reedy, J., Morrissette, M., Lytle, L., & Yaroch, A. (2009). Measures of
the food environment: A compilation of the literature, 1990–2007. American
Journal of Preventive Medicine, 36(4 Suppl), S124–S133.
Moore, L., & Diez Roux, A. (2006). Associations of neighborhood characteristics with
the location and type of food stores. American Journal of Public Health, 96(2),
325–331.
Morland, K., Wing, S., & Diez Roux, A. (2002a). The contextual effect of the local food
environment on residents’ diets: The atherosclerosis risk in communities study.
American Journal of Public Health, 92(11), 1761–1767.
Morland, K., Wing, S., Diez Roux, A., & Poole, C. (2002). Neighborhood characteristics
associated with the location of food stores and food service places. American
Journal of Preventive Medicine, 22(1), 23–29.
Ohls, J., Ponza, M., Moreno, L., Zambrowski, A., & Cohen, R. (1999). Food stamp
participants’ access to food retailers. Princeton, NJ: Mathematica Policy Research
Inc..
Powell, L., Auld, M. C., Chaloupka, F., O’Malley, P., & Johnston, L. (2007a).
Associations between access to food stores and adolescent body mass index.
American Journal of Preventive Medicine, 33(Suppl), S301–S307.
Powell, L., Slater, S., Mirtcheva, D., Bao, Y., & Chaloupka, F. (2007b). Food store
availability and neighborhood characteristics in the United States. Preventive
Medicine, 44(3), 189–195.
Racine, E. F., Smith Vaughn, A., & Laditka, S. B. (2010). Farmers’ market use among
African–American women participating in the special supplemental nutrition

program for women, infants, and children. Journal of the American Dietetic
Association, 110(3), 441–446.
Rose, D., Bodor, J. N., Swalm, C., Rice, J., Farley, T., & Hutchinson, P. (2009). Deserts in
New Orleans? Illustrations of urban food access and implications for policy. Ann
Arbor, MI: University of Michigan National Poverty Center/USDA Economic
Research Service Research.
Rose, D., & Richards, R. (2004). Food store access and household fruit and vegetable
use among participants in the US Food Stamp Program. Public Health Nutrition, 7
(8), 1081–1088.
Ruelas, V., Iverson, E., Kiekel, P., & Peters, A. (2012). The role of Farmers’ markets in
two low income, urban communities. Journal of Community Health, 37(3),
554–562.
Sigurdsson, V., Larsen, N. M., & Gunnarsson, D. (2014). Healthy food products at the
point of purchase: An in-store experimental analysis. Journal of Applied Behavior
Analysis, 47(1), 151–154.
Ver Ploeg, M., Breneman, V., Farrigan, T., Hamrick, K., Hopkins, D., Kaufman, P., . . .
Tuckermanty, E. (2009). Access to affordable and nutritious food: Measuring and
understanding food deserts and their consequences report to congress. USDA
Economic Research Service.
Walker, R., Keane, C., & Burke, J. (2010). Disparities and access to healthy food in the
United States: A review of food deserts literature. Health & Place, 16(5),
876–884.
Weatherspoon, D., Oehmke, J., Dembélé, A., Coleman, M., Satimanon, T., &
Weatherspoon, L. (2013). Price and expenditure elasticities for fresh fruits in
an urban food desert. Urban Studies, 50(1), 88–106.
Widener, M., Metcalf, S., & Bar-Yam, Y. (2012). Developing a mobile produce
distribution system for low-income urban residents in food deserts. Journal of
Urban Health, 89(5), 733–745.
Widener, M., Metcalf, S., & Bar-Yam, Y. (2013). Agent-based modeling of policies to
improve urban food access for low-income populations. Applied Geography, 40
(1), 1–10.
Windmoeller, M. (2012). Mobile Markets. Community commons. Retrieved from
<http://www.communitycommons.org/2012/11/mobile-markets-one-fooddesert-solution/> (03/17/2014).
Wrigley, N., Warm, D., & Margetts, B. (2003). Deprivation, diet, and food-retail
access: Finding from the Leeds ‘‘Food Deserts” study. Environment and Planning
A, 35(1), 151–188.
Zenk, S., Schulz, A., Kannan, S., Lachance, L., Mentz, G., & Ridella, W. (2009).
Neighborhood retailer food environment and fruit and vegetable intake in a
multiethnic urban population. American Journal of Health Promotion, 23(4),
255–264.
Zepeda, L. (2009). Which little piggy goes to market? Characteristics of US Farmers’
market shoppers. International Journal of Consumer Studies, 33(3), 250–257.

Ann Oper Res (2011) 190:339–358
DOI 10.1007/s10479-009-0614-4

A tactical model for planning the production
and distribution of fresh produce
Omar Ahumada · J. Rene Villalobos

Published online: 1 September 2009
© Springer Science+Business Media, LLC 2009

Abstract We present an integrated tactical planning model for the production and distribution of fresh produce. The main objective of the model is to maximize the revenues of a
producer that has some control over the logistics decisions associated with the distribution of
the crop. The model is used for making planning decisions for a large fresh produce grower
in Northwestern Mexico. The decisions obtained are based on traditional factors such as
price estimation and resource availability, but also on factors that are usually neglected in
traditional planning models such as price dynamics, product decay, transportation and inventory costs. The model considers the perishability of the crops in two different ways, as
a loss function in its objective function, and as a constraint for the storage of products. The
paper presents a mixed integer programming model used to implement the problem as wells
as the computational results obtained from it.
Keywords Production and distribution planning · Perishable products · Linear
programming

1 Introduction
In recent years, a renewed interest on the application of advanced planning tools for fresh
agricultural supply chains has emerged. However, adapting existing planning techniques to
fresh agricultural supply chains is a task whose complexity is compounded by the perishable
nature of these products. Among the critical issues in the planning of growing perishable
products we can mention the long supply lead times, the short shelf lives, as well as significant supply and demand uncertainties (Lowe and Preckel 2004). These peculiarities call for
planning models that incorporate decisions regarding harvesting, market access, method-ofsale, logistics, vertical coordination, and risk management (Epperson and Estes 1999). In
this paper we present an integrated modeling approach for the tactical planning of the production and distribution of perishable agricultural products. Tactical decisions include those
O. Ahumada · J.R. Villalobos ()
Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287-5906, USA
e-mail: Rene.villalobos@asu.edu

340

Ann Oper Res (2011) 190:339–358

that have to be made at the beginning of the season such as what to sow, when to sow it,
and what potential markets to target. In particular, in this paper we are concerned with the
planning problem for an individual grower of fresh produce who would like to maximize his
revenues and avert the risks associated with an uncertain market. Some of the factors to consider are the crops’ prices, demand, yields and labor availability. The model to be presented
handles decisions such as detailed planting plans, labor requirements, rough estimates of
distribution planning, harvesting and growing policies.
The underlying motivation of the work to be presented is to assist the producers in the increasingly complex and competitive fresh agricultural industry. This industry requires large
investments in technology, facilities and labor; while being subjected to highly variable crop
yields and market prices, thus making planning tools, like the ones we present in this research, a necessity for the long term survival and profitability within this industry. The paper
applies the proposed model to the case of the Mexican export-oriented fresh produce industry. This particular application deals with the emerging requirements of the fresh produce
industry, such as contracted production and more distribution activities.
In the reminder of this paper we first present a review of the literature (Sect. 2); the description and development of the tactical model in Sect. 3; the case study of export oriented
Mexican growers in Sect. 4; and, finally, in Sect. 5 we present our conclusions and plans for
future research.

2 Literature review
Traditionally, agricultural planning models have incorporated a variety of decisions, such as
crop planning, machine scheduling and harvest planning, among many others (Glen 1987;
Lowe and Preckel 2004). The majority of existing planning models has focused on traditional crops, with a limited number of them focusing on highly perishable products such as
fresh produce. However, the rate of development for highly perishable crops has been increasing in recent years; especially for the planning of high value crops, such as flowers and
horticultural products that require large investments in technology and labor (Ahumada and
Villalobos 2009). In this section, we present some of the most relevant contributions in the
area of tactical planning of perishable crops, including crop planting, harvesting and transportation planning. From the large number of papers available, we focus mainly on those
models that have been successfully implemented.
From the perspective of tactical planning, Van Berlo (1993) developed a goal programming model to plan and coordinate the production and supply of raw materials from the
field to a processing plant, in a vertically integrated vegetable processing industry with final
products in the form of cans of peas. A related contribution is the work of Hamer (1994) who
developed a model that allowed the simultaneous selection of different varieties of Brussels’
sprouts and the planting and scheduling decisions to assure a steady supply of quality and
size of Brussels sprouts demanded from the customers over the planning horizon.
More recently, other applications of tactical planning have emerged, such as the work
by Caixeta-Filho et al. (2002). These authors use an LP model for planning the production
of flowers in a Brazilian greenhouse. The main decision variable is the number of flowers
to produce in each specific greenhouse at a particular time period. Some of the constraints
of the model include the amount to harvest and plant for each period. The objective of
the model is to satisfy the demand of customers while maximizing revenue. The reported
benefits of using this model are additional sales and profits, with an estimated 32% increase
in the farmers’ profit margin.

Ann Oper Res (2011) 190:339–358

341

Other models handle only parts of the tactical plan, such as harvesting or transportation.
An example of this type of model is given by the work of Allen and Schuster (2004), who
developed a model for calculating the capacity and rate of harvest required for grape production. The objective of the model is to minimize the losses in crops, caused by weather,
and to minimize overinvestment costs resulting from installing excess capacity. A major
contribution of the paper is the use of nonlinear programming to reduce the risk of uncertain weather. The benefits reported from the use of this model include $2 million in capital
avoidance from reduced investment on harvesting equipment and improved risk assessment
for the incorporation of new crop areas. A second application in grape harvesting is given by
the work of Ferrer et al. (2008) who developed an MIP model for optimally scheduling the
harvesting operations of wine grapes with the objective of reducing total costs. The model
considers the costs of harvesting activities and the loss of quality of the grapes for delaying
harvesting. The decisions in the model include the amount of grapes to harvest from the different plots in each period, the routing of harvesting among plots and the number of workers
to hire or lay off for each period of the harvesting season. The authors estimated the benefits
of the use of the model at a 27% decrease in operational cost and a 16% reduction in labor
costs.
Two recent contributions in the area of transportation planning for perishable crops include the works by Faulin (2003) and Osvald and Stirn (2008). Faulin describes the implementation of an algorithm for planning the routes of a company handling frozen vegetables.
The objective is to improve the efficiency metrics of the fleet and reduce the costs incurred.
One drawback of this model is that it did not consider the perishable nature of the products
being transported. In contrast the work of Osvald and Stirn considers the perishable nature
of the products by using a linear loss function that is dependent on the time spent during
transportation. The objective of the model is to minimize the distribution costs, through the
costs of vehicles, total distance traveled and the loss of quality of the products. The authors
estimate that there are significant reductions in overall costs (27% or more) by using their
model.
From the review of the available literature, we are able to draw the conclusion that there
are no models that consider production and distribution decisions in an integrated way. Another finding is that there are considerable benefits from applying advanced planning techniques to the supply chain of fresh agricultural products. The benefits reported in the literature include an increase in profit margins, reductions in operational costs and better planning
of activities at the tactical and operational level. Given the size and recent consolidation of
the fresh produce industry, if only a small fraction of the reported benefits are realized, this
would more than cover any effort or investment required to develop, run and maintain the
decision tools needed for the planning of perishable products.
In the next section we present a general model that meets the particular needs of fresh
agricultural producers. One of the contributions of the model presented here is that it integrates planting, harvesting and distribution decisions into a single tactical model. Traditionally, these decisions have been addressed individually. A second contribution is the inclusion
of the perishable nature of fresh produce in the model.

3 Tactical planning model
In the fresh produce industry, vertically integrated producers are called grower/shippers and
they form an increasingly important component of the supply chain of fresh horticultural
products. Grower/shippers face changing market conditions due to the consolidation of supermarket chains and the new purchasing policies implemented by those chains (Dimitri

342

Ann Oper Res (2011) 190:339–358

et al. 2003). Furthermore, it is expected that the consolidation of supermarket chains will
continue, or even increase, in the near future, resulting in large customers who will demand
a closer relationship with growers and year round supply of fresh produce (Kaufman et al.
2000). The producers that have the possibility of efficiently accessing directly the final market (grower-shippers) can benefit from the planning tools presented in this paper since they
must design and coordinate their supply chain to meet the new realities of the market. In
fact, we believe that only those growers/shippers with access to the appropriate advanced
planning tools will be effective players in the new fresh supply chain.
3.1 Planning activities and labor requirements
As an example of the type of seasonal planning problems in the industry, we provide the
following description of the planning environment surrounding Mexican open-field growers, who produce a significant portion of the fresh tomatoes and peppers consumed in the
US market. These growers and their practices are representative of the horticultural industry in North America. Commonly, fresh produce growers make several decisions regarding
production and planting. Planting decisions include the selection of the best variety, the selection of the total area to plant of each variety and when to plant the selected varieties.
These decisions are, for the most part, made based on forecasts of the demand and prices
expected in the season ahead. The effects of planting decisions affect the growing season,
which ranges between 6–12 months, depending on the particular strategy of the producer
(Thompson and Wilson 1997).
As an example of a typical plan, in Table 1 we present a list of activities represented on
weekly buckets. The major activities in the production plan (Table 1) start with the planting
of crops immediately followed by cultivation, and later, by harvesting activities. In Table 1
it can also be observed that the harvesting plan can last several weeks. From the planning
program depicted in Table 1 it can also be observed that during the growing season, the cultivating and harvesting activities overlap significantly. One of the implications of this overlap
is that these two activities use a commonly scarce resource, i.e., manual labor. This implies
the need for growers to plan and coordinate their labor to handle peak labor requirements.
The main issue with labor planning is that seasonal agricultural labor is in short supply,
and given the large requirements of fresh produce production, labor can become the most
restrictive resource for planning. For this reason, our model accounts for the availability
of seasonal and temporary labor. The difference among these two is that seasonal laborers
are under contract for the entire or most of the season. Hiring seasonal laborers requires a
certain minimum amount of work per week, since if the work falls below the agreed upon
minimum, laborers are paid a predetermined minimal wage. On the other hand, temporary
laborers can be hired weekly or daily as needs arise, but at a premium. The number of both
Table 1 Typical program for growing tomatoes and bell peppers in hectares (ha)

Ann Oper Res (2011) 190:339–358

343

types of laborers is restricted by the supply of seasonal laborers and by the competition from
other growers for the scarce labor.
For the case study to be presented in this paper, labor requirement data was obtained
from interviews with growers with operations in North America. From the data gathered,
we found out that there are similarities in the labor requirements for planting and cultivation
of tomatoes and peppers. But the labor demands from these crops differ significantly at
the time of harvest, for which tomatoes usually require twice the number of workers than
peppers. An expected benefit of the proposed model is that the decisions regarding crops
and labor are planned in an integrated way, instead of planning the planting crops first and
then labor requirements, which is common practice in the industry.
3.2 Handling the planning of perishable crops
One of the main differences between traditional agricultural models and the model we propose, is the management of perishability. Our model handles the decay of fresh products
in two different ways. First, by limiting the maximum storage time allowed, which is crop
dependent (shelf life), and by incorporating metrics that can be used as surrogates for freshness of the product throughout its traversal of the supply chain. For example, in the case of
tomatoes, the color of the fruit can be used to determine the freshness.
Invoking constraints that limit shelf life serves to comply with any proposed productiondistribution plan according to the acceptable characteristics of the crop. However, these
constraints do not encourage the freshness of the products, since as long as products are
within the specifications, the model does not penalize for delivering close to the expiration
date. This could lead to some odd decisions, such as preferring the use of railroad over
trucks for highly perishable items, since railroads usually offer a lower transportation cost.
To avoid these problems, the objective function is modified to include the cost of inventory
lost while being transported, which can be modeled using a decay function (Nahmias 1982;
Raafat 1991). This function represents the physical decay of the products, or opportunity
cost, for shelf life reduction, if we assume the price received by the growers is dependent
on the quality and freshness of the products (Goyal and Giri 2001). To model the decay in
the products, a linear function is proposed to reduce the value of the products according to
the length of transportation time; this function is similar to that proposed by Osvald and
Stirn (2008). Figure 1 shows the decay function used for peppers, which decreases the value
of the product at a constant rate over time until the shelf life of the product is reached.
A similar decay function is used for tomatoes (e.g. shorter shelf life). Such a function was
previously suggested for the case of fresh produce (Fujiwara and Perera 1993), and for
Yoghurt production (Lutke Entrup et al. 2005).
Fig. 1 Loss function for peppers

344

Ann Oper Res (2011) 190:339–358

The model to be presented in this paper will incorporate into the planning process both
types of perishability considerations just presented; i.e., restrictions of maximum elapsed
time between the harvest and reception of the products by the buyer, and the loss of value of
the products by delaying their delivery to the customer.
3.3 Formulation of the model
Producers of fresh agricultural products have different needs according to the particular
characteristics of their business. The integration of the grower in the value chain is one of
the main drivers of the coordination between production and distribution. In this regard,
some producers leave the marketing and distribution of their products to brokers and other
intermediaries, while other producers market and distribute their own products to obtain a
larger share of the total value chain. Figure 2 presents a simplified schematic of the different
transactions in the fresh-produce supply chain as seen by a grower/shipper.
According to this schematic, grower-shippers may have several locations for planting
their crops (L1–L3). These locations could be significantly different from each other in
terms of their geographical position or existing infrastructure such as greenhouses and other
protected agricultural facilities. Growers can also have one or more packing facilities to
process their crops (P1, P2), warehousing facilities to store the crops (W1, W2) and distribution centers (D1, D3) to deliver products to customers (C1–C3). In this context, the purpose
of the formulation to be presented is to provide a basic model that can be easily adaptable to
the needs of the different fresh agricultural producers.
In this paper we assume that the distribution of demand, yield and market prices can be
represented by their respective expected values. We also assume that the basic infrastructure
(land, warehouse facilities, etc.) is already in place and its capacity has already been set;
thus, we assume that strategic decisions have already been made and we focus only on tactical level planning. Consequently, we assume that the growing locations (L1–L3) and their
production characteristics are already defined. However, the total amount to plant in each
location is a tactical decision variable limited by the maximum amount of land available.
The main decisions in the tactical model involve when and how much to plant of each
crop, when to harvest and sell the crops, the labor resources to contract and the transportation
mode to use to deliver the product to the different customers. For each one of the customers
we assume that the total amount of product to deliver exceeds a certain minimum, which
varies, depending on the supply contracts between the growers and their customers. Other
decisions include the storage of products for later use and the selection of facilities from
which to ship the products. In addition, the model selects the best transportation mode based
on costs, the loss of quality during transit, the maximum delivery time allowed by the customer and other service requirements. For these transportation decisions we assume, since
transportation is usually hired from third-party providers, unlimited transportation capacity.
Fig. 2 Grower-shippers’
transactions

Ann Oper Res (2011) 190:339–358

345

The objective of the model is to maximize the expected revenue for the grower. This
revenue is given by the combination of external conditions, which are out of the farmer’s
control, such as expected market prices, and those determined by the farmer himself, such
as when, what and how much to plant and harvest in a given season. For this model all
planning periods (t ) are assumed to be finite, discrete, and defined in multiples of weeks
(see Appendix A). The periods in which planting is feasible are represented by the subset
TP, and those periods in which it is feasible to harvest are represented by the subset TH.
A particular product (k) is formed by the combination of the crop included, the size of the
fruits and the quality of the crop; this implies that for any given crop there could be multiple
products. For instance, the crop “tomato” can be segregated into different qualities and sizes
that correspond to different products. Other discrete sets represent the options available for
transportation modes (r), packing facilities (f ), customers (i), and warehouses (w). The
tactical model is in essence a multiproduct production and distribution problem formulated
as a mixed integer program (MIP) whose constraints are:

j

Plantpj l ≤ Totlandl


j

p

(1)

p

Cplantj l · Plantpj l ≤ Totinvest

(2)

Waterj · Plantpj l ≤ Totwater

(3)

l


j

all l where l ∈ L, p ∈ TP(j, l) and j ∈ J

p

l

Minj · Yjp ≤ Plantpj l ≤ Maxj · Yjp

all j, p and l

(4)

Harvestphj l = Yieldphj l · Totalpj l · Plantpj l all p, h, j, l where h ∈ T H (j, l)

SPphj lf = Harvestphj l all p, h, j, l, where f ∈ P F

(5)
(6)

f

MenLtl + MenT tl ≥



MenPptj · Plantpj l

pj

+



MenH phj · Harvestphj l

all t, l where h = t

(7)

pj l

Hiretl + Firetl = MenLtl − MenLt−1l
Hiretm l + MenLtm −1l ≥ MenLtl

all l and t < tm

all l and tm ≤ t

Hiretl ≤ Mfix, MenT tl ≤ Maxtemp all t, l

Packhkqf = Colorhkq
(1 − Salvphj l )SPphj lf · Prodphj k /Weightk

(8)
(9)
(10)

phj k

all h, f, k where k ∈ K(j )

HourF hf ≥
HourLk · Packhkf

(11)
all h, f where t = h

(12)

k

SK hj =


l

p

Salvphj l · Harvestphj l

all j, h

(13)

346

Ann Oper Res (2011) 190:339–358

Packhkqf =



SCt1 kqf ir +



i

SPDht2 kqf dr +



SPW ht3 kqf wr

w

d

all h, f, k where t1 = h + Timef ir , t2 = h + TimePW f wr , t3 = h + TimePDf dr (14)

Invwhtkqw = Invwht−1kqw +
SPW htkqf wr
−



f

SW ht4 kqwir −



i

SWDht5 kqwdr + Ztkw

d

all t >= h, k, w where t4 = t + TimeW wir , t5 = t + TimeWDwdr



SPDhtkqf dr −
SDht6 kqdir +
SWDhtkqwdr
Invdhtkqd = Invdht−1kqd +
f

i

w

all t >= h, k, d where t6 = t + TimeDdir

Invdhtkqd /Palletk ≤ Dcapd all t, d
h

k

h

k




Invwhtkqw /Palletk ≤ Wcapw

Packhkqf ≤ PFcapf

k


f

SCtkqf ir +


h

all t, w

all h, f

SW htkqwir +

w

(16)
(17)
(18)
(19)


h

SDhtkqdir = Demtki

d

all t, k, i, where t ≤ h ≤ t − SLk and q ≤ Qmaxi
Timef ir · TCtkf ir ≤ LTi

(15)

all t, k, f, i, r

(20)
(21)

TimeW wir · TW tkwir ≤ LT i

all t, k, w, i, r

(22)

TimeDdir · TDtkdir ≤ LTui

all t, k, d, i, r

(23)

Constraints (1)–(4) represent the main resources limiting the operations. Usually these resources are the result of strategic decisions such as total water and land available. In particular, constraints (1), (2) and (3) make sure that the resources used by a solution do not
exceed the total availability of land, capital and water. Constraint (4) invokes the lower and
upper bounds for the planting area for each crop per period. This is determined based on the
farmers’ previous commitments such as predetermined contracts with buyers. Constraint (5)
limits the quantity to harvest to the hectares planted. Since some crops can be harvested for
multiple periods, Yieldphj l provides the expected yield of harvest for a given planting (p)
and harvesting period (h), and Totalpj l provide the expected yield for all the weeks for those
crops planted in period p. Equation (6) is a transportation restriction that directs the harvested crops to the most convenient packaging facility according to their transportation cost.
Equation (7) ensures that there are enough workers available for planting and harvesting
during the time period. Equation (8) handles the hiring decision for seasonal workers, who
can be hired incrementally according to the labor requirements. There is also a restriction
on the last time period to hire these workers (9), since, after period tm , the hiring of workers dwindles. There is also a restriction on the maximum number of seasonal and temporal
workers to hire (10).

Ann Oper Res (2011) 190:339–358

347

Constraint (11) models the packing of crops which change from weight units to packaging units at the packing facilities. Equation (12) ensures there are enough workers for
processing the required products at the packing facility. In constraint (13), we estimate
the percentage of crops that need to be salvaged because of substandard quality. In (14)
the products are sent to storage in warehouses (SPW) and DCs (SPD) or directly to customers (SC). Constraint (15) provides conservation of flow for the warehouses, which can
receive cargo from packing facilities (SPW) and ship to DCs (SWD) or directly to customers
(SW). Equation (16) is the conservation of flow restriction for DCs, which can only ship
to customers (SD) or receive cargo from warehouses (SWD) and packing facilities (SPD).
Equations (17), (18) and (19) are capacity restrictions for warehouses and DCs and packing facilities. Equation (20) assures that customer demands are satisfied, using products that
have been harvested in a period that observes their maximum storage time and that comply
with the maximum color (Qmax) accepted by the customer. Constraints (21) to (23) restrict
the lead-time of the last leg of transportation to be less than a predetermined delivery time
imposed by the customers. The size of the problem described is multiplicative in nature and
depends on the number of crops to grow, the planning periods and the options for transportation modes and final destinations, among some others. In the next section we present some
experiments regarding different crops and distribution requirements to demonstrate how it
affects the size of the problem to solve.
The objective of the MIP model is to maximize profits. The first part of the objective
function represents the revenue obtained by prices of the products shipped to the customers,
plus the revenue obtained by salvaging the unshipped crops minus the costs of transportation, planting, harvesting, holding and purchasing products from other vendors. This first
part of the objective function is given by:





Pricetki
SCtkf ir +
SW htkwir +
SDhtkdir
Max
tki

+



f





CT f ir SCtkqf ir −

tkqf ir

−








CTPDf dr SPDhtkqf dr −

−



CTWDwdr SWDhtkqwdr

htkqwdr

(Ccasek + Coperk )Packhf k −
CLabor · MenLtl −



pj l

−

CTPW f wr SPW htkqf wr

htkqf wr

f hk

−

CTW wir SW htkqwir



CTDdir SDhtkqdir −

htkqf dr

−

d

htkqwir

htkqdir

−

h

Psalvagej SK hj

hj

−

w

h



Cplantj l Plantpj l

pj l

Chire · Hiretl −

tl



Ctemp · MenT tl

tl


1 
CLabor · HourLtf −
Chwkw Invwtkw
40 tf
tkw

tkd

Chdkd Invdtkd −

.

tkw

Pavgtk Ztkw

(24)

348

Ann Oper Res (2011) 190:339–358

The second part of the objective function is a loss function for the decay of the products
while being transported by the different transportation modes. This function is given by:

SCtkqf ir Pricetki Timef ir /SLk
−
tkqf ir

−



Pricetki TimeW wir SW htkqwir /SLk

htkqwir

−



Pavgtk TimePW f wr SPW htkqf wr /SLk

htkqf wr

−



Pavgtk TimePDf dr SPDhtkqf dr /SLk

htkqf dr

−



Pavgtk TimeWDwdr SWDhtkwdr /SLk

htkqwdr

−



Pricetki TimeDdir SDhtkqdir /SLk

(25)

htkqdir

Equation (25) is a linear function that penalizes the transportation time according to the
prices of the products (Fig. 1). Decay of the product is estimated based on the type of transportation link used. We use the value to the final customer (Pricetki ) for final delivery transportation links (SD, SW, and SC). For the rest of the transportation links we use the average
FOB (Free on Board) value of the crops (Pavgtk ).
3.4 Structure of the model and computational results
One potential issue in solving model (1)–(25) is that it can easily grow very large, given
the different options of crops, planting locations, harvesting decisions, and the behavior of
each of the potential markets. All these different combinations cause the size of the problem
to grow exponentially. The complexity of the problem is compounded by the planting and
variety selection decisions, which are modeled with integer variables (Wolsey 1998), since
each integer solution leads to a different planting and distribution structure. However the
structure of the problem has some characteristics that can render it suitable for the use of
decomposition methods. For instance, once the binary decision variables are set to particular
values, the combination of production, storage and transportation decisions in the present
model form a characteristic staircase pattern, which very often leads to the application of
computationally efficient decomposition methods. Another feature of the problem is that
the distribution decisions can be modeled as a multi-commodity flow problem. In particular,
transportation variables can be represented by a minimum cost network flow problem with
uncapacitated arcs. This particular property transforms the selection of the transportation
decisions into a minimum spanning tree problem (Ahuja et al. 1993). The selection of the
spanning tree is only dependant on the cost of transportation for available transportation
links, which are those that have not been precluded by lead time or shelf life restrictions.
In order to test the proposed model, we used several instances of the planning problem with different combinations of crops to plant and distribution structure. The model was
solved using CPLEX 10.0 on a Pentium 4 machine with 2GB of Memory. The factors considered in the evaluation were the number of crops (J ), periods (T ), production periods (TP),
harvest periods (TH), customers (I ), locations (L), plants (P ), warehouses (W ), DCs (D),

Ann Oper Res (2011) 190:339–358

349

Table 2 Results from the experiments
T

TP

8

30

8

15

8

30

8

16

8

40 18

26

J

TH I

D

K

H

Row

2

2

8

3

64,106

2

10

8

1

22,134 123,088 311,129 39,776 0.0%

2

2

2

8

3

L P

W

3

2

2

100

2

2

3

2

9,639

Col

Non

Binary Gap

71,242 233,055 12,304 0.0%
40,384 124,379 15,424 0.1%

Time
42
58
73

8

40 18

26

100

2

2

2

10

8

1

29,016 136,682 359,299 60,407 1.7% 3,600

35

40 18

26

3

2

2

2

2

8

3

25,222

35

40 18

26

100

2

2

2

10

8

1

44,271 184,814 561,660 60,893 0.0% 3,299

77,162 319,583 19,510 2.8% 3,600

products (K), and transportation modes (H ). The results obtained for each experiment are
presented in terms of CPU seconds (Time) and optimality gap (Gap) in Table 2. Other fields
in Table 2 are related to the problem size including the number of rows in the model (Row),
the number of variables (Col), the nonzero coefficients in the matrix (Non), and the number
of binary variables (Binary).
From the experiments performed, it is evident that the growth in binary variables is the
main contributor to the total processing time observed. In particular, those related to the
selection of crops (J ), which form a knapsack problem that is inherently hard to solve.
However, the source of the complexity of the problem might change if the resolution for the
decision variables is changed from weeks to days; then, the number planting and harvesting
variables would increase greatly, with a consequent increase of the running time and size
of the problem. A further reduction in running times could be explored by exploiting the
structure of the model to use decomposition algorithms, such as Bender’s decomposition to
decrease the processing time required by larger instances such as those with more distribution points, customers and products, which might make this planning problem very time
consuming. Exploration of other solution alternatives is left as future research. We now focus the validation of the proposed model on the impact of the results on the operations of a
grower/shipper.

4 Validation and case study
To validate our model, we prepared a plan for planting, harvesting, and distributing the crops
of a hypothetical producer who is based in the northwest region of Mexico and exports most
of its produce to the US winter market. The model results in a plan that covers the entire
growing season, from the time the crops are planted until the last day of harvest. We assume
that the grower produces two crops: green bell peppers and vine ripe tomatoes.
To estimate production yields from these crops, we used publicly available data (Table 3)
given by third party providers. This data is usually generated through field experiments ran
by universities and growers associations and performed every year to test new varieties that
come into the market (Arellano 2001). Table 3 indicates the expected yield (in pounds) for
four different varieties (A–D) of vine ripe tomatoes. The harvested fruit can have different
sizes (4 × 4, 4 × 5, as shown in Table 3) and qualities according to the classification of
USDA (1991).
To obtain the costs of labor, seeds, fertilizer and agrochemicals used, we use data from
a government study that interviewed producers in the State of Sinaloa (FIRA 2007). We
also gathered the transportation cost and the estimated travel time for the three main transportation modes within North America: truck, rail and air. This information was obtained

350

Ann Oper Res (2011) 190:339–358

Table 3 Yield per hectare of tomatoes
Variety

Pounds per hectare of tomato
4×4

4×5

5×5

5×6

6×6

6×7

Total

A

27,623

42,320

18,745

13,133

1,978

667

104,466

B

17,641

32,200

37,697

34,592

5,129

1,012

128,271

C

16,629

31,947

39,606

41,170

7,981

1,518

138,851

D

2,990

15,709

33,442

63,342

24,150

8,280

147,913

Table 4 Transportation data for Warehouse-DC links
Warehouse

DC

Trans Mode

Time (Weeks)

Cost/Pack

W1

D1

TRUCK

0.49

$1.73

W1

D1

AIR

0.14

$31.25
$1.25

W1

D1

RAIL

0.71

W1

D2

TRUCK

0.14

$0.50

W1

D2

AIR

0.14

$13.02
$0.40

W1

D2

RAIL

0.29

W2

D1

TRUCK

0.40

$1.39

W2

D1

AIR

0.14

$20.83
$0.81

W2

D1

RAIL

0.43

W2

D2

TRUCK

0.43

$1.24

W2

D2

AIR

0.14

$29.69

W2

D2

RAIL

0.71

$1.18

from public sources that include trucking transportation rates published by USDA (2006).
For the case of railroad and air transportation we used interviews with providers of these
services, these interviews were performed by Sanchez (2007). Table 4 presents a sample of
the transportation data, which includes the transportation links between the warehouses and
the distribution centers and the corresponding times and costs. We assume the warehouses
are located in Nogales (W1) and McAllen (W2) and the distribution centers are located in
Chicago (D1) and Los Angeles (D2). Time is given in weeks and the cost per pack is the
prorated cost for a given product (Ex. 1920 boxes per truck). We developed tables similar to
these for all combinations of transportation links.
A major decision presented by the model is the timing of planting and harvesting to
satisfy customer demand. In order to satisfy these requirements, we developed a matrix of
the potential planting dates and the expected harvesting days using the information provided
from Robles and Santana (1997) and Arellano (2001), which is presented in Table 5 for one
of the tomato crops (variety A), and a similar matrix is developed for all the varieties being
considered.
The market information include the FOB prices of the “shipping points” of crops (Table 6) published by USDA (2007), which provides information for several types of standardized packages for tomato and other crops (e.g. 4 × 4). In addition to the FOB prices, we
also gathered prices for two “terminal markets” (Chicago and Los Angeles), which we used
to represent other customers. For this example we gathered the information for the years

Ann Oper Res (2011) 190:339–358

351

Table 5 Plant and harvest matrix for Crop A
Week 15
1
2
3
4
5

16

17

18

19

20

21

22

23

24

25

26

27

29

29

0.01 0.07 0.17 0.17 0.20 0.04 0.10 0.05 0.07 0.04 0.08

1.00

0.01 0.07 0.17 0.17 0.20 0.04 0.10 0.05 0.07 0.04 0.08

1.00

0.01 0.07 0.17 0.17 0.20 0.04 0.10 0.05 0.07 0.04 0.08
0.01 0.07 0.17 0.17 0.20 0.04 0.10 0.05 0.07 0.04 0.08

Table 6 Price in dollars per box
of tomatoes (23 lbs per box)

Total
1.00

0.01 0.07 0.17 0.17 0.20 0.04 0.10 0.05 0.07 0.04 0.08

6

30

1.00
1.00

0.11 0.06 0.09 0.22 0.26 0.08 0.04 0.06 0.03 0.06 1.00

Year

4×4

4×5

5×5

5×6

1998

9.86

9.87

8.49

7.59

1999

7.71

7.71

6.59

5.93

2000

7.87

7.87

6.82

6.36

2001

9.54

9.54

7.59

6.73

2002

10.85

10.85

8.78

7.74

2003

10.01

9.93

8.40

7.71

2004

11.05

11.05

9.58

8.54

2005

13.40

13.40

11.55

10.61

Average

10.01

10.00

8.45

7.62

1998–2005 on a weekly basis and obtained the expected prices for the season by adjusting
prices for the inflation during those same years.
The proposed supply chain of the grower includes two locations for planting, a total of
area of 500 hectares divided in two locations, two packing facilities, two potential warehouses, three customers and two distribution centers. We also set the maximum crop storage
time, with the help of previous studies (Welby and McGregor 2004). For tomatoes, we set the
maximum storage life to one week; and for peppers, two weeks. We also set the maximum
cycle time for delivery to the final customer to be one week (both peppers and tomatoes),
which is a common service requirement for agricultural industries (Perosio et al. 2001).
We modeled the problem with the aid of the AMPL modeling language in conjunction
with CPLEX® Solver version 10.0. The planting schedule recommended by the model is presented in the Table 7. This table displays the crop selection, timing of planting and amount
of hectares of each crop to plant. Tomato varieties B and D (TCB and TCD) were selected
and the bell pepper variety D (CCD). According to these results, the growers should plant
28 hectares of bell peppers of variety D in the first week of production. Also relevant is the
distribution of the planting effort, which called for most of planting to occur in the early
periods of the planning horizon (Weeks: 1–6) and complementing the planting in the last
periods (16–17).
The model prescribes direct distribution to customers in terminal markets and FOB deliveries at warehouses in the border city of Nogales. Results suggest that by changing from
their current strategy (FOB sales) to one with direct deliveries to final customers, growershippers can increase their revenues by about 25%. These results are very promising, even
when considering the additional risk of handling perishable products for a larger period of
time than with FOB sales. As part of the validation of the model, we shared the resulting

352

Ann Oper Res (2011) 190:339–358

Table 7 Crop selection and
planting period

Week

Crops selected
CCD

TCB

TCD

Week total

1

28

–

–

28

3

–

114

–

114

4

–

20

–

20

6

–

–

29

29

7

22

–

24

46

11

–

85

–

85

12

23

–

–

23

13

–

47

–

47

16

20

–

–

20

18

–

88

–

88

Crop total

93

354

53

500

Table 8 Boxes of product shipped for the base case
From

Truck
C1

Warehouse

806,989

DC
Packhouse

1,045,116

Total

1,852,105

Air
C2

C3

C1

Rail
C2

C3

C1

C2

C3

112,198
21,538

226,193

133,736

226,193

7,799
7,799

planting and harvesting plans with growers, and they confirmed the feasibility of the recommendations.
The model also allows for targeted market segmentation, given the particular demands of
each customer and the corresponding prices they are willing to pay. For instance, according
to Table 8 the targeted production allocation should be 83% for FOB customers (C1), 10%
for Chicago (C3) and the remaining 7% for Los Angeles (C2). This table also shows that
trucking is the preferred transportation mode due to the lead time requirements and the
penalty costs proportional to the traveling time. For terminal markets the model suggests
using distribution centers for final delivery; but, for FOB sales, the preferred way is through
sales at warehouses located at shipping points. The growers we consulted confirmed that
these distribution patterns seem reasonable according to their experience in the different
markets.
To estimate the effects of perishability, we developed three experiments with the same
data, but eliminated some of the constraints and the second part of the objective function,
which relates to perishable items. The results from these changes can be observed in Table 9. Scenario 1 relaxes the crop color restriction; scenario 2 eliminates the decay function
and scenario 3 combines the two previous scenarios. From this table it is evident that the
transportation plan shifts from sending most of the shipments to the closest customer, to
customers located at further distances. The preferred transportation method also changes,
depending on the inclusion of the decay function.
An additional benefit of the mathematical models used, is the possibility of performing
sensitivity analysis. For example, it is possible to estimate the benefits of hiring more sea-

Ann Oper Res (2011) 190:339–358

353

Table 9 Comparison of results with different perishable features
Scenario

From

Truck

Rail

C1
1

Warehouse

1

DC

1

Packhouse
Total

2

Warehouse

2

DC

2

Packhouse

C2

774,860

C2

C3

82,839
716,021

1,405,305

82,839

716,021

363,188

378,765
987,464

363,188

3

Warehouse

442,623

3

DC

3

Packhouse

Table 10 Shadow prices of
hiring additional personnel

C1

630,445

Total

Total

C3

987,464

332,726
378,765

489,466

135,140

523,490

135,140

1,307,888

312,167
442,623

Scenario

312,167

156,740

784,398

Dual price
Seasonal (US$)

Temporal (US$)

1

7,040

5,400

2

9,280

6,883

3

10,302

7,246

4

7,161

3,837

sonal and temporary workers, as shown in Table 10. In this table the first three scenarios
are those from the previous table and scenario 4 is the current model with all of the perishability restrictions and functions. Table 10 presents the shadow prices for seasonal and
temporary workers, which indicates the maximum contribution, if it were possible to hire
extra workers. The contribution of seasonal workers is given by the extra production given
by an additional worker and by replacing the more expensive temporary workers in several
periods during the season. However, the contribution of temporary workers is only valid
for those periods in which the amount to harvest is constrained by the number of available
workers, which is small in the season. A similar analysis can be undertaken to determine the
robustness of the current results; for example, what are the benefits of using one variety over
another, or the benefits of having a distribution center or a warehouse in a certain region.
Such information can provide growers with better tools regarding the potential changes in
their current tactical plans.

5 Conclusions and future research
This paper presented a model to perform the tactical planning for a grower/shipper of fresh
produce. This model is our first attempt at tackling the complicated planning problem of
the management of the supply chain of fresh produce. As it demonstrates, the planning of

354

Ann Oper Res (2011) 190:339–358

fresh produce not only requires the selection and timing of crop planting, but also the decay
of fresh products and labor management issues. From the perspective of the results of the
case study, the model suggests that the current strategy of using mostly truck for moving
highly perishable crops is adequate, given the high value of these crops and the potential
loss incurred during transportation.
The paper also explored the current distribution tactic used by the growers, and compared
it to one based on direct distribution of products. The revenues obtained by direct distribution are significantly higher than those that result from current practice. The benefits of
direct distribution are part of the reason why some large, fresh-produce growers are getting
involved in the distribution of their crops. Furthermore, through the research reported in the
literature and our interviews with producers, we have found that there is currently a need
for integrated planning tools that support decisions made by growers/shippers. The tools
proposed in this paper can help them to succeed in a very complex supply chain, such as the
one for fresh agricultural commodities. As demonstrated through the case study, the use of
these tools has the potential of improving significantly the revenue obtained by the potential
users.
In addition, we believe that the proposed model is particularly well suited for the case of
contracted production, in which growers have fixed volume commitments, or have agreed
on a certain price for their products, or some combination of both. Using the present model,
growers can determine their growing, labor and transportation requirements throughout the
season. Growers can also use it to determine their most profitable customers, not only based
on the prices they pay, but also based in their transportation requirements and other conditions, such as quality and service, that they impose.
Some of the areas for further expansion of the present tactical model include:
1. Incorporate capital or investment options, such as renting or buying more land or more
equipment.
2. Explore the use decomposition algorithms, such as Bender’s decomposition to decrease
the run time to solve for larger-instances.
Acknowledgements The authors would like to acknowledge the anonymous reviewers of this paper whose
input significantly improved its quality. We also would like to acknowledge the Confederation of Associations
of Growers in the State of Sinaloa (CAADES) and Mexico’s National Council for Science and Technology
(CONACYT) for their support for the realization of this research.

Appendix A
Indexes and sets
l∈L
t ∈T
j ∈J
p ∈ TP(j, l) ⊆ T
h ∈ TH(j, l) ⊆ T
k ∈ K(j )
q ∈Q
w∈W
i∈I
d ∈D

Locations available for planting
Planning periods (weeks)
Potential crops and/or varieties to plant
Set of feasible planting weeks for crop j in location l
Set of feasible harvesting weeks for crop j in location l
Products obtained from crop j
Quality of crop at harvest (color)
Warehouses available for storage
Customers
Distribution centers

Ann Oper Res (2011) 190:339–358

f ∈ PF
r ∈ TM

355

Packaging facilities
Transportation mode

Parameters
Waterj
Totlandl
Wcapw
Dcapd
PFcapf
ShelfLk
LeadT i
MenPptj
MenH phj
HourLK
Yieldphj l
Totalpj l
Prodphj k
Salvagephj l
Weightk
Palletk
Maxj
Minj
Totlabor
Maxtemp
Colorhj q
Qmaxi
Demandtki
Totinvest
Totwater
M

Water required per acre of crop j in cubic meters (in cubic meters)
Land available at location l (in hectares)
Capacity of warehouse w (in pallets)
Capacity of DC d (in pallets)
Capacity of packing facility f (boxes per week)
Shelf life of product k (in weeks)
Required lead time by customer i (in weeks)
Workers required at period t for cultivating crop j planted at period p
(men-week/Ha)
Workers required at period t for harvesting crop j planted at period p
(men-week/Ha)
Man-hours required for packing a box of product k
Yield of crop j planted in location l at time p and harvested in
week h (percentage of total)
Total production of crop j planted in location l at time p (pounds per
hectare)
Percentage of product k from crop planted in location l at time p
harvested in week h (percentage)
Percentage of salvaged crop j planted in location l at time p harvested in week h
Quantity of crop j required to pack a box of product k (in pounds)
Boxes of product k required to form a pallet
Maximum amount to plant of crop j (in hectares)
Minimum amount to plant of crop j (in hectares)
Maximum number of contracted seasonal workers available (menweek)
Maximum number of temporal day laborers available (men-day)
Percentage of harvested fruits with color q from crop j at period h
Higher level of color in the product to send to customer i (colors
3–6)
Number of boxes of product k demanded by customer i at time t
(boxes per week)
Investment quantity available (dollars)
Water restriction (in cubic meters)
Large quantity to prevent shipment if route is not selected

Cost parameters
Pricetki
Psalvj
Pavgtk
Cplantj l
Ctemp

Price for the grower of a box of product k sold to customer i at
time t
Salvage value of a pound of crop j
Price on the open market for a box of product k at period t
Cost per hectare of production for crop j planted in location l
Cost of one man-day for day-laborers

356

Clabor
Ccasek
Chire
Coperk
Chwkw
Chdkd

Ann Oper Res (2011) 190:339–358

Cost of seasonal laborers per man-week
Cost to package a box of product k
Fixed cost of hiring a seasonal laborer at the field site
Cost of labor for packing a box of product k
Holding cost of a pallet of product k in warehouse w
Cost to hold a pallet of product k in DC d

Transportation parameters
Timef ir
TimeW wir
TimeDdir
TimePW f wr
TimePDf dr
TimeWDwdr
CT f ir
CTW wir
CTDdir
CTPW f wr
CTPDf dr
CTWDwdr

Transit time from facility f to customer i using transportation
mode r
Transit time from warehouse w to customer i using transportation
mode r
Transit time from DC d to customer i using transportation mode r
Time from packing facility f to warehouse w using transportation
mode r
Transit time from packing facility f to DC d using transportation
mode r
Transit time from warehouse w to DC d using transportation mode r
Cost of transportation from packing facility f to customer i using
transportation mode r
Cost of transportation from warehouse w to customer i using transportation mode r
Cost of transportation from DC d to customer i using transportation
mode r
Cost of transportation from packing facility f to warehouse w using
transportation mode r
Cost of transportation from packing facility f to DC d using transportation mode r
Cost of transportation from warehouse w to DC d using transportation mode r

Variables
Plantpj l
Harvestphj l
Packhkqf
MenLtl
HourF hf
Hiretl
Firetl
MenT tl
SPphj lf
SCtkqf ir

Area to plant of crop j , in period p at location l (in hectares)
Harvest (pounds) of crop j in period h and planted in period p at
location l
Quantity of product k with color q packed at facility f in period h
(in boxes)
Seasonal laborers required at location l and time t (men-week)
Operator hours allocated at facility f and harvest time h
Number of workers hired at period t in location l
Number of workers terminated at period t in location l
Number of temporal laborers hired at period t in location l (menweek)
Pounds of crop j to ship from location l to facility f in period h
Boxes of product k with color q shipped to customer i from facility
f in period t by transportation mode r

Ann Oper Res (2011) 190:339–358

SPDhtkqf dr
SPW htkqf wr
SDhtkqdir
SWDhtkqwdr
SW htkqwir
Invwhtkqw
Invdhtkqd
SK hj
Ztkw
TCtkf ir
TPW tkf wr
TWDtkwdr
TPDtkf dr
TW tkwir
TDtkdir
Yjpl

357

Boxes of product k harvested in period h with color q shipped from
facility f to DC d in period t by transportation mode r
Boxes of product k harvested in period h with color q shipped from
facility f to warehouse w in period t by transportation mode r
Boxes of product k harvested in period h with quality q shipped from
facility f to DC
Boxes of crop k harvested in period h with color q shipped from
warehouse w to DC d in period t by transportation mode r
Boxes of product k harvested in period h with color q shipped from
warehouse w to customer i in period t by transportation mode r
Stored boxes of product k harvested at period h with color q in warehouse w in period t
Stored boxes of product k harvested at period h with color q in DC
d at period t
Surplus of crop j at time h in the field (in pounds)
Boxes of product k to purchase in period t for warehouse w
Transportation mode r selected for transporting product k from f to
i at time t, 1 if selected, 0 otherwise
Transportation mode r selected for transporting product k from f to
i at time t, 1 if selected, 0 otherwise
Transportation mode r selected for transporting product k from w to
d at time t, 1 is selected, 0 otherwise
Transportation mode r selected for transporting product k from f to
d at time t, 1 is selected, 0 otherwise
Transportation mode r selected for transporting product k from w to
i at time t, 1 is selected, 0 otherwise
Transportation mode r selected for transporting product k from d to
i at time t, 1 is selected, 0 otherwise
1 if crop j is planted at period p at location l, 0 otherwise

Here
Y, TC, TPW, TWD, TPD, TW, TD ∈ B n
Plant, Harvest, Pack, Opl, Opf , Hire, Fire, Opt, K, Z ∈ R+n
SP, SPD, SPW, SD, SWD, SW, Innw, Invd ∈ R+n

References
Ahuja, R. K., Magnanti, T. L., & Orlin, J. B. (1993). Network flows: theory, algorithms and applications.
Upper Saddle River: Prentice-Hall.
Ahumada, O., & Villalobos, J. R. (2009). Application of planning models in the supply chain of agricultural
products: a review. European Journal of Operational Research, 190(1), 1–20.
Allen, S. J., & Schuster, E. W. (2004). Controlling the risk for an agricultural harvest. Manufacturing &
Service Operations Management, 6(3), 225–236.
Arellano, J. (2001). Validación de Hortalizas, Campo Experimental del Valle de Culiacán, Mayo. Unpublished study of the Commission for the Research and Defense of Horticultural Crops of the State of
Sinaloa (http://www.cidh.org.mx). Copies of this document available upon request.
Caixeta-Filho, J. V., van Swaay-Neto, J. M., & Wagemaker, A. P. (2002). Optimization of the production
planning and trade of lily flowers at Jan de Wit Company. Interfaces, 32(1), 35–46.

358

Ann Oper Res (2011) 190:339–358

Dimitri, C., Tegene, A., & Kaufman, P. R. (2003). US fresh produce markets: marketing channels, trade
practices, and retail pricing behavior. Agricultural Economic Report No. 825, USDA.
Epperson, J. E., & Estes, E. A. (1999). Fruit and vegetable supply-chain management, innovations, and competitiveness: cooperative regional research project S-222. Journal of Food Distribution, 30, 38–43.
Glen, J. (1987). Mathematical-models in farm-planning—a survey (1987). Operations Research, 35(5), 641–
666.
Faulin, J. (2003). Applying MIXALG procedure in a routing problem to optimize food product delivery.
Omega, 31, 387–395.
Ferrer, J. C., Mac Cawley, A., Maturana, S., Toloza, S., & Vera, J. (2008). An optimization approach for
scheduling wine grape harvest operations. International Journal of Production Economics, 112(2), 985–
999.
FIRA (2007). Rentabilidad y costos de cultivo de tomate en Sinaloa. [Profitability and costs of tomato
production in Sinaloa]. http://portal.fira.gob.mx/Files/TOMATE%20Sinaloa%20-%20Analisis%20de%
20Costos.pdf. Accessed May 15, 2009.
Fujiwara, O., & Perera, U. L. J. S. R. (1993). EOQ models for continuously deteriorating products using
linear and exponential penalty costs. European Journal of Operational Research, 70, 104–114.
Goyal, S. K., & Giri, B. C. (2001). Recent trends in modeling of deteriorating inventory. European Journal
of Operational Research, 134, 1–16.
Hamer, P. J. C. (1994). A decision support system for the provision of planting plans for Brussels sprouts.
Computers and Electronics in Agriculture, 11, 97–115.
Lutke Entrup, L. M., Gunther, H. O., Van Beek, P., Grunow, M., & Seiler, T. (2005). Mixed-integer linear
programming approaches to shelf-life-integrated planning and scheduling in yogurt production. International Journal of Production Research, 43(23), 5071–5100.
Kaufman, P., Handy, C., McLaughlin, E. V., Park, K., & Green, G. M. (2000). Understanding the dynamics
of produce markets: consumption and consolidation grow. US Department of Agriculture-Economic
Research Service, Market and Trade Economics Division. Agricultural Information Bulletin No. 758.
Lowe, T. J., & Preckel, P. V. (2004). Decision technologies for agribusiness problems: a brief review of
selected literature and a call for research. Manufacturing & Service Operations Management, 6(3),
201–208.
Nahmias, S. (1982). Perishable inventory theory: a review. Operations Research, 30(4), 680–708.
Osvald, A., & Stirn, L. Z. (2008). A vehicle routing algorithm for the distribution of fresh vegetables and
similar perishable food. Journal of Food Engineering, 85, 285–295.
Perosio, D. J., McLaughlin, E. W., Cuellar, S., & Park, K. (2001). Supply chain management in the produce
industry. Produce Marketing Association: Newark (pp. 22–32).
Raafat, F. (1991). Survey of literature on continuously deteriorating inventory models. Journal of the Operational Research Society, 42(1), 27–37.
Robles, M. H., & Santana, L. (1997). Programación de siembra de tomate temporada 1997–1998. Comisión
para la Investigación y Defensa de las Hortalizas (CIDH). Unpublished study of the Commission for the
Research and Defense of Horticultural Crops of the State of Sinaloa (http://www.cidh.org.mx). Copies
of this document available upon request.
Sanchez, O. (2007). Strategic design of a logistics platform for fresh produce. Master of Science Thesis,
Arizona State University.
Thompson, G. D., & Wilson, P. N. (1997). The organizational structure of the North American fresh tomato
market: implications for seasonal trade disputes. Agribusiness, 13(5), 533–547.
United States Department of Agriculture (1991). United States Standards for Grades of Fresh Tomatoes. http://www.ams.usda.gov/AMSv1.0/getfile?dDocName=STELPRDC5050331. Accessed May 15,
2007.
United States Department of Agriculture (2006). Fruit and Vegetable Truck Rate Report, WA_FV190.
http://www.ams.usda.gov/fv/mnmovement.htm. Accessed May 15, 2009.
United States Department of Agriculture (2007). Shipping point Report, Agricultural Marketing Services.
http://marketnews.usda.gov/portal/fv. Accessed May 15 2009.
Van Berlo, J. M. (1993). A decision support tool for the vegetable processing industry; an integrative approach
of market, industry and agriculture. Agricultural Systems, 43, 91–109.
Welby, E. M., & McGregor, B. (2004). Agricultural export transportation handbook. Agricultural Marketing
Service. US Department of Agriculture.
Wolsey, L. A. (1998). Integer programming. New York: Wiley. Tables

Available online at www.sciencedirect.com

European Journal of Operational Research 195 (2009) 1–20
www.elsevier.com/locate/ejor

Invited Review

Application of planning models in the agri-food supply chain: A review
Omar Ahumada, J. Rene Villalobos *
Department of Industrial Engineering, Arizona State University, P.O. Box 875906, Tempe, AZ 85287, United States
Received 24 May 2007; accepted 14 February 2008
Available online 21 February 2008

Abstract
The supply chain of agricultural products has received a great deal of attention lately due to issues related to public health. Something
that has become apparent is that in the near future the design and operation of agricultural supply chains will be subject to more stringent
regulations and closer monitoring, in particular those for products destined for human consumption (agri-foods). This implies that the
traditional supply chain practices may be subject to revision and change. One of the aspects that may be the subject of considerable scrutiny is the planning activities performed along the supply chains of agricultural products. In this paper, we review the main contributions
in the ﬁeld of production and distribution planning for agri-foods based on agricultural crops. We focus particularly on those models
that have been successfully implemented. The models are classiﬁed according to relevant features, such as the optimization approaches
used, the type of crops modeled and the scope of the plans, among many others. Through our analysis of the current state of the research,
we diagnose some of the future requirements for modeling the supply chain of agri-foods.
Ó 2008 Elsevier B.V. All rights reserved.
Keyword: OR in agriculture

1. Introduction
The supply chain practices of agricultural food products
are currently under public scrutiny. This is the result of several factors, such as the national attention given to recent
cases of fresh produce contamination (van der Vorst,
2006), the changing attitudes of a more health conscious
and better informed consumer who wants to have precise
information about the farming, marketing, and distribution practices used to bring the agricultural products into
the shelves of the neighborhood supermarket. This scrutiny
will undoubtedly translate into additional regulations and
market driven standards that will aﬀect the design and
operation of an already complex supply chain. This complexity is particularly critical in the case of perishable
agricultural commodities where the traversal time of the
products through the supply chain and the opportunities

Corresponding author. Tel.: +1 480 965 0437; fax: +1 480 965 8692.
E-mail address: rene.villalobos@asu.edu (J.R. Villalobos).
0377-2217/$ - see front matter Ó 2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2008.02.014

to use inventory as a buﬀer against demand and transportation variability are severely limited. This complexity is
compounded when the supply chain encompasses two or
more countries. Thus, the opening of domestic markets
to international competition throughout the world will
undoubtedly result in shifting the focus from a single
echelon, such as the farmer, to the eﬃciency of the overall
supply chain. In order to meet these new challenges, it is
necessary to take a critical look at the current supply chain
practices to determine the best strategies to accommodate
the new global conditions. In particular, it is necessary to
investigate if there exist better ways to design and operate
a supply chain that is increasingly globally integrated. In
this paper we focus primarily on planning models used in
the diﬀerent aspects of the supply chain of agricultural food
products obtained from crops, or agri-food products. This
review does not include the supply chains of other products
such as cattle, meats, and other agricultural products not
directly related to crops.
The term agri-food supply chains (ASC) has been coined
to describe the activities from production to distribution

2

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

that bring agricultural or horticultural products (Aramyan
et al., 2006) from the farm to the table. ASC are formed by
the organizations responsible for production (farmers), distribution, processing, and marketing of agricultural products to the ﬁnal consumers.
The supply chain of agri-foods, as any other supply
chain, is a network of organizations working together in
diﬀerent processes and activities in order to bring products
and services to the market, with the purpose of satisfying
customers’ demands (Christopher, 2005). What diﬀerentiates ASC from other supply chains is the importance
played by factors such as food quality and safety, and
weather related variability (Salin, 1998). Other relevant
characteristics of agri-foods include their limited shelf life,
their demand and price variability, which makes the underlying supply chain more complex and harder to manage
than other supply chains.
This paper gives an assessment of the state of the art in
the area of planning models for the diﬀerent components of
agri-food supply chains. Fig. 1 presents the factors used to
dissect and organize this review. For instance, from the
perspective of storability of the products, we make the
distinction between those papers whose main focus is on
perishable products from those that focus mostly on nonperishable products. From the perspective of the scope;
we divide the papers into strategic, tactical, and operational planning. From the perspective of modeling uncertainty, we divide the papers into deterministic and
stochastic. In a second level of the classiﬁcation, we make
a further categorization using the particularities of the
modeling approaches used. For instance, we divide the
deterministic models into those based on linear programming, dynamic programming, etc. We also divide the
papers using stochastic modeling approaches into stochastic programming and stochastic dynamic programming.

Non-perishable
Perishable

Agricultural

Scope

Strategic

Tactical

Operational

Reviewed Models

LP

DP

SDP

Deterministic

SP

The papers not ﬁtting exactly one of these categories
appear in more than one group or, for those papers not ﬁtting any of the categories; they are grouped in a special category presented at the end of the classiﬁcation.
1.1. Scope of the review
We are aware of at least three previous literature reviews
in areas related to the topic of planning models of agricultural supply chains; the earliest was performed by Glen
(1987), and the latest by Lowe and Preckel (2004). Glen
performed an exhaustive search of the literature (previous
to the year of 1985) covering crop and livestock production
models. The review by Lowe and Preckel focused on the
main modeling approaches used in crop planning in the
context of agribusiness. Their review included some of
the relevant papers covered in Glen (1987), but also some
papers that were published after Glen’s review. Although
Lowe and Preckel’s review is not extensive, it highlights
some potential areas for future research in the area.
Another review that focused on the topic of location analysis applied to agriculture was compiled by Lucas and
Chhajed (2004). This review covers applications related
to location of warehouses and processing plants from the
year 1826 to the year 2000. In their paper, Lucas and
Chhajed recognized the complexity and challenges of strategic production–distribution models applied to the agricultural industry, and the need to consider uncertainty in
the planning models. These authors also emphasize the
emerging use of these models by large corporations.
Our intention in this paper is to complement and expand
the previous works by identifying the works that either
were not covered or were published after these reviews.
Another objective is to frame the literature in the context
of supply chain planning. In this paper we take a similar
approach to that of Lowe and Preckel’s by focusing on
those papers aimed at the production and distribution of
crops. We also aim to perform an extensive search of those
papers that have been published from the year 1985, the
year of Glen’s review, to the present. As it was the case
in the previous reviews, we do not cover macroeconomic
models designed to plan crop production for entire regions
or countries; instead we focus on those models targeted to
be used by a single user, which may be a farmer or a company. The underlying reason for this approach is to look at
the ASC planning problem from the perspective of the individual farmers, or group of farmers, facing an increasingly
integrated and more complex production–distribution system. Most of the models addressed in this review come
from journals in the agricultural sciences, supply chain,
and operations research literature.

Stochastic

1.2. Plan for this research
Modeling

Fig. 1. Supply chain literature and its relation with our review.

The organization of this paper is as follows: we ﬁrst
present (Section 2) some background about the importance
of agriculture and the fresh produce industry. Section 3

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

presents a brief description of the supply chain planning
framework. In Section 4 we present supply chain planning
models that have been developed for non-perishable agrifood products. Section 5 presents those supply chain planning models used in perishable agri-foods. Sections 4 and 5,
in addition to identify and classify the planning models,
also present some salient examples of the models and the
planning approaches that have been used on speciﬁc problems. Those papers not ﬁtting the classiﬁcation scheme
used in Sections 4 and 5 are presented in Section 6. Finally
in Sections 7 and 8, we provide a summary of the review
and identify some existing gaps in the literature that we
believe should be addressed in the future.
2. Background
Contrary to common belief, the supply chain of agricultural products continues to be important, in terms of consumption and monetary value. For instance, Kinsey (2001)
estimates that in the year 2000, the food and agricultural
sector (not accounting its auxiliary services) made up over
9% of the US gross domestic product (GDP). He shows
that the contribution to the GDP of some agricultural sectors is actually expanding. It has been reported that the US
agricultural market has maintained a steady growth in production fueled by the internal demand. However, the main
future growth in demand for agricultural products is
expected to be generated by developing nations, which
are actively increasing their consumption of proteins,
fruits, and vegetables (Boehlje et al., 2003).
Fresh produce is one of the most dynamic sectors of the
industry (Huang and Sophia, 2004). For example, the US
market for fresh agri-foods represents nearly a quarter of
all US food expenditures, with annual consumption of over
a $100 billion in products related to fruits and vegetables
(Epperson and Estes, 1999). Statistics from the United
Sates Department of Agriculture (USDA, 2007) suggest
that the per-capita consumption of fresh vegetables has
been steadily increasing since the early 1980s, while the
per capita consumption of traditional crops such as wheat
and other grains have increased at a much slower pace and,
for the last few years, it has actually decreased. The largest
portion of the reported increases in consumption has been
attributed to population growth, but also to market
changes, such as the increasing public awareness of the
beneﬁts of healthier diets and to the higher incomes among
the US population (McLaughlin et al., 1999). This
increased demand for fresh products and an expectation
of their year-around availability has in turn fueled the
expansion of the underlying supply chains to include overseas production units such as winter produce from Mexico,
Chile and other South American countries. The same trend
has been observed in Europe were the norm is to ﬁnd that
the winter fresh produce sold in Northern Europe is produced in Spain, Turkey, North African countries, and
beyond. These changes in demand and distribution patterns are expected to continue or even accelerate in the near

3

future, thus making it necessary to look for ways to
improve the current supply chain practices. In particular,
the improvements should consider the new challenges
imposed by the changing demand in agri-foods, and the
new realities of the industry, which involves a global marketplace and more strict food safety regulations.
The structure of global market for agri-foods and the
associated supply chains is not static. On the contrary, it is
currently undergoing a drastic transformation. For instance,
the proﬁle of the typical player in the agri-food supply chain
is changing from family based, small-scale, independent
ﬁrms to one in which larger ﬁrms are more tightly aligned
across the production and distribution value chain (Boehlje,
1999). The sophistication needed to successfully compete in
these emerging supply chains makes it more likely that some
of the concepts of supply chain planning and coordination
that have been successfully applied in the manufacturing sector can be adapted to ﬁt the agri-food supply chains. For
instance, the academic and practice-oriented literature
related to improvements of non-agricultural supply chains
is ample (see for instance Vidal and Goetschalckx, 1997; Sarmiento and Nagi, 1999; Min and Zhou, 2002; Meixell and
Gargeya, 2005). But implementing supply chain practices
that have been eﬀective in other ﬁelds, to the ASC is not easy,
since the supply chain of agri-foods is characterized by very
long lead times, as well as signiﬁcant supply and demand
uncertainties (Lowe and Preckel, 2004). These issues are
even more complex for fresh products, where producers also
face additional marketing uncertainties and a shorter life of
the product. Thus, in order to adequately plan the operations in the supply chain of fresh products; it is necessary
to formulate speciﬁc planning models that incorporate issues
such as harvesting policies, marketing channels, logistics
activities, vertical coordination, and risk management
(Epperson and Estes, 1999).
An additional reason for the lack of planning models for
the ASC has been the fragmented nature of the industry, in
which advanced planning systems have not been easily
adapted, implemented and maintained (McCarl and Nuthall, 1982). Lately, however, the new level of consolidation
being observed in the fresh produce industry has resulted in
more logistical activities performed directly by the producers, such as packaging, distribution, and ﬁnal delivery of
the products to the customers (Kader, 2001). These recent
developments have increased the need for more sophisticated planning strategies and tools in this industry.
The present review aims at determining current state of
the art in models and strategies for planning the ASC, and
at the identiﬁcation of the research gaps as a ﬁrst step to
the development of the solutions needed by the agricultural
logistics industry.
3. Supply chains, supply chain planning and modeling
approaches
We have identiﬁed two main types of ASCs, the ﬁrst one
is the supply chain of fresh agri-foods, and the second one

4

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

is the supply chain for non-perishable agri-foods. Fresh
products include highly perishable crops such as fresh
fruits and vegetables whose useful life can be measured in
days, non-perishable products are those that can be stored
for longer periods of time such as grains, potatoes, and
nuts. In the present review fresh products are of particular
interest due to their added logistical complexity, their limited shelf life, and the renewed interest of the general public
on the safety of these products.

during their distribution. Some storage-related decisions
also include the amount to store and sell in each planning
period and how to position the inventory along the supply
chain. Finally, the distribution function involves moving
the product down the supply chain to deliver it to the consumers. The decisions associated with distribution include
selecting the transportation mode, the routes to use and
the shipping schedule to deliver the product.
3.3. Modeling approaches in SCP

3.1. Scope of decision-making in supply chain planning
Planning in supply chain of agri-foods usually involves
several levels of hierarchical decisions. These decisions
can be classiﬁed as strategic, tactical or operational,
depending on their eﬀects to the overall supply chain (Simchi-Levi, 2003; Chopra and Meindl, 2003). In the present
research we review those supply chain planning models
focused on strategic, tactical, and operational decision for
the agri-food supply chain. We place special attention to
those models dealing with coordination of tactical decisions such as production and distribution.
3.2. Decisions in supply chain planning
Planning is an activity that supports decision-making by
identifying potential alternatives and making the best decisions according to the objectives of planners (Fleischmann
et al., 2005). Supply chain planning (SCP) is comprised, at
the highest level, of three main decision-making functional
processes: production planning, inventory control and
physical distribution (Beamon, 1998). Fleischmann et al.
(2005) divides the supply chain activities into four functional areas: procurement, production, distribution and
sales. These functional areas play an important part in
the architecture of advanced planning systems for commercial packages (Stadler, 2005). Although these tasks have
traditionally been modeled independently, there is a consensus in the supply chain literature that two or more of
these processes should be modeled together for improving
the overall supply chain performance (Chandra and Fisher,
1994).
In the context of the ASC, we have identiﬁed four main
functional areas: production, harvest, storage, and distribution. Decisions made in production include those related
to cropping, such as the land to allocate to each crop, timing of sowing, and the determination of resources required
for growing the crops. During harvest, some of the decisions that need to be made include the timing for collecting
the crops from the ﬁelds and the determination of the level
of resources needed to perform this activity. Some other
decisions made at harvest include the scheduling of equipment, labor, and transportation equipment. Sometimes
these decisions also involve the scheduling of the packing
or processing plant. The third function is storage, which
includes the inventory control of the agri-foods, which is
required when the products need to be stored before or

From a modeling perspective, the models for supply
chain planning can be classiﬁed as deterministic or stochastic, according to the certainty of the value of the parameters used (Min and Zhou, 2002). We further reﬁne this
classiﬁcation according to the main mathematical techniques used for ﬁnding solutions to these models. In those
cases where all of the model’s parameters are assumed
deterministic, the researchers have traditionally used
approaches such as linear programming (LP), dynamic
programming (DP), mixed integer programming (MIP),
and goal programming (GP). Otherwise, stochastic modeling approaches are used, these include stochastic programming (SP), stochastic dynamic programming (SDP),
simulation (SIM), risk programming (RP).
We are aware of alternative modeling approaches for
modeling agri-food related activities, which we do not
cover in the present review. In general these approaches
are not as commonly used for applications in the ASC,
but there are applications in related areas of agricultural
research in which these modeling approaches are useful.
One example is the use of multi-objective and multi-criteria
decision-making models, which have been applied to subsistence farms and agricultural policy planning. The interested reader in multi-criteria decision-making is referred
to the work of Hayashi (2000), who presents a comprehensive list of articles on the topic of multi-criteria agricultural
decision making. Another example is the use of models to
predict plant growth and the timing of their maturity,
which have been applied to estimate production yield as
a function of time. For a detailed description of these models the reader is advised to consult the reviews by Marcelis
et al. (1998) and van Ittersum et al. (2003).
4. Planning models for ASC of non-perishable products
In this section we present those works dealing with models for the planning of activities in the supply chain of nonperishable agri-foods. The complete list of the research
papers covered is presented in Table 1. This table presents
the leading authors and the publication year of the paper.
The second column gives a brief description of the papers
and their main objective(s). In the remainder of this section
we classify these papers according to their planning scope,
the functional nature of the decisions being modeled and the
modeling approaches used. In order to better illustrate
the diﬀerent classiﬁcations we brieﬂy describe one or more

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

5

Table 1
List of models non-perishable agricultural products
Model

Main objective of the paper

Torkamani (2005)

Evaluate prospective technology options using SP with the aim of maximizing the farmer’s utility (exponential utility
maximizing objective)
Develop a RP model for capturing joint stochastic distributions (parametric and non-parametric) using a mean–variance
objective function
Design a supply chain network for growing, harvesting, transporting and processing of a pea-based product using a MIP that
minimizes total cost
Develop a harvest schedule for a sugar cane farms using a LP model that maximizes the sugar content in the crops for a
harvest season
Plan seasonal crops within a year, using a fuzzy program with the objective of increasing utilization of land, labor,
production and proﬁts
Determine farm planning strategies (crop and livestock) with a MIP that maximize the proﬁt earned, given the level of risk
selected
Design a plan for planting decisions for a two period SP problem for a corn seed producer with variable yield with the
objective of reducing cost
Develop a farm plan that includes scheduling ﬁeld tasks and analyzing investments with the objective of minimizing costs
using a MIP model
Prepare a plan for cropping tasks with a LP, satisfying precedence and time window constraints with the objective of
minimizing costs
Schedule the roster for harvest of a sugar cane region using MIP with the objective of reducing costs in transportation and in
the processing plant
Develop a SP model for planning production and consumption of a farmer for a given rainfall, with the objective of
minimizing shortages
Design a DP model for planning the decisions of multi-echelons agri-chains, to satisfy demand at the minimum total chain
cost
Plan the introduction of improved cultivation systems using a MIP model for semi-subsistence farmers with the purpose of
increasing discounted return
Analyze the farmers response to diﬀerent type of subsidies in whole-farm, and their attitude towards risk through a SP with
utility maximizing objective
Determine the best combination of equipment and crop mix with the objective of maximizing revenue using a SP model
Estimate the impact of price and yield uncertainty on the introduction of crops using SP, with the objective of maximizing
expected utility of a farmer
Plan irrigation and production tasks with a LP model to ﬁnd the best compromise between net beneﬁts, agricultural
production and labor employed
Schedule harvesting and replanting operations with a LP model, considering available processing capacity with the objective
of maximize net revenue
Generate crop plan alternatives that are close to the optimal decisions for farmers with diﬀerent objectives and using a LP
model
Develop a plan for multi-crop water allocation and intra-seasonal stochastic irrigation scheduling using DP and SDP models
to maximize revenues
Develop of a model for selecting and scheduling the machinery for a multicrop farm using search techniques for minimizing
the cost
Design a utility eﬃcient non-linear SP model used for analyzing the economic eﬃciency of farmers with several utility
maximizing functions
Determine the production policy of double cropping and crop rotations with a MOTAD objective (maximizing revenue and
minimizing low returns)
Design a crop plan with an expert systems and a LP model with the objective of maximizing proﬁts
Analyze long-term farm planning decisions under provisions of 1990 farm bill using a SDP model with the objective of
maximizing expected present value
Determine the potential impact of climate change using a SP model that maximizes revenue under diﬀerent simulated
scenarios
Develop a LP model for planning the production, harvest, storage and marketing of crops and livestock, with the objective of
maximizing revenue
Design a SP model applied to a sequential decision-making under weather uncertainty for selecting cereal technologies that
maximize proﬁts
Evaluate economic performance of farmers using a SP model with 3 risk preferences (max utility, max probability and
chance constraint)
Develop a simulation tool for maximizing proﬁt and minimizing yield risk, by planning sowing date, fertilizer treatment and
plant population
Determine the relation of farm programs to the farmer’s hedging decisions with futures and options. By using SP with utility
maximizing objective
Develop a production system that minimizes machine demand in a two-stage cost minimizing application using LP and DP
Design a multi-period MIP model to identify the participation in government programs and crop mix with the objective of
maximizing net present value
(continued on next page)

Kobzar et al. (2005)
Apaiah and Hendrix
(2005)
Jiao et al. (2005)
Biswas and Pal (2005)
Visagie et al. (2004)
Jones et al. (2003)
Recio et al. (2003)
Vitoriano et al. (2003)
Higgins (2002)
Maatman et al. (2002)
Gigler et al. (2002)
Glen and Tipper (2001)
Lien and Hardaker (2001)
Ekman (2000)
Schilizzi and Kingwell
(1999)
Raju and Kumar (1999)
Higgins et al. (1998)
Abdulkadri and Ajibefun
(1998)
Sumanatra and Ramirez
(1997)
Lazzari and Mazzetto
(1996)
Torkamani and Hardaker
(1996)
Burton et al. (1996)
Nevo et al. (1994)
Duﬀy and Taylor (1993)
Kaiser et al. (1993)
Dobbins et al. (1992)
Adesina and Sanders
(1991)
Nanseki and Morooka
(1991)
Alocilja and Ritchie (1990)
Turvey and Baker (1990)
Bin Deris and Ohta (1990)
Perry et al. (1989)

6

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

Table 1 (continued)
Model

Main objective of the paper

Clarke (1989)
Kaiser and Apland
(1989)
Lambert and McCarl
(1989)
Turvey et al. (1988)
Tan and Fong (1988)

Determine the cropping pattern that maximizes the return from the farm, applied to a farm in Bangladesh using a LP model
Determine production and marketing plans for two crops using a SP model with the objective of maximizing proﬁt and reduce
proﬁt deviation
Develop a discrete SP for selecting among marketing alternatives with the objective of maximizing revenues

Glen (1986)
El-Nazer and McCarl
(1986)
Butterworth (1985)
Stoecker et al. (1985)

Design a RP model for providing useful alternatives to the variance–covariance quadratic programming method
Determine cropping decisions for a perennial crops, with the objective of maximizing revenue with MOTAD and using a LP
model
Design a plan for an integrated crop and beef production with internal production of feed stuﬀ, using a LP model for
maximizing revenue
Develop a LP model to design and determine the optimal long-run rotation of crops with the objective of maximizing revenue
with risk aversion
Develop a MIP model for whole farm plan with crop, livestock and labor decisions with the objective of maximizing revenues
Design of an application of LP and DP models for determining production, irrigation, drilling and water distribution decisions
for maximizing revenues

representative papers for each classiﬁcation. In selecting
these papers we favored those that were motivated by a
concrete need and oﬀered evidence of a successful
implementation.
4.1. Planning scope for ASC of non-perishable products
Table 2 presents the papers for non-perishable agrifoods organized according to the planning scope being
addressed. The papers are classiﬁed as strategic (S), tactical
(T) or operational (O). The fourth column of Table 2
shows whether the papers provide evidence that the
described models were implemented and used (Y/Y); just
applied to a case study, but not to a speciﬁc real life situation(Y/N); or not applied at all (N). As it can be observed
in the table, the reported models that have been successfully applied to the planning the ASC are a minority. This
same pattern has been reported in other reviews of agricultural models (Higgins et al., 2007). For this reason we highlight in our review those models that have been successfully
applied to the planning of diﬀerent aspects of the ASC.
The next column of Table 2 identiﬁes the targeted user
of the model. We identify this user as the decision maker
(DM). The decision maker can be a farmer, external advisor, planner, or a centralized decision maker in the supply
chain (SC). We deﬁne the planner as a decision maker that
is in charge of a large operation, such as large farmers,
cooperatives or corporations. Usually this type of planner
requires more sophisticated tools and strategies than a
small of medium size farmer. The reason for the inclusion
of this additional classiﬁcation dimension is to provide an
assessment of the level of technical sophistication required
for the application of the planning models, which increases
from the farmer, at the lowest level, to the centralized decision maker, at the highest level of the classiﬁcation.
4.1.1. Models for strategic planning in ASC of nonperishable products
In this section we discuss in more detail strategic models
aimed at the ASC with a particular emphasis on those that

deal with farming decisions. The papers reviewed cover a
wide range of strategic decisions such as equipment selection, selection of farming technology, ﬁnancial planning,
design of supply networks, reservoir management, evaluation of perennial crops, and crop rotation strategies. In
terms of their objective functions, these models include
proﬁt and revenue maximization, utility maximization,
net present value, and cost minimization. In the remainder
of this section we discuss some representative examples of
these papers.
From the list of articles in Table 2, there are only a few
models aimed at purely strategic decisions. For instance,
Ekman (2000) presents an example of strategic planning
applied to technology selection. The paper describes an SP
model for selecting the best mix of equipment and tillage
schedule for an individual farm with the purpose of
maximizing revenue. The model uses discrete probability
distributions to represent the available working days. The
distributions are used to determine the optimal amount of
equipment required to meet tillage schedule. The results presented indicate that deterministic models underestimate the
capacity requirements for unfavorable-weather years. The
main contribution of this work is the selection of machinery
investment with uncertain constraints (time available for
tillage) given by the stochastic nature of the weather.
Tan and Fong (1988) present an LP model to select the
best crop mix for a perennial crop plantation. The objective
is to maximize the revenue and to consider risky outcomes
by penalizing negative returns, also known as mean absolute deviation (MOTAD). One of the main considerations
in evaluating perennial crops is the determination of the
multiple periods in which the model has to be evaluated,
and the corresponding uncertainty in the prices of the
crops. The researchers use the net present value of the
mean absolute deviation to evaluate the alternative crops.
An eﬃcient frontier is developed with the diﬀerent potential plans from which the decision makers can select
according to their level of risk. The main contribution of
this paper is the development of a methodology for making
long term decisions under uncertainty.

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

7

Table 2
Planning scope and decision variables for non-perishable agricultural products
Model

Torkamani (2005)
Kobzar et al. (2005)
Apaiah and Hendrix (2005)
Jiao et al. (2005)
Biswas and Pal (2005)
Visagie et al. (2004)
Jones et al. (2003)
Recio et al. (2003)
Vitoriano et al. (2003)
Higgins (2002)
Maatman et al. (2002)
Gigler et al. (2002)
Glen and Tipper (2001)
Lien and Hardaker (2001)
Ekman (2000)
Schilizzi and Kingwell (1999)
Raju and Kumar (1999)
Higgins et al. (1998)
Abdulkadri and Ajibefun (1998)
Sumanatra and Ramirez (1997)
Lazzari and Mazzetto (1996)
Torkamani and Hardaker (1996)
Burton et al. (1996)
Nevo et al. (1994)
Duﬀy and Taylor (1993)
Kaiser et al. (1993)
Dobbins et al. (1992)
Adesina and Sanders (1991)
Nanseki and Morooka (1991)
Alocilja and Ritchie (1990)
Turvey and Baker (1990)
Bin Deris and Ohta (1990)
Perry et al. (1989)
Clarke (1989)
Kaiser and Apland (1989)
Lambert and McCarl (1989)
Turvey et al. (1988)
Tan and Fong (1988)
Glen (1986)
El-Nazer and McCarl (1986)
Butterworth (1985)
Stoecker et al. (1985)

Planning scope
S

T

X

X
X
X
X
X
X
X
X
X

X

X

X

O

X
X

X

X
X
X
X

X
X
X
X
X
X

X
X

X
X
X
X

X

X
X
X
X
X
X
X
X
X
X

X

X
X

X
X
X
X
X

X
X
X
X

X
X

Decision variables
A

DM

P

Y/N
Y/N
N
Y/Y
Y/N
Y/N
Y/Y
Y/Y
N
Y/Y
Y/Y
N
Y/N
N
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N
N
N
N
N
Y/Y
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N
N
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N

Advisor
Planner
SC
Planner
Advisor
Farmer
Planner
Advisor
Planner
Planner
Advisor
SC
Advisor
Planner
Farmer
Advisor
Advisor
Planner
Farmer
Advisor
Advisor
Planner
Advisor
Farmer
Planner
Farmer
Advisor
Advisor
Planner
Advisor
Planner
Advisor
Farmer
Advisor
Farmer
Advisor
Advisor
Planner
Advisor
Advisor
Advisor
Farmer

X
X
X

H

X
X

D

I

X

X
X
X
X
X
X
X
X
X
X
X
X
X

X

X

X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X

X
X

X
X
X
X
X
X

X
X

X

SCM

Other decisions considered

1
1
3
1
1
1
1
1
1
1
1
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

Labor and ﬁnancial
Risk reduction
Production at plant

Livestock planning
Scheduling of activities
Modeling approach
Reduce variability at plant
Consumption and purchase
Selection fallow system
Subsidies, labor
Equipment investment and tilling schedule
Crop rotations
Planning of irrigation, labor
Replanting decisions
Generate alternative plans
Irrigation scheduling
Equipment sizing/scheduling
Utility functions
Crop rotations and labor
Participation on program
Tilling schedule
Activities schedule
Purchasing and consumption
Labor requirements
Sowing date and fertilizer use
Financial and hedging
Scheduling of machines
Program participation
Crop selection and rotation
Tillage and marketing
Utility function
Assign crops to soil type
Livestock decisions
Design of crop rotations
Livestock an labor
Irrigation and aquifer

S: strategic, P: production variables/decisions, T: tactical, H: harvesting variables/decisions, O: operational, D: distribution variables/decisions, A:
application of the models, I: inventory variables/decisions, DM: decision maker for which the model is designed, SCM: echelons of the supply chain.

We close this discussion about strategic modeling by
describing a paper for modeling the economic beneﬁts of
irrigation development over a depleting aquifer (Stoecker
et al., 1985). The problem is formulated as an LP problem
that makes short term crop mix decisions (one year plan),
which is combined with a DP approach that deﬁnes the
long-term cropping plans by carrying over the eﬀects of
the yearly decisions. The main objective of the model is
to maximize the net present value of multi-period revenue.
The decision variables in the model include system variables such as crop production, drilling policy, area developed for irrigation, and water allocation. The main
contribution of the model is to simultaneously determine

the optimal discrete capital expenditure patterns for both
single period and multi-period groundwater utilization.
4.1.2. Models for tactical planning in ASC of non-perishable
products
Tactical models for non-perishable agri-foods handle
short to medium term decisions in farm planning, such as
cropping plans, harvesting, and planting policies. Accordingly, the papers presented in Table 2 deal with crop
allocation, drilling policy, participation on government
programs, water allocation, scheduling of tillage, labor
requirements, harvesting, marketing, ﬁnancial, and postharvesting decisions. The objective functions of these

8

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

models include traditional one-dimensional objective functions such as proﬁt maximization, cost minimization, and
production maximization. Sometimes the models also include
alternative objectives such as risk reduction while increasing proﬁt, investment hedging, and multi-objective criteria.
Examples of papers dealing with tactical models that have
been successfully implemented are presented next.
Jiao et al. (2005) present a harvest-scheduling model for
a region in Australia with multiple independent sugar cane
ﬁelds. The paper presents an LP model for determining the
amount of crops to harvest along the season with the objective of increasing the amount of sugar obtained. The model
also restricts the harvest decisions to assure fairness to the
farmers in the region. The main contribution of the paper is
the development of a statistical analysis that predicts sugar
content and the integration of the statistical analysis to the
optimization model. This model has been converted into a
software tool, which is currently used by more than 20
growers in several regions of Australia.
A second example of a tactical model is given by the
work of Maatman et al., 2002. This model helps a subsistence farmer to determine strategies for the production,
consumption, selling, storing, and purchasing of crops.
The problem is modeled as a two-stage SP, where the
ﬁrst-stage decisions involve what and how much to produce given that a certain amount of rain is observed. In
the second-stage decisions (post-harvest), the farmer decides the consumption, storage, selling, and purchasing of
the crop. The main objective of this model is the minimization of food shortages for the farmer and his family. The
researchers claim that their approach is simple to apply;
given the limited number of options and scenarios available
to the farmers. They also report that the use of this model
has inﬂuenced agricultural policies in Burkina Faso and
allowed to test alternative production methods adapted
to the characteristics of the farmers.

4.1.3. Models for operational planning in ASC of nonperishable products
Regarding the models related to operational planning
(Table 2), we can observe that there are fewer papers in
the area of operational planning than in the area of tactical
planning. This diﬀerence may reﬂect the importance of tactical over operational planning for non-perishable products. Most of the operational models presented are
concerned with determining harvesting plans, equipment
scheduling, water allocation, and land preparation.
The work of Recio et al. (2003) is an example of the
models that include tactical and operational decisions. This
work embeds a mixed integer program (MIP) into a decision support systems (DSS) that provides detailed plans
for farmers’ activities such as crop selection, scheduling
of ﬁeld tasks, investment analysis, machinery selection
and other aspects of the production process. The objective
of the model is the minimization of the costs incurred by
farmers during the cropping season. The model has been

successfully used as part of extension services in Spain to
provide recommendations about crops proﬁtability.
The second example is a model that deals exclusively
with operational decisions (Higgins, 2002). In this work,
models are developed to deal with operational decisions
for scheduling harvesting operations. The main objective
of this work is to minimize the costs incurred while meeting market demand constraints. The planning problem
presents two main issues, how to eﬃciently harvest the
product, and how to reduce the operational costs at a processing plant. Other byproducts of the model include
obtaining a more reliable transportation, a constant daily
supply of crops; reducing capital expenditures and reducing the cost of scheduling mechanical harvesters. The
author indicates that the successful application of this
model resulted in signiﬁcant cost savings for the sugar cane
industry of Australia.
4.2. Planning decisions for ASC of non-perishable products
The second part of Table 2 presents the classiﬁcation of
papers for non-perishable agri-foods according to the
activity of the supply chain they target. The main activities
in ASC involve the planting (P), harvesting (H), storing (I),
and distributing (D) crops to the customers downstream in
the supply chain. This table also presents the ﬁeld ‘‘SCM”,
which provides information about how many echelons of
the supply chain are considered in the models. For example, if only the decisions of the farmer are considered, it
is said that only one echelon is covered by the model. However, if decisions aﬀecting the farmer and distributor are
considered, then two supply chain echelons are being modeled. The last column of Table 2 presents succinct information on additional planning decisions addressed by the
models. Examples of such decisions include ﬁnancial and
purchasing considerations, capacity planning, crop rotation, irrigation, and fertilizer use. In the following sections
we discuss in more detail some of these models, particularly
those that are used for planning the production and distribution of crops.
4.2.1. Production models of non-perishable products
From the list of models presented in Table 2, we can
observe that production (P) related decisions are the most
common of the models presented. Usually production decisions are related to the timing and the amount to plant of
each crop, as well as to the rotation of the crops along several time periods. Most of these models are designed to
plan the production from the perspective of a single participant of the supply chain, such as determining the production of a single farm.
One example of production models is the work presented by Dobbins et al. (1992). These authors evaluate
crop production alternatives using an LP model. The LP
model includes planting, harvesting, processing, and the
storage of crops. The objective is to maximize revenues
by preparing an optimal cropping plan for the year. The

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

resources considered in the model include land, labor,
machinery, and other constraints, such as processing,
storage, and institutional constraints. At the time that the
paper was written, the model had been successfully used
for approximately 25 years, which included periodic maintenance and additions to keep up with the needs of the
farmers.
A second example of production related models is given
by the work of Schilizzi and Kingwell (1999). These
authors investigate the impact of price and yield uncertainty in cropping decisions for a farm in Western Australia. The objective is to maximize the expected utility
function of the farmers. The SP model includes decision
variables such as crop rotation, crop selection, and land
allocation. These decisions take into consideration constraints related to the soil type, crop rotation, available
crops, expected yield, the farmer’s risk attitudes and the
weather patterns. Of particular importance is the eﬀect of
the weather on production, which is modeled through a
set of discrete weather conditions with a corresponding
probability of occurrence. The models presented include
the use of farmer’s speciﬁc utility functions and the modeling of weather uncertainty.
4.2.2. Production–distribution models of non-perishable
products
Diﬀerent functions in the ASC have been traditionally
modeled independently. This is mostly due to the added
complexity of developing and ﬁnding solutions for integrated multi-echelon models (Thomas and Griﬃn, 1996).
This is particularly true when the echelons being modeled
include those for production and distribution activities.
Integrated models, although challenging to develop and
solve, oﬀer potential cost saving beneﬁts. For instance,
Chandra and Fisher (1994) report savings of up to 20%
when integrated decision models are used. In agriculture,
judging at the relative few applications that consider production and distribution decisions in the same model, this
integration has also been diﬃcult to achieve. However,
while we did not ﬁnd evidence that any of the models
had been implemented in a real operation; the integrated
models found in the literature document the potential high
beneﬁts of using such models in the planning of ASC.
An example of an integrated model is given by the work
of Apaiah and Hendrix (2005). These authors designed a
network model for growing, harvesting, transporting and
processing a pea-based product. The supply chain modeled
is divided into three phases: production (growing and harvesting), ingredient preparation (milling and concentration) and product processing. These phases are connected
by transportation links using diﬀerent transportation
modes. The objective is to minimize the overall costs of
the supply chain, which is composed of all the production
and transportation activities required to obtain the ﬁnal
product. The problem is modeled using an LP formulation
that when solved gives the amount of peas to produce at
each growing location, the amount of peas to transport

9

from the growing areas to the plants, the amount of pea
concentrate to process at the plant, the quantity of concentrate to transport to the product processing facilities, and
the products to process at each facility. One of the beneﬁts
of the model is that it provides an estimate of the costs
involved in the operation of a new product line.
4.3. Modeling approaches in ASC of non-perishable products
The modeling approaches used in agricultural planning
are presented in Table 3. These include stochastic programming (SP), linear programming (LP), dynamic programming (DP), stochastic dynamic programming (SDP), and
mixed integer programming (MIP). According to the
results reported in the literature, some of the approaches
have been applied more successfully to the planning of
ASC than others. For instance, the papers based on LP
(Dobbins et al., 1992; Higgins, 2002) and SP models (Jones
et al., 2003) have a good record of rendering successful
applications.
The most popular modeling approach in agricultural
planning has been LP. The extended use of LP models
for planning agricultural activities is surprising given the
high level of uncertainty present in the estimation of the
parameters of the models such as yield, proﬁt, etc. However, the popularity of LP can be explained by the simplicity of use and the ﬂexibility of LP models to capture a large
variety of decisions, such as crop scheduling, resource
assignment, selection of production methods, and investment decisions (Hazell and Norton, 1986).
An example of LP modeling is provided by Vitoriano
et al. (2003). This model is used to plan farm resources
and to schedule the diﬀerent activities required for growing
the crops. The overall objective of the model is to minimize
total costs. The model considers time windows, precedence
and resource constraints to restrict the scheduling of production activities in the farm. The paper considers two
modeling approaches, one that partitions time into discrete
units, and a second one that uses a continuous time horizon. The former is preferred for short term planning horizons, while the latter is used for long planning horizons
with loose time windows. For a more general perspective
on the developments of LP modeling in agricultural planning the reader is referred to Hazell and Norton (1986)
and Dent et al. (1986).
Some authors have modiﬁed traditional LP models to
account for the uncertainty present in most farming activities. The eﬀects of uncertainty are particularly important if
farmers are risk averse, which it has been traditionally
assumed in the economics literature (Hardaker et al.,
1991). The modeling of uncertainty and risk attitudes in
the objective function has been called risk programming.
The formulation of objective functions includes the
mean–variance (E–V), minimization of the total absolute
deviations (MOTAD), utility maximization and other formulations. We are aware of at least two previous reviews
related with RP, the ﬁrst one, by Hardaker et al. (1991),

10

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

Table 3
Modeling approaches used for planning non-perishable agricultural products
Model
Torkamani (2005)
Kobzar et al. (2005)
Apaiah and Hendrix (2005)
Jiao et al. (2005)
Biswas and Pal (2005)
Visagie et al. (2004)
Jones et al. (2003)
Recio et al. (2003)
Vitoriano et al. (2003)
Higgins (2002)
Maatman et al. (2002)
Gigler et al. (2002)
Glen and Tipper (2001)
Lien and Hardaker (2001)
Ekman (2000)
Schilizzi and Kingwell (1999)
Raju and Kumar (1999)
Higgins et al. (1998)
Abdulkadri and Ajibefun (1998)
Sumanatra and Ramirez (1997)
Lazzari and Mazzetto (1996)
Torkamani and Hardaker (1996)
Burton et al. (1996)
Nevo et al. (1994)
Duﬀy and Taylor (1993)
Kaiser et al. (1993)
Dobbins et al. (1992)
Adesina and Sanders (1991)
Nanseki and Morooka (1991)
Alocilja and Ritchie (1990)
Turvey and Baker (1990)
Bin Deris and Ohta (1990)
Perry et al. (1989)
Clarke (1989)
Kaiser and Apland (1989)
Lambert and McCarl (1989)
Turvey et al. (1988)
Tan and Fong (1988)
Glen (1986)
El-Nazer and McCarl (1986)
Butterworth (1985)
Stoecker et al. (1985)

LP

SP

DP

SDP

MIP

X

Other aspects
Nonlinear SP
Risk programming

X
X
X
X

Regression analysis
Fuzzy goal programming
Risk programming

X
X
X
X

X

Decision support systems
Tabu search

X
X
X

X
X
X
X

Time series

X
X
X

MCDM and constraint prog.
Modeling to generate alternatives
X

X
Search methods

X
X
X

Expert systems
Time series
Simulation and time series

X
X
X
X
X

Simulation
Utility functions

X
X

X
X

X
X
X
X
X
X
X

Time series and regression
Time series and regression
Risk programming
Multiple objectives and MOTAD
X
MOTAD
X

X

X

presents a list of programming models for farm planning
under uncertainty, with particular focus on RP models.
The second, by Backus et al. (1997), reviews several aspects
of farm decision-making under risk, including utility functions, risk preferences and modeling approaches that have
been applied in RP problems.
The quest for more realistic modeling alternatives has
popularized the use of stochastic programming. Examples
of the use of stochastic programming in agricultural planning include the models developed by Jones et al. (2001,
2003). The authors model the production of crops to
obtain seeds for a seed-corn company, with two sequential
production periods under random yields and uncertain
demand. The decision variables include the amount of
crops to be produced in a ﬁrst period planted in spring
and harvested in late summer (North America), and the

production in the second period harvested in winter (South
America) to satisfy an uncertain annual seed demand of the
spring of the following year. The objective of the problem
is to maximize the expected gross margin given the costs of
production incurred and expected yields at the two-stages.
The authors report that the use of the SP model and the
application of the proposed planning methodology resulted
in increasing the proﬁt margins of the company by 24%.
Also popular in the agricultural planning literature is the
use of DP (Stoecker et al., 1985) and SDP (Sumanatra and
Ramirez, 1997). These models have traditionally been used
in multi-period settings, where the decisions made in the
time period being analyzed have consequences over several
periods into the future. The decisions considered in the
models reviewed include decisions such as irrigation
planning, and the long term planning of crops (Duﬀy and

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

Taylor, 1993). The reader interested in speciﬁc details for
developing DP models for agricultural problems is referred
to Taylor (1993).
Additional modeling approaches in agricultural planning include the use of simulation for estimating the
growth and yield of crops (Alocilja and Ritchie, 1990),
fuzzy programming (Biswas and Pal, 2005), and search
methods to ﬁnd useful solutions (Lazzari and Mazzetto,
1996). Other tools have been used in combination with
LP, SP and DP, such as time series analysis (Lien and
Hardaker, 2001), utility function elicitation (Turvey and
Baker, 1990), decision support systems (Recio et al.,
2003) and expert systems (Nevo et al., 1994).
5. Planning models for ASC of fresh products
The second part of our review covers those papers that
deal with fresh or perishable agri-foods. The complete list
of reviewed models for fresh agri-foods is presented in
Table 4. This table includes the main objectives of the
papers and their corresponding authors. Comparing Tables

11

4 and 1, we can observe that there are fewer articles dealing
with perishable agri-foods than with non-perishable ones.
However, given the increasing economic importance of perishable agri-foods and the renewed interest on food safety,
we expect that the number of papers published in this area
will increase in the near future. In fact, as it can be seen in
Table 4, most of the papers that focus in perishable products have been published recently. In the following sections, we will dissect these works using the same criteria
presented in Section 4.
5.1. Planning scope for ASC of fresh products
Table 5 organizes the papers in terms of the planning
scope of the models presented. The models in the papers
can be classiﬁed into strategic (S), tactical (T), and operational (O) categories. Because of its implications for ASC
planning, the table also identiﬁes whether the model presented include a shelf life feature (SL). However, as it can
be observed from Table 5 only a few papers explicitly
model the shelf life of agri-foods. The ﬁfth column of Table

Table 4
List of models for fresh agricultural products
Model

Main objective of the paper

Ferrer et al. (2008)

Determine a plan for the optimal scheduling of the harvest of wine grapes using a LP model with the objective of minimizing
operational and grape quality costs
Design of a DP model to integrate production, harvest and storage of perishable items with growth and loss functions for
maximizing the demand satisﬁed
Development of a LP that links chemical, biological and logistics constraints to the quality of the fruit to harvest, with objective
of maximizing revenue
Design a two-stage SP to determine the olive trees to contract in the season for an oil producer with uncertain yield and
demand, for maximizing revenue
Determine the optimal rate of harvesting and capital investment (capacity) using a nonlinear program, to reduce losses by
weather and overcapacity
Design a production–distribution model for the supply chain of a seedlings with the objective of minimizing costs
Design a model for crop planning with uncertain values, described with fuzziness and randomness, with the objective of
maximizing minimum value of revenue
Develop a LP model for maximizing the expected gross revenue of a greenhouse by designing an appropriate marketing and
planting plan
Develop a whole farm model to compare between diﬀerent farming technologies before empirical work starts. With economic
and environmental goals
Design of a SP model for determining the optimal planting plans for a vegetable crop with the help of weather scenarios, with a
revenue maximizing objective
Determine an eﬃcient cropping pattern by considering the risk of the producers with a multi-objective (max revenue, min
variability) model
Design of a production model with tactical and operational decisions with the objective of increasing proﬁtability
Develop optimal production and marketing decisions for a nursery producing ornamental plants using SDP with revenue
maximizing objective
Develop a SP model that optimizes revenue by changing the capacity of food preservation facilities and considering the
uncertainties in crop markets
Determine a plan for production and harvesting of a packing plant with a LP and fuzzy programs with the objective of
minimizing costs
Determine a planting and harvesting plan for fresh crops using a LP model with the objective of maximizing proﬁts
Develop a RP decision model for landscape land production, with the objective of maximizing returns for a given level of risk
aversion
Determine sowing, harvesting and production plans using a LP model with the objective of minimizing costs across the
logistical chain
Determine a plan for the location of pot-plants inside a greenhouse with the objective of minimizing costs using heuristics and
genetic algorithms
Develop a plan for a pot-plant greenhouse with two models, one LP for future plans and one MIP for transition plans, with the
aim of maximizing revenue

Widodo et al. (2006)
Caixeta-Filho (2006)
Kazaz (2004)
Allen and Schuster
(2004)
Rantala (2004)
Itoh et al. (2003)
Caixeta-Filho et al.
(2002)
Berge ten et al. (2000)
Darby-Dowman et al.
(2000)
Romero (2000)
Leutscher et al. (1999)
Stokes et al. (1997)
Aleotti et al. (1997)
Miller et al. (1997)
Hamer (1994)
Purcell et al. (1993)
van Berlo (1993)
Annevelink (1992)
Saedt et al. (1991)

12

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

Table 5
Planning scope and decision variables for fresh agricultural products
Model

Planning scope
S

Ferrer et al. (2008)
Widodo et al. (2006)
Caixeta-Filho (2006)
Kazaz (2004)
Allen and Schuster (2004)
Rantala (2004)
Itoh et al. (2003)
Caixeta-Filho et al. (2002)
Berge ten et al. (2000)
Darby-Dowman et al. (2000)
Romero (2000)
Leutscher et al. (1999)
Stokes et al. (1997)
Aleotti et al. (1997)
Miller et al. (1997)
Hamer (1994)
Purcell et al. (1993)
van Berlo (1993)
Annevelink (1992)
Saedt et al. (1991)

X
X

X

X

T

O

SL

A

DM

X
X
X
X

X
X

X
X

Y/N
N
Y/N
Y/N
Y/Y
Y/N
N
Y/Y
Y/N
Y/N
N
N
Y/N
Y/N
Y/N
Y/N
Y/N
Y/N
N
Y/Y

Planner
SC
Planner
Planner
Planner
SC
Farmer
Farmer
Advisor
Farmer
Planner
Farmer
Farmer
Farmer
Planner
Farmer
Advisor
Farmer
Farmer
Farmer

X
X
X
X
X
X
X
X
X

X

X

X

X

X

Decision variables

X
X
X
X

X
X

P

H

D

I

X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X

X
X
X
X
X
X

X

X

X

X
X

X

SCM

Other decisions considered

1
2
2
1
1
2
1
1
1
1
1
1
1
1
1
1
1
2
1
1

Labor and routing

Purchase from other source
Capacity planning
Open/close facilities

Technology selection
Capacity decisions
Operational policies
Selling or retain
Preservation technology
Variety selection
Processing schedule
Spatial location
Transition planning

S: strategic, P: production variables/decisions, T: tactical, H: harvesting variables/decisions, O: operational, D: distribution variables/decisions, A:
application of the models, I: inventory variables/decisions, DM: decision maker for which the model is designed, SCM: echelons of the supply chain.

5 provides information about the extent of the application
of these models (A). This column shows whether the papers
provide evidence that the described models were implemented and used (Y/Y); just applied to a case study, but
not to a speciﬁc real life situation (Y/N); or not applied
at all (N). As it can be seen from the table, only a few
works are motivated and fully applied to a real operation.
Finally, the sixth column of the table identiﬁes the targeted
user of the models; which we labeled as the decision maker
(DM). In the subsequent subsections we dissect the models
presented in Table 5 using the same criteria previously used
in Section 4. We also discuss some representative examples
of models for each classiﬁcation criteria. In the selection of
papers to discuss we favored those that had been successfully applied to solve a concrete and real problem.
5.1.1. Models for strategic planning of ASC of fresh products
The strategic models for perishable agri-foods, shown in
Table 5, cover several types of decisions, such as the design
of supply networks, ﬁnancial planning, capacity, and technology selection. Some of the most common objective functions of these models include proﬁt/revenue maximization
and cost minimization. Most of the models identiﬁed that
cover strategic decisions also include some aspects of tactical planning. In the examples presented next we discuss two
models, one that covers exclusively strategic level decisions
and another one that covers both strategic and tactical
aspects of planning.
Allen and Schuster (2004) developed a model for calculating the capacity and rate of harvest required for the production of grapes. The objective of the model is to
minimize the losses in crops, caused by weather, and to

minimize overinvestment costs of installing excess capacity.
A major contribution of the paper is the use of nonlinear
programming to reduce the risk of uncertain weather.
The beneﬁts reported from the use of this model include
$2 million in capital avoidance from harvesting equipment
and improved risk assessment for the incorporation of new
crop areas.
Berge ten et al. (2000) developed a model for a farm to
compare the potential performance of alternative farming
technologies. The objective of the model is to select those
technologies that give the best tradeoﬀ between economic
and environmental goals. The authors present a case study
of the methodology using a problem that is modeled as a
multiple-goal linear program for planning the optimal crop
rotation for ﬂower bulb farming. The objective function of
the model is to maximize farm gross margin, and to minimize the use of pesticides and fertilizers. The strategic decisions included the selection of growing technology and the
tactical decisions included the selection of crop rotations.
5.1.2. Models for tactical planning of ASC of fresh products
Tactical planning models are the most popular applications for fresh ASC (Table 5). Some of the decisions presented in these models include crop scheduling, harvest
planning, crop selection, and labor capacity. We now present two tactical planning models that have been implemented, and have provided signiﬁcant beneﬁts to the
farmers, attesting of the potential beneﬁts of these types
of models in ASC.
The ﬁrst example is the work of Caixeta-Filho et al.
(2002). These authors use an LP model for planning the
production of ﬂowers in a Brazilian greenhouse. The main

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

13

decision variable is the number of ﬂowers to produce in
each speciﬁc greenhouse at a particular time period. The
model includes decisions for planting and harvesting in several periods of the year. Some of the constraints of the
model include the amount to harvest and plant for each
period. The objective of the model is to satisfy the demand
of customers while maximizing revenue. The LP model is
embedded in the planning software of the greenhouse company, thus giving the decision makers and supervisors the
tools for planning the operations of the company. The
reported beneﬁts of using this model are additional sales
and proﬁts, with a 32% increase in the farmers’ proﬁt
margin.
Another application of tactical planning to greenhouse
production is provided by Saedt et al. (1991), who develop
a production planning model for a pot-plant greenhouse.
The model handles two types of plans, one for future production and a transition plan to move from current state of
the greenhouse to one that meets the future production
needs. The future and transition production plan is determined with the help of an LP model. The decisions
included in the model are production scheduling, determination of labor and space needs. The beneﬁts obtained
from the implementation of this model include increasing
the net proﬁts by about 10%.

production period. The objective of the model is to minimize costs and to increase the utilization of a greenhouse.
The model is solved in an iterative way by solving tactical
and operational models, with the help of heuristics such as
clustering heuristics and genetic algorithms that provide
good working solutions.

5.1.3. Models for operational planning in ASC of fresh
products
In this section, we present the papers for fresh ASC that
focus on short-term or operational planning. Among the
operational decisions considered in these models are harvesting, scheduling of production activities, intermediate
storage and packing planning. Comparing the number of
operational models for perishable products shown in Table
5 with those for non-perishable commodities shown in
Table 2, we can observe that there is a greater emphasis
on short-term planning in the production of perishable
products. The diﬀerences between the models for fresh
and non-perishable crops support the idea that the operational decisions in the management of highly perishable
products are extremely important. The particular characteristics of operational models for the fresh ASC are illustrated next through the description of some of the papers
presented in Table 5.
Miller et al. (1997) develop two models for harvesting
and packing fresh tomatoes, one using an LP formulation
and a second one using a program obtained by adding
fuzzy type constraints to the model. Some of the decisions
included in the model are the quantity to harvest per period
and the inventory to keep for the next period. The objective
is to minimize the cost of operation in the harvesting and
packing operations.
A second example of operational planning dealing with
production scheduling in a pot-plant greenhouse is given
by the work of Annevelink (1992). This model takes the
information of a tactical crop mix plan, and develops an
operational plan for the spatial allocation of pots in each

5.2.1. Production models for fresh products
Production decisions are the most popular in the models
presented in Table 5. Models dealing with production planning for greenhouses are particularly proliﬁc. Production
decisions include determining the amount, mix and timing
for planting each crop, and the scheduling of resources
such as labor and transportation. Some of the common
objectives of these models include the minimization of costs
and maximization of proﬁts subject to demand constraints.
Kazaz (2004) presents an SP model for a Turkish company producing olive oil. The company has the option of
leasing the olive trees to grow the olives or to buy the olives
in the open market at a higher price. The planning model
consists of two-stages, where the decisions at each stage
depend on the stochastic distribution of demand and the
uncertain yield of the olive trees. In the ﬁrst-stage the company determines the amount of trees to lease, and in the
second-stage, based on the yield and the prices of olives
in the open market, the company determines the amount
of olive oil to produce and olives to buy from the farmers.
The objective of the model is to maximize the expected
proﬁt subject to demand and the sales price of the olive oil.
van Berlo (1993) presents a model to plan and coordinate
the production and supply of raw materials from the ﬁeld to
a processing plant. The targeted operation is a vertically
integrated vegetable processing industry. The coordination
is performed with the use of a linear goal programming
model that satisﬁes several competing objectives that
include the minimization of the cost of sowing, optimization
of the utilization of the processing plant, and meeting
the market’s demand. This model considers not only the

5.2. Planning decisions for ASC of fresh products
In the second part of Table 5, we present the diﬀerent
decisions variables of the models reviewed. These include
production (P), harvesting (H), distribution (D), and storage (I). When comparing Tables 2 and 5, it is evident that
the papers reviewed are clustered around production and
harvesting decisions with distribution and storage falling
behind. It is also evident from the table that few models
combine these decisions, to develop production–distribution or harvesting–distribution models. Comparing Tables
2 and 5, we notice that Table 5 contains a higher number of
papers dealing with harvesting decisions. This may be the
result of the short shelf life of the products and the lack
of mechanized equipment for harvesting these crops. Other
decisions in the models include labor planning, capacity
planning, spatial location, technology selection, purchase
decisions, and processing schedule.

14

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

planning of activities at the farm level but also the supply of
these agri-foods to a processing plant down the supply
chain and the planning of production at the plant. The
motivation of the model is to coordinate the production
and harvest activities to meet market’s demand in terms
of quantity and quality.
5.2.2. Production–distribution models for fresh products
The number of models designed for supporting production–distribution decisions is still small compared to the
total number of papers reviewed. We found only two examples supporting production–distribution decisions, none of
which had been fully implemented. We believe that this
lack of production–distribution applications will change
as the industry materializes the potential beneﬁts of including the shelf life of the products in this type of models and
the other beneﬁts that have been reported in other segments of the industry when using production–distribution
planning (see Section 4.2.2).
Rantala (2004) presents an MIP model for designing the
integrated production–distribution plans for the seedling
supply chain of a ﬁnnish nursery company. Some of the
decisions included in the model are the total number of
seedlings to be produced and transported from nurseries
to cooled warehouses, or transported directly to customers,
or transported from warehouses to customers. The model
also includes capacity constraints and capacity-related
decisions. The main objective of the model is to minimize
the total cost of producing and transporting the products
needed to meet customers demand.
Aleotti et al. (1997) provides a second example of production–distribution models. He describes an MIP formulation for selecting the best design for the post-harvest
handling of fresh vegetable crops between the harvest
and the ﬁnal market. The purpose of Aleotti’s research is
to maximize the beneﬁts from capital investment in foodpreservation facilities under conditions of uncertain production and demand. The uncertainty in the environment
is modeled as an SP problem using a set of market and crop
scenarios. The objective of the model is the maximization
of the expected proﬁt through the selection of the best combination of post-harvest processes.
5.2.3. Harvesting models for fresh products
The most common activities of the harvesting models
reviewed included decisions related to the amount of product to harvest per period, how to transport the harvested
product, how to allocate transportation equipment, and
the scheduling activities of packing and processing plants.
The work by Ferrer et al. (2008) can be considered a
good representative of the papers dealing with harvesting
planning. This paper presents an MIP model for optimally
scheduling the harvesting operations of wine grapes. The
model considers the costs of harvesting activities and the
loss of quality of the grapes for delaying harvesting. The
decisions in this model include the amount of grapes to
harvest from the diﬀerent plots in each period, the routing

of harvesting among plots and the number of workers to
hire or lay oﬀ for each period of the harvesting season.
One of the main contributions of this model is the representation of the quality loss in the objective function of the
model.
A second paper dealing with a harvesting model (Caixeta-Filho, 2006) uses an LP formulation to link the pertinent
chemical, biological and logistical restrictions to the quality
of the fruit to be harvested. The model considers two
potential objective functions, one that maximizes the number of boxes of fruit produced and another that maximizes
total revenue. The second objective was considered a better
objective for the case of an orange juice producer that
schedules the harvest of several independent farmers. The
decision variables of the model are the monthly amount
of crop to harvest from a grove.
5.3. Modeling approaches in ASC for fresh products
The main modeling approaches used in the papers listed
in Table 6 are LP, MIP, SP, DP and SDP. Other
approaches used include growth simulation, nonlinear
optimization, fuzzy programming, risk programming, goal
programming and multi-objective programming. As it was
the case in non-perishable agri-foods, the most popular
modeling approach for fresh agri-foods, and the one with
the most successful applications, is LP. We now describe
some examples of the papers aimed at the planning of fresh
agri-foods activities.
Hamer (1994) uses an LP model to determine the best
planting and scheduling decisions to assure a steady supply
of Brussels sprouts over a long planning horizon. The
author assumes the demand and quality of the product is
known in advance and that there exists a way to estimate
the distribution of the yield for diﬀerent crops. The main
objective of the model is to satisfy the market demand
and to maximize proﬁt, subject to factors such as scheduling of transplanting, direct drilling, grading, packaging,
seeding, land preparation, growing, and harvesting.
An extension of Hamer’s model is presented by DarbyDowman et al. (2000). The model uses the results of
Hamer’s model as an input for an SP model. The main contribution of this paper is the introduction of stochastic
behavior and a utility function to minimize the risk
incurred by the grower, resulting in a robust production
plan. The decision variables for the model are the amount
of land allocated to each crop, the timing of the sowing, the
amount of product to harvest, sell and purchase to satisfy
the demand of the customers. The yield of the products is
assumed uncertain, due to the weather variability. The
weather-yield relationship is formulated using 31 weather
scenarios. The results from the experiment indicate that
using a stochastic model rendered more robust plans than
just using deterministic models.
Widodo et al. (2006) present a diﬀerent approach for
integrating the production, harvesting and inventory planning of ﬂowers through the use of growth and loss func-

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

15

Table 6
Modeling approaches used for planning fresh agricultural products
Model

LP

Ferrer et al. (2008)
Widodo et al. (2006)
Caixeta-Filho (2006)
Kazaz (2004)
Allen and Schuster (2004)
Rantala (2004)
Itoh et al. (2003)
Caixeta-Filho et al. (2002)
Berge ten et al. (2000)
Darby-Dowman et al. (2000)
Romero (2000)
Leutscher et al. (1999)
Stokes et al. (1997)
Aleotti et al. (1997)
Miller et al. (1997)
Hamer (1994)
Purcell et al. (1993)
van Berlo (1993)
Annevelink (1992)
Saedt et al. (1991)

X

SP

DP

SDP

MIP

Other aspects

X

Relaxation heuristic
Growth and loss functions

X
X
X

Nonlinear optimization
Nonlinear optimization

X
X
X
X

X
Fuzzy programming
Multi-objective programming
X

X
X

Risk programming
Simulation and regression
X
X

X

X
X
X
X
X
X

Fuzzy programming
Decision support system
Risk programming
Goal programming
Genetic algorithm

Table 7
Other agricultural supply chains planning models
Model

Main objective of the paper

Schepers and van Kooten
(2006)
Higgins and Laredo (2006)

Plan the value chain of fresh fruits (producer, trader and retailer) using systems dynamics with the objective of maximizing
total revenue
Develop an IP model for harvesting and transporting crops, together with the rationalization of railroads with the objective
of minimizing total cost
Develop a framework for integrating harvesting and transportation decisions in the Australian sugar value chain to
minimize costs
Schedule harvest date and crop cycle, considering transportation and capacity restrictions using an IP model that maximizes
the net revenue
Develop a model for estimating the quality of harvested products aﬀected by temperature, chilling injury, and diﬀerent levels
of initial quality
Plan the use of new technologies, demand management, and sensitivity analysis to improve the performance of a cranberry
packing plant
Develop a tactical plan for capacity and staﬃng decisions for improving the eﬃciency of a cranberry packing plant using
queuing models

Higgins et al. (2004)
Higgins (1999)
Tijskens and Polderdijk
(1996)
Porteus (1993b)
Porteus (1993a)

tions. They use a DP model to deal with periodical harvests
subject to periodical ﬂowering for maximizing the level of
demand coverage per period. The objective is the minimization of the loss caused by premature harvesting, and the
loss from transporting and storing products at the retailer’s
site. The main decision variable is the amount of product to
be harvested at each harvesting period.
Stokes et al. (1997) presents an SDP model for managing a nursery, with two interesting features, the consideration of after-tax proﬁts and the uncertainty of proﬁt.
The problem is to arrive to an optimal marketing and production plan for a nursery that produces ornamental
plants. The nursery considered, produces diﬀerent sizes of
crops. The crops increase their value with growth, but this
growth also results in higher operating costs. The states of
the model are the production area dedicated to each type of
crop, a possible carry-over loss, and the net income
obtained. The decisions include the determination of the

size and the timing of the crops to sell. The risks faced
by the producers include cost, and yield uncertainty. These
risks are assumed to be reﬂected in the stochastic behavior
of the prices obtained.
6. Other related models
As part of the literature review, we found other papers
that although related to agricultural planning, do not
directly ﬁt the classiﬁcation scheme used in this paper.
Table 7 presents a list of these papers with the intention
of informing the interested reader of additional contributions in the planning of ASC.
7. Conclusions
Diﬀerent conclusions can be drawn from the previous
review. One is that the use of integrated planning models

16

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

in the ASC is still very limited. While we believe that these
models would be useful in the modeling of all the agri-food
products, they would be particularly useful for perishable
crops. Although integrated models are inherently more
complex, than those dealing with a single planning aspects,
the potential beneﬁts of these models usually outweigh the
added complexity. This is particularly true in planning the
coordination of production and distribution activities for
large and medium size companies (Boehlje, 1999). The need
for integrated models is reinforced by Perosio et al. (2001)
who recognize the increasing importance of grower/shippers who are in charge of not only producing the crops
but also of their distribution. The importance of these
growers in the ASC is expected to expand as more retailers
and processors continue to buy directly from producers,
bypassing the traditional wholesalers and intermediaries
(Kaufman et al., 2000). For these growers the use of integrated models to better plan their activities might represent
substantial savings and increased eﬃciencies.
A second ﬁnding that can be drawn from the reviewed
papers is that planning models dealing with perishable
products very often fail to incorporate realistic stochastic,
and shelf life features present in the diﬀerent echelons of
the supply chain. Perhaps the reason for this lack of more
realistic scenarios is the added complexity of ﬁnding solutions for the resulting models. In the few cases that reality-based stochastic features were introduced into the
models the results justiﬁed the added complexity of the
model (Jones et al., 2003; Allen and Schuster, 2004). Most
troubling is the lack of shelf life features in the majority of
the models developed for planning perishables agri-foods,
since these features are essential for maintaining the quality
and freshness of perishable products.
We also found that there are a limited number of models
dealing with operational planning. This paucity of applications is evident in the case of integrated models that aim at
planning more than one aspect of the ASC. Given the thin
proﬁt margins observed by the producers, eﬃcient operational planning could make the diﬀerence between a successful and an unproﬁtable operation. The relevance of
operational models is even more accentuated in the case
of perishable crops because of the critical impact of their
limited shelf life on harvesting and transportation
decisions.
Finally, judging by the numbers of published papers, we
concluded that the focus of agricultural planning has been
mostly on non-perishable products. However, we also
detected a change in this trend since most of the papers
aimed at perishable products have been developed in the
last six years. Perhaps the lack of research on perishable
products was due to the perceived less importance of these
crops over the traditional or program crops such wheat,
corn and cotton. However, there is a new reality since the
current markets for fresh products are very dynamic and
even evolving faster than traditional crops (Huang and
Sophia, 2004). The ever increasing demand of consumers
for healthy products and the more stringent regulations

in the handling of fresh products will undoubtedly create
the need for improving the current supply chain planning
practices. Judging from the publication trends, we believe
that this need is already being reﬂected in the papers
reviewed and we expect that the research activity in the
area will increase signiﬁcantly in the near future.
8. Identiﬁcation of gaps in the literature and call for research
In closing, we would like to give an assessment of the
gaps in the existing literature on planning models of the
ASC. In order to identify these gaps we take two diﬀerent
approaches. The ﬁrst approach is to compare and contrast
the existing research and research trends in planning models for ASC to those related planning activities within the
manufacturing supply chains, a sector considerably more
research-mature than that of ASC. The second approach
is to assess the future needs of the industry based on projecting the current trends of the industry into the future.
Regarding the ﬁrst approach, we believe that the state of
the art in models for planning ASC are still lagging behind
the research aimed at some manufacturing supply chains,
such as electronics and automotive manufacturing.
Researchers in manufacturing supply chains are currently
developing models for designing supply chain networks
for local and international markets (Goetschalckx et al.,
2002; Meixell and Gargeya, 2005), coordinating the activities of companies in the supply chain (Sarmiento and Nagi,
1999; Thomas and Griﬃn, 1996), planning transportation
operations and developing information management systems (Stadler and Kilger, 2005; Helo and Szekely, 2005).
Of particular relevance is the research on supply chain
coordination, which identiﬁes the activities and polices to
be pursued by the diﬀerent supply chain participants to
obtain the maximum beneﬁt of the entire supply chain
(Kouvelis et al., 2006; Chen and Paulraj, 2004). Evidence
of these coordination-needs in ASC, is the development
of programs such as eﬃcient consumer response and other
supply chain coordination initiatives that have been championed by retailers. Among the preferred tools for supply
chain coordination, has been the use of contracts, which
includes policies for buying, selling, delivering, and pricing
of products. Similar contracting arrangements have also
been gaining popularity in ASC, but still there is a need
to research their design and eﬀects for the particular characteristics of the agricultural markets (MacDonald et al.,
2004). Other areas of expertise in manufacturing supply
chains are internal logistics, which include the activities
within a single ﬁrm that are necessary for the eﬃcient ﬂow
of services and goods (CLM, 2006). An evident gap is the
lack of models applied to the distribution of perishable
products, such as those developed in the inventory literature (Goyal and Giri, 2001).
Regarding the identiﬁcation of future needs based on
industry trends, we can mention the industry consolidation
and the vertical integration of the supply chains. The consolidation of the agri-food industry has evolved from the

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

need for economies of scale, strategic positioning, risk management and market control (Boehlje, 1999). On the other
hand, vertical integration has been motivated by a host of
technological, regulatory and ﬁnancial reasons, in addition
to changes in consumer preferences, such as increased
quality and product safety (Hobbs and Young, 2000). These
trends have motivated new initiatives in ASC, such as
traceability, quality certiﬁcations, food safety, and quick
response just to name a few of the latest developments in
the industry (Bourlakis and Weightman, 2004). Some of
these trends and eﬀorts sometimes are lumped together
under the term ‘‘Agroindustrialization of Operations.” This
indicates that there are now more similarities between
manufacturing supply chains and ASC than ever before
(Reardon and Barret, 2000). In response to these challenges
some potential innovations can be identiﬁed. For instance,
we believe that there is a need for models that include more
realistic features, such as uncertain information, logistics
integration, risk modeling, regulatory environment, quality
and security of products. In particular, we have identiﬁed
the need for stochastic models for the tactical planning of
perishable and non-perishable agri-foods. Stochastic models can be used to plan the production of crops, and to make
these plans robust to uncertainty. We envision the extension
of these models to incorporate other risk reduction alternatives, such as the use of contracts, ﬁnancial and real options
as diversiﬁcation strategies. Such models can aid the growers to make more holistic decisions, in terms of risk and
expected revenues. Although some of these risk reduction
issues have been modeled in the past, they have not considered market, production, distribution and the uncertainty
of the models’ parameters.
Other potential contributions include operational models which integrate production and distribution decisions.
The need for such logistical models has promoted the emergence of the ﬁeld of ‘‘Agribusiness Logistics”, which studies
the impact of logistical issues in ASC (Biere, 2001). The
importance of Agricultural Logistics issues is particularly
evident in the case of perishable products where the limited
shelf life of the product requires a very careful planning of
the transportation and inventory decisions to reduce the
deterioration of the products and preserve their value. In
our opinion, there is a particularly a conspicuous lack of
adequate models for planning operational decisions for
production/harvest and distribution for perishable crops.
The development of these models is an immediate need
not only for the beneﬁt of industry but also for the beneﬁt
of the ﬁnal consumer.

Acknowledgements
The authors would like to acknowledge the anonymous
reviewers of this paper whose input signiﬁcantly improved
its quality. We also would like to acknowledge the Confederation of Associations of Growers in the State of Sinaloa
(CAADES) and Mexico’s National Council for Science

17

and Technology (CONACYT) for their support for the
realization of this research.
References
Abdulkadri, A., Ajibefun, I.A., 1998. Developing alternative farm plans for
cropping systems decision-making. Agricultural Systems 56 (4), 431–442.
Adesina, A.A., Sanders, J.H., 1991. Peasant farmer behavior and cereal
technologies: Stochastic programming analysis in Niger. Agricultural
Economics 5, 21–38.
Aleotti, L.O., Araujo, R., Yahya, R., 1997. Selection of postharvest
technology routes by mixed-integer linear programming. International
Journal of Production Economics 49, 85–90.
Allen, S.J., Schuster, E.W., 2004. Controlling the risk for an agricultural
harvest. Manufacturing & Service Operations Management 6 (3), 225–
236.
Alocilja, E.C., Ritchie, J.T., 1990. The application of SIMOPT2: Rice to
evaluate proﬁt and yield-risk in upland-rice production. Agricultural
Systems 33, 315–326.
Annevelink, E., 1992. Operational planning in horticulture: Optimal space
allocation in pot-plant nurseries using heuristic techniques. Journal of
Agricultural Engineering Research 51, 167–177.
Apaiah, R.K., Hendrix, E.M.T., 2005. Design of supply chain network for
a pea-based novel protein foods. Journal of Food Engineering 70, 383–
391.
Aramyan, C., Ondersteijn, O., van Kooten, O., Lansink, A.O., 2006.
Performance indicators in agri-food production chains. In: Quantifying the Agri-Food Supply Chain. Springer, Netherlands (Chapter 5),
pp. 49–66.
Backus, G.B.C., Eidman, V.R., Dijkhuizen, A.A., 1997. Farm decision
under risk and uncertainty. Netherlands Journal of Agricultural
Science 45, 307–328.
Beamon, B.M., 1998. Supply chain design and analysis: Models and
methods. International Journal of Production Economics 55, 281–294.
Berge ten, H.F.M., van Ittersum, M.K., Rossing, W.A.H., van de Ven,
G.W.J., Schans, J., van de Sanden, P.A.C.M., 2000. Farming options
for the Netherlands explored by multi-objective modeling. European
Journal of Agronomy 13, 263–277.
Biere, A., 2001. Agribusiness Logistics: An Emerging Field in Agribusiness Education, International Food and Agribusiness Management
Association, Agribusiness Forum and Symposium, Sydney, Australia,
<http://www.ifama.org/conferences/2001Conference/Papers/
Area%20I/Biere_Arlo.PDF>.
Bin Deris, S., Ohta, H., 1990. A machine-scheduling model for large-scale
rice production in Malaysia. Journal of the Operational Research
Society 41 (8), 713–723.
Biswas, A., Pal, B.B., 2005. Application of fuzzy goal programming
technique to land use planning in agricultural systems. Omega 33, 391–
398.
Boehlje, M., 1999. Structural changes in the agricultural industries: How
do we measure, analyze and understand them? American Journal of
Agricultural Economics 81 (5), 108–1041.
Boehlje, M., Fulton, J., Gray, A., Nilsson, T., 2003. Strategic Issues in the
Changing Agricultural Industry, Purdue University, Department of
Agricultural Economics, CES-341.
Bourlakis, M.A., Weightman, P.W.H., 2004. Food Supply Chain Management. Blackwell Publishing, Oxford, UK.
Burton, R.O., Crisostomo, M.F., Berends, P.T., Kelley, K.W., Buller,
O.H., 1996. Risk/return analysis of double-cropping and alternative
crop rotations with and without government programs. Review of
Agricultural Economics 18, 681–696.
Butterworth, K., 1985. Practical application of linear/integer programming in agriculture. Journal of the Operational Research Society 36
(2), 99–107.
Caixeta-Filho, J.V., van Swaay-Neto, J.M., Wagemaker, A.P., 2002.
Optimization of the production planning and trade of lily ﬂowers at
Jan de Wit Company. Interfaces 32 (1), 35–46.

18

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

Caixeta-Filho, J.V., 2006. Orange harvesting scheduling management: A
case study. Journal of the Operational Research Society 57 (6), 637–
642.
Chandra, P., Fisher, M.L., 1994. Coordination of production and
distribution planning. European Journal of Operational Research 72,
503–517.
Chen, I.J., Paulraj, A., 2004. Understanding supply chain management;
critical research and theoretical framework. International Journal of
Production Research 42 (1), 131–163.
Chopra, S., Meindl, P., 2003. Supply Chain Management: Strategy,
Planning and Operation. Pearson Education, Inc., Upper Saddle
River, New Jersey.
Christopher, M., 2005. Logistics and Supply Chain Management. Prentice
Hall, London.
Clarke, H.R., 1989. Combinatorial aspects of cropping pattern selection in
agriculture. European Journal of Operational Research 40, 70–77.
Council of Logistics Management (CLM), 2006. <http://www.clm1.org>,
Consulted on November.
Darby-Dowman, K., Barker, S., Audsley, E., Parsons, D., 2000. A twostage stochastic programming robust planting plans in horticulture.
Journal of the Operational Research Society 51, 83–89.
Dent, J.B., Harrison, S.R., Woodford, K.B., 1986. Farm Planning with
Linear Programming: Concept and Practice. Butterworths.
Dobbins, C.L., Preckel, P.V., Han, Y., Doster, D.H., 1992. An application
of linear programming to planning crop systems. In: Proceedings of
fourth International Conference on computers in Agricultural Extension Programs, January 28–31, Orlando, FL.
Duﬀy, P.A., Taylor, C.R., 1993. Long-term planning on a corn-soybean
farm: A dynamic programming analysis. Agricultural Systems 42, 57–71.
Ekman, S., 2000. Tillage system selection: A mathematical programming
model incorporating weather variability. Journal of Agricultural
Engineering Research 77 (3), 267–276.
El-Nazer, T., McCarl, B.A., 1986. The choice of crop rotation: A
modeling approach and case study. American Journal of Agricultural
Economics, 127–136.
Epperson, J.E., Estes, E.A., 1999. Fruit and vegetable supply-chain
management, innovations, and competitiveness: Cooperative Regional
Research Project S-222. Journal of Food Distribution 30, 38–43.
Fleischmann, B., Meyr, H., Wagner, M., 2005. Advanced planning. In:
Supply Chain Management and Advanced Planning: Concepts Models, Software and Case Studies. Springer, Berlin, Germany (Chapter 4).
Ferrer, J.C., MacCawley, A., Maturana, S., Toloza, S., Vera, J., 2008. An
optimization approach for scheduling wine grape harvest operations.
International Journal of Production Economics 112 (2), 985–999.
Glen, J.J., 1986. A linear programming model for an integrated crop and
intensive beef production enterprise. Journal of the Operational
Research Society 37 (5), 487–494.
Glen, J.J., 1987. Mathematical-models in farm-planning – a survey.
Operations Research 35 (5), 641–666.
Glen, J.J., Tipper, R., 2001. A mathematical programming model for
improvement planning in a semi-subsistence farm. Agricultural
Systems 70, 295–317.
Gigler, J.K., Hendrix, E.M.T., Heesen, R.A., van den Hazelkamp,
V.G.W., Meerdink, G., 2002. On optimization of agri chains by
dynamic programming. European Journal of Operational Research
139, 613–625.
Goetschalckx, M., Vidal, C.J., Dogan, K., 2002. modeling and design of
global logistics systems: A review of integrated strategic and tactical
models and design algorithms. European Journal of Operational
Research 143, 1–18.
Goyal, S.K., Giri, B.C., 2001. Recent trends in modeling of deteriorating
inventory. European Journal Of Operational Research 134 (1), 1–16.
Hamer, P.J.C., 1994. A decision support system for the provision of
planting plans for Brussels sprouts. Computers and Electronics in
Agriculture 11, 97–115.
Hardaker, J., Pendey, S., Patten, L., 1991. Farm planning under
uncertainty: A review of alternative programming models. Review of
Marketing and Agricultural Economics 59 (1), 9–22.

Hayashi, K., 2000. Multicriteria analysis for agricultural resource management: A critical survey and future perspectives. European Journal
of Operational Research 122, 486–500.
Hazell, P.B.R., Norton, R.D., 1986. Mathematical Programming for
Economic Analysis in Agriculture. Macmillan Publishing Company,
New York, NY.
Helo, P., Szekely, B., 2005. Logistics information systems: An analysis of
software solutions for supply chain co-ordination. Industrial Management & Data systems 105, 5.
Higgins, A.J., 1999. Optimizing cane supply decisions within a sugar mill
region. Journal of Scheduling 2, 229–244.
Higgins, A.J., 2002. Australian sugar mills optimize harvester roster to
improve production. Interfaces 32 (3), 15–26.
Higgins, A.J., Laredo, L.A., 2006. Improving harvesting and transport
planning within a sugar value chain. Journal of the operational
Research society 57, 367–376.
Higgins, A.J., Muchow, R.C., Rudd, A.V., Ford, A.W., 1998. Optimising
harvest date in sugar production: A case study for the Mossman mill
region in Australia. Field Crops Research 57, 153–162.
Higgins, A., Thorburn, P., Archer, A., Jakku, E., 2007. Opportunities for
value chain research in sugar industries. Agricultural Systems 94, 611–
621.
Higgins, A., Antony, G., Sandell, G., Davies, I., Prestwidge, D., Andrew, B.,
2004. A framework for integrating a complex harvesting and transport
system for sugar production. Agricultural Systems 82, 99–115.
Hobbs, J.E., Young, L.M., 2000. Closer vertical co-ordination in agrifood supply chains: A conceptual framework and some preliminary
evidence. Supply Chain Management 5 (3), 131–142.
Huang, Sophia W., 2004. Global Trade Patterns in Fruits and Vegetables,
United States Department of Agriculture, Agriculture and Trade
Report No. WRS-04-06.
Itoh, T., Hiroaki, I., Teruaki, N., 2003. A model of crop planning under
uncertainty in agricultural management. International Journal of
Production Economics 81–82, 555–558.
Jiao, Z., Higgins, A.J., Prestwidge, D.B., 2005. An integrated statistical
and optimization approach to increasing sugar production within a
mill region. Computers and Electronics in Agriculture 48, 170–181.
Jones, P.C., Lowe, T.J., Traub, R., 2001. Matching supply and demand:
The value of a second chance in producing seed corn. Review of
Agricultural Economics 24 (1), 222–238.
Jones, P.C., Lowe, T.J., Traub, R., 2003. Managing the seed-corn supply
chain at Sygenta. Interfaces 33 (1), 80–90.
Kader, A.A. (Eds.), 2001. Postharvest Technology of Horticultural Crops,
University of California Division of Agriculture and Natural
Resources, Publication 3311, pp. 5–30.
Kaiser, H.M., Apland, J., 1989. DSSP: A model of production and
marketing decisions on a Midwestern crop farm. North Central
Journal of Agricultural Economics 11 (1), 105–115.
Kaiser, H.M., Riha, S.J., Wilks, D.S., Rossiter, D.G., Sampath, R., 1993.
A farm-level analysis of economic and agronomic impacts of gradual
climate warming. American Journal of Agricultural Economics 75,
387–398.
Kaufman, P., Handy, C., McLaughlin, E.W., Park, K., Green, G.M.,
2000. Understanding the dynamics of produce markets: Consumption
and consolidation grow. US Department of Agriculture-Economic
Research Service, Market and Trade Economics Division, Agricultural
Information Bulletin No. 758, August.
Kazaz, B., 2004. Production planning under yield and demand uncertainty
with yield-dependent cost and price. Manufacturing & Service Operations Management 6 (3), 209–224.
Kinsey, J.D., 2001. The new food economy: Consumers, farms, pharms
and science. American Journal of Agricultural Economics 83 (5),
1113–1130.
Kobzar, O.A., van Asseldonk, M.A.P.M., Huirne, R.B.M., 2005. Wholefarm planning under risk: Application of alternative risk programming
techniques to support portfolio-decisions in Dutch agriculture. In:
Agricultural Economics Society Annual Conference, University of
Nottingham, England.

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20
Kouvelis, P., Chambers, C., Wang, H., 2006. Supply chain management
research and production and operations management: Review, trends,
and opportunities. Production and Operations Management 15 (3),
449–469.
Lambert, D.K., McCarl, B.A., 1989. Sequential modeling of white wheat
marketing strategies. North Central Journal of Agricultural Economics 11 (2), 157–169.
Lazzari, M., Mazzetto, F., 1996. A PC model for selecting multicropping
farm machinery system. Computers and Electronics in Agriculture 14,
43–59.
Leutscher, K.J., Renkema, J.A., Challa, H., 1999. Modeling operational
adaptations of tactical production plans on pot plan nurseries: A
simulation approach. Agricultural Systems 59, 67–78.
Lien, G., Hardaker, J.B., 2001. Whole farm planning under uncertainty:
Impacts of subsidy scheme and utility function on portfolio choice in
Norwegian agriculture. European Review of Agricultural Economics
28 (1), 17–36.
Lowe, T.J., Preckel, P.V., 2004. Decision technologies for agribusiness
problems: A brief review of selected literature and a call for research.
Manufacturing & Service Operations Management 6 (3), 201–208.
Lucas, M.T., Chhajed, D., 2004. Applications of location analysis in
agriculture: A survey. Journal of the Operational Research Society 55,
561–578.
Maatman, A., Schweigman, C., Ruijs, A., van der Vlerk, M.H., 2002.
Modeling farmer’s response to uncertain rain fall in Burkina Faso: A
stochastic programming approach. Operations Research 50 (3), 399–
414.
MacDonald, J., Perry, J., Ahearn, M., Banker, D., Chambers, W.,
Dimitri, C., Key, N., Nelson, K., Southard, L., 2004. Contracts,
Markets, and Prices: Organizing the Production and Use of Agricultural Commodities, United States. Department of Agriculture, Economic Research Service, Agricultural Economic Report No. 837.
Marcelis, L.F.M., Heuvelink, E., Goudriaan, J., 1998. Modelling biomass
production and yield of horticultural crops: A review. Scientia
Horticultusrae 74, 83–111.
McCarl, B.A., Nuthall, P., 1982. Linear-programming for repeated use in
the analysis of agricultural systems. Agricultural Systems 8 (1), 17–39.
McLaughlin, E.W., Green, G.M., Park, K., 1999. Changing Distribution
Patterns in the US Fresh Produce Industry: Mid/Late-70s to Mid/
Late-90s, Department of Agricultural, Resource, and Managerial
Economics, Cornell University, Ithaca, NY 14853, June.
Meixell, M.J., Gargeya, V.B., 2005. Global supply chain design: A
literature review and critique. Transportation Research Part E 41, 531–
550.
Miller, W.A., Leung, L.C., Azhar, T.M., Sargent, S., 1997. Production
planning for fresh tomato packing. International Journal of Production Economics 53, 227–238.
Min, H., Zhou, G., 2002. Supply chain modeling: Past, present and future.
Computers & Industrial Engineering 43, 231–249.
Nanseki, T., Morooka, Y., 1991. Risk preference and optimal crop
combinations in upland Java, Indonesia: An application of stochastic
programming. Agricultural Economics 5, 39–58.
Nevo, A., Oad, R., Podmore, T., 1994. An integrated expert system for
optimal crop planning. Agricultural Systems 45, 73–92.
Perosio, D.J., McLaughlin, E.W., Cuellar, S., Park, K., 2001. Supply
chain management in the produce industry. Produce Marketing
Association, Newark, Delaware, 22–32.
Perry, G.M., McCarl, B.A., Rister, M.E., Richardson, J.W., 1989.
Modeling government program participation decisions at the farm
level. American Journal of Agricultural Economics 71 (4), 1011–1020.
Porteus, E.L., 1993a. Case analysis: Analyses of the national cranberry
cooperative – 1. Tactical options. Interfaces 23 (4), 21–39.
Porteus, E.L., 1993b. Case analysis: Analyses of the national cranberry
cooperative – 2. Environmental changes and implementation. Interfaces 23 (6), 81–92.
Purcell, D.L., Turner, S.C., Houston, J., Hall, C., 1993. A portfolio
approach to landscape plant production and marketing. Journal of
Agriculture and Applied Economics 25 (2), 13–26.

19

Raju, K.S., Kumar, D.N., 1999. Multicriterion decision-making in
irrigation planning. Agricultural Systems 62, 117–129.
Rantala, J., 2004. Optimizing the supply chain strategy of a multi-unit
ﬁnish nursery. Silva Fennica 38 (2), 203–215.
Reardon, T., Barret, C.B., 2000. Agroindustrialization, globalization, and
international development: An overview of issues, patterns, and
determinants. Agricultural Economics 23, 195–205.
Recio, B., Rubio, F., Criado, J.A., 2003. A decision support system for
farm planning using AgriSupport II. Decision Support Systems 36 (2),
189–203.
Romero, C., 2000. Risk programming for agricultural resource allocation.
Annals of Operation Research 94, 57–68.
Saedt, A.P.H., Hendriks, T.H.B., Smits, F.M., 1991. A transition planning
method applied in a decision support system for pot-plant nurseries.
European Journal of Operations Research 52, 142–154.
Salin, V., 1998. Information technology in agri-food supply chains.
International Food and Agribusiness Management Review 1 (3), 329–
334.
Sarmiento, A.M., Nagi, R., 1999. A review of integrated analysis
of production–distribution systems. IIE Transactions 31, 1061–
1074.
Schilizzi, S.G.M., Kingwell, R.S., 1999. Eﬀects of climatic and price
uncertainty on the value of legume crops in a Mediterranean-type
environment. Agricultural Systems 60, 55–69.
Schepers, H., van Kooten, O., 2006. Proﬁtability of ‘ready-to-eat’
strategies: Towards model assisted negotiation in a fresh-produce
chain. In: Quantifying the Agri-Food Supply Chain. Springer, Netherlands (Chapter 9).
Simchi-Levi, D., 2003. Designing and Managing the Supply Chain:
Concepts, Strategies, and Case Studies. McGraw-Hill, New York, NY.
Stadler, H., 2005. Supply chain management and advanced planningbasics, overview and challenges. European Journal of Operational
Research 163, 575–588.
Stadler, H., Kilger, C., 2005. Supply Chain Management and Advanced
Planning: Concepts Models, Software and Case Studies. Springer,
Berlin, Germany.
Stoecker, A.L., Seidmann, A., Lloyd, G.S., 1985. A linear dynamic
programming approach to irrigation system management with depleting groundwater. Management Science 31 (4), 422.
Stokes, J., Mjelde, J., Hall, C., 1997. Optimal marketing of nursery crops
from container-based production systems. American Journal of
Agricultural Economics 79, 235–245.
Sumanatra, J., Ramirez, J.A., 1997. Optimal stochastic multi-crop
seasonal and intraseasonal irrigation control. Journal of Water
Resources Planning and Management 123 (1), 39–48.
Tan, L.P., Fong, C.O., 1988. Determination of the crop mix of a rubber
and oil palm plantation – a programming approach. European Journal
of Operations Research 34, 362–371.
Taylor, C.R. (Ed.), 1993. Applications of Dynamic Programming to
Agricultural Decision Problems. Westview Press, Boulder, CO.
Thomas, D.J., Griﬃn, P.M., 1996. Coordinated supply chain management. European Journal of Operations Research 94, 1–15.
Tijskens, L.M.M., Polderdijk, J.J., 1996. A generic model for keeping
quality of vegetable produce during storage and distribution. Agricultural Systems 51 (4), 431–452.
Torkamani, J., Hardaker, J.B., 1996. A study of economic eﬃciency of
Iranian farmers in Ramjerd district: An application of stochastic
programming. Agricultural Economics 14, 73–83.
Torkamani, J., 2005. Using whole-farm modeling approach to assess
prospective technologies under uncertainty. Agricultural Systems, 138–
154.
Turvey, C.G., Driver, H.C., Baker, T.G., 1988. Systematic and nonsystematic risk in farm portfolio selection. American Journal Agricultural
Economics 70 (4), 831–835.
Turvey, C.G., Baker, T.G., 1990. A farm-level ﬁnancial analysis
of farmer’s use of futures and options under alternative
farm programs. American Journal of Agricultural Economics,
946–957.

20

O. Ahumada, J.R. Villalobos / European Journal of Operational Research 195 (2009) 1–20

United States Department of Agriculture (USDA), 2007. Fruits and
Vegetables (farm weight): Per Capita Availability, 1970–2005,
http://www.ers.usda.gov/Data/FoodConsumption/spreadsheets/fruitveg.
xls.
van Berlo, M. Jules, 1993. A decision support tool for the vegetable
processing industry; an integrative approach of market, industry and
agriculture. Agricultural Systems 43, 91–109.
van der Vorst, J.G.A.J., 2006. Product traceability in food-supply chains.
Acreditation and Quality Assurance 11, 33–37.
van Ittersum, M.K., Leﬀelaar, P.A., van Keulen, H., Kropﬀ, M.J.,
Bastiaans, L., Goudriaan, J., 2003. On approaches and applications of
the Wageningen crop models. European Journal of Agronomy 18,
201–234.

Vidal, C.J., Goetschalckx, M., 1997. Strategic production–distribution
models: A critical review with emphasis on global supply chain models.
European Journal of Operational Research 98, 1–18.
Visagie, S.E., de Kock, H.C., Ghebretsadik, A.H., 2004. Optimising an
integrated crop-livestock farm using risk programming. Operations
Research Society of South Africa 20 (1), 29–54.
Vitoriano, B., Ortuño, M.T., Recio, B., Rubio, F., Alonso-Ayuso, A.,
2003. Two alternative models for farm management: Discrete versus
continuous time horizon. European Journal of Operational Research
114, 613–628.
Widodo, K.H., Nagasawa, H., Morizawa, K., Ota, M., 2006. A periodical
ﬂowering-harvesting model for delivering agricultural fresh products.
European Journal of Operational Research 170, 24–43.

Proceedings of the 3rd Annual
IEEE Conference on Automation Science and Engineering
Scottsdale, AZ, USA, Sept 22-25, 2007

MoRP-A05.1

Automated Feature Selection Methodology for Reconfigurable
Automated Visual Inspection Systems
Hugo C. Garcia and J. Rene Villalobos

Abstract—The lack of flexibility of the current Automated
Visual Inspection (AVI) systems to accommodate new
products is one of the main problems faced by the users of
these systems. In this paper, the authors propose a framework
that will facilitate the design of highly reconfigurable AVI
systems. We focus on two issues: the general description of a
framework for the development of reconfigurable systems and
the description of a method for feature selection that will
make the automation of this process easier.

I. INTRODUCTION
The major disadvantages of the existing inspection
systems are high set-up costs, resulting from extensive preinspection set-up by an experienced operator, hardware and
software development costs, labor, and maintenance costs
[1]. Therefore, current inspection systems are each customdesigned for a specific task and often are very hard to
change for new applications. Moreover, the reconfiguration
of AVI systems is a difficult task because knowledge in
many technical areas including illumination, cameras,
computer interfaces, programming and image processing is
required. Therefore, finding or developing all of the
expertise in one person or in a small group of people is
difficult.
In order to improve the current state of the art in AVI
applications, it is necessary to develop methodologies and
tools that give these systems the needed automation,
flexibility and reliability to allow the rapid introduction of
new products. For example, the AVI systems should not be
rendered obsolete by incremental changes in component
technology or product design. Thus, the development of the
enabling tools for the emergence of general reconfigurable
inspection systems is the central goal of the research
presented here. Although the main focus of this research is
on industrial AVI systems, the tools to be developed will be
general enough to insure their portability to other
applications.
In this paper we focus on two issues: general description
of a framework for the development of reconfigurable
The authors would like to acknowledge the support provided by the
National Science Foundation through the grant DMI-0300361 for the
realization of this research project.
The authors are with the Department of Industrial Engineering at
Arizona State University, Tempe, AZ 85281, USA. E-mail: {Hugo.Garcia,
Rene.Villalobos}@asu.edu.

1-4244-1154-8/07/$25.00 ©2007 IEEE.

systems and the description of a method for feature
selection that will make the automation of this process
easier.
II. LITERATURE REVIEW
Although many reconfigurable AVI prototypes have
been reported in literature, only a limited number of these
have been fully installed on factory floors. For instance, in
an article surveying over 100 applications of AVI systems
[2], only one system that was designed for a more general
purpose [3] was cited. In this application [3], the authors
used geometric models and a laser range data finder to
inspect different mechanical parts. Although an important
potential market for fast and easy reconfigurable inspection
systems exists [4], the problem is how to match the
technology developed for a specific application to a more
general application in an optimal, easy and cost-effective
way. In this context, from an industrial perspective,
according with Sablatning [1], there are three main
difficulties in developing generic AVI systems that can be
applied to a wide range of products, namely:
Cost: Development costs are still too high; vision
software-hardware development, in particular makes up a
substantial portion of the cost of a new application.
Technology: Presently AVI systems require a controlled
environment for each particular application; this means that
the components under inspection must be free of
obstructions. For instance, almost all of the currents AVI
systems are unable to handle shadows, highlights, and
occlusions during the inspection process.
Ad Hoc Solutions: Most of the systems lack a structured
approach for the development of the inspection algorithms.
The Ad Hoc approach seems to be the best way to solve a
particular problem since such solutions are dealt with caseby-case and tend to be simpler, more compact and cheaper.
However, customized systems for specific problems can
not be duplicated or reused.
One of the keys for the success of any inspection system
is to reduce the per-unit inspection cost of the nondefective parts produced. An approach used to reduce the
inspection cost is amortizing the overall cost of the
inspection systems over a large number of manufactured
units. In order to reduce this cost it is necessary to improve
the flexibility of the inspection systems by designing the

542

MoRP-A05.1

intervention. The main requirement of this module is
that the inspection system is designed based on a set of
parametric features (each feature has at least one
parameter that must be tuned according to the physical
characteristics of each component) available to be used
by the inspection algorithm.

systems using structured and reconfigurable algorithmic
approaches to obtain a system that will last longer. There is
a requirement for generic AVI systems based on the
principles of “Soft Automatization”, in which product
modeling is coupled with flexible manufacturing and
inspection systems which can produce any product with
minimum design and quality control constraints [5].
Ideally, an AVI system should be designed to take into
account operation speed, reliability, ease of use, and
modular flexibility, in order to be easily adapted to
different inspection tasks [6].
While the literature on each of the individual areas
(hardware and software) of AVI systems is abundant,
literature on the integration of these areas is scarce. For
instance, some applications of AVI systems in the
electronic industry are documented in [7], [8], [9], [10],
[11], [12], [13] and [14]. In addition, surveys in the area of
algorithm development of AVI systems are documented in
[4], [2], [15], [6], [16], [17] and [18]. In the area of PCB
inspection the documentation has been done by [19].
In order to improve the current state of the art in AVI
applications, it is necessary to minimize the intervention of
human developers to obtain inspection algorithms that meet
minimal performance in terms of component
discrimination. In this paper, we introduce a framework
that has the most essential elements required to automate
the development of reconfigurable inspection algorithms
and systems. It is expected that having a framework
methodology for the reconfiguration process will
significantly shorten the set-up costs and time, in particular
for those cases in which a preexisting AVI systems are
already in place.

Figure 1 – Framework Modules
B. The Feature Selection Module (FSM)
The objective of this module is the selection of the
most adequate subset of available features to inspect
the new component in the minimum amount of time.
The purpose of this module is to automatically select
the features to provide the greatest level of
discrimination between defective and non-defective
components. In addition, it is expected to do this
process with the least training data possible.
Furthermore, if the level of discrimination desired by
the user is not obtained using the selected set of
features, the FSM could also be used to reduce the
number of features with the same or a better level of
discrimination or inspection error rate than using the
full set of features. One of the most important
functions of this module is feature selection. We will
talk in more detail of this function in the Section IV of
this paper.

III. ENVISIONED RECONFIGURABLE FRAMEWORK
In order to develop the framework for the automated
reconfiguration of the AVI systems, it is necessary to
decompose the overall problem into simpler sub-problems.
As part of this decomposition strategy, the principal subproblems of the reconfiguration process have been grouped
into five functional modules that comprise the envisioned
framework. The five proposed modules are: the Feature
Generation Module (FGM), the Feature Selection Module
(FSM), the Decision Engine Module (DEM), the
Performance Assessment Module (PAM), and the
Refinement and Improvement Module (RIM). A schematic
of the framework modules is presented in Figure 1. The
objectives and interactions of these modules are described
next.

C. The Decision Engine Module (DEM)
The objective of this module is to perform the decision
making process to classify the components under
inspection as conforming or non-conforming to design
specifications. In this case, the classification process is
defined as the assignment of a component, described
by a set of features, to a specific class or population.
Given that the set of features and classes can be
written as vectors, this process can be formulated in
mathematical terms as a mapping from a feature space
to a class space. In summary, the classification process
is a transformation between two vector spaces through
a mathematical algorithm.

A. The Feature Generation Module (FGM)
The objective of this module is to adapt and optimize
the pre-existing features in the inspection system
and/or construct new ones. The purpose of this module
is to perform these processes with very minimal human

543

MoRP-A05.1

D. The Performance Assessment Module (PAM)
The objective of this module is to estimate the
reliability of the inspection algorithm, in the training
period, as well as in the factory floor, by using training
and inspection data sets. In order to detect the
effectiveness of an inspection algorithm, it is necessary
to know the way that the subset of features selected by
the FSM behaves in the training and inspection phases.
Although it is relatively “easy” to measure the
reliability in the training period, projections of future
performance become more complex and less reliable if
the populations are not stable. Therefore, it is
necessary to measure the performance of the inspection
algorithm in the factory floor, based on the inspectionerror rate rendered in the training period and on the
historical performance of the inspection system.
E. The Refinement and Improvement Module (RIM)
The objective of this module is to enhance the
performance of the inspection algorithms through the
use of statistical tools. For instance, if a targeted level
of discrimination between the components cannot be
attained, this module will, among other things, develop
new features, cluster the components for inspection,
and optimize the combination of features.
The details of the interactions between the modules of
the framework and the description of the information
exchange between the components of these modules are
presented in [21].
IV.

THE FEATURE SELECTION PROCESS

The objective of this process is to allow the automated
and rapid selection of features to achieve the fast
generation of new or the reconfiguration of pre-existing
inspection algorithms with the lowest misclassification
error rate possible. The optimal feature selection problem is
typically defined as follows: “Given a set of p features,
select a subset of size m that leads to the smallest
classification error” [22]. The optimal feature selection
process is very relevant in the application of the inspection
algorithms because it reduces the number of features in the
classification algorithm and removes irrelevant, redundant,
and noisy data during the training period of the classifier.
The advantages of selecting an appropriate feature set in
the inspection systems are in two dimensions: speeding up
the inspection algorithm and improving the algorithm
performance
(classification
accuracy
and
the
understandability of the inspection results).
The topic of optimal feature selection has been addressed
extensively for applications other than Automated Visual
Inspection (AVI) systems, such as engineering, social and
medical sciences. The literature associated with feature
selection for discriminant analysis is vast. Some examples

include [14], [23], [24], [25], and [26].
The complexity in finding the optimal subset of features
exists when a large number of features are available to
select from. When this condition occurs, it is impossible to
develop an exhaustive search to find the optimal subset that
optimized the decision criterion defined by the user, i.e. the
subset with the minimum misclassification error rate. The
limitation of the exhaustive search is due to the fact that the
feature selection process belongs to the group of NP-hard
problems [27]. The search process to find the optimal
subset of features is in a O(2p) space (p = number of
features); therefore, even for medium values of p, this task
is practically impossible to accomplish [28]. For a
discussion in the underlying complexity of an exhaustive
search, the reader is referred to [29].
Although an exhaustive search guarantees finding the
optimal result according to the evaluation criterion used, a
heuristic search does not have to be exhaustive in order to
guarantee completeness, i.e., no optimal subset is missed
[30]. Consequently, different heuristic methods can be used
to reduce the search space O(2p) without putting at risk the
opportunities of finding the optimal subset. Some examples
of heuristic methods are branch and bound, beam search,
and sequential search. Furthermore, the sequential search is
segmented in different heuristics such as forward selection,
backward elimination and stepwise selection.
Sequential methods based on statistical distances
between the populations to classify are the most common
methods for feature selection. The principal deficiency of
these criteria is that they measure a type of statistical
distance between the populations and their principal
assumption is that these measurements are highly correlated
to the Misclassification Error Rate (MER) that will be
generated by the classifier using the particular subset of
features being evaluated. This assumption means that
when the evaluation criterion is maximized or minimized
by the final subset, this subset also will be minimized the
MER of the classifier. However, this assumption is not
always true because heuristic methods using the MER as
the evaluation criterion (i.e., the classifier is the evaluation
function) give a superior performance than the other
criteria, but it also tends to be the most computationally
expensive [30] and [31]. The authors are developing a
feature selection method that combines the simplicity of the
sequential stepwise selection process with the observable
qualities of the methods based on MER. This method is
briefly described next.
V. THE STOCHASTIC APPROXIMATION METHOD
The proposed method it is based on the stochastic
representation of the misclassification errors produced by a
Quadratic Discrimination Function (QDF) [14]. In this new
method, the stochastic representations for the exact

544

MoRP-A05.1

distributions of the QDF are used to estimate the MER
based on normal approximations instead of running
simulations as was proposed by [32]. The approximation of
the MER is based on the assumption that the distributions
of the MER for defective (Q1) and non-defective
populations (Q2) follow normal distributions. Thus, the
MER is estimated as:

~  k − µ Q1
MER = Φ −

σQ
1



~  k − µ Q2
p +Φ
1
 σ

Q2




p
 2


(1)

Where the means and standard deviations correspond to
the stochastic representations of the misclassification
errors, k corresponds to a predetermined classification
threshold and p1 and p2 are the a-priori probabilities of the
defective (Π1) and non-defective (Π2) populations. The
graphical representation of the MER is presented in Figure
2. If there are more than two populations or classes then
Eq. 1 becomes considerable more complex due to the
combinatorial nature of the problem.

Figure 2 – Approximation Method
A. Experimental Results
In order to evaluate the performance of the proposed
method, it was compared with some of the more important
and efficient existing evaluation criteria. These evaluation
criteria are: Wilk’s Lambda, Unexplained Variance,
Mahalanobis Distance, Smallest F Ratio, and Rao’s V. The
five methods analyzed gave the same subset using α equal
to 0.15 to include or exclude the features of the subset.
These methods are grouped and referred to as conventional
methods.
For the experimental validation, the new evaluation
criterion was evaluated with respect to conventional
methods through an exhaustive search for 15 features. This
example of feature selection was using simulated pseudorandom data for two multivariate normal distributions with
15 features (A to O) simulating two populations: Π1 and
Π2. For instance, Table 1 presents the statistical
characteristics of the features; means and variances. In this
example, the means within the features of Π1 and Π2 are 0
and 1000 respectively; the only difference between the

features is the variance. Furthermore, with the same
statistical characteristics of the features presented in the
previous table, five samples of 1,000 data points were
generated with different levels of correlation between the
features. The levels of correlation were 0.0, 0.25, 0.5, 0.75
and 0.99 respectively. The results of this experiment are
presented next.
The summary of the feature selection experiments using
the five different levels of correlation between the features
is presented in Table 2. The first column presents the level
of the correlation. Although the feature space of these
experiments is of 215-1 (32,767), since numerous subsets
have the same MER, the space number is decreased in these
examples. To obtain the different number of subsets, the
MER for the 32,767 subsets was calculated using the
resubstitution method. The number of different subsets
based on the MER is presented in the second column for
each correlation level of the section Data.
The MER of the full subset (using the 15 features) and
the position of this subset with respect to the optimal subset
for each level of correlation are presented in the column
called Full. Furthermore, the results of three methods of
this analysis are presented in the sections Conventional,
Approximation and Resubstitution respectively. The subset
of features selected is presented in the first column of each
method. The second column of each section presents the
MER using the resubstitution method. The next column
shows the number of features in the selected subset.
The final column gives the position of this subset with
respect to the optimal subset of features based on the MER.
In addition, the last section of this table shows the optimal
subset with its corresponding MER and number of features.
For the last two rows of the optimal subset, there are
multiples subsets with the MER equal to zero; the size for
these examples represents the number of features in the
largest subset.
For instance, for the case of the features having a
correlation level of 0.5 (third row of second table) if the
feature selection is based on the conventional method 14
features would be selected to perform the discrimination
with a MER of 0.0085. This selection is not optimal since
the optimal feature set would be composed of 12 features
(last column of table) with a MER of 0.0065. The
Approximation and Resubstitution methods used 13 and 12
features each with a MER of 0.007 and 0.0075
respectively.
From Table 2 it can be concluded that the approximation
method exhibits similar performance than the conventional
methods. The computational time to get the estimate the
MER using the Approximation was of about 0.02 second
using MATLAB® and a Pentium® based computer. In the
three variables (number of features, rank and MER)
analyzed in the previous experiment the values obtained are
similar. However, unlike the conventional methods, the
proposed method gives a direct assessment of the
probability of misclassification. However, it is necessary to

545

MoRP-A05.1

Table 1 - Features’ Statistical Characteristics I
Parameters
µ1
µ2
σ1=σ2

A
0
1000
100

B
0
1000
200

C
0
1000
300

D
0
1000
400

E
0
1000
500

F
0
1000
600

G
0
1000
700

Features
H
0
1000
800

I
0
1000
900

J
0
1000
1000

K
0
1000
1100

L
0
1000
1200

M
0
1000
1300

N
0
1000
1400

O
0
1000
1500

Table 2 – Feature Selection Summary I
Data
r Sets
0.00 678
0.25 665
0.50 664
0.75 560
0.99 557

Full
MER Rank
0.0310 4
0.0250 5
0.0080 4
0.0005 2
0.0000 1

Conventional
Subset
MER
ABCDEFGHLMNO
0.0315
ABCDEFGIJKLMNO 0.0255
ABCDEFGIJKLMNO 0.0085
ABCDEFGJKLMNO
0.0000
ABCDEFGHIJKLMNO 0.0000

Size Rank
12
4
14
4
14
5
13
1
15
1

Approximation
Subset
MER
ABCDEFGHLMNO
0.0315
ABCDEFGIJKLMNO 0.0255
ABCDEFIJKLMNO
0.0070
ABCDEFGJKLMNO
0.0000
ABCDEFGHIJKLMNO 0.0000

explain that the approximation method did not use any
threshold for the inclusion or elimination of the features.
This means that any reduction of the MER (using the
approximation method) was enough to include a feature in
the final subset. Therefore, the user can determine the
threshold to include the features and obtain the optimal
number of features versus the MER based on the inspection
time and cost constraints. In conclusion, the user can
determine when to stop the feature selection process
according with the desire MER and the inspection
constraints.

[2]
[3]

[4]
[5]
[6]

VI. CONCLUSION
In this paper a general framework for the automated
reconfiguration of AVI systems was presented. We also
gave a general overview of one of the most important
functions of the proposed framework: the feature selection
method amenable for automation. The objective of the
envisioned framework is to create the general basis and
tools for the automated generation and reconfiguration of
visual inspection algorithms. The general methodology
outlined in this paper calls for the automated/iterative
solution of the main problems faced by the developer when
retrofitting an inspection system to inspect new products or
components. The ultimate objective of the research
described in this paper is to obtain a new generation of
flexible inspection systems with longer economic lives.
Given the space restrictions of the current paper the details
of the feature selection methodology were omitted.
However, the results presented support the viability of the
proposed feature selection method from the perspective of
discrimination
performance
along
with
reduced
development time. The further details the interested reader
is referred to [33].

[7]
[8]

[9]
[10]
[11]
[12]

[13]
[14]

[15]

REFERENCES
[1]

R. Sablatning, “Increasing Flexibility for Automatic Visual
Inspection: The General Analysis Graph” Machine Vision and
Applications, Vol. 12, 2000, pp. 158-169.

[16]

546

Size Rank
12
4
14
4
13
2
13
1
15
1

Resubstitution
Subset
MER
ABCDEFGJLMNO 0.0300
ABCDEFJLMNO
0.0245
ABCDEFHJKMNO 0.0075
ACFLNO
0.0005
ABCINO
0.0000

Size Rank
12
2
11
2
12
3
6
2
6
1

Optimal
Subset
ABCDEFGIJLNO
ABCDEFHIJLMNO
ABCDEIJKLMNO
357 Subset
695 Subset

MER
0.0290
0.0240
0.0065
0.0000
0.0000

Size
12
13
12
14
15

T. Newman and A. Jain, “A Survey of Automated Visual
Inspection”, Computer Vision and Image Understanding, Vol. 61,
No. 2, Match, 1995, pp.231-262.
A. Marshall, R. Martin and D. Hutber, “Automatic Inspection of
Mechanical Parts Using Geometric Models and Laser Range Finder
Data”, Computer Standards & Interfaces, Vol. 4, No. 3, 1991, pp.
191-208. May.
E. Malamas, G. Petrakis, M. Zervakis and L. Petit, “A Survey on
Industrial Vision Systems, Applications and Tools” Image and
Vision Computing, Vol. 21, 2003, pp. 171-188.
J. Todd, “Advanced Vision Systems for Computer-Integrated
Manufacture – Part I” Computer Integrated Manufacturing Systems,
Vol. 1, No. 3, 1998, pp. 143-154.
E. Bayro, “Review of Automated Visual Inspection: Part I
Conventional Approaches”, in Intelligent Robots and Computer
Vision XII: Algorithms and Techniques, David P. Casasent, Editor,
Proc. SPIE, 1993, pp. 128-158.
Z. Ibrahim and S. Al-Attas, “Wavelet-Based Printed Circuit Board
Inspection Systems”, International Journal of Signal Processing, Vol.
1, No. 1, 2004, pp. 73-79.
K. Choi, J. Pyun, N. Kim, B. Choi and S. Ko, “Real-Time Inspection
Systems for Printed Circuit Boards”, Pattern Recognition,
Proceedings Lecture Notes in Computer Science, No. 2781, 2003,
pp. 458-465.
E. Whittenberger, A. Brito and S. Cabrera, “Applying the Eigenfaces
and Fisherfaces Methods to Circuit Board Inspection”, Optical
Engineering, Vol. 39, No. 12, 2000, pp.3154-3164, December.
E. Guerra and J. Villalobos “A Three Dimensional Automated Visual
Inspection System for SMT Assembly”, Computers and Industrial
Engineering, Vol. 40. No. 1-2, 2001, pp. 175-190.
J. Villalobos, M. Arellano, A. Medina and F. Aguirre, “Vector
Classification of SMD Images,” Journal of Manufacturing Systems,
Vol. 22, No. 4, 2004, pp. 265-282.
S. Gokturk, L. Akarun and H. Bozma, “Automated Inspection of
PCB’s Using a Novel Approach”, Proceedings of 1999
IEEE/EURASIP Workshop on Nonlinear Signal Processing and
Image Processing, Antalya, 1999.
M. Moganti and F. Ercal, “A Subpattern Level Inspection System for
Printed Circuit Boards”, Computer Vision and Image Understanding,
Vol. 70, No.1, pp. 51-62, April, 1998.
H. Garcia, J. Villalobos and G. Runger “Automated Feature
Selection for Visual Inspection Systems”, IEEE Transactions on
Automation Science and Engineering, Vol. 3, No.4, pp. 394-406,
October, 2006.
A. Thomas, M. Rodd, J. Holt and C. Neill, “Real-Time Industrial
Visual Inspection: A Review” Real-Time Imaging, Vol. 1, 1995, pp.
139-158.
E. Bayro, “Review of Automated Visual Inspection: Part II
Approaches to Intelligent Systems”, in Intelligent Robots and

MoRP-A05.1

[17]
[18]
[19]
[20]
[21]

[22]
[23]

[24]

[25]

[26]

[27]

[28]
[29]
[30]
[31]
[32]

[33]

Computer Vision XII: Algorithms and Techniques, David P.
Casasent, Editor, Proc. SPIE, 1993, pp. 159-169.
R. T. Chin, “Automated Visual Inspection: 1981 to 1987: Survey”
Computer Vision, Graphics, and Image Processing, Vol. 41, 1988,
pp. 346-381.
R. Chin and C. Harlow, “Automated Visual Inspection: Survey”,
IEEE Transactions on Pattern Analysis and Machine Intelligence,
No. 6, 1982, pp. 557-573.
M. Moganti, F. Ercaal, C. Dagli and S. Tsunekawa, “Automatic PCB
Inspection Algorithms”, Computer Vision and Image Understanding,
Vol. 63, No. 2, March, 1996, pp. 287-313.
R. Valdovinos, J. Sanchez and R. Barandela, “Dynamic and Static
Weighting in Classifier Fusion”, Lecture Notes in Computer Science,
Vol. 3523, 2005, pp. 59-66.
H. C. García and J. R. Villalobos “Development of a Methodological
Framework for the Self Reconfiguration of Automated Visual
Inspection Systems”, Submitted to Proceeding of INDIN 2007 5th
International Conference on Industrial Informatics, Vienna, Austria,
July 23-27, 2007.
A. Jain, R. Duin and J. Mao “Statistical Pattern Recognition: A
Review”, IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol. 22, No.1, 2000, January.
A. Gupta, T. Logan and J. Chen “A Variable Selection Technique in
Discriminant Analysis with Application in Marketing Data”, Journal
of Statistical Computation and Simulation, Vol. 63, No. 2, 1999, pp.
187-199.
J. Glenn “Integer Programming Methods for Normalization and
Variable Selection in Mathematical Programming Discriminant
Analysis Models”, Journal of the Operation Research Society, Vol.
50, No. 10, 1999, October, pp. 1043-1053.
O. Snorrason and F. Garber “Evaluation of Nonparametric
Discriminant Analysis Techniques for Radar Signal Feature and
Extraction”, Optical Engineering, Vol. 31, No 12, 1992, December,
pp. 2608-2617.
E. Kinney and D. Murphy “Comparison of the ID3 Algorithm Versus
Discriminant Analysis for Performing Feature Selection”, Computer
and Biomedical Research, Vol. 20, No. 5, 1987, October, pp. 467476.
R. Meiri and J. Zahavi “Using Simulated Annealing to Optimize the
Feature Selection Problem in Marketing Applications”, European
Journal of Operation Research, Vol. 171, No. 3, 2006, pp. 842-858,
June.
G. McCabe “Computations for Variable Selection in Discriminant
Analysis”, Technometrics, Vol. 17, No. 1, 1975, February.
T. Pavlenko “On Feature Selection, Curse-of-Dimensionality and
Error Probability in Discriminant Analysis”, Journal of Statistical
Planning and Inference, Vol. 115, No. 2, 2003, August, pp. 565-584.
H. Liu and L. Yu “Toward Integrating Feature Selection Algorithms
for Classification and Clustering”, IEEE Transactions on knowledge
and Data Engineering, Vol. 17, No. 4, 2005, pp.491-502.
M. Dash and H. Liu “Feature Selection for Classification”, Intelligent
Data Analysis, Vol. 1, 1997, pp. 131-156.
R. McFarland and D. Richard (2002) “Exact Misclassification
Probabilities for Plug-in Normal Quadratic Discriminant Function,
The Heterogeneous Case” Journal of Multivariate Analysis, Vol. 82,
pp.299-330.
H. C. García and J. R. Villalobos “Development of a Methodology
for the Feature Selection using the Quadratic Discriminant Analysis”,
Working Paper, Industrial Engineering Dept., Arizona State
University, 2007.

Hugo C. Garcia received a B.S. in Industrial and
Systems Engineering from the Instituto Tecnológico y de
Estudios Superiores de Monterrey, Campus Ciudad
Juárez in 2000 (México). He holds a M.B.A. from the
Universidad Autónoma de Ciudad Juárez in 2002
(México) and M.S. in Statistics from The University of
Texas at El Paso in 2003. Currently, he is pursing a
Ph.D. degree in Industrial Engineering at Arizona State.
His main research interest lies on automated quality and inspection
systems.
J. René Villalobos is an associate professor in the
Department of Industrial Engineering in the Ira A. Fulton
School of Engineering at ASU. Villalobos received his
B.S. in Industrial-Mechanical Engineering from the
Chihuahua Technological Institute in Mexico in 1982, a
M.S. in Industrial Engineering from the University of
Texas at El Paso in 1987, and a Ph.D. in Industrial
Engineering at Texas A & M University in 1991. His
research interests are in the areas of logistics, automated quality systems,
manufacturing systems and applied operations research.

547

