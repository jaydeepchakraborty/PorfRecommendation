Lecture Notes in Computer Science
Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen

4731

Editorial Board
David Hutchison Lancaster University, UK Takeo Kanade Carnegie Mellon University, Pittsburgh, PA, USA Josef Kittler University of Surrey, Guildford, UK Jon M. Kleinberg Cornell University, Ithaca, NY, USA Friedemann Mattern ETH Zurich, Switzerland John C. Mitchell Stanford University, CA, USA Moni Naor Weizmann Institute of Science, Rehovot, Israel Oscar Nierstrasz University of Bern, Switzerland C. Pandu Rangan Indian Institute of Technology, Madras, India Bernhard Steffen University of Dortmund, Germany Madhu Sudan Massachusetts Institute of Technology, MA, USA Demetri Terzopoulos University of California, Los Angeles, CA, USA Doug Tygar University of California, Berkeley, CA, USA Moshe Y. Vardi Rice University, Houston, TX, USA Gerhard Weikum Max-Planck Institute of Computer Science, Saarbruecken, Germany

Andrzej Pelc (Ed.)

Distributed Computing
21st International Symposium, DISC 2007 Lemesos, Cyprus, September 24-26, 2007 Proceedings

13

Volume Editor Andrzej Pelc Département d'informatique Université du Québec en Outaouais Gatineau, Québec J8X 3X7, Canada E-mail: pelc@uqo.ca

Library of Congress Control Number: 2007935053 CR Subject Classification (1998): C.2.4, C.2.2, F.2.2, D.1.3, F.1.1, D.4.4-5 LNCS Sublibrary: SL 1 ­ Theoretical Computer Science and General Issues ISSN ISBN-10 ISBN-13 0302-9743 3-540-75141-6 Springer Berlin Heidelberg New York 978-3-540-75141-0 Springer Berlin Heidelberg New York

This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, reproduction on microfilms or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer. Violations are liable to prosecution under the German Copyright Law. Springer is a part of Springer Science+Business Media springer.com © Springer-Verlag Berlin Heidelberg 2007 Printed in Germany Typesetting: Camera-ready by author, data conversion by Scientific Publishing Services, Chennai, India Printed on acid-free paper SPIN: 12162741 06/3180 543210

Preface

DISC, the International Symposium on Distributed Computing, is an annual forum for presentation of research on all aspects of distributed computing, including the theory, design, implementation and applications of distributed algorithms, systems and networks. The 21st edition of DISC was held during September 24­26, 2007, in Lemesos, Cyprus. This volume of proceedings begins with abstracts of three invited talks. The keynote speakers of DISC 2007 were: Burkhard Monien from the University of Paderborn, Germany, David Peleg from The Weizmann Institute of Science, Israel, and Michel Raynal from IRISA, Universit´ e de Rennes, France. There were 100 ten-page-long extended abstracts submitted to DISC this year and this volume contains 32 contributions selected by the Program Committee among these 100 submissions. Every submitted paper was read and evaluated by Program Committee members assisted by external reviewers. The final decisions regarding acceptance or rejection of each paper were made during the electronic Program Committee meeting held in June/July 2007. Revised and expanded versions of a few best selected papers will be considered for publication in a special issue of the journal Distributed Computing. The Best Student Paper Award of DISC 2007 was awarded to David Eisenstat for the paper "Fast Robust Approximate Majority" coauthored with Dana Angluin and James Aspnes. This volume of proceedings also contains nine two-page-long brief announcements (BA). These BAs present ongoing work or recent results whose full description is not yet ready; it is expected that full papers containing those results will soon appear in other conferences or journals. The main purpose of the BA track is to announce ongoing projects to the distributed computing community and to obtain feedback for the authors. Each BA was also read and evaluated by the Program Committee. This volume concludes with a section devoted to the 20th anniversary of the DISC conferences that took place during DISC 2006, held September 18­20, 2006, in Stockholm, Sweden. DISC 2007 was organized in cooperation with the University of Cyprus. The main sponsor of DISC 2007 was CYTA - Cyprus Telecommunications Authority. The support of the Cyprus Tourism Organisation, Microsoft (Cyprus) and COST Action 295 DYNAMO is also gratefully acknowledged. July 2007 Andrzej Pelc

The 2007 Edsger W. Dijkstra Prize in Distributed Computing

The 2007 Edsger W. Dijkstra Prize in Distributed Computing was presented at DISC 2007 for the paper "Consensus in the Presence of Partial Synchrony" by Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer, which appeared in the Journal of the ACM (Vol. 35, No. 2, April, 1988. pages 288­323). A preliminary version appeared in PODC 1984. This paper introduces a number of practically motivated partial synchrony models that lie between the completely synchronous and the completely asynchronous models, and in which consensus is solvable. It gives practitioners the right tool for building fault-tolerant systems, and contributes to the understanding that safety can be maintained at all times, despite the impossibility of consensus, and progress is facilitated during periods of stability. These are the pillars on which every fault-tolerant system has been built for two decades. This includes academic projects such as Petal, Frangipani, and Boxwood, as well as real-life data centers, such as the Google file system. In distributed systems, balancing the pragmatics of building software that works against the need for rigor is particularly difficult because of impossibility results such as the FLP theorem. The publication by Dwork, Lynch, and Stockmeyer was in many respects the first to suggest a path through this thicket, and has been enormously influential. It presents consensus algorithms for a number of partial synchrony models with different timing requirements and failure assumptions: crash, authenticated Byzantine, and Byzantine failures. It also proves tight lower bounds on the resilience of such algorithms. The eventual synchrony approach introduced in this paper is used to model algorithms that provide safety at all times, even in completely asynchronous runs, and guarantee liveness once the system stabilizes. This has since been established as the leading approach for circumventing the FLP impossibility result and solving asynchronous consensus, atomic broadcast, and state-machine replication. In particular, the distributed systems engineering community has been increasingly drawn towards systems architectures that reflect the basic split between safety and liveness cited above. Dwork, Lynch, and Stockmeyer thus planted the seed for a profound rethinking of the ways that we should build, and reason about, this class of systems. Following this direction are many foundational solutions. First, these include state-machine replication methods such as Lamport's seminal Paxos algorithm and many group communication methods. Another important branch of research that directly follows this work is given by Chandra and Toueg's unreliable failure detector abstraction, which is realized in the eventual synchrony model of this paper. As Chandra and Toueg write:"we argue that partial synchrony assumptions can be encapsulated in the

VIII

Preface

unreliability of failure detectors. For example, in the models of partial synchrony considered in Dwork et al. it is easy to implement a failure detector that satisfies the properties of W." Finally, the insight by Dwork, Lynch, and Stockmeyer also led to various timed-based models of partial synchrony, such as Cristian and Fetzer's Timed-Asynchronous model and others. The award committee would like to acknowledge the sincere efforts by the nominators of this work, as well as all other (worthy!) nominations which came short of winning. The Committee wishes to pay a special tribute via this award to Larry Stockmeyer, who passed away on July 31, 2004. Larry's impact on the field through this paper and many others will always be remembered. The Committee of the 2007 Edsger W. Dijkstra Prize in Distributed Computing: Hagit Attiya Dahlia Malkhi Keith Marzullo Marios Mavronicolas Andrzej Pelc Roger Wattenhofer (Chair)

Organization

DISC, the International Symposium on Distributed Computing, is an annual forum for presentation of research on all aspects of distributed computing. It is organized in cooperation with the European Association for Theoretical Computer Science (EATCS). The symposium was established in 1985 as a biannual International Workshop on Distributed Algorithms on Graphs (WDAG). The scope was soon extended to cover all aspects of distributed algorithms as WDAG came to stand for International Workshop on Distributed AlGorithms, and in 1989 it became an annual symposium. To reflect the expansion of its area of interest, the name was changed to DISC (International Symposium on DIStributed Computing) in 1998. The name change also reflects the opening of the symposium to all aspects of distributed computing.

Program Committee Chair Andrzej Pelc, Universit´ e du Qu´ ebec en Outaouais, Canada Organizing Committee Chair Chryssis Georgiou, University of Cyprus, Cyprus Steering Committee Chair Alexander Shvartsman, University of Connecticut, USA

Organizing Committee
Chryssis Georgiou Marios Mavronicolas Nicolas Nicolaides Anna Philippou University of Cyprus, Cyprus (Chair) University of Cyprus, Cyprus Congresswise, Cyprus (Financial Officer) University of Cyprus, Cyprus

Steering Committee
Hagit Attiya Shlomi Dolev Pierre Fraigniaud Rachid Guerraoui Technion, Israel Ben Gurion University, Israel CNRS and Universit´ e Paris 7, France EPFL, Switzerland, (Vice Chair)

X

Organization

Andrzej Pelc Sergio Rajsbaum Alexander Shvartsman

Universit´ e du Qu´ ebec en Outaouais, Canada Universidad Nacional Autonoma de Mexico, Mexico University of Connecticut, USA (Chair)

Program Committee
James Aspnes Reuven Cohen Sajal Das Paola Flocchini Eli Gafni Leszek G¸ asieniec Cyril Gavoille Chryssis Georgiou Amir Herzberg Alex Kesselman Rastislav Kr´ alovi c Zvi Lotker Marios Mavronicolas Michael Merritt Thomas Moscibroda Achour Mostefaoui Andrzej Pelc Michael Reiter Eric Ruppert Arun Somani Paul Spirakis Sam Toueg Jennifer Welch Udi Wieder Masafumi Yamashita Yale University, USA Technion, Israel University of Texas at Arlington, USA University of Ottawa, Canada UCLA, USA University of Liverpool, UK University of Bordeaux, France University of Cyprus, Cyprus Bar Ilan University, Israel Intel, Israel Comenius University, Slovakia Ben Gurion University, Israel University of Cyprus, Cyprus AT&T Research, USA Microsoft Research at Redmond, USA IRISA, France Universit´ e du Qu´ ebec en Outaouais, Canada (Chair) Carnegie Mellon University, USA York University, Canada Iowa State University, USA Computer Technology Institute, Greece University of Toronto, Canada Texas A&M University, USA Microsoft Research at Silicon Valley, USA Kyushu University, Japan

Organization

XI

Sponsoring Institutions

CYTA - Cyprus Telecommunications Authority

University of Cyprus

Cyprus Tourism Organisation

COST Action 295 DYNAMO
DISC 2007 Webmasters Chryssis Georgiou, University of Cyprus, Cyprus Richard Kr´ alovi c, Comenius University, Slovakia

Microsoft

Referees
N. Agmon M. K. Aguilera L. Alvisi H. Attiya Y. Bartal S. Baswana A. Beimel P. Bille E. Bortnikov D. Benjamin Carbajal J. Chalopin B. Charron-Bost I. Chatzigiannakis G. Chockler S. Das C. Delporte S. Dolev A. Dvir M. Elkin X. Euthimiou G. Even P. Fatourou H. Fauconnier A. Fernandez F. Freiling S. Funke S. Ganguly P. W. Goldberg O. Goussevskaia R. Guerraoui T. Harris Y. Haviv D. Hendler M. Herlihy E. Hillel J.-H. Hoepman S. L. Horn Z. Hu R. Kat L. Katzir D. Kaynar I. Keidar A. Kinalis R. Klasing S. Kontogiannis A. Korman D. R. Kowalski R. Kr´ alovi c D. Krizanc F. Kuhn P. Kuznetov C. Lavault P. Leone V. Liagkou

XII

Organization

M. Liu A. Lopez-Ortiz N. Lynch R. Majundar D. Malkhi G. De Marco E. Markou T. Mchenry L. Michael M. Moir P. M. Musial G. Mylonas

N. C. Nicolaou A. Nisgav F. Oprea P. Panagopoulou D. Pardubska B. Patt-Shamir T. Plachetka G. Prencipe R. De Prisco G. Proietti C. Raptopoulos E. M. Schiller

G. Shegalov H. Shpungin S. Smorodinski G. Stupp C. Travers U. Vaccaro R. Wattenhofer T. Wong Q. Xin I. Yoffe S. Zaks J. Zhang

Table of Contents

Invited Talks
Routing and Scheduling with Incomplete Information . . . . . . . . . . . . . . . . . Burkhard Monien and Karsten Tiemann Time-Efficient Broadcasting in Radio Networks . . . . . . . . . . . . . . . . . . . . . . David Peleg A Subjective Visit to Selected Topics in Distributed Computing . . . . . . . Michel Raynal 1 3 5

Regular Papers
Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage Without (Unproven) Cryptographic Assumptions . . . . . . . . . . . . . Amitanand S. Aiyer, Lorenzo Alvisi, and Rida A. Bazzi A Simple Population Protocol for Fast Robust Approximate Majority . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dana Angluin, James Aspnes, and David Eisenstat A Denial-of-Service Resistant DHT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Baruch Awerbuch and Christian Scheideler Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks . . . . Roberto Baldoni, Kleoni Ioannidou, and Alessia Milani Self-stabilizing Counting in Mobile Sensor Networks with a Base Station . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Joffroy Beauquier, Julien Clement, Stephane Messika, Laurent Rosaz, and Brigitte Rozoy Scalable Load-Distance Balancing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Edward Bortnikov, Israel Cidon, and Idit Keidar Time Optimal Asynchronous Self-stabilizing Spanning Tree . . . . . . . . . . . . Janna Burman and Shay Kutten Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links . . . J´ er´ emie Chalopin, Shantanu Das, and Nicola Santoro Weakening Failure Detectors for k -Set Agreement Via the Partition Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Wei Chen, Jialin Zhang, Yu Chen, and Xuezheng Liu 7

20 33 48

63

77 92 108

123

XIV

Table of Contents

Amnesic Distributed Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gregory Chockler, Rachid Guerraoui, and Idit Keidar Distributed Approximations for Packing in Unit-Disk Graphs . . . . . . . . . . Andrzej Czygrinow and Michal Ha´ n´ ckowiak From Crash-Stop to Permanent Omission: Automatic Transformation and Weakest Failure Detectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Carole Delporte-Gallet, Hugues Fauconnier, Felix C. Freiling, Lucia Draque Penso, and Andreas Tielmann Deterministic Distributed Construction of Linear Stretch Spanners in Polylogarithmic Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Bilel Derbel, Cyril Gavoille, and David Peleg On Self-stabilizing Synchronous Actions Despite Byzantine Attacks . . . . Danny Dolev and Ezra N. Hoch Gossiping in a Multi-channel Radio Network: An Oblivious Approach to Coping with Malicious Interference (Extended Abstract) . . . . . . . . . . . . Shlomi Dolev, Seth Gilbert, Rachid Guerraoui, and Calvin Newport The Space Complexity of Unbounded Timestamps . . . . . . . . . . . . . . . . . . . Faith Ellen, Panagiota Fatourou, and Eric Ruppert Approximating Wardrop Equilibria with Finitely Many Agents . . . . . . . . Simon Fischer, Lars Olbrich, and Berthold V¨ ocking Energy and Time Efficient Broadcasting in Known Topology Radio Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Leszek G¸ asieniec, Erez Kantor, Dariusz R. Kowalski, David Peleg, and Chang Su A Distributed Algorithm for Finding All Best Swap Edges of a Minimum Diameter Spanning Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Beat Gfeller, Nicola Santoro, and Peter Widmayer On the Message Complexity of Indulgent Consensus . . . . . . . . . . . . . . . . . . Seth Gilbert, Rachid Guerraoui, and Dariusz R. Kowalski Gathering Autonomous Mobile Robots with Dynamic Compasses: An Optimal Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Taisuke Izumi, Yoshiaki Katayama, Nobuhiro Inuzuka, and Koichi Wada Compact Separator Decompositions in Dynamic Trees and Applications to Labeling Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Amos Korman and David Peleg

139 152

165

179 193

208 223 238

253

268 283

298

313

Table of Contents

XV

On the Communication Surplus Incurred by Faulty Processors . . . . . . . . . Dariusz R. Kowalski and Michal Strojnowski Output Stability Versus Time Till Output (Extended Abstract) . . . . . . . . Shay Kutten and Toshimitsu Masuzawa A Distributed Maximal Scheduler for Strong Fairness . . . . . . . . . . . . . . . . . Matthew Lang and Paolo A.G. Sivilotti Cost-Aware Caching Algorithms for Distributed Storage Servers . . . . . . . Shuang Liang, Ke Chen, Song Jiang, and Xiaodong Zhang Push-to-Pull Peer-to-Peer Live Streaming . . . . . . . . . . . . . . . . . . . . . . . . . . . Thomas Locher, Remo Meier, Stefan Schmid, and Roger Wattenhofer Probabilistic Opaque Quorum Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Michael G. Merideth and Michael K. Reiter Detecting Temporal Logic Predicates on Distributed Computations . . . . . Vinit A. Ogale and Vijay K. Garg Optimal On-Line Colorings for Minimizing the Number of ADMs in Optical Networks (Extended Abstract) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Mordechai Shalom, Prudence W.H. Wong, and Shmuel Zaks Efficient Transformations of Obstruction-Free Algorithms into Non-blocking Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gadi Taubenfeld Automatic Classification of Eventual Failure Detectors . . . . . . . . . . . . . . . . Piotr Zieli´ nski

328 343 358 373 388 403 420

435

450 465

Brief Announcements
When 3f + 1 Is Not Enough: Tradeoffs for Decentralized Asynchronous Byzantine Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Alysson Neves Bessani, Miguel Correia, Henrique Moniz, Nuno Ferreira Neves, and Paulo Verissimo On the Complexity of Distributed Greedy Coloring . . . . . . . . . . . . . . . . . . . Cyril Gavoille, Ralf Klasing, Adrian Kosowski, and Alfredo Navarra Fault-Tolerant Implementations of the Atomic-State Communication Model in Weaker Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Colette Johnen and Lisa Higham Transaction Safe Nonblocking Data Structures . . . . . . . . . . . . . . . . . . . . . . Virendra J. Marathe, Michael F. Spear, and Michael L. Scott 480

482

485 488

XVI

Table of Contents

Long Live Continuous Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Tal Mizrahi and Yoram Moses Fully Distributed Algorithms for Convex Optimization Problems . . . . . . . Damon Mosk-Aoyama, Tim Roughgarden, and Devavrat Shah On the Power of Impersonation Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Michael Okun Perfectly Reliable and Secure Communication in Directed Networks Tolerating Mixed Adversary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Arpita Patra, Ashish Choudhary, Kannan Srinathan, and Chandrasekharan Pandu Rangan A Formal Analysis of the Deferred Update Technique . . . . . . . . . . . . . . . . . Rodrigo Schmidt and Fernando Pedone

490 492 494

496

499

DISC 20th Anniversary
DISC at Its 20th Anniversary (Stockholm, 2006) . . . . . . . . . . . . . . . . . . . . . Michel Raynal, Sam Toueg, and Shmuel Zaks Time, Clocks, and the Ordering of My Ideas About Distributed Systems (DISC 20th Anniversary: Invited Talk) . . . . . . . . . . . . . . . . . . . . . . Leslie Lamport My Early Days in Distributed Computing Theory: 1979-1982 (DISC 20th Anniversary: Invited Talk) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Nancy Lynch Provably Unbreakable Hyper-Encryption Using Distributed Systems (DISC 20th Anniversary: Invited Talk) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Michael O. Rabin Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501

504

505

506

509

Routing and Scheduling with Incomplete Information
Burkhard Monien and Karsten Tiemann
Faculty of Computer Science, Electrical Engineering, and Mathematics, University of Paderborn, 33102 Paderborn, Germany {bm,tiemann}@uni-paderborn.de Abstract. In many large-scale distributed systems the users have only incomplete information about the system. We outline game theoretic approaches that are used to model such incomplete information settings.

1

Introduction

In recent years, combining ideas from game theory and computer science to study distributed systems has become increasingly popular. While most work in this direction assumes that each player is completely informed there are also approaches that allow the more realistic case of incomplete information. Following the concept of Bayesian decision theory it is for incomplete information settings usually assumed that a player who does not know relevant parameters of a game is aware of probability distributions over the possible outcomes of these parameters. Hence there is a similarity to stochastic programming models where probability distributions for the uncertain data of optimization problems are known. We will now outline two different approaches that are used to handle incomplete information.

2

Harsanyi's Incomplete Information Model

The Nobel laureate Harsanyi [4] introduced in the 1960s an elegant approach that can be used to study non-cooperative games with incomplete information. The Harsanyi transformation converts such a game with incomplete information to a game where players have different types. In the resulting Bayesian game, the players' uncertainty about each other's type is described by a probability distribution over all possible type profiles. Each player selects a strategy for each of his types. A stable state in which all types of all players minimize their individual cost is called a Bayesian Nash equilibrium. Recently, Harsanyi's approach was used to study a selfish routing scenario in networks where the players do not know each other's weight. In these socalled Bayesian routing games, each type of a player corresponds to some weight. Gairing et al. [2] considered for these games the existence of equilibria, the computation of equilibria, and the so-called price of anarchy that measures the worst-possible inefficiency of equilibria with respect to a social welfare measure.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 1­2, 2007. c Springer-Verlag Berlin Heidelberg 2007

2

B. Monien and K. Tiemann

3

Incomplete Information Models Where Every Player Minimizes Its Expected Individual Cost

Another possibility to handle incomplete information is to assume that each player bases his decisions on the expected values of unknown parameters. To do so, a player computes the expected values of parameters based on his own probability distributions over the possible outcomes of the parameters. Although this approach is popular in the scientific literature we only sketch two models here that are based on this approach. Expected outcomes of unknown parameters were used for network routing games where the players have incomplete information about the edge latency functions. Since each player obtains for each edge his own expected latency function we get games with player-specific latency functions [6]. For these games positive and negative results on the existence of equilibria, convergence to equilibria, computation of equilibria, and the price of anarchy are known [3,5,6]. The approach to use expected outcomes for unknown parameters was also used by Dong et al. [1] who developed a supply chain network model consisting of manufacturers and retailers where the demands associated with the retail outlets are random. Here every retailer maximizes its expected profit, which is the difference between the expected revenues, the handling cost, and the payout to the manufacturers. Dong et al. [1] focused on the existence of equilibria, uniqueness of equilibria, and convergence to equilibria.

References
1. Dong, J., Zhang, D., Nagurney, A.: A supply chain network equilibrium model with random demands. European Journal of Operational Research 156(1), 194­212 (2004) 2. Gairing, M., Monien, B., Tiemann, K.: Selfish Routing with Incomplete Information. In: Proceedings of the 17th Annual ACM Symposium on Parallel Algorithms and Architectures, pp. 203­212. ACM Press, New York (2005)Also accepted to Theory of Computing Systems. 3. Gairing, M., Monien, B., Tiemann, K.: Routing (Un-) Splittable Flow in Games with Player-Specific Linear Latency Functions. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006. LNCS, vol. 4051, pp. 501­512. Springer, Heidelberg (2006) 4. Harsanyi, J.C.: Games with Incomplete Information Played by Bayesian Players, I, II, III. Management Science 14, 159­182, 320­332, 468­502 (1967) 5. Mavronicolas, M., Milchtaich, I., Monien, B., Tiemann, K.: Congestion Games with Player-Specific Constants. In: Proceedings of the 32nd International Symposium on Mathematical Foundations of Computer Science (to appear, 2007) 6. Milchtaich, I.: Congestion Games with Player-Specific Payoff Functions. Games and Economic Behavior 13(1), 111­124 (1996)

Time-Efficient Broadcasting in Radio Networks
David Peleg
Department of Computer Science and Applied Mathematics, The Weizmann Institute of Science, Rehovot 76100, Israel david.peleg@weizmann.ac.il

As broadcasting is one of the primary functions in radio networks, fast algorithms for performing it are of considerable interest. A radio network consists of stations that can act at any a given time step either as transmitters or as receivers. Given a deployment of the stations, the reception conditions can be modeled by a graph, where the existence of an edge between two nodes indicates that transmissions of one of them can reach the other, i.e., these nodes can communicate directly. The message transmitted by a node in given time step is delivered in the same time step to all of its neighbors in the graph. A node acting as a receiver in a given step will successfully receive a message if and only if exactly one of its neighbors transmits in that step. If two or more neighbors of a node transmit simultaneously, then a collision occurs and none of the messages is heard by the node in that step. Broadcasting is the following basic communication task. Initially, one distinguished node, called the source, has a message which has to be disseminated to all other nodes. Typically, not all stations are within the source's transmission range, hence the source message must be propagated to remote nodes via intermediate ones. The model considered is synchronous, namely, all nodes have individual clocks that tick at the same rate, measuring time steps or rounds. The execution time of a broadcasting algorithm in a given radio network is the number of rounds it takes since the first transmission until all nodes of the network have received the source message. The task of broadcasting in radio networks has been studied in a large variety of different models and under different requirements. Some of the main parameters giving rise to the different variants of the problem are the following. Collision detection: There are two common models regarding the effect of a collision. The collision detection model assumes that a node acting as a receiver can recognize the fact that a collision has occurred. The alternative model is based on the assumption that the receiving node cannot distinguish a collision from background noise, hence it cannot tell whether a collision occurred or no transmissions took place. Wake-up mode: The broadcasting protocol can be activated at the network stations in accordance to one of the following two models. In the spontaneous wake up model, all stations wake up when the source transmits for the first time, i.e., their clocks start simultaneously with the source. Consequently, all nodes may contribute to the efficiency of the broadcasting process by transmitting various preparatory control messages even before they receive the source message. In
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 3­4, 2007. c Springer-Verlag Berlin Heidelberg 2007

4

D. Peleg

contrast, in the conditional wake up model, the stations other than the source are initially dormant and do not transmit until they receive a message for the first time and wake up. Thus in this model, the clock of a node starts on the round when it first receives the source message. Distributed vs. centralized models: In the distributed setting, it is usually assumed that nodes are unaware of the topology of the network and do not know its diameter, its size or other parameters. It is often assumed that the nodes are not even aware of their immediate neighborhood. Networks featuring these characteristics are sometimes called ad hoc networks. In contrast, in certain contexts one may be interested in solving the time-efficient broadcasting problem under more favorable conditions, such as full information or assuming a central authority monitoring the broadcasting process. In such cases, it may be possible to pre-compute optimal or near-optimal broadcast schedules. Topology classes: In addition to the study of general (arbitrary topology) radio networks, some recent interest arose concerning a natural subclass, referred to as UDG radio networks, where the network is modeled as a unit disk graph (UDG) whose nodes are represented as points in the plane. It is assumed that the transmission devices are capable of transmitting to distance 1, hence two points are joined by an edge if and only if their Euclidean distance is at most 1. Use of randomness: Both deterministic and randomized broadcasting algorithms were considered in the literature. The talk will review the literature on time-efficient broadcasting algorithms for radio networks under a variety of models and assumptions.

A Subjective Visit to Selected Topics in Distributed Computing
Michel Raynal
IRISA, Universit´ e de Rennes, 35042 Rennes, France raynal@irisa.fr

After presenting a personal view of distributed computing (of course, being personal, this view is partial and questionable), this invited talk will address distributed computing problems that have recently received attention in the literature. For each of them, the talk presents the problem, results from the community, results from the author (and his co-authors), and questions that remain open. The following are among the topics covered in the talk. Exclude or go fast enough?. The obstruction-free approach consists in designing algorithms that always preserve the safety property and always terminate when the process that issued the operation can execute alone during a "long enough" period of time [7]. It has been shown that the minimal information on failures needed to transform an obstruction-free algorithm into its wait-free counterpart [6] are failure detectors that allows a single process at a time to proceed [5]. This approach requires the processes to exclude each other. The notion of a timed register has recently been proposed [9]. Such a register places a timing constraint on each write operation that follows a read operation (on that register). The write succeeds if it occurs "quick enough", otherwise it fails. Such registers allows implementing consensus for any number of processes in systems that satisfy a relatively weak timing assumption. So, this approach demands each process to go fast enough between a read of a timed register and the following write on the same timed register. Each of the previous approaches can be seen as a particular facet of the same scheduling problem. The statement of a general scheduling framework that would unify them seems to be challenging open problem. Towards a hierarchy of sub-consensus tasks. Asynchronous shared memory systems prone to process crashes defines a land only few parts of which have been explored. But that land has been provided with two main lighthouses: Herlihy's notion of wait-free synchronization (and the associated notion of consensus number) [6], and Gafni's notion of read/write reductions [2]. So, given an asynchronous shared memory system enriched with some base objects, fundamental questions are the following ones: which problems can be wait-free solved? Is there a hierarchy for wait-free subconsensus problems? Is it possible to define a measure (similar to consensus number) for these problems? Etc. Focusing on the adaptive renaming problem, the talk will present reductions showing that that problem lies exactly in between the set agreement problem and the test-and-set problem, thereby defining a hierarchy of sub-consensus problems [4]. Enriching this hierarchy remains one of the most fundamental challenges of asynchronous computability in presence of failures [2].
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 5­6, 2007. c Springer-Verlag Berlin Heidelberg 2007

6

M. Raynal

An arithmetic of synchronous set-agreement. Synchronous k -set agreement in presence of up to t faulty processes requires Rt,f rounds where Rt,f = min f k + t + 1 rounds (f denoting the number of actual crashes in a run, 0  f  t). 2, k This is a tight bound [1,3]. So, an interesting question is the following: given base objects of some type, how much do they allow bypassing that bound? The talk will explore the case where the base objects (denoted [m, ] SA objects) allow solving the -set agreement problem among m processes (m < n) t ) rounds (more precisely, [8]. It will present an algorithm that requires O( mk f t k Rt,f,m, = min  + 2,  + 1 , with  = m + (k mod )). Open problems related to the synchronous set agreement will also be presented.

Acknowledgments
The work described in this talk is not only mine. It mainly results from common works and inflamed discussions with colleagues and friends (Eli Gafni, Rachid Guerraoui, Achour Mostefaoui, Sergio Rajsbaum, Gadi Taubenfeld and Corentin Travers), with whom I additionally share a common view of what are computing science and science.

References
1. Chaudhuri, S., Herlihy, M., Lynch, N., Tuttle, M.: Tight Bounds for k-Set Agreement. Journal of the ACM 47(5), 912­943 (2000) 2. Gafni, E.: Read-Write Reductions. In: Chaudhuri, S., Das, S.R., Paul, H.S., Tirthapura, S. (eds.) ICDCN 2006. LNCS, vol. 4308, pp. 349­354. Springer, Heidelberg (2006) 3. Gafni, E., Guerraoui, R., Pochon, B.: From a Static Impossibility to an Adaptive Lower Bound: The Complexity of Early Deciding Set Agreement. In: STOC 2005. Proc. 37th ACM Symposium on Theory of Computing, pp. 714­722. ACM Press, New York (2005) 4. Gafni, E., Raynal, M., Travers, C.: Test&Set, Adaptive Renaming and Set Agreement: a Guided Visit to Asynchronous Computability. In: SRDS'07. Proc. 26th IEEE Symposium on Reliable Distributed Systems. IEEE Computer Society Press, Beijing (China) (2007), ftp://ftp.irisa.fr/techreports/2007/PI-1837.pdf 5. Guerraoui, R., Kapalka, M., Kouznetsov, P.: The Weakest Failure Detectors to Boost Obstruction-Freedom. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 376­390. Springer, Heidelberg (2006) 6. Herlihy, M.P.: Wait-Free Synchronization. ACM Transactions on Programming Languages and Systems 13(1), 124­149 (1991) 7. Herlihy, M., Luchangco, V., Moir, M.: Obstruction-free synchronization: doubleended queues as an example. In: CDCS '03. Proc. 23rd IEEE Int'l Conference on Distributed Computing Systems I(CDCS '03), pp. 522­529. IEEE Computer Society Press, Los Alamitos (2003) 8. Mostefaoui, A., Raynal, M., Travers, C.: Narrowing Power vs Efficiency in Synchronous Set Agreement. Tech Report # 1836, IRISA, Universit´ e de Rennes (France) (2007), ftp://ftp.irisa.fr/techreports/2007/PI-1836.pdf 9. Raynal, M., Taubenfeld, G.: The Notion of a Timed Register and its Application to Indulgent Synchronization. In: SPAA'07. 19th ACM Symposium on Parallel Algorithms and Architectures, pp. 200­207. ACM Press, New York (2007)

Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage Without (Unproven) Cryptographic Assumptions
Amitanand S. Aiyer1 , Lorenzo Alvisi1, , and Rida A. Bazzi2
1 2

Department University Department Arizona

of Computer Sciences, of Texas at Austin of Computer Sciences, State University

Abstract. We present the first optimally resilient, bounded, wait-free implementation of a distributed atomic register, tolerating Byzantine readers and (up to one-third of) Byzantine servers, without the use of unproven cryptographic primitives or requiring communication among servers. Unlike previous (non-optimal) solutions, the sizes of messages sent to writers depend only on the actual number of active readers and not on the total number of readers in the system. With a novel use of secret sharing techniques combined with write back throttling we present the first solution to tolerate Byzantine readers information theoretically, without the use of cryptographic techniques based on unproven numbertheoretic assumptions.

1

Introduction

Distributed storage systems in which servers are subject to Byzantine failures have been widely studied. Results vary in the assumptions made about both the system model and the semantics of the storage implementation. The system parameters include the number of clients (readers and writers), the synchrony assumptions, the level of concurrency, the fraction of faulty servers, and the faulty behavior of clients. In the absence of synchrony assumptions, atomic [8] read and write semantics are possible, but stronger semantics are not [7]. We consider implementations with atomic semantics in this paper. We consider solutions in an asynchronous system of n servers that do not communicate with each other (non-communicating servers) and in which up to f servers are subject to Byzantine failures (f -resilient), any number of clients can fail by crashing (wait-free), and readers can be subject to Byzantine failures. Systems in which servers do not communicate with each other are interesting because solutions that depend on communication between servers tend to have high message complexity, quadratic in the number of servers [10,4].
This work was supported in part by NSF awards CSR--PDOS 0509338 and CyberTrust 043051.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 7­19, 2007. c Springer-Verlag Berlin Heidelberg 2007

8

A.S. Aiyer, L. Alvisi, and R.A. Bazzi

In the non-communicating servers model, the best previous solution that provides wait-free atomic semantics requires 4f + 1 servers [3]. That solution (i) requires clients and servers to exchange a finite number of messages and (ii) limits the size of the messages sent by the servers to the readers: the size of these messages is bound by a constant times the logarithm of the number of write operations performed in the system--or, equivalently, by a constant times the size of a timestamp. Unfortunately, this solution allows messages sent to writers to be as large as the maximum number of potential readers in the system, even during times when the number of actual readers is small. Recently, and concurrently with our work, a wait-free atomic solution that requires not more than 3f + 1 servers was proposed, but that solution requires unbounded storage, message of unbounded size, and an unbounded number of messages per read operation [6]. None of these solutions consider Byzantine readers. Byzantine behavior of readers is relevant because wait-free atomic solutions require that readers write to servers [5]. All existing work that considers Byzantine readers uses cryptographic techniques based on unproven number-theoretic assumptions [4,9]. So, the existing results leave open two fundamental questions: ­ Is the additional cost of f replicas over the optimal for unbounded solutions required to achieve a bounded wait-free solution? ­ Is the use of cryptographic techniques required to tolerate Byzantine readers? We answer both questions in the negative. We show that tolerating Byzantine readers can be achieved with information-theoretic guarantees and without the use of unproven number-theoretic assumptions. We also show that a bounded wait-free implementation of a distributed storage with atomic semantics is possible for n = 3f + 1 (which is optimal). Our solution also bounds the size of messages sent to writers--a significant improvement over Bazzi and Ding's nonoptimal solution [3]. To achieve our results, we refine existing techniques and introduce some new techniques. The ideas we refine include concurrent-reader detection and writeback throttling, originally proposed in the atomic wait-free solution of Bazzi and Ding [3]. In what follows we give a high level overview of the new techniques we introduce. Increasing resiliency. We increase the resiliency of our solution by introducing a new way by which a reader selects the timestamp of the value it will try to read. Instead of choosing the f + 1'st largest among the received timestamps, in our protocol the reader chooses the 2f + 1'st smallest. In fact, we realized that the f + 1 largest timestamp worked well for n = 4f + 1 simply because, for that value of n, the f + 1 largest received timestamp coincides with the 2f + 1 smallest. We guarantee the liveness of our new selection process by having the reader continuously update the value of the 2f + 1'st smallest timestamp as it receives responses from new servers. Bounding message sizes to writers. We bound the sizes of messages sent to servers using three rounds of communication between writers and servers. These

Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage

9

rounds occur in parallel with the first two rounds of the write protocol and no server receives a total of more than two messages across the three rounds. In the first round, the writer estimates the number of concurrent readers; in the second and third rounds it determines their identities. Tolerating Byzantine readers. We use write back throttling combined with secret sharing to tolerate Byzantine readers. The idea is to associate a random secret with each write and share the secret among the servers in such a way that it can only be reconstructed if enough servers reveal their shares. By requiring that a correct server only divulge its share if the write has made sufficient progress, we use a reader's ability to reconstruct the secret as a proof that the reader is allowed to write back. By using secret sharing, we avoid relying on unproven number theoretic assumptions and achieve instead information-theoretic guarantees.

2

Model/Assumptions

The system consists of a set of n replicas (servers), a set of m writers and a set of readers. Readers and writers are collectively referred to as clients. Clients have unique identifiers that are totally ordered. When considering boundedness of the sizes of messages, we assume that a read operation in the system can be uniquely identified with a finite bit string (otherwise any message sent by a reader can be unbounded in size). The identifier consists of a reader identifier and a read operation tag. Similarly write operations are identified by the writer identifier and the timestamp of the value being written. Since timestamps are non-skipping [2], writes can also be represented by finite strings. Clients execute protocols that specify how read and write operations are implemented. We assume that clients do not start a new operation before finishing a previous operation. We assume that up to f servers may deviate arbitrarily from the specified protocol (Byzantine) and that the remaining (n - f ) servers are correct. We require that the total number of servers n be at least 3f + 1. We assume that messages cannot be spoofed. While this is typically enforced in practice using digital signatures, based on public key cryptography, such techniques are not necessarily required to enforce our requirement. We assume FIFO point-to-point asynchronous channels between clients and servers. Servers do not communicate with other servers. Writers are benign and can only fail by crashing. In Section 3 we also assume that the readers are benign; we relax this assumption in Section 5 where we consider Byzantine readers . When considering Byzantine readers, we make the additional assumption that the channels between the servers and the writers are private i.e. messages sent over these channels cannot be eves-dropped by the adversary. For our implementation, the probability that a given read operation by a Byzantine reader improperly writes back a value is 2-k where k is a security parameter. We choose k to be sufficiently large so that the probability of failure for all operations is small. If k = o + k bits, where o is the number of bits required to represent one operation, then the system failure probability is 2-k .

10

A.S. Aiyer, L. Alvisi, and R.A. Bazzi

Schemes based on public key cryptography, in the best case, also suffer from this negligible small probability of error. If the unproven assumptions that they are based upon do not hold, their probability of error can be significantly larger.

3

Bounded Atomic Register

We present a single-writer protocol that implements a wait-free atomic register using 3f + 1 replicas where the size and the number of messages exchanged per operation is bounded. Figures 1­3 present a single-writer-multiple-reader version of the protocol that assumes that the readers are benign. In Section 5 we show how to extend this protocol to handle Byzantine readers. These protocols can also be easily extended to support multiple-writers, using ideas from [3]. We refer the reader to [1] for proofs and a more detailed discussion. 3.1 Protocol Overview

The write operation. The write operation is performed in two phases. In phase 1, the writer sends the value to all the servers and waits for (n - f ) acknowledgements. The writer also initiates, in parallel, the GetConcurrentReaders protocol to detect concurrent readers. The GetConcurrentReaders is a bounded protocol, described in Section 3.3, that detects all read operations which are considered to be active at all the non-faulty servers, when the protocol is executed. In phase 2, the writer asks all the servers to update their current timestamp, and to forward the values that they have to all the concurrent readers detected in phase 1. On receiving (n - f ) acknowledgements, the write operation completes. This two-phase mechanism guarantees that if a non-faulty server updates its current timestamp, then at least f + 1 non-faulty servers must have already received the value. The read operation. To understand the reader's protocol, we consider a simple scenario. The reader starts by requesting second phase information from the servers. Each server replies with the most current timestamp for which it knows that the corresponding write operation reached its second phase. Now, assume that the reader receives replies from all correct servers in response to its request for second phase information. The timestamps returned by these correct servers can be quite different because the reader's requests could reach them at different times and the writer could have executed many write operations during that time. Of special interest is the largest second phase timestamp returned by a correct server. Let us call that timestamp tlargest . If the writer executes no write operation after its write of tlargest , then, when the reader receives the second phase response with tlargest , it can simply request all first phase messages and be guaranteed to receive f + 1 replies with identical value v and timestamp tlargest ; at that time, the reader would be able to determine that, by reading v , it would not violate atomic semantics. The reader then writes back the value and then the timestamp in two phases to complete the read operation.

Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage

11

write( ) { inc(ts) // Phases W1 cobegin { writeVal(); CR = GetConcurrentReaders() } coend

} writeVal( ) { send (WRITE VAL, v, ts ) to all wait for (n - f ) acks. }

// Phase W2 send (WRITE TS, ts, CR) to all wait for (n - f ) acks.

Fig. 1. The Writer's Protocol

While this scenario is instructive, it overlooks some complications. For instance, a fast writer might write many values with timestamps larger than tlargest . Also, the reader does not know when it has received replies from all correct servers. If we assume, for now, that the reader can tell when it has received values from all correct servers, then we can solve the problems caused by a fast writer by having the fast writer help the reader to terminate. This is done by having the writer detect concurrent read operations and then have the writer request from the server to flush out the written value to concurrent readers. Our solution requires that servers keep the 3 most up to date written values because the detection of concurrent readers is only guaranteed when the writer completely writes a value whose timestamp is larger than tlargest + 1. There remains the problem of the reader not knowing when it has received replies from all correct servers. In fact, in response to its request for second phase information, the reader can receive replies only from n - f servers--f of which may be faulty--and it might not be able to terminate based on these responses. We handle this situation by simply assuming that these n - f messages are all from correct servers. If they indeed are, then the reader will for sure be able to decide on tlargest by requesting the first phase information (it is possible that the reader will be able to decide even if they are not correct). If, however, the reader is unable to decide, then there are other correct servers whose replies are not amongst the n - f replies, and, by waiting long enough, the reader will eventually receive some message from one of those servers. When an undecided reader receives a new message, it recalculates tlargest assuming that, with the new messages it received, it must finally have replies from all correct servers: therefore, the reader re-requests the first phase information from all servers. This process continues until the reader indeed receives replies from all correct servers, in which case, it is guaranteed to decide. Finally, in the above discussion we have assumed that the reader knows what tlargest is--in reality, in our protocol the reader can only estimate tlargest by using the 2f + 1'st smallest second phase timestamp. We can show that this is sufficient to guarantee that the reader can decide and that its decision is valid [1]. 3.2 Protocol Guarantees

The protocol presented provides atomic semantics. The reader and the writer protocols always terminate and are wait-free. Boundedness. A solution is amortized bounded if m operations do not generate more than m × k messages, for some constant k without some servers being

12

A.S. Aiyer, L. Alvisi, and R.A. Bazzi
Initialization: READERS :=  RNextVal:=  server( ) { // Write Protocol messages on receive (WRITE VAL, v, ts ) from writer if (RVal.ts < ts) (RPrev2 , RPrev , RVal) := ( RPrev , RVal, v, ts ) send WRITE-ACK1 to the writer on receive (WRITE TS, ts, CR) from writer if (Rcts< ts) Rcts:= ts for each r  CR: send (FWD, s, RVal, { RVal, RPrev , RPrev2 }) to r READERS = READERS \ CR send WRITE-ACK2 to the writer // Read Protocol messages on receive (GET TS) from reader r: READERS .enqueue(r) send (TS, s, Rcts) to r on receive (GET VAL) from reader r send (VALS, s, { RVal, RPrev }) to r // Write back Messages on receive (WBACK VAL, v, ts ) from reader r wait for ( Rcts ts - 1 ) if (RVal.ts < ts) (RPrev2 , RPrev , RVal) := ( RPrev , RVal, v, ts ) send WBACK-ACK1 to r on receive (WBACK TS, ts) from reader r wait for ( RVal.ts  ts ) if (Rcts< ts) Rcts:= ts READERS .remove(r) send WBACK-ACK2 to r // GetConcurrentReaders Protocol messages on receive (GET ACT RD CNT) from writer send (RDRS CNT, s, READERS .size()) to writer on receive (GET ACT RDS, count) from writer send (READERS, s, READERS [1:count]) to writer on receive (GET ACT RDS INS, A) from writer send (RDRS INS, s, READERS  A) to writer }

Definitions: valid( v, ts ) |{s : v, ts  Values[s] }|  f + 1 notOld( v, ts ) |{s : last comp[s]  ts}|  2f + 1 fwded( v, ts ) |{s : f wd[s] = v, ts }|  f + 1 read( ) { s: last comp[s] =  ; fwd[s] =  ; Values[s] =  // Phase R1 send (GET TS) to all repeat on receive (TS, s, ts) from server s last comp[s] = ts on receive (FWD, s, v, ts , V als) from server s fwd[s] = v, ts Values[s] = Values[s]  V als until (|{x : last comp[x] =}|  n - f ) // Phase R2 send (GET VAL) to all repeat on receive (TS, s, ts) from server s last comp[s] = ts send (GET VAL) to all on receive (VALS, s, V als) from server s Values[s] = Values[s]  V als on receive (FWD, s, v, ts , V als) from server s fwd[s] = v, ts Values[s] = Values[s]  V als until ( vc , tsc : fwded( vc , tsc ) ( notOld( vc , tsc )  valid( vc , tsc ) )) // Phase R3 WriteBack(tsc ) return vc , tsc

}

WriteBack( ts ) { // Round 1 send (WBACK VAL, v, ts ) to all wait for (n - f ) acks. // Round 2 send (WBACK TS, ts) to all wait for (n - f ) acks. }

Fig. 2. Reader's protocol

Fig. 3. Protocol for server s

detected as faulty. In an amortized bounded solution, a client executing a particular operation might have to handle an unbounded number of late messages. In a bounded solution a client operation will always handle no more than k messages for some constant k and if more than k messages are received, the faulty behavior of some servers will be detected. Our solution is amortized bounded. This does not rule out the possibility that a reader receives many unsolicited messages from a server. All we can do in that case is to declare the server faulty and our proof of boundedness does not apply to such rogue servers that are detected to be faulty. To make the solution bounded for the reader techniques such as [3] can be used. 3.3 Bounded Detection of Readers

The protocol requires that the writer be able to detect ongoing read operations. A writer that invokes GetConcurrentReaders() after all correct servers have started processing a read request r issued by client cr must be able to identify r (assuming r does not terminate before the end of the execution of the detection protocol).

Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage
Definitions: notLarge( s ) |{x : count[x]  count[s]}|  f + 1 GetConcurrentReaders() { s: readers[s] :=  s: count[s] :=  s: sent[s] := false union set :=  // Get Active reader count send (GET ACT RD CNT) to all servers // Get Active reader lists from servers with valid count repeat on receive (RDRS CNT, s, count) from server s count[s] = count p: if (notLarge(p)  sent[p] = false) send (GET ACT RDS) to server p sent[p] := true on receive (READERS , s, R) from server s if ( ¬ sent[s]  (sent[s]  count[s] = |R| )) detect failure of s else } readers[s] := R until (|{readers[s] : readers[s] =}|  f + 1) union set := s readers[s]

13

// Get union set  Active reader lists from the rest for each (s : sent[s] = true) send (GET ACT RDS INS, union set) to server s repeat on receive (READERS, s, R) from server s if ( ¬ sent[s]  (sent[s]  count[s] = |R|)) detect failure of s else readers[s] := R on receive (RDRS INS, s, R) from server s if ( R  union set ) detect failure of s else readers[s] := R until (|{s : readers[s] =}|  n - f ) CR = {x : |{s : x  readers[s]}|  (f + 1)} return CR

Fig. 4. Bounded detection of readers: Writer code

A simple way to implement the required functionality is for the writer to collect, from all servers, the sets of ongoing read operations (the active reader operations) and to identify those among them that appear in at least f + 1 sets: this is the approach taken in [3]. Because it is possible that some servers may have begun processing read requests that have not yet reached the other servers, faulty servers can send arbitrarily long lists of bogus active operations without being detected as faulty. Our protocol rectifies this problem, and is shown in Figure 4. Protocol Description. The idea of the protocol is to first estimate the number of active read operations in the system and then accept lists of active reader operations whose size is bounded by this estimate. The difficulty is in ensuring that all genuinely active operations, and only those, are detected. The protocol has two phases. In the first phase, the writer determines a set of servers who are returning a valid active list count, i.e. a count of active reader operations that does not exceed the count returned by at least some correct server. In the second phase, the writer requests these servers for their active lists, which are known not to be too large. For servers, whose count is not known to be valid, the writer cannot request the active list since it could be too large. However, once the writer has collected f + 1 active lists from servers with a valid count, the writer sends the union of these lists to the remaining servers and only requests for the elements in the server's active list that is present in the union. On receiving the active sets (or their intersection) from at least n - f servers, the writer computes the set of concurrent readers. Protocol guarantees. Since the writer only needs to wait for (n - f ) responses, this sub-protocol always terminates. The number of messages exchanged in this sub-protocol is bounded as the writer does not send or receive more than two messages to any server. The messages sent in the first two phases are bounded in size because the writer only requests active lists from servers with a valid count.

14

A.S. Aiyer, L. Alvisi, and R.A. Bazzi

The messages in the third step is also bounded in size, because the size of the computed union set is bounded.

4

Tolerating Byzantine Readers

In a wait-free atomic implementation of replicated storage, readers must write to servers to ensure read-read atomicity [5]. With Byzantine readers, servers need guarantees that the values written by readers are valid. This can be satisfied by having the reader present a proof that a correct server vouches for the value and such a proof can be satisfied by having the reader present evidence that f + 1 servers vouch for the value it wants to write back. Traditionally, such vouchers or proofs rely on public key cryptography, which depend on unproven assumptions such as the hardness of factoring, or the hardness of computing discrete logarithms [9]. In general, it is not sufficient for a reader to prove that the value it is writing originated from the writer. For instance, if the reader is expected to write more than one value in some order, then the reader should not write a later value without having completed writing the previous value in the order. Omitting to write some values can in general lead to violations of the protocol's requirements. With respect to the protocol presented in Section 3, the servers should verify two things: 1. A reader is allowed to write back a value only if it proves that it received the value from a correct server (i.e. by having f + 1 servers vouch for the value). 2. A reader is allowed to perform a second round write back of the timestamp only after f + 1 correct servers have accepted the first round write back message (i.e. 2f + 1 servers accepted the first round write back). With public key cryptography, these proofs can be easily implemented. A server signs messages it sends to a reader and a reader can provide as proof the requisite number of signed messages. An important observation is that these (signed) messages indicate the progress in terms of the server state and are not specific to the particular read operation. The protocol remains correct even if these proofs are put together from signed messages sent by servers in response to different read operations. We present a secret-sharing-based approach that can be used to implement these types of proofs. This shows that the (strong) assumption of computationally one-way functions, used by PKIs, is not required for these applications. We believe that our approach can be used not just with the protocol presented in Section 3, but also with other protocols that have a similar structure--however, characterizing such protocols and developing a general framework to replace cryptographic-based techniques with our techniques are left for future work. 4.1 Provably Correct Proofs Using Secret Sharing

Consider the read and write protocols in Figure 5, which are typically part of larger protocols.

Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage
Write(value, ts) send (value, ts) to all wait for n - f acks Read(ts) send wait send wait

15

READ REQUEST to all for t replies with same value v and timestamp ts v to all for n - f acks

Fig. 5. Simple client protocols

In the protocol, the reader is attempting to read a value whose timestamp is ts and the writer writes a value whose timestamp is ts (not necessarily the same). The server code corresponding to these two protocols is the obvious one. As presented, the reader protocol is not guaranteed to terminate and typically, it will be part of a larger protocol that ensures termination. We will not concern ourselves with termination in the remainder of this section. We would like to transform the two protocols so that correct readers are not affected and faulty readers cannot write back a value that was not written by the writer. We achieve the transformation by splitting the write operation into two parts. In a first part, the writer sends the value to be written along with some other information that we will explain shortly. In the second part of a write operation, the writer sends a message indicating that the first part has finished. The servers process the message with the values, exactly as in the simple protocol, but only when it receives the FINISHED SENDING VALUES message. In other words, the values received in the first part are hidden and are not processed (or sent to readers) unless the server knows that the writer finishes the first part. This knowledge can be obtained directly from the writer or indirectly from the reader. So, we need a way for a reader that received t identical values to convince a correct server that only received a first message from the writer to open the value that the server received from the writer. By doing this, the reader is effectively writing back a value but without having to sign the value. The write back consists of making a hidden value non-hidden. A reader knows that it can write back to all correct servers if it knows that the writer finished the first part and started the second part at some correct server because, in that case, all correct servers will eventually receive the value from the writer which is sent in the first part. So, the question is how can we provide a proof that the writer made enough progress. This is where the other information enter the picture. The main idea is to have the writer associate a randomly generated secret with the value it wants to write. The writer generates this secret, creates shares, and sends these shares to servers before (roughly speaking) starting the actual write operation. A server which knows that the writer has completed the first part and started the second part, is willing to provide the value as well as shares of the secret. The secret is shared such that if (and only if) enough messages are received, the reader will be able to reconstruct the secret. Thus, reconstructing the secret can be used as a proof that the writer made enough progress in its write operation. The details of the secret sharing scheme are given below. The modified writer and reader protocols are shown in Figure 6. Each secret is split using the techniques of [11] such that k.t shares are required to reconstruct the secret, where k > f . Each server is given k + 1

16

A.S. Aiyer, L. Alvisi, and R.A. Bazzi

Write(value, ts) Read(ts) generate secret s send READ REQUEST to all 1  i  n generate shares si [1 . . . k + 1] value read =  1  i  n send (value, ts, si [1 . . . k + 1]) to serveri repeat send FINISHED SENDING VALUES to all receive message (v, ts, si [1 . . . k]) from server i if received  t messages with the same v and ts wait for n - f replies fork { send (v, collectedSharesFor(v)) to all wait for n - f acks value read = v } until (value read != )

Fig. 6. Modified client protocols

shares, along with the message that was going to be sent in the simple protocol. After sending these messages to all the servers, the writer sends the FINISHED SENDING VALUES message. If a read request reaches a server after the server received the FINISHED SENDING VALUES message from the writer, the server sends the value to the reader along with k of the shares the server received from the writer. One share is never revealed and is used by servers only for verifying that a reconstructed secret is correct. Secret sharing ensures that the secret can be reconstructed if and only if t number of servers give away their shares. The server now accepts a write back from a reader only if the reader can provide enough shares that can reconstruct the correct secret at the server. The server can accept a write back even before receiving the FINISHED SENDING VALUES message. If the server decides to accept the write back message, the server unhides the message and acts as if it has received a FINISHED SENDING VALUES message directly from the writer. Note that unlike the case with cryptographic solution in which the reader could determine locally whether the received signatures are valid, in our protocol the reader needs the cooperation of the servers in order to determine if the received shares can reconstruct the correct secret. Here, the reader cannot determine if the proof is going to be valid because it does not have any shares to verify against. This may cause the write backs to not succeed if there are not enough correct shares. Also, the shares that enable the reader to write back might not arrive all at once. The reader might first receive n - f replies that do not include the crucial shares of some slow correct servers. So, the reader's protocol would have the reader request verification from servers every time the reader receives a new share. The reader can finish when it has received enough identical values and it knows that it can write back.

5

Protocol Tolerating Byzantine Readers

To convert the protocol presented in Section 3 to a Byzantine reader tolerant protocol, the writer has to perform an extra phase. This extra phase contains the secrets required for both the phases of the write protocol. So phase 1 and Phase 2 of the original protocol are replaced with (Phase 1' || Phase 2'), Phase 1", and Phase 2" where the primes and double primes are used to indicate the transformed phases and the || indicates that the first transformed phases are executed concurrently.

Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage

17

5.1

The Write Protocol

Before beginning the two phases of the write, the writer generates two random secrets. The writer sends the shares for these secrets, along with the value and timestamp information that it is going to write, to all the servers before initiating the write phases. The first secret is used to prove that the reader has received f + 1 identical values and is split such that t = f + 1. The second secret is used to prove that the reader has received (n - f ) acknowledgements to the first phase write back and is split such that t = (n - f ). On receiving these shares and information regarding the value and timestamp that is going to be written, the servers hold them separately and do not update any values or timestamps that are used in the original protocol. After sending these shares and values to all the servers, the writer begins the original write protocol, asking the servers to update the value and then the timestamp. Only on receiving the message from writer to update the value, or on accepting a write back message, will a server update its value and reveal its secret shares. The same holds true for updating the timestamp. 5.2 The Read Protocol

The read protocol is similar to the original read protocol in Section 3. The only difference being that the reader needs to include the collected shares as a proof to be allowed to perform the write back. As in Section 3, the reader waits to collect f + 1 matching responses for an acceptable timestamp. It then tries to write back the value providing as a proof the set of shares collected so far. Retrying each time it receives more shares, or when it can try to write back a different value. The server will only accept the write back if value being written back matches the value that was initially received from the writer, and the shares can reconstruct the correct secret. If at least one non-faulty server has revealed the value, then all servers will eventually receive the value and the shares sent by the writer to be able to verify the information provided by the reader. Thus valid write backs from correct readers will be eventually accepted. Moreover, since the servers receive the value from the writer directly, the reader need not send the value along with the write back. It is sufficient to use a lightweight tag, or the timestamp to identify the write [5]. Thus, preventing the servers from having to process large messages from bad readers. For simplicity, we assume that the protocol does not have this optimization. On accepting the first write back for the value, the server responds with its shares of the second secret. On receiving (n - f ) of these responses, the reader proceeds to write back the timestamp in the second phase sending the shares it received in these responses as proof that (n - f ) servers have accepted the first phase write back. The reader retries writing back whenever it receives additional shares. When all the correct servers accept the write back value and respond with their shares of the second secret, the reader will have enough correct shares to reconstruct the secret correctly and complete the write back.

18

A.S. Aiyer, L. Alvisi, and R.A. Bazzi

Late write backs. One complication is that if a first round write back arrives late at a server, the server might not have the shares to give the reader because the old shares might have been replaced with newer ones due to subsequent writes. If a server that receives a write back message has a current timestamp that is larger than the timestamp being written back, it simply sends a write back acknowledgment, but without the shares (sending  for the shares). The meaning of a write back without shares is that the writer has started the second phase of the write of a value with a higher timestamp. When the reader finishes its first round of write back it will collect (n - f ) acknowledgments, some with shares and some without shares, and send these along with its second phase write back. If one of the acknowledgments without shares is from a correct server, then this means that the writer must have started writing a new value and finished the second phase of the write operation for which the reader is sending a second phase write back, and therefore all correct servers will eventually receive the second phase message from the writer and can accept the write back. If none of the acknowledgments without secrets is from a correct server, then the reader will eventually receive either enough secrets (as we argued in the previous paragraph) or one acknowledgment without secrets from a correct server; in either case, the correct reader will be able to finish its second phase write back. Bounding number of retries. The reader only tries to write back values that have been received from at least f + 1 different servers. Since the reader queries the servers for values up to f + 1 times and gets up to 2 values from each server in addition to 3 values in the forwarded message, the reader can get up to 2(f + 1) + 3 different values from each server. Thus a correct reader may f +5) different values. Also, since each only retry writing back a maximum of n(2 f +1 value will only be retried f + 1 times, the number of messages exchanged due to the retries is still bounded. Acknowledgements. We thank Allen Clement and Harry Li for their helpful discussions on secret verification.

References
1. Aiyer, A., Alvisi, L., Bazzi, R.A.: Bounded Wait-free Implementation of Optimally Resilient Byzantine Storage without (Unproven) Cryptographic Assumptions. Technical Report TR-07-32, University of Texas at Austin, Department of Computer Sciences (July 2007) 2. Bazzi, R.A., Ding, Y.: Non-skipping timestamps for byzantine data storage systems. In: Guerraoui, R. (ed.) DISC 2004. LNCS, vol. 3274, pp. 405­419. Springer, Heidelberg (2004) 3. Bazzi, R.A., Ding, Y.: Bounded wait-free f-resilient atomic byzantine data storage systems for an unbounded number of clients. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 299­313. Springer, Heidelberg (2006) 4. Cachin, C., Tessaro, S.: Optimal resilience for erasure-coded byzantine distributed storage. In: DSN, Washington, DC, USA, pp. 115­124. IEEE Computer Society Press, Los Alamitos (2006)

Bounded Wait-Free Implementation of Optimally Resilient Byzantine Storage

19

5. Fan, R., Lynch, N.: Efficient replication of large data objects. In: Fich, F.E. (ed.) DISC 2003. LNCS, vol. 2848, pp. 75­91. Springer, Heidelberg (2003) 6. Guerraoui, R., Vukolic, M.: Refined Quorum Systems. Technical Report LPDREPORT-2007-001, EPFL (2007) 7. Herlihy, M.: Wait-free synchronization. ACM Transactions on Programming Languages and Systems 13(1), 124­149 (1991) 8. Lamport, L.: On interprocess communication. part i: Basic formalism. Distributed Computing 1(2), 77­101 (1986) 9. Liskov, B., Rodrigues, R.: Byzantine clients rendered harmless. In: Fraigniaud, P. (ed.) DISC 2005. LNCS, vol. 3724, pp. 311­325. Springer, Heidelberg (2005) 10. Martin, J.-P., Alvisi, L., Dahlin, M.: Minimal byzantine storage. In: Malkhi, D. (ed.) DISC 2002. LNCS, vol. 2508, pp. 311­325. Springer, Heidelberg (2002) 11. Tompa, M., Woll, H.: How to share a secret with cheaters. J. Cryptol. 1(2), 133­138 (1988)

A Simple Population Protocol for Fast Robust Approximate Majority
Dana Angluin1, James Aspnes1 , , and David Eisenstat2 ,
Yale University, Department of Computer Science {dana.angluin,james.aspnes}@yale.edu 2 Princeton University, Department of Computer Science deisenst@cs.princeton.edu
1

Abstract. We describe and analyze a 3-state one-way population protocol for approximate majority in the model in which pairs of agents are drawn uniformly at random to interact. Given an initial configuration of x's, y 's and blanks that contains at least one non-blank, the goal is for the agents to reach consensus on one of the values x or y . Additionally, the value chosen should be the majority non-blank initial value, provided it exceeds the minority by a sufficient margin. We prove that with high probability n agents reach consensus in O(n log n) interactions and the value chosen is the majority provided that its initial margin is at least  ( n log n). This protocol has the additional property of tolerating  Byzantine behavior in o( n) of the agents, making it the first known population protocol that tolerates Byzantine agents. Turning to the register machine construction from [2], we apply the 3-state approximate majority protocol and other techniques to speed up the per-step parallel time overhead of the simulation from O(log 4 n) to O(log 2 n). To increase the robustness of the phase clock at the heart of the register machine, we describe a consensus version of the phase clock and present encouraging simulation results; its analysis remains an open problem.

1 Introduction
Population protocols [1] model distributed systems in which individual agents are extremely limited, in fact finite-state, and complex behavior of the system as a whole emerges from the rules governing pairwise interaction of the agents. Such models have been defined and used in other fields, for example, statistics, epidemiology, physics and chemistry; understanding their behavior is a fundamental scientific problem. The new perspective we bring as computer scientists is to ask what computational behaviors these systems can exhibit. In addition to fundamental scientific knowledge, answers may provide novel designs for distributed computational systems at many scales. Chemists have defined a standard model of small molecules in a well-mixed solution, in which the molecules are agents, the state of an agent represents the chemical species of the molecule, and interaction rules specify the probable products of a collision between two molecules; the sequence of collisions is determined by uniform
Supported in part by NSF grant CNS-0435201. Supported in part by a Gordon Y. S. Wu Fellowship and a National Defense Science and Engineering Graduate Fellowship.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 20­32, 2007. c Springer-Verlag Berlin Heidelberg 2007

A Simple Population Protocol for Fast Robust Approximate Majority

21

random draws of a pair of agents to interact [4, 5]. In [1] it is shown that this model in principle permits the design of a "computer in a beaker," that is, we can design interaction rules that allow a population of n molecules to simulate the behavior of a register machine with a constant number of registers holding numbers of magnitude O(n) for poly (n) steps with error probability 1/ poly (n) in parallel time that is a factor of poly (n) larger than the number of simulated instructions. In [2] we have shown that a careful analysis of the properties of epidemics permits us to design a much more efficient simulation, in which the per-step slowdown factor is O(log4 n) parallel time. The register machine of [2] has several shortcomings: it requires an initial configuration with a designated leader, it does not tolerate faults, and O(log4 n) is still fairly slow. One goal of this paper is to develop more tools for the design of fast robust population protocols, with improvement of the register machine as a critical testbed. The main tool we develop is our 3-state protocol for approximate majority; this is a protocol that rapidly takes a configuration of x's, y 's and b's to a configuration that is all x's or all y 's. The final value represents the majority value (among x's and y 's) in the initial configuration, provided it exceeds the minority value  by a sufficient margin, namely   ( n log n). Moreover, we show that is robust to o( n) Byzantine agents, the first population protocol that provably tolerates any Byzantine agents. With a sufficiently redundant representation of register values, this protocol gives a fast comparison operation, which, when combined with other techniques, reduces the slowdown factor of the simulation to O(log2 n) parallel time. Though the approximate majority protocol has only 3 states, its analysis is nontrivial; we expect that the protocol and its analysis will find other applications in the design of fast robust population protocols. The register machine of [2] has at its heart the construction of a phase clock that causes the agents to move together rapidly through a fixed cycle of phases; together meaning that no two agents are more than a very few phases apart, and rapidly meaning the parallel time to complete a cycle is O(log n). We would like to avoid the need for a designated leader or leaders in the initial configuration to synchronize the phase clock, and we would like the phase clock to be robust. In Section 8 we describe an apparently robust consensus variant of the phase clock, and a protocol to start it from a uniform initial configuration. We give simulation results suggesting that it performs well. However, the problem of analyzing it formally remains open; more tools are necessary.

2 Model
A population protocol consists of a finite set of states Q, a finite set of input symbols X  Q, a finite set of output symbols Y , an output function  : Q  Y , and a joint transition function  : Q × Q  Q × Q. A population protocol is executed by a fixed finite population of agents with states in Q. For convenience, we assume that each agent has an identity v  V , but agents do not know their own identities or others'. Initially, each agent is assigned a state according to an input x : V  X that maps agent identities to input symbols. In the general population protocol model, there is an interaction graph, a directed graph G = (V, A) without self-loops, whose arcs indicate the possible agent interactions that may take place. (G is directed because we assume that interacting agents are able to break symmetry.) In this paper, G will always be a complete graph.

22

D. Angluin, J. Aspnes, and D. Eisenstat

During each execution step, an arc (v, w) is chosen uniformly at random from A. The "source" agent v is the initiator, and the "sink" agent w is the responder. These agents update their states jointly according to  : if v is in state qv and w is in state qw , the state of v becomes 1 (qv , qw ), the state of w becomes 2 (qv , qw ), where i gives the ith coordinate of the output of  . The states of all other agents are unchanged. For any given V , a population protocol computes a (possibly partial) function f : X V  Y in steps with error probability if for all x  f -1 (Y ), the configuration c : V  Q after steps satisfies the following properties with probability 1 - . ­ All agents agree on the correct output: for all v  V , f (x) =  (c(v )). ­ This remains true with probability 1 in the future. We are interested in the guarantees one can make about a fixed protocol over a family of functions f defined for all finite populations. Although we have described the population protocol model in a sequential light, in which each step is a single pairwise interaction, interactions between pairs involving different agents are independent and may be thought of as occurring in parallel. In measuring the speed of population protocols, then, we define 1 unit of parallel time to be |V | steps. The rationale is that in expectation, each agent initiates 1 interaction per parallel time unit; this corresponds to the chemists' idealized assumption of a wellmixed solution. 2.1 Byzantine Agents We extend the basic randomized population protocol model described above to allow Byzantine behavior from some of the agents. In addition to the n normal agents we allow a population to include z Byzantine agents. For each interaction, an ordered pair of agents is selected uniformly at random from the population of normal and Byzantine agents. A Byzantine agent may simulate any normal agent state in an interaction, and its choice of state may depend on both the global configuration and the identity of the specific agent it encounters. The state of Byzantine agents is not meaningful and so is not included in the description of a configuration. We first describe our protocol and analyze its behavior without Byzantine agents.

3 A 3-State Approximate Majority Protocol
We analyze the behavior of the following population protocol with states Q = {b, x, y }. The state b is the blank state. Row labels give the initiator's state and column labels the responder's state. x b y x (x, x) (x, x) (x, b) b (b, x) (b, b) (b, y ) y (y, b) (y, y ) (y, y ) Note that this protocol is one-way: every interaction changes at most the responder's state; thus it can be implemented with one-way communication. Only the interactions

A Simple Population Protocol for Fast Robust Approximate Majority

23

xb, yb, xy , and yx change the responder's state; we may think of these as the only interactions that consume energy. The blank configuration of all b's is stable, but cannot be reached from any non-blank configuration because no interaction can eliminate the last x or y . The configurations of all x's and all y 's are stable, and every non-blank configuration can reach at least one of them. An intuitive description of the process is that agents in state b are undecided, while initiators in states x and y are attempting to convert responders that they meet to adopt their respective states. Such an initiator immediately converts an undecided responder, but only succeeds in reducing an opposing responder to undecided status. The process may also be thought of as two competing epidemics, x's and y 's, with the ability to reverse each other's progress. In Sections 4 and 5, we show that with high probability this protocol (a) converges from any non-blank configuration to a stable configuration in O(n log  n) interactions; and (b) correctly computes the initial majority x or y value provided  ( n log n) more agents carry this value in the starting configuration than carry the opposing value. In  Section 6, we show that it can tolerate o( n) Byzantine agents; the formal definition of this property is given there.

4 Convergence
We show that, from any non-blank initial configuration, the 3-state approximate majority protocol converges to either all x tokens or all y tokens within O(n log n) interactions with high probability. We divide the space of non-blank configurations into four regions: three corners, where most tokens are b, x, or y , and a central region where the tokens are more evenly balanced. We show that the number of interactions in each region is bounded by O(n log n) w.h.p., by constructing a family of supermartingales of the form M = eaS/n f (x, y ) where a > 0 is a constant, S counts the number of interactions of a particular type and f is a potential function defined across the entire space of configurations. (We overload x, y and b to denote the number of each token in a configuration.) Specifically, we let  be the stopping time at which the protocol converges, and let  = min( , kn log n) for some fixed k . Assuming f does not vary too much over the space of configurations, we can use the supermartingale property E[M ]  M0 to show that eaS /n is small, and then use Markov's inequality to get the bound on S . Summing the bounds for each region then gives the total bound on the number of interactions. Though it would seem that truncating at time kn log n assumes what we are trying to prove, in fact we show that with high probability the total number of interactions is much less than kn log n, implying that we do in fact converge by the given time bound. The resulting proof requires a careful selection of f . To keep the argument at least locally simple, we construct separate potential functions to bound different classes of operations, based on the type of interaction that occurs and which region of the configuration space it occurs in. The reason for this classification is that the behavior of the protocol is qualitatively different in different regions of the configuration space. When most tokens are blank, the protocol acts like an epidemic, with non-blank tokens rapidly

24

D. Angluin, J. Aspnes, and D. Eisenstat

infecting blank tokens. When most tokens carry the same non-blank value, the protocol acts like coupon collector, with the limit on convergence being the time for the few remaining minority tokens to be converted to the majority value. In the central region, where no token type predominates, the protocol acts like a random walk with increasing bias away from the center. Unfortunately, in none of these configurations does the protocol act enough like the analogous well-known stochastic processes to permit a direct reduction to previous results, and the behavior in border areas blends smoothly between one form and another. The supermartingale/potential function approach allows separate arguments designed for each region to be blended smoothly together. Unfortunately, this still requires considerable calculation to verify that each potential function does what it is supposed to. In this extended abstract, the detailed calculations are omitted for reasons of space. The reader may be surprised to find that such a simple protocol requires such a lengthy proof. Despite substantial efforts, we were unable to apply more powerful tools to this problem. Part of the reason is that we are trying to obtain exact asymptotic bounds on a system much of whose interesting behavior occurs when particular tokens are very rare or when the behavior of the protocol is highly random (e.g., with evenly balanced numbers of x and y tokens); this (together with the fact that the corresponding systems of differential equations do not have closed-form solutions) appears to rule out arguments based on classical techniques involving reduction to a continuous process in the limit (e.g., [6,7]). Similarly, approaches based on direct computation of hitting times or eigenvalues of the resulting Markov chain would appear to require substantially more work than a direct potential function argument. It is possible that such difficulties are an inherent property of randomized population protocols. The ability to construct register machines using such protocols [1,2] suggests that analysis of an arbitrary protocol for arbitrarily large populations quickly enters the realm of undecidability. But we cannot rule out the possibility that a more sophisticated approach might give an easier proof of the convergence rate for the particular protocols we are interested in. Our results are stated using explicit constant factors. The reader should be warned that in many cases these are gross overestimates, and that from simulation we observe that the expected number of interactions to convergence seems to be less than 4n log n from two challenging initial configurations (see Figure 1.) The first of these, an initial population evenly divided between x and y with no blank tokens, can be shown numerically for reasonably small n to be the configuration that maximizes expected convergence time. The full convergence bound is stated below. Theorem 1. Let  be the time at which x = n or y = n first holds. Then for any fixed c > 0 and sufficiently large n, Pr [  6754n log n + 6759cn log n]  5n-c . Proof. We give here a brief sketch only. First, we show that the potential function f = 1 (x-y )2 +2n is reduced by -(1/n) of its previous value on average conditioned on an xb or a yb interaction, and that it rises by a smaller relative amount conditioned on an xy or a yx interaction. It follows that f · eO(S -T )/n is a supermartingale, where S

A Simple Population Protocol for Fast Robust Approximate Majority

25

Starting configuration: half Xs, half Ys 50 Steps per agent 40 30 20 10 100 1000 10000 Population size 100000 Parallel time to reach consensus 3.03 * log(x) 50 Steps per agent 40 30 20 10 100

Starting configuration: one X, one Y Parallel time to reach consensus 2.55 * log(x)

1000 10000 Population size

100000

Fig. 1. Simulation results: parallel time of approximate majority from two initial conditions

counts xy and yx interactions, T counts xb and yb interactions, and  < 1. The factor of O(n) drop in f between the maximum initial f = 1/(2n) and final f = 1/(n2 + 2n) allows only a similar expected rise in e(S -T )/n and thus (with high probability) only an O(n log n) rise in S - T . Since all but at most n - 1 initial blank tokens destroyed in T must have previously been created in S , T  S + n, giving an O(n log n) bound on, in turn: (a) S alone; (b) S + T ; (c) all interactions in the central region (where xy , yx, xb, or yb interactions are likely). Separate potential functions are used to bound the remaining interactions in the corners.

5 Correctness of Approximate Majority
Not only does the 3-state protocol converge quickly, but it also converges to the dominant non-blank value in its input if there is a large enough initial majority. Theorem 2. With high probability, the 3-state approximate majority protocol converges to the initial majority value  if the difference between the initial majority and initial minority populations is  ( n log n). Proof. Without loss of generality, assume that the initial majority value is x. We consider a coupled process (ut , ut ) where ut = (xt - yt ) and ut is the sum of a series of fair ±1 coin flips. Initially u0 = u0 . Later values of ut are specified by giving a joint distribution on (u, u ). We do so as follows. Let p be the probability that u = 1 and q the probability that u = -1. Then let  (0, 0) with probability 1 - p - q,    (1, 1) with probability 1 2 (p + q ), (u, u ) =  (1 , - 1) with probability p -1  2 (p + q ),   (-1, -1) with probability q. The probability in the third case is non-negative if p/(p + q ) = Pr[u = 1|u = 0]  1 2 . This holds as long as u  0; should u ever drop to zero, we end the process. Observe that unless this event happens, we have ut  ut . We can also verify by summing the cases that u rises with probability exactly p and drops with probability

26

D. Angluin, J. Aspnes, and D. Eisenstat

exactly q ; and that u rises or drops with equal probability 1 2 (p + q ). So we have E[u ] = 0 and that |u |  1, the preconditions for Azuma's inequality. Theorem 1 shows that the process converges before O(n log n) interactions with high probability. Suppose the process converges at some time  = O(n log n). Then  | = O( n log n) throughout this interval by Azuma's inequality we have that |u - u0 with high probability. So if u0 = u0 =  ( n log n), it follows that u0  u0  0 throughout the execution, and in particular that the process does not terminate before convergence and that u is non-negative at convergence. But this excludes the y = n case, so the process converges to the initial majority value.

6 Tolerating Byzantine Agents
In this section, we show that the 3-state approximate majority protocol can tolerate up to o( n) Byzantine agents, computing the correct majority value in O(n log n) time with high probability despite their interference. However, to do so we must both assume a somewhat larger initial majority, and slightly relax the criterion for convergence. The issue with convergence is that Byzantine agents can always pull the normal agents out of a converged configuration. For example, if all normal agents are in the x state, any encounter with a Byzantine initiator can shift the normal agent to a b state, and a second encounter can shift it to a y state, even though there are no normal y agents in the population. So we must accept a small number of normal agents that do not have the correct value. But in fact the situation is worse: if we run long enough, there exists a trajectory with nonzero probability that takes us to the blank configuration, which is stable. So we must also accept a small probability that we reach the blank configuration quickly, and the assurance that we reach it with probability 1 after a very long time. However, we can show that with high probability neither outcome occurs within a polynomial number of steps. Our technique is to adjust the potential functions used by the non-Byzantine process to account for Byzantine transitions. We then use these adjusted potential functions to show that (a) strong pressure exists to keep the process out of the high-b corner and in the high-x and high-y corners, and (b) the number of interactions (including Byzantine interactions) to reach the x or y corner is still small. 6.1 Biased-Walk Barriers Let us begin by showing that it is difficult even for Byzantine agents to force the protocol into a configuration with a low value of vt = xt + yt . Observe that if the Byzantine agents attempt to minimize v , v nonetheless increases at each interaction with likelihood proportional to vb and decreases with likelihood proportional to 2xy + zv . So the probability of an increase conditioned on any change in v is vb/(vb + 2xy + zv )  vb/(vb + v 2 /2 + zv ) = b/(b + z + v/2)  b/n provided z  v/2. For large b and small z this gives a random-walk behavior that is strongly biased upwards.   v/2, so Pr[v = Suppose n  v  n/8. Then b  (7/8)n and z = o( n) 1|v = 0]  7/8. We wish to bound the probability starting from some initial v0

A Simple Population Protocol for Fast Robust Approximate Majority

27

 in this range that v reaches n before it reaches n/8. Though the probability that v rises or falls changes over the interval, the position of v can be lower-bounded by the position of a coupled variable v that moves according to a biased random walk with fixed probability p = 7/8 of increasing by 1 and q = 1/8 of decreasing by 1. From the standard analysis of the gambler's ruin problem,1 we have that (q/p)vt is a martingale, and thus that the quantity Pr[v reaches
   n before n/8](q/p) n + Pr[v reaches n/8 before n](q/p)n/8

is equal to (q/p)v0 . Because (q/p)n/8 = (1/7)n/8 is exponentially small, it makes sense to ignore the second addend, leaving Pr[v reaches or
  n before n/8](q/p) n < (q/p)v0

  Pr[v reaches n before n/8] < (q/p)v0 - n .   n before It follows that if v0  n + c log7 n, then the probability that v drops to  reaching n/8 is bounded by n-c . Once v reaches n/8, further drops to n become exponentially improbable even conditioned on starting at v = n/8 - 1. We thus have:

  n) and let v0  n + c log7 n. Then for sufficiently Lemma 1. Fix c > 0. Let z = o(  large n, the probability that vt  n for any t < en/8 n-c is less than 2n-c .  Proof. The probability that v reaches n before reaching n/8 for the first time is at most n-c . For , there is a probability of at most  each subsequent drop to n/8 - 1  (1/7)n/8-1- n  exp(-n/8)) that v reaches n before returning to n/8. Since each such excursion below n/8 involves at least one interaction, en/8 n-c interactions   gives at most an expected n-c drops to n for a total probability of reaching v = n -c bounded by 2n . We can apply a similar analysis to the x and y corners, but here the protocol drifts toward the all-x or all-y configuration instead of away from it. Here we track 3y + b for the x corner and 3x + b for the y corner. Because these functions can change by more than just ±1, the simple random walk analysis becomes more difficult. Instead, we proceed by showing that is a supermartingale, and bound the probability  exp(3y + b)  of moving from 2 n to 3 n by exp(- n), the inverse of the change in exp 3y + b. Formally, we have:   Lemma 2. Fix c > 0. Let z = o( n) and 2 n. Then for sufficiently  let 3y0 + b0  large n, the probability that 3yt + bt  3 n for any t < e n-1 n-c is less than n-c . Proof. (Omitted for reasons of space.)
1

See, for example, [3, §XIV.2].

28

D. Angluin, J. Aspnes, and D. Eisenstat

6.2 Convergence Time with Byzantine Agents The convergence time is given in the following theorem.    Theorem 3. Let  be the time at which x  n - n, y  n - n, or v  n first holds. Let v0 be the initial  number of x's and y 's. Then for any fixed c > 0 and sufficiently large n, if v0  n + c log7 n, then  Pr   6754n log n + 6759cn log n or v  n = n-c+o(1) . Proof. (Proof omitted for reasons of space.) Note that once we are in the x or y corner, Lemma 2 tells us that we remain there with high probability for exponential time. So we have a complete  characterization of the convergence behavior of the 3-state majority protocol with o( n) Byzantine agents.  It is also not hard to see that the proof of Theorem 2 also continues to hold for z = o ( n),  provided we increase the size of the initial majority to  ( n log n ) to compensate for  the offset of o( n log n) generated by Byzantine interactions.

7 Speeding Up the Register Machine Construction
In this section we show how to use the 3-state approximate majority protocol and other techniques to speed up the register machine construction in [2] so that it has per-step parallel time overhead of O(log2 n) instead of O(log4 n). The original construction is based on a single agent representing a finite-state controller operating via commands spread by epidemics on register values represented in unary by tokens scattered across the population. A major bottleneck in [2] is the difficulty of carrying out exact comparisons (performed in O(log2 n) time using O(log n) rounds that alternate cancellation with amplification) and of performing subtractions (done in O(log3 n) using addition, comparison, and binary search). Our approximate majority protocol gives a simpler and faster implementation of comparison, provided we pad out the register values to avoid near-ties. A further adjustment to the representation gives cheap subtraction. Because space does not permit us to repeat the description of the original construction here, we refer the reader to [2] for details. We begin by replacing the original O(log2 n) parallel time comparison operation by our new O(log n) parallel time approximate majority protocol. To ensure that a comparison is correct with high probability, we need to ensure that the register values being compared differ by  ( n log n). We guarantee this by having registers hold values that are multiples of n2/3 ; three such registers are sufficient to represent n = (n1/3 )3 different values, thinking of them as the high, middle and low order wide-digits of a number in base n1/3 . Thus, to compare two wide digits, say A and B , we do an approximate majority comparison of A + (1/2)n2/3 with B ; if the result is that A is in the majority, then we conclude that B  A, otherwise that A < B . To compare two registers composed of O(1) wide-digits it suffices to proceed digit by digit. The subtraction operation of [2] requires O(log n) rounds of binary search where the O(log2 n) parallel time comparison operation dominates the cost of each round. Though we could replace these comparisons with our faster comparison operation and

A Simple Population Protocol for Fast Robust Approximate Majority

29

reduce the cost of subtraction to O(log2 n), we can obtain a still better reduction to O(log n) parallel time by use the logician's construction of the integers from the natural numbers: the value A in a register is represented by the difference A+ - A- of values in two different registers. To compute C  A + B , we compute C+  A+ + B+ and C-  A- + B- . To compute C  A - B , we compute C+  A+ + B- and C-  A- + B+ . These operations both take parallel time O(log n), because addition is already O(log n) in the previous construction. An additional clock cycle of cancellation keeps the + and - components from overflowing. For registers with this balanced representation, we must revisit comparison. To compare A with B , we compare (A+ + B- ) with (A- + B+ ). Since these differ by a multiple of n2/3 , our previous comparison method works. The result is that subtraction can be done with O(1) additions and comparisons, which gives parallel time of O(log n). The most expensive operation in [2] is division by a constant, which is based on O(log n) rounds of binary search in which subtraction dominates. The improved cost of subtraction immediately reduces the parallel time for division to O(log2 n) without any change to the division algorithm. The remaining issue is how to convert the input values in the registers, which are represented in simple unary, into the wide-digits representation. We use the previous machine operations to create a reference value of magnitude (n2/3 ) in a register and the usual base-conversion algorithms to extract the wide digits of each input register value and store them multiplied by the reference value; this initialization takes polylogarithmic parallel time [2], after which the per-step overhead of simulating the register machine is O(log2 n). Thus, for simulating the register machine specified in [2], we have the following improvement. Theorem 4. A probabilistic population of n agents with a designated leader can simulate the steps of the virtual register machine defined in [2], such that the probability that any single step in the simulation fails or takes more than O(n log2 n) interactions can be made O(n-c ) for any fixed c.

8 A More Robust Phase Clock?
The fault tolerance of the 3-state approximate majority protocol and the inherent redundancy of the wide-digit representation of register values is encouraging: perhaps there is a fast and provably robust version of the register machine construction. However, there is a second component of our register machine that must be made more robust. This is the phase clock, a subprotocol used to count off intervals of (log n) parallel time so that the leader can estimate when an epidemic has finished propagating. The phase clock of [2] is a protocol with m states and one designated leader. The state of an agent represents the phase that it is in. A responder in phase i adopts the phase of any initiator in phases i + 1 mod m through i + m/2 mod m, but ignores initiators in other phases. New phases are triggered by the leader agent. When the leader responds to an initiator in its own phase, that leader moves to the next phase. Counting the number of interactions between the event that the leader enters phase 0 until the event that the leader next enters phase 0 as a round, it is shown that with high probability each of

30

D. Angluin, J. Aspnes, and D. Eisenstat

a polynomial number of rounds takes parallel time (log n) with inverse polynomial probability of error. Note that although this protocol also works when given (n1- ) designated leaders, it requires an initial configuration with the appropriate number of designated leaders, and is not robust to errors. We now describe (a) a method for quickly starting the phase clock from a uniform initial state, and (b) a consensus variant of the phase clock that appears to be more robust. Our present techniques are not sufficient to analyze the resulting algorithm, so we must make do with simulation results. Simulation results suggest that the following protocol can start up a phase clock with high probability in (log n) parallel time. This protocol is one-way, so for brevity we identify  with 2 . This protocol is the semidirect product of several components. The first component allows us to make approximate coin tosses. The states are Qcoin = {0, 1}, with initial value xcoin = 0, and the transition function is coin (q, q ) = 1 - coin (q ). Starting from any configuration, this protocol rapidly converges towards an equal proportion of agents in each state. The second component counts the number of consecutive coin values equal to 1 the agent has seen immediately prior, up to a maximum of > 0. The states are Qcount = {0, 1, . . . , }, xcount = 0, and the transition function count (q, q ) = coin (q )(min{count (q ) + 1, }). The third component approximates an exponential decay process. There is a parameter k1  . The states are Qdecay1 = {0, 1}, xdecay1 = 1, and the update function is decay1 (q, q ) = [count (q ) < k1 ]decay1 (q ). The idea is that for all 0 <  < 1 and 0 < c, we can find k1 such that with high probability, there is a period of cn log n steps where the number of agents with decay1 value of 1 is between 1 and n . In this period, using agents with decay1 value of 1 as temporary leaders, we can run a disposable phase clock that functions correctly only for a constant number of phases before all the values of decay1 become 0. This phase clock is used to choose a stable leader population of size (n1- ), which in turn supports a second copy of the phase clock that runs for polynomially many steps. Our consensus variant of the phase clock from [2] works as follows. In addition to the phases 0, 1, . . . ,  - 1, we have a blank "phase". Thus Qphase1 = {b, 0, 1, . . . ,  - 1}. xphase1 = 0. If x is a nonblank phase, then let succ(x) = (x +1) mod  be the successor phase of x. We have  p if p = b      p if p = b      p if p = p = b and decay1 (q ) = 0  phase1 (q, q ) = succ(p ) if p = p = b and decay1 (q ) = 1   p if p, p = b and p = succ(p)      p if p, p = b and succ(p ) = p    b otherwise, where p = phase1 (q ) and p = phase1 (q ). If the phase of the initiator is blank or one behind the responder's, the responder's phase is unchanged. If the phase of the responder is blank, it copies the phase of the initiator. If the phases are non-blank and

A Simple Population Protocol for Fast Robust Approximate Majority

31

equal, the responder increments its phase if and only if the initiator has decay value 1 (temporary leader status.) If the initiator's phase is one more than the responder's, the responder increments its phase. In all other cases, the responder sets its phase to blank. In summary, we are following a multiple-valued generalization of the 3-state majority algorithm except when the phases are nonblank and within distance 1 of one another. In this case, we revert to behavior like that of the original phase clock. Once the disposable phase clock is running, it is used to select the real phase clock's leaders. This is accomplished by having another exponential decay process that is reset by the disposable phase clock each complete cycle. Thus we need a way to detect approximately the onset of each cycle. Our criterion is for each agent to keep a local "maximum" of the phases it has been in, and perform the reset when this maximum wraps around. Formally, Qmax = {0, 1, . . . ,  - 1}, xmax = 0, and max (q, q ) = p if p = b and (p - max (q )) mod   /2 max (q ) otherwise,

where p = phase1 (q, q ). Since the last cycle may be partial, we also need a one-value history for the decay process. Now Qdecay2 = {0, 1} × {0, 1}, xdecay2 = (1, 1), and decay2 (q, q ) = (1, y ) if max (q, q ) < max (q ) ([count (q ) < k2 ]y, y ) otherwise,

where (y, y ) = decay2 (q ). The final set of leaders are those agents with y = 1 when the disposable phase clock stops running. A second copy of the consensus phase clock, running from the initial configuration using y = 1 to designate leaders, rapidly converges in simulation to correct robust phase clock behavior when the number of leaders becomes appropriate. We implemented the disposable phase clock leader election protocol and tried it once on each value of 1.01n between 100 and 100000 for integers n, with every agent in the same initial state. There are three parameters to tune: , k1 , and k2 . The protocol is not very sensitive to the settings of these parameters, but the setting  = 9, k1 = 5, and k2 = 4 worked better than many others.
600 500 Steps per agent 400 100 300 200 100 100 10 Parallel time until leaders stable 36.1 * log(x) 1000 10000 Final number of leaders 0.377 * x**0.603

1000

10000

100000

1 100

1000

10000

100000

Population size

Population size

Fig. 2. Simulation results: parallel time of leader election and final number of leaders

32

D. Angluin, J. Aspnes, and D. Eisenstat

The results are depicted in Figure 2. As can be seen, it seems that the protocol generally leaves (n1- ) leaders and completely converges in O(log n) time.

Acknowledgments
The second author would like to thank Joanna Ellman-Aspnes for suggestions that helped overcome an obstacle in the proof of the main lemma bounding convergence. The authors would like to thank the DISC 2007 reviewers for their helpful comments.

References
1. Angluin, D., Aspnes, J., Diamadi, Z., Fischer, M.J., Peralta, R.: Computation in networks of passively mobile finite-state sensors. Distributed Computing, 235­253 (2006) 2. Angluin, D., Aspnes, J., Eisenstat, D.: Fast computation by population protocols with a leader. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 61­75. Springer, Heidelberg (2006) 3. Feller, W.: An Introduction to Probability and its Applications, 3rd edn., vol. 1. John Wiley and Sons, Chichester (1958) 4. Gillespie, D.T.: Exact stochastic simulation of coupled chemical reactions. Journal of Physical Chemistry 81(25), 2340­2361 (1977) 5. Gillespie, D.T.: A rigorous derivation of the chemical master equation. Physica A 188, 404­ 425 (1992) 6. Kurtz, T.G.: Approximation of Population Processes. Number 36 in CBMS-NSF Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics, Philadelphia (1981) 7. Wormald, N.C.: Differential equations for random processes and random graphs. Annals of Applied Probability 5(4), 1217­1235 (1995)

A Denial-of-Service Resistant DHT
Baruch Awerbuch1 , and Christian Scheideler2
1 2

Dept. of Computer Science, Johns Hopkins University, Baltimore, MD 21218, USA baruch@cs.jhu.edu Institut f¨ ur Informatik, Technische Universit¨ at M¨ unchen, 85748 Garching, Germany scheideler@in.tum.de

Abstract. We consider the problem of designing scalable and robust information systems based on multiple servers that can survive even massive denial-of-service (DoS) attacks. More precisely, we are focusing on designing a scalable distributed hash table (DHT) that is robust against so-called past insider attacks. In a past insider attack, an adversary knows everything about the system up to some time point t0 not known to the system. After t0 , the adversary can attack the system with a massive DoS attack in which it can block a constant fraction of the servers of its choice. Yet, the system should be able to survive such an attack in a sense that for any set of lookup requests, one per non-blocked (i.e., non-DoS attacked) server, every lookup request to a data item that was last updated after t0 can be served by the system, and processing all the requests just needs polylogarithmic time and work at every server. We show that such a system can be designed.

1

Introduction

On Feb 6 of this year, hackers launched a distributed denial-of-service (DoS) attack on the root servers of the Domain Name System (DNS) [10]. DoS attacks can overwhelm servers with hacker-generated traffic and make them unavailable for legitimate communications. While the attacks significantly slowed the operations of some of the servers, they caused no problems for the overall DNS system because the system shifts work to other root servers if it has trouble with the first ones it tries to reach. This is possible because information is replicated among all root servers, and the root servers together have sufficient bandwidth to handle even major DoS attacks. In this paper, we consider the problem of designing distributed information systems that are highly resilient against DoS attacks even if every piece of information is not replicated everywhere but only among a small subset of the servers. For distributed information systems that are connected to the Internet, like the DNS system, the servers may be known and therefore open to DoS attacks. There are various forms of DoS attacks. Application-layer DoS attacks, that try
Supported by NSF CCF 0515080, ANIR-0240551, CT-0716676, CCR-0311795, and CNS-0617883.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 33­47, 2007. c Springer-Verlag Berlin Heidelberg 2007

34

B. Awerbuch and C. Scheideler

to abuse the protocols of the system in order to prevent it from functioning correctly, or network-layer DoS attacks that just aim at overloading servers with junk or faked messages in order to prevent them from processing legal ones. We are interested in designing a scalable information system (i.e., a system in which the data and requests are handled in a scalable way) that can withstand even massive application-layer and network-layer DoS attacks (i.e., the attacker is powerful enough to generate requests or junk that can affect a constant fraction of the servers). Certainly, if the attacker has complete knowledge of the information system, then scalability and robustness against massive DoS attacks cannot be achieved at the same time. But what if the attacker only has complete knowledge up to some time step t0 (that may not be known to the system)? Would it at least be possible to protect everything that was inserted into the system or updated after time step t0 ? To answer this question, let us first formally define the attack model we will be focusing on in this paper. 1.1 The Past Insider Attack Model

The past insider attack model is motivated by the fact that a large percentage of the security breaches in corporate systems have internal reasons, many of them being caused by human error or negligence or insider attacks. In these cases, the entire system may be temporarily exposed, with potentially severe consequences for its functionality if this exposure is abused. In the past insider attack model, we assume that an attacker has complete knowledge of the system up to some time step t0 that is not known to the system. It can use this knowledge to attack the system at any time point after t0 . Given n servers, we allow the attacker to generate any collection of lookup requests it likes, one per non-blocked server, including lookup requests to blocked or non-existing data, and to block any set of n servers for some sufficiently small constant 0 < < 1. The goal is to design a storage strategy for the data and a lookup protocol so that the following conditions are met: For every data item d, the total space for storing d in the system is by at most a polylogarithmic factor larger than the size of d, and for any set of lookup requests with at most one request per non-blocked server, the following holds: ­ Scalability: Every server in the system spends at most a polylogarithmic amount of work and time on the requests. ­ Robustness: Every lookup request to a data item inserted after t0 (or a nonexisting data item) is served correctly. ­ Correctness: Every lookup request to a data item is served correctly whenever the system is not under a DoS attack. By "served correctly" we mean that the latest version of the data item is returned to the server requesting it. (We assume that there is a unique way of identifying the latest version such as the version number or a time stamp.) Note that our model is different from proactive security models in which the adversary can never learn too much about the system within a certain time frame. Approaches for this model aim at protecting everything in the system,

A Denial-of-Service Resistant DHT

35

but this comes at a high price because this means that all the information in the system has to be continuously refreshed, which may not be feasible in practice. We can show that one can protect nearly everything without continuous refreshing, and most importantly, everything that was updated after the security breach is over. 1.2 Towards Robustness Against Past Insider Attacks

Let us have a quick look at the basic approaches for storing data in a distributed system. ­ An explicit data structure such as a distributed search tree or skip graph: This approach is scalable but has, due to its structure, major problems with correctness and robustness under DoS-attacks. ­ An implicit data structure like a hash table: The hash table is structureless and therefore has less problems with correctness. It is also scalable, but it is not robust because the adversary knows exactly where the copies of a data item are located and can therefore block these. ­ The random placement of data copies among the servers: This is not scalable but certainly robust. Is there a way of combining these approaches in order to achieve scalability, robustness and correctness at the same time? Our main contribution in this paper is to show that a certain hybrid version of a hash table and random placement can achieve this task. More precisely, we will prove the following result. Theorem 1. Given n servers, our storage strategy just needs O(log2 n) copies per data item so that our lookup protocol can serve any set of lookup requests, one per non-blocked server, in a scalable, robust and correct way, w.h.p. The robustness holds for any DoS attack in which at most n servers are blocked, where > 0 is a sufficiently small constant. In the proof of the theorem, we assume that the servers are completely interconnected since we are only focusing on reliable servers, so there are no scalability problems w.r.t. connectivity. Although we consider only problems where all lookup requests are given at the beginning, we note that our lookup protocol can also be applied in a scenario where continuously new requests are generated. Furthermore, the correctness condition can be strengthened in a sense that beyond O(n + D/nk ) data items, where D is the total number of data items in the system and k can be an arbitrary constant, all of the data inserted before t0 can still be accessed by our lookup protocol under a DoS attack, but it can obviously not be guaranteed that everything is still accessible. Using coding strategies (like Reed-Solomon codes), the storage overhead for the data items can be reduced to O(log n) in Theorem 1. The constant that we need in our proofs is < 1/144, but we did not try to optimize constants in this paper. The beauty of our approach is that, even though it uses much more sophisticated concepts, it is still based on the well-known consistent hashing principle

36

B. Awerbuch and C. Scheideler

[5], i.e., the servers are assigned to points in the [0, 1)-interval and a data copy mapped to point x  [0, 1) is stored at the server that is the closest predecessor of x in [0, 1). Thus, it could in principle be used on top of existing DHTs based on consistent hashing such as Chord in order to turn these into highly DoS-resilient DHTs. However, notice that in the scenarios considered in this paper, we can afford a completely interconnected network though most DHT implementations are based on bounded degree overlay networks, which would create an additional vulnerability. Finally, we remark that we do not address the problem of handling insert requests in this paper but only how to store data in the system in a scalable way so that it can be retrieved despite massive DoS attacks. Managing insert requests is a tricky issue when application- and network-layer DoS attacks are allowed, and we discuss some of the reasons behind that in Section 2.1. Taking this restriction into account, our strategies would work best for archival systems or systems for information retrieval like Google, CiteSeer or Akamai. 1.3 Related Work

The most prominent approach for a scalable information system is to implement a distributed hash table, or DHT. Well-known examples of DHTs are Chord [22], CAN [18], Pastry [3], and Tapestry [24]. Most of the DHT-based systems are based on concepts proposed in two influential papers: a paper by Plaxton et al. on locality-preserving data management in distributed environments [17] and a paper by Karger et al. on consistent hashing and web caching [5]. However, since in both cases the data management is based on hashing, none of these approaches is robust against past-insider attacks. Various attacks on the data management layer of DHTs have been considered in the past. Most of the work considers the flash crowd scenario in which many peers want to access the same information at the same time. When using a pure DHT design, this can lead to severe bottlenecks. To remove these bottlenecks, various caching strategies have been proposed. Among them are CoopNet [16], Backslash [19], PROOFS [20] in the systems community and [14] in the theory community. However, being able to handle flash crowds is not sufficient to handle arbitrary collections of lookup and insert requests in a scalable way because much worse than having many requests to the same data item is to have many requests to different data items at the same location. Standard combining or caching strategies do not work here, but work on deterministic simulations of CRCW PRAMs (e.g., [11]) turned out come to the rescue here. These concepts allow the design of insert and lookup protocols that are guaranteed to handle any set of requests with at most one request per server that can be chosen by an adversary knowing everything about the system [2]. Thus, application-layer DoS attacks can be handled but not network-layer attacks since the protocols in [2] are purely hash-based. There is a vast amount of literature on network-layer DoS attacks (see, e.g. [4,12] for a taxonomy of these DoS attacks). Several authors have explored the use of DHTs to prevent DoS attacks from outsiders (e.g., [8,6,13]). Secure

A Denial-of-Service Resistant DHT

37

Overlay Services (SOS) [8], for example, uses a proxy approach based on the Chord network to protect applications against flooding DoS attacks. WebSOS [21] is an implementation of SOS for web servers that makes use of graphical Turing tests, web proxies and client authentication. Mayday [1] generalizes the SOS architecture and analyzes the implications of choosing different filtering techniques and overlay routing mechanisms. Internet Indirection Infrastructure (i3) [9] also uses the Chord overlay to protect applications from direct DoS attacks. Other DoS limiting overlay network architectures have been explored in, e.g., [15,23]. Most of the approaches above use traffic analysis or indirection approaches to make DoS attacks hard, but none of these would be able to survive the attackers considered in this paper since they essentially rely on the ability to protect servers from direct hits of adversarial traffic.

2

A DoS-Resistant DHT

In this section we describe how to store and retrieve information in a scalable and robust way in a DHT of completely interconnected servers. The DHT is based on the consistent hashing principle in a sense that the servers (also called nodes henceforth) are given points in the [0, 1)-ring and any data copy that is mapped to a point x  [0, 1) is stored at the node that is the closest predecessor of x in [0, 1). First, we present our data storage strategy. Afterwards, we present and analyze our lookup protocol. For simplicity, we make the following assumptions: ­ The number of nodes in the DHT is fixed to n, and n is a power of 2. ­ The nodes are numbered from 0 to n - 1, and node i is responsible for the interval [i/n, (i + 1)/n) in [0, 1). Both assumptions can be relaxed (one can imagine, for example, that the nodes are randomly spread in [0, 1) so that the DHT does not need central coordination), but we use them here since they will keep our proofs simple. 2.1 The Storage Strategy

Like in [2], we use c = (log m) hash functions, denoted by h1 , . . . , hc , that map data names to points in the [0, 1) interval, where m represents the size of the universe of all data names, but this is the only feature the approach in this paper has in common with [2]. First, we introduce some notation. We assume that the points in [0, 1) are given in binary form, i.e., point x  [0, 1) is given as (x1 , x2 , . . .)  {0, 1} with x = i1 xi /2i . For any two bit sequences x, y  {0, 1}, x  y is the unique point z  [0, 1) with (z1 , z2 , . . .) = (x1 , . . . , x|x| , y1 , . . . , y|y| ). For any point x  [0, 1) and  N, we call set T (x) = {z  [0, 1) | z = y  x for some y  {0, 1} } the set of all points at distance from x. A route to x of length is any sequence of points R = (z , z -1 , . . . , z0 ) with the property that z0 = x and for every i > 0, zi+1 = b  zi for some bit b  {0, 1} (which implies that zi  Ti (x) for every i). Let R (x) be the set of all possible routes of length to x. A random route to

38

B. Awerbuch and C. Scheideler

x is a route R chosen uniformly and independently at random from R (x) (i.e., z is chosen uniformly and independently at random from T (x)). When a data item d is inserted or updated in the system, we select a random route Ri = (zi,log n , zi,log n-1 , . . . , zi,0 )  Rlog n (hi (d)) for every i  {1, . . . , c}. For each distance j  {0, . . . , log n}, we store  log n copies of d, for some constant  that will be determined later. For each of these copies, we select an i  {1, . . . , c} uniformly and independently at random and store the copy in point zi,j (resp. the node owning that point according to the consistent hashing scheme). Hence, altogether, we store O(log2 n) copies of each data item in the system. It would be sufficient for our lookup protocol if instead of storing O(log n) copies for each data item for each distance j , we use Reed-Solomon or other codes to store each data item in O(log n) encoded pieces for each distance. That would reduce the overall storage overhead to O(log n) in Theorem 1, but for simplicity we will just assume that copies of d are stored. Notice that as long as the set of DoS-attacked nodes is static, our storage strategy could be transformed into an efficient insert protocol together with techniques in [2] to avoid congestion problems. However, for a dynamically changing set of DoS-attacked nodes this is tricky since some non-DoS-attacked nodes are now missing information that is necessary for our lookup protocol to work correctly. A potential countermeasure here could be to delay the execution of the insert protocol at DoS-attacked nodes until the DoS-attack goes away. This requires extra management overhead and complicates the design of the insert protocol, which is why we left it out here. For the rest of this paper, we will assume that the binary representations of all points are rounded to log n bits (i.e., the points are multiples of 1/n, so there is one point for each node). For any  N let T = {T (x) | x  [0, 1)}, where we consider the points in T (x) to be rounded to log n bits. That is, |T | = n/2 and each set in T has a size of 2 . 2.2 The Lookup Protocol

We assume that we are given any collection of lookup requests, one per nonblocked server. The lookup protocol consists of two stages. The first stage is the contraction stage and the second stage is the expansion stage. During the contraction stage, the lookup requests are forwarded along random routes towards the hash values of the requested data items. Each lookup request encountering too many blocked or congested nodes stops and waits for the expansion stage to be executed. During the expansion stage, lookup requests are woken up in a controlled manner, and node sets of exponentially increasing size are explored in order to search for copies of the requested data items until sufficiently many copies have been found or the protocol decides that the item does not exist in the system. We start with the formal description of the contraction stage. The contraction stage. Each lookup request for some data item d chooses, for each i  {1, . . . , c} and j  {1, . . . ,  log n}, a random route Ri,j  Rlog n (hi (d)) of length log n to hi (d), where  is a sufficiently large constant. Hence, altogether

A Denial-of-Service Resistant DHT

39

there are c log n random routes. For every i, let Q ,i be the set of all nodes in the routes Ri,j to hi (d) that belong to T (hi (d)), and let Q ,i be the non-blocked nodes in Q ,i . Initially, all lookup requests are active, and all i  {1, . . . , c} are active for all lookup requests. Then the contraction stage proceeds in rounds, executed from log n down to 0. In round r, every active request for some data item d sends a message to all nodes in its set Qr,i for all active i. Each of the nodes v  Qr,i replies back to that request. The reply contains the number mv,i (d) of messages it has received from requests to data item d for index i, which is called the multiplicity of d at v , and the number Cv,i of different data items for which it was contacted by requests for index i, which is called the congestion at v . Afterwards, each lookup request checks the following rules: 1. For each active i  {1, . . . , c} with |Qr,i | < ( log n)/2, the request deactivates i. If the total number of deactivated i's is at least c/2, the request becomes inactive. 2. For each active i  {1, . . . , c} with |{v  Qr,i | |Cv,i |  2c log n}|  |Qr,i |/2, the request deactivates i. If the total number of deactivated i's is at least c/2, the request becomes inactive. 3. If there is an active i for which there is a node v in Qr,i with mv,i (d)  2 log n, the request becomes inactive. Inactive lookup requests do not participate any further in the contraction stage. The expansion stage. Each lookup request for some data item d that was active till the end of the previous stage, gets the most up-to-date copy of d from every non-blocked hi (d), returns the most up-to-date copy among these and finishes. For the other requests, the expansion stage proceeds in rounds, this time numbered from 1 to log n. In round r, every lookup request for some data item d that got deactivated in a round r < r and is not finished yet sends a message of the form (d, r, i, -) (where "-" is an empty placeholder for a copy of d) to a random node in Qi,r for each i that was active at the end of that round in the contraction stage. Each node v stores the IDs of the nodes that sent messages to it in Sv and stores the messages it received from them into its active pool of messages Av , one copy for each (d, r, i, -). If |Av | > 3c/ , then any set of messages is discarded from Av to get down to |Av | = 3c/ , where the constant  is chosen as in Lemma 1 below. For any remaining (d, r, i, -) in Av for which v stores a copy b of d (due to the data storage strategy defined above), (d, r, i, -) is replaced by (d, r, i, b). Afterwards, Av is managed as a FIFO queue. Every node v in the system executes the following push strategy O(c log n) many times: ­ v dequeues one message (d, r, i, b) from Av , enqueues it back to Av and sends a copy of it to a random node in Tr (hi (d)). ­ For each message (d, r, i, b) received by v , v first checks whether Av contains some message (d, r, i, b ) in which copy b is older than b (or empty). If so,

40

B. Awerbuch and C. Scheideler

v replaces b by b. Otherwise, v checks if |Av | = 3c/ . If so, v discards the message. Otherwise, it checks whether it stores a copy b of d that is younger than b. If so, v inserts (d, r, i, b ) into Av , and otherwise it inserts (d, r, i, b) into Av . If after these steps |Av | = 3c/ , then v sends for each node w  Sv with original message (d, r, i, -) the message (d, r, i, ) back to w, where the "" indicates that v was too congested. Otherwise, v sends (d, r, i, b) in Av back to w. Each lookup request that receives at most c/4 many (d, r, i, ) messages returns the message (d, r, i, b) with the most up-to-date b (which may also be "-" if no copy was found) to whoever generated the request and is finished. Otherwise, it continues to participate in round r + 1. 2.3 Robust Hash Functions

In this section, we specify a central property the c hash functions h1 , . . . , hc have to satisfy for the lookup protocol to work correctly and efficiently. Given a set S of data items and a k  N, we call F  S × {1, . . . , c} a k -bundle of S if every d  S has exactly k many tuples (d, i) in F . Given h1 , . . . , hc and a distance , let F, (S ) = (d,i)F T (hi (d)). Let U be the set of all possible (names of the) data items and H be the collection of hash functions h1 , . . . , hc , and let m = |U |. Given a 0 <  < 1, we call H a (k,  )-expander if for any  log n, any S  U with |S |  n/2 , and any k -bundle F of S it holds that |F, (S )|  2 |S |. Lemma 1. Let 0 <  < 1 be any constant. Then it holds for any c  8 log m and   1/24 that if the functions h1 , . . . , hc are chosen uniformly and independently at random, then H is a (c/4,  )-expander with high probability. Proof. Suppose that, for randomly chosen functions h1 , . . . , h2c-1 , H is not a (c/4,  )-expander. Then there exists an i  log n and a set S  U with |S |  n/2i and a c/4-bundle F of S with |F,i (S )| < 2i |S |. We claim that the probability ps,i that such a set S of size s exists is at most m s This holds because there are
cs cs/4

cs cs/4
m s

n/2i · s

s n/2i

cs/4

ways of choosing a subset S  U . Furthermore,
i

2 ways of choosing cs/4 pairs (d, j ) for F and at most n/ ways there are s of choosing a set W of s sets in Ti witnessing a bad expansion of the pairs in F . The fraction of collections H for which the selected pairs (d, j ) indeed have the s cs/4 because the hash functions property that Ti (hj (d))  W is equal to ( n/ 2i ) h1 , . . . , hc are chosen independently and uniformly at random. Next we simplify ps,i . Using the conditions on c and  in the lemma it holds that

A Denial-of-Service Resistant DHT
cs/4

41

m s 

cs cs/4 em s
s

n/2i · s en s2 i

s n/2i
s

(4e)cs/4
1+4/c

s2 n

i

cs/4

 em · =  s 1 2 4e1+4/c ·  1 ms s2 n

i

1-4/c

c/4

s 

 m · 4e

·

1-4/c

c/4 s



m·

c/4 s

if c  8 log m and m is sufficiently large. Hence, summing up over all possible values of s and i, we obtain a probability of having a bad c/4-bundle of at most (2 log n)/m, which proves the lemma. We remark that the hash functions have to form a (c/4,  )-expander for some constant  for our lookup protocol to work, but they do not have to be chosen at random. The proof above just illustrates that if they are chosen at random, they will form a (c/4,  )-expander w.h.p. 2.4 Analysis of the Lookup Protocol

Next we show that the lookup protocol is correct, robust and efficient, i.e., for every lookup request for some data item d inserted after time t0 (the threshold in the past insider model), a correct answer will be delivered for any DoS attack under our model, and every node spends only polylogarithmic time and work on the requests in the system. The correctness condition for all other data items is implied by our proofs. First, we prove the correctness of a lookup request given that it finishes in the expansion stage. Lemma 2. If a lookup request for some data item d finishes in round r of the expansion stage and d was inserted after t0 , then it returns the most up-to-date version of d. Proof. Consider any lookup request for some data item d that finishes in round r of the expansion stage and d was inserted after t0 . This means that it got at most c/4 many messages of the form (d, r, i, ). The request only participates in round r of the expansion stage if it was still active at the beginning of round r - 1 in the contraction stage, which is only the case if it had at least c/2 active indices i at the beginning of round r - 1. These indices were all used in round r of the expansion stage, which means that at the end of that round the request got at least c/4 messages back of the form (d, r, i, b) where b is the most up-to-date copy of d that the node contacted by the request in Tr (hi (b)) found. Let I be the set of these indices. From the fact that each i  I was active at the beginning of round r - 1 in the contraction stage it follows that in round r of that stage, Qr,i  ( log n)/2 (because otherwise i would have been deactivated in that round). Hence, at least half of the nodes in Qr,i that were sampled from Tr (hi (b)) are non-blocked nodes. Since the nodes in Qr,i were

42

B. Awerbuch and C. Scheideler

chosen independently at random, it follows from the Chernoff bounds that the total number of blocked nodes in Tr (hi (b)) is at most 2|Tr (hi (b))|/3, w.h.p., if  is a sufficiently large constant. We call a Tr (hi (b)) satisfying such a property non-blocked in this proof. We can show the following claim. Claim 1. For any node v in a non-blocked set Tr (hi (b)) it holds that if after c log n executions of the push strategy in the expansion stage, where  is a sufficiently large constant, |Av | < 3c/ , then there are lookup requests for less than 3c/ data items that sent messages to nodes in Tr (hi (b)) and every entry (d, r, i, b) in Av stores the most up-to-date copy of d among the non-blocked nodes in Tr (hi (b)) at the end. Proof. Can be shown along the lines of existing proofs on random, push-based broadcasting in complete networks (e.g., [7]). Hence, the lookup request obtains at least c/4 many replies (d, r, i, b) with most up-to-date copies in the respective sets Tr (hi (b)). Since for each i  I at least a third of the nodes in Tr (hi (b)) are not blocked, w.h.p., the lookup request returns the most up-to-date copy for at least (c/4) · (2r /3) = c2r /12 of the c2r nodes in all sets Tr (hi (b)), i  {1, . . . , c}. According to the robust storage strategy,  log n many most up-to-date copies of d are randomly distributed among these c2r nodes, and none of these locations is known to the adversary, so the probability that none of these is stored in the at least c2r /12 non-blocked nodes accessed by the request is at most (1 - 1/12) log n , which is polynomially small if  is a sufficiently large constant. We remark that for the data items least recently updated before t0 , at most f = max{n, D/nk } many of them are bad, w.h.p., in a sense that none of their most up-to-date copies is stored in the at least c2r /12 non-blocked nodes, where D is the number of data items in the system and k can be any constant. This is because there are at most 2n ways of selecting c2r /12 non-blocked out of at most n considered nodes and D f ways of selecting bad data items while the probability that these are indeed bad is at most ((1 - 1/12) log n )f . If  is sufficiently large compared to k , multiplying these terms gives a polynomially small probability. Hence, apart from f data items, the lookup protocol will deliver correct answers also for data items inserted or updated before t0 , w.h.p. Next, we look at the robustness and efficiency and will show that within a polylogarithmic time every request finishes, w.h.p. First, we consider the contraction stage. We show that the number of messages sent to any node is polylogarithmic in every round, which implies that every node spends only a polylogarithmic time and work on the contraction stage. Lemma 3. For every round r, at most O(c log2 n) messages are sent to any node in the system, w.h.p. Proof. Consider some fixed node v in some set T  Tr . First, we bound mv,i (d) for any data item d and index i with Tr (hi (d)) = T . Suppose that mv,i (d) 

A Denial-of-Service Resistant DHT

43

8 log n. Since every request for d chooses the nodes in Qr,i independently at random from the nodes in T , it follows from the Chernoff bounds that the expected number of messages for d at a node in T is at least 6 log n, w.h.p. Hence, the expected number of messages for d in Tr+1 (hi (d)) was at least 3 log n, w.h.p. However, in this case it holds for every node w  Tr+1 (hi (d)) that mw,i (d) was at least 2 log n, w.h.p. Thus, every request for d that was still active at round r + 1 must have either deactivated i or became inactive. Hence, it must hold for every node v  T that mv,i (d)  8 log n, w.h.p., for any data item d and i with Tr (hi (d)) = T . A congestion bound of |Cv,i |  8 log n can be shown along exactly the same lines. Since there are c different indices i, the total number of messages sent to v is bounded by c(8 log n)2 = O(c log2 n). Hence, the runtime of the contraction stage and work per node is O(c log3 n), w.h.p. Next we analyze the number of different data items for which lookup requests become inactive. This will be important to bound the congestion at the nodes in the expansion phase. Let Dr be the set of all data items for which there are lookup requests that become inactive in round r. Furthermore, let BCr be the set of data items with requests that become inactive due to too many inactive indices and M Cr be the set of data items with requests that become inactive due to a too high multiplicity. Certainly, Dr = BCr  M Cr . First, we bound BCr . Lemma 4. If w.h.p. < 1/144, then it holds for every round r that |BCr |  8 n/2r ,

Proof. For any r and any T  Tr , we call T blocked if the attacker blocks more than a third of its nodes with its DoS attack, and T is called congested if more than a third of the nodes in T have a congestion of at least 2c log n. Consider any data item d. We call d blocked at round r if at least c/4 of its c sets Tr (hi (d)) are blocked, and we call it weakly blocked at round r if there are blocked sets Tr1 (hi1 (d)), Tr2 (hi2 (d)), . . . , Trk (hik (d)) with r1 , . . . , rk  r and k = c/4 and i1 , . . . , ik being pairwise different. Similarly, we call d congested at round r if at least c/4 of its c sets Tr (hi (d)) are congested, and we call it weakly congested at round r if there are congested sets Tr1 (hi1 (d)), Tr2 (hi2 (d)), . . . , Trk (hik (d)) with r1 , r2 , . . . , rk  r and k = c/4 and i1 , . . . , ik being pairwise different. In the following, W Br denotes the set of weakly blocked data items and W Cr the set of weakly congested data items at round r. Claim 2. Whenever a request for some data item d deactivates some index i in round r, then Tr (hi (d)) is either blocked or congested, w.h.p. Proof. Consider any request for some data item d in round r. Index i is deactivated for that request if 1. there are at least ( log n)/2 nodes in Qr,i that are blocked, or 2. there are at least ( log n)/2 nodes in Qr,i that are congested.

44

B. Awerbuch and C. Scheideler

In the first case, suppose that Tr (hi (d)) is not blocked. Then the probability that Ri,j chooses a node in Tr (hi (d)) that is blocked is at most 1/3 and, hence, the expected number of nodes chosen by the routes Ri,1 , . . . , Ri, log n that are blocked is at most ( log n)/3. Since the nodes are chosen independently at random, it follows from the Chernoff bounds that the probability that at least ( log n)/2 nodes in Qr,i are blocked is polynomially small in n (if the constant  is sufficiently large). Hence, if the request deactivates index i because of at least ( log n)/2 blocked nodes, then Tr (hi (d)) is blocked, w.h.p. The arguments for the congestion follow along the same lines. Now suppose that a request for data item d becomes inactive at round r due to at least c/2 deactivated indices. Then there are at least c/4 indices for which condition 1 in the contraction stage is true or at least c/4 indices for which condition 2 is true. In the first case, it follows from Claim 2 that d is weakly blocked, and in the second case, it follows from Claim 2 that d is weakly congested, w.h.p. For weakly blocked data items, the following claim holds. Claim 3. If s blocked nodes can cause a set of b weakly blocked data items at round r, then a set of 2s blocked nodes can cause a set of b blocked data items at round r. Proof. Consider item d to be weakly blocked, and let Tr1 (hi1 (d)), Tr2 (hi2 (d)), . . . , Trk (hik (d)) be the sets witnessing that with k = c/4. Any route through a set Tr (hi (d)) with r > r can have at most 2r -r sets T  Tr it can go through, and each of these sets T has a size of |Tr (hi (d))|/2r -r . Hence, the number of nodes causing Tr (hi (d)) to be blocked is sufficient to block also all T  Tr reachable from Tr (hi (d)). Hence, for any set of b weakly blocked data items, we can turn them into blocked data items when moving the blocking of nodes at distance r > r to nodes at distance r. Since we have to keep those sets Tr (hi (b)) with r = r blocked, we have to at most double the number of nodes needed to transform weakly blocked data items into blocked data items. If the adversary can block at most 2 n nodes, then at most 6 n/2r of the n/2r sets in Tr can be blocked, which covers at most 6 n nodes. Suppose the attacker can block a set S of data items in round r. Then there is a c/4-bundle F for S . According to Lemma 1, it holds that |F,r (S )|  2r |S | if |S |  n/2r . Since the largest possible size of F,r (S ) is 6 n, it follows that |S |  6 n/2r , which is less than n/2r (so that Lemma 1 implies an upper bound on |S |) if 6 < 1/24, or < 1/144. Hence, if the adversary can block at most 2 n nodes, then it can cause at most 6 n/2r blocked data items in round r. This implies together with Claim 3 that if the adversary can block at most n nodes, then it can cause at most 6 n/2i weakly blocked data items in round r. Combining this with Claim 2, it follows that if the adversary can block at most n nodes, then |W Br |  6 n/2r , w.h.p. Using the same arguments for congested data items, it also follows that |W Cr |  6 n/2r , w.h.p. Hence, |BCr |  |W Br | + |W Cr |  12 n/2r , w.h.p. Next we bound M Cr .

A Denial-of-Service Resistant DHT

45

Lemma 5. For every round r, |M Cr |  n/2r . Proof. Suppose that there are many lookup requests for data item d in round r. Then the expected number of messages per node w.r.t. i in any Tr (hi (d)) is ( log n) /2r . If  2r , this is at most  log n. Since each message chooses a node independently at random, it follows from the Chernoff bounds that the probability that there is a node in a set Tr (hi (d)) with at least 2 log n messages for d is polynomially small. Hence, if a lookup request for some data item d becomes inactive due to condition 3 of the contraction stage, then d has a multiplicity of at least 2r , w.h.p. Since there are at most n active requests at round r, it follows that |M Cr |  n/2r . Combining Lemmas 4 and 5 it follows that |Dr |  12 n/2r + n/2r  2n/2r , w.h.p. Now we are ready to analyze the expansion stage. First, we bound the number of messages sent to a node in each round. Lemma 6. For every round r, at most O(c log n) messages are sent to any node in the system, w.h.p. Proof. Only requests that were active in round r - 1 of the contraction stage will participate in round r of the expansion stage. According to Lemma 3, the number of messages sent to any node in the system in any round of the contraction stage is O(c log2 n) w.h.p. Since every lookup request sends out c log n messages in the contraction stage but only at most c messages in the expansion stage, it follows that the number of messages sent to any node in the expansion stage is at most O(c log n), w.h.p. The description of the expansion stage and Lemma 6 immediately imply that the runtime and work per node of the expansion stage is at most O(c log3 n). Combining this with our bounds for the contraction stage, we get: Lemma 7. For any collection of lookup requests, one per non-blocked node, the lookup protocol needs at most O(c log3 n) time and work at every node. Next, we show that every request will eventually finish in the lookup protocol, w.h.p. Lemma 8. For every round r, the number of data items with requests participating in it is less than 3n/2r . Proof. We prove the lemma by induction on the number of rounds. For round 1, the lemma certainly holds. So consider any round r  1 for which the lemma holds. A set T  Tr is called congested if there are messages for at least 3c/ many different data items in T . Since there are requests for less than 3n/2r many data items and the messages for each data item are limited to c sets T  Tr , there must be less than n/2r many sets T  Tr that are congested. A data item d is called congested if there are at least c/4 many indices i for which Tr (hi (d)) is congested. Let S be the set of congested data items. Then there is

46

B. Awerbuch and C. Scheideler

a c/4-bundle F for S . According to Lemma 1, it holds that |F,r (S )|  2r |S | if |S |  n/2r . Since the largest possible size of F,r (S ) is less than n, it follows that |S | < n/2r . As a worst case, we assume that all requests for congested data items will not finish in round r and therefore have to continue in round r + 1. These will combine with the requests for at most n/2r + 12 n/2r data items with requests that became inactive in round r of the contraction stage, which gives an upper bound of less than 3n/2r+1 on the number of data items with requests in round r + 1 (given that < 1/144 and   1/24). Hence, in round r = log n, there are at most 3 data items left with requests. In this round, all sets Tr (hi (d)) are equal to the entire node set, so there is no congested node set left. Hence, according to the expansion protocol, every request will finish in that round. Combining this with Lemmas 2 and 7 proves Theorem 1.

3

Conclusions

In this paper we showed that a DHT for scalable data storage and retrieval can be designed that is provably robust against massive application- and networklayer DoS attacks. Certainly, low-level protocols still have to be developed for our operations that work well and correctly in an asynchronous environment. Also, it would be interesting to find out whether adaptations of our strategies are possible to bounded degree DHTs so that they can sustain DoS attacks of a similar magnitude as considered in this paper.

References
1. Andersen, D.G.: Mayday: Distributed filtering for internet services. In: 4th Usenix Symp. on Internet Technologies and Systems (2003) 2. Awerbuch, B., Scheideler, C.: Towards a scalable and robust DHT. In: Proc. of the 18th ACM Symp. on Parallel Algorithms and Architectures (SPAA). ACM Press, New York (2006), http://www14.in.tum.de/personen/scheideler 3. Druschel, P., Rowstron, A.: Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems. In: Proc. of the 18th IFIP/ACM International Conference on Distributed Systems Platforms (Middleware 2001). ACM Press, New York (2001) 4. Dittrich, D., Mirkovic, J., Dietrich, S., Reiher, P.: Internet Denial of Service: Attack and Defense Mechanisms. Prentice Hall PTR, Englewood Cliffs (2005) 5. Karger, D., Lehman, E., Leighton, T., Levine, M., Lewin, D., Panigrahi, R.: Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web. In: Proc. of the 29th ACM Symp. on Theory of Computing (STOC), pp. 654­663. ACM Press, New York (1997) 6. Kargl, F., Maier, J., Weber, M.: Protecting web servers from distributed denial of service attacks. World Wide Web, pp. 514­524 (2001) 7. Karp, R., Shenker, S., Schindelhauer, C., V¨ ocking, B.: Randomized rumor spreading. In: Proc. of the 41st IEEE Symp. on Foundations of Computer Science (FOCS), pp. 565­574. IEEE Computer Society Press, Los Alamitos (2000)

A Denial-of-Service Resistant DHT

47

8. Keromytis, A.D., Misra, V., Rubenstein, D.: SOS: Secure Overlay Services. In: Proc. of ACM SIGCOMM, pp. 61­72. ACM Press, New York (2002) 9. Lakshminarayanan, K., Adkins, D., Perrig, A., Stoica, I.: Taming ip packet flooding attacks (2003) 10. Lawton, G.: Stronger domain name system thwarts root-server attacks. IEEE Computer, 14­17 (May 2007) 11. Mehlhorn, K., Vishkin, U.: Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel mamories. Acta Informatica 21, 339­374 (1984) 12. Mirkovic, J., Reiher, P.: A taxonomy of ddos attacks and defense mechanisms. ACM SIGCOMM Computer Communications Review 34(2) (2004) 13. Morein, W.G., Stavrou, A., Cook, D.L., Keromytis, A.D., Misra, V., Rubenstein, D.: Using graphic turing tests to counter automated ddos attacks against web servers. In: Proc. of the 10th ACM Int. Conference on Computer and Communications Security (CCS), pp. 8­19. ACM Press, New York (2003) 14. Naor, M., Wieder, U.: Novel architectures for P2P applications: the continuousdiscrete approach. In: Proc. of the 15th ACM Symp. on Parallel Algorithms and Architectures (SPAA). ACM Press, New York (2003) 15. Oikonomou, G., Mirkovic, J., Reiher, P., Robinson, M.: A framework for collaborative ddos defense. In: Jesshope, C., Egan, C. (eds.) ACSAC 2006. LNCS, vol. 4186. Springer, Heidelberg (2006) 16. Padmanabhan, V.N., Sripanidkulchai, K.: The case for cooperative networking. In: Proc. of the 1st International Workshop on Peer-to-Peer Systems (IPTPS) (2002) 17. Plaxton, G., Rajaraman, R., Richa, A.W.: Accessing nearby copies of replicated objects in a distributed environment. In: Proc. of the 9th ACM Symp. on Parallel Algorithms and Architectures (SPAA), pp. 311­320. ACM Press, New York (1997) 18. Ratnasamy, S., Francis, P., Handley, M., Karp, R., Shenker, S.: A scalable contentaddressable network. In: Proc. of the ACM SIGCOMM '01. ACM Press, New York (2001) 19. Stading, T., Maniatis, P., Baker, M.: Peer-to-peer caching schemes to address flash crowds. In: Proc. of the 1st International Workshop on Peer-to-Peer Systems (IPTPS) (2002) 20. Stavron, A., Rubenstein, D., Sahn, S.: A lightweight robust P2P system to handle flash crowds. In: Proc. of the IEEE Intl. Conf. on Network Protocols (ICNP). IEEE Computer Society Press, Los Alamitos (2002) 21. Stavrou, A., Cook, D.L., Morein, W.G., Keromytis, A.D., Misra, V., Rubenstein, D.: Websos: An overlay-based system for protecting web servers from denial of service attacks (2005) 22. Stoica, I., Morris, R., Karger, D., Kaashoek, M.F., Balakrishnan, H.: Chord: A scalable peer-to-peer lookup service for Internet applications. In: Proc. of the ACM SIGCOMM '01, ACM Press, New York (2001), See also http://www.pdos.lcs.mit.edu/chord/ 23. Yang, X., Wetherall, D., Anderson, T.: A dos-limiting network architecture. In: Proc. of the ACM SIGCOMM. ACM Press, New York (2005) 24. Zhao, B.Y., Kubiatowicz, J., Joseph, A.: Tapestry: An infrastructure for faulttolerant wide-area location and routing. Technical report, UCB/CSD-01-1141, University of California at Berkeley (2001), See also http://www.cs.berkeley.edu/ ravenben/tapestry

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks
Roberto Baldoni, Kleoni Ioannidou, and Alessia Milani
Universit´ a di Roma La Sapienza, Italy baldoni@dis.uniroma1.it, ioannidu@cs.toronto.edu, milani@dis.uniroma1.it

Abstract. We present a model of a mobile ad-hoc network in which nodes can move arbitrarily on the plane with some bounded speed. We show that without any assumption on some topological stability, it is impossible to solve the geocast problem despite connectivity and no matter how slowly the nodes move. Even if each node maintains a stable connection with each of its neighbours for some period of time, it is impossible to solve geocast if nodes move too fast. Additionally, we give a tradeoff lower bound which shows that the faster the nodes can move, the more costly it would be to solve the geocast problem. Finally, for the onedimensional case of the mobile ad-hoc network, we provide an algorithm for geocasting and we prove its correctness given exact bounds on the speed of movement. Keywords: Mobile ad-hoc networks, geocast, speed of movement vs cost of the solution, distributed systems.

1

Introduction

There has been increasing interest in mobile ad-hoc networks with nodes that move arbitrarily on the plane. This is justified by the significance of (wireless) mobile computing in emerging technologies. Current technologies require a stable infrastructure which is used for communication between mobile nodes. Unfortunately, in some cases, such as a military operation or after some physical disaster, a fixed infrastructure cannot exist. For such cases, it is desirable to program the mobile nodes to solve important distributed problems within specific geographical areas and without depending on a stable infrastructure. This is why there has been an increasing interest in studying "geo" related problems in mobile ad-hoc networks such as georouting [1,2], geocasting [3,4,5,6], geoquorums [6], etc. Geocasting is a variant of the multicast problem [7]. In geocasting, the nodes eligible to deliver a message are the ones that belong to a specific geographical area. Specifications to this problem can be either best effort or deterministic. An implementation of a best effort specification aims to maximize the probability
The work described in this paper was partially supported by the Italian Ministry of Education, University, and Research (MIUR) under the ISMANET project and by the European Community under Resist Network of Excellence.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 48­62, 2007. c Springer-Verlag Berlin Heidelberg 2007

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks

49

that nodes eligible to deliver the information, they actually deliver it [3,4,5]. Deterministic specifications define a set of nodes and an implementation of such a specification ensures that each of these nodes will deliver the information [6]. When geocasting is solved for mobile ad-hoc networks, the speed of how nodes move becomes an important factor. This is because it can heavily influence, for example, the completion time of the message diffusion in a certain geographical area till making geocasting unsolvable if these speeds are too high. In an extreme (unrealistic) scenario, nodes can move fast enough to ensure that no two neighbours stay connected for enough time to complete the receipt of a message. Geocasting cannot be solved in this scenario even though the topology of the mobile ad-hoc network never disconnects. To our knowledge this relation among problem solvability, the cost of a solution, and mobility has never been investigated. This paper focuses on geocasting based on deterministic specifications investigating the relation between cost of solving geocasting and mobility. In particular, we firstly provide a model of computation (Section 2) and a specification for the geocasting problem (Section 3) which both take into account (explicitly or implicitly) node mobility. The model makes a distinction between strong and weak connectivity. A system strongly connected has some assurance of topological stability, i.e., there is always a path between every two nodes formed by strong neighbors, where strong neighbors means that they remain neighbors for some period of time. A connected system that does not satisfy the previous property is weakly connected. Our model does not rely on either GPS or synchrony being thus very weak with respect to other models presented in the literature [8]. The geocasting specification is split in three properties: reliable delivery, integrity, and termination. Reliable delivery states that all nodes, which remain for some positive time C within distance d from the location l where the geocast has been issued, will deliver the geocast information. Conversely, integrity defines the minimum distance between the location l and a node in order that the latter does not deliver the geocast information. Termination states that after some period of time C from geocasting of some information, there will be no more communication related to this geocast. Hence, a general framework of geocasting algorithms is proposed (Section 3.2), which captures existing geocast algorithms. An algorithm belonging to this framework acts as follows: once a node receives a message (with the geocast information) broadcast by a neighbour, it may repeat a (local) broadcast k times, once every  rounds, depending on some condition. Using this framework in our model several results have been proved: (i) if nodes are weakly connected geocasting cannot be solved no matter how slowly the nodes can move (Theorem 1); (ii) if stronger connectivity holds, then geocasting is still impossible for some bound of node's speed of movement (Theorems 2 and 3); (iii) a tradeoff lower bound that relates the cost of geocasting to the speed of movement of nodes (Theorem 4). Finally, if the speed is small enough, we show how to solve the geocasting problem in a one-dimensional setting (Section 5). We prove that the time

50

R. Baldoni, K. Ioannidou, and A. Milani

complexity of this algorithm increases with the speed of nodes. The algorithm does not require any knowledge of the topology of the system. These results confirm the intuition that the fastest the nodes move, the more expensive it would be to solve the geocasting problem and if nodes move too fast then no solution can be achieved.

2

A Model for Mobile Ad-Hoc Networks

We consider a system of (mobile) nodes which move with bounded speeds in a continuous manner on the plane. There is no known upper bound on the number of nodes in the system and nodes do not fail. Nodes communicate by exchanging messages over a wireless radio network. To define neighbourhood of nodes, let distance(p, p , t) denote the physical distance between two nodes p and p at time t. Two nodes p and p are neighbours at some time t, if distance(p, p , t)< r, for fixed r > 0. We assume that each node can have at most H neighbours at each time. Nodes do not have access to a global clock, instead they have (not necessarily synchronized) local clocks which run at the same rate. Within a small time period, called a round, a node can execute in a sequential and atomic manner receiving at most H messages, broadcasting at most one message, and local computation. To perform a local broadcast of a message m, a node p is provided with a primitive denoted broadcast (m). It takes at least one round for a broadcast message m to be received by a node which then generates a receive (m) event. For simplicity of presentation, the duration of a round is one time unit (i.e., in [t, t + i], i rounds have elapsed). If broadcast (m) is performed by node p at time t then all nodes that remain neighbours of p throughout [t, t + T ] receive m by time t + T , for some fixed integer T > 0. It is possible that some nodes that are neighbours of p at times in [t, t + T ] also receive m but no node receives m after time t + T . If two or more nodes perform broadcasts concurrently there may be interference and messages may be lost. We assume this to be dealt by a lower level communication layer [9] within the T rounds it takes for a message to be (reliably) delivered to its destination. There is no other way that messages can be lost. Connectivity. The standard definition of connectivity, called weak connectivity, ensures that for every pair of nodes p and p and every time t, there is at least one path of neighbours connecting p and p at time t. Weak connectivity allows an adversary to continually change the neighbourhood of nodes and render impossible even the basic task of geocasting (Theorem 1). For this reason, we assume a stronger version, called strong connectivity. To define this, first, we introduce the notion of strong neighbours. If there is an upper bound on the speed of nodes, then the closer two neighbours are located to each other, the longer they will remain neighbours. Hence, if nodes are located fairly close, then their connection is guaranteed for some period of time. Formally, Definition 1 (Strong Neighbours). Let 2 = r and 1 be fixed positive real numbers such that 1 < 2 . Two nodes p and p are strong neighbours at some

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks

51

time t, if there is a time t  t such that distance(p, p , t ) 1 and distance(p, p , t )< 2 for all t  [t , t]. Assumption 1 (Strong Connectivity). For every pair of nodes p and p and every time t, there is at least one path of strong neighbours connecting p and p at t. Two nodes strongly connect when they become strong neighbours and they lose their connection or disconnect when they cease being neighbours. By increasing 1 , the set of strong neighbours of each node either remains the same or increases. This is desirable, because then strong connectivity is not too much stronger than weak connectivity. Therefore, for practical applications, we would like to design algorithms considering values of 1 that are as large as possible. Because of this, 2 in this paper, we assume 1  2 . Mobility. We assume an upper bound on the speed of node movement which exists in practical situations. Then, Lemma 1 describes some topological stability. Formally, Assumption 2 (Movement Speed). It takes at least T > 0 rounds for a 1 node to travel distance  = 2 - on the plane. 2 From Definition 1 and Assumption 2, we gain some topological stability in the network, which is formally expressed in the following lemma: Lemma 1. If two nodes become strong neighbours at time t, then they remain (strong) neighbours throughout [t, t + T ] (i.e., for T rounds). Proof. If p and p become strong neighbours at time t, then distance(p, p , t)= 1 . To disconnect, they must move away from each other so that their distance is larger than or equal to 2 (traversing in total distance at least 2 ). From assumption 2, this takes at least T rounds when they travel in opposite directions.

3

The Geocast Problem

The goal of geocasting is to deliver information to nodes in a specific geographical area. The geocast problem can be solved by a geocast service, implemented by a geocast algorithm which runs on mobile nodes. The geocast service supports each mobile node with two primitives: Geocast(I, d) to geocast information I at distance d and Deliver(I ) to deliver information I . As illustrated in Figure 1, on each mobile node there is a process running the geocast algorithm and a co-located user of the service which invokes geocast. The geocast algorithm uses broadcast (m) and receive (m) to achieve communication among neighbours.

52

R. Baldoni, K. Ioannidou, and A. Milani

Mobile Node USER

Geocast (I, d)

Deliver (I )

broadcast (m)
WIRELESS NETWORK

GEOCAST SERVICE

receive (m)
Fig. 1. System Architecture

3.1

A Geocast Specification

The geocast information is initially known by exactly one node, the source. If the source performs Geocast(I, d) at time t from location l, then: Property 1 (Reliable Delivery). There is a positive integer C such that, by time t + C , information I is delivered by all nodes that are located at distance at most d away from l throughout [t, t + C ]. The following properties rule out solutions which waste resources causing continuous communication or distribution of information I among all nodes. Property 2 (Termination). If no other node issues another call of geocast then there is a positive integer C such that after time t + C , no node performs any communication triggered by a geocast (i.e. local broadcast). Property 3 (Integrity). There is d > d such that, if a node has never been within distance d from l, it never delivers I . 3.2 A General Framework for Geocasting Algorithms

We present a framework, (k, )-Geocast(I, d) for   1, that describes a large class of geocast algorithms. When the source invokes (k, )-Geocast(I, d), k messages containing I are broadcast, once every  rounds. When a node receives a message containing I , k broadcasts of messages containing I are generated, once every  rounds as long as some condition (described by a boolean function CHECK ) holds. CHECK can be different for each algorithm in this class. More precisely, for each call of Geocast(I, d), each node p stores a variable timep,I , and a boolean variable f lagp,I , initially set to  and 0, respectively. We denote clockp the current value of the physical clock at p. When source s executes (k, )-Geocast(I, d), times,I is set to clocks and every  rounds f lags,I is set to 1 (illustrated in Figure 2 (a)). This causes the broadcast of a message m containing information I (illustrated in Figure 2 (b)).

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks (k, ) - Geocast(I, d) by s 1 times,I  clocks ; 2 for (i = 1; i + +; i  k) 3 when (clocks == times,I + [(i - 1))]) 4 f lags,I  1; (a) when (f lags,I == 1) 1 trigger broadcast(m) ; % I  m % 2 f lags,I  0; (b) Fig. 2. (k, )-Geocast(I, d) algorithm performed by the source s

53

The first time a node p executes receive (m), timep,I gets the value of clockp . Any time p receives a message with information I , if CHECK is true, it sets f lagp,I to 1, every  rounds for k times (illustrated in Figure 3 (a)), which in turn causes a broadcast of a message containing I (illustrated in Figure 3 (b)). After each such broadcast, f lagp,I is set to 0. Note that if p receives more than one message containing I within  rounds, only one broadcast is triggered. Hence at most one broadcast happens at p every  rounds. The above is ensured by setting f lagp,I to 1 only at certain times as shown in line 7 of Figure 3 (a). In particular, f lagp,I is set to 1, k times, starting at the closest time after tp,I that is equal to timep,I + j (where j is an integer).
upon event receive(m) by p trigger Deliver(I ) ; % I is contained in m % tp,I  clockp ; if (timep,I == ) then timep,I  tp,I ; if (CHECK ) then for (i = 1; i + +; i  k) t -time when (clockp == timep,I + [ p,I  p,I + (i - 1)]) f lagp,I  1; (a) 1 2 when (f lagp,I == 1) by p trigger broadcast(m) ; % I  m % f lagp,I  0; (b) Fig. 3. (k, )-Geocast(I, d) algorithm performed by node p

1 2 3 4 5 6 7 8

54

R. Baldoni, K. Ioannidou, and A. Milani

4

Impossibility Results

End-to-end communication is impossible if the system remains disconnected. Eventual connectivity [10] ensures the existence of a path between sender and receiver with edges which transmit infinitely many messages if infinitely many messages are sent through. Eventual connectivity is necessary for achieving endto-end communication in general networks. For our mobile ad-hoc network, we show that it is impossible to solve the geocast problem using algorithms in (k, ) - Geocast under weak connectivity, or under strong connectivity if nodes move too fast. To do so, we relate the speed of movement (which is inversely related to T ) to the speed of communication (which is inversely related to T ). We also show how the speed of nodes relates to the cost of any (k, ) - Geocast algorithm. For the following impossibility results, we set CHECK to true because if reliable delivery is impossible when the maximum broadcasts are allowed, it is also impossible for less broadcasts. The fact that the (k, ) - Geocast class of algorithms contains a large class of natural geocasting algorithms (including existing ones) makes our impossibility results significant for practical applications. We note that the following lower bounds are not necessarily tight. Theorem 1. No algorithm in (k, ) - Geocast(I, d) can solve the geocast problem under the weak connectivity assumption no matter how slowly the nodes move. Proof. Assume that the maximum speed of the nodes is v > 0. Consider a state, spq , such that all nodes are located on a straight line. The source s is the leftmost node at position l. The only neighbour, p, of s is on its right at distance r - d {,T } from l, at position l1 such that d  v min2 . There is a node q located on the right of p at distance d from p at position l2 , as illustrated in Figure 4. {,T } Because d  v min2 , distance 2d can be traversed during min{, T } d rounds. From state spq at time t, node q moves with speed min2 {,T } until it

,T } . Then, node p moves away from l with reaches location l1 at time t + min{2 2d speed min{,T } until it reaches location l2 at time t + min{, T }. The state, sqp , reached is the same as spq if we replace p by q and q by p. Weak connectivity is preserved. Because the switch between spq and sqp takes min{, T } rounds, and according to the algorithm at most one broadcast can be initiated every  rounds, it is possible to create an execution where the source starts a local broadcast either at state spq , or at state sqp and its neighbourhood changes within min{, T } rounds. This implies that no node ever delivers I . In particular, if  > T then there can be at most one message broadcast every T rounds and this message will be lost because the neighbourhood changes within min{, T } = T rounds. Otherwise,   T . Since there is at most one message broadcast every  rounds, every such message will be lost because the change in the neighbourhood happens within min{, T } =   T rounds.

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks

55

l state spq at time t: state at time ,T } t + min{ : 2 state sqp at time t + min{, T }:

r

l1 d
p

l2

s s s

q

q
p

q

p

Fig. 4. Proof of Theorem 1
2 As stated in Section 2, we assumed that 1  2 which is reasonable for practical applications. The following results in this section hold given this assumption. Our lower bounds would be stronger if they held for all values of 1 . This extension would be of theoretical interest and we leave it as future work.

Theorem 2. No algorithm in (k, ) - Geocast(I, d) can solve the geocast problem if T < T 4 even if strong connectivity holds. Proof. Consider a (k, )-Geocast(I, d) algorithm executed at time t by the source s. We will describe an execution of this algorithm (illustrated in Figure 5) during which no node (other than s) knows information I , violating reliable delivery. Let spq be the state at time t with the following properties: all nodes are located on a single line; the source, s, is the leftmost node located at position l; the first node, p, located on the right of l is at position l1 at distance 1 from l; the second node, q , located on the right of l is at position l2 at distance 2 from l and at distance 2 - 1 = 2 from l1 ; all other nodes of the system are located on the right of s at distance at least 1 + 2 from l. Node p is the only (strong) neighbour of s. Nodes s and q are the only (strong) neighbours of p, p is the only (strong) neighbour of q located on the left of q at time t, and the remaining (strong) neighbours, Q, of node q are located on its right at distance exactly 1 . We conclude that strong connectivity holds at state spq . Assume that, from state spq , p moves from l1 to l2 and q moves from l2 to l1 on a straight line with their highest speed. Each of them will traverse a path of distance 2 and arrive at its destination at time t + 2T (by the communication speed assumption). Strong connectivity holds throughout [t, t + 2T ] because throughout [t, t + 2T ), the sets of strong neighbours of every node in the system does not change, and the state, sqp , reached at time t + 2T is the same as the state at time t if we replace p by q and q by p. If the above movement happens continually, then for any even integer i, we reach state spq and for any odd integer i, we reach state sqp at time ti = t + 2T i.

56

R. Baldoni, K. Ioannidou, and A. Milani

l state spq at time ti : state at some time in (ti , ti+1 ): state sqp at time ti+1 :

2 1

l1 2 p q q

l2

2 1 q Q Q Q

s s s

p p

Fig. 5. Proof of Theorem 2

Let t be a time at which the source s performs a (local) broadcast during its call of (k, )-Geocast(I, d). We consider the following two cases for i = odd (the proof for i = even is symmetrical): ­ There is i such that ti = t . Because i is odd, the system is in state sqp at time ti , it reaches state spq at time ti+1 , and ti+1 - ti = 2T . At time t , q is the only neighbour of s. Node q will stop being a neighbour of s at time ti+1 = ti + 2T = t + 2T which happens before time t + T because T < T 4. Therefore q will not receive the message being broadcast by s at time t . ­ Otherwise, there is i such that ti < t < ti+1 . Because i is odd, the system is in state sqp at time ti and the only neighbours of s at time t are p and q . But node q will cease being a neighbour of s at time ti+1 and node p will cease being a neighbour of s at time ti+2 . The local delivery of the broadcast message completes at time t + T . Node q will not receive the message broadcast at time t by s because t + T > ti + 2T = ti+1 . Similarly, p will not receive this message because t + T > ti + 4T = ti+2 . In both cases, any node that will become a neighbour of s after time t will not receive the broadcast message either. Theorem 3. No algorithm in (k, ) - Geocast(I, d) can solve the geocast problem if T < T 1 , for a system with unbounded number of nodes even if strong connectivity holds. Proof. We describe an execution (illustrated in Figure 6) during which all nodes are placed on a straight line and a node receives a message containing I if and only if it is located on or on the left of the original location, l, of the source s = p0 . In this execution, there is a node, q , always located on the right of this position at distance less than d, and hence, never delivers I , violating reliable delivery. Initially, at time t = t0 , the nodes are placed on a line on the right of

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks

57

nodes with information nodes without information
d 1 t0 = t s 1 t1 = t + T s p1 1
...

1 pi q 1 pi+1 q pi+2
...

p1

pi+1

...

...

p2

Fig. 6. Proof of Theorem 3

q0 , one every 1 distance, with the exception of q . Let pi be the node located at distance i1 on the right of l at time t0 (for i  0). At time t0 , the only 2 neighbours of p0 are p1 and possibly q because, since 1  2 , all other nodes are at distance at least 2 from p0 . Similarly, at time t0 , the only neighbours of pi (for i  1) are pi-1 , pi+1 and possibly q . All nodes pi for (i  0) move 1 continually, with speed  T towards the left. Note that this is possible because T  1 T < 1 which implies that  T is smaller than the maximum speed (i.e., T ). All other nodes pi for i  0 form a path such that each two consecutive nodes are strong neighbours. Furthermore, q is always a strong neighbour of the first node on its right throughout the execution because their distance is at most equal to 1 . We conclude that strong connectivity holds. At time t0 only p0 (at location l) knows I . Node p1 delivers I at time t1 = t + T when it is at location l. This is because during T rounds, p1 moves distance T 1 T = 1 and it moves towards the left starting from a location at distance 1 on the right of l. At time t1 , both p0 and p1 will rebroadcast messages with information I . Similarly, node pi is the rightmost node to deliver I at time ti = t + iT when at location l. All other nodes that delivered I are on the left of location l at that time. Since q is never a neighbour of any node on or on the left of position l, it will never deliver I .
1  Theorem 4. Assuming that T > max{ 4 , 1 }T , then if it is possible to solve d-2 geocast, it would take more than (  - T  + 1)T rounds to ensure reliable de1 T

livery, using any (k, )-Geocast(I, d) algorithm for a system with more than d-2 nodes even if strong connectivity holds.  - T
1 T

Proof. We describe an execution (illustrated in Figure 7) of a geocast algorithm that causes as much rebroadcasting as possible and which cannot guarantee -2 reliable delivery in less than ( d- + 1)T rounds. During this execution there T
1 T

58

R. Baldoni, K. Ioannidou, and A. Milani

nodes with information nodes without information
d 1 t0 = t s 1 s p1
T T

1 p2

...

1
...

pi 1

q pi+1
...

t1 = t + T

p1 1

p2

...

pi
 iT T

q pi+1 ... pi+1 q

ti = t + iT

...

pi

1

Fig. 7. Proof of Theorem 4

is a node, q , located exactly at distance d from the original location, l, of the source, s = q0 . At time t0 , the nodes (other than q ) are placed on a line on the right of q0 , one every 1 distance. Let pi be the node at distance i1 on the right of l at time t0 (for i  0). At time t0 , the only neighbours of p0 are p1 2 and possibly q because (since 1  2 ) all other nodes are at distance at least 2 from p0 . Similarly, at time t0 , the only neighbours of pi (for i  1) are pi-1 , pi+1 and possibly q . All nodes pi for (i  0) move continually, with their maximum  speed (i.e., T ) towards the left. Strong connectivity holds because, all nodes pi (for i  0, other than q ) form a path of strong neighbours and q is a strong neighbour of the first node on its right throughout the execution because their distance is at most equal to 1 . At time t = t0 only p0 knows I . Node p1 first delivers I at time t1 = t + T  when it is at distance 1 - T T on the right of l . Node pi is the rightmost node to  deliver I at time ti = t + iT when at distance i1 - i T T on the right of l . Node q can only deliver I within T rounds after at least one of its neighbours has delivered I . The earliest this happens is within T rounds after I is delivered by a neighbour of q on its left. This neighbour has to be at distance smaller than 2 from q . Hence, reliable delivery cannot happen before time tj + T (= t + (j + 1)T ) d-2  for the smallest possible j for which d - (j1 - j T T ) < 2 (i.e., j >  - T  ).
1 T

Theorem 4 verifies the intuition that the larger the speed of the nodes can be (which is inversely related to T ) the more time it would take to solve geocasting.

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks

59

5

A Geocasting Algorithm

We consider a special case of the mobile ad-hoc model, called one-dimensional mobile ad-hoc model, for which the nodes move on a line. Inter-vehicle communication [11] is an application of geocast in this model. For simplicity, the line is straight and horizontal and the locations are real numbers representing points which increase towards the right. We show that (7, T ) - Geocast(I, d) works if T > 9T . We attach a counter, cmsg , to each message which is set to zero only in the first message broadcast by the source. Each node maintains, in a local counter, the largest counter value it has either received or broadcast. Every time it is ready to broadcast (i.e. its flag is set to 1), it increments its local counter by one and appends this new value to the message. Upon receiving a message with counter cmsg , the receiver evaluates CHECK which returns true iff cmsg  6T (i + 1) + 2T , where i =  -d6T . Assume that the source s = q0
1

initiates a call of (7, T ) - Geocast(I, d) at time t = t0 from location l = l0 . Next, we prove that I propagates from l0 towards the right of l0 . (For the left of l0 , the proof is symmetrical.) This happens in steps so that within a small period of time, I moves from a node, qj (at time tj and location lj ), to another node, qj +1 at some large distance away. The proofs of lemmata 2, 3, and 4 used for correctness appear in the full version of the paper [12]. Lemma 2. If T > 9T and node qj delivers I at time tj when at location lj then, assuming that CHECK returns true for all nodes throughout [tj , tj +1 ], there is a node qj +1 which delivers I at time tj +1 at location lj +1 such that tj +1 - tj  6T and lj +1 - lj  1 - 6T /T .

T

Lemma 3. If T > 9T and there is a j such that a node p is located in [lj , lj +1 ] at some time t  [tj , tj +1 ], then, assuming that CHECK returns true for all nodes throughout [tj , tj +1 ], p delivers I by time tj +1 + 2T . Lemma 4. If a node q stays within distance d from l throughout [t0 , ti+1 ] for i such that l + d  [li , li+1 ], then there is j  i such that q is located at some position in [lj , lj +1 ] at some time in [tj , tj +1 ]. Theorem 5. If T  9T , then (7, T ) - Geocast(I, d) ensures reliable delivery for C  6T (i + 1) + 2T rounds, where i =  -d6T .
1 T

take in any execution such that li  l + d (i.e., (l + d)  [li , li+1 )). Next, we show that it suffices that I gets delivered and rebroadcast by a node at location i -l and because li+1 . From Lemma 2, li - l  i(1 - 6T /T ). Then i   l- 6T
1

Proof. During [t, t + C ], any node's CHECK =true because if geocast starts at time t, then (by induction on time) during [t, t + C ], all messages broadcast or received have counters at most equal to C . Then, we show that C  6T (i + 1) + 2T , where i =  -d6T . First, we calculate the maximum value that i could
1 T

li - l  d, i 

distance d from l(= l0 ) throughout [t, t + C ], also remain within that distance

d T 1 - 6T

. It remains to calculate C . All nodes that remain within

T

60

R. Baldoni, K. Ioannidou, and A. Milani

throughout [t0 , ti+1 ], (recall that t = t0 ). If p remains in this area throughout [t0 , ti+1 ] then from Lemma 4, there is a j such that p is located at some position in [lj , lj +1 ] at some time in [tj , tj +1 ] for j  i and from Lemma 3, p delivers I by time tj +1 + 2T . Therefore, since j  i, all nodes within distance d from l deliver I by time ti+1 + 2T = t + C . By Lemma 2, ti+1 - t  6T (i + 1) and C  6T (  -d6T + 1) + 2T .
1 T

Theorem 6. If T  9T then (7, T ) - Geocast(I, d) ensures termination for C = (6T (i + 1) + 2T + 1)T + 8T rounds, where i =  -d6T .
1 T

Proof. Every message received causes rebroadcasting of I in a message with counter at least incremented by one and this will happen at least once every T rounds (for at least 7 times). Termination happens within 7T rounds from the time after which any message received has counter larger than 6T (i + 1) + 2T , where i =  -d6T . This happens within (6T (i + 1) + 2T + 1)T + T rounds,
1

because all messages broadcast after time (6T (i + 1) + 2T + 1)T have counters at least equal to 6T (i + 1) + 2T + 1 and all such messages are received within at most another T rounds. Therefore, C = (6T (  -d6T + 1) + 2T + 1)T + 8T
1

T

rounds.

T

Theorem 7. If T  9T then (7, T ) - Geocast(I, d) ensures integrity. Proof. A broadcast message will be received at least after one round during  which any node can traverse distance at most T . Therefore, if a node broadcasts a message from location l at time t , then its neighbours receive it the earliest  at time t + 1, when at distance less than 2 + T away from l . Then, if the source starts (7, T ) - Geocast(I, d) at time t from location l, at time t + m, the  ) away from l. furthest node that delivers I is at distance less than m(2 + T By Theorem 6, after time t + C , no node broadcasts messages with information I . Therefore, no node delivers I after time t + C + T . But at time t + C + T ,  all nodes that have delivered I are within distance less than (C + T )(2 + T )  from l. Therefore, if a node remains further than d = (C + T )(2 + T ) from l, it will never deliver I .

6

Related Work

Geocast was introduced by Navas et al. [2,1]. Geocast algorithms for mobile ad-hoc networks [3,7,5,4], unlike our deterministic solution, only provide probabilistic guarantees. This may not suffice. For example, Dolev et al. [6] need deterministic geocast to implement atomic memory. Deterministic solutions are given for multicast [13,14,15] and broadcast [8] for mobile ad-hoc networks. Both solutions in [13,14] consider a finite and fixed number of mobile nodes arranged somehow in logical or physical structures. They divide the nodes into groups each of which has a special node which coordinates message propagation and collects acknowledgments. Moreover, they make the following stronger than necessary assumption: they require that the network topology stabilizes for periods

Mobility Versus the Cost of Geocasting in Mobile Ad-Hoc Networks

61

long enough to ensure delivery. Finally, simulation results [16] show that the approach proposed in [13] does not work if nodes move fast. Bounds that allow the algorithms to work correctly are not presented. Chandra et al. [15] provide a broadcasting algorithm and show by experiments that either all or none of the nodes get the message with high probability. Mohsin et al. [8] implement (deterministic) broadcast for a synchronous mobile ad-hoc network with restricted movement patterns. In particular, nodes move on top of a grid such that at the beginning of each round nodes are located at grid points. They assume that all nodes move at the same constant speed and direction of movement cannot change within a round. Finally, nodes need to inform their neighbours about their future moving pattern for short future time periods.

7

Conclusion and Future Work

To the best of our knowledge, this is the first time in which bounds are formally defined on the speed of node movement which make it possible to solve geocasting and relate its time complexity to the speed. This formally verifies that the faster nodes move, the most costly it would be to solve geocasting. Our upper bounds and lower bounds do not match neither for the cost of geocast, nor for the bounds on speed of movement. Although the gap is not large, it would have theoretical interest to match these bounds. We proved our results for the case 2 2 . It is unknown whether our lower bounds still hold for 1 < 2 . where 1  2 Another future direction would be to design a geocast algorithm that works for a two-dimensional model including failures.

References
1. Imielinski, T., Navas, J.C.: Gps-based geographic addressing, routing, and resource discovery. Communication of the ACM 42(4), 86­92 (1999) 2. Navas, J.C., Imielinski, T.: Geocast: geographic addressing and routing. In: Proceedings of the 3rd Annual ACM/IEEE International Conference on Mobile Computing and Networking (MobiCom), pp. 66­76. ACM Press, New York (1997) 3. Ko, Y., Vaidya, N.H.: Geotora: a protocol for geocasting in mobile ad hoc networks. In: Proceedings of the 8th International Conference on Network Protocols (ICNP), p. 240. IEEE Computer Society Press, Los Alamitos (2000) 4. Ko, Y., Vaidya, N.H.: Flooding-based geocasting protocols for mobile ad hoc networks. Mobile Network and Application 7(6), 471­480 (2002) 5. Liao, W., Tseng, Y., Lo, K., Sheu, J.: Geogrid: A geocasting protocol for mobile ad hoc networks based on grid. Journal of Internet Technology 1(2), 23­32 (2001) 6. Dolev, S., Gilbert, S., Lynch, N., Shvartsman, A., Welch, J.: Geoquorum: Implementing atomic memory in ad hoc networks. In: Proceedings of the 17th International Conference on Principles of DIStributed Computing (DISC), pp. 306­320 (2003) 7. Boleng, J., Camp, T., Tolety, V.: Mesh-based geocast routing protocols in an ad hoc network. In: Proceedings of the 15th International Parallel & Distributed Processing Symposium (IPDPS), pp. 184­193 (April 2001)

62

R. Baldoni, K. Ioannidou, and A. Milani

8. Mohsin, M., Cavin, D., Sasson, Y., Prakash, R., Schiper, A.: Reliable broadcast in wireless mobile ad hoc networks. In: Proceedings of the 39th Hawaii International Conference on System Sciences (HICSS), p. 233.1. IEEE Computer Society Press, Los Alamitos (2006) 9. Koo, C., Bhandari, V., Katz, J., Vaidya, N.H.: Reliable broadcast in radio networks: the bounded collision case. In: Proceedings of the 25th Annual ACM Symposium on Principles of Distributed Computing (PODC), pp. 258­264. ACM Press, New York (2006) 10. Ellen, F.: End to end communication. In: Proceedings of the 2nd International Conference On Principles Of Distributed Systems (OPODIS), Hermes, pp. 37­44 (1998) 11. Benslimane, A.: Optimized dissemination of alarm messages in vehicular ad-hoc networks (vanet). In: Proceedings of the 7th IEEE International Conference on High Speed Networks and Multimedia Communications (HSNMC), pp. 655­666 (2004) 12. Baldoni, R., Ioannidou, K., Milani, A.: Mobility versus the cost of geocasting in mobile ad-hoc networks. Technical report, 3/07 MIDLAB - Universit di Roma "La Sapienza" (2007) 13. Pagani, E., Rossi, G.P.: Reliable broadcast in mobile multihop packet networks. In: Proceedings of the 3rd Annual ACM/IEEE International Conference on Mobile Computing and Networking (MobiCom), pp. 34­42. ACM Press, New York (1997) 14. Gupta, S.K.S., Srimani, P.K.: An adaptive protocol for reliable multicast in mobile multi-hop radio networks. In: Proceedings of the 2nd Workshop on Mobile Computing Systems and Applications (WMCSA), p. 111. IEEE Computer Society Press, Los Alamitos (1999) 15. Chandra, R., Ramasubramanian, V., Birman, K.P.: Anonymous gossip: Improving multicast reliability in mobile ad-hoc networks. In: Proceedings of the 21st International Conference on Distributed Computing Systems (ICDCS), pp. 275­283. IEEE Computer Society Press, Los Alamitos (2001) 16. Pagani, E., Rossi, G.P.: Providing reliable and fault tolerant broadcast delivery in mobile ad-hoc networks. Mobile Networks and Applications 4(3), 175­192 (1999)

Self-stabilizing Counting in Mobile Sensor Networks with a Base Station
Joffroy Beauquier, Julien Clement, Stephane Messika, Laurent Rosaz, and Brigitte Rozoy
Univ. Paris Sud, LRI, UMR 8623, Orsay, F-91405, CNRS, Orsay, F-91405 Abstract. Distributed computing must adapt its techniques to networks of mobile agents. Indeed, we are facing new problems like the small size of memory and the lack of computational power. In this paper, we extend the results of Angluin et al (see [4,3,2,1]) by finding self-stabilizing algorithms to count the number of agents in the network. We focus on two different models of communication, with a fixed base station or with pairwise interactions. In both models we decide if there exist algorithms (probabilistic, deterministic, with k-fair adversary) to solve the self-stabilizing counting problem.

1

Introduction

Habitat and environmental monitoring represents a class of sensor network applications with enormous potential benefits both for scientific communities and for society as a whole. The intimate connection with its immediate physical environment allows each sensor to provide localized measurements and detailed information that is hard to obtain through traditional instrumentation. Many environmental projects use sensor networks. The SIVAM project in Amazonia is related to meteorological predictions, sensors are placed in glacial areas for measuring the impact of the climate evolution, (see [9]), use of sensors is considered in Mars exploration (see [10]) or for detecting the effect of the wind or of an earthquake on a building (see [11]). A sensor network has been deployed on Great Duck Island (see [8]) for studying the behavior of Leachs Storm Petrels. Seabird colonies are notorious for the sensibility to human disturbance and sensor networks represent a significant advance over traditional methods of monitoring. In [1], Angluin et al. introduced the model of population protocols in connection with distributed algorithms for mobile sensor networks. A sensor is a package containing power supply, processor, memory and wireless communication capacity. Some physical constraints involve limitations of computing or storage capacity and communication. In particular, two sensors have to be close enough to be able to communicate. A particular static entity, the base station, is provided with more computing resources. The codes in the base station and in the sensors define what happens when two close sensors communicate and how they communicate with the base station. An important assumption made in this model is that the interactions between
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 63­76, 2007. c Springer-Verlag Berlin Heidelberg 2007

64

J. Beauquier et al.

the sensors themselves and between the sensors and the base station are not controlled. Also, a hypothesis of fairness states that in an infinite computation the numbers of interactions between two given sensors or between a particular sensor and the base station are infinite. Eventually the result of the computation is stored at the base station and does not change any more. This model takes into account the inherent power limitation of the real sensors and also the fact that they can be attached to unpredictably moving supports. For being still more realistic the model should consider the possibility for the sensors to endure failures. Temperature variations, rain, frost, storm, etc. have consequently, in the real world, that some sensors are crashed and that some others are still operating, but with corrupted data. Most of population protocols do not consider the possibility of failures. The aim of this paper is to perform computation in mobile sensor networks subject to some type of failures. The framework of self-stabilization is particularly well adapted for dealing with such conditions. A self-stabilizing system, after some memory corruptions hit some processors, regains its consistency by itself, meaning that first, (convergence) it reaches a correct global configuration in a finite number of steps and second, (correction) from then its behavior is correct until the next corruption. It is important to note that this model assumes that the code is immutable, e.g, stored in a ROM and then cannot be corrupted. Traditionally self-stabilization assumes that failures are not too frequent (for giving enough time to the system for recovery) and thus the effect of a single global failure is considered. That is equivalent to consider that the system may be started in any possible global configuration. Note that the issue of combining population protocols with self stabilization has been addressed for ring networks in [4] and in a different framework in [6]. In the present work we make the assumption that, if the input variables can be corrupted, as any other variable, first they do not change during the time of the computation and second they are regularly read by the sensor. Then eventually a sensor deals with its correct input values. In this paper we consider the very basic problem of computing the number of (not crashed) sensors in the system, all sensors being identical (same code, no identifiers), when their variables are arbitrary initialized (but the input value of each sensor is 1). This problem is fundamental, first because the ability of counting makes easier the solution of other problems (many distributed algorithms assume that the size of the network is known in advance) and second because if counting is feasible, sum, difference and test to 0 are too. In practice, one might want to count specific sensors, for example those carried by sick petrels. We present a study of this problem, under slightly different models. The variations concern the determinism or the randomization of the population protocols. In a sub model, the sensors only communicate with the base station and in another they communicate both between each other and with the base station. According to the different cases, we obtain solutions or prove impossibility results.

Self-stabilizing Counting in Mobile Sensor Networks with a Base Station

65

2

Motivation and Modelization

Imagine the following scenario : A group of birds (petrels) evolves on an island, carrying on their body a small sensor. Whenever a petrel is close enough to the base station, its sensor interacts with the base station, which can read the value of the sensor, compute, and then write in the petrel sensor memory. Depending on the hypothesis, the sensors may or may not interact with each other when two petrels approach close enough. 2.1 Mobile Sensor Networks with a Base Station

A mobile sensor network is composed of an base station, and of n undistinguishable mobile sensors (In the sequel we will use the term of petrel, in relation with our motivation example, instead of sensor.) The network configuration considers the memory content of the base station, a, and the petrels' state, pi . We denote the network configuration by (a, p1 , ..., pn ) where pi is the state of the ith petrel. There are two kinds of events: · the meeting of petrel number i with the base station. After that meeting, pi is changed, according to the protocol, to pi , and a to a , depending on (a, pi ) (Note that the transition is independent of i, because petrels are not distinguishable). · the meeting of petrel number i with petrel number j . After that meeting, pi and pj are changed to pi and pj , depending on (pi , pj ) (here again, independently of (i, j )). In the Sensors-To-base-station-Only model (TB for short), only the first kind of event is possible. i.e. the sensors do not interact with each other. In the petrels-To-Base-station-and-To-Petrels model (TBTP for short), both events are possible: sensors do interact with each other. For deterministic protocols, the last model can be divided into two sub-models, the symmetric (STBTP), resp. the asymmetric one (ATBTP): When two petrels meet, if their state is the same, they have to, resp. they don't have to, change to the SAME state. A probabilistic algorithm can use coin flips and perform an election between meeting petrels to simulate the Asymmetric TBTP model. 2.2 The Problem

The number of petrels is unknown from the base station, which aims at counting them. We want that, eventually, the P etrelN umber variable in the base station is and remains equal to n. In probabilistic algorithms (we consider non-oblivious daemons, that is, they can decide what is the next event depending on previous results of coin flips), we require that this property is obtained with probability 1. More generally, our algorithms must be self-stabilizing (see [7]), i.e., whatever the initial configuration (but we initialize the base station), the base station must give the exact number of petrels in the network (with probability 1, for probabilistic algorithms) within a finite number of steps. This requirement does not allow us to make any assumption on the initial configuration (except for the base station), or to reset the value of the sensors.

66

J. Beauquier et al.

2.3

Executions, Daemons, Fairness, Rounds

Definition 1 (Execution). An execution is an infinite sequence (Cj )j N (where N denotes the set of non-negative integers) of configurations and an infinite sequence (ej )j N\{0} of events such that Cj +1 is obtained after ej occurs on Cj . The daemon is the imaginary adversary that chooses the initial configuration and that schedules the possible actions at every step. To solve the problem, the daemon must be fair: Definition 2 (Fairness). An execution is fair if every petrel communicates with the base station infinitely often, and, in the TBTP model, if every two petrels communicate with each other infinitely often. (Note that this fairness is weaker than the one used by Angluin et al., which says that a configuration that is reachable infinitely often is eventually reached) ·A daemon for a deterministic protocol is fair if every execution is fair. ·A daemon for a probabilistic protocol is strongly fair if every execution is fair and it is weakly fair if the measure of the set of the fair executions is one. The distinction between weak and strong fairness is of little importance in this paper. Definition 3 (k-fairness). Let k be an integer. An execution is k-fair, if every petrel communicates with the base station at least once in every k consecutive events, and, in the TBTP model, if every two petrels communicate with each other in every k consecutive events. A daemon is k -fair if the execution is k -fair. In this paper when the daemon is k -fair, the value of k is not assumed to be known by the base station. Throughout the paper, the daemon is assumed to be fair, unless it is explicitly assumed to be k -fair. Definition 4 (Oblivious). For probabilistic algorithms, a daemon is nonoblivious if the decision of what is the next event can depend on the result of previous coin flips. An oblivious daemon could be able to decide at the start of the execution what the whole sequence of events will be. Definition 5 (Rounds). A round is a sequence of consecutive events, during which every petrel meets the base station at least once, and in the TBTP model, every two petrels meet each other. The first round is the shortest round starting from initial configuration, the second round is the shortest round starting from the end of the first round, and so on. 2.4 Initial Conditions

Throughout the paper, we assume that the petrels are arbitrarily initialized, but that an initial value can be chosen for the base station. This assumption is

Self-stabilizing Counting in Mobile Sensor Networks with a Base Station

67

justified if one thinks of mobile sensors networks as the petrel population and the base station. The existence of a base station and the possibility to initialize it are the main differences between our model and classical sensor networks. Note that if both the petrels and the base station can be initialized , then the problem is obvious, with only one bit per petrel sensor. Note also that if one can initialize neither the petrels nor the base station, then there is no protocol to count the petrels (unless the daemon is k -fair, see remark 1 in Sec. 3). Indeed, assume on the contrary that there is such a protocol. Let the daemon repeat the following: it waits till every petrel has met the base station and P etrel N umber = n (this will eventually happen), then it holds back one particular petrel. When P etrelN umber is n - 1 (this will eventually happen since the configuration is the same as if there were n petrels), the daemon frees the last petrel. With such a daemon, P etrelN umber will never stabilize so the protocol fails. If the protocol is deterministic, the daemon is fair, if the protocol is probabilistic, it is weakly fair. It can also be proved that there does not exist any algorithm under a stronglyfair daemon P etrelN umber will stabilize with probability 0 although the daemon is strongly-fair. 2.5 Memories

We will not make limitation on the memory size of the base station. (Note: The codes will often use "infinite" arrays (indexed by integers), but only a finite number of register will contain non-0 values. Of course, in practice, arrays will have to be replaced by data structures to keep only the non-0 registers.) On the other hand, we will make more or less strong assumptions on the memory size of the petrel sensors: Definition 6 (Size of the petrel sensor memories). The memory is infinite if it is unlimited. In particular, it can carry integers as large as needed, which can drift, that is, which can tend to infinity as times passes by. (this has a practical application only if the drift is slow and there are enough bits in the sensors to carry "large"integers). The memory is bounded if an upper-bound P on the number of petrels is known, and if the number of different possible states of the memory is (P ) for some function . The protocol may use the knowledge of P . The memory is finite if the number of different possible states of the memory is a constant .

3

The Petrels-To-Base-Station-Only Model (TB)

In this section, the sensors can only communicate with the base station. People acquainted with classical sensor networks may question the point of such a model. There are two justifications for looking for solutions in that model : 1. Sensors are meant to be small. To implement that model, sensors only need to carry a device so that the base station can read and write in their memories. All the code can be implemented in the base station.

68

J. Beauquier et al.

2. The decision to run such algorithms can be made by just changing the code in the base station. This is doable even if the sensors are already away. For example, if an observation made of the petrels, given you the idea to count something new and not forecast, you can use a TB algorithm. 3.1 With Infinite Memory

In this subsection, we assume that the petrel sensors (and the base station) have an infinite memory. In this case, there exist self-stabilizing deterministic algorithms to solve the problem. The way the first algorithm works is simple. The drift of integers is fast, and convergence is obtained after two rounds. The second one is a little tricky. The drift is slow, but it converges in about P (i.e. the number of petrels) rounds.
variables [each petrel] number :integer [base station] R : array[integers] of booleans, initialized at 0 [base station] PetrelNumber to maintain as cardinal{i | R[i]=1} [base station] LargestNumber : integer initialized at 0 When a petrel p approaches the base station : if R[number_p] = 1 then R[number_p] <- 0 number_p <- LargestNumber R[LargestNumber] <- 1 LargestNumber ++ Algorithm 1. For Unbounded Memory

3.2

With Finite or Bounded Memory

Under a fair daemon In this paragraph we show that if the daemon only respects fairness, there neither exists a deterministic algorithm nor a probabilistic algorithm making it possible for the base station to count the number of sensors present. Proposition 1. The daemon is supposed to be fair. If the sensors have a finite memory then, there is no deterministic algorithm solving the counting problem. Proof. The idea of this proof is to exhibit two executions resulting from two different initial configurations that will appear to be identical for the base station.The proof is analog to the one of proposition 10, but details are in the full version [5]. Then, it becomes natural to try to build a probabilistic algorithm in order to break the symmetry. Indeed, the daemon has no control on the random, thus we can hope to beat him. Unfortunately, even in this case, there is no solution: Proposition 2. Suppose that the daemon is strongly fair and non-oblivious. If the sensors have a finite memory, then there does not exist any probabilistic algorithm solving the counting problem

Self-stabilizing Counting in Mobile Sensor Networks with a Base Station

69

variables [each petrel] number :integer [base station] R : array[integers] of booleans, initialized at 0 [base station] PetrelNumber to maintain as sum{R[i]} When a petrel p approaches the base station : if R[number_p] > 1 then R[number_p] -number_p ++ R[number_p] ++ Algorithm 2. For Unbounded Memory

Proof. Let us consider a daemon D with n sensors (p1 , . . . , pn ) initialized in I = (x1 , x2 , . . . , xn ). The sensors' memory being finite, for every petrel p, in particular for the last one, there is a state s and a positive real number  such as : P{p goes infinitely often in s}   In order to "confuse" the base station, let the deamon D proceeds as follow with n + 1 sensors (p1 , . . . , pn , pn+1 ): it puts them in the initial configuration I = (x1 , x2 , . . . , xn , s). There is an integer k1 such that with D, with probability at least (1 - ), if pn gets in state s infinitely often, then it gets once in state s during the k1 next events and every sensor has met the base station at least once. The daemon D holds back the sensor pn+1 and for at most k1 events, lets evolve the other n's as would do daemon D until pn gets in state s. If k1 events have been done without pn getting in state s then D has lost (note that the daemon may lose either because s does not appear infintely often with D or because the first occurrence of s arrives too late with D). Otherwise D frees pn+1 and holds pn . The daemon D resumes simulating D with pn+1 instead of pn and as in the first step, but with k1 replaced by k2 such that the probability is now at least  ) instead of (1 - ). The daemon keeps on with that technique, with kl for (1 - 2 th the l step so that the probability is at least (1 - 2l -1 ).  ) > 0. i 2 i=0 In this case, from the point of view of the base station, the execution is indistinguishable from D, so P etrelN umber is eventually equal to n which is wrong. So, the base station has a non null probability to lose. Therefore, D wins with probability  (1 - Note that the proofs work with no assumption on n (the number of petrels) which may be equal to 1. Thus the impossibility is proved both for finite and bounded memories. 3.3 A k-Fair Daemon
l

Under the assumption of fairness, there exists neither a deterministic algorithm nor a probabilistic algorithm. Thus, we have to reduce the capacities of the daemon. If we assume the daemon is k-fair, we will get both deterministic and probabilistic solutions.

70

J. Beauquier et al.

Deterministic algorithm The algorithm 3 is given below.
variables [each petrel] bit : boolean [base station] i, cpt, PetrelNumber : integers [base station] bit_A : boolean, initialized at 0 The base station does : For i from 0 to infinity do cpt <- 0 do 2^i times : wait till a petrel p approaches if bit_p = bit_A then cpt ++ bit_p <- not(bit_p) PetrelNumber <- cpt bit_A <- not(bit_A) Algorithm 3. Deterministic, k-fair daemon

The convergence time is less than 8k . The reader may find details in the full version [5] Remark 1. This algorithm works even if the base station variables are not initialized but a large initial value of i induces a large convergence time. This deterministic algorithm requires an infinite memory of the base station, due to the drift of 2i (and of i). This can be avoided by the following probabilistic algorithm. Probabilistic algorithm, k-fair daemon or oblivious daemon The algorithm is as follows:
variables [each petrel] number, color : integer [base station] R : array[integers] of [0...2] /* 0 stands for empty, others for colors */ initialized at empty [base station] PetrelNumber to maintain as card{j | R[j] is non empty} When a petrel p approaches the base station : h <- the minimum integer such that R[h] = empty if R[number_p] <> color_P /* including if one of them is 0 */ then number_p <- h else if h<number_p then R[number_p]<-0 number_p<-h color_p <- random{1..2} R[number_p] <- color_p

Algorithm 4. Probabilistic, k-fair daemon

Self-stabilizing Counting in Mobile Sensor Networks with a Base Station

71

The proof of convergence is in the full version [5] We obtain a worse time of convergence (possibly exponential) than with the deterministic algorithm but we observe that the base station requires a finite memory.

4

The Petrels-To-Base-Station-And-To-Petrels Model (TBTP)

We recall that P is an upper bound of the number of petrels and (P ) is the number of the different possible states of the memory. In a first section we introduce deterministic algorithms solving the counting problem. Then, in a second part, we get interested in the lowest value (P ) may get. 4.1 Bounded Memory, Algorithms

Proposition 3. There are deterministic solutions, with (P )  P , to the counting problem. We are going to exhibit different algorithms. The two first ones concern the ATBTP model and the third one the STBTP model. It is interesting to note that we need more memory in the STBTP model. The question remains open to know what is the minimal memory required in the symmetric case, and if it really needs to be larger than in the asymmetric case. Explanations of the algorithms are in the full version [5] The ATBTP Model. We propose two algorithms : ­ The first one with (P ) = P + 1, converges in three rounds. ­ The second one with (P ) = P , converges in P + 1 rounds.
variables [each petrel] number :integer in [0..P] [base station] T : array [1..P] of boolean, initialized at 0 everywhere [base station] PetrelNumber to maintain as cardinal{i | T[i]=1 } When a petrel p approaches the base station : if number_p = 0 then number_p <- an integer y such that T[y]=0 T[number_p] <- 1 else T[number_p] <- 1 When two petrels meet : If their numbers are the same then the number of one petrel becomes 0 Algorithm 5. Deterministic asymmetric algorithm with (P ) = P + 1

72

J. Beauquier et al.

variables [each petrel] number :integer in [1..P] [base station] T : array [1..P] of boolean, initialized at 0 everywhere [base station] PetrelNumber to maintain as cardinal{i | T[i]=1 } When a petrel p approaches the base station : T[number_p] <- 1 When two petrels meet : If their numbers are the same integer x then the number of one petrel becomes x+1 mod P Algorithm 6. Deterministic asymmetric algorithm with (P ) = P

The STBTP Model. The following symmetric algorithm with (P ) = 4P converges in three rounds:
variables [each petrel] number :integer in [1..2P] [each petrel] Intention : (Keep,GiveUp) [base station] T array [1..2P] of (Free,Taken,GivenUp), initialized at Free everywhere [base station] PetrelNumber to maintain as cardinal{i | T[i]=Taken } When a petrel p approaches the base station : Depending on Intention_p : Keep : T[number_p] <- Taken /* even if T[number_p] was GivenUp */ GiveUp : T[number_p] <- GivenUp number_p <- a y such that T[y] = Free T[number_p] <- Taken Intention_p <- Keep When two petrels meet : If their numbers are the same integer x and their both intentions are Keep Then their both intentions change to GiveUp

Algorithm 7. Deterministic symmetric algorithm with (P ) = 4P

4.2

Bounded Memory, Minimum Value for (P )

We prove in this section there does not exist asymmetric algorithms with (P )  P - 1. The non-existence of algorithms with (P )  P - 2 is much easier to prove than the non-existence of algorithms with (P ) = P - 1. So let us start with the easier case: Proposition 4. There is no deterministic solution, with (P )  P - 2, to the counting problem. Proof. Assume that there is a solution. Consider an execution E with P - 1 sensors (p1 , . . . , pP -1 ) initialized in the states (x1 , . . . , xP -1 ). There is a state y

Self-stabilizing Counting in Mobile Sensor Networks with a Base Station

73

and two petrels p and p such that infinitely often, p and p will be simultaneously in state y . Now, as a daemon, perform the following execution E with P sensors: Initialize them in (x1 , . . . , xP -1 , y ), then repeat the following: · Hold back petrel pP and proceed as in E until every petrel but pP has met each other petrel but pP , and p and p are in state y . · Free pP , hold back p, proceed as in E with pP instead of p until pP has met every other petrel (but p), and pP and p are again in state y . · Free p, hold back p , proceed as in E with pP instead of p until pP has met p, and pP and p are again in state y . The daemon is fair, and from the point of view of the base station, E and E are identical, thus in E , P etrelN umber will stabilize to P - 1, as in E , which is a wrong result. This is a contradiction. We are now going to look to the case where (P ) = P - 1. Proposition 5. There is no deterministic solution with, (P ) = P - 1, to the counting problem. Proof. Assume on the opposite that there is such a solution. Consider an execution E with P - 1 sensors (p1 , . . . , pP -1 ) initialized in the states (x1 , . . . , xP -1 ). If there is a state y and two petrels p and p such that infinitely often, p and p are simultaneously in state y , then one can conclude as in the previous proof, so we can assume from now on it is not the case. This implies that eventually, say from instant T , all petrels have distinct states. This means first, that, in E , from T , the base station never changes the state of a petrel it meets. Second, the rule when two petrels with different states meet must be that they keep their current state (or exchange them, which is of little effect). Thus the protocol rules for meeting petrels are such that the states can change only if the meeting petrels are in the SAME state. Lemma 1. There is a state y and a finite piece of execution EKL with P petrels, starting with two petrels in state y and one petrel in each other state, finishing in the same configuration, during which petrels do not meet the base station, and whose first event is the meeting of the two petrels in state y . The end of the proof is analog to the proof of proposition 10, but the reader may find the entire proof in the full version [5] It remains now to prove the key lemma (the detailled proof is in the full version [5]): Let us introduce two kinds of vectors, the first one for representing the states of all the sensors at a given time, the second one to represent the effect of the meeting of petrels. Definition 7. The vector of configuration VC of configuration C is the vector in NP -1 whose ith coordinate is the number of sensors in the ith state si .

74

J. Beauquier et al.

For each state x, let us define y (x) and z (x) to be the states that two petrels' sensors get when they meet while both in state x. Definition 8. The vector of variation Vx of state x is 1 I y (x ) + 1 Iz(x) - 21 Ix . The ith coordinate of Vx represents the variation of the number of sensors in state si when two petrels in state x meet, and indeed, if, from a configuration represented by V , two petrels in state x meet, the new configuration is represented by V + Vx . We claim first that there is a non-null linear combination of the vectors of variations, with non-negative integer coefficients, which is null. To prove the claim, start with P petrels and repeat making two petrels in the same state meet each other (you will always find two such petrels). The vectors of configuration you will get will stay in Y = {(qj )1j<P |qj  N, j qj = P } which is finite. So if you let long enough petrels with same state meet, you will encounter twice the same configuration. The set of meetings between the two appearances of that configuration gives you the wanted combination. More formally: define (Vy,i )0i  Y N and (Vw,i )1i  Z N by induction as follows: Vy,0 = (2, 1, 1, 1, . . . , 1). Once Vy,i is defined, find a coefficient x of Vy,i which is at least 2 (there is such a coefficient), then define Vy,i+1 = Vy,i + Vx and Vw,i+1 = Vx . It is easy to check that Vy,i+1 will be in Y . Since Y is finite, there are two integers i1 and i2 , with 0  i1 < i2  card(Y ) such that Vy,i1 = Vy,i2 . Then i1 <ii2 Vw,i fulfills the requirement since Vy,i2 = Vy,i1 + i1 <ii2 Vw,i . That first claim is proved.Let x x Vx be such a combination (So, x, x  N, x, x > 0, and x x Vx = (0, 0, . . . , 0)). For the sake of simplicity, let us assume that our combination minimizes x x . Let H be the multi set of vectors of variations where each Vx appears x times. Our second claim is that there is an index y and an ordering (h1 , h2 , ..., hcard(H ) ) of the elements in H , such that h1 = Vy , and for every i  [1, card(H )], the  (i)th coordinate of Zi = (1, 1, . . . , 1) + 1 Iy + j<i hj is 2 or more, and no coordinate of Zi is negative (where delta(i) is the index such that h(i) = V(i) ). Proof of the second claim Let y be an index such that y > 0. We build the hi by induction on i : Let h1 = Vy Assume the hj 's have been built up to j = i - 1, let us build hi , for some i  [2, card(H )]: Let Zi = (1, 1, . . . , 1) + 1 Iy + j<i hj . Since it is in Y , there is an index x such that Zi |x  2 (where M |v denotes the v th coordinate of M ). We may assume Iy , that x = y or (x = y and Zi |x  3) (indeed, otherwise, Zi = (1, 1, . . . , 1) + 1 which implies that j<i hj = 0, which contradicts the minimality of x x in our combination).

Self-stabilizing Counting in Mobile Sensor Networks with a Base Station

75

Thus j<i hj |x > 0, but since hH h|x = 0, it means that there is an element in H , not taken yet, whose xth coordinate is negative. This element is Vx for it is the only vector of variations whose xth may be negative. Let hi = Vx . The built sequence (h1 , h2 , ..., hcard(H ) ) satisfies the requirement, so the second claim is proved. The EKL execution is the following: Start with P petrels, two of them in state y , and one of them in each other state. For i from 1 to card(H ), make two petrels in state xi meet, where xi is the state such that Vxi = hi (there are two such petrels thanks to the propriety on Zi is the second claim) Note on the key lemma: The upper-bound on the length of EKL given by the proof is card{{(qj )1j<P |qj  N, j qj = P } which is exponential in P . One can wonder if it has to be large, or if there is such an execution EKL of size polynomial in P . The answer is that it might be indeed exponential. Consider the set of states [0, P - 1]. Take y = 0 (that is, start, with two petrels in state 0, and one in each state i  [1, P - 1]), and let the protocol be that when two petrels in state i meet, one of them gets in state 0, the other one gets in state (i + 1) mod P .

5

Resume

The TB model
model \ memory deterministic convergence time probabilistic convergence time Bounded Bounded,kfair daemon impossible impossible Algorithm 3 4k events impossible impossible Algorithm 4 exponential in k Finite Unbounded Algorithm 1-2 depends on which algorithm unneeded

The TBTP model
model \ memory Finite Bounded,(P ) < Bounded,(P )  P P impossible Algorithm 7 (P ) = 4P , 3 rounds Algorithm 5 or 6 (P ) = P + 1, 3 rounds (P ) = P , P+1 rounds

symmetric determinis- impossible tic convergence time asymmetric determin- impossible istic convergence time

impossible

76

J. Beauquier et al.

6

Final Remarks

In this article, we have studied the problem of self-stabilizing counting in different models of mobile sensor networks. We designed different algorithms depending on the communication model and the class of daemon. We also gave some proof of impossibility. In the cases where no deterministic (symmetric) solutions exist, we proposed probabilistic solutions. The knowledge of the size of a population is at the basis of the solutions of more complex problems, in particular when different types of population are present. An interesting perspective could be to model the movement of the sensors, by random processes for example, in order to improve our algorithms and to get better bounds for the convergence time. Acknowledgments. This work was supported by grants from Region Ile-deFrance.

References
1. Angluin, D., Aspnes, J., Diamadi, Z., Fischer, M.J., Peralta, R.: Computation in networks of passively mobile finite-state sensors. In: Proc. of 23 Annual Symposium on Principle of Distributed Computing PODC 2004 (2004) 2. Angluin, D., Aspnes, J., Eisenstat, D.: Fast computation by population protocols with a leader. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 61­75. Springer, Heidelberg (2006) 3. Angluin, D., Aspnes, J., Eisenstat, D., Ruppert, E.: The computational power of population protocols. In: Proc. of 25 Annual Symposium on Principle of Distributed Computing PODC 2006 (2006) 4. Angluin, D., Aspnes, J., Fischer, M.J., Jiang, H.: Self-stabilizing population protocols. In: Anderson, J.H., Prencipe, G., Wattenhofer, R. (eds.) OPODIS 2005. LNCS, vol. 3974, pp. 103­117. Springer, Heidelberg (2006) 5. Beauquier, J., Clement, J., Messika, S., Rosaz, L., Rozoy, B.: Self-stabilizing counting in mobile sensor networks. In: Technical Report L.R.I, number 1470 (March 2007) 6. Delporte-Gallet, C., Fauconnier, H., Guerraoui, R., Ruppert, E.: When birds die: Making population protocols fault-tolerant. In: Gibbons, P.B., Abdelzaher, T., Aspnes, J., Rao, R. (eds.) DCOSS 2006. LNCS, vol. 4026, pp. 51­66. Springer, Heidelberg (2006) 7. Dolev, S.: Self-Stabilization. MIT Press, Cambridge (2000) 8. Mainwaring, A., Polastre, J., Szewczyk, R., Culler, D., Anderson, J.: Wireless sensor networks for habitat monitoring (2002) 9. Martinez, K., Hart, J.K., Ong, R.: Environmental sensor networks. IEEE Computer 37(8), 50­56 (2004) 10. Ulmer, C., Yalamanchili, S., Alkalai, L.: Wireless distributed sensor networks for in-situ exploration of mars. In: Georgia Institute of Technology and California Institute of Technology, editors, Technical Report (2003) 11. Xu, N., Rangwala, S., Chintalapudi, K.K., Ganesan, D., Broad, A., Govindan, R., Estrin, D.: A wireless sensor network for structural monitoring. In: SenSys '04: Proceedings of the 2nd international conference on Embedded networked sensor systems, pp. 13­24. ACM Press, New York (2004)

Scalable Load-Distance Balancing
Edward Bortnikov, Israel Cidon, and Idit Keidar
Department of Electrical Engineering The Technion Haifa 32000 ebortnik@techunix.technion.ac.il {cidon,idish}@ee.technion.ac.il

Abstract. We introduce the problem of load-distance balancing in assigning users of a delay-sensitive networked application to servers. We model the service delay experienced by a user as a sum of a network-incurred delay, which depends on its network distance from the server, and a server-incurred delay, stemming from the load on the server. The problem is to minimize the maximum service delay among all users. We address the challenge of finding a near-optimal assignment in a scalable distributed manner. The key to achieving scalability is using local solutions, whereby each server only communicates with a few close servers. Note, however, that the attainable locality of a solution depends on the workload ­ when some area in the network is congested, obtaining a near-optimal cost may require offloading users to remote servers, whereas when the network load is uniform, a purely local assignment may suffice. We present algorithms that exploit the opportunity to provide a local solution when possible, and thus have communication costs and stabilization times that vary according to the network congestion. We evaluate our algorithms with a detailed simulation case study of their application in assigning hosts to Internet gateways in an urban wireless mesh network (WMN). Keywords: Local Computation, Distributed Algorithms, Load-Distance Balancing, Wireless Networks.

1 Introduction
The increasing demand for real-time access to networked services is driving service providers to deploy multiple geographically dispersed service points, or servers. This trend can be observed in various systems, such as content delivery networks (CDNs) [12] and massively multiplayer online gaming (MMOG) grids [8]. Another example can be found in wireless mesh networks (WMNs) [2]. A WMN is a large collection of wireless routers, jointly providing Internet access in residential areas with limited wireline infrastructure via a handful of wired gateways. WMNs are envisaged to provide citywide "last-mile" access for numerous mobile devices running media-rich applications with stringent quality of service (QoS) requirements, e.g., VoIP, VoD, and online gaming. To this end, gateway functionality is anticipated to expand, and to deploy application server logic [2].
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 77­91, 2007. c Springer-Verlag Berlin Heidelberg 2007

78

E. Bortnikov, I. Cidon, and I. Keidar

Employing distributed servers instead of centralized server farms enables locationdependent QoS optimizations, which enhance the users' soft real-time experience. Service responsiveness is one of the most important QoS parameters. For example, in the first-person shooter (FPS) online game [8], the system must provide an end-to-end delay guarantee of below 100ms. In VoIP, the typical one-way delay required to sustain a normal conversation quality is below 120ms [10]. Deploying multiple servers gives rise to the problem of service assignment, namely associating each user session with a server or gateway. For example, each CDN user gets its content from some proxy server, a player in a MMOG is connected to one game server, and the traffic of a WMN user is typically routed via a single gateway [2]. In this context, we identify the need to model the service delay of a session as a sum of a network delay, incurred by the network connecting the user to its server, and a congestion delay, caused by queueing and processing at the assigned server. Due to the twofold nature of the overall delay, simple heuristics that either greedily map every session to the closest server, or spread the load evenly regardless of geography do not work well in many cases. In this paper, we present a novel approach to service assignment, which is based on both metrics. We call the new problem, which seeks to minimize the maximum service delay among all users, load-distance balancing (Section 3). Resource management problems in which the assignment of every user to the closest server leads to unsatisfactory results are often solved centrally. For example, Cisco wireless local area network (WLAN) controllers [1] perform global optimization in assigning wireless users to access points (APs), after collecting the signal strength information from all managed APs. While this approach is feasible for medium-size installations like enterprise WLANs, its scalability may be challenged in large networks like an urban WMN. For large-scale network management, a distributed protocol with local communication is required. We observe that, however, load-distance-balanced assignment cannot always be done in a completely local manner. For example, if some part of the network is heavily congested, then a large number of servers around it must be harnessed to balance the load. In extreme cases, the whole network may need to be involved in order to dissipate the excessive load. A major challenge is therefore to provide an adaptive solution that performs communication to a distance proportional to that required for handling the given load in each problem instance. In this paper, we address this challenge, drawing inspiration from workload-adaptive distributed algorithms [6,14]. In Section 4, we present two distributed algorithms for load-distance balancing, Tree and Ripple, which adjust their communication requirements to the congestion distribution, and produce constant approximations of the optimal cost. Tree and Ripple dynamically partition the user and server space into clusters whose sizes vary according to the network congestion, and solve the problem in a centralized manner within every such cluster. Tree does this by using a fixed hierarchy of clusters, so that whenever a small cluster is over-congested and needs to offload users, this cluster is merged with its sibling in the hierarchy, and the problem is solved in the parent cluster. While Tree is simple and guarantees a logarithmic convergence time, it suffers from two drawbacks. First, it requires maintaining a hierarchy among the servers, which may be difficult in a dynamic network. Second, Tree fails to load-balance across the boundaries of the

Scalable Load-Distance Balancing

79

hierarchy. To overcome these shortcomings, we present a second distributed algorithm, Ripple, which does not require maintaining a complex infrastructure, and achieves lower costs and better scalability, through a more careful load sharing policy. The absence of a fixed hierarchical structure turns out to be quite subtle, as the unstructured merges introduce race conditions. In the full version of this paper [7], we prove that Tree and Ripple always converge to solutions that approximate the optimal one within a constant factor. For simplicity, we present both algorithms for a static workload. In Appendix A, we discuss how they can be extended to cope with dynamic workloads. We note that even as a centralized optimization problem, load-distance balancing is NP-hard, as we show in the full version of this paper [7]. Therefore, Tree and Ripple employ a centralized polynomial 2-approximation algorithm, BFlow, within each cluster. For space limitations, the presentation of BFlow is also deferred to the full paper. Finally, we empirically evaluate our algorithms using a case study in an urban WMN environment (Section 5). Our simulation results show that both algorithms achieve significantly better costs than na¨ ive nearest-neighbor and perfect load-balancing heuristics (which are the only previous solutions that we are aware of), while communicating to small distances and converging promptly. The algorithms' metrics (obtained cost, convergence time, and communication distance) are scalable and congestion-sensitive, that is, they depend on the distribution of workload rather than the network size. The simulation results demonstrate a consistent advantage of Ripple in the achieved cost, due to its higher adaptiveness to user workload.

2 Related Work
Load-distance balancing is an extension of the load balancing problem, which has been comprehensively addressed in the context of tightly coupled systems like multiprocessors, compute clusters etc. (e.g., [4]). However, in large-scale networks, simple load balancing is insufficient because servers are not co-located. While some prior work [8,12] indicated the importance of considering both distance and load in wide-area settings, we are not aware of any study that provides a cost model that combines these two metrics and can be analyzed. Moreover, in contrast with distributed algorithms for traditional load balancing (e.g., [11]), our solutions explicitly use the cost function's distancesensitive nature to achieve locality. A number of papers addressed geographic load-balancing in cellular networks and wireless LANs (e.g., [5,9]), and proposed local solutions that dynamically adjust cell sizes. While the motivation of these works is similar to ours, their model is constrained by the rigid requirement that a user can only be assigned to a base station within its transmission range. Our model, in which network distance is part of cost rather than a constraint, is a better match for wide-area networks like WMNs, CDNs, and gaming grids. Dealing with locality in this setting is more challenging because the potential assignment space is very large. Workload-adaptive server selection was handled in the context of CDNs, e.g., [12]. In contrast with our approach, in which the servers collectively decide on the assignment, they chose a different solution, in which users probe the servers to make a selfish choice. The practical downside of this design is a need to either install client software, or to run probing at a dedicated tier.

80

E. Bortnikov, I. Cidon, and I. Keidar

Local solutions of network optimization problems have been addressed starting from [16] ,in which the question "what can be computed locally?" was first asked by Naor and Stockmeyer. Recently, different optimization problems have been studied in the local distributed setting, e.g., Facility Location [15], Minimum Dominating Set and Maximum Independent Set [13]. While some papers explore the tradeoff between the allowed running time and the approximation ratio (e.g., [15]), we take another approach ­ namely, the algorithm achieves a given approximation ratio, while adapting its running time and communication distance to the workload. Similar methods have been applied in related areas, e.g., fault-local self-stabilizing consensus [14], and local distributed aggregation [6].

3 Definitions and System Model
Consider a set of k servers S and a set of n user sessions U , such that k n. The users and the servers reside in some metric space, in which the network delay function, D : (U × S )  R+ , captures the network distance between a user and a server. Consider an assignment  : U  S that maps every user to a single server. Each server s has a monotonic non-decreasing congestion delay function, s : N  R+ , reflecting the delay it incurs to every assigned session. For simplicity, all users incur the same load. Different servers can have different congestion delay functions. The service delay (u, ) of session u in assignment  is the sum of the two delays: (u, ) D(u, (u)) + (u) (|{v : (v ) = (u)}|).

Note that our model does not include congestion within the network. Typically, application-induced congestion bottlenecks tend to occur at the servers or the last-hop network links, which can be also attributed to their adjacent servers. For example, in a CDN [12], the assignment of users to content servers has a more significant impact on the load on these servers and their access links than on the congestion within the public Internet. In WMNs, the effect of load on wireless links is reduced by flow aggregation [10], which is applied for increasing the wireless capacity attainable for real-time traffic. The last-hop infrastructure, i.e., the gateways' wireless and wired links, is mostly affected by network congestion [2]. The cost of an assignment  is the maximum delay it incurs on a user: M ((U )) max (u, ).
uU

The LDB (load-distance balancing) assignment problem is to find an assignment  such that M ( (U )) is minimized. An assignment that yields the minimum cost is called optimal. The LDB problem is NP-hard. Our optimization goal is therefore to find a constant approximation algorithm for this problem. We denote the problem of computing an -approximation for LDB as -LDB. We solve the -LDB problem in a failure-free distributed setting, in which servers can communicate directly and reliably. The network delay function D and the set of server congestion functions {s } are known to all servers. We concentrate on synchronous protocols, whereby the execution proceeds in phases. In each phase, a server

Scalable Load-Distance Balancing

81

can send messages to other servers, receive messages sent by other servers in the same phase, and perform local computation. This form of presentation is chosen for simplicity, since in our context synchronizers can be used handle asynchrony (e.g., [3]). Throughout the protocol, every server knows which users are assigned to it. At startup, every user is assigned to the closest server (this is called a NearestServer assignment). Servers can then exchange the user information, and alter this initial assignment. Eventually, the following conditions must hold: (1) the assignment stops changing; (2) all inter-server communication stops; and (3) the assignment solves -LDB for a given . In addition to the cost, in the distributed case we also measure for each individual server its convergence time (the number of phases that this server is engaged in communication), and locality (the number of servers that it communicates with).

4 Distributed LD-Balanced Assignment
In this section, we present two synchronous distributed algorithms, Tree and Ripple, for -LDB assignment. These algorithms use as a black box a centralized algorithm ALG (e.g., BFlow [7]), which computes an rALG -approximation for a given instance of the LDB problem. They are also parametrized by the required approximation ratio , which is greater or equal to rALG . Both algorithms assume some linear ordering of the servers, S = {s1 , . . . , sk }. In order to improve communication locality, it is desirable to employ a locality-preserving ordering (e.g., a Hilbert space-filling curve on a plane [17]), but this is not required for correctness. Both Tree and Ripple partition the network into non-overlapping zones called clusters, and restrict user assignments to servers residing in the same cluster (we call these internal assignments). Every cluster contains a contiguous range of servers with respect to the given ordering. The number of servers in a cluster is called the cluster size. Initially, every cluster consists of a single server. Subsequently, clusters can grow through merging. The clusters' growth is congestion-sensitive, i.e., loaded areas are surrounded by large clusters. This clustering approach balances between a centralized assignment, which requires collecting all the user information at a single site, and the nearest-server assignment, which can produce an unacceptably high cost if the distribution of users is skewed. The distance-sensitive nature of the cost function typically leads to small clusters. The cluster sizes also depend on : the larger  is, the smaller the constructed clusters are. We call a value , such that  = (1 + )rALG , the algorithm's slack factor. A cluster is called -improvable with respect to ALG if the cluster's cost can be reduced by a factor of 1 +  by harnessing all the servers in the network for the users of this cluster. improvability provides a local bound on how far this cluster's current cost can be from the optimal cost achievable with ALG. Specifically, if no cluster is -improvable, then the current local assignment is a (1 + )-approximation of the centralized assignment with ALG. A cluster containing the entire network is vacuously non-improvable. Within each cluster, a designated leader server collects full information, and computes the internal assignment. Under this assignment, a cluster's cost is defined as the maximum service delay among the users in this cluster. Only cluster leaders engage in

82

E. Bortnikov, I. Cidon, and I. Keidar

inter-cluster communication. The distance between the communicating servers is proportional to the larger cluster's diameter. When two or more clusters merge, a leader of one of them becomes the leader of the union. Tree and Ripple differ in their merging policies, i.e., which clusters can merge (and which leaders can communicate for that). 4.1 Tree - A Simple Distributed Algorithm We present a simple algorithm, Tree, which employs a fixed binary hierarchy among servers. Every server belongs to level zero, every second server belongs to level one, and so forth (that is, a single server can belong to up to log2 k levels). For i  0 and l > 0, server i × 2l is a level-l parent of servers 2i × 2l-1 (i.e., itself) and (2i + 1) × 2l-1 at level l - 1. The algorithm proceeds in rounds. Initially, every cluster consists of a single server. During round l > 0, the leader of every cluster created in the previous round (i.e., a server at level l - 1) checks whether its cluster is -improvable. If it is, the leader sends a merge request to its parent at level l. Upon receiving this request from at least one child, the parent server merges all its descendants into a single cluster, i.e., collects full information from these descendants, computes the internal assignment using ALG, and becomes the new cluster's leader. Collecting full information during a merge is implemented through a sending a query from the level-l leader to all the servers in the new cluster, and collecting the replies. A single round consists of three synchronous phases: the first phase initiates the process with a "merge" message (from a child to its parent), the second disseminates the "query" message (from a leader to all its descendants), and the third collects the "reply" messages (from all descendants back to the leader). Communication during the last two phases can be optimized by exploiting the fact that a server at level l - 1 that initiates the merge already possesses full information from all the servers in its own cluster (that is, half of the servers in the new one), and hence, this information can be queried by its parent directly from it. If the same server is both the merge initiator and the new leader, this query can be eliminated altogether. Fig. 1(a) depicts a sample clustering of Tree where 16 servers reside on a 4 × 4 grid and are ordered using a a Hilbert curve. The small clusters did not grow because they were not improvable, and the large clusters were formed because their sub-clusters were improvable. Note that the size of each cluster is a power of 2. Tree guarantees that no -improvable clusters remain at the end of some round 1  L  log2 k , and all communication ceases. We conclude the following (the proof appears in the full paper [7]). Theorem 1. (Tree's convergence and cost) 1. If the last communication round is 1  L  log2 k , then there exists an improvable cluster of size 2L-1 . The size of the largest constructed cluster is min(k, 2L ). 2. The final (stable) assignment's cost is an -approximation of the optimal cost. Tree has some shortcomings. First, it requires maintaining a hierarchy among all servers. Second, the use of this static hierarchy leads it to make sub-optimal merges.

Scalable Load-Distance Balancing

83









( (
   

'

' $ %

&









&









% $

(a) Sample Tree clustering

(b) Hard workload for Tree

(c) Sample Ripple clustering

Fig. 1. Example workloads for the algorithms and clusters formed by them in a 4 × 4 grid with Hilbert ordering. (a) A sample clustering {A, B, C, D, E } produced by Tree. (b) A hard workload for Tree: 2N users in cell 8 (dark gray), no users in cell 9 (white), and N users in every other cell (light gray). (c) A sample clustering {A, B, C, D, E } produced by Ripple.

Fig. 1(b) shows an example workload on the network in Fig. 1(a). The congestion delay of each server is zero for a load below N + 1, and infinite otherwise. Assume that cell 8 contains 2N users (depicted dark gray in the figure), cell 9 is empty of users (white), and every other cell contains N users (light gray). An execution of Tree eventually merges the whole graph into a single cluster, for any value of , because no clustering of s1 , . . . , s8 that achieves the maximum load of at most N (and hence, a finite cost) exists. Therefore, due to the rigid hierarchy, the algorithm misses the opportunity to merge s8 and s9 into a single cluster, and solve the problem within a small neighborhood. 4.2 Ripple - An Adaptive Distributed Algorithm Ripple, a workload-adaptive algorithm, remedies the shortcomings of Tree by providing more flexibility in the choice of the neighboring clusters to merge with. Unlike Tree, in which an -improvable cluster always expands within a pre-defined hierarchy, in Ripple, this cluster tries to merge only with neighboring clusters of smaller costs. This typically results in better load-sharing, which reduces the cost compared to the previous algorithm. The clusters constructed by Ripple may be therefore highly unstructured (e.g., Fig. 1(c)). The elimination of the hierarchy also introduces some challenges and race conditions between requests from different neighbors. We first make some formal definitions and present Ripple at a high level. Following this, we provide the algorithm's technical details. Finally, we claim Ripple's properties; their formal proofs appear in the full version of this paper [7]. Overview. We introduce some definitions. A cluster is denoted Ci if its current leader is si . The cluster's cost and improvability flag are denoted by Ci .cost and Ci .imp, respectively. Two clusters Ci and Cj (1  i < j  k ) are called neighbors if there exists an l such that server sl belongs to cluster Ci and server sl+1 belongs to cluster Cj . Cluster Ci is said to dominate cluster Cj if:

84

E. Bortnikov, I. Cidon, and I. Keidar
Semantics Assignment summary (cost and -improvability) Proposal to join Accept to join, includes full assignment information Size small, fixed small, fixed large, depends on #users Value 0, 1, the server's id Semantics the cluster leader's id the internal assignment the cluster's cost the L/R neighbor cluster leader's id "probe" to L/R neighbor sent? "probe" from the L/R neighbor received? "propose" from L/R neighbor received? need to forward "probe" to L/R? need to send "probe" to L/R in the next round? need to send "propose" to L/R? need to send "accept" to L/R? Initial value Id NearestServer M (NearestServer ) {Id - 1, Id + 1} {false, false} {false, false} {false, false} {false, false} {true, true} {false, false} {false, false}

Message "probe",id,cost,imp "propose",id "accept",id,,nid Constants L, R, Id Variable LdrId  Cost NbrId[2] ProbeS[2] ProbeR[2] PropR[2] ProbeFwd[2] Probe[2] Prop[2] Acc[2]

Fig. 2. Ripple's messages, constants, and state variables

1. Ci .imp = true, and 2. (Ci .cost, Ci .imp, i) > (Cj .cost, Cj .imp, j ), in lexicographic order (imp and cluster index are used to break ties). Ripple proceeds in rounds. During a round, a cluster that dominates some (left or right) neighbor tries to reduce its cost by inviting this neighbor to merge with it. A cluster that dominates two neighbors can merge with both in the same round. A dominated cluster can only merge with a single neighbor and cannot split. When two clusters merge, the leader of the dominating cluster becomes the union's leader. Dominance alone cannot be used to decide about merging clusters, because the decisions made by multiple neighbors may be conflicting. It is possible for a cluster to dominate one neighbor and be dominated by the other neighbor, or to be dominated by both neighbors. The algorithm resolves these conflicts by uniform coin-tossing. If a cluster leader has two choices, it selects one of them at random. If the chosen neighbor also has a conflict and it decides differently, no merge happens. When no cluster dominates any of its neighbors, communication stops, and the assignment remains stable. Detailed Description. Fig. 2 provides a summary of the protocol's messages, constants, and state variables. See Fig. 4 for the pseudo-code. We assume the existence of local functions ALG : (U, S )  , M :   R+ , and improvable : (, )  {true, false}, which compute the assignment, its cost, and the improvability flag. In each round, neighbors that do not have each other's cost and improvability data exchange "probe" messages with this information. Subsequently, dominating cluster leaders send "propose" messages to invite others to merge with them, and cluster leaders that agree respond with "accept" messages with full assignment information. More specifically, a round consists of four phases:

Scalable Load-Distance Balancing
S1
prob e

85

S2
e prob
Phase 1

S1
prob e

S2
Phase 1

S1

probe

robe e prob probe p

S2

S3
Phase 1

Phase 2

prop ose pt acce

e prob prop ose pt acce

Phase 2

Phase 2

Phase 3

Phase 3

e pos pro accep t

e pos pro

Phase 3

Phase 4

Phase 4

Phase 4

(a) Simultaneous probe: s1 and s2 send messages in Phase 1.

(b) Late probe: s2 sends message in Phase 2.

(c)  conflict resolution: s2 proposes to s1 and rejects s3 .

S1

probe

robe e prob probe p

S2

S3
Phase 1

S1

probe

S2

S3

S4
Phase 1

Phase 2

e prob propose

prob e

Phase 2

propos e pt acce

ose prop
`

Phase 3

Phase 3

accept
Phase 4

Phase 4

(d)  conflict resolution: s2 accepts s1 and rejects s3 .

(e) Probe forwarding: s2 forwards to s1 , s3 forwards to s4 .

Fig. 3. Ripple's scenarios. Nodes in solid frames are cluster leaders. Dashed ovals encircle servers in the same cluster.

Phase 1 - probe initiation. A cluster leader sends a "probe" message to neighbor i if Probe[i] is true (ll. 4­6). Upon receiving a probe from a neighbor, if the cluster dominates this neighbor, the cluster's leader schedules a proposal to merge (line 53), and also decides to send a probe to the neighbor in this direction in the next round (line 55). If the neighbor dominates the cluster, the cluster's leader decides to accept the neighbor's proposal to merge, should it later arrive (line 54). Fig. 3(a) depicts a simultaneous mutual probe. If neither of two neighbors sends a probe, no further communication between these neighbors occurs during the round. Phase 2 - probe completion. If a cluster leader does not send a "probe" message to some neighbor in Phase 1 and receives one from this neighbor, it sends a late "probe" in Phase 2 (ll. 14­16). Fig. 3(b) depicts this late probe scenario. Another case that is handled during Phase 2 is probe forwarding. A "probe" message sent in Phase 1 can arrive to a non-leader due to a stale neighbor id at the sender. The receiver then forwards the message to its leader (ll. 19­20). Fig. 3(e) depicts this scenario: server s2 forwards a message from s1 to s4 , and s3 forwards a message from s4 to s1 . Phase 3 - conflict resolution and proposal. A cluster leader locally resolves all conflicts, by randomly choosing whether to cancel the scheduled proposal to one neighbor, or to reject the expected proposal from one neighbor (ll. 58­68). Figures 3(c) and 3(d) illustrate the resolution scenarios. The rejection is implicit: simply, no "accept" is sent.

86

E. Bortnikov, I. Cidon, and I. Keidar

Finally, the leader sends "propose" messages to one or two neighbors, as needed (ll. 28­29). Phase 4 - acceptance. If a cluster leader receives a proposal from a neighbor and accepts this proposal, then it updates the leader id, and replies with an "accept" message with full information about the current assignment within the cluster, including the locations of all the users (line 37). The message also includes the id of the leader of the neighboring cluster in the opposite direction, which is anticipated to be the new neighbor of the consuming cluster. If the neighboring cluster itself is consumed too, then this information will be stale. The latter situation is addressed by the forwarding mechanism in Phase 2, as illustrated by Fig. 3(e). At the end of the round, a consuming cluster's leader re-computes the assignment within its cluster (ll. 70­72). Note that a merge does not necessarily improve the assignment cost, since a local assignment procedure ALG is not an optimal algorithm. If this happens, the assignment within each of the original clusters remains intact. If the assignment cost is reduced, then the new leader decides to send a "probe" message to both neighbors in the next round (ll. 73­74). Ripple's Properties. We now discuss Ripple's properties. Their proofs appear in the full version of this paper [7]. Theorem 2. (Ripple's convergence and cost) 1. Within at most k rounds of Ripple, all communication ceases, and the assignment does not change. 2. The final (stable) assignment's cost is an -approximation of the optimal cost. Note that the theoretical upper bound on the convergence time is k despite potentially conflicting coin flips. This bound is tight (see [7]). However, the worst-case scenario is not representative. Our case study (Section 5) shows that in realistic scenarios, Ripple's average convergence time and cluster size remain flat as the network grows. For some workloads, we can prove Ripple's near-optimal locality, e.g., when the workload has a single congestion peak: Theorem 3. (Ripple's locality) Consider a workload in which server si is the nearest server for all users. Let C be the smallest non--improvable cluster that includes si . Then, the size of the largest cluster constructed by Ripple is at most 2|C | - 1, and the convergence time is at most |C | - 1. An immediate generalization of this claim is that if the workload is a set of isolated congestion peaks that have independent local solutions, then Ripple builds these solutions in parallel, and stabilizes in a time required to resolve the largest peak.

5 Numerical Evaluation
In this section, we employ Tree and Ripple for gateway assignment in an urban WMN, using the BFlow centralized algorithm [7] for local assignment. We compare our algorithms with NearestServer.

Scalable Load-Distance Balancing
1: Phase 1 {Probe initiation} : 2: forall d  {L, R} do 3: initState(d) 4: if (LdrId = Id  Probe[d]) then 5: i  improvable(, ) 6: send "probe", Id, Cost, i to NbrId[d] 7: ProbeS[d]  true 8: Probe[d]  false 9: 10: forall recv "probe", id, cost, imp do handleProbe(id, cost, imp) 43: procedure initState(d) 44: ProbeS[d]  ProbeR[d]  false 45: Prop[d]  Acc[d]  false 46: ProbeFwd[d]  false

87

11: Phase 2 {Probe completion} : 12: if (LdrId = Id) then 13: forall d  {L, R} do 14: if (¬ProbeS[d]  ProbeR[d]) then 15: i  improvable(, ) 16: send "probe", Id, Cost, i to NbrId[d] 17: else 18: forall d  {L, R} do 19: if (ProbeFwd[d]) then 20: send the latest "probe" to LdrId 21: 22: forall recv "probe", id, cost, imp do handleProbe(id, cost, imp)

47: procedure handleProbe(id, cost, imp) 48: d  dir(id) 49: ProbeR[d]  true 50: NbrId[d]  id 51: i  improvable(, ) 52: if (LdrId = Id) then 53: Prop[d]  dom(Id, Cost, i, id, cost, imp) 54: Acc[d]  dom(id, cost, imp, Id, Cost, i) 55: Probe[d]  Prop[d] 56: else 57: ProbeFwd[d]  true 58: procedure resolveConflicts() 59: { Resolve  or  conflicts} 60: forall d  {L, R} do 61: if (Prop[d]  Acc[d]) then 62: if (randomBit() = 0) then 63: Prop[d]  false 64: else 65: Acc[d]  false 66: {Resolve  conflict} 67: if (Acc[L]  Acc[R]) conflicts then 68: Acc[randomBit()]  false 69: procedure computeAssignment() 70:   ALG(Users(), Servers()) 71: if (M ( ) < M ()) then 72:    ; Cost  M ( ) 73: forall d  {L, R} do 74: Probe[d]  true 75: function dom(id1 , cost1 , imp1 , id2 , cost2 , imp2 ) 76: return (imp1  (imp1 , cost1 , id1 ) > (imp2 , cost2 , id2 )) 77: function dir(id) 78: return (id < Id) ? L : R

23: Phase 3 {Conflict resolution & proposal} : 24: if (LdrId = Id) then 25: resolveConflicts() 26: 27: 28: 29: 30: 31: {Send proposals to merge} forall d  {L, R} do if (Prop[d]) then send "propose", Id to NbrId[d] forall recv "propose", id do PropR[dir(id)]  true

32: Phase 4 {Acceptance or rejection} : 33: forall d  {L, R} do 34: if (PropR(d)  Acc[d]) then 35: {I do not object joining} 36: LdrId  NbrId[d] 37: send "accept , Id, , NbrId[d] to LdrId 38: 39: 40: 41: 42: forall recv "accept", id, , nid do     ; Cost  M () NbrId[dir(id)]  nid if (LdrId = Id) then computeAssignment()

Fig. 4. Ripple's pseudo-code: single round

88

E. Bortnikov, I. Cidon, and I. Keidar

The WMN provides access to a real-time service (e.g., a network game). The mesh gateways, which are also application servers, form a rectangular grid. This topology induces a partitioning of the space into cells. The wireless backbone within each cell is an 16 × 16 grid of mesh routers, which route the traffic either to the gateway, or to the neighboring cells. The routers apply flow aggregation [10], thus smoothing the impact of network congestion on link latencies. Each wireless hop introduces an average delay of 6ms. The congestion delay of every gateway (in ms) is equal to the load. For example, consider a workload of 100 users uniformly distributed within a single cell, under the NearestServer assignment. With high probability, there is some user close to the corner of the cell. The network distance between this user and the gateway is is 16 wireless hops, incurring a network delay of 16 × 6ms  100ms, and yielding a maximum service delay close to 100 + 100 = 200ms (i.e., the two delay types have equal contribution). Every experiment employs a superposition of uniform and peaky workloads. We call a normal distribution with variance R around a randomly chosen point on a plane a congestion peak. R is called the effective radius of this peak. Every data point is averaged over 20 runs, e.g., the maximal convergence time in the plot is an average over all runs of the maximal convergence time among all servers in individual runs. Sensitivity to slack factor: We first consider a 64-gateway WMN (this size will be increased in the next experiments), and evaluate how the algorithms' costs, convergence times, and locality depend on the slack factor. The workload is a mix of a uniform distribution of 6400 users with 6400 additional users in ten congestion peaks with effective radii of 200m. We consider values of  ranging from 0 to 2. The results show that both Tree and Ripple significantly improve the cost achieved by NearestServer (Fig. 5(a)). For comparison, we also depict the theoretical cost guarantee of both algorithms, i.e., (1 + ) times the cost of BFlow with global information. We see that for  > 0, the algorithms' costs are well below this upper bound. Fig. 5(b) demonstrates how the algorithms' convergence time (in rounds) depends on the slack factor. For  = 0 (the best possible approximation), the whole network eventually merges into a single cluster. We see that although theoretically Ripple may require 64 rounds to converge, in practice it completes in 8 rounds even with minimal slack. As expected, Tree converges in log2 64 = 6 rounds in this setting. Note that for  = 0, Tree's average convergence time is also 6 rounds (versus 2.1 for Ripple) because the algorithm employs broadcasting that involves all servers in every round. Both algorithms complete faster as  is increased. Fig. 5(c) depicts how the algorithms' average and maximal cluster sizes depend on . The average cluster size does not exceed 2.5 servers for   0.5. The maximal size drops fast as  increases. Note that for the same value of , Ripple builds slightly larger maximal-size clusters than Tree, while the average cluster size is the same (hence, most clusters formed by Ripple are smaller). This reflects Ripple's workload-adaptive nature: it builds bigger clusters where there is a bigger need to balance the load, and smaller ones where there is less need. This will become more pronounced as the system grows, as we shall see in the next section. Sensitivity to network size: Next, we explore Tree's and Ripple's scalability with the network size, for  = 0.5 and the same workload as in the previous section. We

Scalable Load-Distance Balancing
12 Ripple(), maximal Ripple(), average Tree (), maximal Tree (), average

89

70 60 50
Cluster size

1800 1600 1400
Cost

Convergence time (rounds)

Ripple() Tree () NearestServer (1+)BFlow

10

Ripple(), maximal Ripple(), average Tree (), maximal Tree (), average

8

40 30 20 10 0 0

1200 1000 800 600 400 0

6

4

2

0 0

0.2

0.5

0.2

0.5

1 Slack factor ()

1.5

2

1 Slack factor ()

1.5

2

0.2

0.5

1 Slack factor ()

1.5

2

(a) Cost

(b) Convergence time (maximal/average)

(c) Cluster size (maximal/average)

Fig. 5. Sensitivity of Tree()'s and Ripple()'s cost, convergence time (rounds), and locality (cluster size) to the slack factor, for mixed user workload: 50%uniform/50%peaky (10 peaks of effective radius 200m)

gradually increase the number of gateways from 64 to 1024. Fig. 6 depicts the results in logarithmic scale. We see that thanks to Ripple's flexibility, its cost scales better than Tree's, remaining almost constant with the network growth (Fig. 6(a)). Note that NearestServer becomes even more inferior in large networks, since it is affected by the growth of the expected maximum load among all cells as the network expands. Fig. 6(b) and Fig. 6(c) demonstrate that Ripple's advantage in cost does not entail longer convergence times or less locality: it converges faster and builds smaller clusters than Tree. This happens because Tree's rigid cluster construction policy becomes more costly as the network grows (the cluster sizes in the hierarchy grow exponentially). Sensitivity to user distribution: In the full paper [7], we also study the algorithms' sensitivity to varying workload parameters, like congestion skew and the size of congested areas. We demonstrate that whereas our algorithms perform well on all workloads, their advantage for peaky distributions is most clear. Here too, Ripple achieves a lower cost than Tree. The algorithms' maximal convergence times and cluster sizes are high only when the workload is skewed.
2200 2000 1800 1600 Ripple( 0.5) Tree ( 0.5) NearestServer

3 Ripple(0.5) Tree (0.5)
Convergence time (rounds)

3.5 Ripple(0.5) Tree (0.5) 3
Cluster size

2.5

Cost

1400 1200 1000 800 600 400 64 128 256 512

2

2.5

1.5

2

1024

1 64

Number of servers (log-scale)

128 256 512 Number of servers (log-scale)

1024

1.5 64

128 256 512 Number of servers (log-scale)

1024

(a) Cost

(b) Average convergence time

(c) Average cluster size

Fig. 6. Scalability of Ripple(0.5) and Tree(0.5) with the network's size (log-scale), for mixed workload: 50% uniform/50% peaky (10 peaks of effective radius 200m)

90

E. Bortnikov, I. Cidon, and I. Keidar

6 Conclusions
We defined a novel load-distance balancing (LDB) problem, which is important for delay-sensitive service access networks with multiple servers. In such settings, the service delay consists of a network delay, which depends on network distance, and a congestion delay, which arises from server load. The problem seeks to minimize the maximum service delay among all users. The -LDB extension of this problem is achieve a desired -approximation of the optimal solution. We presented two scalable distributed algorithms for -LDB, Tree and Ripple, which compute a load-distance-balanced assignment with local information. We studied Tree's and Ripple's practical performance in a large-scale WMN, and showed that the convergence times and communication requirements of these algorithms are both scalable and workload-adaptive, i.e., they depend on the skew of congestion within the network and the size of congested areas, rather than the network size. Both algorithms are greatly superior to previously known solutions. Tree employs a fixed hierarchy among the servers, whereas Ripple requires no pre-defined infrastructure, scales better, and consistently achieves a lower cost.

References
1. Cisco Wireless Control System: http://www.cisco.com/univercd/cc/td/doc/product/wireless/wcs 2. Akylidiz, I.F., Wang, X., Wang, W.: Wireless Mesh Networks: a Survey. Computer Networks Journal (Elsevier) (March 2005) 3. Awerbuch, B.: On the Complexity of Network Synchronization. J. ACM 32, 804­823 (1985) 4. Barak, A., Guday, S., Wheeler, R.: The MOSIX Distributed Operating System. LNCS, vol. 672. Springer, Heidelberg (1993) 5. Bejerano, Y., Han, S.-J.: Cell Breathing Techniques for Balancing the Access Point Load in Wireless LANs. IEEE INFOCOM (2006) 6. Birk, Y., Keidar, I., Liss, L., Schuster, A., Wolff, R.: Veracity Radius ­ Capturing the Locality of Distributed Computations. ACM PODC (2006) 7. Bortnikov, E., Cidon, I., Keidar, I.: Scalable Load-Distance Balancing in Large Networks. Technical Report 587, CCIT, EE Department Pub No.1539, Technion IIT (May 2006), http://comnet.technion.ac.il/magma/ftp/LDBalance tr.pdf 8. Chen, J., Knutsson, B., Wu, B., Lu, H., Delap, M., Amza, C.: Locality Aware Dynamic Load Management form Massively Multiplayer Games. PPoPP (2005) 9. Du, L., Bigham, J., Cuthbert, L.: A Bubble Oscillation Algorithm for Distributed Geographic Load Balancing in Mobile Networks. IEEE INFOCOM (2004) 10. Ganguly, S., Navda, V., Kim, K., Kashyap, A., Niculescu, D., Izmailov, R., Hong, S., Das, S.: Performance Optimizations for VoIP Services in Mesh Networks. JSAC 24(11) (2006) 11. Ghosh, B., Leighton, F.T., Maggs, B., Muthukrishnan, S., Plaxton, G., Rajaraman, R., Richa, A., Tarjan, R., Zuckerman, D.: Tight Analyses of Two Local Load Balancing Algorithms. ACM STOC (1995) 12. Hanna, K.M., Nandini, N.N., Levine, B.N.: Evaluation of a Novel Two-Step Server Selection Metric. IEEE ICNP (2001) 13. Kuhn, F., Moscibroda, T., Nieberg, T., Wattenhoffer, R.: Local Approximation Schemes for Ad Hoc and Sensor Networks. ACM DIALM-POMC (2005) 14. Kutten, S., Peleg, D.: Fault-Local Distributed Mending. J. Algorithms (1999)

Scalable Load-Distance Balancing

91

15. Moscibroda, T., Wattenhoffer, R.: Facility Location: Distributed Approximation. ACM PODC (2005) 16. Naor, M., Stockmeyer, L.: What can be Computed Locally? ACM STOC(1993) 17. Niedermeyer, R., Reinhardt, K., Sanders, P.: Towards Optimal Locality in Mesh Indexings. In: Chlebus, B.S., Czaja, L. (eds.) FCT 1997. LNCS, vol. 1279, pp. 364­375. Springer, Heidelberg (1997)

A Handling a Dynamic Workload
For the sake of simplicity, both Tree and Ripple have been presented in a static setting. However, it is clear that the assignment must change as the users join, leave, or move, in order to meet the optimization goal. In this section, we outline how our distributed algorithms can be extended to handle this dynamic setting. We observe that the clustering produced by Tree and Ripple is a partition of a plane into regions, where all users in a region are associated with servers in this region. As long as this spatial partition is stable, it can be employed for dynamic assignment of new users that arrive to a region. In a given region, the leader can either (1) re-arrange the internal assignment by re-running the centralized algorithm in the cluster, or (2) leave all previously assigned users on their servers, and choose assignments for new users so as to minimize the increase in the cluster's cost. Tree and Ripple can be re-run to adjust the partition either periodically, or upon changes in the distribution of load. Simulation results in Section 5 suggest that the overhead of re-running both algorithms is not high. However, this approach may force many users to move, since the centralized algorithm is non-incremental. In order to reduce handoffs, we would like to avoid a global change as would occur by running the algorithm from scratch, and instead make local adjustments in areas whose load characteristics have changed. In order to allow such local adjustments, we change the algorithms in two ways. First, we allow a cluster leader to initiate a merge whenever there is a change in the conditions that caused it not to initiate a merge in the past. That is, the merge process can resume after any number of quiet rounds. Second, we add a new cluster operation, split, which is initiated by a cluster leader when a previously congested cluster becomes lightly loaded, and its sub-clusters can be satisfied with internal assignments that are no longer improvable. Note that barring the future load changes, a split cluster will not re-merge, since non-improvable clusters do not initiate merges. This dynamic approach eliminates, e.g., periodic cluster re-construction when the initial distribution of load remains stationary. Race conditions that emerge between cluster-splitting decisions and concurrent proposals to merge with the neighboring clusters can be resolved with the conflict resolution mechanism described in Section 4.2.

Time Optimal Asynchronous Self-stabilizing Spanning Tree
Janna Burman and Shay Kutten
Dept. of Industrial Engineering & Management Technion, Haifa 32000, Israel bjanna@tx.technion.ac.il, kutten@ie.technion.ac.il

Abstract. This paper presents an improved and time-optimal selfstabilizing algorithm for a major task in distributed computing- a rooted spanning tree construction. Our solution is decentralized ("truly distributed"), uses a bounded memory and is not based on the assumption that either n (the number of nodes), or diam (the actual diameter of the network), or an existence of cycles in the network are known. The algorithm assumes asynchronous and reliable FIFO message passing and unique identifiers, and works in dynamic networks and for any network topology. One of the previous time-optimal algorithms for this task was designed for a model with coarse-grained atomic operations and can be shown not to work properly for the totally asynchronous model (with just "read" or "receive" atomicity, and "write" or "send" atomicity). We revised the algorithm and proved it for a more realistic model of totally asynchronous networks. The state in the presented algorithm does not stabilize until long after the required output does. For such an algorithm, an increased asynchrony poses much increased hardness in the proof.

1

Introduction

A system that reaches a legal state starting from an arbitrary one is called selfstabilizing [15]. The stabilization time is the time from the moment of the faults till the system reaches a legal state. The task of a directed spanning tree construction requires the marking, in each node, of some of the node's edges such that the collection of marked edges forms a tree. Moreover, we mark in each node the edge leading to its parent on the tree. Given a spanning tree, most of the major tasks for distributed network algorithms become much easier, including the tasks of reset, broadcast, topology learning and updating, mutual exclusion, voting, committing, querying, scheduling, leader election, and others. In this extended abstract, we directly address only the task of constructing a spanning tree with diam height in O(diam) time, but this, together with the method of [9], also yield an O(diam) time reset algorithm. In the context of self stabilization, it was observed that a self stabilizing reset protocol can translate
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 92­107, 2007. c Springer-Verlag Berlin Heidelberg 2007

Time Optimal Asynchronous Self-stabilizing Spanning Tree

93

a non-self stabilizing protocol into a self stabilizing one [4,9,6,7]. Another application of a reset protocol is to translate protocols that use unbounded event counters (e.g. sequence numbers of messages) to use bounded ones [8]. These applications of the reset simplify protocols' design. An optimal self stabilizing algorithm for constructing a spanning tree was presented in [7]. As opposed to some previous protocols, it was not based on the assumption that either n (the number of nodes) or diam (the actual diameter of the network) or an existence of cycles in the network were known. It used a bounded memory. For that, it assumed that some bound on the diameter was known. This bound may have been very large, but it did not affect the time complexity which was O(diam). The effect of the bound on the size of the memory was only polylogarithmic. The algorithm of [7], however, was designed for a model with a coarse-grained atomicity. That is, it assumed that a node could read a value of a variable written by a neighbor, and also perform an operation based on that read value in one atomic step. Only after both actions, could the neighbor change the value of its own variable. We show a scenario where that algorithm does not yield a correct result in a totally asynchronous model, that is, when an atomic operation contains either a read, or a write, but not both. In this paper, we modify the algorithm of [7] so that it functions correctly also in a fully asynchronous environment. We kept the main algorithmic ideas of [7], while adding a few tricks that may prove useful in translating also other such protocols. The main contribution of this paper is in the proof that the algorithm after the changes is correct for the more realistic asynchronous send/receive atomicity model (similar to "Atomic Read/ Atomic Write" model, but for a message passing model). Note, that combining the algorithm in [7] with some existing efficient transformer (e.g. [10] or [26]) to refine the atomicity would not have yielded optimal stabilization times in our case. When such a transformer is combined with a coarse-grained atomicity algorithm, a resulted fine-grained atomicity algorithm suffers from a reduction in concurrency due to the mutual exclusion procedures used in the implementation of the transformer. This loss of concurrency results in a higher than a constant delay (up to  (n)) between two successive atomic step executions by a particular process. Numerous self stabilizing reset and spanning tree algorithms that were less efficient than [7] also appeared. We mention some of them below. Many (starting with [17]) championed the claim that a self stabilizing algorithm should use finegrained atomic operations. We did not see how to use the methods used in [17] (for making their O(n) time algorithm work under fine-grained atomicity) to make the algorithm of [7] also work for fine-grained atomicity. We note that moving from a coarse-grained atomicity to fine-grain atomicity of operations is even impossible for some tasks in some models, and for other tasks it is tricky. This is especially tricky for the algorithm of [7], since the latter keeps multiple trees that do not stabilize in the required time O(diam), although the output in their algorithm does stabilize in O(diam) time in the coarse-grained operations

94

J. Burman and S. Kutten

model. The proof requires us to reason about these not yet stabilized trees (to show that they do not prevent also the output tree from stabilizing in O(diam) time). Proving a property of a not yet stabilized tree is made more difficult by the asynchronicity. It would have been much easier to prove that some condition holds for a node in a tree if it was certain (as it is in a coarse-grain atomicity model) that this node has not lost its parent or children without yet knowing about the loss. The current paper makes this necessary step from coarse-grained atomicity to asynchronous networks for the major tasks it solves. Other related work. Due to the lack of space, we give here a somewhat limited survey of the related work for the well-studied problem of a spanning tree construction. A thorough survey is deferred to the full paper [12]. In [3], a spanning tree construction algorithm with the stabilization time of O(n2 ) (where n is unknown) is given. In [18], a randomized spanning tree protocol is given implicitly, with the expected stabilization time of O(diam · log n), where diam is unknown. (If n is known in advance then the stabilization time is O(diam)). In [5], the time complexity is O(diam) (where diam is unknown), but the memory space (and the length of a message) is not bounded. In [2] and [19,13,14], generic self-stabilizing solutions solving also the task of a spanning tree construction are given. These papers present algorithms for weaker models (with unidirectional communication links and even with unreliable communications in [13,14]), but the time-optimal stabilization is not achieved in them. In [20], an algorithm for maintaining a spanning tree for a completely connected topology stabilizes in O(log n/loglog n) with high probability. In [24], a completely connected topology is assumed too, but the model is weaker than in [20]. In [25], a self-stabilizing algorithm for a minimum spanning tree construction is presented for an asynchronous message-passing reliable network. The time complexity in [25] depends on n (and hence, cannot be time-optimal). In addition, that algorithm of [25] requires a bound on the time that it takes to travel over a path of n nodes in the network. In [23] and [1], spanning tree algorithms for ad hoc networks with larger than O(diam) stabilization times are given. For an additional survey one can refer to [21]. Finally, we note that the algorithm presented in this paper can be combined with some hierarchial structure (e.g. [22]) to improve the stabilization time in some favorable settings (see [22]), but not in the worst case. 1.1 Notations and Model of Computation

System Model. The system topology is represented by an undirected graph G = (V, E ), where nodes represent processors and edges represent communication links. Each node has a unique identifier denoted by ID. The number of the nodes is denoted by n = |V |. The actual diameter of the graph is denoted by diam. We assume that there is a known upper bound on the diameter of the network, denoted by D and called the bound. This upper bound serves only for the purpose of having finite space protocols, and does not appear in the time complexity. For v  V , we define N(v ) = {u | (v, u)  E }, called the neighbors of v . We assume that the topology is dynamically changing- node/link addition

Time Optimal Asynchronous Self-stabilizing Spanning Tree

95

or removal are possible (and modeled as faults). We consider an asynchronous message passing network model. The message delivery time can be arbitrary, but for the purpose of time analysis only, we assume that each message is delivered in at most one time unit. On each link, messages are delivered in the same order as they have been sent. The number of messages that may be in transit on any link in each direction and at the same time is bounded by some parameter B (independent of the network size). It is necessary, as shown in [16], for self stabilization. We adopt the usual definitions of the following: a local state of a node (an assignment of values to the local variables and the program counter); a global state of a system (the cross product of the local states of all the nodes, plus the contents of the links); the semantics of protocol actions (possible atomic (computation) steps and their associated state changes); an execution of a protocol P (a possibly infinite sequence of global states in which each element follows from its predecessor by the execution of a single atomic step of P). Informally, a distributed system allows the processors to execute steps concurrently; however, when processors execute steps concurrently, we assume that there is no influence of these concurrent steps on each other. To catch this formally, we assume that at each given time, only a single processor executes a step. Each step consists of an internal computation and a single communication operation: a send or receive. Every state transition of a process is due to a communication step execution (including the following local computations). Let us call this the send/receive atomicity network model. Fault Model. The legality predicate (defined on the set of global states) of our protocol becomes true when the collection of internal variables of the nodes defines a spanning tree with diam height, rooted at the minimal ID node. A protocol is called self-stabilizing if starting from a state with an arbitrary number of faults (or from an arbitrary state) such that no additional faults hit the system for "long enough", the protocol reaches a legal global state eventually and remains in legal global states thereafter. When this happens, we say that the stabilization has occurred. The maximum number of time units that takes to reach the stabilization is called the stabilization time of the protocol.

2

The Algorithm

Due to the fine-grained communication atomicity, we use the notion of base and image variables. Each node v has a set of internal variables it writes to- the base variables. Consider some base variable of v , varv . Every neighbor u of v maintains an internal copy of varv in its corresponding image variable varu [v ] at u. The copies get their values from InfoMsg messages sent from v to u repeatedly. Node u reads varu [v ] for algorithm computations. By then, this copy can have a different value then varv at v , if v has changed it meanwhile. This is the main difficulty encountered by the current paper, as opposed to [7].

96

J. Burman and S. Kutten

2.1

Ideas Adopted from [7]

The algorithm runs multiple versions of the Bellman-Ford's algorithm ([11], see Rule 1 bellow). When running a single version alone, a stabilized tree results (this was observed by [17] for their algorithm which is similar to Bellman-Ford's, and by [7] and others for Bellman-Ford's algorithm itself). The stabilized tree is rooted at the minimal ID node in the network and the stabilization occurs in O(D) time when D is given. This is similar e.g. to [6,17]. Therefore, if the bound parameter D is close to the actual diameter diam, Rule 1 is close to optimal. Unfortunately, typically, a hardwired bound will be much larger than the actual diameter, to accommodate for extreme cases, and to have room for scaling up the network size. Nevertheless, coming up with some bound is pretty easy. Hence, log D + 1 versions are run in parallel. Each version i, 0  i  log D, executes the Bellman-Ford version with bound parameter 2i . Versions with "large enough" bound parameters 2i (the "higher versions") will stabilize to a desired spanning tree in O(2i ) time. However, the other versions (the "lower versions") can stabilize only in (n) as demonstrated in [7]. The trick there was that one does not need the smaller i versions to stabilize. One only needs them to detect and inform each node that the version has not yet constructed the required spanning tree which contains all the nodes. This is done by the standard technique of a broadcast over a tree. That is, each version i maintains at each node two bits: up cover and down cover. Using the up cover bit, nodes report towards the root that the version tree has not yet spanned all the nodes; this information is propagated up the tree by having each node take a logical and of the up cover bits of its children repeatedly. The purpose of the down cover bit is to disseminate this fact down the tree, by having each node copy the down cover bit of its parent repeatedly. See Rule 2. Then, each node v selects its output by finding the minimum i such that down coverv = 1 for version i. The tree edges of that version are the output of the combined protocol. A Counter Example. In the sequel we mention some problems that prevent one from using the solution of [7] in the weaker model we address in this paper. This is demonstrated in [12], because of the lack of space here. We just mention here that the problems manifest in the trees that have not stabilized yet, when the output was supposed to have stabilized already. In an asynchronous network, such a not yet stabilized tree may "pretend" successfully to have stabilized, and to cover the network. 2.2 Revising the Algorithm Now, we introduce three mechanisms we incorporate in the new algorithm to make the ideas above also work in the weaker model we use. · Non-stabilization Detector: The algorithm of [7] propagated up a tree the information that a node in that tree has a neighbor not in the tree. This information turned out to be unstable, causing the counter example mentioned above. Hence, we augment the above with a new notion of local "non-stabilization detector": if it looks as if any of the neighbors of a node v may still have to

Time Optimal Asynchronous Self-stabilizing Spanning Tree

97

change its state, then v observes that the current configuration is not yet stable, and propagates this observation upwards. Below, we give the formal definition of this idea- the consistentv predicate for each node v (see Def. 1). Now, in contrast with the previous solution, the definitions of the rules for the up cover and down cover bits use the consistent predicate. They are given in Rule 2. · Strict Communication Discipline: We adopt this module as is from [4] (see Sec. 2.3 below). The main property of the discipline is that before a node v may change any of its base variables, all its neighbors "know" the value of v 's base variables (see Lem. 3). The proof of Lem. 4 uses this property heavily. · Local Reset: We use this mechanism to ensure the following. Whenever a node v changes the values of its tree variables (line 6, Fig. 1), no neighbor u considers v as its parent (that is, paru = v ). Note, that in our model this is not a condition that v can check directly, since by the time v reads u's variables, u may have changed them. We found this property very useful in several places in the proof. For example, whenever a node joins a tree on some specific path called a legal branch (see Def. 4), it joins initially with up cover = 0 (Lem. 7). This helps to prove that even though the lower versions trees do not stabilize (at least, not in O(diam) time), their up cover does stabilize to zero (Lem. 8). Lines 6-11 (Fig. 1) implement the local reset mechanism. Intuitively, the main difficulty the revised algorithm overcomes is the following. Consider a branch of a tree whose nodes have a root value of v , however, they are disconnected from v . (A formal definition of this structure we call a "sprig" will follow (Def. 3).). Note, that v does exist. This phenomena is different than branches of a tree of a ghost root- that is, branches with nodes whose root variable contains an ID of a node that does not exist. The latter branches disappear by time Tvalid = O(2i ) for version i (see Def. 2 and Obs. 1), but new sprigs can be created by the lower versions of the algorithm up until time  (n). We needed the above revisions in the algorithm to show that the sprigs do not confuse themselves to have up cover = 1 (Lem. 10). Nor do they confuse the nodes of the legal tree of v (nodes that are indeed connected (via a chain of parent markings) to v where v is the value of their root variable). See Lem. 8. 2.3 Algorithm Details

Variables. Graph property variables are represented using a boldface font, as in dist(v, u), which denotes the true distance in the graph between nodes v and u. Protocol variables are represented using teletype font. The variables appearing and manipulated in the code of Fig. 1 are local protocol variables of process v . The subscript v of each variable in the figure emphasizes that fact. Each node u  V sends a set of values of its internal variables (its base variables) periodically to all its neighbors by an InfoMsgu message. Each neighbor v of u copies these values to its local copies (to simplify the code, we omit this

98

J. Burman and S. Kutten

simple operation from the algorithm code, but we assume it is performed for each message receive event). The copy of a variable varu at v (an image variable of u at v ) is denoted by varv [u]. Node u does not send its neighbors any of its image variables. Note, that for each v  V, in addition to N(v ), we use a variable Nlistv which is a local list of node identities such that the incident link from v to each node u in the list is believed to be operational and the processor at each such node u is also believed to be up. A detailed verbal explanation of all the variables appears in [12]. Communication Discipline. Every node sends the InfoMsg messages repeatedly, using the communication discipline mentioned above. We treat the discipline as being embodied by a lower layer process. Whenever any InfoMsg arrives at some node v , the discipline (layer) copies the content of the message into the appropriate image variables. Then, the communication discipline can decide either to pass the message to the higher level algorithm or to discard it. If it decides to pass the message, we say that InfoMsg is accepted (then, this message is processed according the code in Fig. 1). Otherwise, we say that InfoMsg is received (and no further processing takes place). Note, that in both cases, v copies the message content (receives InfoMsg). For the complexity analysis, we define a time step, which is the maximum time for a message to get accepted (it is equivalent to 3 time units). Lem. 3 states the communication discipline property formally. Algorithm Formal Definition. The formal code for version i of the algorithm in node v appears in Fig. 1; it applies Rules 1 and 2 below. In each iteration, node v outputs its tree edges according to the lowest version in v for which up cover = 1. Rule 1. (Bellman-Ford with IDs and Bound Parameter D (used in [11,17,6] for a single version)) Let v be a node. 1. rv  min {IDv , rv [u]} where u  Nlistv and dv [u] < D. 1 + min {dv [u] : u  Nlistv and rv = rv [u]} , if rv = IDv , 2. dv  0, if rv = IDv . Definition 1. (Used in Rule 2 below) · The parent of a node v , denoted parv , is (supposed to be) the smallest ID neighbor u of v for which rv = rv [u] and dv [u] = dv - 1, if rv = v . Otherwise, if rv = v , the parent of v is null. · The children of v , denoted childv , are the set {u | parv [u] = v }. · Given a node v , the predicate consistentv is defined by uNlistv (rv [u] = rv )  (|dv [u] - dv |  1)  (u = parv  u is the minimal ID node such that dv - dv [u] = 1)  (dv [u] - dv = 1 = parv [u]  v ) · Node v is a consistent node, if consistentv = true.

Time Optimal Asynchronous Self-stabilizing Spanning Tree

99

Rule 2 Calculate parv , childv , and consistentv such that they conform to Def. 1.  if childv =    consistentv ,    up coverv [u] , if childv =   consistentv up coverv    u  child v    0, otherwise down coverv  down coverv [parv ]  consistentv , if rv = v up coverv  consistentv , if rv = v

Procedure Send() (* sending InfoMsgv *) 1 Send InfoMsgv  [ rv , dv , parv , local resetv , up coverv , down coverv ] to Nlistv Procedure LocalReset() (* performing a local reset *) 2 rv  v, dv  0, down coverv  up coverv  0, local resetv  true 3 Send() Do forever: 4 Send() (* this line is executed atomically *)

Upon accepting InfoMsgv [u] message from neighbor u  Nlistv (* the following is executed atomically (not including reception) *) 5 Use Rules 1, 2 to calculate temporary variables as follows: a temporary variable t varv for each varv computed in the rules. *) *) *) *)

(* tree changes generate a local reset if [ ¬local resetv  (parv = t parv  rv = t rv  dv = t dv ) ] (* reset propagates down the tree 7 [ u  Nlistv : local resetv [u]  u = parv ] (* local reset not yet completed 8 [ local resetv  u  Nlistv : parv [u] = v ] (* candidate parent reset not yet completed 9 [ local resetv  t parv = null  local resetv [t parv ] ] then 10 LocalReset() 11 return end if (* a local reset exit (if local resetv switches from true to false ) 12 local resetv  false 6 13 Update each variable varv by the value of the temporary variable t varv . 14 Send()

*)

Fig. 1. Algorithm for version i at node v

100

J. Burman and S. Kutten

3

Preliminary Analysis

In the following analysis, we prove stabilization assuming that there are no faults or topological changes after some time t0 = 0 (at least till the time when algorithm reaches a global legal state). From now on, let us consider the execution of the algorithm after time t = 2 (2 time steps after time t0 ). It is easy to see that after 2 time steps, no damaged (by faults) messages exist in the network and at least one authentic InfoMsg message has been accepted at each node from each neighbor. Definition 2. Consider a node v  V and some time T . · Let Tvalid be time t0 + 2 + 2i . · Node v is a root node, if rv = IDv ( v ). / {IDu |u  V}, then rv is a ghost root. · If rv  · Let u  N(v ). If at time T , varu [v ] = varv , we say that node u knows the value of varv at T . def · The depth of v is depth(v ) = max {dist(v, u) | u  V }. · Let us denote by vmin the node with the minimum ID that exists in the network. · Let us denote by a local reset operation of node v an invocation of the LocalReset() procedure at line 10 of the algorithm code. · Assume that node v performs a local reset operation at time T . We denote by a local reset exit the first time after time T when v executes line 12 of the algorithm code. · Let us denote by a local reset mode the state of node v between a local reset operation and the subsequent local reset exit at v . Lem. 1 is a rather known property of Bellman-Ford's algorithm. Its proof, as well as some of the following proofs, due to the lack of space, appear in [12]. Lemma 1. Starting from any initial assignments of the d and r variables, for any node v , after t time steps, dv  min(t, dist(rv , v )). The following observation follows from Lem. 1 and Rule 1-1. (Note, that for every ghost root v , dist(rv , v ) = .) Observation 1. After time Tvalid , for every node v , rv is not a ghost root and dv  dist(v, rv ). Definition 3. Let 0  k  n - 1. Let v, xj  V for each 0  j  k . · A parent path of v to x0 is a path of nodes (x0 , x1 , x2 , . . . , xk = v ) such that for each j , rxj = rv and for each 1  j  k , parxj = xj -1 . · We say that v is a descendant of x0 and x0 is an ancestor of v . · A connection path of v is a parent path of v to rv = x0 such that x0 is a root node. Let us denote a connection path of v (to x0 ) by Cv (x0 ). · Node v is connected (to node x0 ) if there is a connection path of v (to x0 ). · Let rv = x0 . If node v has no connection path, node v is disconnected (from node x0 ). · Let a sprig of v be a maximal set of nodes X  V that satisfies the following

Time Optimal Asynchronous Self-stabilizing Spanning Tree

101

conditions: (1) v  / X ; (2) x  X , rx = v and x is disconnected; (3) Every ancestor or descendant x of any x0  X is in X . · Let us denote some sprig A of v at time t by Av (t). The following is one of our main lemmas. Informally, we found it harder to ensure properties for sprigs than for legal trees. We use the following lemma to show a property of sprigs that is somewhat similar to the property of the d values in a legal tree (shown in Lem. 1). Informally, the lemma shows that a node's d is high if it remains in the same sprig for a long time (an "old" sprig). This may not hold for a node who leaves one sprig (having a high d) and joins another (with a lower d). Such a leave and join may happen even very late in the execution since new sprigs may be created very late in the execution. However, we handle such "new" sprigs later. Lemma 2. Consider nodes v, u  V. Let t  2i and t1 > 2. Let  be the following set of assumptions on u: (1) node u is disconnected from ru = v ; (2) node u does not change its ru value. If  holds for u during the whole time interval [t1 , t1 + 2t], then at time t1 + 2t, du  t. Proof: By induction on the time t. For t = 0, the lemma holds since du  0 always. Assume, that the lemma holds for t = k for every node. For t = k + 1, assume that  holds for some node u for the time interval [t1 , t1 +2(k +1)]. By Def. 3, node u is disconnected and is not a root. Hence, paru = null. When  holds, node u cannot change its paru . Otherwise, the condition at line 6 must hold beforehand and then, u must perform a local reset and assign ru  u (becoming a root) in line 2- a contradiction to . Hence, for some node w  Nlist(u), w = paru throughout [t1 , t1 + 2k + 2]. Let  be an InfoMsg message sent by w at time tsent and accepted at u at time trcv such that  is the last such message accepted at u by time t1 + 2k + 2. By our model assumptions on communication links , trcv  t1 + 2k + 1. Hence,  is sent by w at tsent  t1 + 2k . Clearly,  holds at w during [t1 , tsent ], otherwise it would not have held in u during [t1 , trcv ] since in that case, either u would have been connected (if w would have been connected) or some InfoMsgw and line 6 of the algorithm would have caused u to perform a local reset and become a root, violating . Hence ,by the induction hypothesis, dw  k at time t1 + 2k . Moreover, if dw is changed at w in [t1 , tsent ], then InfoMsgw stating this fact and line 6 of the algorithm would have caused u to perform a local reset and become a root, violating . Thus, dw stays unchanged during [t1 , tsent ]. Hence, dw  k during the whole interval [t1 , tsent ] and du [w]  k at u during the whole of [t1 , t1 + 2k + 2]. Now, since paru = w during whole [t1 , t1 + 2k + 2], by Rule 1, du = du [w] + 1 during whole this interval. Thus, at time t1 + 2k + 2, du = du [w] + 1  k + 1. Hence, the lemma holds for t = k + 1 too. The following lemma is ensured by the communication discipline, which we assume for the algorithm (Sec. 2.3). The lemma is proved in [7]- Lem. 4.2 (we reworded it somewhat). Lemma 3. [7] If at some time T , after time t0 + 2, some node v changes any of its base variables, then, at time T , u  Nlist(v ), for every base variable varv

102

J. Burman and S. Kutten

of v , varu [v ] = varv (u's image variable of varv equals varv at time T ; or in other words, node u knows the value of varv at time T ). The following lemma can be proven for our algorithm, but not for the previous one [7]. This proved to be a major reason why our algorithm can function in the atomic send / atomic receive model. Lemma 4. Assume that at some time T , some node u with paru = w assigns paru  w. Then, at time T , v  Nlist(u), parv = u. Proof: (see Fig. 2) To prove the lemma, we assume, by way of contradiction, that some neighbor v of u assigns1 parv  u before or at time T and parv = u holds till time T inclusively. To assign a new value to paru , node u must execute line 13 at time T (also see line 5 and Rule 2). This requires that just before that, u was at a local reset mode and performed a local reset exit at line 12. Since u changes a base variable at time T , by Lem. 3, every neighbor v of u (and v in particular) knows at time T that u is in a local reset mode (local resetv [u] = true). Let time T  be the first time when v assigns local resetv [u]  true such that local resetv [u] stays true until T inclusively. Recall, that we consider the execution of the algorithm after time t=2. Thus, due to line 9, it is guaranteed that v does not change its parent pointer to point at u (v cannot perform parv  u) throughout [T  , T ]. (A local reset operation and then its exit must precede any tree structure base variables change at any node; but, a local reset exit is impossible at v in [T  , T ], since the condition at line 9 holds throughout this time interval.) Let T x be the time when v sends message InfoMsgv x that is the last one to be received by u from v before time T . This message must cause u to set paru [v ] = u, otherwise, by the condition at line 8, u cannot become a descendant of w at time T . Hence, at time T x , parv = u holds at v . Thus, and due to the guarantee of the time interval [T  , T ] that we have shown above, we must assume that v assigns parv  u (at line 13) in the time interval (T x , T  ). When this happens, v sends an InfoMsgv (= y ) (line 14) with this new information (parv = u) to u. Recall, that by Lem. 3, node u learns at some time T  < T  < T that its neighbor v knows that u is in a reset mode (at T  , u learns that local resetv [u] = true). Clearly, for u, to learn this, a message of the communication discipline should be sent from v at or after time T  and should be received at u before time T . Let z be such a message. We proved above that message y is sent before T  . Hence, and because of the FIFO assumption for each link, message y is received at u before message z , and thus, before time T  and before T. Since InfoMsg message y is sent after InfoMsg message x, it should arrive at u after x, but we have assumed above that x is the last InfoMsg to be received from v just before time T . Thus, y = x - a contradiction since these messages bear different information about the value of parv .
1

The algorithm calculates the par pointers by invoking Rule 2 at line 5 and then assigns them at line 13 (Fig.1).

Time Optimal Asynchronous Self-stabilizing Spanning Tree

103

Fig. 2. Illustration for the proof of Lem. 4

4

Analysis of the Lower Version Case (2i < depth(vmin))

In this section we prove for every node in every lower version that down cover = 0 holds in O(2i ) time steps and remains thereafter. First, we prove this for legal trees and then for sprigs. To prove this for legal trees, we use the notion of a legal branch. There may be several shortest pathes between two nodes, however, only one of them is a legal branch as defined in the following definition. (This matches the way parent pointers are chosen by the algorithm. See Def. 1.) Definition 4. Let u, v  V. · Node u is foreign to node v if after time Tvalid , ru = v always. · Let u be foreign to v . A legal branch Rv (u) of v via u is a shortest path between v and u (x1 = v, x2 , . . . , xk = u) s.t. 1  k  dist(v, u)+1 and for each 2  j  k , xj -1 is the smallest ID neighbor of xj with dist(v, xj -1 ) = dist(v, xj )-1. Denote the length of Rv (u) by |Rv (u)|. Lemma 5. If 2i < depth(vmin ), then after time Tvalid , for each node v , there exists a node u such that u is foreign to v and either (1) dist(v, u) = 2i + 1 or (2) dist(v, u) < 2i + 1 and v > u. Definition 5. Let v, w  V. · Let fv = u for some u as defined in Lem. 5. · A zero path of node v , denoted by Zv , is a maximal parent path of nodes {x|Cx (v )  x  Rv (fv )}. A fringe node of Zv is w  Zv which is the furthest node (in the number of hops) from v . Then, we also denote a zero path of v by Zv (w) and its length by |Zv (w)| (or |Zv |). Note, that every root node has a zero path (possibly containing only a root). A zero path may change dynamically in time. Nodes may join and leave a zero path. A zero path may "disappear" (if a root node stops to be a root) and "reappear" (if a node becomes a root again). The set of a zero paths stabilizes only in  (n) in some cases. Yet, by the following lemmas 6 to 8, we show that up cover = 0 holds at every node of every zero path that exists after a certain time that is O(2i ) time after the starting time t0 . The proofs of lemmas 6 - 8 establish an induction on the order of the nodes on a zero path, starting from a fringe node neighboring to a foreign node and proceeding to the root. This

104

J. Burman and S. Kutten

induction shows that up cover stabilizes fast to zero over each legal tree (one that has a root, as opposed to a sprig that is disconnected from its root). Let us comment that we needed to introduce the notion of a legal branch for these proofs. Moreover, we needed to add the check (in the consistent predicate, in Rule 2) that a branch is indeed legal when up cover is updated. The proofs of lemmas 6 - 8 appear in [12]. Lemma 6. Let v, u  V and let u be a fringe node of Zv (a zero path of v ), at some time T after Tvalid . Then, if u stays a fringe node of Zv , in at most 1 time step, at time T  T 1  T + 1, up coveru  0. Lemma 7. Let v, u  V and assume that u joins a Zv (a zero path of v ) at some time T after Tvalid (u was not in Zv just before T ). Then, at time T , up coveru  0. Lemma 8. Let v, u, x  V and Zv (x) be a zero path of v . Let u  Zv (x) be such that dist(v, u) = dist(v, fv ) - j (for some 1  j  dist(v, fv )  2i + 1). Then, after Tvalid + j time steps, up coveru = 0. Let Tccover be the time that is Tvalid + 2 · 2i time steps after time t0 . By lemmas 8 and 4 and Rule 2, it is easy to show that down cover also stabilizes fast to zero on legal trees. Thus, Lem. 9 also follows. Lemma 9. After Tccover time for any connected node u  V the following holds: down coveru = 0. To conclude the analysis for versions for which 2i  depth(vmin ), we need to show that for every node u that is disconnected from ru (u is a sprig node), down coveru = 0 holds in O(2i ) time steps and remains such thereafter too. First, let us formalize all the possible modifications that a sprig can encounter. Note, that there may exist several sprigs of v at the same time. Definition 6. Consider a sequence of events in the execution, and let tj be the time of the j -th event in the sequence. Consider some sprig Av (tj ). At time tj +1 , we consider sprigs that are non-empty and that have non-empty intersection of nodes with the original sprig Av (tj ). That is, we consider all the possible modifications of Av at time tj +1 : (1) Sprig Av (tj +1 ) =  is one of the following: ­ Av (tj ). / Av (tj ). Note, that by Lem. 4, x has ­ Av (tj )  {x} (a join of node) for some x  no descendants at tj +1 . (Also, w.l.o.g., no two events happen at the same time.) ­ Av (tj ) \ {y } (a loss of node) for some y  Av (tj ). (2) Non-empty sprigs A1v (tj +1 ), A2v (tj +1 ), · · · such that A1v (tj +1 )  A2v (tj +1 )  · · · {z } = Av (tj ) (a split of sprig Av ) for some node z who left sprig Av at tj . Lemma 10. Let u  V be disconnected from ru at some time after Tccover + 2 · 2i + 1. Then, down coveru = 0. Proof: Any disconnected node u belongs to some sprig. Let ru  v . Only two kinds of sprigs exist after time Tccover by Obs. 1:

Time Optimal Asynchronous Self-stabilizing Spanning Tree

105

(1) The old sprigs- these that already exist at Tccover . We consider them old even when they get modified later. Moreover, if Av (tj ) is an old sprig at some time tj  Tccover , then any resulting sprig Av (tj +1 ) (see Def. 6) at time tj +1 > tj is an old sprig too. (2) The new sprigs- those that are newly created after Tccover by the following event. Let w  V be a root node and X be a set of the nodes connected to w (X is a tree rooted at w). When some node x  X (not a leaf node) leaves X , a new sprig (or sprigs) of w is (or are) created. When a new sprig gets modified, the resulting sprig (or sprigs) is (or are) considered a new sprig (or sprigs). First, let us consider a set of the old sprigs  after Tccover . We show that after at most 2 · 2i + 1 time steps  = . By Lem. 2, in at most 2 · 2i time steps, for any disconnected node x  B  , dx  2i . Hence, x leaves sprig B in at most additional 1 time step by Rule 1-1, if x has not left sprig B before that. Any node y  / B   that joins sprig B , assigns dy  dy [z ] + 1 for z  B such that pary  z . Thus, by this and Lem. 2, for any x  B  , at time Tccover + 2t, dx  t. Hence, after at most Tccover + 2 · 2i + 1,  = . Finally, let us consider a new sprig A that is newly created at some time T > Tccover . By Lem. 9, at time T for each x  A, down coverx = 0. After time T sprig A can change in time: split or join/loose nodes. Note, that no sprig has a root node and thus, by Rule 2, the down coverx bit is calculated by down coverx  (down coverx [parx ]  consistentx ). Hence, a split or a loss of nodes cannot switch the down cover bit (from 0 to 1) at the resulting sprig/s. Now, consider the case that some node y  / A joins A. Node y becomes a child of some node x  A. At that time, by Lem. 4, a local reset exit occurs at y (line 12) and then, at line 13, node y assigns pary  x and by Rule 2, adopts the down coverpary which is 0 as shown above. Thus, any node that joins A adopts 0 in its down cover (recall that by Lem. 4, it joins alone). Hence, after time Tccover + 2 · 2i + 1, only new sprigs exist (beside legal trees) and every disconnected node u in such a sprig has down coveru = 0.

5

Analysis of the Higher Version Case (2i  depth(vmin))

Lem. 11 is important to prove the stabilization of the higher versions (Lem. 12). Specifically, it helps to show that a local reset mode in each node has finite duration. Lemma 11. Assume a node v performing a local reset operation at some time T . (1) Then, in at most 2 time steps after time T , unless the condition of line 9 holds, v performs line 12 of the algorithm (a local reset exit occurs at v ). (2) During the local reset mode at v (starting at time T and till the local reset exit at v ), up coverv = 0 and down coverv = 0. Lemma 12. If 2i  depth(vmin ), then version i stabilizes in O(2i ). Lemma 13. If 2i  depth(vmin ), then in O(2i ) time, at every node v , down coverv = 1 for version i.

106

J. Burman and S. Kutten

Lem. 12 above establishes that there exists some higher version that stabilizes at time O(diam). Lem. 13 establishes that in this version down cover  1. Hence, the algorithm can output the tree this version produces. Recall that Sec. 4 shows that in lower versions down cover stabilizes to zero. All these establish the following theorem. Theorem 1. In O(diam) time units, the algorithm produces a shortest paths tree rooted at the minimal ID node in the network. The proofs (see [12]) in this section are rather similar to the proofs for the higher versions in [7] except for one important point. The local reset we use here has the potential to destabilize these versions. We show that a local reset always ends. Moreover, since a local reset is transferred to children, not to parents, the reset does not destabilize the tree rooted in the minimal ID node.

References
1. Abbas, S., Mosbah, M., Zemmari, A.: Distributed Computation of a Spanning Tree in a Dynamic Graph by Mobile Agents. In: IEEEIS'06 (2006) 2. Afek, Y., Bremler-Barr, A.: Self-stabilizing Unidirectional Network Algorithms by Power-Supply. In: SODA'97 (1997) 3. Afek, Y., Kutten, S., Yung, M.: Memory-Efficient Self-Stabilizing Protocols for General Networks. In: van Leeuwen, J., Santoro, N. (eds.) Distributed Algorithms. LNCS, vol. 486. Springer, Heidelberg (1991) 4. Afek, Y., Kutten, S., Yung, M.: The Local Detection Paradigm and its Applications to Self-Stabilization. In: TCS' 97, vol. 186(1­2) (1997) 5. Aggarwal, S., Kutten, S.: Time Optimal Self-stabilizing Spanning Tree Algorithms. In: Shyamasundar, R.K. (ed.) Foundations of Software Technology and Theoretical Computer Science. LNCS, vol. 761. Springer, Heidelberg (1993) 6. Arora, A., Gouda, M.G.: Distributed Reset. In: Veni Madhavan, C.E., Nori, K.V. (eds.) Foundations of Software Technology and Theoretical Computer Science. LNCS, vol. 472. Springer, Heidelberg (1990) 7. Awerbuch, B., Kutten, S., Mansour, Y., Patt-Shamir, B., Varghese, G.: Time Optimal Self-stabilizing Syncronization. In: STOC'93 (1993) 8. Awerbuch, B., Patt-Shamir, B., Varghese, G.: Bounding the Unbounded. In: INFOCOM'94 (1994) 9. Awerbuch, B., Patt-Shamir, B., Varghese, G.: Self-stabilization by Local Checking and Correction. In: FOCS'91 (1991) 10. Beauquier, J., Datta, A.K., Gradinariu, M., Magniette, F.: Self-stabilization Local Mutual Exclution and Daemon Refinement. In: Herlihy, M.P. (ed.) DISC 2000. LNCS, vol. 1914. Springer, Heidelberg (2000) 11. Bellman, R.: On routing problem. Qurterly of Applied Mathematics 16(1), 87­90 (1958) 12. Burman, J., Kutten, S.: Time Optimal Asynchronous Self-stabilizing Spanning Tree (extended version), http://tx.technion.ac.il/ bjanna/ 13. Dela¨ et, S., Ducourthial, B., Tixeuil, S.: Self-stabilization with r-operators in Unreliable Directed Networks. TR 1361, LRI (2003) 14. Dela¨ et, S., Ducourthial, B., Tixeuil, S.: Self-stabilization with r-operators revised. In: JACIC (2006)

Time Optimal Asynchronous Self-stabilizing Spanning Tree

107

15. Dijkstra, E.W.: Self-stabilization in spite of Distributed Control. Comm. ACM 17, 643­644 (1974) 16. Dolev, S., Israeli, A., Moran, S.: Resource Bounds for Self-stabilizing Message Driven Protocols. In: PODC'91 (1991) 17. Dolev, S., Israeli, A., Moran, S.: Self-stabilization of Dynamic Systems Assuming Only Read/Write Atomicity. DC 7, 3­16 (1994) 18. Dolev, S., Israeli, A., Moran, S.: Uniform Dynamic Self-stabilizing Leader Election (extended abstract). In: Toueg, S., Kirousis, L.M., Spirakis, P.G. (eds.) Distributed Algorithms. LNCS, vol. 579. Springer, Heidelberg (1992) 19. Ducourthial, B., Tixeuil, S.: Self-stabilization with r-operators. DC 14(3), 147­162 (2001) 20. Garg, V.K., Agarwal, A.: Distributed Maintenance of A Spanning-Tree Using Labeled Tree Encoding. In: Cunha, J.C., Medeiros, P.D. (eds.) Euro-Par 2005. LNCS, vol. 3648. Springer, Heidelberg (2005) 21. G¨ artner, F.C.: A Survey of Self-Stabilizing Spanning-Tree Construction Algorithms. TR, EPFL (October 2003) 22. G¨ artner, F.C., Pagnia, H.: Time-Efficient Self-stabilizing Algorithms Through Hierarchical Structures. In: Huang, S.-T., Herman, T. (eds.) SSS 2003. LNCS, vol. 2704. Springer, Heidelberg (2003) 23. Gupta, S.K.S., Srimani, P.K.: Self-stabilizing Multicast Protocols for Ad Hoc Networks. JPDC 63(1) (2003) 24. Herault, T., Lemarinier, P., Peres, O., Pilard, L., Beauquier, J.: Self-stabilizing Spanning Tree Algorithm for Large Scale Systems. TR 1457, LRI (August 2006) 25. Higham, L., Liang, Z.: Self-stabilizing Minimum Spanning Tree Construction on Message-Passing Networks. In: Welch, J.L. (ed.) DISC 2001. LNCS, vol. 2180. Springer, Heidelberg (2001) 26. Nesterenko, M., Arora, A.: Stabilization-Preserving Atomicity Refinement. J. Parallel Distrib. Comput. 62(5), 766­791 (2002)

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links
J´ er´ emie Chalopin1 , , Shantanu Das2 , and Nicola Santoro3
LaBRI Universit´ e Bordeaux 1, Talence, France chalopin@labri.fr School of Information Technology and Engineering, University of Ottawa, Canada shantdas@site.uottawa.ca 3 School of Computer Science, Carleton University, Canada santoro@scs.carleton.ca
1

2

Abstract. A group of identical mobile agents moving asynchronously among the nodes of an anonymous network have to gather together in a single node of the graph. This problem known as the (asynchronous anonymous multi-agent) rendezvous problem has been studied extensively but only for networks that are safe or fault-free. In this paper, we consider the case when some of the edges in the network are dangerous or faulty such that any agent travelling along one of these edges would be destroyed. The objective is to minimize the number of agents that are destroyed and achieve rendezvous of all the surviving agents. We determine under what conditions this is possible and present algorithms for achieving rendezvous in such cases. Our algorithms are for arbitrary networks with an arbitrary number of dangerous channels; thus our model is a generalization of the case where all the dangerous channels lead to single node, called the Black Hole. We do not assume prior knowledge of the network topology; In fact, we show that knowledge of only a "tight" bound on the network size is sufficient for solving the problem, whenever it is solvable.

1
1.1

Introduction
The Problem

Consider a networked environment, modelled as a simple connected graph, in which operate a set of mobile computational entities, called agents or robots. A central problem in such systems is the so called rendezvous (or gathering) problem which requires the agents to meet together in a single node of the network. This problem has been extensively studied in the literature (see, for example, [1,3,10,15,17,19,23]) under a variety of models with different assumptions on the identity of the network nodes and/or of the agents (anonymous or distinct labels), the existence of timing bounds on the agents' actions (synchronous or asynchronous), the intercommunication mechanisms (whiteboards
Supported in part by grant ANR-06-SETI-015-03 awarded by the Agence Nationale de la Recherche.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 108­122, 2007. c Springer-Verlag Berlin Heidelberg 2007

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links

109

or pebbles/tokens), the amount and type of memory, etc. In spite of their widely different models, the existing studies on the rendezvous problems share the common assumption that the environment where the agents operate is safe. Unlike previous studies on the rendezvous problem, we consider the case when the environment where the rendezvous must take place, is not safe. In our model, some of the edges in the graph are harmful for the agents; specifically, any agent that attempts to traverse any such an edge (from either direction) simply disappears, without leaving any trace. The location of the unsafe links are initially unknown to the agents; we only assume that the unsafe links do not disconnect the network. Notice that if all the edges incident to a node u are unsafe, then node u can never be reached by any agent and is equivalent to a black hole, i.e., a node that destroys any incoming agent (e.g., [8,9,11,12,13,18]). In other words, the black hole model is just a specific case of the model considered in this paper. We investigate the problem in a very weak (and thus computationally difficult) setting: the network nodes do not have distinct identities (i.e., the network is anonymous), the agents are identical, and their actions (computations and movements) take a finite but otherwise unpredictable amount of time. The only previous result for rendezvous in faulty networks was in the case of the ring network containing two unsafe links leading to a single node--the black hole [13]. Our investigation is thus a generalization of these studies to networks of arbitrary topology that contain faults at arbitrary locations. 1.2 Our Results

In this paper we provide a full characterization of the rendezvous problem of asynchronous anonymous agents in anonymous networks with unsafe links. Assuming that the safe part of the network is connected, any port on a safe node which leads to an unsafe part of the network, is called a faulty link. We present the following results in this paper: ­ We first show that, if there are  unsafe links in the network and k agents, then it is not possible, in general, for k agents to rendezvous if k > k -  . ­ We then prove that the rendezvous of k - agents is deterministically possible only when the network is covering minimal. Even in this case, rendezvous is not possible if the agents do not know the size of the network or at least a tight upper bound. In fact, we prove that a loose upper bound n  Bn  2n is not sufficient. ­ We then show that this result is tight. In fact, we present an algorithm, RDV that requires only the knowledge of a tight upper bound B on the number of nodes n, such that n  B < 2n. This algorithm allows rendezvous of k -  agents in networks where such a rendezvous is possible; the rendezvous occurs with explicit termination for each surviving agent. ­ The total number of moves made by the agents during the execution of algorithm RDV is O(m(m + k )) where m is the number of edges in G. We prove that this cost is optimal; in fact, we show that solving rendezvous of k -  agents in networks where it is solvable, requires at least  (m(m + k )) moves, even when the network topology is known a priori.

110

J. Chalopin, S. Das, and N. Santoro

­ Finally we show that, there exists no effective algorithm for maximal rendezvous, i.e. there does not exist an algorithm that when executed on any arbitrary network achieves the rendezvous of as many agents as deterministically possible on that network. Due to the limitations of space, the proofs of some lemmas and theorems have been omitted; These can be found in the full paper. 1.3 Related Work

The problem of Rendezvous has been extensively studied mostly using randomized methods (see [1] for a survey). Among deterministic solutions to rendezvous, Yu and Yung [23] and Dessmark et al. [10] presented algorithms for agents with distinct labels. In the anonymous setting, the problem has been studied under different models (synchronous or asynchronous), using either whiteboards [3] or, pebbles/tokens [19]. Some of the recent studies have focussed on minimizing the memory required by the agents for rendezvous([15,17]). Most of these solutions are designed for anonymous graphs (i.e. graphs where nodes do not have distinct identities) which present the most challenging (i.e. computationally difficult) situations. The issue of computability in anonymous graphs, have been studied by many authors including Angluin [2], Yamashita and Kameda [22], Mazurkiewicz [20], Sakamoto [21], and Boldi and Vigna [4]. Most of these studies have concentrated on the problem of symmetry-breaking or leader election which is in fact, closely related (and sometimes equivalent [7]) to the rendezvous problem for mobile agents. However, all the above results are restricted to safe or fault-free networks. Recently attention has focused on designing mobile agent protocols for networks which are faulty, in particular, where there is a black hole, that is a harmful network site that destroys any visiting agent. The research on such networks have concentrated on locating the black hole. In asynchronous systems, this has been studied under two different methods--using whiteboards [11,12] or using tokens [14] to mark edges. The objective here is minimizing the number of agents that fall into the black hole and the number of moves. In the case of synchronous agents, the objective is to minimize the time taken by the surviving agents to locate the black hole [9,18]. The general case of multiple black holes has been considered only by Cooper et al. [8]. All these problems assume that the team of agents start from the same node, i.e. they are co-located. When the agents start from distinct nodes, it is very difficult to gather the agents while avoiding the black hole nodes. This has been studied earlier only in the case of ring networks containing a single black hole, by Dobrev et al. [13], where the authors give solutions to rendezvous and near-gathering assuming the knowledge of topology and the size of the network.

2
2.1

The Model and Definitions
The Model

The environment is modelled by the tuple (G, , p, ,  ) where G is an undirected connected graph,  is a set of agents and p specifies the initial placement of the

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links

111

agents in the graph G (i.e. A  , p(A) = v : v  V (G) ). The number of nodes is denoted by n = |V (G)| and the number of agents is denoted by k = | |. The agents can move from one node to its adjacent node by traversing the edge connecting them. The edges incident to a node v are locally oriented i.e. they are labelled as 1, 2, . . . , d(v ), where d(v ) is the degree of node v . Notice that each edge e = (u, v ) has two labels, one for the link or port at node u and another for the link at node v . The edge labelling of the graph G is specified by  = {v : v  V }, where for each vertex u, u : {(u, v )  E : v  V }  {1, 2, 3, . . . , d(u)} defines the labelling on its incident edges. For any edge (u, v ) we use (u, v ) to denote the pair (u (u, v ), v (u, v )). The function  : E (G)  {0, 1} denotes which edges are safe/faulty. An edge e  E (G) is safe if  (e) = 1 and faulty otherwise. The faults are permanent, so any edge that is faulty at the start of the algorithm remains so until the end and no new faulty edge appears during the execution of the algorithm. The node from where an agent A starts the algorithm (i.e. the initial location) is called the homebase of agent A. The agents are all identical (i.e. they do not have distinct names or labels) and they execute the same algorithm. An agent may enter the system at any time and at any location, and on entry, an agent immediately starts its individual execution of the algorithm. The system is totally asynchronous, such that every action performed by an agent takes a finite but otherwise unpredictable amount of time. As in previous papers on the subject, we assume that the agents communicate by reading and writing information on public whiteboards locally available at the nodes of the network. Thus, each node v  G has a whiteboard (which is a shared region of its memory) and any agent visiting node v can read or write to the whiteboard. Access to the whiteboard is restricted by fair mutual exclusion, so that, at most one agent can access the whiteboard of a node at the same time, and any requesting agent will be granted access within finite time. An agent that is granted access to the whiteboard at node v , is allowed to complete its activity at that node before relinquishing access to the whiteboard (i.e. access control is non preemptive). Note that it is not necessary for two agents A and B traversing the same edge e = (u, v ) of the graph, to arrive at node v in the same order in which they left node u. However, using the whiteboards at the nodes, it is easy to implement a first-in first-out (FIFO) strategy such that agents traversing an edge can be assumed to have reached their destination in order (i.e. an agent cannot overtake another while traversing an edge). For the rest of this paper, we shall assume this FIFO property; this will simplify the description of our algorithms. 2.2 Directed Graphs and Coverings

In this section, we present some definitions and results related to directed graphs and their coverings, which we use to characterize those network where rendezvous is possible. A directed graph(digraph) D = (V (D), A(D), sD , tD ) possibly having parallel arcs and self-loops, is defined by a set V (D) of vertices, a set A(D) of arcs and by two maps sD and tD that assign to each arc two elements of V (D) : a source and a target (in general, the subscripts will be omitted). A digraph D is

112

J. Chalopin, S. Das, and N. Santoro

strongly connected if for all vertices u, v  V (D), there exists a path between u and v . A symmetric digraph D is a digraph endowed with a symmetry, that is, an involution Sym : A(D)  A(D) such that for every a  A(D), s(a) = t(Sym(a)). A bidirectional network can be represented by a strongly connected symmetric digraph, where each edge of the network is represented by a pair of symmetric arcs. In this paper, we consider digraphs where the vertices and the arcs are labelled with labels from a recursive label set L and such digraphs will be denoted by (D, D ), where D : V (D)  A(D)  L is the labelling function. In general, the label on an arc a would be a pair (x, y ) and the labelling D should satisfy the property that if D (a) = (x, y ) then D (Sym(a)) = (y, x), for every arc a  D. A digraph homomorphism  between the digraph D and the digraph D is a mapping  : V (D)A(D)  V (D )A(D ) such that if u, v are vertices of D and a is an arc such that u = s(a) and v = t(a) then  (u) = s( (a)) and  (v ) = t( (a)). A homomorphism from (D, D ) to (D , D ) is a digraph homomorphism from D to D which preserves the labelling, i.e., such that D ( (x)) = D (x) for every x  V (D)  A(D). We now define the notion of graph coverings, borrowing the terminology of Boldi and Vigna[5]. A covering projection is a homomorphism  from D to D satisfying the following: (i) For each arc a of A(D ) and for each vertex v of V (D) such that (v ) = v = t(a ) there exists a unique arc a in A(D) such that t(a) = v and (a) = a . (ii) For each arc a of A(D ) and for each vertex v of V (D) such that (v ) = v = s(a ) there exists a unique arc a in A(D) such that s(a) = v and (a) = a . The fibre over a vertex v (resp. an arc a ) of D is the set -1 (v ) of vertices of D (resp. the set -1 (a ) of arcs of D). If a covering projection  : D  D exists, D is said to be a covering of D via  and D is called the base of . A symmetric digraph D is a symmetric covering of a symmetric digraph D via a homomorphism  if D is a covering of D via  such that a  A(D), (Sym(a)) = Sym((a)). A digraph D is symmetriccovering-minimal if there does not exist any graph D not isomorphic to D such that D is a symmetric covering of D . The notions of coverings extend to labelled digraphs in an obvious way: the homomorphisms must preserve the labelling. Given a labelled symmetric digraph (H, H ), the minimum base of (H, H ) is defined to be the labelled digraph (D, D ) such that (i) (H, H ) is a symmetric covering of (D, D ) and (ii) (D, D ) is symmetric covering minimal. The following results on digraph coverings were proved in [5]. Property 1. Given two non-empty strongly connected digraphs D, D , each covering projection  from D to D is surjective; moreover, all the fibres have the same cardinality. This cardinality is called the number of sheets of the covering. Property 2. If the digraph (H, H ) is a covering of (D, D ) via , then any execution of an algorithm P on (D, D ) can be lifted up to an execution on

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links

113

(H, H ), such that at the end of the execution, for any v  V (H ), v would be in the same state as (v ). 2.3 Definitions and Properties

Given any deterministic (distributed) algorithm P and a network (G, , p, ,  ), the order in which the various actions are performed by the agents defines an execution of the algorithm on the network (G, , p, ,  ). We define the synchronous execution of an algorithm P to be the particular execution where all agents start executing at exactly the same time and every action taken by any agent takes exactly one unit of time. We define the extended-view of the network (G, , p, ,  ) as the labelled digraph (H, H ) such that, H consists of two disjoint vertex sets V1 and V2 and a set of arcs A as defined below: ­ V1 = V (G); ­ H (v ) = |{A   : p(A) = v }|, v  V1 ; ­ For every safe edge e = (u, v )  E (G), there are two arcs a1 , a2  A such that s(a1 ) = t(a2 ) = u, s(a2 ) = t(a1 ) = v , and H (a1 ) = (u (e), v (e)), H (a2 ) = (v (e), u (e)). ­ For every faulty edge e = (u, v ), there are vertices u and v  V2 with H (u ) = H (v ) = -1 and arcs (u, u ), (u , u), (v, v ) and (v , v )  A with labels (e (u), 0), (0, e (u)), (e (v ), 0), and (0, e (u)) respectively; Here, the vertices in V1 represent the (safe) nodes of the network and the vertices in V2 represent (imaginary) Black-Holes. The label on a safe vertex v denotes the number of agents that started from the corresponding node, whereas the label on a black-hole vertex is always -1. Intuitively, the extended-view can be thought of as a canonical representation of the network. The following results follow from the definition of the extended-view of a network and the Properties 1 and 2. Lemma 1. For any deterministic algorithm P , a synchronous execution of P on the network (G,  , p, ,  ) is equivalent to a synchronous execution of algorithm P on the extended-view (H, H ), such that the final state of any node in G is exactly same as the state of the corresponding vertex in H . Lemma 2. If the extended view of two networks have same minimum-base (D, D ) then all nodes in the two networks which belong to the pre-image of a vertex v  D would always be in the same state, during a synchronous execution of any algorithm P .

3

Impossibility Results

In this section, we determine some necessary conditions for solution to the rendezvous problem. In the following, whenever the extended-view of a network is symmetric-covering minimal, we shall say the network is minimal.

114

J. Chalopin, S. Das, and N. Santoro

Lemma 3. In a network containing  dangerous links and k dispersed agents,  agents may die while executing any algorithm for rendezvous. Thus, it is not always possible to rendezvous more than k -  agents even if the network topology is known to the agents. Lemma 4. It is impossible to rendezvous k -  agents in a network whose extended-view is not symmetric-covering minimal.

G1

G2

(a)

G1

G2

(b)

(c)

Fig. 1. The network in (a) cannot be distinguished from the networks in (b) and (c) due to the slow edges. (The slow edges are marked by arrows and the dashed lines represent faulty edges).

In the next section, we show how to solve rendezvous of k -  agents in any network that is minimal. Thus, we have a complete characterization of networks where rendezvous of k -  agents is possible. However, solution to rendezvous requires at least some prior knowledge about the network, as we show below. Notice that since the network is asynchronous, an agent may take an arbitrarily long time to traverse some edge. Thus, a slow edge (i.e. one which the agents take a long time to traverse) is indistinguishable from a faulty edge. During the execution of an algorithm, the presence of slow edges may divide the network into two equal parts (for example, see Figure 1) and in such cases, it is not possible for the agents to terminate the algorithm unless an accurate estimate of network size is available. Lemma 5. It is impossible to solve rendezvous (with termination detection) of k -  agents even in minimal networks if the agents know only an upper bound B on the number of nodes n such that n  B  2n. The algorithm presented in this paper works only for networks which are minimal. We say that an algorithm P is an universally effective algorithm for rendezvous of w > 1 agents if, when executed on any network where rendezvous of w agents is possible, algorithm P always succeeds in achieving rendezvous within a finite time. We have the following negative result on the existence of such an algorithm.

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links
2 1 2 1

115

3

4

3

4

G1
4 3

G1

G1
4 3

G1

1

2

1

2

2

1

2

1

3

4

3

4

G1
2

G1

3

(a)

G1
4 1

(b)

(c)

Fig. 2. The networks in (a) and (b) have same topology but differ in the location of faulty edges (shown by dashed lines). (c) The minimum base for the network in (a).

Lemma 6. There does not exist any universally effective algorithm for rendezvous of 1 < w  k agents even if the network topology is known a-priori to the agents. Proof. Consider the two networks shown in Figure 2(a) and (b). Each network has the same topology and there are three faulty edges in each (but their locations are different). In the network of Figure 2(b) there are two edges (marked by arrows) which are very slow. Since slow edges edges can not be distinguished from faulty edges, the agents would not be able to determine whether they are in the first network or the second. Thus any rendezvous algorithm P (that terminates within a finite time) must achieve the same result in both networks. Notice that the first network has an extended view which is not minimal (the minimum base is shown in Figure 2(c)). Since algorithm P must fail to achieve rendezvous in the first network, it must also fail in the second one, even though it is possible to rendezvous in the second network. Thus algorithm P is not effective.

4

Solution Protocol

In this section we present an algorithm for solving Rendezvous in faulty networks, using the knowledge of only an upper bound B on the network size, such that n  B < 2n. As shown in the previous section, there is no effective algorithm for Rendezvous in faulty networks. Our algorithm always works for any network whose extended-view is covering minimal, achieving the rendezvous of the maximum number of agents possible (i.e. k -  agents). We also analyze the complexity of our algorithm and show that it is optimal in terms of the number of moves made by the agents. Theorem 1. For solving rendezvous of (k -  ) agents in an arbitrary network (G, , p, ,  ) without any knowledge other than the size of the network, the agents need to make at least  (m(m + k )) moves in total.

116

J. Chalopin, S. Das, and N. Santoro

4.1

The Algorithm for Rendezvous

We can ensure that no more than one agent dies while traversing the same link, using the cautious walk technique as in [13]. At each node, all the incident edges are considered to be unexplored in the beginning. Whenever an agent A at a node u has to traverse an unexplored edge e = (u, v ), agent A first marks link u (e) as "Being Explored" and if it is able to reach the other end v successfully, it immediately returns to node u and re-marks the link u (e) as "safe". During the algorithm we follow the rule that no agent ever traverses a link that is marked "Being Explored". This ensures no more than  agents may die during the algorithm. We now briefly describe our algorithm for rendezvous (Algorithm RDV). Due to the space constraint, we present only an oversimplified version of the algorithm without the minor technical details. The complete pseudo-code for the algorithm can be found in the full paper. At any stage of the algorithm, there are teams of agents, each team possessing a territory which is a connected acyclic subgraph of G (disjoint from other territories). Each team of agents tries to expand its territory until it spans a majority of the nodes. Once a team is able to acquire more than half the nodes of the network, it wins and agents from all other teams join the winning team to achieve rendezvous. Initially each territory consists of only the starting node(homebase) of an agent (all agents that start from that node are in this team). Note that if an agent on start-up, finds that its homebase has already been acquired by some other team, it simply joins this team. The algorithm proceeds in a series of exploring and competing rounds. In an exploring round, the team of agents try to expand its territory by exploring new edges and acquiring new nodes. On the other hand, in the competing round a team tries to defeat another team and conquering their territory. The competition between two teams occurs by comparison using the tuple (j, Code) where j is the round number and Code is an encoding of the territory(and its immediate neighborhood). The territory of each team is a rooted tree and the information stored in the root defines the status of the agents in the team (e.g. whether they are in an exploring round or competing round). Every other node in the tree stores a pointer to its parent in the tree. The status of the root can either INITEXPLORE, INIT-COMPETE, COMPETE, LOST, or END. Only one agent in a team can be competing state (this is called the active agent and all others are passive). When a team is in exploring round, the active agent initiates the exploration and thereafter every agent may participate in the exploration. Each agent explores one unexplored edge and if it succeeds in reaching the other side, it reports this to the root of the tree and the tree is updated accordingly. If the edge connects to a node that is already explored (by another agent or the same agent) then it is marked as tree edge (T-edge) and the new node becomes part of the tree. Otherwise it is marked as non-tree edge (NT-edge). In the algorithm below, T refers to the tree representing the territory to which the agent belongs. The root of T is denoted by r and Root Status(T ) is the status

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links

117

of the root as written on the whiteboard of r (A copy of T is also stored on the whiteboard). A node u is called a neighbor of the tree T if there is a NT-edge between u and some node in T . Each agent starts in active state, but may become passive or finished. The following steps are executed by an agent A: Algorithm RDV
< T , rN um > := Initialize; While (Status(A) = finished ) { Case(Status(A) is active and Root Status(T) is INIT-EXPLORE) { If (|T | > B/2) terminate; InitExplore; If (Root Status(T) = LOST ) become passive and exit Case; Set Root Status(T) to EXPLORE and become Passive; } Case( Status(A) is passive ) { / { EXPLORE, INIT-EXPLORE, END } ) While(Root Status(T)  Sleep until woken up; If(Root Status(T) is END) become finished and exit; Explore an unexplored edge and go back to r to update T ; If(Root Status(T) is EXPLORE) // i.e. no active agents become active and set Root Status(T) to INIT-COMPETE } Case(Status(A) is active and Root Status(T) is INIT-COMPETE) { do{ TOLD := T ; InitCompete(rN um); If (Root Status(T) = LOST ) become passive and exit Case; Result = Compete(rN um); rN um := rN um + 1; }While( TOLD = T ); If(Root Status(T) = LOST) Root Status(T):= INIT-EXPLORE; } }

PROCEDURE Initialize : If the homebase of the agent is already part of a tree, then the agent joins this team in passive state. Otherwise, it initializes the tree with only the homebase node and then starts the algorithm in active state with rN um = 1. PROCEDURE InitExplore : The agent initiates a new exploring round by traversing T writing "EXPLORE" on every node and waking up as many agents as needed for exploration (more precisely, it wakes-up x agents where x is the number of unexplored edges currently incident to T , unless less than x agents are available in T --in which case it wakes-up everyone). PROCEDURE InitCompete (j): The agent initiates the competing round j by traversing T , writing (COM P ET E, j ) on each node also assigning labels to the nodes in T . Next, it reads the labels written on the neighboring nodes and constructs an encoding of T and its neighbors, called CODE (this is used for comparisons). Finally it writes the CODE on every node in T .

118

J. Chalopin, S. Das, and N. Santoro

PROCEDURE Compete (j): During this procedure, agent A competes with each node u that is neighbor of T , until it wins or loses. If u is marked END, agent A terminates after writing END on every node of T , waking up any sleeping agents and merging with the tree containing u. Else, if u is in a bigger competing round or in the same competing round with a larger Code, then agent A loses (i.e. it becomes passive and goes back to its root to sleep). Otherwise if u is in exploring round or in a smaller competing round or same round but has smaller Code, then Invade(u) is invoked to determine if the result of the competition is win or loss. PROCEDURE Invade (u): The agent attempts to acquire the tree containing node u. The agent follows the father-links from u to reach the current root ru of u. At node ru it uses the usual comparison criteria and if ru is bigger or equal, then it loses (i.e. it becomes passive and goes back to its root to sleep). Otherwise it wins and it acquires the Tree rooted at ru , by reversing the father-links in the path from ru to v where v is the node in the agent's territory from where it started the Invade procedure. PROCEDURE terminate : The agent writes END on all nodes in T and then writes Root Status(T)=END at the root of T ; The agent now becomes finished and locally terminates. 4.2 Analysis of the Algorithm

Theorem 2. Algorithm RDV correctly solves Rendezvous for k -  agents in any network whose extended view is symmetric-covering minimal. This result follows from the following lemmas: Lemma 7. During the algorithm RDV, the following holds: (i) Each territory is a Tree. (ii) The territories are disjoint. (iii) There is at most one active agent in each territory. Proof. (i) Initially each territory is a tree by construction; addition of a new edge does not create cycles because only tree edges are added. When two trees are merged, one is `larger' than other and the active agent of the larger tree performs the merging by changing the father-links and updating the root. (Notice that there can not be two active agents in a territory.) Thus the merged territory is a tree. (ii) Each tree is a rooted tree and every node contains a pointer to its parent in the tree. Thus, a node cannot belong to two trees. (iii) Initially an agent becomes active only when it successfully constructs T and writes it at the root node. Thus, due to the mutual exclusion property of the whiteboards, there is only one active agent initially in the tree (all other agents in the same tree must start in passive state). A passive agent can become active only when there are no active agents in the tree. Lemma 8. There is no deadlock in the algorithm RDV.

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links

119

Proof. Notice that as long as there is some active agent, the algorithm progresses. In each tree T , there is at least one active agent unless root-status is EXPLORE. Suppose the root-status of every tree is EXPLORE, then each agent is currently exploring some unexplored edge. There are only  faulty links in the network and k >  , so at least one of the exploring agents must return safely from the exploration. This agent now becomes active. Lemma 9. (i) If the network is minimal, then exactly one node has RootStatus(T) = END. (ii) When one node has Root-Status(T) = END, every alive agent in G eventually joins this tree. (iii) (k -  ) agents eventually become finished and reach the node having Root-Status(T)= END. Proof. (i) First notice that due to the constraint on the bound B , only one Tree T can have a size greater than B/2. Due to Lemma 7, the trees are disjoint and each has a unique root. Thus, at most one node may have Root-Status(T)=END. We now show that at least one node eventually reaches Root-Status(T)=END. A team of agents in tree T stops expanding its Tree only when either RootStatus(T )=END or, all the trees neighboring T have the same CODE and round number. In the later case, the team starts an exploring round. Notice that there must exist a Tree T which has some non-faulty edges incident to it that are either unexplored or being explored (otherwise the network is either disconnected or, not minimal). So one of these edges would be added to T . Thus the size of T would keep increasing until it contains more than B/2 nodes and Root-Status(T ) = END. (ii) Every agent either dies or reaches a node labelled "END". All nodes that are labelled "END" are part of the same tree. (iii) Due to the use of cautious walk, at most  agents may die. Thus each of the surviving agent eventually reaches a node v labelled "END" and goes to the root of this tree which is the node having Root-Status(T)=END. We now analyze the cost of algorithm RDV. We first count the number of competing rounds and exploring rounds performed during the algorithm. Lemma 10. There are O(m + k ) competing rounds. The number of exploring rounds is at most the number of competing rounds. Proof. A new competing round is started whenever T is expanded, i.e. the new territory T contains one more edge or one more agent than the previous territory TOLD . Thus there can be at most (m+k) competing rounds. After every exploring round completes, there is one competing round. So, the number of exploring rounds can not be more than the number of competing rounds. Lemma 11. The agents make at most O(n(m + k )) moves in the all the exploring rounds combined. Proof. The Procedure InitExplore() makes |T | moves in a tree T . Only one agent (the active one) in every tree executes this procedure. So, this accounts for O(n)

120

J. Chalopin, S. Das, and N. Santoro

moves per exploring round and O(n(m + k )) moves in total. Other than that, every passive agent that is woken up makes O(n) moves to go to an unexplored edge, explore it and report it to the root. Since each unexplored edge will be explored once (or at most twice), this cost can be counted per newly explored edge. Thus, this accounts for O(n.m) moves. Lemma 12. The agents make at most O(m(m + k )) moves in all the competing rounds combined. Proof. Only the active agent in a tree participates in the competing round. During procedure InitCompete every edge in G is traversed a constant number of times; this accounts for O(m) moves per round. Similarly O(m) moves are made per round during procedure Compete, for the comparisons with each neighbor. Each execution of Invade() takes O(n) moves, because only tree-edges are traversed and no edge is traversed twice. Whenever an agent execute Invade(), it either wins or loses. A losing agent never competes again, so the total contribution from losing agents is O(k.n). Edges traversed by the wining agents are disjoint, so this accounts for O(n) moves per round. Due to the above lemmas and Theorem 1, we have the following result: Theorem 3. The moves complexity of algorithm RDV is O(m(m + k )). Thus algorithm RDV is optimal.

5

Conclusions

We considered the problem of rendezvous of mobile agent in a faulty network and showed that it is possible to rendezvous at most k -  in any network containing k dispersed agents and  faulty links. We determined the condition under which this is possible and gave an algorithm for solving the problem under this condition. The algorithm we presented is optimal in terms of the total number of moves made by the agents and requires no prior information about the network topology (except the size). Moreover, we showed that it is impossible to have an effective algorithm for rendezvous, one that always achieves the rendezvous of as many agents as possible in any given network. Notice that the only information needed by our algorithm is a strict upper bound on the number of nodes. We assumed that the faulty links do not disconnect the network. In the case of a disconnected network, we can still rendezvous the agents in a connected component if we know a good bound on its size. For example, if the component contains a majority of the nodes (i.e. more than half of them), then original network size can be used as the bound. In this case, if  is equal to the number of outgoing edges from the component then we can rendezvous k -  agents where k agents are initially located in this component. For networks containing a single black hole (which does not disconnect the network), our algorithm can be used to rendezvous k - dBH agents where dBH is the degree of the black-hole.

Rendezvous of Mobile Agents in Unknown Graphs with Faulty Links

121

For the results in this paper, we considered two optimization criteria --minimizing the number of agents that are destroyed and the number of moves taken by the surviving agents to rendezvous. It would be interesting to also consider the optimization of whiteboard memory or agent memory when solving the rendezvous problem in faulty networks.

References
1. Alpern, S., Gal, S.: The Theory of Search Games and Rendezvous. Kluwer, Dordrecht (2003) 2. Angluin, D.: Local and global properties in networks of processors. In: STOC '80. Proc. 12th ACM Symp. on Theory of Computing, pp. 82­93. ACM Press, New York (1980) 3. Barri` ere, L., Flocchini, P., Fraigniaud, P., Santoro, N.: Rendezvous and Election of Mobile Agents: Impact of Sense of Direction. Theory of Computing Systems 40(2), 143­162 (2007) 4. Boldi, P., Vigna, S.: An effective characterization of computability in anonymous networks. In: Welch, J.L. (ed.) DISC 2001. LNCS, vol. 2180, pp. 33­47. Springer, Heidelberg (2001) 5. Boldi, P., Vigna, S.: Fibrations of graphs. Discrete Math. 243, 21­66 (2002) 6. Chalopin, J., Das, S., Santoro, N.: Rendezvous of Mobile Agents in Anonymous Networks with Faulty Channels. Technical Report TR-2007-02, University of Ottawa (2007) 7. Chalopin, J., Godard, E., M´ etivier, Y., Ossamy, R.: Mobile agents algorithms versus message passing algorithms. In: Shvartsman, A.A. (ed.) OPODIS 2006. LNCS, vol. 4305. Springer, Heidelberg (2006) 8. Cooper, C., Klasing, R., Radzik, T.: Searching for black-hole faults in a network using multiple agents. In: Shvartsman, A.A. (ed.) OPODIS 2006. LNCS, vol. 4305, pp. 320­332. Springer, Heidelberg (2006) 9. Czyzowicz, J., Kowalski, D., Markou, E., Pelc, A.: Searching for a black hole in synchronous tree networks. Combinatorics, Probability and Computing (to appear, 2007) 10. Dessmark, A., Fraigniaud, P., Kowalski, D., Pelc, A.: Deterministic rendezvous in graphs. Algorithmica 46, 69­96 (2006) 11. Dobrev, S., Flocchini, P., Prencipe, G., Santoro, N.: Mobile agents searching for a black hole in an anonymous ring. Algorithmica (to appear, 2007) 12. Dobrev, S., Flocchini, P., Prencipe, G., Santoro, N.: Finding a black hole in an arbitrary network: optimal mobile agents protocols. Distributed Computing (to appear, 2007) 13. Dobrev, S., Flocchini, P., Prencipe, G., Santoro, N.: Multiple agents rendezvous in a ring in spite of a black hole. In: Papatriantafilou, M., Hunel, P. (eds.) OPODIS 2003. LNCS, vol. 3144, pp. 34­46. Springer, Heidelberg (2004) 14. Dobrev, S., Flocchini, P., Kralovic, R., Santoro, N.: Exploring a dangerous unknown graph using tokens. In: Proc. of 5th IFIP International Conference on Theoretical Computer Science (TCS'06) (2006) 15. Gasieniec, L., Kranakis, E., Krizanc, D., Zhang, X.: Optimal memory rendezvous of anonymous mobile agents in a unidirectional ring. In: Wiedermann, J., Tel, G.,  Pokorn´ y, J., Bielikov´ a, M., Stuller, J. (eds.) SOFSEM 2006. LNCS, vol. 3831, pp. 282­292. Springer, Heidelberg (2006)

122

J. Chalopin, S. Das, and N. Santoro

16. Godard, E., M´ etivier, Y., Muscholl, A.: Characterization of classes of graphs recognizable by local computations. Theory of Computing Systems 37(2), 249­293 (2004) 17. Klasing, R., Markou, E., Pelc, A.: Gathering asynchronous oblivious mobile robots in a ring. In: Asano, T. (ed.) ISAAC 2006. LNCS, vol. 4288, pp. 744­753. Springer, Heidelberg (2006) 18. Klasing, R., Markou, E., Radzik, T., Sarracco, F.: Hardness and approximation results for black hole search in arbitrary networks. Theoretical Computer Science (to appear, 2007) 19. Kranakis, E., Krizanc, D., Markou, E.: Mobile agent rendezvous in a synchronous torus. In: Correa, J.R., Hevia, A., Kiwi, M. (eds.) LATIN 2006. LNCS, vol. 3887, pp. 653­664. Springer, Heidelberg (2006) 20. Mazurkiewicz, A.: Distributed enumeration. Inf. Processing Letters 61(5), 233­239 (1997) 21. Sakamoto, N.: Comparison of initial conditions for distributed algorithms on anonymous networks. In: PODC '99. Proc. 18th ACM Symposium on Principles of Distributed Computing, pp. 173­179. ACM Press, New York (1999) 22. Yamashita, M., Kameda, T.: Computing on anonymous networks: Parts I and II. IEEE Trans. Parallel and Distributed Systems 7(1), 69­96 (1996) 23. Yu, X., Yung, M.: Agent rendezvous: A dynamic symmetry-breaking problem. In: Meyer auf der Heide, F., Monien, B. (eds.) ICALP 1996. LNCS, vol. 1099, pp. 610­621. Springer, Heidelberg (1996)

Weakening Failure Detectors for k-Set Agreement Via the Partition Approach
Wei Chen1 , Jialin Zhang2 , , Yu Chen1 , and Xuezheng Liu1
Microsoft Research Asia {weic,ychen,xueliu}@microsoft.com 2 Center for Advanced Study Tsinghua University zhanggl02@mails.tsinghua.edu.cn Abstract. In this paper, we propose the partition approach and define several new classes of partitioned failure detectors weaker than existing failure detectors for the k-set agreement problem in both the sharedmemory model and the message-passing model. In the shared-memory model with n + 1 processes, for any 2  k  n, we first propose a partitioned failure detector  k that solves k-set agreement with shared read/write registers and is strictly weaker than k , which was conjectured to be the weakest failure detector for k-set agreement in the sharedmemory model [19]. We then propose a series of partitioned failure detectors that can solve n-set agreement, yet they are strictly weaker than  [10], the weakest failure detector ever found before our work to circumvent any asynchronous impossible problems in the shared-memory model. We also define two new families of partitioned failure detectors in the message-passing model that are strictly weaker than the existing ones for k-set agreement. Our results demonstrate that the partition approach opens a new dimension for weakening failure detectors related to set agreement, and it is an effective approach to check whether a failure detector is the weakest one or not for set agreement. So far, all previous candidates for the weakest failure detectors of set agreement have been disproved by the partitioned failure detectors. Keywords: Failure detector, partitioned failure detectors, k-set agreement.
1

1

Introduction

Failure detector abstractions are first proposed by Chandra and Toueg in [3] to circumvent the impossibility result of consensus [9], and have since become a powerful technique to encapsulate system conditions needed to solve many distributed computing problems. Among them the problem of k -set agreement has received many attention from the research community. Informally, in k -set agreement each process proposes some value and eventually all correct processes (those
This work was supported in part by the National Natural Science Foundation of China Grant 60553001, and the National Basic Research Program of China Grant 2007CB807900,2007CB807901.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 123­138, 2007. c Springer-Verlag Berlin Heidelberg 2007

124

W. Chen et al.

that do not crash) decide on at most k different values [4]. It has been shown that k -set agreement cannot be solved in asynchronous systems when k or more processes may crash [1,12,20]. In recent years, a number of studies have focused on failure detectors for solving k -set agreement problem [21,18,11,16,17,19,10,7]. These studies form the collective effort in the pursuit of the weakest failure detector for k -set agreement, a goal yet to be reached. A particular candidate k was conjectured to be the weakest failure detector for wait-free k -set agreement [19] in the shared-memory model. Consider distributed shared-memory model with n + 1 processes. In a very recent paper [10], Guerraoui et.al define a new class of failure detectors  and show that among a wide range of failure detectors defined as eventually stable failure detectors,  is the weakest one necessary to solve any impossible problem in shared-memory distributed systems, and  solves the n-set agreement problem. The  failure detector disproves the conjecture on k for the case of k = n. For a general k , a generalized  k is proposed to solve k -set agreement, but only when at most k processes may crash, so it does not disprove the conjecture on k for wait-free k -set agreement. The eventually stable failure detectors encompass most failure detectors known to solve distributed decision tasks in the shared-memory model prior to [10], as the authors claimed. Therefore, as the title of their paper says, indeed  is the weakest failure detector ever found that solves any impossible problem in distributed computing. In this paper, we introduce a new breed of failure detectors -- partitioned failure detectors -- that could be made strictly weaker than k and  but are still strong enough to solve the set agreement problem. Our motivation is based on the following observation: In k -set agreement when k > 1, different processes may decide on different values, and thus it is possible that processes may be partitioned to different components, each of which decides on different values but together they still decide on at most k values. In other words, k -set agreement (with k > 1) exhibits the partition nature. The partitioned failure detectors are defined by consistently applying a method that captures the partition nature to weaken existing failure detectors, for which we called the partition approach. In the partition approach, failure detectors partition the processes into multiple components and only processes in one of the components (called a live component) are required to satisfy all safety and liveness properties (of an existing failure detector), while processes in other components only need to satisfy safety properties. Since those processes in non-live components may generate quite arbitrary failure detector outputs, intuitively the partitioned failure detectors are a new breed that does not fall into the eventually stable failure detectors covered by [10]. We study the partitioned failure detectors in both the shared-memory model and the message-passing model. In the main part of this paper, we apply the partition approach to failure detectors k and  in the shared-memory model to define weaker failure detectors. More specifically, we first define a new class of failure detectors  k by applying static partitions to k . We show that  k

Weakening Failure Detectors for k-Set Agreement results in [10] 2 n-1 n  0   n  n-1  n  n-1  2  1  2  1 1

125

Fig. 1. Relationship diagram for failure detectors in the shared-memory model (n  3). If A  B , then A can be transformed into B . If there is no directed path from A to B , then A cannot be transformed into B (Footnote 1 contains the only exception).

is strong enough to solve k -set agreement with shared read/write registers but it is not comparable with  , for all k = 2, 3, . . . , n. One direct consequence is that  k is strictly weaker than k (because k is stronger than  ), which disproves the conjecture that k is the weakest failure detector for wait-free k -set agreement in the shared-memory model for any k  2. Moreover,  k is the first failure detector class that solves k -set agreement (for generic k ) but is incomparable with  . For example, even though failure detector  2 solves 2-set agreement, it is not stronger than  . Next, we define failure detectors weaker than  but are still strong enough to solve n-set agreement. We achieve this by mixing some of the properties of  k and  and define another class of partitioned failure detectors  k . We show that for any 1  k  n,  k can still solve n-set agreement but it is strictly weaker than both  k and  . Moreover, as k increases, the strength of  k is strictly weakened. Hence, we find a family of n different failure detector classes strictly weaker than  , which is the weakest one ever found before our work. Figure 1 characterizes the exact relationship among all failure detectors we proposed in this paper for the shared-memory model and the previously defined ones k and  . Note that every nonexistent directed path in the figure corresponds to an impossible transformation from the source class to the destination class, with only one exception.1 Since  is already very weak, one can imagine that it would be very delicate to define the new partitioned failure detectors and prove that they are incomparable to or strictly weaker than  . Indeed, the definitions of failure detectors are subtle, and the proofs of the impossible transformations are the most delicate and technically involved. We also apply the partition approach to failure detectors k ×  in the message-passing model, where  is the class of quorum failure detectors needed to work with k to solve k -set agreement in the message-passing model. We
1

The exception is the following problem that is still open: Can  k be transformed into  k-1 for any k  2? However, we have proven that  k+1 cannot be transformed into  k-1 for any k  2.

126

W. Chen et al.

define two new families of partitioned failure detectors that are strictly weaker than k ×  but are strong enough to solve k -set agreement in the messagepassing model. These partitioned failure detectors are different from the ones in the shared-memory model in that they integrate the partition of quorums in their definitions. Moreover, one family of failure detectors incorporates dynamic splitting of partitions, while all failure detectors in the shared-memory model are statically partitioned. Our results not only show a number of new failure detectors that are strictly weaker than existing ones such as k and  , but more importantly, they demonstrate the power of the partition approach: The partition approach opens a new dimension for weakening various failure detectors related to set agreement, and it is an effective approach to check whether a failure detector could be the weakest one solving set agreement or not. Using the approach, we have successfully shown that (1) k is not the weakest failure detector for k -set agreement in the shared-memory model for any k  2; (2)  is not the weakest failure detector for n-set agreement in the shared-memory model; and (3) k ×  is not the weakest failure detector for k -set agreement in the message-passing model for any k  2. So far, all failure detectors that were considered as the candidates for the weakest failure detectors for set agreement have been disproved using our partition approach. Therefore, we believe that partitioned failure detectors demonstrate the flexibility in achieving set agreement, and it is important to use the partition approach as an effective research tool in our pursuit to the ultimate weakest failure detectors for set agreement. The rest of the paper is organized as follows. Section 2 provides the sharedmemory model used in our paper. Section 3 defines  k and shows how it solves k -set agreement. Section 4 defines  k . Section 5 provides a central place to show the relationship among all failure detectors in the shared-memory model as captured by Figure 1. Section 6 summarizes the results in the message-passing model. We conclude the paper in Section 7. Further results including some k -set agreement algorithms and all correctness proofs are covered by two technical reports [7,5] on message-passing model and shared-memory model, respectively.

2

Model

We consider asynchronous shared-memory distributed systems augmented with failure detectors. Our model is the same as the model in [10], which is based on the models of [13,14,2]. We provide the necessary details of the model below. We consider a system with n +1 processes P = {p1 , p2 , . . . , pn+1 } where n  1. Let T be the set of global time values, which are non-negative integers. Processes do not have access to the global time. A failure pattern F is a function from T to 2P , such that F (t) is the set of processes that have failed by time t. Failed processes do not recover, i.e., F (t)  F (t +1) for all t  T . Let correct(F ) denote the set of correct processes, those that do not crash in F . A process is faulty if it is not correct. A failure detector history H is a function from P × T to an output range R, such that H (p, t) is the output of the failure detector module of process

Weakening Failure Detectors for k-Set Agreement

127

p  P at time t  T . A failure detector D is a function from each failure pattern to a set of failure detector histories, representing the possible failure detector outputs under failure pattern F . Processes communicate with each other by writing to and reading from shared atomic registers. A deterministic algorithm A using a failure detector D is a collection of n + 1 deterministic automata, one for each process. Processes execute by taking steps. In each step, a process p: (a) reads from a shared register to obtain a value, or writes a value to a shared register, or queries its failure detector module, based on its current local state; and (b) transitions its current state to a new state, based on its current state, the value returned from the read or from the failure detector module, and the algorithm automaton on p. Each step is completed at one time point t, but the process may crash in the middle of taking its step. A run of algorithm A with failure detector D under a failure pattern F is an infinite sequence of steps such that every correct process takes an infinite number of steps and no faulty process takes any step after it crashes. We say that a failure detector class C1 is weaker than a failure detector class C2 , if there is a transformation algorithm T such that using any failure detector D2  C2 , algorithm T implements a failure detector D1  C1 . By implementing D1 we mean that for any run of algorithm T with failure detector D2 under a failure pattern F , T generates the outputs of D1 as a distributed variable D1 -output such that there exists failure detector history H  D1 (F ) and H (p, t) = D1 -output(p, t) for all p  P and all t  T , where D1 -output(p, t) is the value of the variable D1 C2 and also output on p at time t. If C1 is weaker than C2 , we denote it as C1 refer to it as C2 can be transformed into C1 . if C1 C2 and C2 C1 , we say that C1 is strictly weaker than C2 and denote it as C1  C2 . If C1 C2 and C2 C1 , we say that C1 and C2 are equivalent and denote it as C1  C2 . In k -set agreement with 1  k  n, each process proposes a value, and makes an irrevocable decision on one value. It needs to satisfy the following three properties: (1) Validity: If a process decides v , then v has been proposed by some process. (2) Uniform k -Agreement: There are at most k different decision values. (3) Termination: Eventually all correct processes decide. Two related failure detector classes are k and  . Failure detectors in k output a subset of P of size at most k , and there is a time after which all processes always output the same nonempty set, which contains at least one correct processes. Failure detectors in  also output a subset of P , and there is a time after which all processes always output the same nonempty set, which is not exactly the set of correct processes.

3
3.1

Failure Detector  k
Specification of  k

The class of partitioned failure detectors  k is obtained by applying static partitions to k , as explained below. The output of  k for process p is a tuple (isLeader, lbound, cid), where isLeader is a boolean value indicating whether this process is a leader or not, lbound is a non-negative integer indicating the upper

128

W. Chen et al.

bound on the number of possible leaders in p's partitioned component, and cid is a component ID drawn from an ID set I or is a special value   I . The cid output indicates the component the process belongs to and could be  for an initial period before the failure detector decides on a partition. For a failure detector output x, we use x.v to denote the field v of x, where v could be isLeader, lbound, or cid in the case of  k . We say that a process p is an eventual leader (under a failure pattern F and a failure detector history H ) if p is correct and there is a time after which the isLeader output on p is always True. A partition of P is  = {P1 , . . . , Ps }, where s  1 and Pi 's are non-empty subsets of P such that they do not intersect with one another and their union is P . For a process p, we use  [p] to denote the partitioned component that contains p. For a component Pj  P (under a failure pattern F and a failure detector history H ), we define lbound(Pj ) = max{H (p, t).lbound | t  T , p  Pj \ F (t)},2 and Leaders(Pj ) = {p  Pj  correct(F ) | t, t > t, H (p, t ).isLeader = True}. The value lbound(Pj ) is the maximum lbound value among processes in component Pj , while Leaders(Pj ) is the set of eventual leaders in Pj . A failure detector D is in the class  k if for any failure pattern F and any failure detector history H  D(F ), there exists a partition  = {P1 , . . . , Ps } of P , such that the following properties hold. First, the cid output needs to satisfy these properties: (C 1) The cid outputs on all correct processes eventually always output non- values. Formally, t0  T , p  correct(F ), t  t0 , H (p, t).cid = . (C 2) The non- cid outputs distinguish different components. Formally, t1 , t2  T , p1  F (t1 ), p2  F (t2 ), (H (p1 , t1 ).cid =   H (p2 , t2 ).cid = )  ((H (p1 , t1 ).cid = H (p2 , t2 ).cid)  ( [p1 ] =  [p2 ])). Next, the isLeader and lbound outputs satisfy the following set of safety and liveness properties. The safety property is: ( 1) The sum of the maximum lbound outputs in all partitioned components s does not exceed k . Formally, j =1 lbound(Pj )  k . The liveness part specifies that there exists one partitioned component Pj such that: ( 2) Eventually lbound outputs by all processes in Pj are the same. Formally, t0  T , t1 , t2  t0 , p1  Pj \ F (t1 ), p2  Pj \ F (t2 ), H (p1 , t1 ).lbound = H (p2 , t2 ).lbound. ( 3) Eventually the isLeader outputs on any correct process in Pj do not change. Formally, t0  T , t > t0 , p  Pj \ F (t), H (p, t).isLeader = H (p, t0 ).isLeader. ( 4) There is at least one eventual leader. Formally, |Leaders(Pj )|  1. ( 5) The number of eventual leaders is eventually bounded by the lbound outputs. Formally, t0  T , t  t0 , |Leaders(Pj )|  H (p, t).lbound.
2

As a convention, max  = 0.

Weakening Failure Detectors for k-Set Agreement

129

We call a component that satisfies the liveness properties ( 2­5) a live component, and other components non-live components. Let ki = lbound(Pi ). Intuitively, each component Pi has a failure detector with the safety properties of ki restricted to Pi ,3 while at least one component Pj also satisfies all liveness properties of kj . Intuitively, this is to guarantee that when running a k -set agreement algorithm with  k , each component Pi may decide on at most ki values, so with ( 1) there are at most k decisions, while the live component Pj can make progress and decide eventually. The strength of  k is fully characterized by Figure 1. We defer to Section 5 as a central place to study and compare the strength of all proposed failure detectors and avoid repetitions. We summarize the strength of  k comparing with k and  in the following theorem. Theorem 1. The followings hold regarding the strength of  k . (1)  1  1 . j and j  k (2)  k  j for all k  2, j  1, and k  j . (3)  k for all k  2 and k < j  n. (4)  k   k-1 for all k  2. (5)  k  and   k , for all k  2. The key result is that  k is incomparable with  for all k  2. Therefore,  k is a new class of failure detectors that is strictly weaker than k , but is strong enough to solve k -set agreement in shared-memory systems with arbitrary failure patterns. It is the only class known (to our best knowledge) that solves k -set agreement with arbitrary failure patterns and is strictly weaker than k and is incomparable with  .4 3.2 Solving k-Set Agreement with  k

The algorithm using  k to solve k -set agreement is based on an extension of the k ­ converge algorithm presented in [21]. The original k ­ converge algorithm forces every participant to use the same value of "k ". With  k failure detectors, we need processes in each component to try to converge on some decisions, the number of which is bounded by the lbound output of the failure detector. Therefore we extend the k ­ converge algorithm by moving "k " into the parameter of the routine and rename the routine to converge(). We adjust the specification of converge() as follows. Routine converge() takes in three parameters: is the upper bound on the number of values can be committed (this parameter corresponds to the "k " in k ­converge), p is the process identifier, and v is the input value of the process. It outputs a pair (c, v ), where c is a boolean and v is one of the input value. When p outputs (c, v ), we say that p picks v , and if c = True, we say that p commits to v . The routine satisfies the following properties: (1) C-Termination: Every correct process picks some value. (2) C-Validity: If a process p picks value
3 4

In [6] we show that a variation of failure detectors that output isLeader and lbound, named k , is equivalent to k failure detectors. The  k failure detector proposed in [10] only solves k-set agreement in systems with at most k failures.

130

W. Chen et al.

Shared variables: Register D, initially  converge() instances: converge[ ][ ] Output of failure detector   k on process pi : isLeaderi , lboundi , cidi Code for process pi : 1 v  the input value of pi 2 repeat 3 cid  cidi 4 until cid =  5 r0 6 repeat 7 c  False 8 if isLeaderi = True then 9 r r+1 10 (c, v )  converge[cid][r ](lboundi , i, v ) 11 if c = True then 12 D  v ; return (D) 13 until D =  14 return (D )

Fig. 2. k-set agreement algorithm using  k

v , then some process q invoked converge() with parameter v . (3) C-Agreement: If a process p commits to a value, then at most max values are picked, where max is the maximum that processes pass into converge(). (4) Convergence: If all processes use the same value in the parameter ( > 0), and if there are no more than distinct input values, then every process that picks a value commits. The first two properties are the same as in [21], while the last two properties are adjusted to accommodate different input values of . Although the interface and the specification are changed, the algorithm is exactly the same as in [21], and the proof only needs some minor adjustment. The algorithm and its proof are included in [5]. Based on the converge() routine, we provide an algorithm to solve k -set agreement using  k in Figure 2. The algorithm is straightforward. We use cid output of failure detectors to isolate each component and make sure only processes in the same component could run the same instance of converge() routine. Within a component, only those processes with isLeader output being True can run converge() instances. Each converge() instance only uses the output of the previous converge() instance as the input, which is important to guarantee the safety of the algorithm. In any converge() instance if some process p commits to a value v , then p writes v to a shared variable D and decides on v , and eventually all correct processes will see a non- D value and decide. The following theorem summarizes the correctness of the algorithm. Theorem 2. Algorithm in Figure 2 solves k -set agreement using failure detectors in  k , for any k  1. Proof. It's obvious that k -set Validity holds.

Weakening Failure Detectors for k-Set Agreement

131

For Uniform k -Agreement, we only need to consider decisions made in line 12, since decisions made in line 14 do not generate new decision values. Consider every component Pi . If some process decides in line 12, we consider the earliest such decision, say by a process p  Pi . Process p decides v because it commits to v in an instance converge[cid][r](). By the C-Agreement property of converge(), at most max values can be picked in this converge[cid][r]() instance, where max is the maximum lbound values in the input of this instance. Since the algorithm guarantees for any r > r, instances converge[cid][r ]() only uses the values picked in instance converge[cid][r](), we know that there are at most max values can be decided in line 12 by processes in component Pi . By definition, max  lbound(Pi ). Then, by property ( 1), there are at most k values that can be decided. So Uniform k -Agreement holds. For k -set Termination, first by property (C 2) all correct processes eventually exit the loop in lines 2­4. In the live component Pj that satisfies ( 2­5), eventually there is at least one correct process and at most processes in Pj invoking converge(), where is the eventually converged lbound output value. Moreover, all these processes invoke converge() with the same first parameter value . Thus, the C-Termination and Convergence properties guarantee that all correct processes in Pj eventually commit to some value in some converge() instance. Therefore, eventually D is written. Once D is written, all correct processes eventually decide. 2

4

Failure Detector  k

After defining  k , our next step is to find a mixture of  k and  such that the new failure detectors are weaker than both and are still strong enough to solve n-set agreement. Since we know that  k and  are not comparable, it immediately means that the new failure detectors are strictly weaker than both  k and  . This leads us to the discovery of failure detectors  k . The output of  k for process p is a tuple (S, lbound, cid), where S is a subset of P that informally matches the output of  , and lbound and cid outputs have the same value range and same informal meaning as the ones in  k . For a component Pj , let correct(Pj ) = correct(F )  Pj , the set of correct processes in Pj (under a failure pattern F ). A failure detector D is in the class  k if for any failure pattern F and any failure detector history H  D(F ), there exists a partition  = {P1 , . . . , Ps } of P , such that the following properties hold. The cid properties and safety properties are the same as  k , namely (C 1), (C 2), and ( 1). The liveness part specifies that there exists one partitioned component Pj such that ( 2) of  k and the following property hold: ( 1) Pj contains at least one correct process, and eventually all correct processes in Pj output the same S  Pj such that S is not the set of correct processes in Pj and either S =  or the number of correct processes is bounded by the eventual lbound output. Formally, correct(Pj ) =   S0 

132

W. Chen et al.

Pj , S0 = correct(Pj ), t0 , (p  correct(Pj ), t > t0 , (H (p, t).S = S0  (S0 =   |correct(Pj )|  H (p, t).lbound))). We call a component that satisfies the liveness properties ( 2) and ( 1) a live component, and other components non-live components. Intuitively, in the live component Pj , the S output behaves almost the same as the output of  , except that S may eventually stabilize to , in which case the number of correct processes in Pj must be bounded by the eventual lbound output. This mixture is important in making  k strictly weaker than  . In particular,  0 is welldefined since lbound outputs could always be 0. However, in  0 the above mixture of requirements on S and on lbound is gone, and we will show that  0 is equivalent to  (the proof is not straightforward though). The follow theorem summarizes the results on the strength of  k comparing with  k and  , which is captured in Figure 1 and will be studied in Section 5. The key result is that  k is strictly weaker than  for any k  1, and as k increases, its strength is strictly weakened. Therefore, we found a new family of n classes of failure detectors that are all strictly weaker than  . It not only shows that  is not the weakest failure detector ever, but also suggests that there are still quite some room under  to fit in non-trivial failure detectors. Theorem 3. The followings hold regarding the strength of  k . (1)  0   . (2)  k   k-1 for all k  1. (3)  j  k for all 1  k  n and 1  j  n. (4)  k  j for all k  j  1. (5)  k  j for all j  k + 2 and k  1. The algorithm that solves n-set agreement using  k is based on the algorithm using  in [10], with modifications to (a) isolate the algorithm for each individual component; (b) obtain the size of each component; and (c) deal with the case that S =  in the live component. The full algorithm and its proof are included in [5].

5

Comparing Failure Detectors

This section is the central place to show all the results captured in Figure 1 and stated in Theorems 1 and 3. Since  is already a very weak failure detector, one can imagine that it would be a subtle and delicate task to show that under  there are still such structure in which a series of failure detectors have various strengths. Indeed, besides those obvious transformations, other results on possible or impossible transformations are quite delicate and require subtle techniques to prove them (and a few of them are still open). These proofs really show the subtle relationship between the failure detectors. Unfortunately, due to the space constraint, we can only include the full proofs in [5]. To compensate, we provide intuitive ideas and proof outlines for some key proofs. 5.1 Possible Transformations

For possible transformations, we need to prove all the arrows in Figure 1. Most transformations are obvious from the failure detector definitions.

Weakening Failure Detectors for k-Set Agreement

133

Lemma 1. (1)  k  k .

 k-1 ; (2)  k

 k-1 ; (3)  k

k ; (4)

Proof. The first two parts hold directly by the definition of failure detectors. The last two parts hold because we can treat k and  as a special case of partitioned failure detectors with only a single component P . 2 Lemma 2.  k  k for all k  1.

Proof Outline. For the transformation from  k to  k , the idea is for each component to come up with the set of at most lbound leaders, then the S output of  k is the complement of the leader set with respect to the component, and lbound and cid outputs of  k are copied from  k . The key is that for a live component, the leader set stabilizes and contains at least one correct process. Therefore, its complement S cannot be the set of correct processes. Moreover, if S = , it means that all processes in the component are eventual leaders, in which case the lbound must be at least the number of correct processes in the component. The transformation still needs to solve the problem of estimating the membership of each component, which is addressed in the full transformation algorithm and its proof in [5]. 2 Lemma 3. (1) 1  1 ; (2)   0

The transformations for the above lemma are not straigthforward [5]. 5.2 Impossible Transformations

Proving the impossible transformations is the critical step to establish the results of this paper. For these proofs, it is sometimes convenient to view it as an adversary trying to defeat any possible transformations. The adversary can (a) see the current output generated by a transformation; (b) manipulate the outputs of the failure detector to be transformed; (c) schedule the executions of processes; and (d) crash processes to prevent the transformation from succeeding. Among all the impossible transformations captured by the non-existent directed path in Figure 1, several of them are critical ones, meaning that their impossibility implies the rest impossible transformations. This is based on the fact that if we show that C1 C2 , then for all C3 C1 and all C4 C2 , we have C3 C4 . The following lemma shows one such critical impossible transformations. Lemma 4.  2 cannot be transformed into  , i.e.,  2 .

Proof Outline. We know that n can be transformed to  easily by taking the complement of the n output. The reason that this transformation cannot be adapted to  k is that  k allows a live component Pj in which all processes are eventual leaders and lbound stabilizes to |Pj |. If we take the complement of the leader set in Pj with respect to Pj we get an empty set. The proof explores this basic idea.

134

W. Chen et al.

In the case of  2 , suppose for a contradiction that there is a transformation T from  2 to  . The adversary constructs a run in which the  2 has a partition  = {P1 , P2 }, where P1 = {p}. It sets lbound of every process to 1 and p's isLeader always to True, making P1 a live component of  2 . It will manipulate the isLeader outputs for processes in P2 to create a contradiction. Whenever the S output of  in P stabilizes to some subset Si , the adversary suppresses all processes in P \ Si (i.e., prohibit these processes from taking any steps) for long enough time to force T to stabilize the S output to a different set Si+1 = Si , because Si appears to be the exact set of correct processes. Once T changes the S output, the adversary releases the suppressed processes so that they take some steps, and then it repeats the procedure for Si+1 , and so on. The adversary can keep doing so because P \ Si contains either p or some process in P2 , and thus it can always set isLeader of some process in P \ Si to True without violating the  2 requirement. The result is that the adversary forces T into an infinite run in which the S output never stabilizes, a contradiction. 2 Lemma 4 implies that for all  k with k  2,  k cannot be transformed into  . This is the first key result. Moreover, because  k can be transformed into  k , Lemma 4 further implies that  k is strictly weaker than  , the second key result of the paper. Next lemma shows another key result of the paper. Lemma 5.  cannot be transformed into  n when n  2. Proof Outline. Suppose there is a transformation T . If the partition of  n generated by transformation T contains only a single component, then the proof is the same as proving  cannot be transformed into n in [10]. If the partition of  n has at least two components, let P1 be one of the components. The adversary first sets the  output to P \ P1 , and then repeatedly suppress the leader processes in all components that are potentially live components for  n (these are called quasi-live components in the proofs), the purpose of which is to construct an infinite run in which there is no live component. The only way the transformation can counter this measure is by setting the lbound outputs of processes in P1 to |P1 |. But the adversary can counter this again by crashing all processes in P1 , setting  output to P1 , and re-apply the suppression technique. The result is a run in which no live component exists. The key is that the adversary need to wait until the lbound output on P1 is at least the size of a component to crash the component. This guarantees that the transformation cannot set lbound on P \ P1 to |P \ P1 | to defeat the adversary. 2 Lemma 4 and 5 establish that  and  k with k  2 are not comparable. Together with the possible transformations of Lemma 2, they immediately imply that  k is strictly weaker than both  and  k for any k  2. Next lemma summarizes all other critical impossible transformations proven so far. The proofs to these results are technically involved and can be found in [5]. Lemma 6. The following results hold: (1) k  k-1 for any k  2. (2)  k  k-1 for any k  1. (3)  k+1  k-1 for any k  2.

Weakening Failure Detectors for k-Set Agreement

135

In conclusion, Theorem 1 is implied by Lemma 1(1)(3), Lemma 3(1), Lemma 4, Lemma 5 and Lemma 6(1). Theorem 3 is implied by Lemma 1(2)(4), Lemma 3(2) and Lemma 6(2)(3). There are still an open problem left before we can completely characterize all relationships in Figure 1. It is whether  k can be transformed into  k-1 for any k  2. We conjecture that this transformation is impossible. If so, Figure 1 is indeed a full characterization of all relationships.

6

Results in the Message-Passing Model

Partition approach can also be applied in the message-passing model to define weaker failure detectors for k -set agreement. We briefly summarize some of the results we obtained in the message-passing model. The complete results are included in [7]. In the message-passing model, it is shown in [17] that besides k a majority of correct processes is required to solve k -set agreement. The majority requirement can be generalized to the class of quorum failure detectors  defined in [8]: a failure detector in  outputs a set of processes called quorum such that: ( 1) any two quorums intersect; and ( 2) eventually all quorums contain only correct processes. Thus, we applied the partition approach to the class of failure detectors k ×  to define weaker failure detectors.5 We first applies static partitions to k ×  and define k , which is similar to  k but replacing the cid output with the quorum output. More specifically, the output of a failure detector D in k for process p is a tuple (isLeader, lbound, Quorum), where isLeader is a Boolean value indicating whether this process is a leader, lbound is a non-negative integer indicating the upper bound on the number of possible leaders in p's partitioned component, and Quorum  P . A failure detector D is in the class k if for any failure pattern F and any failure detector history H  D(F ), there exists a partition  = {P1 , . . . , Ps } of P , such that H satisfies the following set of safety and liveness properties. The safety properties are ( 1) as for  k and the following two properties related to the quorum outputs: ( 1) The quorum output of a process p is always contained within p's partitioned component. Formally, t  T , p  F (t), H (p, t).Quorum   [p]. ( 2) The quorum outputs in the same partitioned component always intersect. Formally, t1 , t2  T , p1  F (t1 ), p2  F (t2 ),  [p1 ] =  [p2 ]  H (p1 , t1 ).Quorum  H (p2 , t2 ).Quorum = . The liveness part specifies that there exists one partitioned component Pj such that the properties ( 2­5) of  k hold plus the following: ( 3) Eventually the quorum outputs by all processes in Pj contain only correct processes. Formally t0  T , t  t0 , p  Pj \ F (t), H (p, t).Quorum  correct(F ).
5

Given two classes of failure detectors C1 and C2 , class C1 × C2 is the cross-product of the two, i.e., C1 × C2 = {(D1 , D2 ) | D1  C1 , D2  C2 }.

136

W. Chen et al. 1 ×  2 ×  k-1 ×  k ×   k -1 k
S k -1 S k

1 2
S 1 S 2

Fig. 3. Relationship diagram for failure detectors in the message-passing model. All failure detector classes in the diagram can be used to solve k-set agreement (n  2k - 2 S is required to show that transformations from k ×  to k -1 and stronger classes are impossible).

From the definition, we can see that k follows the partition approach and is a static partitioning of k ×  : each component Pi has a failure detector with all the safety properties of ki ×  resticted to Pi where ki = lbound(Pi ) and ki  k , while at least one component Pj also satisfies all liveness properties of kj ×  . Next we further weaken k by allowing dynamic splitting of components S S . Failure detectors in k during the run, which leads to the definition of k output a tuple (isLeader, lbound, Quorum, cid). Informally, a failure detector in S k allows partitioned components to further split during the run, but it uses cid to differenciate different components and requires the quorum outputs in a component after the splitting intersects with all quorum outputs before the splitting. The formal definition is included in [7]. S }1zk , we With the new families of failure detectors {z }1zk , and {z compare their strengths with {z ×  }1zk . Based on a siginificant amount of proof work, we summarize their relationship with a nice lattice structure shown in Figure 3. Several important results are summarized by the lattice. First, as S further weakens k for all k > 1. we expected k weakens k ×  ,6 and k Second, even failure detectors in 2 with just two components is not strong S enough to be transformed into k ×  , and even failure detectors in 2 with only one dynamic split is not strong enough to be transformed into k . This shows that partitioning and dynamic splitting are indeed efficient techniques that weaken failure detectors. Third, for all z  2, none of the classes z ×  , S S can be transformed into z-1 ×  , z-1 , or z z , and z -1 . In fact, using a S result in [17] we further show that z ×  , z , and z are not strong enough to solve (z - 1)-set agreement. In [7], we further show that the lattice structure
6

Actually, k weakens  in all cases, and weakens k in most cases.

Weakening Failure Detectors for k-Set Agreement

137

in Figure 3 still holds (under certain mild assumptions) even if we assume that a majority of processes are correct in the system model. Finally, we design a new algorithm in the message-passing model that solves S . The algorithm is based on the Paxos algorithm struck -set agreement using k ture [15], but has significant new additions with much more complicated proofs to deal with the subtleties introduced by dynamic splittings of partitioned failure detectors.

7

Concluding Remarks

In [5] we further demonstrate the partition approach by defining a new failure detector  , which is the result of applying the approach directly to  . We show that  is enough to solve n-set agreement but is strictly weaker than  .  is stronger than  n-1 but is incomparable with  k for k  n - 2. We have shown that the partition approach is effective in weakening a number of failure detectors for k -set agreement. However, the partition approach proposed is still an informal method, and sometimes it requires ad-hoc adjustments. One future direction is to see how the approach and the partitioned failure detectors can be formally treated. In particular, it would be interesting to see if one could formally define a general class of partitioned failure detectors and define the weakest failure detectors among all partitioned failure detectors for k -set agreement. The discovery of failure detectors even weaker than  may suggest that the conjecture made in [10] that n-set agreement is the minimum decision task in terms of minimum information required might not be true. This is another research direction to see if there is any other decision task strictly weaker than n-set agreement in terms of failure information needed to solve the problem.

References
1. Borowsky, E., Gafni, E.: Generalized FLP impossibility result for t-resilient asynchronous computations. In: Proceedings of the 25th ACM Symposium on Theory of Computing, pp. 91­100. ACM Press, New York (1993) 2. Chandra, T.D., Hadzilacos, V., Toueg, S.: The weakest failure detector for solving consensus. Journal of the ACM 43(4), 685­722 (1996) 3. Chandra, T.D., Toueg, S.: Unreliable failure detectors for reliable distributed systems. Journal of the ACM 43(2), 225­267 (1996) 4. Chaudhuri, S.: More choices allow more faults: Set consensus problems in totally asynchronous systems. Information and Computation 105(1), 132­158 (1993) 5. Chen, W., Chen, Y., Zhang, J.: On failure detectors weaker than ever. Technical Report TR-2007-50, Microsoft Research (May 2007) 6. Chen, W., Zhang, J., Chen, Y., Liu, X.: Failure detectors and extended Paxos for k-set agreement. Technical Report TR-2007-48, Microsoft Research (May 2007) 7. Chen, W., Zhang, J., Chen, Y., Liu, X.: Partition approach to failure detectors for k-set agreement. Technical Report TR-2007-49, Microsoft Research (May 2007)

138

W. Chen et al.

8. Delporte-Gallet, C., Fauconnier, H., Guerraoui, R., Hadzilacos, V., Kouznetsov, P., Toueg, S.: The weakest failure detectors to solve certain fundamental problems in distributed computing. In: Proceedings of the 23rd ACM Symposium on Principles of Distributed Computing, pp. 338­346. ACM Press, New York (2004) 9. Fischer, M.J., Lynch, N.A., Paterson, M.S.: Impossibility of distributed consensus with one faulty process. Journal of the ACM 32(2), 374­382 (1985) 10. Guerraoui, R., Herlihy, M., Kouznetsov, P., Lynch, N., Newport, C.: On the weakest failure detector ever. In: Proceedings of the 26th ACM Symposium on Principles of Distributed Computing. ACM Press, New York (2007) 11. Herlihy, M., Penso, L.D.: Tight bounds for k-set agreement with limited scope accuracy failure detectors. Distributed Computing 18(2), 157­166 (2005) 12. Herlihy, M., Shavit, N.: The topological structure of asynchronous computability. Journal of the ACM 46(6), 858­923 (1999) 13. Herlihy, M.P., Wing, J.M.: Linearizability: A correctness condition for concurrent objects. ACM Trans. Prog. Lang. Syst. 12(3), 463­492 (1990) 14. Jayanti, P.: Robust wait-free hierarchies. J. ACM 44(4), 592­614 (1997) 15. Lamport, L.: The part-time parliament. ACM Transactions on Computer Systems 16(2), 133­169 (1998) 16. Mostefaoui, A., Rajsbaum, S., Raynal, M.: The combined power of conditions and failure detectors to solve asynchronous set agreement. In: Proceedings of the 24th ACM Symposium on Principles of Distributed Computing, pp. 179­188. ACM Press, New York (2005) 17. Mostefaoui, A., Rajsbaum, S., Raynal, M., Travers, C.: Irreducibility and additivity of set agreement-oriented failure detector classes. In: Proceedings of the 25th ACM Symposium on Principles of Distributed Computing, pp. 153­162. ACM Press, New York (2006) 18. Mostefaoui, A., Raynal, M.: k-set agreement with limited accuracy failure detectors. In: Proceedings of the 19th ACM Symposium on Principles of Distributed Computing, pp. 143­152. ACM Press, New York (2000) 19. Raynal, M., Travers, C.: In search of the holy grail: Looking for the weakest failure detector for wait-free set agreement (Invited talk). In: Shvartsman, A.A. (ed.) OPODIS 2006. LNCS, vol. 4305, pp. 1­17. Springer, Heidelberg (2006) 20. Saks, M., Zaharoglou, F.: Wait-free k-set agreement is impossible: The topology of public knowledge. SIAM Journal on Computing 29(5), 1449­1483 (2000) 21. Yang, J., Neiger, G., Gafni, E.: Structured derivations of consensus algorithms for failure detectors. In: Proceedings of the 17th ACM Symposium on Principles of Distributed Computing, pp. 297­306. ACM Press, New York (1998)

Amnesic Distributed Storage
Gregory Chockler1 , Rachid Guerraoui2 , , and Idit Keidar3
IBM Haifa Research Lab, Haifa, Israel chockler@il.ibm.com School of Computer and Communication Sciences, EPFL, CH-1015, Lausanne, Switzerland rachid.guerraoui@epfl.ch 3 Department of Electrical Engineering, The Technion ­ Israel Institute of Technology
1

2

Abstract. Distributed storage algorithms implement the abstraction of a shared register over distributed base objects. We study a specific class of storage algorithms, which we call amnesic: these have the pragmatic property that old values written in the implemented register might be eventually forgotten, i.e., they are not permanently kept in the storage and might be overwritten in the base objects by more recent values. This paper precisely captures this property and argues that most storage algorithms are amnesic. We establish a fundamental impossibility of an amnesic storage algorithm to implement a robust register abstraction over a set of base objects of which at least one can fail arbitrarily, even if only in a responsive manner, unless readers are allowed to write to the base objects. Our impossibility helps justify the assumptions made by practical robust storage algorithms. We also derive from this impossibility the first sharp distinction between safe and regular registers. Namely, we show that, if readers do not write, then no amnesic algorithm can implement a regular register using safe registers.

1

Introduction

Storage is a critical aspect of modern computing systems. Today, there is strong interest in distributed storage architectures, either server-based or in the form of storage area networks (SANs), which leverage the technological advances in networks of attached commodity disks to provide increased storage space, availability, and disaster recovery. At the heart of a distributed storage architecture lies an algorithm that implements the read and write operations of a register abstraction over several underlying base objects, sometimes called servers. Such distributed storage algorithms constitute an active area of research. A major challenge addressed by these algorithms is to ensure that (high-level) read and write implementations tolerate asynchrony, contention, and failures. We study in this paper the fundamental limitations of a specific class of storage algorithms, which we define precisely and call amnesic. As we explain
Part of this work was conducted when the author was on sabbatical at MIT CSAIL.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 139­151, 2007. c Springer-Verlag Berlin Heidelberg 2007

140

G. Chockler, R. Guerraoui, and I. Keidar

later (Section 5), most previously suggested storage algorithms are amnesic, e.g., [15,6,13,17,2,16,8,11,12,3], although the notion has never been specifically highlighted. Roughly speaking, an amnesic storage algorithm is one that eventually forgets old values previously stored in the implemented register after some sequence of new values is written. For instance, an algorithm that stores in base objects the last k values written in the implemented register, for some k  0, e.g., [6,13,2,3], is amnesic, because a sequence of k new writes erases all previously stored values. On the other hand, an algorithm that stores the entire history of values written in the base objects, (where values are drawn from an unbounded domain), e.g., [10], is not amnesic. In this sense, amnesia can be seen as a restriction on an algorithm's space consumption, although it is not explicitly formulated this way. Instead, we capture the notion of amnesia in an abstract way in terms of reachable configurations of a distributed storage algorithm (Section 3). Our motivation for refraining from an explicit space restriction is twofold. First, we are interested in algorithms that manipulate potentially unbounded value domains, such as integers, or files. Although in every execution, such an algorithm's space consumption is finite (and depends on the sizes of the written values), it is inherently unbounded. Second, many practical algorithms employ monotonically increasing timestamps [17,14,18,2,10,3,12], which are considered pretty cheap in practice. Thus, the classical concept of bounded memory is inadequate for reasoning about many interesting algorithms. We further note that the concept of bounded memory, by itself, does not capture "reasonable" space restrictions. For example, it does not preclude an algorithm that manipulates a large but finite domain (e.g., files of size 1KB), from storing all the (28192 ) values in its domain if they were all written at some point. Focusing on amnesic algorithms with unbounded domains provides an abstract way to rule out such algorithms, without precluding the use of increasing timestamps. We establish in this paper (Section 4) a fundamental limitation on amnesic storage algorithms. We prove that it is impossible for an amnesic storage algorithm to robustly implement a register abstraction using a set of distributed (failure-prone) base objects, when readers do not write to the base objects. Underlying our impossibility lies the notion of robustness. In short, we consider as robust an algorithm that implements a live regular register [15] in the presence of contention, asynchrony, and arbitrary (Byzantine) failures of base objects [13,5,4]. Our impossibility holds if at least one base object can fail in a responsive yet arbitrary manner (R-Arbitrary failure [13]) among an arbitrarily large set of atomic base objects. (A fortiori, the result also holds if a base object may fail in a non-responsive arbitrary (NR-arbitrary) manner). Such arbitrary failures capture software bugs or malicious intrusions, which cannot be ruled out when the service is geographically disperse. We do not require that the algorithm tolerate process failures, which also strengthens our impossibility result. The assumption that readers do not write is important for large systems with many readers. Whereas it is reasonable for storage servers to communicate using an authenticated channel with a single trusted writer and to assume it

Amnesic Distributed Storage

141

not to be malicious1 , a storage that is accessible to a large population of readers cannot typically trust all of them (authenticating all readers to prevent storage corruption might be infeasible). Hence, a more feasible alternative is to disallow the readers to modify the base object states, which is the assumption under which our impossibility is proven. Given the vast amount of work on practical robust storage, our impossibility result may come as a surprise. In fact, our result does not imply that practical robust storage is unattainable, but rather justifies why previous solutions have had to employ authentication [8,17,19], store unbounded histories [10], have servers actively push updates to clients [18], give up on liveness in some situations [2], allow readers to write [3,11], or implement safe registers instead of regular ones [13,17,2,12] (see Section 5). Our impossibility indeed holds if the implemented register needs to be regular but not if it needs to be safe. In a sense, regularity conveys an important aspect of robustness in the face of concurrency: no value can be returned if it was not written. Since many amnesic robust storage solutions implement safe registers [13,17], we can use our impossibility to derive the first sharp line between safe and regular semantics. We prove that there is no amnesic implementation of a regular register with an unbounded domain from safe ones, if readers do not write (Section 6). The line we draw between safety and regularity is analogous to the celebrated sharp line drawn by Lamport between atomicity and regularity [15], which states that no bounded memory algorithm can implement an atomic register using regular ones, if the readers do not write [15]. No such separation between safety and regularity has ever been established. We identify such a separation by replacing the notion of bounded algorithm in Lamport's formulation with our alternative notion of amnesic algorithm. To summarize, this paper makes the following contributions: ­ We define the notion of amnesia, capturing a pragmatic property of many storage algorithms. ­ We prove the impossibility of devising a storage algorithm that is robust and amnesic without allowing readers to write. ­ We derive the first sharp distinction between safe and regular register semantics.

2
2.1

Model
Shared Memory Model

We consider an asynchronous shared memory system consisting of a finite collection of processes interacting through n base objects O1 , . . . , On . The term base objects is used to distinguish these from the higher level object abstraction implemented by the shared memory algorithm.
1

With a compromised writer, the stored information is rendered meaningless anyway, regardless of any distributed storage algorithm's actions [17,19].

142

G. Chockler, R. Guerraoui, and I. Keidar

We consider storage algorithms that tolerate at least one arbitrary failure of a base objects. We focus on a weak form of such failures, called responsive arbitrary (R-Arbitrary) [13]. This means that the base objects always respond to an invocation, but may respond with an arbitrary value. This assumption strengthens our impossibility result, which directly applies to the more severe non-responsive arbitrary (NR-Arbitrary) [13] failures. Processes are sequential in their ways of invoking high-level operations. That is, after invoking a high-level operation, and until it obtains a response, a process does not invoke a new high-level operation. After invoking a high-level operation, a process might invoke a sequence of low-level operations. We do not assume these low-level invocations to be sequential. That is, a process might invoke several operations on low-level base objects concurrently. The solution must be live in the sense that every high-level operation must eventually complete. We do not require the algorithm to terminate in the presence of process failures, i.e., it does not have to be wait-free. 2.2 Registers

We study more specifically storage algorithms that deterministically implement the abstraction of a register, which is accessed using Read() and Write() operations. If the base objects are also registers, we denote their (low-level) operations as read() and write(), to avoid confusion with the high-level operations. To strengthen our impossibility result, we restrict our attention to storage algorithms that emulate a single-writer single-reader (SWSR) register: that is, the emulated register is only writable by a single process (the writer), and is read by a single process (the reader). The result a fortiori holds for multi-writer multi-reader registers. We assume an infinite value domain V from which the parameters of the Write() operation can be arbitrarily chosen by the writer. When it completes, Write() simply returns an Ok indication. A Read() operation does not have any input parameter, and returns a value from V upon completion. The sequential specification of a register stipulates that a read should return the last value written. When read and write operations may overlap, several semantics have been defined [15]: A register is called safe if every read operation that does not overlap any write operation returns the register's value when the read was invoked, i.e., the latest written value or the initial value of the register if no value was written. A register is regular if it is safe and every read operation that overlaps some write operations returns either one of the values written by overlapping writes or the register's value before the first overlapping write is invoked. A register is atomic if it is regular and if, for any two write operations W and W with respective input values v and v such that W is invoked after W returns, and any two read operations R and R such that R is invoked after R returns, if R returns v , then R does not return v . To strengthen our impossibility result, we will allow base objects to be atomic. 2.3 Configurations

In this paper, we are only interested in the states of base objects, and not of processes. Therefore, by slight abuse of terminology, we define a configuration

Amnesic Distributed Storage

143

to be a set of states of all the correct base objects (and not the processes). Basically, the system starts from an initial configuration and each atomic step of the algorithm, e.g., a low-level write() on a base object, leads the system to a new configuration. The execution of a high-level operation involves several atomic steps that lead the system from a configuration C to another configuration C . The assumption that the reader does not write means that C = C in case no Write() is invoked. We say that a configuration C is write-reachable from a configuration C if there is a sequence S of Write() operations that, when executed without overlapping with any other high-level operation, leads the system to C . If all input parameters of the Write() operations of sequence S are from a set V  V , we say that C is write-reachable from C using V .

3

Amnesic Storage Algorithms

We introduce in this section the notion of an amnesic storage algorithm . We characterize this notion in terms of configurations and write-reachability. Intuitively, a storage algorithm is amnesic if all but a finite number of configurations reached by the algorithm can be eventually erased if a sufficient number of different values are written after them. In short, erasing a configuration C , itself obtained from some configuration C , means reaching a new configuration C (after writing a sufficient number of different values) that makes it impossible to tell whether C was indeed reached. That is, C could be reached directly from C without going through C . The sequence that reaches C from C is in a sense an eraser sequence. An observer of C does not know whether C occurred or not. To preclude the trivial case where erasing a configuration is always performed by the very same sequence, i.e., some sort or reset sequence, we require that the configuration C could be obtained from C by using values from any infinite subset of values. Notice that we do not require that any sufficiently long sequence erases every configuration. Yet, our definition is rather weak because we simply require that every configuration has an eraser sequence using any infinite subset of values: such a weak definition strengthens our impossibility result. Formally, Definition 1 (Amnesic Storage). A storage algorithm A is amnesic if in every execution of A in which infinitely many different values are written, there is a point t, so that for every configuration C reached from point t onward, every configuration C 1 write-reachable from C , and every infinite subset of values V  V , there is a configuration C 2 that is write-reachable from both C and C 1 using V . We say that (see Figure 1) t is the amnesia point, C 2 is an eraser configuration of C 1 from C using V ; the sequence of Writes() used to reach C 2 from C 1 is an eraser sequence of C 1 from C using V ; the sequence used to reach C 2 from C is a bypass sequence of C 1 from C using V . Clearly, an algorithm that recalls the entire history of written values in the base registers is not amnesic. On the other hand, an algorithm that stores in all

144

G. Chockler, R. Guerraoui, and I. Keidar
erased configuration eraser configuration C1 C2 eraser sequence (only values from V')

amnesia point

C

bypass sequence

(only values from V')

Fig. 1. Amnesia

base registers the last k values written in the high-level register is amnesic. The eraser sequence S 2 simply needs to be of size k and from V . The amnesia point captures a situation where an algorithm initially stores some finite number of values forever, but eventually, (at the amnesia point), its storage is "exhausted" and it cannot store additional values forever. To be non-amnesic, an algorithm with an infinite domain needs to be able to recall (in the sense that they cannot be erased) infinitely many configurations that it visited. It is important to notice here that storage algorithms can also record timestamps while being amnesic. Consider for instance an algorithm that stores in the base registers the last k values written, as well as the total number of values written. Consider a configuration C 1 obtained after writing i values, starting from an initial configuration C . An eraser sequence using some infinite subset V consists of k new different Write() invocations with parameters from V , and a bypass sequence consists of i + k different Write() invocations with parameters from V , the latter k being the same as for the eraser sequence. Note that violating amnesia does not directly translate to excessive storage requirements. An algorithm may be able to represent some property of an unbounded history in a bounded way, much like a finite-state automaton can recall that an unbounded string belongs to some regular language2 . Nevertheless, we are unaware of any previous storage algorithm that employs such a succinct representation, and so our impossibility result has broad applicability.

4

Impossibility of Amnesic Robust Storage

In this section, we establish the impossibility of devising a storage algorithm that is at the same time amnesic and robust when readers do not write. Recall that in our context, a storage algorithm is robust if (a) at least one base object can suffer an (R-arbitrary) failure; (b) the implemented register is regular, i.e., tolerates contention; (c) every invoked high-level operation terminates, including in the presence of contention. 4.1 Simplifications

As we assume that only the writer of the implemented register can modify the base objects, these are also, without loss of generality, single-writer registers, with the same writer as that of the implemented register. Still without loss of generality, since we preclude the reader from writing, we can assume that:
2

We are grateful to Prasad Jayanti for pointing this out.

Amnesic Distributed Storage

145

1. Every high level Read() invocation translates into a finite series of concurrent read invocations of all base objects O1 , O2 , .., On . 2. The set of configurations obtained after performing any sequence S of Write() operations without any overlapping Read() is the same as if this sequence was invoked concurrently with this Read(). Recall that our definition of configurations only includes base object states. 4.2 Overview of the Impossibility Proof

To prove our impossibility, we proceed by contradiction. More specifically, we assume that there is an amnesic robust storage algorithm A that implements a register over an infinite domain V and we exhibit a scenario where A violates the regularity of the register. We show that A violates regularity by having the reader return a value that was never written to the implemented register. Not surprisingly, this value is obtained from a low-lever read of the faulty base register. Our scenario has the reader unable to distinguish the response of the faulty base register from the response of a correct one, precisely because A is amnesic, and readers do not write. The proof then goes through three steps: ­ Step 1. (Using the amnesia assumption.) We construct an execution E , which we call an amnesic execution, where sequences of Write() operations erase each other in turn, using n different subsets of value domains, V1 , . . . , Vn . We will argue that every amnesic storage algorithm can generate such an execution. ­ Step 2. (Using the assumption that readers do not write.) We next construct a slight modification E of E , where the reader samples one base register after each sequence above has erased the previous configuration. No matter how many samples are taken, the reader still cannot obtain evidence of any written value from more than one base register, because the evidence is continuously erased. Finally, the reader returns some value vi from some subset Vi , for which it saw evidence in some base register Oi . ­ Step 3. (Using robustness.) Finally, we construct an execution Ei in which no value from Vi is written, bypassing the configurations where values from Vi are stored. In Ei , Oi incurs an R-arbitrary failure, and returns the same values as in E . Since the reader cannot distinguish Ei from E , it returns vi , which was never written. 4.3 Impossibility

Theorem 1. No storage algorithm can be amnesic and robust if a single base register can suffer a responsive arbitrary failure and readers do not write. Proof (Proof of Theorem 1.). We assume a storage algorithm A that deterministically implements the abstraction of a register with an infinite value domain V , using a collection of n base objects O1 , . . . , On . We assume by contradiction that A is robust and amnesic.

146

G. Chockler, R. Guerraoui, and I. Keidar

We partition V into n + 1 infinite subsets V0 , V1 , . . . , Vn . (The intersection of every two subsets is empty, and the union of all subsets is V ). Step 1. We construct an infinite execution of A, E , which we call an amnesic execution (see Figure 2). E goes through the infinite sequence of configurations: C1,1 , . . . , C1,n , C2,1 , . . . , C2,n , . . . , Cj,1 , . . . , Cj,n , . . . such that for every i  {1, .., n}, there is an execution Ei such that (a) Ei is the subsequence of E obtained by omitting all the configurations Ck,i , k > 0. (That is, configurations Ck,i for all k are skipped.) And (b) no value from Vi is ever written in Ei .
C E V0 V1 E1 bypass using V2 E2 bypass using V3 C1,1 V2 C1,2 V3 C1,3 V4 Vn C1,n V1 E1 bypass using V2 E2 bypass using V3 C2,1 V2 C2,2 V3

...

...

En bypass using V1

Fig. 2. Amnesic execution E; execution E2 (bypassing V2 ) highlighted

We construct E recursively as follows. We perform a series of Writes() of different values from V0 until the algorithm reaches an amnesia point at some configuration C . Then we apply a Write() of a single value from V1 . We denote the resulting configuration C1,1 . Then we use the assumption that the storage algorithm is amnesic and apply to C1,1 a sequence that erases C1,1 from C using V2 . We denote the resulting configuration by C1,2 . Then we use again the assumption that the storage algorithm is amnesic and apply to C1,2 a sequence that erases C1,2 from C1,1 using V3 . We denote the resulting configuration by C1,3 . And so forth recursively. We apply to Cj,k a sequence that erases Cj,k from Cj,k-1 using Vk+1 , where Cj,n+1 = Cj +1,1 and Vn+1 = V1 . The resulting execution E is infinite and can be generated from every amnesic storage algorithm. In addition, for every 1  i  n, execution Ei is constructed, also recursively, as follows (see Figure 2). Up to C1,i-1 , Ei is exactly like E and hence no Write() uses any value from Vi . Configuration C1,i+1 is then (directly) reached from C1,i-1 via the bypassing sequence of C1,i from C1,i-1 using values from Vi+1 . Then we continue as in E until C2,i-1 , at which point we execute the bypass sequence of C2,i from C2,i-1 . And so forth: we apply the same sequence as in E to reach Cj,k from Cj,k-1 for k = i, and for i, we apply the bypass sequence of Cj,i from Cj,i-1 to reach Cj,i+1 from Cj,i-1 . It is easy to see that properties (a) and (b) above hold for every Ei , for i  {1, .., n}. Step 2. We now construct an execution E interleaving a single Read() with the sequences of Write() operations involved in E . Remember that, without loss of

Amnesic Distributed Storage

147

generality, we assume that every Read() implementation consists of a sequence of concurrent invocations of all base objects. The interleaving in execution E is constructed as follows. Read() is invoked when the base objects are in configuration C1,1 . The reader returns from the k th read() of base object Oj when the system is in configuration Ck,j . For instance, the reader returns from the first read() of the first base object, O1 , when the system is in configuration C1,1 , then from the first read() of the second base object, O2 , when the system is in configuration C1,2 . By the assumption that the reader does not write, execution E can also be generated by every amnesic storage algorithm. By our liveness assumption, the Read() eventually returns a value. Since the Read() is invoked after the first write from V1 , by regularity, it returns a value vi from some Vi for 0 < i  n. Step 3. We now make use of our assumption of robustness to derive a contradiction. We construct execution Ei , which is the same as Ei , with two exceptions: 1. For every j = i, as in E , we apply the k th read() of base object Oj when the system is in configuration Ck,j . 2. The k th read() of base object Oi occurs during the k th bypass. 3. Oi returns the same response to its k th read invocation in Ei as in in execution E . Execution Ei can also be generated by every amnesic storage algorithm with base object Oi failing in an arbitrary way. Executions E and Ei look the same to the reader (by construction), which then returns a value vi from Vi in Ei . But no value from Vi is written in Ei , contradicting regularity.

5

Amnesic Algorithms and Circumventing the Impossibility

Our impossibility justifies certain assumptions and design decisions made by existing storage algorithms. In this section, we illustrate the importance of the notion of amnesia, by showing that the majority of reliable storage algorithms in the literature are amnesic, and discuss how existing algorithms circumvent our impossibility result. First note that every bounded memory algorithm is by definition trivially amnesic, because no infinite sub-domains of its domain exist. Since our impossibility result only applies to algorithms that can store values from unbounded domains, it is more interesting to consider algorithms that can manipulate such domains. Interestingly, most bounded memory algorithms in the literature can be easily extended to support unbounded domains. For example, Jayanti et al. [13] present an emulation of a safe register from ones that can suffer NR-Arbitrary faults. Although originally described as a bounded memory algorithm, it does not make any use of the domain size, and only stores values from the domain. Hence, this algorithm can easily work with an unbounded domain, where in each execution, it consumes storage as required for representing the values written in that execution. This algorithm circumvents our impossibility result by implementing only safe storage.

148

G. Chockler, R. Guerraoui, and I. Keidar

Lamport [15] presents a bounded memory algorithm for implementing a waitfree regular register from safe bits, with readers that do not write. Given the existence of robust safe register emulations [13,17,2], had this algorithm manipulated unbounded domains, it would have contradicted our impossibility. However, this algorithm is aware of its value domain and makes heavy use of this knowledge­ it stores one bit for each value in the value domain of the register. The algorithm works as follows: a write operation of the ith value in the domain changes the ith bit to 1, and subsequently writes 0 in bits i - 1, i - 2, . . . , 1. The reader reads the bits 1, 2, 3, . . . until it encounters a 1 in some bit i, at which point it stops reading and returns i. We observe that neither the reader nor the writer ever accesses a bit higher than the one pertaining to the largest written value. Therefore, this algorithm too can be extended to unbounded domains, e.g., integers, by allocating the ith bit the first time a value greater or equal to i is written. Despite its exponential storage requirements, the resulting algorithm is also amnesic, since writing a single value larger than all previously written ones is an eraser sequence. So how does this algorithm circumvent our impossibility result? We observe that the extended algorithm no longer ensures liveness, because if the writer writes an infinite monotonically increasing sequence of values (an amnesic sequence), the reader can "trail" the writer, and never encounter a 1 in any register. Thus, even such exponential storage does not save us from the impossibility. Many algorithms that store unbounded timestamps are also amnesic, including the classical ABD [6] algorithm, which tolerates only crash failures of base objects, and the safe register emulations of [17,2,12]. Other amnesic algorithms provide atomic semantics (which are stronger than regular) either by assuming a stronger model where data is self-verifying (and hence cannot be forged by base objects), or by having readers write [17,16,8,11,3]. It is also possible to circumvent our impossibility with amnesic algorithms by providing weaker (non-terminating in case of contention) termination guarantees [2]. Specifically, Abraham et al. [2] propose a termination condition called finite writes (FW), which guarantees progress only in executions with a finite number of writes, and present a amnesic storage algorithm implementing a regular FW-terminating register. A few algorithms circumvent our impossibility by forgoing amnesia. These include the Pasis system [10], which achieves atomic semantics. It circumvents our impossibility by having authenticated readers that are allowed to modify the data stored at the base registers (storage nodes). The storage nodes also keep all versions of data that have been written in the execution, and are therefore, not amnesic. To prevent storage exhaustion, the system implements a garbage collection mechanism, which works well in practice, but, as the authors point out in [9], might fail to terminate in some scenarios. Martin et al. [18], as well as Bazzi and Ding [7] also provide atomic semantics. They assume storage servers (instead of base registers) that communicate with each other, and a subscription model whereby storage servers push writer updates to subscribed clients. Since theoretically, there is no bound on the

Amnesic Distributed Storage

149

number of messages in the reliable push channel in an asynchronous system, these algorithms are also not amnesic. This approach nevertheless, is a viable design alternative in the settings where servers are available and the number of clients is limited.

6

Sharp Separation Between Regularity and Safety

Many amnesic robust storage solutions implement safe registers [13,17,2]. This maybe surprising, as safe and regular semantics are commonly believed to be "equivalent", justified by the existence of known bounded memory reductions from regular registers to safe ones. In particular, Lamport [15] presents a bounded memory algorithm for emulating a regular register from safe ones, in which readers do not write. The algorithm assumes a bounded value domain and its storage requirements, as well as the number of memory accesses in the algorithm, are notoriously high (proportional to the number of possible values the regular register can hold). The following theorem is an immediate corollary of Theorem 1 and the existence of amnesic storage algorithms that implement a t-tolerant wait-free safe register (with unbounded value domains) from a collection of n base registers up to t of which can suffer arbitrary failures [13,17,2]. Theorem 2. If readers do not write, it is impossible to implement a live regular register from safe ones with an amnesic algorithm and an infinite domain. Interestingly, Lamport has proved the following [15] : If readers do not write, it is impossible to implement an atomic register from regular ones with a bounded algorithm. Thus, Lamport has shown that in bounded memory implementations, disallowing readers to write draws a sharp line between regularity and atomicity, but not between safety and regularity. Hence, our result shows that, when one considers amnesic with infinite value domain instead of bounded memory, the same sharp line does exist between regularity and safety.

7

Concluding Remarks

The observation that no existing storage algorithm with reasonable space requirements that is regular, live in the presence of contention, and does not require readers to write, or preclude arbitrary faults of base registers was made by Abraham et al. [1]. They conjectured that, roughly speaking, if readers do not write, then the storage size grows linearly with the number of values written over the execution's time span. The difficulty in proving this conjecture stems from the lack of appropriate definitions, since the classical notion of bounded memory cannot capture "linear growth" in storage requirements. Our notion of amnesic memory is

150

G. Chockler, R. Guerraoui, and I. Keidar

an attempt to capture practical limitations on the information an algorithm recalls about its history, and gives an explanation to the observation that led to this conjecture. An interesting direction for future work may be providing a concrete lower bound on the space requirements of robust storage algorithms that are not amnesic. In addition, we believe that more impossibilities and fine grained distinctions could be obtained if one reconsiders bounded memory restrictions with our amnesic notion in mind.

Acknowledgments
We thank Ittai Abraham, Lorenzo Alvisi, Faith Ellen, Eli Gafni, Prasad Jayanti, Nancy Lynch, Dahlia Malkhi, Jean-Philippe Martin, Michel Raynal, and Marko Vukoli´ c for many fruitful discussions on robust storage algorithms.

References
1. Abraham, I., Chockler, G., Keidar, I., Malkhi, D.: Private communication (2002) 2. Abraham, I., Chockler, G., Keidar, I., Malkhi, D.: Byzantine Disk Paxos: Optimal resilience with byzantine shared memory. Distributed Computing 18(5), 387­408 (2004) 3. Abraham, I., Chockler, G., Keidar, I., Malkhi, D.: Wait-free regular storage from byzantine components. Information Processing Letters (IPL) 101(2), 60­65 (2007) 4. Afek, Y., Greenberg, D.S., Merritt, M., Taubenfeld, G.: Computing with faulty shared objects. Journal of the ACM 42(6), 1231­1274 (1995) 5. Afek, Y., Merritt, M., Taubenfeld, G.: Benign failures models for shared memory. In: Schiper, A. (ed.) Distributed Algorithms. LNCS, vol. 725, pp. 69­83. Springer, Heidelberg (1993) 6. Attiya, H., Bar-Noy, A., Dolev, D.: Sharing memory robustly in message-passing systems. Journal of the ACM 42(1), 124­142 (1995) 7. Bazzi, R.A., Ding, Y.: Non-skipping Timestamps for Byzantine Data Storage Systems. In: Guerraoui, R. (ed.) DISC 2004. LNCS, vol. 3274, pp. 405­419. Springer, Heidelberg (2004) 8. Cachin, C., Tessaro, S.: Optimal resilience for erasure-coded Byzantine distributed storage. In: DSN 2006. Intl. Conference on Dependable Systems and Networks, pp. 115­124 (June 2006) 9. Goodson, G., Wylie, J., Ganger, G., Reiter, M.: Efficient byzantine-tolerant erasurecoded storage. Technical Report CMU-PDL-03-104, Parallel Data Laboratory, CMU (December 2003) 10. Goodson, G., Wylie, J., Ganger, G., Reiter, M.: Efficient byzantine-tolerant erasurecoded storage. In: DSN-2004. The International Conference on Dependable Systems and Networks (June 2004) 11. Guerraoui, R., Levy, R.R., Vukolic, M.: Lucky read/write access to robust atomic storage. In: DSN, pp. 125­136. IEEE Computer Society Press, Los Alamitos (2006) 12. Guerraoui, R., Vukoli´ c, M.: How Fast Can a Very Robust Read Be? In: PODC'06. 25th ACM Symposium on Principles of Distributed Computing. ACM Press, New York (2006) 13. Jayanti, P., Chandra, T., Toueg, S.: Fault-tolerant wait-free shared objects. Journal of the ACM 45(3), 451­500 (1998)

Amnesic Distributed Storage

151

14. Lakshmanan, S., Ahamad, M., Venkateswaran, H.: Responsive security for stored data. IEEE Trans. on Parallel and Distributed Systems 14(19), 818­828 (2003) 15. Lamport, L.: On interprocess communication ­ Part II: Algorithms. Distributed Computing 1(2), 86­101 (1986) 16. Liskov, B., Rodrigues, R.: Byzantine clients rendered harmless. In: Fraigniaud, P. (ed.) DISC 2005. LNCS, vol. 3724. Springer, Heidelberg (2005) 17. Malkhi, D., Reiter, M.: Byzantine quorum systems. Distributed Computing 11(4), 203­213 (1998) 18. Martin, J.-P., Alvisi, L., Dahlin, M.: Minimal byzantine storage. In: Proceedings of the 16th International Symposium on Distributed Computing (DISC) (October 2002) 19. Rodrigues, R., Liskov, B.: Rosebud: A Scalable Byzantine-Fault-Tolerant Storage Architecture. Technical Report MIT-LCS-TR-932, MIT Laboratory for Computer Science (2004)

Distributed Approximations for Packing in Unit-Disk Graphs
Andrzej Czygrinow1 and Michal Ha´ n´ ckowiak2
Department of Mathematics and Statistics Arizona State University Tempe, AZ 85287-1804, USA andrzej@math.la.asu.edu Faculty of Mathematics and Computer Science Adam Mickiewicz University, Pozna´ n, Poland mhanckow@amu.edu.pl
1

2

Abstract. We give a distributed approximation algorithm for the vertexpacking problem in unit-disk graphs. Given a graph H , the algorithm finds in a unit-disk graph G a collection of pairwise disjoint copies of H of size which is approximately equal to the packing number of H in G. The algorithm is deterministic and runs in a poly-logarithmic number of rounds in the message passing model.

1

Introduction

Distributed complexity of vast majority of graph-theoretic problems is still far from well understood. In contrast to the sequential or parallel model of computations, the distributed model has resisted efficient solutions for even the most simple graph-theoretic problems. Consider, for example, the maximal independent set problem. A trivial sequential solution for the problem exists, a non-trivial deterministic PRAM algorithm is known (Small-spaces derandomization procedure of Luby [L86]) but the distributed complexity of the maximal independent set problem is still an outstanding open problem. As distributed complexity of many graph-theoretic problems seems very hard to be determined in general graphs, it is natural to consider important sub-classes of graphs. In this paper, we will consider a class of unit-disk graphs and give an efficient distributed approximation algorithm for the vertex-packing problem. The algorithm is purely deterministic and finds a solution of value which is within (1 - o(1)) of the optimal. Many different distributed models can be considered (see Peleg [P00]) each capturing different aspect of distributed computations. Here, we will work in the distributed message-passing model of computations (see for example Linial [L92]) in which the underlying network forms a graph with vertices which correspond to computational units and edges that correspond to communication links between the units. The computations are synchronized and proceed in rounds. In a single round each vertex can send messages to its neighbors, can receive messages from
This work was supported by grant N206 017 32/2452 for years 2007-2010.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 152­164, 2007. c Springer-Verlag Berlin Heidelberg 2007

Distributed Approximations for Packing in Unit-Disk Graphs

153

its neighbors, and can perform local computations. In this model the emphasis on the extent to which local computations can be used to determine a global function of the network. The global function considered in this paper comes from the vertex-packing problem. In this problem, we are given a graph H of a fixed size and want to find a maximum number of vertex-disjoint copies of H in the underlying network G. More formally, let G be graph on |G| vertices and let H be a graph on a fixed (independent of G) number of vertices. An L-packing of H in G is a collection of pairwise vertex-disjoint subgraphs H1 , H2 , . . . , HL of G such that each Hi is isomorphic to H . The packing number of H , H (G), is the largest L such that an L-packing of H in G exists. Finding H (G) is a classical problem in graph theory with many important special cases. In particular, the size of a maximum matching in graph G is equal to K2 (G). 1.1 Model and Notation

As noted before, we will consider the message-passing synchronized distributed model of computations from [L92]. In addition, we assume that vertices in the network have unique identifiers which are in the set {1, . . . , m} where m is a polynomial in |G| and is globally known. The underlying topology of the network will be of a unit-disk graph which is commonly used to model mobile ad-hoc networks. A graph G = (V, E ) is called a unit-disk graph if there is an injective function f : V  R2 such that {u, v }  E if and only if ||f (u) - f (v )||2  1. Although the fact that G is a unit-disk graph is critical for our analysis, we will assume that no information (including the Euclidean distance) about a geometric representation of G is available to nodes. Finally, we will use the graph-theoretic notation and terminology from [D05]. In particular, we will denote by |G| the number of vertices of G and by ||G|| the number of edges. 1.2 Results

We give a distributed approximation algorithm for the packing problem in unitdisk graphs. The algorithm is deterministic, runs in a poly-logarithmic number of rounds, and does not use any information about a geometric representation of the unit-disk graph. Let H be a graph and let G be a unit-disk graph. Given k and H , the algorithm finds in a poly-logarithmic number of rounds a collection of pairwise vertex-disjoint subgraphs H1 , . . . , HL of G with L  (1 - O(1/ logk |G|))H (G) (Theorem 3). The algorithm extends the clustering procedure from [CH06b] by adopting it to a more general class of graphs than the one considered in [CH06b]. Specifically, we will consider the so-called (C, q )-bounded growth graphs (see Definition 1 on page 155) which arise naturally when considering certain auxiliary graphs of a unit-disk graph. In [CH06b] a clustering algorithm for (C, 2)-bounded growth graphs with C constant is given. Here, we found it necessary to generalize the procedure from [CH06b] to (C, q )-bounded growth graphs for which q is constant but C may depend on |G|. The algorithm essentially works as follows. First a maximal independent set is found. The set is used to partition the graph into clusters of two types: the large clusters that contain many vertices and the other clusters. In the large clusters

154

A. Czygrinow and M. Ha´ n´ ckowiak

copies of H are packed greedily into subgraphs induced by N [v ]. The packing problem in the rest of the graph is reduced to the problem of approximating a maximum independent set in a special graph. We argue that error made in both of the above parts is small and prove that there exists a solution to the packing problem of value which approximates the optimal with the property that every copy of H is either entirely contained in large clusters or is contained in the rest of the graph. 1.3 Related Work

Studying distributed message passing approximation algorithms for unit-disk graphs was initiated by Kuhn et. al. in [KMNW05b] where efficient approximations for the Maximum Independent Set (MaxIS) Problem and the Minimum Dominating Set (MDS) Problem are given. Research described in the series of papers [KMW05], [KMNW05b], and [KMNW05a] is the main motivation for our work here. Continuing the program of Kuhn et. al., we recently gave distributed approximations for two additional problems, the Minimum Connected Dominating Set (MCDS) Problem and the Maximum Matching (MM) Problem (see [CH06b]). All of the above results indicate that distributed poly-logarithmic approximations are fairly easy to find if the underlying graph has the unit-disk graph topology. The second motivation for our study comes from the recent work [CH06a] and [CHS06] in which distributed approximations for MaxIS Problem, MM Problem, and MDS Problem in planar and minor-closed families of graphs are given. The vertex-packing problem is another important problem which admits a distributed approximation in unit-disk graphs. Although minor-closed families of graphs provide different challenges than unit-disk graphs, we also hope that some of the techniques developed in this paper will prove useful in attacking the packing problem for such families of graphs. It is possible that the running time of our packing algorithm can be improved but it seems unlikely that the poly-log bound can be beaten as for example in the case of general graphs, it is shown in [KMW04] that required to achieve a constant or a polylogarithmic approximation ratio for an inclusion maximal matching is at least  ( log |G|/ log log |G|) or  (log / log log ), where  denotes the maximum degree of the graph. 1.4 Organization

In the next section we will give our modified clustering algorithm. The algorithm and its analysis follows the pattern of the corresponding discussion in in this [CH06b]. In Section 3, we will present the main result of the paper, a distributed algorithm which yields an almost exact approximation of the vertex-packing problem in unit-disk graphs.

2

Clustering Algorithm

We shall start with the following basic property of unit-disk graphs. Although very rudimentary, the property is a key to our clustering and packing algorithms.

Distributed Approximations for Packing in Unit-Disk Graphs

155

A geometric representation of a unit-disk graph G is an injective function f : V (G)  R2 such that two distinct vertices u, v from V (G) form an edge in G if and only if ||f (u) - f (v )||2  1. Lemma 1. Let G be a unit-disk graph and let k be a positive integer. For any independent set I in G and any geometrical representation f of G, the number of vertices from f (I ) which are contained in a ball in R2 of radius k is at most 4(k + 0.5)2 . 2.1 Clustering Algorithm

Our algorithm for unit-disk graphs uses a small modification of the clustering procedure from [CH06b] which in turn is based on the ruling set algorithm from [AGLP89]. In [CH06b] it is shown that the clustering procedure works in the so-called (C, 2)-bounded growth graphs (see Definition 1) where C is a constant. Here, we will need a modification of this algorithm which works in a more general class of graphs. Recall that the distance between two vertices u, v in a graph G, dG (u, v ), is the length of the shortest path in G joining u with v . Definition 1. A graph G has a (C, q )-bounded growth if for every vertex v in G and every nonnegative integer m, |{u  V (G) : dG (u, v )  m}|  Cmq + 1. In our applications q will always be a fixed constant, C however will depend on the graph G. In fact, to get a desired approximation error we will set C := (logl |G|) for a fixed constant l. Algorithm from [CH06b] uses the ruling forest technique from [AGLP89] and relies on the (C, 2)-bounded growth property. We will show that a modification of the algorithm from [CH06b] yields a generalization which is can be applied to the packing problem. The performance of the modified algorithm is summarized in the next theorem. Theorem 1. Let q be a fixed constant. There is a distributed algorithm which given a (C, q )-bounded growth graph G with identifiers in {1, . . . , m}, |G|  m, and 0 < = (|G|)  1 finds a partition (V1 , . . . , Vs ) of V (G) such that the following two properties are satisfied. ­ The number of edges between different partition classes is O( C |G|) and ­ For i = 1, . . . , s, the diameter of G[Vi ] is O log C +log 1/ . The algorithm runs in O C
log C +log 1/ q+1

logq+1 m log 1/

rounds.

The algorithm from Theorem 1 is the procedure Clustering which we will describe shortly. The algorithm is based on the ruling set procedure of Awerbuch et. al. [AGLP89]. A D-ruling set in a graph G = (V, E ) is a subset S of V that has two properties: ­ For any two distinct vertices s, s from S , the distance (in G) between s and s is at least D.

156

A. Czygrinow and M. Ha´ n´ ckowiak

­ For any vertex v  V there is a vertex s  S such that the distance between s and v is at most D log |G|. There is an easy distributed algorithm which finds a D-ruling set in any graph. Theorem 2 ([AGLP89]). Let G be a graph such that identifiers of vertices from V (G) are in {1, . . . , m} where m is globally known and is a polynomial in |G|. There is a distributed algorithm which finds in G a D-ruling set in O(D log |G|) rounds. Our algorithm uses parameters , D, and F (used in Clustering) and assumes that graph G has the (C, q )-bounded growth. Parameter q is assumed to be constant, C and can depend on |G|. For 0 < < 1 and C , let l be the smallest positive integer with the property
q (1 + )l  Cl + 1.

(1)

It is easy to check that when q is constant and l = O In addition, let D be such that D > 2 l . log C + log 1/

is small then . (2)

(3)

In the next two procedures we will find a clustering of a graph which has a (C, q )-bounded growth. First procedure is essentially one iteration of the main algorithm. We will denote by N (U ) the set of vertices in G which are at distance at most one of a vertex from U . ClusterSet Input: Constant q . Graph G = (V, E ) which has (C, q )-bounded growth and such that identifiers of V are bounded by m. Parameters: 0 < < 1 (arbitrary) and D which satisfies (3). Output: A family of subsets of V . (1) Find a D-ruling set {v1 , v2 , . . . , vl } in G. (2) For every vi in parallel: (a) Let Ui := {vi }, Ni := N (Ui ) \ Ui . (b) while |Ni |  |Ui | · Ui := Ui  Ni , Ni := N (Ui ) \ Ui . (3) Return U1 , U2 , . . . , Ul . We shall start the analysis of CluserSet with the following property of the D-ruling set obtained in step one. Lemma 2. The number of vertices in the D-ruling set obtained in step one of ClusterSet is at least |G|/(CDq logq |G| + 1).

Distributed Approximations for Packing in Unit-Disk Graphs

157

Proof. Let {v1 , v2 , . . . , vl } denote the ruling set obtained in step one. For every vi consider the set Wi of vertices in G which are within distance D log |G| of vi . From Definition 1, |Wi |  CDq logq |G| + 1. Since vi 's form a D-ruling set we l have |G|  | i=1 Wi |. Thus |G|  l(CDq logq |G| + 1). In the next two lemmas we will show that each of Ui 's has small diameter and more importantly the total number of edges that intersect different Ui 's is small. Lemma 3. Let l be such that inequality (1) holds and let {v1 , v2 , . . . , vl } be the ruling set from step one. Then for every i = 1, . . . , l,
uUi ( l)

max dG (vi , u)  l .

Proof. Let Ui denote the set Ui in the lth iteration of the while loop from (0) ( l) step 2(b). Then |Ui | = 1 and in the lth iteration |Ui |  (1 + )l . On the ( l) other hand, by Definition 1, |Ui |  Clq + 1. Consequently, if l is the smallest q positive integer such that (1 + )l  Cl + 1 then l  l and so for any u  Ui , the distance dG (vi , u)  l . Lemma 4. Sets U1 , U2 , . . . , Ul returned by ClusterSet are pair-wise disjoint. In addition, if e(Ui , V \ Ui ) denotes the number of edges between Ui and V \ Ui then
l l

e(Ui , V \ Ui ) = O(C
i=1 i=1

|Ui |).

Proof. From Lemma 3 for every u  Ui , dG (vi , u)  l and so if Ui  Uj is non-empty then the distance between vi and vj is at most 2l which contradicts the fact that vi , vj are in the D-ruling set with D > 2l by (3). To prove the second part, note that for every Ui returned in step 3, the set Ni = N (Ui ) \ Ui is such that |Ni | < |Ui |. Since G has the maximum degree of at most C , the number of edges between Ui and Ni is C |Ui |. Recall that identifiers of V are in set {1, . . . , m}. We finally note that the running time of ClusterSet is O D log m +
log C +log 1/

.
log C +log 1/

Lemma 5. The number of rounds of ClusterSet is O D log m + is followed by l = O
log C +log 1/

.

Proof. There are O(D log m) rounds to find the D-ruling forest in step 1. This iterations in step 2. Our main clustering procedure will call ClusterSet repeatedly. In each call, sets U1 , . . . , Ul are obtained and vertices in Ui are deleted from the graph G. Finally, after trimming G with repeated application of ClusterSet, the remaining vertices will form one-element clusters.

158

A. Czygrinow and M. Ha´ n´ ckowiak

Clustering Input: Constant q . Graph G = (V, E ) which has (C, q )-bounded growth and such that the identifiers of V are positive integers which are less than or equal to m. Parameters: 0 < < 1 (arbitrary), D (must satisfy (3)), F (arbitrary). Output: A partition P of V . (1) Repeat F times: (a) Call ClusterSet in G. Add all sets Ui obtained from ClusterSet to family P . (b) Delete from G vertices from Ui and edges incident to these vertices. (2) For every vertex left in G create a set which contains only this vertex and add it to P . Return P . We will analyze Clustering in the next two lemmas. Lemma 6. Let P = (V1 , . . . , Vt ) be a partition of V returned by Clustering. The number of edges of G = (V, E ) connecting vertices from different Vi 's is O 1- 1 CDq logq |G| + 1
F

+

C |G| .

Proof. First note that since G is a graph with a maximum degree O(C ), ||G|| = O(C |G|). Consider sets added to P in iterations from step 1. Edges with exactly one endpoint in these sets are deleted in step 1(b) and by Lemma 4, the number of them is O(C |G|). The remaining edges which must be counted are the edges of G from step two. To estimate these, we note that by Lemma 2, the number of vertices in this graph is O( O 1-
1 CDq logq |G|+1 F

1-

CDq

1 logq |G|+1

F

|G| . Con-

sequently, the number of edges of G connecting vertices from different Vi 's is + C |G| .
log C +log 1/

Lemma 7. Clustering runs in O F D log m +

rounds.

Proof. There are F iterations of step 1, in which, by Lemma 5, the sets are found in O D log m + log C +log 1/ rounds. Proof of Theorem 1. Let D := 2l + 1 = O log C +log 1/ and let F := q q (log 1/ )CD log m . Then, by Lemma 7, the number of rounds is
O F D log m + log C + log 1/ =O C log 1/ log C + log 1/
q +1

log q+1 m .

The number of edges which connect different clusters is O( C |V |) by Lemma 6 and the diameter of each cluster is O(l ) by Lemma 3 which by (1) is O log C +log 1/ .

Distributed Approximations for Packing in Unit-Disk Graphs

159

3
3.1

Main Algorithm
Properties of Vertex-Packing in Unit-Disk Graphs

Let G = (V, E ) be a unit-disk graph and let H be a fixed connected graph with h = |H |. An L-packing of H in G is a collection {H1 , . . . , HL } of pairwise vertex-disjoint subgraphs of G such that each Hi is isomorphic to H . Let H (G) be the maximum integer L such that there exists an L-packing of H in G. In this section, we will give a distributed algorithm which given a constant k finds in a poly-logarithmic number of rounds a collection of L subgraphs H1 , . . . , HL of G such that each Hi is isomorphic to H and L  (1 - O(1/ logk |G|))H (G). A starting point to the algorithm is a maximal independent set in G which can be found using the procedure from [KMNW05a] which gives an auxiliary graph Aux(G). Properties of this auxiliary graph will be useful when proving some facts about an optimal packing of H in G. Definition 2 (Auxiliary graph). Let I = {v1 , . . . , vz } be a maximal independent set in graph G = (V, E ), H be a graph with h = |H |, and l be a fixed positive number. Let Vi be the set of neighbors of vi such that if w  Vi then vi is the neighbor of w in I with the least identifier, that is Vi = {w  N (vi ) : ID(vi ) = min{ID(a) : a  N (w)  I }}, ¯i = Vi  {vi }. Let and let V ¯i |  2h3 logl |G|}, S = {i : |V ¯i | < 2h3 logl |G|}, B = {i : |V and B=
iB

¯i , S = V
iS

¯i . V

In addition, let Aux(G) be the graph (W , E ) with W = {1, . . . , z } and {i, j }  E ¯i and a vertex whenever i = j and there is an edge in G between a vertex from V ¯j . from V Clearly, we have |B |  2h3 |B| logl |G|

¯i is a and because G is a unit-disk graph, for every i, the subgraph induced by V union of at most five cliques. As a result, for every i  B we can pack H 's greedily in Vi 's as long as the number of vertices is at least 5(h - 1) + 1. Consequently we can pack at least (2h3 logl |G| - 5(h - 1) + 1)/h  h2 logl |G| vertex-disjoint ¯i ]. copies of H in G[V In addition, from Lemma 1, we see that Aux(G) has the (C, 2)-bounded growth with C = 48. Lemma 8. Aux(G) has (48, 2)-bounded growth.

160

A. Czygrinow and M. Ha´ n´ ckowiak

In particular, the maximum degree of Aux(G) is at most 48 and the number of vertices in Aux(G) which are within distance h of B is at most 48h2 + 1. We first observe that it is possible to obtain an almost optimal solution to the packing problem with the property that every copy of H is either disjoint with ¯i with i  B . ¯i with i  B or is entirely contained in some V all V Lemma 9. Let H1 , . . . , HL be an L-packing of a connected graph H in G with L = H (G). There exists an M -packing H1 , . . . , HM of H in G with the following properties. ¯j for some j  B or V (H )  V ¯j =  1. For every i = 1, . . . , M , either V (Hi )  V i for every j  B . 2. M  (1 - O(1/ logl |G|))H (G). Proof. We will call a vertex H -saturated by a packing if it is contained in a copy of H from the packing and we call the vertex H -free otherwise. We start with and optimal L-packing H1 , . . . , HL and modify it as follows. For every i = 1, . . . , L ¯j | < |V (Hi )| then delete Hi from the if there is a j  B , with 0 < |V (Hi )  V packing. In the next step, for every j  B  S if there are at least 5h H -free ¯i ] as possible. This will result ¯i then pack as many copies of H in G[V vertices in V in an M -packing H1 , . . . , HM of H with at most 5h - 1, H -free vertices in every ¯j . Now we count vertices which were H -saturated in the first packing and are V H -free in the second packing. Since we deleted copies of H that had at least ¯j 's which are within one vertex in B , the vertices of such H 's are contained in V ¯ distance h (in Aux(G)) of B . As all these Vj 's will have the property that there ¯j in the second packing, the number of are at most 5h - 1, H -free vertices in V vertices which are H -saturated in the first packing and are H -free in the second is at most (5h - 1)|B|(48h2 + 1) = O(h3 |B|). As a result, M  L - O(h2 |B|). On the other hand, L  h2 |B| logl |G| ¯i . Thus as we can pack at least h2 logl |G| copies of H in each V M  (1 - O(1/ logl |G|))H (G). We will now turn our attention to the set S from Definition 2 and we extract a few useful properties of G[S ]. Obviously as an induced subgraph of G, G[S ] is a unit-disk graph. In addition, the maximum degree, S , of G[S ] satisfies S < 96h3 logl |G|. (4)

However much more is true, in fact G[S ] must have the (48(S + 1), 2)-bounded growth. Let BG (v, r) be the set of vertices in G which are within distance r in G of v . If for some r and v , |BG[S ] (v, r)| > 48(S + 1)r2 + 1 then the graph induced by B (v, r) contains an independent set with more than 48r2 vertices. However all vertices from BG[S ] (v, r) must be contained in Euclidean ball of

Distributed Approximations for Packing in Unit-Disk Graphs

161

radius r around v in any geometrical representation of G[S ] which contradicts Lemma 1. Although the fact that G[S ] has bounded growth is an indication of the direction the algorithm will take, we need to consider a different auxiliary graph. Definition 3. Let G = (V, E ) be a unit-disk graph and let H be a connected graph on h vertices. Let GH be the h-uniform hypergraph with the vertex set V and hyperedge on the set U  V if and only if |U | = h and H is a subgraph of G[U ]. In addition, let Gc = (W, F ) be the graph obtained from GH by setting W = E (GH ) and connecting e, f  W by an edge when e = f and e  f = . Any packing of H in G[S ] corresponds to an independent set in G[S ]c . Consequently to approximate an optimal packing in G it is enough to approximate a maximum independent set in G[S ]c . To do the latter we will again use the bounded growth property. Lemma 10. Graph G[S ]c has (M, 2h)-bounded growth with M < h(48(S + 1)h2 )h . Proof. Let e be a vertex in G[S ]c and let m be a nonnegative integer. To estimate |BG[S ]c (e, m)| we first observe that vertices from G[S ] which are contained in f 's with f  BG[S ]c (e, m) are within distance hm of any vertex which is contained in e. Consequently the number of such vertices is at most 48(S + 1)(hm)2 + 1. The number of hyperedges on this vertex set is therefore less than (48(S + 1)(hm)2 + 1)h which is M m2h for M < h(48(S + 1)h2 )h . 3.2 Algorithm

Our algorithm proceeds in two main steps. In the first step, we find a maximal independent set I = {v1 , . . . , vl } in a unit-disk graph G using the procedure ¯i and to B and S . from [KMNW05a]. Set I leads via Definition 2 to sets V Lemma 9 implies that it is enough to approximate a packing of H with the property that every copy of H is either entirely contained in B or in S . Finding a packing in G[B ] is trivial and follows again from Lemma 9. Finding a packing in G[S ] requires clustering and an approximation of a maximum independent set in Gc [S ]. We will first give a procedure that finds an approximation of a maximum independent set in a (C, q )-bounded growth graph. ApproxMaxIS Input: Constants q , and l (positive integer). G = (V, E ) which has (C, q )-bounded growth and which identifiers are bounded by m, number K = K (|G|). Output: An independent set I in G.
1 (1) Let := C 2 K . Use the algorithm from Theorem 1 to find a partition P of G. ¯ be the set obtained from P by deleting all vertices (3) For every set P  P , let P in P which have a neighbor in V \ P . In parallel, for each P  P , find a ¯ ]. maximum independent set IP in G[P (4) Return P IP .

162

A. Czygrinow and M. Ha´ n´ ckowiak

Lemma 11. Let G be a (C, q )-bounded growth graph with identifiers which are in {1, . . . , m}. Algorithm ApproxMaxIS finds an independent set I in G with |I |  (1 - (1/K ))(G) where (G) is the size of a maximum independent set in G. The algorithm runs in q+1 log C + log 1/ O C logq+1 m log 1/ rounds, where = 1/(C 2 K ).

Proof. First note that I = P IP returned by the algorithm is an independent set. Let I  be an independent set in G of size (G). For every P  P , ¯ |. |IP |  |I   P Then |I | + |V (G) \
P

(5)

¯|  P
P

¯ | + |V (G) \ |I   P
P

¯ |  |I  | = (G). P

(6)

¯ |  C |V (G)| and since (G)  C we have (C + 1)(G)  Finally, |V (G) \ P P 1 |V (G)|. Therefore, as = C 2 K, |V (G) \
P

¯ |  ((G)/K ). P

(7)

By (6) and (7), |I |  (1 - (1/K ))(G). To establish the time complexity, note that by Theorem 1, the partition P is found in q+1 log C + log 1/ O C logq+1 m log 1/ rounds. The diameter of each graph G[P ] is at most O Ii can be found locally in diam(G[P ]) rounds. ApproxPackingUDG Input: Constant k . Unit-disk graph G = (V, E ), and graph H on a fixed number of vertices h. Output: Packing of H in G. (1) Call Kuhn et. al. algorithm from [KMNW05a] to find a maximal independent ¯s and B , S given in Definition 2 with ¯1 , . . . , V set in G. Consider the sets V l := k . ¯i ]. (2) For every vi  B find an optimal packing of H in G[V
log C +log 1/

and each

Distributed Approximations for Packing in Unit-Disk Graphs

163

(3) Consider graph Gc [S ]. Set C := h(48(96h3 logk |G| + 1)h2 )h and let q = 2h. Call ApproxMaxIS in Gc [S ] with K := logk |G| to find an independent set I in Gc [S ]. (4) For each i  I , add the hyperedge of GH (copy of H ) which corresponds to i to the packing.

Theorem 3. For given k and graph H , algorithm ApproxPackingUDG finds in a unit-disk graph G an M -packing of H with M  1 - O(1/ logk |G|) H (G) where H (G) is the packing number of H in G. The algorithm runs in logO(kh |G| rounds.
2

)

Proof. From Lemma 9 there is an M -packing of H in G with M  (1 - O(1/ logk |G|))H (G) and such that every copy of H is either entirely contained ¯j 's with j  B . Let MB be ¯j , for some j  B , or does not intersect any of V in V ¯j , for some the number of copies of H in this packing which are contained in V j  B and let MS be the number of remaining copies of H . Then M = MB + MS and the number of copies of H , solB , found in the step 2 of the algorithm is such that (8) solB  MB . Any copy of H counted by MS is entirely contained in G[S ]. Therefore, MH  (Gc [S ]) as any independent set in Gc [S ] gives a packing of H in G[S ] and any packing of H in G[S ] gives an independent set in Gc [S ]. By Lemma 10, Gc [S ] has (L, 2h)-bounded growth with L < h(48(96h3 logk |G| + 1)h2 )h . As K = logk |G|, by Lemma 11, ApproxMaxIS finds in Gc [S ] an independent set I of size |I |  (1 - (1/ logk |G|))(Gc [S ]). Therefore, solS = |I |  (1 - (1/ logk |G|))(Gc [S ])  (1 - (1/ logk |G|))MS . (9)

Consequently, by (8) and (9), the number of copies of H in our solution is at least
solB + solS  (1 - (1/ log k |G|))M  (1 - (1/ log k |G|)) 1 - O(1/ logk |G|) H (G)

and so

solB + solS  (1 - O(1/ logk |G|))H (G).

To prove the time complexity, note that identifiers of V are bounded by a polynomial in |G|. In addition h and k are fixed constants, K = O(logk |G|), C = O(logkh |G|), = 1/(C 2 K ), and q = 2h is constant. Therefore, Approx2 PackingUDG runs in logO(kh ) |G| rounds.

164

A. Czygrinow and M. Ha´ n´ ckowiak

References
[AGLP89] Awerbuch, B., Goldberg, A.V., Luby, M., Plotkin, S.A.: Network Decomposition and Locality in Distributed Computation. In: Proc. 30th IEEE Symp. on Foundations of Computer Science, pp. 364­369 (1989) Czygrinow, A., Ha´ n´ ckowiak, M.: Distributed almost exact approximations for minor-closed families. In: 14th Annual European Symposium on Algorithms (ESA), pp. 244­255 (2006) Czygrinow, A., Ha´ n´ ckowiak, M.: Distributed approximation algorithms in unit disc graphs. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 385­398. Springer, Heidelberg (2006) Czygrinow, A., Ha´ n´ ckowiak, M., Szyma´ nska, E.: Distributed approximation algorithms in planar graphs. In: Calamoneri, T., Finocchi, I., Italiano, G.F. (eds.) CIAC 2006. LNCS, vol. 3998, pp. 296­307. Springer, Heidelberg (2006) Diestel, R.: Graph Theory, 3rd edn. Springer, Heidelberg (2005) Dai, F., Wu, J.: An Extended Localized Algorithm for Connected Dominating Set Formation in Ad Hoc Wireless Networks. IEEE Transactions on Parallel and Distributed Systems 15(10), 908­920 (2004) Dubhashi, D., Mei, A., Panconesi, A., Radhakrishnan, J., Srinivasan, A.: Fast Distributed Algorithms for (Weakly) Connected Dominating Sets and Linear-Size Skeletons. In: Proc. of the ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 717­724 (2003) Kuhn, F., Moscibroda, T., Wattenhofer, R.: What Cannot Be Computed Locally! In: Proceedings of 23rd ACM Symposium on the Principles of Distributed Computing (PODC), pp. 300­309 (2004) Kuhn, F., Moscibroda, T., Wattenhofer, R.: On the Locality of Bounded Growth. In: 24th ACM Symposium on the Principles of Distributed Computing (PODC), Las Vegas, Nevada, USA, pp. 60­68 (2005) Kuhn, F., Moscibroda, T., Nieberg, T., Wattenhofer, R.: Fast Deterministic Distributed Maximal Independent Set Computation on GrowthBounded Graphs. In: 19th International Symposium on Distributed Computing (DISC), Cracow, Poland, pp. 273­287 (September 2005) Kuhn, F., Moscibroda, T., Nieberg, T., Wattenhofer, R.: Local Approximation Schemes for Ad Hoc and Sensor Networks. In: 3rd ACM Joint Workshop on Foundations of Mobile Computing (DIALM-POMC), Cologne, Germany, pp. 97­103 (2005) Linial, N.: Locality in distributed graph algorithms. SIAM Journal on Computing 21(1), 193­201 (1992) Luby, M.: A simple parallel algorithm for the maximal independent set problem. SIAM J. Comput. 15(4), 1036­1053 (1986) Panconesi, A., Rizzi, R.: Some Simple Distributed Algorithms for Sparse Networks. Distributed Computing 14, 97­100 (2001) Peleg, D.: Distributed Computing: A Locality-Sensitive Approach. SIAM (2000)

[CH06a]

[CH06b]

[CHS06]

[D05] [DW04]

[DPRS03]

[KMW04]

[KMW05]

[KMNW05a]

[KMNW05b]

[L92] [L86] [PR01] [P00]

From Crash-Stop to Permanent Omission: Automatic Transformation and Weakest Failure Detectors
Carole Delporte-Gallet1 , Hugues Fauconnier1 , Felix C. Freiling2 , Lucia Draque Penso2 , and Andreas Tielmann1 ,
1

Laboratoire d'Informatique Algorithmique, Fondements et Applications (LIAFA), University Paris VII, France 2 Laboratory for Dependable Distributed Systems, University of Mannheim, Germany

Abstract. This paper studies the impact of omission failures on asynchronous distributed systems with crash-stop failures. We provide two different transformations for algorithms, failure detectors, and problem specifications, one of which is weakest failure detector preserving. We prove that our transformation of failure detector  [1] is the weakest failure detector for consensus in environments with crash-stop and permanent omission failures and a majority of correct processes. Our results help to use the power of the well-understood crash-stop model to automatically derive solutions for the general omission model, which has recently raised interest for being noticeably applicable for security problems in distributed environments equipped with security modules such as smartcards [2,3,4].

1

Introduction

Message omission failures, which have been introduced by Hadzilacos [5] and been refined by Perry and Toueg [6], put the blame of a message loss to a specific process instead of an unreliable message channel. Beyond the theoretical interest, omission models are also interesting for practical problems like they arise from the security area: Assume that some kind of trusted smartcards are disposed on untrusted processors. If these smartcards execute trusted algorithms and are able to sign messages, then it is relatively easy to restrict the power of a malicious adversary to only be able to drop messages of the trusted smartcards or to stop the smartcards themselves. Following this approach, omission models have lead to the development of reductions from security problems in the Byzantine failure model [7] such as fair-exchange [4,3], and secure multiparty computation [2] to well-known distributed problems in the general omission model, such as consensus [8], where both process crashes and message omissions may take place. Apart from that, omission failures can model overflows of local message buffers in typical communication environments.
Work was supported by grants from R´ egion Ile-de-France.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 165­178, 2007. c Springer-Verlag Berlin Heidelberg 2007

166

C. Delporte-Gallet et al.

The message omission and crash failures are considered here in asynchronous systems. Due to classical impossibility results concerning problems as consensus [9] in asynchronous systems, following the failure detector approach [10], we augment the system with oracles that give information about failures. The extension of failure detectors to more severe failure models than crash failures is unclear [11], because in these models failures may depend on the scheduling and on the algorithm. As it is easy to transform the general omission model into a model with only permanent omissions using standard techniques like the piggybacking of messages, we consider only permanent omissions and crashes. This means that if an omission failure occurs, then it occurs permanently. In this model, precise and simple definitions for failure detectors can easily be deduced from the ones in the crash-stop model. To provide the permanent omission model with the benefits of a well-understood system model like the crash-stop model, we give automatic transformations for problem specifications, failure detectors, and algorithms such that algorithms designed to tolerate only crash-stop failures can be executed in permanent omission environments and use transformed failure detectors to solve transformed problems. Specifically, we give two transformations. At first, one that works in every environment, but that transforms uniform problems into problems with only limited uniformity, and at second one that works only with a majority of correct processes, but transforms uniform crash-stop problems into their uniform permanent omission counterpart. An interesting point is the fact that the transformation of the specification gives for most of the classical problems the standard specification in the message omission and crash failure model. For example, from an algorithmic solution A of the consensus problem with a failure detector D in the crash-stop model, we automatically get A = trans(A), an algorithmic solution of the consensus problem using D = trans(D) in the message omission and crash failure model. Moreover, our first transformation preserves also the "weaker than" relation [1] between failure detectors. This means that if a failure detector is a weakest failure detector for a certain (crash-stop) problem, then its transformation is a weakest failure detector for the transformed problem. We can use this to show that our transformation of failure detector  [1] is the weakest failure detector for (uniform) consensus in an environment with permanent omission failures and a majority of correct processes. It is interesting to note that this transformed version of  can be implemented in partially synchronous models using some weak timing assumptions [12]. The problem of automatically increasing the fault-tolerance of algorithms in environments with crash-stop failures has been extensively studied before [13,14,15,16]. The results of Neiger and Toueg [14], Delporte-Gallet et al. [15], and Bazzi and Neiger [16] assume in contrast to ours synchronous systems and no failure detectors. Neiger and Toueg [14] propose several transformations from crash-stop to send omission, to general omission, and to Byzantine faults. Delporte-Gallet et al. [15] transform round-based algorithms with broadcast primitives into crash-stop-, general omission-, and Byzantine-tolerant

From Crash-Stop to Permanent Omission

167

algorithms. Asynchronous systems are considered by Basu, Charron-Bost and Toueg [13] but in the context of link failures instead of omission failures and also without failure detectors. The types of link failures that are considered by Basu, Charron-Bost and Toueg [13] are eventually reliable and fair-lossy links. Eventually reliable links can lose a finite (but unbounded) number of messages and fair-lossy links satisfy that if infinitely many messages are sent over it, then infinitely many messages do not get lost. To show our results, we extend the system model of Basu, Charron-Bost and Toueg [13] such that we can model omission failures, failure patterns, and failure detectors. Another definition for a system model with crash-recovery failures, omission failures, and failure detectors is given by Dolev et al. [17]. In this model, the existence of a fully connected component of processes that is completely detached from all other processes is assumed and only the processes in this component are declared to be correct. To the best of our knowledge, this is the first paper that investigates an automatic transformation to increase the fault tolerance of distributed algorithms in asynchronous systems augmented with failure detectors. We organize this paper as follows. In Section 2, we define our formal system model, in Section 3, we define our general problem and algorithm transformations, in Section 4 we state our theorems, and finally, in Section 5, we summarize and discuss our results. Due to the lack of space, we omit some parts of the proofs here. They can be found elsewhere [18].

2

Model

The asynchronous distributed system is assumed to consist of n distinct fullyconnected processes  = {p1 , . . . , pn }. The asynchrony of the system means, that there are no bounds on the relative process speeds and message transmission delays. To allow an easier reasoning, a discrete global clock T is added to the system. The system model used here is derived from that of Basu, CharronBost and Toueg [13]. It has been adapted to model also failure detectors and permanent omission failures. Algorithms. An algorithm A is defined as a vector of local algorithm modules (or simply modules ) A( ) = A(p1 ), . . . , A(pn ) . Each local algorithm module A(pi ) is associated with a process pi   and defined as a deterministic infinite state automaton. The local algorithm modules can exchange messages via send and receive primitives. We assume all messages to be unique. Failures and Failure Patterns. A failure pattern F is a function that maps each value t from T to an output value that specifies which failures have occurred up to time t during an execution of a distributed system. Such a failure pattern is totally independent of any algorithm. A crash-failure pattern C : T  2 denotes the set of processes that have crashed up to time t (t : C (t)  C (t +1)). Additionally to the crash of a process, it can fail by not sending or not receiving a message. We say that it omits a message. The message omissions do

168

C. Delporte-Gallet et al.

not occur because of link failures, they model overflows of local message buffers or the behavior of a malicious adversary with control over the message flow of certain processes. It is important that for every omission, there is a process responsible for it. As we already mentioned, we consider only permanent omissions and leave the treatment of transient omissions over to the underlying asynchronous communication layer. Intuitively, a process has a permanent send omission if it always fails by not sending messages to a certain other process after a certain point in time. Analogously, a process has a permanent receive omission if it always fails by not receiving messages from a certain other process after a certain point in time. The permanent omissions are modeled via a sendand a receive-omission failure pattern: OS : T  2 × and OR : T  2 × . If (ps , pd )  OS (t), then process ps has a permanent send-omission to process pd after time t. If (ps , pd )  OR (t), then process pd has a permanent receiveomission to process ps after time t. All the failure patterns defined so far can be put together to a single failure pattern F = (C, OS , OR ). With such a failure pattern, we define a process to be correct, if it experiences no failure at all. We assume that at least one process is correct. A process p is crash-correct (p  cr.-correct(F )) in F , if it does not crash. An in-connected process is a process that is crash-correct and receives all messages from a correct process (possibly indirectly) and an out-connected process is a process where a correct process receives all messages from it (also possibly indirectly). If a process p is in-connected and out-connected in a failure pattern F , then we say that p is connected in F (p  connected(F )). This means that between connected processes there is always reliable communication possible. With a simple relaying algorithm, every message can eventually be delivered. Note that it is nevertheless still possible that connected processes receive messages from disconnected processes or disconnected processes receive messages from connected ones. The difference between connected and disconnected processes is that the former are able to send and to receive messages to/from correct processes and therefore are able to communicate in both directions. It is easy to see that crash-correct(F )  connected(F )  correct(F ). We say that a failure pattern F is an omission equivalent extension of another failure pattern F (F om F ), if the set of crash-correct processes in F is at all times equal to the set of connected processes in F and there are no omission failures in F . We define an environment E to be a set of possible failure patterns. f denotes the set of all failure patterns where only crash-stop faults occur and Ec.s. f at most f processes crash. Ep.o. denotes the set of all failure patterns where crashstop and permanent omission faults may occur and at most f processes are not f f  Ep.o. ). connected (clearly, Ec.s. Failure Detectors. A failure detector provides (possibly incorrect) information about a failure pattern [10]. Associated with each failure detector is a (possibly infinite) range R of values output by that failure detector. A failure detector history FDH with range R is a function from  × T to R. FDH(p, t) is the value of the failure detector module of process p at time t. A failure detector D is a function that maps a failure pattern F to a set of failure detector histories with

From Crash-Stop to Permanent Omission

169

range R. D(F ) denotes the set of possible failure detector histories permitted by D for the failure pattern F . Note that a failure detector D is specified as a function of the failure pattern F of an execution. However, an implementation of D may use other aspects of the execution such as when messages are arrived and executions with the same failure pattern F may still have different failure detector histories. It is for this reason that we allow D(F ) to be a set of failure detector histories from which the actual failure detector history for a particular execution is selected non-deterministically. Take failure detector  [1] as an example. The output of the failure detector module of  at a process pi is a single process, pj , that pi currently considers to be crash-correct. In this case, the range of output values is R =  . For each failure pattern F ,  (F ) is the set of all failure detector histories FDH with range R that satisfy the following property: There is a time after which all the crash-correct processes always trust the same crash-correct process: t  T , pj  cr.-correct(F ), pi  cr.-correct(F ), t  t : FDH (p, t ) = pj The output of failure detector module  at a process pi may change with time, i.e. pi may trust different processes at different times. Furthermore, at any given time t, processes pi and pj may trust different processes. A local algorithm module A(pi ) can access the current output value of its local failure detector module using the action queryFD. Histories. A local history of a local algorithm module A(pi ), denoted H [i], is a finite or an infinite sequence of alternating states and events of type send, receive, queryFD, or internal. We assume that there is a function time that assigns every event to a certain point in time and define H [i]/t to be the maximal prefix of H [i] where all events have occurred before time t. A history H of A( ) is a vector of local histories H [1], H [2], . . . , H [n] . Reliable Links. A reliable link does not create, duplicate, or lose messages. Specifically, if there is no permanent omission between two processes and the recipient executes infinitely many receive actions, then it will eventually receive every message. We specify, that our underlying communication channels ensure reliable links. Problem Specifications. Let  be a set of processes and A be an algorithm. We define H(A( ), E ) to be the set of all tuples (H, F ) such that H is a history of A( ), F  E , and H and F are compatible, that is crashed processes do not take any steps after the time of their crash, there are no receive-events after a permanent omission, etc. A system S (A( ), E ) of A( ) is a subset of H(A( ), E ). A problem specification  is a set of tuples of histories and failure patterns, because (permanent) omission failures are not necessarily reflected in a history (e.g., if a process sends no messages). A system S satisfies a problem specification  , if S   . We say that an algorithm A satisfies a problem specification  in environment E , if H(A( ), E )   .

170

C. Delporte-Gallet et al.

Take consensus as an example (see Table 1): It is specified by making statements about some variables propose and decide in the states of a history (e.g. the value of decide has eventually to be equal at all (crash-)correct processes). This can be expressed as the set of all tuples (H, F ) where there exists a time t and a value v , such that for all pi  cr.-correct(F ), there exists an event e in H [i] with time(e)  t and for all states s after event e, the value of the variable decide in s is v .

3

From Crash-Stop to Permanent Omission

We will give here two transformations: one general transformation for all environments, where we provide only restricted guarantees for disconnected processes, and one for environments where less than half of the processes may not be connected, where we are able to provide for all processes the same guarantees as for the crash-stop case. To improve the fault-tolerance of algorithms, we simulate a single state of the original algorithm with several states of the simulation algorithm. For these additional states, we augment the original states with additional variables. Since an event of the simulation algorithm may lead to a state where only the augmentation variables change, the sequence of the original variables may stutter. We call a local history H [i] a stuttered and augmented extension of a history H [i] (H [i] sa H [i]), if H [i] and H [i] differ only in the value of the augmentation variables and some additional states caused by differences in these variables (in particular, H [i] sa H [i] for all H [i]). If H [i] sa H [i] for all pi   , we write H sa H . We say that a problem specification  is closed under stuttering and augmentation, if (H, F )   and H sa H implies that (H , F ) is also in  . Most problems satisfy this natural closure property (e.g. consensus). 3.1 The General Transformation

Transformation of Problem Specifications. To transform a problem specification, we first show a transformation of a tuple of a trace and a failure pattern. Based on this transformation, we transform a whole problem specification. The intuition behind this transformation is that we demand only something from processes as long as they are connected. After their disconnection, processes may behave arbitrary. More formally, let tc.s. (i) be the time at which process pi crashes in F (tc.s. (i) = , if pi never crashes). Analogously, let tp.o. (i) be the time at which process pi becomes disconnected in F (tp.o. (i) = , if pi never becomes disconnected). Then: (H , F )  trans((H, F )) : pi   : H [i]/tc.s. (i) sa H [i]/tp.o. (i)

and for a whole problem specification: trans( ) := {(H , F ) | (H , F )  trans((H, F ))  (H, F )   } A transformation of non-uniform consensus, where properties of certain propose- and decision-variables of (crash-)correct processes are specified would

From Crash-Stop to Permanent Omission

171

lead to a specification where the same properties are ensured for the states of connected processes, because only histories with the same states (disregarding the augmentation variables) are allowed in the transformation at this processes (see Table 1). We also take the states of processes before they become disconnected into account, because they (e.g. their initial states for the propose variables) may also have an influence on the fulfillment of a problem specification, although they are after their disconnection not allowed to have this influence anymore. Since we impose no restriction on the behavior of processes after their disconnection, the transformed problem specification allows them to decide a value that was never proposed (although our transformation algorithms guarantee that this will not happen). A transformation of uniform consensus leads to a problem specification where the uniform agreement is only demanded for processes before their time of disconnection. This means that it is allowed that after a partitioning of the network, the processes in the different network partitions come to different decision values. Another transformation, in which uniform consensus remains truly uniform is given in Section 3.2. Transformation of Failure Detector Specifications. We allow all failure detector histories for a failure pattern F in trans(D) that are allowed in the crash-stop version F of F in D: trans(D)(F ) :=
F

{D(F ) | F om F }

Consider failure detector  [1].  outputs only failure detector histories that eventually provide the same crash-correct leader at all crash-correct processes. Then, trans( ) outputs these failure detector histories if and only if they provide a connected common leader at all connected processes. Transformation of Algorithms. In our algorithm transformation, we add new communication layers such that some of the omission failures in the system become transparent to the algorithm (see Figure 1). We transform a given algorithm A into another algorithm A = trans(A) in two steps: ­ In the first step, we remove the send and receive actions from A and simulate them with a three-way-handshake (3wh) algorithm. The algorithm is described in Figure 2. The idea of the 3wh-algorithm is to substitute every send-action with an exchange of three messages. This means that to send a message to a certain process, it is necessary for a process to be able to send and to receive messages from it. Moreover, while the communication between connected processes is still possible, processes that are only in-connected or only out-connected (and not both) become totally disconnected. Hence, we eliminate influences of disconnected processes not existing in the crash-stop case. ­ Then, in the second step, we remove the send and receive actions from the three way handshake algorithm and simulate them with a relaying algorithm.

172

C. Delporte-Gallet et al.
A A pi m pj m m

three way handshake layer

m

m

relaying layer

Fig. 1. Additional Communication Layers

The idea of the relay algorithm is to relay every message to all other processes, such that they relay it again and all connected processes can communicate with each other, despite the fact that they are not directly-reachable. It is similar to other algorithms in the literature [19]. Its detailed description can be found in Figure 3. To execute the simulation algorithms in parallel with the actions from A, we add some new (augmentation) variables to the set of variables in the states of A. Whenever a step of the simulation algorithms is executed, the state of the original variables in A remains untouched and only the new variables change their values. Whenever a process queries a local failure detector module D(pi ), we translate it to a query on trans(D)(pi ). The relaying layer overlays the network with the best possible communication graph and the 3wh-layer on top of it cuts the unidirectional edges from this graph.
Algorithm 3wh 1: procedure 3wh-send (m, pj ) 2: relay-send([1, m], pj ); 3: 4: procedure 3wh-receive (m) 5: relay-receive ([l, m ]); 6: if (l = 1) then relay-send([2, m ], sender ([l, m ])); m := ; 7: elseif (l = 2) then relay-send([3, m ], sender ([l, m ])); m := ; 8: elseif (l = 3) then m := m ; 9: elseif [l, m ] =  then m := ; Fig. 2. The Three Way Handshake Algorithm for Process pi

From Crash-Stop to Permanent Omission

173

Algorithm Relay 1: procedure init 2: relayedi := ; deliveredi := ; 3: 4: procedure relay-send (m, pj ) 5: for k := 1 to n do 6: send([m, pj ], pk ); 7: relayedi := relayedi  {[m, pj ]}; 8: 9: procedure relay-receive (m) 10: receive([m , pk ]); 11: if ([m , pk ] = ) then m := ; 12: elseif (k = i) and (m  deliveredi ) then 13: m := m ; deliveredi := deliveredi  {m }; 14: elseif (k = i) and ([m , pk ]  relayedi ) then 15: for l := 1 to n do 16: send([m , pk ], pl ); 17: relayedi := relayedi  {[m , pk ]}; m := ; Fig. 3. The Relaying Algorithm for Process pi

3.2

The Transformation for n > 2f

If only less than a majority of the processes are disconnected (n > 2f ), then we only need to adapt the problem specification to the failure patterns of the new environment. We indicate this adaptation of a problem specification with the index p.o. and specify it in the following way: p.o. := {(H, F ) | (H, F )    F om F } If we adapt consensus to omission failures, then we get Consensusp.o. as in Table 1. The failure detector specifications can be transformed as in Section 3.1. The algorithm transformation trans2 works similar as in the previous section, but we add an additional two-way-handshake (2wh) layer between the relaying layer and the 3wh layer. The algorithm is described in Figure 4 and is similar to an algorithm in the literature [13]. The idea of the algorithm is to broadcast every message to all other processes and to block until f + 1 processes have acknowledged the message. In this way, disconnected processes block forever (since they receive less than f + 1 acknowledgements) and connected processes can continue. Thus, we emulate a crash-stop environment.

174

C. Delporte-Gallet et al. Table 1. Transformations of the Consensus Problem Consensus Validity: The decided value of every process must have been proposed. Non-Uniform No two cr.-correct Agreement: processes decide differently. Uniform No two processes Agreement: decide differently. trans(Consensus) The decided value of every connected process must have been proposed. No two connected processes decide differently. No two processes decide differently before their disconnection. Termination: Every cr.-correct Every connected process eventually process eventually decides. decides. Consensusp.o. The decided value of every process must have been proposed. No two connected processes decide differently. No two processes decide differently.

Every connected process eventually decides.

Algorithm 2wh 1: procedure init 2: receivedi := ; Acki := 0; 3: 4: procedure 2wh-send(m, pj ) 5: relay-send([m, pj , ONE], pk ) to all other pk ; Acki := 1; 6: while (Acki  f ) do 7: relay-receive ([m , pk , num])); 8: if (num = TWO) and (m = m) and (k = j ) then inc(Acki ); 9: elseif (num = ONE) then add [m , pk , num] to receivedi ; 10: 11: procedure 2wh-receive (m)) 12: m := ; relay-receive (m ); 13: if (m = ) then add m to receivedi ; 14: if ([m , pk , ONE]  receivedi ) for any m , pk then 15: relay-send([m , pk , TWO], sender ([m , pk , ONE])); 16: if (k = i) then m := m ; Fig. 4. The Two Way Handshake Algorithm for Process pi

4

Results

In our first theorem, we show that for any algorithm A, for any failure detector D, and for any problem specification  , trans(A) using trans(D) solves trans( ) in a permanent omission environment if and only if A using D solves  in a crashstop environment. This theorem does not only show that our transformation

From Crash-Stop to Permanent Omission

175

works, it furthermore ensures that we do not transform to a trivial problem specification, but to an equivalent one, since we prove both directions. Theorem 1. Let  be a problem specification closed under stuttering and augmentation. Then, if A is an algorithm using a failure detector D and A = trans(A) is the transformation of A using trans(D), it holds that:
f f f with 0  f  n : (H(A( ), Ec.s. )    (H(A ( ), Ep.o. )  trans( )

Proof. (Sketch) Due to lack of space, we only sketch the proof of the theorem here. The detailed proof can be found elsewhere [18]. The proof is divided up into f f ) and Sp.o. := (H(A ( ), Ep.o. ) and assume two parts. Let Sc.s. := (H(A( ), Ec.s. that A = trans(A). "": Assume that Sc.s.   . By constructing for a given (H, F ) in Sp.o. a tuple (H , F ) in Sc.s. with (H, F )  trans((H , F )), we can show that Sp.o.  trans(Sc.s. ). In this construction, we remove the added communication layers from H and use the properties of our two send-primitives to prove the reliability of the links in H . We ensure "No Loss" with the relaying algorithm and "No Creation" with the three way handshake algorithm. As we know from the definition of trans, that trans(Sc.s. )  trans( ), we can conclude that Sp.o.  trans( ). "": Assume that Sp.o.  trans( ) and (H, F )  Sc.s. . We then build a new history H from H and simulate all links according to the specification of the three-way-handshake and the relay algorithm such that (H , F )  f trans((H, F )) and (H , F )  Sp.o.  trans( ) (F  Ec.s. implies that F f is in Ep.o ). This means that there exists a (H , F )   , with (H , F )  trans((H , F )). Since in both, F and F occur only crash failures, F = F and therefore for all pi , H [i] sa H [i]. Together with the fact that  is closed under stuttering and augmentation, we can conclude that (H , F )   . H and H differ only in the augmentation variables that are not relevant for the fulfillment of trans( ) and therefore: (H, F )   . Our second theorem shows, that with a majority of connected processes (n > 2f ), trans2 can be used to solve the adaptation of a problem to the general omission model. Theorem 2. If A is an algorithm using a failure detector D and A = trans2 (A) is the transformation of A using trans2 (D) and  is closed under stuttering and augmentation, then it holds that:
f f )    (H(A ( ), Ep.o. )  p.o. f with f < n/2 : (H(A( ), Ec.s.

Proof. (Sketch) Due to lack of space, we only sketch the proof of the theorem f ) here. The detailed proof can be found elsewhere [18]. Let Sc.s. := (H(A( ), Ec.s. f and Sp.o. := (H(A ( ), Ep.o. ) and assume that A = trans(A). It is sufficient to show, that (H, F )  Sp.o. , (H , F )  Sc.s. : (H sa H )  (F om F ) (1)

176

C. Delporte-Gallet et al.

To show this, we construct (H , F )  Sc.s. for a given (H, F )  Sp.o. in the following way: We first remove the variables, events, and states of the relayalgorithm, then remove the same for the 2wh-algorithm, and then remove the 3wh-algorithm to get H . F is a failure pattern, such that F om F . We need to show, that (H , F ) fulfills the properties of equation 1. From the construction it is clear, that H sa H and F om F . It remains to show, that (H , F )  Sc.s. . This means, that at most f processes crash in F , H is a history of A( ) using D, all links are reliable in (H , F ), and H and F are compatible. Here we can use the properties of the 2wh-algorithm to ensure that a process that is crashed in F takes no steps in H after the time of its crash. Weakest Failure Detectors. A failure detector [1] is a weakest failure detector for a problem specification  in environment E , if it is necessary and sufficient. Sufficient means, that there exists an algorithm using this failure detector that satisfies  in E , whereas necessary means, that every other sufficient failure detector is reducible to it. A failure detector D is reducible to another failure detector D , if there exists a transformation algorithm TDD , such that for every tuple (H, F )  H(TDD ( ), E ), H is equivalent to a failure detector history FDH in D (F ). We call the problem specification that arises in emulating D , Probl(D ). In the following theorem, we show that trans preserves the weakest failure detector property for non-uniform1 failure detectors. Theorem 3. For all f with 1  f  n: If a non-uniform failure detector D is f and  is closed under stuttering and a weakest failure detector for  in Ec.s. f augmentation, then trans(D) is a weakest failure detector for trans( ) in Ep.o. .
f , then trans(D) is sufficient Proof. If D is a weakest failure detector for  in Ec.s. f for trans( ) in Ep.o. (Theorem 1). It remains to show that trans(D) is also necessary. f . Clearly,   Assume a failure detector D is sufficient for trans( ) in Ep.o. f trans( ) (since H sa H for all H ). Therefore, D is sufficient for  in Ec.s. , f and moreover, D is reducible to D in Ec.s. (since D is a weakest failure detector f for  in Ec.s. ). This means that it is possible to emulate D using D (i.e. a problem specification Probl(D) that is equivalent to D). If the reduction algorithm f is TD D , then trans(TD D ) using trans(D ) emulates trans(Probl(D)) in Ep.o. (Theorem 1) and since D is non-uniform, the transformation of the problem specification, trans(P robl(D)) is equivalent to the transformation of the failure detector trans(D) (trans does not change the meaning of Probl(D) since only the states of connected processes matter). Therefore, D is reducible to trans(D) f . in Ep.o.

With Theorem 1, 2, and 3 we are able to show, the following: Theorem 4. trans( ) is a weakest failure detector for uniform Consensusp.o. with a majority of correct processes.
1

A non-uniform failure detector D outputs always the same set of histories for two failure patterns F and F in which correct(F ) = correct(F ) (i.e. D(F ) = D(F )).

From Crash-Stop to Permanent Omission

177

Proof. Since we know, that  is a weakest failure detector for non-uniform Consensus [1] and  is clearly non-uniform, together with Theorem 3, trans( ) is a weakest failure detector for non-uniform trans(Consensus). Since non-uniform trans(Consensus) is strictly weaker than uniform Consensusp.o. , trans( ) is especially necessary for uniform Consensusp.o. . To show that trans( ) is sufficient for uniform Consensusp.o. , we can simply use Theorem 2, since we know that  is sufficient for uniform Consensus with a majority of correct processes.

5

Conclusion

We have given transformations for algorithms, failure detectors, and problem specifications, so crash-stop resilient algorithms can be automatically enhanced to tolerate the more severe general omission failures, highly applicable in practical settings running security problems. Furthermore we have shown that trans( ) is the weakest failure detector for consensus in an environment with permanent omission failures where less than half of the processes may crash. Additionally, we have proven that our transformation preserves the weakest failure detector property for all non-uniform failure detectors. As an open problem, we think that it would be interesting to replace the requirement of a correct majority in our second transformation with a failure detector  [20] that will also be sufficient. Apart from that, it may be possible to give more specific transformations that are less general, but also less communication expensive than our transformation.

References
1. Chandra, T.D., Hadzilacos, V., Toueg, S.: The weakest failure detector for solving consensus. In: Herlihy, M. (ed.) PODC'92. Proceedings of the 11th Annual ACM Symposium on Principles of Distributed Computing, Vancouver, BC, Canada, pp. 147­158. ACM Press, New York (1992) 2. Fort, M., Freiling, F., Penso, L.D., Benenson, Z., Kesdogan, D.: Trustedpals: Secure multiparty computation implemented with smartcards. In: Gollmann, D., Meier, J., Sabelfeld, A. (eds.) ESORICS 2006. LNCS, vol. 4189, pp. 34­48. Springer, Heidelberg (2006) 3. Freiling, F., Herlihy, M., Penso, L.D.: Optimal randomized omission-tolerant uniform consensus in message passing systems. In: Anderson, J.H., Prencipe, G., Wattenhofer, R. (eds.) OPODIS 2005. LNCS, vol. 3974. Springer, Heidelberg (2006) 4. Avoine, G., G¨ artner, F.C., Guerraoui, R., Vukolic, M.: Gracefully degrading fair exchange with security modules. In: Dal Cin, M., Ka^ aniche, M., Pataricza, A. (eds.) EDCC 2005. LNCS, vol. 3463, pp. 55­71. Springer, Heidelberg (2005) 5. Hadzilacos, V.: Issues of fault tolerance in concurrent computations (databases, reliability, transactions, agreement protocols, distributed computing). PhD thesis, Harvard University (1985) 6. Perry, K.J., Toueg, S.: Distributed agreement in the presence of processor and communication faults. IEEE Trans. Softw. Eng. 12(3), 477­482 (1986) 7. Lamport, L., Shostak, R., Pease, M.: The byzantine generals problem. ACM Trans. Program. Lang. Syst. 4(3), 382­401 (1982)

178

C. Delporte-Gallet et al.

8. Chaudhuri, S.: Agreement is harder than consensus: set consensus problems in totally asynchronous systems. In: Proceedings of Principles of Distributed Computing 1990 (1990) 9. Fischer, M.J., Lynch, N.A., Paterson, M.S.: Impossibility of distributed consensus with one faulty process. J. ACM 32(2), 374­382 (1985) 10. Chandra, T.D., Toueg, S.: Unreliable failure detectors for reliable distributed systems. Journal of the ACM 43(2), 225­267 (1996) 11. Doudou, A., Garbinato, B., Guerraoui, R., ´ e Schiper, A.: Muteness failure detectors: Specification and implementation. In: Hlavicka, J., Maehle, E., Pataricza, A. (eds.) Dependable Computing - EDDC-3. LNCS, vol. 1667, pp. 71­87. Springer, Heidelberg (1999) 12. Delporte-Gallet, C., Fauconnier, H., Freiling, F.C.: Revisiting failure detection and consensus in omission failure environments. In: Van Hung, D., Wirsing, M. (eds.) ICTAC 2005. LNCS, vol. 3722, pp. 394­408. Springer, Heidelberg (2005) 13. Basu, A., Charron-Bost, B., Toueg, S.: Simulating reliable links with unreliable ¨ Marzullo, K. (eds.) links in the presence of process crashes. In: Babao glu, O., WDAG 1996. LNCS, vol. 1151, pp. 105­122. Springer, Heidelberg (1996) 14. Neiger, G., Toueg, S.: Automatically increasing the fault-tolerance of distributed algorithms. Journal of Algorithms 11(3), 374­419 (1990) 15. Delporte-Gallet, C., Fauconnier, H., Guerraoui, R., Pochon, B.: The perfectlysynchronised round-based model of distributed computing. Information & Computation (to appear, 2007) 16. Bazzi, R.A., Neiger, G.: Simulating crash failures with many faulty processors (extended abstract). In: Segall, A., Zaks, S. (eds.) WDAG 1992. LNCS, vol. 647, pp. 166­184. Springer, Heidelberg (1992) 17. Dolev, D., Friedman, R., Keidar, I., Malkhi, D.: Brief announcement: Failure detectors in omission failure environments. In: Symposium on Principles of Distributed Computing, p. 286 (1997) 18. Delporte-Gallet, C., Fauconnier, H., Freiling, F., Penso, L.D., Tielmann, A.: Automatic transformations from crash-stop to permanent omission. HAL archives ouvertes, hal-00160626 (2007) 19. Srikanth, T.K., Toueg, S.: Simulating authenticated broadcasts to derive simple fault-tolerant algorithms. Distributed Computing 2(2), 80­94 (1987) 20. Delporte-Gallet, C., Fauconnier, H., Guerraoui, R., Hadzilacos, V., Kouznetsov, P., Toueg, S.: The weakest failure detectors to solve certain fundamental problems in distributed computing. In: PODC '04. Proceedings of the twenty-third annual ACM symposium on Principles of distributed computing, pp. 338­346. ACM Press, New York (2004)

Deterministic Distributed Construction of Linear Stretch Spanners in Polylogarithmic Time
Bilel Derbel1 , Cyril Gavoille2, , and David Peleg3 ,
Laboratoire d'Informatique Fondamentale (LIF), Universit´ e de Provence Aix-Marseille 1, France derbel@cmi.univ-mrs.fr 2 Laboratoire Bordelais de Recherche en Informatique (LaBRI), Universit´ e de Bordeaux, France gavoille@labri.fr 3 Department of Computer Science and Applied Mathematics, The Weizmann Institute, Rehovot, Israel david.peleg@weizmann.ac.il Abstract. The paper presents a deterministic distributed algorithm that given an n node unweighted graph constructs an O(n3/2 ) edge 3spanner for it in O(log n) time. This algorithm is then extended into a deterministic algorithm for computing an O(k n1+1/k ) edge O(k)-spanner 1. This estabin 2O (k) logk-1 n time for every integer parameter k lishes that the problem of the deterministic construction of a linear (in k) stretch spanner with few edges can be solved in the distributed setting in polylogarithmic time. The paper also investigates the distributed construction of sparse spanners with almost pure additive stretch (1 + ,  ), i.e., such that the distance in the spanner is at most 1 + times the original distance plus  . It is shown, for every > 0, that in O( -1 log n) time one can deterministically construct a spanner with O(n3/2 ) edges that is both a 3-spanner and a (1 + , 8 log n)-spanner. Furthermore, it is shown that in  nO (1/ log n) + O(1/ ) time one can deterministically construct a spanner with O(n3/2 ) edges which is both a 3-spanner and a (1 + , 4)-spanner. This algorithm can be transformed into a Las Vegas randomized algorithm with guarantees on the stretch and time, running in O( -1 + log n) expected time. Keywords: distributed algorithms, graph spanners, time complexity.
1

1

Introduction

Background: The purpose of this paper is to study the locality properties of graph spanners, and particularly, of efficient deterministic distributed construction methods for spanners. Graph spanners are a fundamental graph structure
Supported by the ANR-project "GRAAL", and the ´ equipe-projet INRIA ´ "CEPAGE". Supported in part by grants from the Israel Science Foundation and the Minerva Foundation.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 179­192, 2007. c Springer-Verlag Berlin Heidelberg 2007

180

B. Derbel, C. Gavoille, and D. Peleg

which can be thought of intuitively as a generalization of the concept of spanning trees. We say that H is an (,  )-spanner of a graph G if H is a spanning subgraph of G and dH (u, v )  · dG (u, v ) +  for all nodes u, v of G, where dX (u, v ) denotes the distance from u to v in the graph X . A pair (,  ) for which H is an (,  )-spanner is called stretch of H , and the size of H is the number of its edges. An (, 0)-spanner is also referred to as an -spanner. The quality of a spanner is measured by the trade-off between its stretch and size. The locality level of constructing a graph spanner can be measured by the time needed to construct such a spanner. In the distributed setting, the best a node can do in t time units is to collect information from its t neighborhood. Hence the time complexity of a distributed algorithm for a given problem can be related to the amount of information needed to solve the problem. Many fundamental problems such as maximal independent set (MIS), coloring, and sparse covers and decompositions, have been studied from the locality point of view in the past. In general, such problems appear to have time efficient distributed algorithms in the randomized setting or for some restricted families of graphs. By "efficient" we mean algorithms breaking the polylogarithmic time barrier. However, no deterministic distributed algorithms having a polylogarithmic running time for every graph are known for any of these problems. The main difficulty in solving such problems is to break the symmetry in a distributed and efficient way when making decisions. While randomization helps to achieve the goal of symmetry breaking, trying to do it by a deterministic method leads to nontrivial combinatorial and algorithmic problems, essentially due to the local nature of distributed computations. Deterministic construction of graph spanners is also a typical problem where breaking the symmetry appears as the major problem for finding fast algorithms. In this paper we overcome this difficulty, showing that near-optimal high quality graph spanners can be constructed in polylogarithmic time. Our algorithm is based on breaking the symmetry using independent dominating sets and on a new sequential construction of spanners exploiting some particular stretch-size properties for bipartite graphs. Constructing spanners efficiently is also of interest from a practical point of view, since such structures are often used in many applications. In fact, graph spanners are in the basis of various applications in distributed systems (cf. [23]). For instance, the relationship between the quality of spanners and the time and message complexity of network synchronizers is established in [24] (see also [1,21]). Spanners are also implicitly used for the design of low stretch routing schemes with compact tables [11,16,25,27,29], and appear in many parallel and distributed algorithms for computing approximate shortest paths and for the design of compact data-structures, a.k.a. distance oracles [8,20,28,30,10]. Related Work: We consider unweighted connected graphs with n nodes. Sparse and low stretch spanners can be constructed from the sparse partitions and covers of [6] or the (d, c)-decompositions of [5], which give a partition of the graph into clusters of diameter at most d such that the graph obtained by contracting each cluster can be properly c-colored. There are several deterministic algorithms for constructing (d, c)-decompositions [2,3,4,22]. The resulting

Deterministic Distributed Construction of Linear Stretch Spanners

181

distributed algorithms provide O(k )-spanners of size O(n1+1/k ), for any integral parameter k 1. However, these algorithms run in  (n1/k+ ) time, where  =  (1/ log n ), and provide a stretch at least 4k - 3. Better stretch-size tradeoffs exist but with an increasing time complexity. More recently, a deterministic distributed algorithm has been proposed for constructing a (2k - 1)-spanner of size O(n1+1/k ) in O(n1-1/k ) time [13]. The latter stretch-size trade-off is optimal since, according to an Erd¨ os Conjecture verified for k = 1, 2, 3, 5 [32], there are graphs with  (n1+1/k ) edges and girth 2k + 2 (the length of the smallest induced cycle), thus for which every (,  )-spanner requires  (n1+1/k ) edges if  +  < 2k + 1. More time efficient algorithms were given in [12] at the price of slightly increas ing the stretch. The algorithm runs in nO(1/ log n) times and provides (k , k )spanners with O(log k · n1+1/k ) edges where k and k depend on the positions of the two leading 1's in the binary representation of k and are essentially in order of k log2 5 . In particular, for k = 2, the stretch is (3, 2). Randomized distributed algorithms achieving better performances exist. There is a straightforward (Las Vegas1 ) randomized implementation of the algorithms of [12] that provides O(k log2 5 )-spanners of O(log k · n1+1/k ) edges in O(log n) expected time. An algorithm for sparsifying a graph was used in [15] at the bottleneck of constructing small connected dominating sets. This (Monte-Carlo2) algorithm constructs, with high probability, a O(log n)-spanner with O(n) edges in O(log3 n) time. A (Monte Carlo) algorithm that computes a (2k - 1)-spanner with expected size O(k n1+1/k ) in O(k 2 ) time was given in [9]. However, as mentioned in [3], a randomized solution (in particular those coming from Monte Carlo algorithms) might not be acceptable in some cases, especially for distributed computing applications. In the case of graph spanners, deterministic algorithms that guarantee a high quality spanner are of more than a theoretical interest. Indeed, one cannot just run a randomized distributed algorithm several times to guarantee a good decomposition, since checking the global quality of the spanner in the distributed model is time consuming. Sequential and distributed algorithms for constructing (1 + ,  )-spanners were developed in [17,19,18]. The resulting spanner size is O(n1+ ) and the construction time is O(n ), where  =  (, ) is independent of n but grows super-polynomially in  -1 and -1 . Recently, a sequential algorithm based on a randomized sampling technique was given in [31], providing a spanner with O(k n1+1/k ) edges such that the distance d between any two nodes in the original graph is bounded by d + o(d) in the spanner. Pure additive spanners, i.e., spanners whose stretch is on the form (1,  ), are known only for k = 2 and k = 3. Sequential algorithms that construct a (1, 2)-spanner with O(n3/2 ) edges and a (1, 6)-spanner with O(n4/3 ) edges were given respectively in [18] and in [7]. See [26] for further discussions. Main results: In this paper, we construct in O(log n) time and for every graph a 3-spanner with O(n3/2 ) edges. This result is generalized to construct in 2O(k)
1 2

The bounds on the stretch and the size are always guaranteed. There are no deterministic guarantees for the size and the stretch.

182

B. Derbel, C. Gavoille, and D. Peleg

logk-1 n time and for every graph a (4k - 3)-spanner with O(k n1+1/k ) edges for every k 1. Our construction improves all previous deterministic constructions of low stretch spanners with few edges. Our algorithms are based on two main ideas. The first idea enables us to achieve the polylogarithmic time complexity. It is based on clustering the graph using any known time-efficient algorithm for constructing an independent dominating set, namely, a set X of pairwise non-adjacent nodes such that every node of the graph is at distance at most  from X . The second idea enables us achieve the linear stretch bound with the desired size. It is based on spanning independently in parallel (i) the intra-cluster edges using any known sequential algorithm, and (ii) the inter-cluster edges in the border of each cluster using a new size-constrained spanner for bipartite graphs. Known algorithms for constructing independent -dominating sets are more time consuming when  is small (typically when  is a constant). The key point of our fast construction is to use our spanner algorithm for bipartite graphs in order to keep the stretch low and to choose  to be any parameter possibly depending on n. In particular, we show that the parameter  does not affect the stretch but only the time construction. The construction time of low stretch spanner is then dominated by the construction of an independent -dominating set. Since the fastest known deterministic algorithm for constructing an independent -dominating set is obtained for  = O(log n) and has running time O(log n), we are able to construct the desired low stretch spanner in polylogarithmic time. A generic scheme called Generic Spanner that utilizes these ideas is first described and analyzed in Section 2. Our generic scheme assumes the existence of a sequential algorithm Z Spannerk for bipartite graphs that constructs a spanner with some desired constraints on the size and the stretch. For the case k = 2, such an algorithm is described and analyzed in detail in Section 3, yielding our first main result: the deterministic construction of an optimal 3-spanner with O(n3/2 ) edges in O(log n) time (Theorem 1). This result is generalized for any k in Section 4, giving Algorithm Z Spannerk which yields our second main result: the deterministic construction of a (4k - 3)-spanner with O(k n1+1/k ) edges in 2O(k) logk-1 n time (Theorem 2). We also investigate the construction of almost pure additive spanners for k = 2. In Section 5, we construct an almost pure additive (1 + ,  )-spanner with O(n3/2 ) edges for any > 0. This is obtained by simply adding breadthfirst searching (BFS) trees up to some fixed parameter around the dense clusters constructed by Generic Spanner for k = 2. This allows us to reduce the stretch while preserving the size bound and increasing the time complexity by only a small factor. Several bounds on the stretch and the time complexity are then obtained by using either an independent O(log n)-dominating set algorithm or a MIS algorithm. More precisely, we refine our 3-spanner construction to obtain and two fast distributed algorithms. The first one runs in O( -1 log n) time  O(1/ log n) provides additive stretch  = 8 log n. The second algorithm runs in n + O( -1 ) time and provides additive stretch  = 4. The latter algorithm can also

Deterministic Distributed Construction of Linear Stretch Spanners

183

be implemented in O( -1 + log n) expected time (using O(log n) expected time algorithm for MIS) with deterministic stretch and size. Model and definitions: We assume the classical LOCAL distributed model of computation (cf. [23], Chapter 2). More precisely, the network is modeled by a connected graph G, whose nodes represent the autonomous computation entities of the network and whose edges represent direct communication links. For simplicity, we assume that communication is synchronous, i.e., there exists a common global clock that generates pulses. At each pulse, nodes can send and receive messages of unlimited size. We assume that a message which is sent at a given pulse arrives before the beginning of the next pulse. The local computations done by a node are assumed to take negligible time. Define the time complexity of a distributed algorithm to be the worst-case number of pulses from the beginning of the algorithm execution to its termination. Given an integer t 1, the t-th power of G, denoted by Gt , is the graph obtained from G by adding an edge between any two nodes at distance at most t in G. For a set of nodes H , G[H ] denotes the subgraph of G induced by H . For X, Y  V , let dG (X, Y ) = min {dG (x, y ) : x  X and y  Y }. We associate with each v  V a region, denoted by R(v ), which is a set of nodes containing v and inducing a connected subgraph of G. We denote by R+ (v ) = {u  V : dG (u, R(v )) 1}. Given a set U  V , we denote by G (U ) the neighborhood of the set U , i.e., G (U ) = {u  V : dG (u, U ) = 1}. Note that G (R(v )) = R+ (v ) \ R(v ). Given a region R(v ), the surrounding graph of R(v ) is the graph Bv induced by the edges {x, y }  E (G) such that x  R(v ) and y  G (R(v )). Informally speaking, Bv is the collection of the outgoing edges of R(v ), namely, the edges having one of their end-points in R(v ) and the other one outside R(v ). One can easily see that Bv is a collection of connected bipartite subgraphs of G lying at the frontier of R(v ). The eccentricity of a node v in G is defined as maxuV dG (u, v ). For a node v  X , we denote by BFS(v, X ) a breadth-first search tree rooted at v and spanning X . We denote by IDS(G, ) (respectively, MIS(G)) any independent dominating set (resp., maximal independent set) of G. One can check that a set is an MIS(G) if and only if it is an IDS(G, 1). We denote by IDS(n, ) the time complexity needed to construct an independent -dominating set on a graph of n nodes. We note that MIS(n) = IDS (n, 1). Due to lack of space, the proofs of our lemmas and theorems are omitted and will appear in the full version of the paper.

2

A General Scheme

In this section we give a high level description of Algorithm Generic Spanner (see Fig 1). It uses two sub-routines: · Seq Spannerk : can be any algorithm that given an n-node graph and a parameter k > 0 constructs a s(k )-spanner with O(k n1+1/k ) edges.

184

B. Derbel, C. Gavoille, and D. Peleg

· Z Spannerk : can be any algorithm that given a bipartite graph B = (W  V, E ) and a parameter k > 0 constructs a z (k )-spanner with O(|V  W | + |W | · |V  W |1/k ) edges. These two algorithms are executed by only some nodes locally and in a noninterfering manner. Thus, we can use any two possibly sequential algorithm without affecting the distributed time complexity of the overall algorithm. Many algorithms are known for the first of the latter tasks, namely, providing spanners with O(k n1+1/k ) edges and stretch s(k ) = 2k - 1 for any graph. In contrast, no trivial constructions are known for the second task, of providing spanners with both a low stretch z (k ) and the constrained size O(|V  W | + 1/k |W | · |V  W | ) for bipartite graphs. Solving this latter task will be the aim of sections 3 and 4. In the rest of this section, we simply assume the existence of such a construction and focus on the properties of the general scheme defined by Generic Spanner. 2.1 Description of the Generic Scheme

Algorithm Generic Spanner (Fig. 1) is based on clustering the dense regions of the graph and spanning the edges of these regions efficiently. The algorithm works in at most k iterations, each of six steps. Step 1 computes the set L of light nodes, that is, the nodes whose corresponding regions have a sparse neighborhood. Step 2 considers (in parallel) each light region R(v ) and its surrounding graph Bv . Fig. 2 gives an idea of how a region R(v ) and the graph Bv may look like.Each connected component of Bv is a bipartite subgraph having one set of nodes on the border of R(v ) and the other set outside R(v ). The intra-region edges are

Input: a graph G = (V, E ) with n = |V |, and integers , k 1. Output: a spanner S of G with stretch max {z (k), s(k)} and size O(k n1+1/k ). Set U := V ; r = 0; S := ; v  V , R(v ) := {v } and c(v ) := v ; For i := 1 to k do: Span light regions 1. L := {v  U : |R+ (v )| ni/k }; 2. For all v  L do (in parallel): (a) Let Bv be the surrounding graph R(v ); (b) S := S  Seq Spannerk (G[R(v )])  Z Spannerk (Bv ); Form new dense regions 3. X := IDS(G2(r+1) [U \ L], ); 4. z  U , if dG (z, X ) (2 + 1)r + 2, then set c(z ) to be the closest node of X , breaking ties with identities; 5. v  X , R(v ) := {z  V : c(z ) = v }; 6. U := X and r := (2 + 1)r + 2; Fig. 1. Algorithm Generic Spanner

Deterministic Distributed Construction of Linear Stretch Spanners

185

using Algorithm Seq Spannerk whereas the inter-region edges (those, of the surrounding graph Bv ) are spanned using Algorithm Z Spannerk . This step can be performed efficiently by collecting a copy of R+ (v ) in v that computes the result and then broadcasts this information. Algorithm Z Spannerk spans inter-region edges using paths zigzagging from the border to the outside of a region, thus avoiding to use long paths going from the border to the center v . The intra-region edges are spanned using any distributed or sequential algorithm in order to guarantee the best possible stretchsize trade-offs inside a region. The radius of constructed regions (which depends only on parameter ) will affect only the time complexity but not the stretch of the obtained spanner. This observation will enable us to use a fast IDS algorithm without constraining  to be too small (typically, by taking  order of log n).

R + (v )

B1 B2 B5

Seq Spanner(R(v ))

R (v )

v
Z Spanner(B5)

B3

B4

Fig. 2. A region R(v ) and its surrounding graph Bv =
j [1,5]

Bj .

After Step 2, only the neighborhood of sparse regions are spanned. In the other steps, the remaining dense regions are processed the in order to merge them together. The goal here is to grow the dense regions until they become sparse. Thus, we would be able to span them without adding too many edges. In fact, in Steps 3, 4, and 5, we construct new dense regions centered around some well chosen dense nodes. First, we construct an independent -dominating set X of the graph G2(r+1) [U \ L] where r is an upper bound of the radius of any region and U is the set of remaining dense nodes. Then, using a classical consistent coloring mechanism (cf. [23], Chapter 22, Lemma 22.1.2), all dense regions are merged into new regions having the nodes of the IDS as their centers (note that a light region might get merged with a dense one). The merging process guarantees that the new regions are disjoint and connected, and their

186

B. Derbel, C. Gavoille, and D. Peleg

radius grows up by at most a multiplicative factor O(). In addition, one should remark that by considering the 2(r + 1) power of G, it is guaranteed that the neighborhood of the regions induced by the set X are disjoint. Thus, each new formed region contains at least its neighborhood. This observation is essential to obtain the desired size for our spanner. In Step 6, the set of dense regions is updated for the next iteration. On iteration k the sparsity condition of Step 1 is always true, hence all the regions are light, which guarantees that all nodes are spanned. 2.2 Analysis of the Algorithm

For every phase i, denote by Li (resp. Xi ) the set L (resp. X ) computed during phase i, i.e., after Steps 1 and 3 of phase i. Similarly, denote by ci (z ) the color of z assigned during phase i, i.e., after Step 4 of phase i. Denote by Ui the set U at the beginning of phase i, and let ri denote the value of r at the beginning of phase i. Observe that Ui = Xi-1 for every i > 1. For a node v  Ui , denote by Ri (v ) the region of v at the beginning of phase i. The following lemma is easily proved by induction relying on the description of the algorithm. Lemma 1. For every phase i > 0, and for every v  Ui , |Ri (v )| + (v )  Ri+1 (v ). if v  Xi , then Ri n(i-1)/k , and

Inspired by the proofs of lemmas 1 to 4 in [12], one can prove the following: Lemma 2. in Algorithm Generic Spanner, the following holds: ­ For every phase i and for every v  Ui , ri is the eccentricity of v in G[Ri (v )]. ­ For every phase i and for all nodes u = v  Ui , Ri (u)  Ri (v ) = . ­ For every node u  V , there exists a phase i and a node v such that u  Ri (v ) and v  Li  Ui . The following two main lemmas are used in our construction. Lemma 3. The output S of Algorithm Generic Spanner is a max {z (k ), s(k )}spanner of G with at most O(k n1+1/k ) edges. Lemma 4. Algorithm Generic Spanner can be implemented in the distributed LOCAL model in O((2 + 1)k-2 · IDS(n, ) + (2 + 1)k-1 ) time.

3

3-Spanner Construction

In order to apply Lemma 3, it is necessary to provide an algorithm that given a surrounding graph Bv of some region R(v ) constructs a spanner with the desired constraints on the stretch and the size. Since any surrounding graph Bv is a collection of connected bipartite components, it is clear that the desired stretch and size spanner can be obtained by providing an algorithm with these

Deterministic Distributed Construction of Linear Stretch Spanners

187

properties for bipartite graphs and then applying that algorithm in parallel for each connected component. In this section we describe in detail a Z Spanner algorithm providing the desired properties for a bipartite graph. More precisely, we prove the following more powerful result. Lemma 5. Every connected bipartite graph B = (W  V, E ) has a spanner S with at most O(|V | + |W |· |V |) edges satisfying the following stretch properties: v, w  V  W, dS (v, w) 2 · dB (v, w) + 1 if dB (v, w) is odd 2 · dB (v, w) + 2 otherwise.

Let us remark that for k = 2 in Generic Spanner, Lemma 5 leads to the construction of a 3-spanner. In [23], it is shown how to construct a (2 log n, 3)-ruling set in O(log n) deterministic time. A (, s)-ruling set with s > 1 is in particular an independent -dominating set. Thus, one can construct an independent 2 log n-dominating set deterministically in O(log n) time. Hence, for k = 2 and  = 2 log n in Generic Spanner, we obtain: Theorem 1. There exists a distributed algorithm that given an n-node graph constructs a 3-spanner with O(n3/2 ) edges in O(log n) deterministic time. In the rest of this section, we give a Z Spanner2 algorithm providing the properties claimed in Lemma 5. 3.1 Description of Z Spanner2

Algorithm Z Spanner2 with input a bipartite graph B is based on a sequential greedy technique (see Fig. 3):
Input: a connected bipartite graph B = (W  V, E ). Output: a 3-spanner S of B with O(|V | + |W | · |V |) edges. Set V1 := V ; W1 := W ; B1 := B ; S := ; i := 1; While Wi =  do: 1. Let wi  Wi with the highest degree in Bi , breaking ties arbitrary. 2. Ni := Bi ({wi }). 3. S := S  Bi [Ni ]. 4. For every j < i such that wi  B (Nj ) do: (a) Let ej,wi be an edge in E connecting Nj to wi . (b) S := S  {ej,wi }. 5. Construct the new graph Bi+1 : (a) Vi+1 := Vi \ Ni and Wi+1 := Wi \ {wi }. (b) Bi+1 := Bi [Vi+1  Wi+1 ]. 6. i := i + 1; Fig. 3. Algorithm Z Spanner2

188

B. Derbel, C. Gavoille, and D. Peleg

In each iteration i  {1, . . . , |W |}, a new graph Bi = (Wi  Vi , Ei ) is considered on the basis of the graph Bi-1 corresponding to the previous iteration (B1 := B ). Some edges of B are added to the spanner S as follows: Select a node wi  Wi with the highest degree in Bi , and add to S its neighborhood Ni in Bi (with its incident edges). Then, if wi is connected in the original graph B to Nj with j < i, a set computed at some previous step, an edge ei,wj connecting wi to Nj is added to S . (See Fig. 4).
Vi
N1 N2 Ni-1 Ni

Vi+1
e2,wi

Wi+1
w1 w2 wi-1 wi

Wi

Fig. 4. The i-th iteration Algorithm Z Spanner2

3.2

Analysis of Z Spanner2

Lemma 6. Let u, v, w be three nodes of a bipartite graph B = (W  V, E ) such that u  V , {u, v }  E and {u, w}  E . The output spanner S of Algorithm Z Spanner2 (B ) satisfies dS (u, v ) 3 and dS (v, w) 4. Lemma 7. For any bipartite graph B = (W  V, E ), the output spanner of Z Spanner2 (B ) has O(|V | + |W | · |V |) edges. Using the previous lemmas, we are able to prove the stretch and size bounds as stated in Lemma 5.

4

O(k)-Spanner Construction

Algorithm Z Spanner2 can be extended for k > 2. The extended algorithm, Z Spannerk , is given in Fig. 5. Given a bipartite graph B = (W  V, E ), we carefully construct a partial partition of B containing clusters of small radius. More precisely, at each iteration, a cluster is grown in a layered fashion around some node w  W until the cluster becomes sparse. Once a cluster is constructed, the neighborhood of the cluster is spanned by a BFS tree and the cluster is removed from the graph B . The algorithm terminates when all the nodes of W are clustered. We remark that at the end of our algorithm, some nodes in V may remain uncovered by the clustering, however each node in W belongs to a cluster. The originality of our algorithm comparing with classical sparse partition (or covers) algorithms is to compute the sparsity of a layer depending on the range of

Deterministic Distributed Construction of Linear Stretch Spanners

189

the layer. Let C be a cluster being constructed in the while loop of Z Spannerk . Suppose that C contains i successive layers L0 , . . . , Li-1 . Then, consider the new layer Li to be processed. Since B is bipartite, then either Li  V (if i is odd) or Li  W (if i is even). In the first case, we add Li to the cluster C if it is dense enough comparing with layer Li-1 . In the second case, we add Li to the cluster C if it is dense enough comparing with layer Li-2 . The main observation that will guarantee the desired size is that in the two cases layers Li-1 or Li-2 belong to W .
Input: a connected bipartite graph B = (W  V, E ) and an integer k > 0. Output: a (4k - 3)-spanner S of B with O(|V  W | + |W | · |V  W |1/k ) edges. S := ; while W =  do pick a node v  W ; C := {v }, L0 := {v } and i := 1; dense := True; while dense do Li := B (C ); j := i - 2 + (i mod 2); if |Li | > |V  W |1/k · |Lj | then C := C  Li ; i := i + 1; else dense := False; S := S  BFS(v, C  Li ); W := W \ C and V := V \ C ;

Fig. 5. Algorithm Z Spannerk

By analyzing Algorithm Z Spannerk , we can show that: Lemma 8. Let k 1. Every connected bipartite graph B = (V  W, E ), has a 1/k (4k - 3)-spanner with O(|V  W | + |W | · |V  W | ) edges. Combining Lemma 3 and 4 for  = 2 log n, we obtain: Theorem 2. There exists a distributed algorithm that given an n-node graph and an integer k 1, constructs a (4k - 3)-spanner for it with O(k n1+1/k ) k -1 O (k ) edges in 2 log n deterministic time.

5

Improving the Stretch for k = 2

It is shown in [14,18] that every graph has a (1, 2)-spanner with O(n3/2 ) edges. Nevertheless, no fast distributed construction of such a spanner is known. In this section, we give fast distributed constructions that enable us to obtain 3spanners of size O(n3/2 ) which are also almost pure additive spanners. More

190

B. Derbel, C. Gavoille, and D. Peleg

precisely, the multiplicative component on the stretch is (1 + ) and the additive component is independent of but depends on the time complexity. Our construction works in two stages. In the first stage, we run Algorithm Generic Spanner with parameter k = 2 and we obtain a spanner S1 and a set of dense nodes X . The set X here denotes the set of nodes computed by the first iteration of Generic Spanner, i.e., X = X1 . In the second stage, we add to S1 a BFS tree up to a depth 2 +  rooted at each node v  X ( is a given parameter). By setting  = 2 log n and  = ( -1 log n) with > 0, and using the O(log n) time deterministic algorithm for independent (2 log n)-dominating sets, one can prove: Lemma 9. There exists a distributed algorithm that given an n-node graph G and a parameter > 0, constructs in O( -1 log n) deterministic time a 3-spanner S with O(n3/2 ) edges and satisfying the following stretch properties: u, v  V, dS (u, v ) dG (u, v ) + 8 log n if dG (u, v ) (1 + ) dG (u, v ) + 8 log n otherwise. 8
-1

log n

Note that if the distance to be approximated is d =  (log n), then the distance in the spanner is at most (1 + ) · d + o(d). Also, by choosing = o(1), Lemma 9 implies the construction in log1+o(1) n time of a 3-spanner with O(n3/2 ) edges which is also a (1 + o(1), 8 log n)-spanner. In order to obtain a better additive stretch, we use an MIS algorithm at the price of increasing the time complexity. In fact, it is also well-known that a MIS can be constructed by a deterministic (resp. randomized) algorithm in  O(1/ log n) (resp. O(log n) expected) time. Thus by taking  = 1 and  = n (MIS(n)), one can prove: Lemma 10. There  exists a distributed algorithm that given an n-node graph G, constructs in nO(1/ log n) deterministic time a spanner S with O(n3/2 ) edges and stretch (,  ) as given by Table 1.
Table 1. Stretches (,  ) for distances d dG (u, v ) dS (u, v ) (,  ) 1 3 (2, 1) 2 6 (2, 2) 3 7 (2, 1) 4 8 (2, 0) 5  9 (1.8, 0) d > nO (1/ log n) (1 + o(1)) d (1 + o(1), 0) d nO (1/
 log n)

d+4

(1, 4)

Deterministic Distributed Construction of Linear Stretch Spanners

191

We observe that the spanner constructed in Lemma 10 has stretch at most (2, 1) except for nodes at distance 2. Combining previous lemmas we obtain: Theorem 3. There exists a distributed algorithm that given an n-node graph G  and a parameter > 0 constructs in nO(1/ log n) + O( -1 ) (resp. O( -1 log n)) deterministic time a 3-spanner with O(n3/2 ) edges which is also a (1 + , 4)spanner (resp. a (1 + , 8 log n)-spanner).

6

Open Problems

While it is well-known that every graph has a (1, 2)-spanner with O(n3/2 ) edges, we leave open the problem to find (distributively or not), for each k > 2, a (1, f (k ))-spanner of size O(n1+1/k ) where f is a polynomial (or even an exponential) function.

References
1. Awerbuch, B.: Complexity of network synchronization. J. ACM 32, 804­823 (1985) 2. Awerbuch, B., Berger, B., Cowen, L.J., Peleg, D.: Near-linear cost sequential and distributed constructions of sparse neighborhood coverss. In: 34th IEEE Symp. on Foundations of Computer Science, pp. 638­647. IEEE Computer Society Press, Los Alamitos (1993) 3. Awerbuch, B., Berger, B., Cowen, L.J., Peleg, D.: Fast distributed network decompositions and covers. J. Parallel and Distributed Computing 39, 105­114 (1996) 4. Awerbuch, B., Berger, B., Cowen, L.J., Peleg, D.: Near-linear time construction of sparse neighbourhood covers. SIAM J. on Computing 28, 263­277 (1998) 5. Awerbuch, B., Goldberg, A., Luby, M., Plotkin, S.: Network decomposition and locality in distributed computation. In: Proc. 30th IEEE Symp. on Foundations of Computer Science, pp. 364­369. IEEE Computer Society Press, Los Alamitos (1989) 6. Awerbuch, B., Peleg, D.: Sparse partitions. In: 31th IEEE Symp. on Foundations of Computer Science, pp. 503­513. IEEE Computer Society Press, Los Alamitos (1990) 7. Baswana, S., Kavitha, T., Mehlhorn, K., Pettie, S.: New constructions of (,  )spanners and purely additive spanners. In: 16th ACM-SIAM Symp. on Discrete Algorithms, pp. 672­681. ACM Press, New York (2005) ~ (n2 ) 8. Baswana, S., Sen, S.: Approximate distance oracles for unweighted graphs in O time. In: 15th ACM-SIAM Symp. on Discrete Algorithms, pp. 271­280. ACM Press, New York (2004) 9. Baswana, S., Sen, S.: A simple and linear time randomized algorithm for computing sparse spanners in weighted graphs. Random Structures and Algorithms 30, 532­ 563 (2007) 10. Cohen, E.: Fast algorithms for constructing t-spanners and paths with stretch t. SIAM J. on Computing 28, 210­236 (1998) 11. Cowen, L.J.: Compact routing with minimum stretch. J. Algorithms 38, 170­183 (2001)

192

B. Derbel, C. Gavoille, and D. Peleg

12. Derbel, B., Gavoille, C.: Fast deterministic distributed algorithms for sparse spanners. In: Flocchini, P., G¸ asieniec, L. (eds.) SIROCCO 2006. LNCS, vol. 4056, pp. 100­114. Springer, Heidelberg (2006) 13. Derbel, B., Mosbah, M., Zemmari, A.: Fast distributed graph partition and application. In: 20th IEEE Int. Parallel and Distributed Processing Symp., IEEE Computer Society Press, Los Alamitos (2006) 14. Dor, D., Halperin, S., Zwick, U.: All pairs almost shortest paths. SIAM J. Computing 29, 1740­1759 (2000) 15. Dubhashi, D., Mai, A., Panconesi, A., Radhakrishnan, J., Srinivasan, A.: Fast distributed algorithms for (weakly) connected dominating sets and linear-size skeletons. J. of Computer and System Sciences 71, 467­479 (2005) 16. Eilam, T., Gavoille, C., Peleg, D.: Compact routing schemes with low stretch factor. J. Algorithms 46, 97­114 (2003) 17. Elkin, M.: Computing almost shortest paths. In: 20th ACM Symp. on Principles of Distributed Computing, pp. 53­62. ACM Press, New York (2001) 18. Elkin, M., Peleg, D.: (1 + ,  )-spanner constructions for general graphs. SIAM J. on Computing 33, 608­631 (2004) 19. Elkin, M., Zhang, J.: Efficient algorithms for constructing (1 + ,  )-spanners in the distributed and streaming models. In: 23rd ACM Symp. on Principles of Distributed Computing, pp. 160­168. ACM Press, New York (2004) 20. Gavoille, C., Peleg, D., P´ erenn` es, S., Raz, R.: Distance labeling in graphs. J. Algorithms 53, 85­112 (2004) 21. Moran, S., Snir, S.: Simple and efficient network decomposition and synchronization. Theoretical Computer Science 243, 217­241 (2000) 22. Panconesi, A., Srinivasan, A.: On the complexity of distributed network decomposition. J. Algorithms 20, 356­374 (1996) 23. Peleg, D.: Distributed Computing: A Locality-Sensitive Approach. SIAM Monographs on Discrete Mathematics and Applications (2000) 24. Peleg, D., Ullman, J.D.: An optimal synchornizer for the hypercube. SIAM J. Computing 18, 740­747 (1989) 25. Peleg, D., Upfal, E.: A trade-off between space and efficiency for routing tables. J. ACM 36, 510­530 (1989) 26. Pettie, S.: Low distortion spanners. In: 34th International Colloquium on Automata, Languages and Programming. LNCS, vol. 4596. Springer, Heidelberg (2007) 27. Roditty, L., Thorup, M., Zwick, U.: Roundtrip spanners and roundtrip routing in directed graphs. In: 13th ACM-SIAM Symp. on Discrete Algorithms, pp. 844­851. ACM Press, New York (2002) 28. Roditty, L., Thorup, M., Zwick, U.: Deterministic constructions of approximate distance oracles and spanners. In: Caires, L., Italiano, G.F., Monteiro, L., Palamidessi, C., Yung, M. (eds.) ICALP 2005. LNCS, vol. 3580, pp. 261­272. Springer, Heidelberg (2005) 29. Thorup, M., Zwick, U.: Compact routing schemes. In: 13th ACM Symp. on Parallel Algorithms and Architectures, pp. 1­10. ACM Press, New York (2001) 30. Thorup, M., Zwick, U.: Approximate distance oracles. J. ACM 52, 1­24 (2005) 31. Thorup, M., Zwick, U.: Spanners and emulators with sublinear distance errors. In: 17th ACM-SIAM Symp. on Discrete Algorithm, pp. 802­809. ACM Press, New York (2006) 32. Wenger, R.: Extremal graphs with no C 4 's, C 6 's, or C 10 's. J. Combinatorial Theory, Series B 52, 113­116 (1991)

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks
Danny Dolev and Ezra N. Hoch
School of Engineering and Computer Science, The Hebrew University of Jerusalem, Israel, {dolev,ezraho}@cs.huji.ac.il

Abstract. Consider a distributed network of n nodes that is connected to a global source of "beats". All nodes receive the "beats" simultaneously, and operate in lock-step. A scheme that produces a "pulse" every Cycle beats is shown. That is, the nodes agree on "special beats", which are spaced Cycle beats apart. Given such a scheme, a clock synchronization algorithm is built. The "pulsing" scheme is self-stabilized despite any Byzantine transient faults and the continuous presence of up to f < n 3 nodes. Therefore, the clock synchronization built on top of the "pulse" is highly fault tolerant. In addition, a highly fault tolerant general stabilizer algorithm is constructed on top of the "pulse" mechanism. Previous clock synchronization solutions, operating in the exact same and converge in linear time, model as this one, either support f < n 4 and have exponential convergence time that also or support f < n 3 depends on the value of max-clock (the clock wrap around value). The proposed scheme combines the best of both worlds: it converges in linear time that is independent of max-clock and is tolerant to up to f < n 3 Byzantine nodes. Moreover, considering problems in a self-stabilizing, Byzantine tolerant environment that require nodes to know the global state (clock synchronization, token circulation, agreement, etc.), the work presented here is the first protocol to operate in a network that is not fully connected.

1

Introduction

Most distributed tasks require some sort of synchronization. Clock synchronization is a very basic and intuitive tool for supplying this. pulse synchronization can be used as an underlying building block to achieve clock synchronization, as well as solving other synchronization problems; in a sense, pulse synchronization is a more fundamental synchronization problem. It thus makes sense to require an underlying pulse synchronization mechanism to be highly fault-tolerant. This paper presents a pulse synchronization algorithm that is self-stabilizing and is tolerant to permanent presence of
Part of the work was done while the author visited Cornell university. The work was funded in part by ISF, ISOC, NSF, CCR, and AFOSR.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 193­207, 2007. c Springer-Verlag Berlin Heidelberg 2007

194

D. Dolev and E.N. Hoch

Byzantine faults. That is, it attains synchronization, once lost, while containing the influence of the permanent presence of faulty nodes. Consider a system in which the nodes execute in lock-step by regularly receiving a common "pulse" or "tick" or "beat". The objective is to agree on some "special beats" that are Cycle beats apart. We will use the "beat" notation for the "global" signal received, and "pulse" for the "special beats" agreed upon. The pulse synchronization problem is to ensure that eventually all correct nodes pulse together, and as long as enough nodes remain correct, they continue to pulse together, Cycle beats apart. For example, given Cycle = 7 we would like all correct nodes, that may start at arbitrary initial states, to eventually pulse together every 7 beats, and continue so as long as there are enough correct nodes. The global beat system provides some measure of synchronization. For example, given a global beat system with beat interval at least as long as the worst-case execution-time for terminating Byzantine agreement, the pulse synchronization problem is solved by initiating a Byzantine agreement on the next time when the nodes should pulse, each time a beat is received. The crux of the problem is to achieve synchronization when it is not given by the global beat system; that is, when the beat interval length is in the order of the communication's end-to-end delay. Since in that scenario the global beat system does not provide - by itself - enough synchronization, and a more complex algorithm is required to exert the required synchronization. The main contribution of the current paper is achieving exactly that. Related Work: pulseing has been used as an underlying fault tolerant mechanism in clock synchronization, token circulation and to create a general stabilizer (see [4] for an overview). All of these algorithms are self-stabilizing and Byzantine tolerant, due to the fault tolerant nature of the underlying pulse mechanism. This gives the motivation for producing robust and efficient pulseing algorithms, as they can be used to improve the robustness of a variety of applications. Clock synchronization is one of the first problems that was solved in a selfstabilizing and Byzantine tolerant fashion. In [9] and [12] it was solved directly, and in [4] it was solved using an underlying pulseing algorithm. [9] was the first work to discuss the exact same model as presented here, as opposed to [4], which operates without a global beat system. Synchronization of clocks of integer values was previously termed digital clock synchronization ([2,7,8,16]) or "synchronization of phase-clocks" ([11]). However, in this paper we concentrate on the pulseing mechanism, as it yields clock synchronization as well as other fault tolerant protocols. Several fault tolerant stabilizers exist (see [1], [10] and [13]) with varying requirement and features (such as local containment of faults). In [3], it was shown that pulse synchronization can be used to create a generalized stabilizer. However, in [3] the stabilizer is complex, and can stabilize a narrow class of algorithms. In Section 9 we show a simpler stabilizer, which can stabilize a wider range of algorithms.

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks

195

Some of the previous results combining Byzantine faults and self-stabilization consider a class of problems in which the state of each correct node is determined locally. Usually such solutions can operate in a general graph (see [17], [15] and [14]) without the need to aggregate or accumulate information across the network. In the class of problems in which the state of each correct node is correlated with the state of the other correct nodes, the current paper is the first paper to present a solution that operates in a network that is not fully connected. Contributions: We construct a self stabilizing pulseing algorithm, that is tolerant to up to f < n 3 Byzantine nodes, and converges in linear time, for any target interval of pulsing. As will be shown in Section 8, clock synchronization and pulse synchronization are equivalent. Hence, this work is compared to the state of the art of previous clock synchronization results that operate in the exact same model. Previous results have either linear convergence time with f < n 4 (see [12]) or exponential convergence time with f < n (see [9]). In this paper we obtain a 3 linear convergence time with f < n . Moreover, our convergence time is indepen3 dent of the max-clock value (the clock wrap around value) of the digital clock, in contrast to [9]. In addition, our algorithm is the first one in this model and for this type of problems that does not require each node to be connected to every other node, it only requires that there are 2 · f + 1 distinct routes between any two correct nodes, matching the lower bound of [5].

2

Model and Definitions

Consider a fully connected network of n nodes (we later generalize the results to a more general network). All the nodes are assumed to have access to a "global beat system" that provides "beats" with regular intervals. The communication network and all the nodes may be subject to severe transient failures, which might leave the system in an arbitrary state. We say that a node is Byzantine if it does not follow the instructed algorithm and non-Byzantine otherwise. Thus, any node whose failure does not allow it to exactly follow the algorithm as instructed is considered Byzantine , even if it does not behave fully maliciously. A non-Byzantine node will be called nonfaulty. In the following discussion f will denote the upper bound on the number of Byzantine nodes.1 The presented solution supports f < n 3. Definition 1. The system is coherent if there are at most f Byzantine nodes, and each message sent at a beat to a non-faulty destination arrives and is processed at its destination before the next beat. Nodes are instructed to send their messages immediately after the occurrence of a beat from the global beat system. Therefore, when the system is coherent
1

In the literature the term "permanent" Byzantine node is sometimes used.

196

D. Dolev and E.N. Hoch

message delivery and the processing involved can be completed between two consecutive global beats, by any node that is non-faulty. More specifically, the time required for message delivery and message processing is called a round, and we assume that the time interval between global beats is greater than and in the order of such a round. Due to transient faults, different nodes might not agree on the current beat/round number. We will use the notion of an external beat number r, which the nodes are not aware of, but will simplify the proofs' presentation and discussion. At times of transient failures there can be any number of concurrent Byzantine faulty nodes; the turnover rate between faulty and non-faulty behavior of nodes can be arbitrarily large and the communication network may also behave arbitrarily. Eventually the system behaves coherently again. At such case a nonfaulty node may still find itself in an arbitrary state. Since a non-faulty node may find itself in an arbitrary state, there should be some time of continues non-faulty operation before it can be considered correct. Definition 2. A non-faulty node is considered correct only if it remains nonfaulty for node rounds during which the system is coherent.2 The algorithm parameters n, f, as well as the node's id are fixed constants and thus are considered part of the incorruptible correct code at the node. Thus, it is assumed that non-faulty nodes do not hold arbitrary values of these constants. 2.1 The pulseing Problem

We say that a system is [,  ]-pulsing if all correct nodes pulse together in the following pattern:  consecutive beats of pulses followed by  consecutive beats of non-pulse. That is, the system has a Cycle of length  +  beats, out of which only the first  beats are pulses. More formally, denote by pulsedp (r) = T rue if p pulsed on beat r and pulsedp (r) = F alse, otherwise. Definition 3. A system is [,  ]-pulsing in the beat interval [r1 , r2 ] if there exists some 0  k <  +  , such that for every correct node p, and for every beat r  [r1 , r2 ], it holds that: 1. pulsedp (r) = T rue, in case 0  r - k (mod  +  ) < ; and 2. pulsedp (r) = F alse in case   r - k (mod  +  ) <  +  . (k denotes the offset, from r1 , of the first pulse in the pattern.) For example, consider "1" to represent a beat in which all correct nodes pulse, and "0" a beat in which all correct nodes do not pulse. Using this notation, the following is a pulseing pattern of a [,  ]-pulsing system.  beats  beats  beats  beats [. . . , 1, 1, . . . , 1, 0, 0, . . . , 0, 1, 1, . . . , 1, 0, 0, . . . , 0, . . .] Cycle beats
2

Cycle beats

The assumed bound on the value of node is defined in Remark 3.

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks

197

Definition 4. The pulseing problem: Convergence: Starting from an arbitrary state, the system becomes [,  ]-pulsing after a finite number of beats. Closure: If the system is [,  ]-pulsing in the beat interval [r1 , r2 ] it is also [,  ]-pulsing in the interval [r1 , r2 + 1]. Definition 5. a [,  ]-pulser is an algorithm A, such that once the system is coherent (and stays so), it solves the pulseing problem. The objective is to develop an algorithm that pulses only once every Cycle. Notation: We denote a [1,  ]-pulser as [ + 1]-pulser. Using the previously used notation of "1" for pulseing and "0" for nonpulseing, a [Cycle]-pulser looks as follows: [. . . , 1, 0, 0, . . . , 0, 1, 0, 0, . . . , 0, . . .] Cycle beats Cycle beats The goal is to build a [Cycle]-pulser for any Cycle > 0. That is, a selfstabilizing, Byzantine tolerant algorithm that eventually pulses every Cycle beats. The following section outlines the solution.

3

Constructing a [Cycle]-pulser

In contrast to previous solutions that were very involved, the new solution presented below is more modular. Addressing the problem in a modular way enabled us to unwrap the difficulties in solving the problem, and to come up with a tight solution. Its modularity also enables to simplify the proof of correctness and to better present the intuition behind it. The core of the protocol is the Large-Cycle-Pulser algorithm that produces a [,  + Cycle ]-pulser. This module uses another module called BBB to limit the ability of the Byzantine nodes to disrupt the protocol. To obtain the complete solution the core protocol is wrapped with two additional modules, as detailed below. We first show how to construct a [,  + Cycle ]-pulser A for any Cycle > , where  is a bound on running a given distributed agreement protocol. We continue by showing how to construct a [ +  ]-pulser from any [,  ]-pulser. Using this, we construct a [2 ·  + Cycle ]-pulser A for any Cycle > . Lastly, using A , we construct a [Cycle]-pulser for any Cycle  1. Remark 1. Note that [ +  ]-pulser is actually [1,  +  - 1]-pulser, and hence a [,  ]-pulser (pulses for  beats then it is quiet for  beats) is transformed into a [1,  +  - 1]-pulser (pulses once, then it is quiet for  +  - 1 beats). The construction of A uses a building block that is essentially a Byzantine consensus. We denote this building block by BBB (Byzantine Black Box).

198

D. Dolev and E.N. Hoch

3.1

The Byzantine Black Box Construction

BBB is defined to be a round based distributed protocol, such that each node p has a binary input value vp and a binary output value Vp . BBB has the following properties: 1. Termination : The algorithm terminates within  rounds. 2. Agreement : All non-faulty3 nodes agree on the same output value V . That is, for any two non-faulty nodes p, p it holds that Vp = Vp = V . 3. Validity : If n - f non-faulty nodes have the same input value  then that is the output value, V =  . BBB is required to be Byzantine tolerant, but is not required to be selfstabilizing. The self-stabilization of the [ +  ]-pulser A (presented later) will not be hampered by this.4 In addition, A will rely only on the properties of BBB (when it is executed by enough correct nodes) for its operation. In A all messages exchanged among the nodes will use BBB. Since the presented BBB can tolerate f < n 3 faulty nodes, A can tolerate the same failure ratio. Remark 2. BBB can be implemented via any algorithm that solves the Byzantine consensus problem; the only difference lies in the "validity" condition, where instead of limiting the validity to the case that "all correct nodes" start with the same initial value, BBB limits the validity condition to having only n - f non-faulty nodes with the same initial value, even if there happen to be more non-faulty nodes at that instance. Remark 3. A non-faulty node that has recently recovered from a transient failure cannot immediately be considered correct. In the context of this paper, a nonfaulty node is considered correct once it remains non-faulty for at least node =  + 1, and as long as it continues to be non-faulty. 3.2 A [,  + Cycle ]-pulser

Figure 1 presents an algorithm that produces a [,  + Cycle ]-pulser, for Cycle > . This algorithm executes  simultaneous BBB protocols. Consider BBBi as a "pointer" to a BBB instance, hence the statement BBB2 := BBB1 ; BBB1 := new BBB("1"); means that BBB2 will contain the previous instance of BBB1 , and BBB1 will contain a new instance of BBB initialized with the input value 1. The output value of BBBi is V (BBBi ).
3 4

In the context of BBB , a node is considered non-faulty only if it is non-faulty throughout the whole execution of BBB . BBB is initiated, executed and terminated repeatedly; each instance starts with a "clean slate", thus not harming the self-stability of the algorithm that uses it.

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks

199

Algorithm Large-Cycle-Pulser

/* executed repeatedly at each beat */

1. for each i  {1, .., } do execute the ith round of the BBB i protocol; 2. (a) if Counter > 0 then Counter := min{Counter - 1, Cycle }; W antT oP ulse := 0; (b) else W antT oP ulse := 1; 3. if V (BBB  ) = 1 then (a) do pulse; (b) Counter := Cycle ; 4. for each i  {2, ..., } do BBB i := BBB i-1 ; 5. initialize a new instance of BBB , BBB 1 = BBB (W antT oP ulse). Fig. 1. A [,  + Cycle ]-pulser algorithm for Cycle > 

4

Proof of Large-Cycle-Pulser's Correctness

All the lemmata, theorems, corollaries and definitions hold only as long as the system is coherent. We assume that nodes may start in an arbitrary state, and nodes may fail and recover, but from some time on, at any round there are at least n - f correct nodes. Let G denote a group of non-Byzantine nodes that behave according to the algorithm, and that are not subject to (for some pre-specified number of rounds) any new transient failures. We will prove that if |G|  n - f and all of these nodes remain non-faulty for a long enough period of time ( () global beats), then the system will converge. For simplifying the notations, the proofs refer to some "external" beat number. The nodes do not maintain it and have no access to it; it is only used for the proofs' clarity. Definition 6. A group G is Correct(,  ) if |G|  n - f , and every node p  G is correct during the beat interval [,  ]. Let  mark the length of the interval, that is  =  -  + 1. Note that each node p  G , when G is Correct(,  ), has not been subject to a transient failure in the beat interval [ - node ,  ]; and is non-faulty during that interval. Definition 7. We say that a system is Correct(,  ) if there exists a set G such that G is Correct(,  ). In the following lemmata, G refers to any set implied by Correct(,  ), without stating so specifically. The proofs hold for any such set G . Note that if the system is coherent, and there has not been a transient failure for at least  + 1 beats, then, by definition, G contains all nodes that were non-faulty during that period.

200

D. Dolev and E.N. Hoch

Lemma 1.   : If the system is Correct(,  ) then at any beat r  [,  ], either all nodes in G pulse or they all do not pulse. Proof. A node pulses only in Line 3.a, which is executed only when the value of V (BBB ) = 1. All nodes in G have not been subject to transient failures in the node =  + 1 beats preceding r. Therefore, BBB has been initialized properly  beats ago, and during the  rounds of BBB 's execution, it has been executed properly by at least n - f nodes. Hence, according to Agreement of BBB, all nodes in G have the same value of V (BBB ). Therefore, all nodes in G "act the same" when considering Line 3.a: either all of them execute Line 3.a or they all do not execute it. This holds for any beat after  (as long as G continues to contain n - f correct nodes). Therefore, at any such beat r  [,  ], either all nodes in G pulse or they all do not pulse. Lemma 2.    +  + Cycle : If the system is Correct(,  ), then at some beat r  [,  ] all nodes in G pulse. Proof. According to the previous lemma, all nodes in G pulse together during the interval [,  ]. Hence, if one of them pulsed in the interval [,  + Cycle ], all of them pulsed, proving the claim. Otherwise, consider the case where no node in G has pulsed in the interval [,  + Cycle ]. Hence, at beat  + Cycle , for all the nodes in G , the Counter variable has decreased to 0 or is negative. This is because Counter is bounded from above by Cycle (which is a fixed parameter of the protocol and is identical at all nodes); and as long as it holds a positive value, it decreases by 1 during each beat of the interval [,  + Cycle ] (since no node pulses in that interval, Counter never increases). Since the interval is at least Cycle beats long, the value of Counter is less than (or equal to) 0. Therefore, at beat  + Cycle there are |G|  n - f correct nodes with W antT oP ulse = 1. Therefore, according to Validity of BBB,  beats afterwards V (BBB ) will output 1, and all nodes in G will pulse. Thus, in the interval [,  +  + Cycle ] all nodes in G pulse. Therefore, the claim holds for any beat interval [,  ], where    +  + Cycle ]. Remark 4. The above lemma proves p rogress. That is, starting from any state, eventually there will be a pulse. Consider a system that is Correct(,  ) (for    +  + Cycle ), from Lemma 1, starting from beat  all nodes in G pulse together. From Lemma 2, by beat  +  + Cycle all nodes in G have pulsed. Therefore, by that round they have all reset their Counter values at the same beat. Since W antT oP ulse depends solely on the value of Counter, and since all nodes in G agree on the output value of the BBB protocols, all nodes in G perform exactly the same lines of code following each beat in the beat interval [ +  + Cycle ,  ]. Lemma 3.    + 3 ·  + 2 · Cycle : If the system is Correct(,  ), then the system is [,  + Cycle ]-pulsing in the beat interval [ + 3 ·  + 2 · Cycle ,  ].

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks

201

Proof. According to previous lemmas, all correct nodes pulse at some beat  , no later than beat  +  + Cycle ; and from then on they all pulse together. At beat  they all reset their counters and will have positive Counter values for at least Cycle rounds. Since Cycle > , in the following  beats, the value of W antT oP ulse will be 0, and hence BBB1 is initialized during these beats with the value 0. Therefore, once these values will emerge from BBB, there will be a period of Cycle >  with no pulses. That "quiet" period will start at beat  + . This quiet period might be longer than Cycle , if there were other pulses during the beat interval [,  + ]. In any case, a quiet period will commence at beat  +  and will be at least Cycle beats long, and no more than Cycle +  beats long. Now consider what happens after this quiet period. Eventually, the value of W antT oP ulse will be set to 1 (after no more than Cycle +  beats), and will stay so until the next pulse. Mark the beat at which all nodes in G set W antT oP ulse to 1 as  . Notice that because the quiet period is greater than , then once its values start emerging of BBB there will be a quiet period for at least  beats. Hence, once W antT oP ulse is set to 1, it will stay that way for  beats, until 1 comes out of BBB . This will happen at beat  + . Once this happens, there are  1's "on the way" in the coming BBBs. Therefore, there will be a pulse for  beats. Due to the first pulse, W antT oP ulse will be 0 for all the  pulse beats. After the last pulse beat, W antT oP ulse will be 0 for an additional Cycle beats. Afterwards, W antT oP ulse will turn to 1, and will stay so for  beats. Thus there is a pattern of W antT oP ulse being 0 for  + Cycle beats then being 1 for  beats, and so on. Therefore, the pulseing pattern will satisfy the requirement. Note that the pulseing pattern starts on beat  + , and the pattern continues (at least) until beat  . Hence, the system is [,  + Cycle ]-pulsing in the beat interval [ +,  ]. Because    +Cycle + and since    ++Cycle , we conclude that  +    + 3 ·  + 2 · Cycle , as required. Remark 5. The above lemma shows that the convergence time of the pulseing algorithm depends on the value of Cycle. However, since for clock synchronization the value of Cycle is in the order of , the convergence of the clock synchronization will depend on  and not on the value of max-clock (the wrap around value of the digital clock). The following theorem states that we have constructed a [,  + Cycle ]-pulser. Theorem 1. The Large-Cycle-Pulser algorithm is a [,  + Cycle ]-pulser. Proof. By Lemma 3, once there are enough nodes that have not been subject to transient failures for 3 ·  + 2 · Cycle beats, the system becomes [,  + Cycle ]pulsing for the beat interval [ + ,  ] . This is true for any    + 3 ·  + 2 · Cycle . Hence, as long as the system is coherent, once the system is [,  + Cycle ]-pulsing in the beat interval [ + ,  ], it is also [,  + Cycle ]pulsing in the beat interval [ + ,  + 1]; and therefore Large-Cycle-Pulser algorithm is a [,  + Cycle ]-pulser.

202

D. Dolev and E.N. Hoch

5

A [Cycle]-pulser for Cycle > 0

In the previous section a [,  + Cycle ]-pulser was presented, for any value of Cycle > . Now a general way to transform a [,  ]-pulser into a [ +  ]pulser is given. Combining this with the previous result produces a [2 ·  + Cycle ]-pulser. Since Cycle > , this technique constructs a [Cycle]-pulser, for any Cycle > 3 · . In Subsection 5.2 this requirement is eliminated, and the objective of building [Cycle]-pulser is achieved for any Cycle > 0. 5.1 [,  ]-pulser to [ +  ]-pulser

Given a [,  ]-pulser A, the algorithm B in Figure 2 uses A as a black-box: Note that the above algorithm B does not rely on anything other than the output of A in the current and previous beats. Hence, if A is self-stabilizing, so is B .
Algorithm [ +  ]-pulser /* executed repeatedly at each beat */

1. execute a single round of A; 2. if A pulsed at the current beat and A did not pulse at the previous beat, then B pulses at the current beat. Fig. 2. An algorithm that transforms a [,  ]-pulser into a [ +  ]-pulser

Theorem 2. The algorithm B is a [ +  ]-pulser. Proof. A is a [,  ]-pulser, hence, it pulses in a pattern of  pulses, then  quiet rounds. Therefore, once every  +  beats, there is a transition from not pulseing to pulseing. Thus, the pulseing output of A, implies that exactly once every  +  beats it holds that A pulsed at the current beat, and did not pulse at the previous beat. This is continuously true (as long as A continues to pulse), which implies that the proposed algorithm B will pulse exactly once every  +  beats, in a pattern of a single pulse, and then  +  - 1 beats of quiet rounds. Since A is a [,  ]-pulser, starting from an arbitrary state, it eventually starts pulseing in the required pattern, and continues so as long as the system is coherent. Hence, the above algorithm B will eventually start pulseing in the expected pattern, and will continue so as long as the system is coherent. Hence it is a [ +  ]-pulser. 5.2 Case Cycle  3 · 

Building upon the [2 ·  + Cycle ]-pulser, B , from the previous subsection, a [Cycle]-pulser, C , for any Cycle  3 ·  is presented in Figure 3. Theorem 3. The algorithm C is a [Cycle]-pulser for any 1  Cycle  3 · .

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks

203

Algorithm [Cycle  3 · ]-pulser

/* executed repeatedly at each beat */

/* set Cycle >  to be such that Cycle + 2 ·  is divisible by Cycle */ 1. execute B; 2. if B pulsed at the current beat then Counter := Cycle + 2 · ; 3. if Counter is divisible by Cycle then C pulses at the current beat; 4. Counter := Counter - 1. Fig. 3. A [Cycle]-pulser algorithm for 1  Cycle  3 · 

Proof. Since B is a [Cycle + 2 · ]-pulser, starting from an arbitrary state, eventually it starts pulseing in a pattern of a single pulse, and then Cycle + 2 ·  - 1 beats of quiet rounds (and continues so as long as the system is coherent). Therefore, eventually, all correct nodes will see the same pulseing output from B . Hence, each time B pulses, all correct nodes set Counter to Cycle + 2 · , and have the same value of Counter at each beat (because they all set it together, and decrease it together). Thus, each time a correct node enters Line 3, all correct nodes do the same. Therefore, all correct nodes have C pulse together. Lastly, since each Cycle beats Counter will be divisible by Cycle, C pulses once every Cycle beats. +Cycle pulses of C . Due to the Therefore, for each pulse of B we have 2· Cycle choice of Cycle such that 2 ·  + Cycle is divisible by Cycle, the pulses are nicely aligned with the pulses of B ; and therefore, the above algorithm C is a [Cycle]-pulser. Theorem 4. For any Cycle > 0, a [Cycle]-pulser can be constructed. Proof. If Cycle > 3 · , then set Cycle := Cycle - 2 · . By Theorem 1, construct a [,  + Cycle ]-pulser and by Theorem 2 construct a [2 ·  + Cycle ]-pulser. According to the choice of Cycle the required [Cycle]-pulser is constructed.
+Cycle If Cycle  3 · , calculate Cycle such that Cycle >  and 2· is Cycle an integer number. Now, by Theorem 1 build a [,  + Cycle ]-pulser. From Theorem 2 construct a [2 ·  + Cycle ]-pulser. Finally, from the above algorithm construct an algorithm that is a [Cycle]-pulser, as required.

6

Network Connectivity

The above discussion did not assume anything about the network connectivity. More precisely, the only connectivity assumption was about the behavior of the BBB protocol. That is, whatever connectivity BBB requires to operate properly, is the required connectivity in order for the [Cycle]-pulser construction to work properly.

204

D. Dolev and E.N. Hoch

In [5] it is shown that Byzantine agreement is achievable if and only if: 1. f is less than one-third of the total number of nodes in the system. 2. f is less than one-half of the connectivity of the system (that is, between any two nodes there are at least 2 · f + 1 distinct paths). These lower bounds clearly hold for Byzantine consensus. Therefore, since BBB is implemented by executing Byzantine Consensus for each node's input 1 value, BBB can be tolerant to up to n- 3 Byzantine faults. In addition BBB can work properly even if the connectivity graph is not fully connected, but rather there are at least 2 · f + 1 distinct paths between any two non-faulty nodes. Remark 6. As noted in [5], the nodes are required to know the connectivity graph while executing the algorithm. This implies, due to self-stabilization, that each node has the network connectivity as incorruptible data.5 Since the pulseing algorithm presented in this paper depends solely on BBB 1 for communication with other nodes, it is tolerant up to n- Byzantine faults 3 and can operate in a network where there are at least 2 · f + 1 distinct paths between any two nodes, and it is optimal with respect to these two parameters. Previous synchronization algorithms do not easily extend to operate in a network that is not fully connected. This is a result of the dependency of their "current state" on messages received in the "current round"; in a network that is not fully connected, such messages are received D rounds later, where D is the diameter of the network. For example, in [12], the DigiClock value depends on the values sent in the current round. Therefore, if the network is not fully connected, node p does not receive messages from node p that is not his neighbor, in the same round. Hence, p cannot change its current state according to the algorithm's definition. This does not mean that previous algorithms cannot be transformed to operate in such a setting, just that it is not straightforward.

7

Complexity Analysis

Using pulseing for clock synchronization leads using a Cycle that is in the order of . Hence, the pulseing algorithm presented in the previous sections converges in O() beats. If the system is fully connected, then  = 2f + 3, because efficient implementations of Byzantine consensus require about 2f + 3 rounds. Therefore, convergence is reached in O(f ) beats. If the system is not fully connected, as discussed in the previous section, and the diameter is D, then  = D · 2 · (f + 1). Therefore, convergence is reached in O(D · f ) beats.
5

One can somewhat relax this assumption, but then either a flooding algorithm needs to be used, or one needs to come up with an algorithm that finds enough independent paths on the fly - despite the Byzantine behavior; we are not aware of any selfstabilizing algorithm to do that.

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks

205

Considering message complexity, at each beat  BBBs are executed simultaneously. Since BBB can be implemented via Byzantine consensus (see Remark 2), it requires n2 messages at each beat. Hence we have that the message complexity at each beat is O(f · n2 ). Note that one can use early stopping agreements. Such agreements will use less messages, if the number of faults is small, but will still take the same worse case time.

8

The Digital Clock Synchronization Problem

In the digital clock synchronization problem, each node has a variable DigiClock , and the objective is to have all correct nodes agree on the value of DigiClock and increase it by one at each beat. A more detailed discussion of this problem (along with a solution) is given in [12]. The digital clock synchronization problem is equivalent to the pulseing problem. Given an algorithm that solves the digital clock synchronization problem, simply pulse every time the DigiClock variable is divisible by Cycle. This produces a [Cycle]-pulser algorithm. The other direction is a bit more complicated. Given a [f + 2]-pulser algorithm, every pulse execute a Byzantine agreement on what the DigiClock value will be in the next pulse. In addition, each beat DigiClock is increased by 1, and when the Byzantine agreement terminates, the DigiClock is set to the agreement value (similar to [6]). This way, all nodes agree on the value of DigiClock and increase it by one at each beat. Note that the digital clock synchronization problem has been solved directly in [12] for f < n 4 and assuming a fully connected graph. Due to the equivalence to the pulseing problem, a digital clock synchronization algorithm can be built with an underlying pulseing algorithm presented in this paper, which supports f<n 3 and assumes only that there are 2 · f + 1 distinct paths between any two nodes. That produces a digital clock synchronization algorithm that is optimal in these two aspects.

9

Byzantine Tolerant Stabilizer

We now present briefly how a stabilizer can be built using the pulseing algorithm provided in the above sections. The stabilizer will stabilize a Byzantine tolerant algorithm A0 . That is, given a Byzantine tolerant algorithm A0 that is not selfstabilizing, the stabilizer will transform it into a self stabilizing version of A0 (preserving the Byzantine tolerance). Clearly, not all algorithms can be viewed as self-stabilizing. E.g. an algorithm that is allowed to do some action Act only once, cannot be a self-stabilizing algorithm. We do not discuss here the requirements of an algorithm A0 so that it can be stabilized. For a more in depth discussion of such requirements, refer to [10] and [13]. In the following, it is assumed that the Byzantine tolerant algorithm A0 has a meaning as a self-stabilizing algorithm.

206

D. Dolev and E.N. Hoch

Intuitively, every so often, all nodes will collect a global snapshot S of the local states of all nodes. Then, all nodes inspect S for any inconsistencies. If any are found, all nodes reset their local state to some consistent state. Given a general Byzantine tolerant algorithm A0 , we construct an algorithm Byz-State-Check. Byz-State-Check gathers a global snapshot of the local states at each node and ensures that the local states are consistent. In addition if the states were consistent to start with, then Byz-State-Check does not alter them. That is, Byz-State-Check alters the local states to a consistent state, only if required. Figure 4 presents the algorithm Byz-State-Check.
Algorithm Byz-State-Check /* executed at node p*/

1. execute a Byzantine agreement on local state of A0 ; 2. agree beats after the the beginning of the execution of line 1: (a) if S represents a legal state repair local state if it is inconsistent with S ; (b) otherwise reset local state. Fig. 4. A Byzantine tolerant state validation and reset

Remark 7. agree is an upper bound on the number of rounds it takes to execute Byzantine agreement (2f +3 for a typical efficient implementation). Since all correct nodes wait agree beats from entering line 1 until entering line 2, it is ensured that all correct nodes enter line 2 after they see the same global snapshot S . Given a general Byzantine tolerant algorithm A0 , a [agree + 1]-pulser P and a Byz-State-Check algorithm C , the algorithm SS-Byz-Stabilizer is constructed, as in Figure 5.
Algorithm SS-Byz-Stabilizer /* executed at each beat */ /* A0 is the algorithm to be stabilized */ /* C is an instance of Byz-State-Check*/ /* P is a [agree + 1]-pulser */

1. 2. 3. 4.

execute a single round of A0 ; execute a single round of C ; execute a single beat of P ; if P pulsed this beat re-initialize C . Fig. 5. A Self-stabilizing Byzantine tolerant Stabilizer

Theorem 5. SS-Byz-Stabilizer transforms a Byzantine tolerant algorithm A0 into Self-stabilizing Byzantine tolerant algorithm. Proof. P is a [agree +1]-pulser. Hence, eventually it starts pulseing agree +1 beats apart. When this happens, C is re-executed periodically, and terminates between such 2 executions . Hence, C performs correctly. This means that the

On Self-stabilizing Synchronous Actions Despite Byzantine Attacks

207

local states of A0 will be consistent. And we have that starting from any initial state of A0 's local states, eventually A0 's local states are consistent.

Acknowledgments
We would like to thank Ariel Daliot for helpful discussions and insightful comments.

References
1. Afek, Y., Dolev, S.: Local stabilizer. In: Proc. of the 5th Israeli Symposium on Theory of Computing Systems (ISTCS97), Bar-Ilan, Israel (June 1997) 2. Arora, A., Dolev, S., Gouda, M.G.: Maintaining digital clocks in step. Parallel Processing Letters 1, 11­18 (1991) 3. Daliot, A., Dolev, D.: Self-stabilization of byzantine protocols. In: Tixeuil, S., Herman, T. (eds.) SSS 2005. LNCS, vol. 3764. Springer, Heidelberg (2005) 4. Daliot, A., Dolev, D., Parnas, H.: Linear time byzantine self-stabilizing clock synchronization. In: Papatriantafilou, M., Hunel, P. (eds.) OPODIS 2003. LNCS, vol. 3144, Springer, Heidelberg (2004), http://arxiv.org/abs/cs.DC/0608096 5. Dolev, D.: The byzantine generals strike again. Journal of Algorithms 3, 14­30 (1982) 6. Dolev, D., Halpern, J.Y., Simons, B., Strong, R.: Dynamic fault-tolerant clock synchronization. J. Assoc. Computing Machinery 42(1), 143­185 (1995) 7. Dolev, S.: Possible and impossible self-stabilizing digital clock synchronization in general graphs. Journal of Real-Time Systems 12(1), 95­107 (1997) 8. Dolev, S., Welch, J.L.: Wait-free clock synchronization. Algorithmica 18(4), 486­ 511 (1997) 9. Dolev, S., Welch, J.L.: Self-stabilizing clock synchronization in the presence of byzantine faults. Journal of the ACM 51(5), 780­799 (2004) 10. Gopal, A.S., Perry, K.J.: Unifying self-stabilization and fault-tolerance. In: IEEE Proceedings of the 12th annual ACM symposium on Principles of distributed computing, Ithaca, New York (1993) 11. Herman, T.: Phase clocks for transient fault repair. IEEE Transactions on Parallel and Distributed Systems 11(10), 1048­1057 (2000) 12. Hoch, E.N., Dolev, D., Daliot, A.: Self-stabilizing byzantine digital clock synchronization. In: Datta, A.K., Gradinariu, M. (eds.) SSS 2006. LNCS, vol. 4280, Springer, Heidelberg (2006) 13. Katz, S., Perry, K.J.: Self-stabilizing extensions for message-passing systems. Distributed Computing 7(1), 17­26 (1993) 14. Nesterenko, M., Arora, A.: Dining philosophers that tolerate malicious crashes. In: 22nd Int. Conference on Distributed Computing Systems (2002) 15. Nesterenko, M., Arora, A.: Tolerance to unbounded byzantine faults. In: SRDS, p. 22 (2002) 16. Papatriantafilou, M., Tsigas, P.: On self-stabilizing wait-free clock synchronization. Parallel Processing Letters 7(3), 321­328 (1997) 17. Sakurai, Y., Ooshita, F., Masuzawa, T.: A self-stabilizing link-coloring protocol resilient to byzantine faults in tree networks. In: OPODIS, pp. 283­298 (2004)

Gossiping in a Multi-channel Radio Network
An Oblivious Approach to Coping with Malicious Interference (Extended Abstract)
Shlomi Dolev1 , Seth Gilbert2 , Rachid Guerraoui3, and Calvin Newport4
1 Ben-Gurion University dolev@cs.bgu.ac.il 2 MIT CSAIL, EPFL IC seth.gilbert@epfl.ch 3 EPFL IC rachid.guerraoui@epfl.ch 4 MIT CSAIL cnewport@mit.edu

Abstract. We study oblivious deterministic gossip algorithms for multi-channel radio networks with a malicious adversary. In a multi-channel network, each of the n processes in the system must choose, in each round, one of the c channels of the system on which to participate. Assuming the adversary can disrupt one channel per round, preventing communication on that channel, we establish - )n ) on the number of + log c n ,  n(1c- a tight bound of max  (1c 2 -1 rounds needed to solve the -gossip problem, a parameterized generalization of the all-to-all gossip problem that requires (1 - )n of the "rumors" to be successfully disseminated. Underlying our lower bound proof lies an interesting connection between -gossip and extremal graph theory. Specifically, we make use of Tur´ an's theorem, a seminal result in extremal combinatorics, to reason about an adversary's optimal strategy for disrupting an algorithm of a given duration. We then show how to generalize our upper bound to cope with an adversary that can simultaneously disrupt t < c channels. Our generalization makes use of selectors: a combinatorial tool that guarantees that any subset of processes will be "selected" by some set in the selector. We prove this generalized algorithm optimal if a maximum number of values is to be gossiped. We conclude by extending our algorithm to tolerate traditional Byzantine corruption faults.

1 Introduction
Malicious adversaries pose a particular threat to radio networks. Due to the shared nature of the communication medium, an adversary can prevent any information exchange between honest processes by jamming the channel with noise. The first attempts to tackle this problem assumed that the malicious adversary could only corrupt honest processes, but not interfere with communication [1, 2, 3]. Another approach assumed that the adversary interferes only in a probabilistic manner, causing either random transient message corruption [4], or random permanent process corruptions [5]. More recent work allows malicious interference--but bounds the number of times that the adversary can disrupt communication [6, 7].
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 208­222, 2007. c Springer-Verlag Berlin Heidelberg 2007

Gossiping in a Multi-channel Radio Network

209

In this paper, we place no such restrictions on the adversary. Instead, we shift our focus to multi-channel radio networks in which each process can make use of any one of c available channels in each round. The adversary can disrupt communication by broadcasting concurrently on a channel with an honest process, causing a collision. This setting is appealing because of its practicality. Almost every major commercial/industrial/military radio device--including sensor motes, laptops running 802.11, and bluetooth-enabled devices--has the capability to switch between multiple communication channels. These multi-channel networks have been studied previously in the context of communication capacity and throughput (e.g., [8, 9]). They have also been studied in the field of Cognitive Radio Networks [10, 11, 12], where algorithms attempt to adaptively compensate for (semi-permanently) disrupted communication channels. The Gossip Problem. We study the fundamental problem of gossip in which processes attempt to exchange rumors. Variants of this problem have been well-studied in (synchronous) single-channel radio networks: for broadcast in a fault-free network, see, for example, [13, 14, 15, 16]; for omissions and crash failures, see, for example, [17, 18]. We introduce a parameterized version of the gossip problem, called -gossip, in which (1 - )n rumors must be disseminated to at least n - 1 processes. (As we later show, it is impossible to disseminate even a single value to all n receivers in this setting). The -gossip problem is a generalization of classical all-to-all gossip (0-gossip) that allows for flexibility in the number of rumors that need to be spread--a desirable feature for many applications (e.g., when only a majority vote is needed). Basic Setting. We assume that honest processes maintain no shared secrets (e.g., information unknown to the adversary). The honest processes could use such information to derive a transmission pattern that appears random to an outside observer. (Military communication systems, like those used by the MILSTAR satellite system, use such a scheme to evade eavesdroppers). We omit this possibility for three reasons. First, we are interested in deterministic solutions that guarantee correctness even in the worst case. Second, for low-resource devices--such as RFID tags or tiny sensor motes-- cryptographic calculations (particularly of the public-key variety) and secure key dissemination may be prohibitively expensive. Third, shared secrets are hard to maintain when the adversary can corrupt and hijack honest devices (addressed in Section 9). With such devices in mind, we focus on oblivious algorithms--those in which a process's decision to transmit or listen in a given round on a given channel is a function only of its unique identifier, the round number, and the number of processes and channels in the network. Oblivious algorithms are appealing because they are considered easy to construct and deploy. (In Section 10, we briefly discuss randomized and adaptive solutions.)
- )n n(1- ) rounds are necesResults. We prove that max  (1c -1 + logc n ,  c2 sary and sufficient to solve -gossip in a setting where the adversary can disrupt one channel per round, and n is the total number of processes. We demonstrate necessity by first reducing -gossip to a graph-theoretic game--(n, )-clique destruction--in which a player tries to remove enough edges from an n-clique to destroy any clique of size greater than n, and then proving a lower bound on this game by appealing to Tur´ an's Theorem [19], a seminal result in extremal combinatorics.

210

S. Dolev et al.

To demonstrate sufficiency, we describe a matching deterministic oblivious algorithm that proceeds in two phases. During the first phase, a sufficient number of values are disseminated to a distinguished group of listeners distributed among the channels. The resulting construction, when considered in the context of extremal graph theory, produces a Tur´ an Graph. During the second phase, these values are disseminated to increasingly larger sets of processes until n - 1 have learned the total requisite knowledge. We then generalize our algorithm for the multi-channel adversary that can simultaneously disrupt up to t < c channels. This models a network with t adversarial processes, each potentially disrupting a different channel. Our algorithm presented for t = 1 extends naturally to this scenario: the generalized first phase, in fact, relates to a conjectured hypergraph-generalization of Tur´ an's Theorem; the second phase uses selectors, a combinatorial device introduced by [20], to generalize the dissemination schedule. We show this solution to be optimal for the natural case of = t/n (i.e., trying to gossip the maximum possible number of values). We conclude by describing how to modify our algorithm to tolerate traditional Byzantine corruption faults.

2 Model
We consider a system of n honest processes, each assigned a unique identifier in the range [1..n]. The processes inhabit a single-hop radio network comprised of c communication channels: 1 < c  n. We first assume the presence of a malicious adversary that can corrupt one channel per round. We then consider the general case where it can simultaneously disrupt t < c channels in each round. Synchronous Rounds. Executions proceed in synchronous rounds. In each round, each process chooses a single channel on which to participate: it can either transmit or receive on that channel. In each round the adversary chooses up to t channels to disrupt. If exactly one process transmits a message m on channel k in a round, and the adversary does not disrupt channel k , then every process receiving on channel k receives message m. When two processes broadcast in the same round on the same channel, the message is lost. (We do not assume that the processes have the capacity to detect collisions.) Deterministic and Oblivious. We consider deterministic, oblivious gossip algorithms. An oblivious algorithm is one in which the broadcast schedule is determined in advance. Formally, a deterministic oblivious algorithm is a sequence A = A1 , . . . , Ar where each Ar : [1..n]  {trans, recv, } × [1..c] is a function describing the behavior of the processes in round i. For example, when Ar (i) = trans, k , it indicates that in round r, process i transmits on channel k ; when Ar (i) = recv, k , then process i receives on channel k . Without loss of generality, for the lower bound we consider full information protocols in which processes always transmit their entire state. Also without loss of generality, we assume each initial value is unique.

3 The Gossip Problem
Each process begins with an initial value (or "rumor") which it attempts to disseminate to the other processes. In this paper we consider the ( ,  )-gossip problem in which all

Gossiping in a Multi-channel Radio Network

211

but n processes must receive a common set of all but n of the initial values. This definition is a generalization of commonly considered communication primitives: for 1 example, all-to-all gossip is (0, 0)-gossip and one-to-all broadcast is (1 - n , 0)-gossip. Formally, algorithm A solves the ( ,  )-gossip problem if and only if, for all possible adversarial choices, at least (1 -  )n honest processes successfully receive a common set of at least (1 - )n initial values. In this setting, it is clearly impossible to ensure that all n honest processes successfully receive even one common initial value. Assume there existed such an ( , 0)-gossip algorithm A, and consider the adversarial strategy C in which the adversary always disrupts the channel on which process 1 either transmits or receives. Under these conditions, process 1 can never successfully transmit or receive any value: it neither learns the initial value of any other process, nor does any other process learn its initial value, implying that A does not solve ( , 0)-gossip. By the same argument, it is impossible to solve (0,  )-gossip. Hence, we focus on solving the ( , t/n)-gossip problem where t/n   1, that is, the problem in which all but n initial values are disseminated to all but t processes. (Considering larger values of  does not allow for significantly faster termination for t )-gossip problem most values of .) For the remainder of the paper, we refer to the ( , n where t/n   1 simply as: -gossip. Roadmap. In Sections 4, 5 and 6, we address the case where t = 1. In Section 4 we present a lower bound, in Section 5 we present a matching algorithm, and in Section 6 we outline the proof. In Section 7, we extend our algorithm to tolerate a multichannel adversary that can block an arbitrary t < c channels. In Section 9, we consider a more general model in which the adversary can corrupt honest players (rather than simply disrupting communication). We conclude with a discussion of open questions in Section 10.

4 A Lower Bound for -Gossip Where t = 1
Let A be an arbitrary deterministic oblivious algorithm for -gossip where t = 1. We show that A requires
max  (1 - )n + logc n ,  c-1 n(1 - ) c2

rounds to terminate. The first term dominates when  1/c (i.e., only a small number of values need to be disseminated), and it follows from the observation that at most c - 1 values can be broadcast in each round. In this section, we focus predominantly on the (more interesting) case where < 1/c. For every execution  of algorithm A, let round Rtrans () be the minimum round such that the following is true: at least (1- )n of the honest processes have broadcast without being disrupted by the adversary in the prefix of  through round Rtrans (). We show that for some execution  of A, ) ). It follows that the protocol cannot terminate in  prior to round Rtrans ()   ( n(1c- 2 trans R ()--implying the second term of our bound.

212

S. Dolev et al.

We prove this result by exploiting an interesting connection between oblivious gossip and graph theory. An oblivious algorithm can be imagined as a sequence of edge removals from an n-clique, and the adversary's optimal strategy can be described by the largest clique that remains after these removals. Accordingly, we turn to the field of extremal combinatorics and apply Tur´ an's Theorem [19] to argue precisely about the size of the cliques that remain in a graph, which in turn tells us the adversary's best strategy. Definition 1 (The (n, )-Clique Destruction Game). Let G = (V, E ) be a graph describing a clique on n nodes. We say that a subset of edges S  E is a solution to the (n, )-clique destruction game if and only if the graph G = (V, E - S ) contains no clique of size greater than n. Tur´ an's theorem relates the largest clique in a graph and the number of edges in a graph: Theorem 1 (Tur´ an's Theorem [19]). If graph G = (V, E ) has no subgraph that is a clique of size k + 1, then |E |  (1 - 1/k ) n2 /2 . From this we derive an immediate corollary: Corollary 1. Fix S  E a solution to the (n, )-clique destruction game. Then |S |  n(1 - )/(2 ). Proof. By Theorem 1 where k = n: subtract from the n 2 edges in an n-clique the maximum number of edges in a graph with no cliques of size n + 1, as per Theorem 1, to get the minimum size of S . We next connect the (n, )-clique destruction game to the -gossip problem: Lemma 1. If for every execution  of algorithm A, Rtrans ()  r, then there exists a solution S to the (n, )-clique destruction game such that |S |  c2 r. Proof. Without loss of generality, assume that A assigns exactly one process to transmit on each channel in each round. Construct S as follows: add edge (a, b) to S if, for some round r  r, processes a and b both broadcast in round r . Since for each r  r there c such pairs, we conclude that |S |  c2 r. are at most 2 Suppose, for contradiction, that nodes V  V form a clique of size > n in the residual graph. We construct the following strategy to thwart A: whenever a process in V attempts to broadcast on channel k , the adversary disrupts channel k . This is always possible as no two processes in V broadcast in the same round (by the construction of S ). This violates the correctness of A, implying a contradiction. We combines these two lemmas to obtain our final bound: Theorem 2. For any deterministic oblivious algorithm, A, that solves -gossip,
|A|  max  (1 - )n + logc n ,  c-1 n(1 - ) c2 .

Gossiping in a Multi-channel Radio Network

213

Proof. If A solves -gossip, then for every execution  of algorithm A, there exists a round r such that Rtrans ()  r. We begin by establishing the first term of the bound. Since at most c - 1 values can be transmitted without disruption in each round, it is clear that r  (1 - )n/(c - 1). Let r be the round in which the last of these (1 - )n values is transmitted. For each of the n - 1 processes that ultimately learn all (1 - )n values, there must be some round  r in which it listens on a non-disrupted channel. Let r be the latest of these rounds. We know r  r + logc n - 1, as over the first logc n - 1 rounds there are only clogc n-1 < n - 1 different channel-listening patterns a process can follow; by the pigeonhole principle this results in two processes listening on the same channels for these first logc n - 1 rounds, allowing the adversary to block both processes. Together these two pieces form the first term of the lower bound. For the second term of the lower bound we turn to our Tur´ an-derived results. From Lemma 1 we know there exists a set of edges S that solves the (n, )-clique destruction game, such that c2 r  |S |. By Corollary 1 we know: |S |  n(1 - )/(2 ). This implies ) . r   n(1c- 2

5 An Upper Bound for -Gossip Where t = 1
In this section we describe a deterministic oblivious algorithm for solving the -gossip problem when < 1/c. In Section 8, we discuss the (simpler) case where  1/c. For the sake of concision, we make a few simplifying assumptions. First, we assume that n > 6c. For smaller values of n, the algorithm can simply restrict itself to a subset of the channels. Second, we assume that the number of channels c is even. For odd c, the +1 (slightly algorithm can restrict itself to c - 1 channels. Finally, we assume that  2cn larger than the trivial  1/n lower bound for ). In Section 5.3 we describe how to remove this last assumption. 5.1 The Gossip Protocol The gossip protocol (see Figure 1) constructs an oblivious algorithm that solves the -gossip problem for < 1/c. Recall, an oblivious algorithm is a sequence A = A1 , A2 , . . . where each Ai is a function from processes [1 . . . n] to actions {trans, recv} and channels [1 . . . c]. Throughout the description, we use the notation divide(S, k ) to refer to a partition of the set S into |S |/k sets of size k , one of which may have fewer than k elements if |S | does not divide evenly by k . Also, i [b] refers to the bth bit of the binary representation of i. Our protocol proceeds in two parts. Part I: Initially, the processes are divided into two sets: a set of 2c listeners, consisting of processes P = {1, . . . , 2c}, and a set of (at least 4c) transmitters Ptran , consisting of the remaining processes. Each channel is assigned a pair of two listeners. Next, the InfoTransfer( ) routine is used to transfer all but n + 2c of the initial values to some pair of listeners. (The additive 2c represents the listeners' values that are not transmitted.) By choosing = - 2c/n, this ensures that all but n initial values are known to some pair of listeners (in Section 5.3 we discuss how to allow the listeners to participate). Part II: The goal of the second part is to disseminate the information acquired by each pair of listeners to all but one process. We say that a set is knowledgeable with respect to

214

S. Dolev et al.

Gossip( ) ; Part I: Transfer info from transmitters to listeners. 3 P  {i | 1  i  2c} 4 Ptran  {i | 2c < i  n} 5 channel-assignment  divide(P , 2) c , Ptran , P , channel-assignement, A1 ,. . . ) 6 InfoTransfer( - 2 n
1 2 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29

; Part II, Step 1: Create c knowledgable sets in two steps. Psets  divide(Ptran , Ptran /c ) for (chan = 1 to c/2) do disseminate2(channel-assignment[chan], Psets[chan], chan, chan+c/2, B1 , . . . ) for (chan = c/2+1 to c) do L  channel-assignment[chan] disseminate2(channel-assignment[chan], Psets[chan], chan-c/2, chan, C1 , . . . ) ; Part II, Step 2: Combine channels. r1 while (|Psets| > 1) do newPsets   s  |P sets|/2 for (i = 1 to s) do P  combine(Psets[i], Psets[i+s], i, i + c/2, Dr , . . . ) newPsets  newPsets  P ; If the size of Psets is odd, we let the last set pass through uncombined. if (2s+1 = |Psets|) then newPsets  newPsets  Psets[2s+1] Psets  newPsets r  r + 6 log n + 36 return A.B.C .D

Fig. 1. An algorithm for solving the -gossip problem ( < 1/c)

some set of initial values if all but one process in the set has received the initial values. The second part of the gossip protocol proceeds in two steps. First, the transmitter processes are divided into c sets, one per channel. (The variable Psets stores these c sets). The two listeners associated with each channel disseminate the information acquired in Part I to the set of transmitters assigned to that channel. This dissemination step uses two channels, and thus we can run c/2 instances in parallel: in the first log n rounds, we perform the dissemination for channels 1, . . . , c/2; in the next log n rounds, we perform the dissemination for channels c/2 + 1, . . . , c. In each case, this dissemination is accomplished using the disseminate2 routine. At the end of this step, each set of processes in Psets is knowledgeable with respect to the set of values known to the listeners on their channel. In the second step, we repeatedly combine pairs of knowledgeable sets (via the combine routine) into larger knowledgeable sets in which the processes know values from both of the original sets. We continue combining sets until we are left with a single set in which the processes know the required (1 - )n values.

Gossiping in a Multi-channel Radio Network

215

InfoTransfer( , Ptran , P , channel-assignment, A1 , . . . ) ; Assign listeners to channels. 3 for every round r, for every channel k do 4 {a, b}  channel-assignment[k] 5 Ar (a)  recv, k 6 Ar (b)  recv, k 7 ; Assign transmitters to channels. 8 r0 9 for every B  divide(Ptran  P , 1/ ) do 10 Bsubs  divide(B, c/2) 11 for every (S1 , S2 )  Bsubs × Bsubs do 12 chan  1 13 for every i  S1  S2 14 Ar (i)  trans, chan 15 chan  chan+1 16 r  r+ 1
1 2

Fig. 2. Routines to transfer information from transmitters to listeners
disseminate2(L, P, c1 , c2 , A1 , . . . ) {a1 ,a2 }  L 3 for b = 1 to lg n 4 Ab (a1 )  trans, c1 5 Ab (a2 )  trans, c2 6 for b = 1 to lg n 7 for each i  P 8 if i[b] = 0 then 9 Ab (i)  recv, c1 10 else if i[b] = 1 then 11 Ab (i)  recv, c2
1 2

Fig. 3. A routine that disseminates data from listeners to arbitrary sets

The Information Transfer Routine. The goal of the InfoTransfer routine (Figure 2) is to ensure that all but n + 2c of the initial values are received by some pair of listeners. Each channel is assigned two listeners, and we assign transmitters to each channel in each round such that the resulting induced graph, as formulated in terms of Lemma 1 and the clique destruction game, forms a Tur´ an Graph.1 We divide all processes into sets {B1 , B2 , . . . , B n } of size 1/ ; there are n such sets. Our goal is to ensure that all but (at most) one transmitter in each set Bi succeeds in transmitting its value to a pair of listeners. We proceed as follows: For each set Bi , we sub-divide Bi into subsets of size c/2, and schedule each of the (2/2 c) pairs of subsets to broadcast in a round (omitting listeners, which are already occupied).
1

A Tur´ an Graph for value k (say, k = n + 1), is the unique graph, as proved by Tur´ an, to contain no cliques of size k and to contain the maximum number of edges for which this condition can be true, as established by the theorem of the same name.

216

S. Dolev et al.

combine(S1 , S2 , c1 , c2 , A1 , . . . ) r0 3 for (i = 1 to 2) do 4 ; Use i and i+2 as witnesses for S1 5 L  S1 [i]  S1 [i+2] 6 P  S 1  S2 - L 7 disseminate2(L, P, c1 , c2 , B1 , . . . 8 AA.B 9 for (i = 1 to 3) do 10 ; Use i and i+3 as witnesses for S2 11 L  S2 [i]  S2 [i+2] 12 P  S 1  S2 - L 13 disseminate2(L, P, c1 , c2 , B1 , . . . 14 AA.B 15 return S1  S2
1 2

:

)

:

)

Fig. 4. A routine to combine knowledgeable sets

Since every pair of non-listeners in Bi broadcast together in some round, the adversary can block at most one non-listener in each set Bi from communicating its value to a listener. Running Time: n 2/2 c . The Disseminate Routines. The disseminate2(L, P, c1 , c2 , . . .) routine (Figure 3) disseminates all values known by both processes in the set L to all but (at most) one process in the set P , using channels c1 and c2 . (We assume |L| = 2). First, we assign the two processes in L to transmit on channels c1 and c2 for log n rounds. In each round b, each process in P chooses a channel on which to receive based on its identifier i: if i[b] = 0, it chooses channel c1 ; if i[b] = 1, it chooses channel c2 . Thus, for any pair of processes in P , there is some round in which they receive on different channels. Running Time: log n . The Combine Routine. The combine(S1 , S2 , c1 , c2 , . . .) routine (Figure 4) begins with two knowledgeable sets. (We assume that S1 and S2 each contain at least 6 processes.) Using two channels, the combine function creates a new knowledgeable set S1  S2 such that all but (at most) one process in the combined set knows the shared information from both S1 and S2 . The routine accomplishes this goal by running disseminate2 five times. The first two times, it uses pairs of witnesses from set S1 to disseminate information to set S2 . Since at most one node in S1 is not knowledgeable, we can conclude that one of these pairs of witnesses is knowledgeable. Hence after the first two calls to the disseminate2 routine, all but one node in S1  S2 are knowledgeable with respect to the values from S1 . The next three times, it uses pairs of witnesses from set S2 to disseminate information to set S1 . Notice that there may be two nodes in S2 that are not fully knowledgeable: one node may not be knowledgeable about S2 's values, and one node may not be knowledgeable about S1 's values. Thus for one of the three pairs of witnesses, both are knowledgeable, and the dissemination succeeds in informing all but one node in S1  S2 . Running Time: 5 log n .

Gossiping in a Multi-channel Radio Network

217

5.2 Running Time of the Gossip Protocol The running time for Gossip is calculated as: [InfoTransfer] + 2[disseminate2] + log c [combine] . This equals: n 2/2 c + 2 log n + log c (5 log n ). We can simplify this to - ) ( n < 1/c  1/2, this is equivalent to ( n(1c 2 ). The modified c2 ). Because algorithm, presented in the next section, improves the running time (marginally) to - ) ( n(1c ), exactly matching our lower bound for the case where < 1/c. As men2 tioned, in Section 8 we provide an algorithm (matching within an additive factor of log2 c) for the less involved case of  1/c. 5.3 Achieving -Gossip for Small The algorithm presented in the previous section assumes that  (2c + 1)/n. We now discuss modifying the algorithm to require only  1/n, the minimum value of for which the problem can be solved. The difficulty occurs in the InfoTransfer routine, where the listeners do not participate in transmitting their values. Unlike the transmitters, their initial values are known only to themselves, not to a pair of processes. The first step in our modification is to schedule the listeners, P , as well as the transmitters, Ptran , to transmit their values during the InfoTransfer. If only one of the two listeners assigned to channel k is scheduled to transmit in a round, then it broadcasts on channel k , resulting in no difficulties. Consider the problematic case where both listeners for channel k are scheduled to transmit in the same round. Since the division into sub-blocks of size c/2 is arbitrary, we can ensure that each of the listeners for channel k is in a different sub-block. Thus, there is only one round for which both listeners for a channel might be forced to broadcast. In this case, two "backup listeners" are recruited to monitor channel k during that round. Since there are only c processes scheduled to broadcast in that round, and only 2c listeners, there remain at least 2c processes to play the role of backup listener. Every channel may be forced to recruit one pair of backup listeners, resulting in 4c listeners and backup listeners whose values need to be propagated in the second part of the protocol; the described algorithm extends immediately to this case.

6 Analysis
We now outline an argument that the algorithm from Section 5 solves -gossip (when < 1/c). We focus on some of the key invariants satisfied by the different components of the construction. First, we observe that the InfoTransfer routine guarantees that a sufficient number of values are transmitted without adversarial disruption:
c rounds, all but n of the processes transmit Lemma 2. During the first r = n 2/ 2 their values in some round  r without disruption.

This claim follows from the construction of the schedule: for each of the n sets, all pairs of processes broadcast together in some round. We therefore conclude that the listeners receive a sufficient set of values:

218

S. Dolev et al.

Corollary 2. At the end of the InfoTransfer routine, there exists some set of values V of size at least (1 - )n such that each value v  V is known to some set of 2 listeners or backup listeners. We next observe that after the first step of Part II (i.e., after the disseminate2 routines) each set in Psets is knowledgeable with respect to some subset of the values V : Lemma 3. There exists a partition V1 , . . . Vc of the values in V such that after the disseminate2 routines (i.e., by line 15), each set Psets [i] is knowledgeable with respect to Vi , 1  i  c. The proof for this claim follows from the fact that the disseminate2 routine successfully transmits the values from the listeners (or backup listeners) to the remaining nodes in the set; since every node's identifier is unique, there will be some round during the disseminate2 routine in which each pair of nodes is listening on a different channel, and hence will receive the appropriate set of values. Finally, we observe that the combine routine successfully merges knowledgeable sets, which concludes the proof: Lemma 4. If Pi and Pj are knowledgeable sets with respect to some sets of values Vi and Vj , then set P  combine(Pi , Pj , . . .) is knowledgeable with respect to Vi  Vj . This fact follows from the correctness of the disseminate2 routine. We thus conclude: Theorem 3. Let A be the deterministic oblivious algorithm constructed by Gossip. If < 1/c, then A solves the -gossip problem in time O(n(1 - )/ c2 ).

7 The Multi-channel Adversary (t < c)
The algorithm described in Section 5 tolerates an adversary that can disrupt one channel in each round. The algorithm naturally extends to an adversary that can disrupt t < c channels. (Again, for the purpose of brevity, we focus on the case where < t/c, as this is the more interesting case. The case of  t/c is described in Section 8.) The overall algorithm maintains the same structure as that presented in Section 5: in the first part, the nodes transmit their values to a set of listeners; in the second part, the listeners become transmitters and create ever-expanding knowledgeable sets. Part I. First, we assign t + 1 listeners to each channel, instead of 2 listeners. The InfoTransfer routine is modified as follows: The processes are divided into n/t sets of size t/ (instead of n sets of size 1/ ). Each of these sets is subdivided into subsets of /c combinations of subsets are scheduled. size c/(t + 1) (instead of size 2). All t(t+1) t+1 This ensures that any combination of t + 1 nodes in a set broadcast in the same round, and thus that there are at most t· n/t nodes that fail to transmit their value. The resulting (approximating the binomial and the fact that t < c). Notice running time is O ne c t that the resulting schedule can be reduced to a (t + 1)-hypergraph, in the same manner that the schedules for t = 1 could be reduced to a graph. If this construction is optimal, then it corresponds to a hypergraph-generalization of a Tur´ an Graph. Finding such an entity (and proving it optimal) remains an open problem in extremal graph theory.
t+1

Gossiping in a Multi-channel Radio Network

219

Part II. In the second part, the listeners disseminate the information to groups of nodes. The basic routine here is a disseminate[t + 1] routine that uses t + 1 channels to distribute data from t + 1 listeners to a set that becomes knowledgeable. Each of t + 1 listeners transmits on one channel throughout the dissemination phase; the rest of the nodes in the set are scheduled to listen on different channels in different rounds such that any set of t + 1 nodes is scheduled in some round to listen on different channels. To accomplish this, we need to introduce an additional tool: selectors, as introduced by Komlos and Greenberg [20] (the term "selector" was coined later by [21]). Let S be a family of sets, where each S  S is a subset of [1, . . . , n]. For integer k  n, we say that S is a (n, k, 1)-selector if for every set A  [1, . . . , n] where |A| = k , there exists a set S  S such that |S  A| = 1. In [20], it was shown that there exist (n, k, 1)-selectors of size at most O(k log n/k ), and [22] shows how to explicitly construct selectors of size O(k polylog(n)). For this section, we use the existential bounds from [20], and assume that for all k  n, Sk is a family of selectors of size O(k log n/k). The schedule is constructed recursively. We define T (c ) recursively to be the number of rounds needed to construct the schedule for c channels. In the beginning, we are constructing a schedule for all c = t + 1 channels. If c = 2, then the recursion terminates: schedule the remaining nodes to listen on those two channels as per the disseminate2 routine, i.e., each node chooses a channel based on its identifier. This takes T (2) = O(log n) rounds. If c > 2, then we use the family of selectors Sc : for each set S  Sc , schedule the nodes in S to listen on one channel for T (c - 1) rounds, and recursively schedule the remaining nodes on the remaining c - 1 channels. For each set in Sk this takes T (c - 1) rounds, and thus T (c ) = |Sc |T (c - 1). Since selector Sc is of size at most O(c log n/c ), we conclude that the entire schedule for T (t + 1) is (roughly) O((t + 1)t logt n). To see that it satisfies the desired property, consider any subset of t + 1 nodes: by definition, exactly one node is selected by one of the sets in St+1 ; at the next step of the recursion, one node is selected by a set in St , one by a set in St-1 , and so on, until the recursion bottoms out at the simple two-channel case. Thus there exists some round in which all t + 1 nodes listen in the same round. The remaining generalizations of the algorithm from Section 5 are straightforward: the combine routine merges sets S1 and S2 as follows: first, it chooses (t + 1)(t + 1) witnesses from set S1 and runs t + 1 iterations of disseminate[t + 1] to set S2 ; it then chooses (2t+1)(t+1) witnesses from set S2 and runs 2t+1 iterations of disseminate[t+ 1] to set S1 . (By contrast, in the t = 1 case, there were two pairs of witnesses chosen for the first dissemination and three pairs of witness chosen for the second.) It should be noted that using selectors here too would result in improved performance. Thus each combine costs O((t + 1)t+1 logt n). Since each combine uses t + 1 channels, it requires c t+1 log c iterations of the main loop to combine all c knowledgeable sets. Noting that t < c < n, we thus conclude that the total running time of the gossip protocol is: net+1 O + c(t + 1)t logt+1 n c t Lower Bound. Proving a matching lower bound for the general case remains an open question. We can prove that the result is optimal for the natural case of = t/n, that is, trying to disseminate the maximum possible number of values. Specifically, we claim

220
n t+1

S. Dolev et al.

c / t+1 rounds to be necessary for all but t nodes to transmit even once without being disrupted. The numerator follows from the observation that in order to transmit all but t values without interference, there must exist, for each combination of t + 1 processes, a round in which all t + 1 processes transmit concurrently (otherwise, the t adversaries can always interference when any of these processes transmit). The denomc inator follows from the observation that at most t+1 unique sets of t + 1 processes can transmit concurrently on c channels during a single round. This fraction simplit+1 fies directly to ( n ct+1 ), matching our above bound for InfoTransfer (within a factor of O(et )). Again, notice that proving a hypergraph-generalization of Tur´ an's Theorem would result in an immediate lower bound. (See [23, 24] for more on hypergraph generalizations of Tur´ an's Theorem.)

8 Achieving -Gossip for Large
In this section, we describe an algorithm to solve -gossip when > t/c. In the case where t = 1, we again divide the protocol into a transmission phase and a dissemination phase. In the transmission phase, we attempt to ensure that (1 - )n values are known to a set of 6c processes. This is accomplished by assigning six listeners to each channel, and dividing the nodes into groups of size c; each group is assigned one round to broadcast for (1 - )n/(c - 1)  n/c rounds, ensuring that in each round c - 1 nodes succeed. The listeners then exchange their values amongst themselves using the combine routine described in Section 5. The total running time of the transmission phase is O((1 - )n/(c - 1) + log2 c). In the dissemination phase, we repeat the following twice, each time with a disjoint set of c "listeners" from the previous phase: each of the "listeners" broadcasts on one of the c channels, and each of the remaining nodes chooses a channel to listen on in each round using the "base-c" representation of its identifier. Since each identifier is unique, we can be sure that for any pair of nodes, in one of these rounds the two nodes choose different channels to listen on, and hence at least one receives the appropriate set of values. The total running time for the dissemination phase is O(logc n). Thus the final overall running time of both phases is O((1 - )n/(c - 1) + logc n + log2 c). When t > 1, this strategy generalizes in the natural way: we assign (2t + 1)(t + 1) listeners in the transmission phase, and c nodes broadcast in each round; at least c - t of them succeed, resulting in at least (1 - )n values being received by listeners since (1 - )n/(c - t)  n/c; as before, the combine routine, generalized for the multichannel adversary, is then used to combine the data. The total running time in this case is (1 - )n/(c - t) + O(c(t + 1)t logt+1 n).

9 Byzantine Adversary
The algorithms described in Sections 5 and 7 tolerate an adversary that can disrupt communication, but not an adversary that can directly corrupt an "honest" player. It is easy, however, to extend our algorithm to tolerate a Byzantine adversary that corrupts up to t honest players. Each corrupt player can either disrupt a channel or send a message in each round. (Thus, up to t channels can be disrupted, as in the previous case.)

Gossiping in a Multi-channel Radio Network

221

The main modification involves the transmission phase: more listeners are needed on each channel, as some may be Byzantine. Instead of t + 1 listeners on each channel, we assign (2t + 1)(t + 1). We then run the disseminate and combine routines 2t + 1 times, each time with a different set of t + 1 listeners representing each channel. An honest process accepts a value as authentic only if it was received in at least t + 1 of the (2t + 1) runs of disseminate and combine. The running time is increased by a factor of (t). With further care, the number of listeners can be reduced. However, we conjecture that the problem is solvable only if n =  (tc), for example, in the case where t = c - 1.

10 Open Questions
The problems discussed introduce several new directions for future research. First, it remains to close the gap between the upper and lower bounds in the case of a multichannel adversary. Such a result would have interesting connections to a hypergraph generalization of Tur´ an's Theorem, an open problem in extremal graph theory. Second, adaptive algorithms can likely achieve better performance than oblivious algorithms. It remains an interesting open question to determine how much efficiency adaptiveness provides. Similarly, it is possible to achieve better performance using a randomized algorithm, at the cost of some probability of failure. A trivial randomized algorithm can solve 1/n-gossip in O(n log n) time (w.h.p.). Is it possible to do better? Third, we believe the techniques developed here for single-hope networks extend to multi-hop networks. Fourth, prior research on the problem of malicious interference has assumed that the adversary can cause only a bounded number of collisions. It may be interesting to consider the possibility of bounded collisions in a multi-channel network. Finally, this paper considers an adversary who wants to prevent communication. Other research (e.g., [25]) has considered an eavesdropper who wants to compromise the secrecy of the information (but who may not disrupt communication). This leaves open the question of whether it is possible to achieve reliable and secret communication in a multi-channel network in the presence of a malicious and disruptive adversary.

References
1. Koo, C.Y.: Broadcast in radio networks tolerating byzantine adversarial behaviour. In: Proceedings of the 23rd Symposium on Principles of Distributed Computing (PODC), pp. 275­ 282 (2004) 2. Bhandari, V., Vaidya, N.H.: On reliable broadcast in a radio network. In: Proceedings of the 24th Symposium on Principles of Distributed Computing (PODC), pp. 138­147 (2005) 3. Bhandari, V., Vaidya, N.H.: On reliable broadcast in a radio network: A simplified characterization. Technical report, University of Illinois at Urbana-Champaign (May 2005) 4. Pelc, A., Peleg, D.: Feasibility and complexity of broadcasting with random transmission failures. In: Proceedings of the 24th Symposium on Principles of Distributed Computing (PODC), pp. 334­341 (2005) 5. Bhandari, V., Vaidya, N.H.: Reliable broadcast in wireless networks with probabilistic failures. In: Proceedings of the 26th Conference on Computer Communications (Infocom), pp. 715­723 (2007)

222

S. Dolev et al.

6. Gilbert, S., Guerraoui, R., Newport, C.: Of malicious motes and suspicious sensors: On the efficiency of malicious interference in wireless networks. In: Shvartsman, A.A. (ed.) OPODIS 2006. LNCS, vol. 4305, pp. 215­229. Springer, Heidelberg (2006) 7. Koo, C.Y., Bhandari, V., Katz, J., Vaidya, N.H.: Reliable broadcast in radio networks: The bounded collision case. In: Proceedings of the 25th Symposium on Principles of Distributed Computing (PODC), pp. 258­264 (2006) 8. Kyasanur, P., Vaidya, N.H.: Capacity of multi-channel wireless networks: Impact of number of channels and interfaces. In: Proceedings of the 11th Annual International Conference on Mobile Computing and Networking (Mobicom), pp. 43­57 (2005) 9. Bhandari, V., Vaidya, N.H.: Connectivity and capacity of multi-channel wireless networks with channel switching constraints. Technical report, University of Illinois at UrbanaChampaign (January 2007) 10. Mitola, J.: Cognitive Radio: An Integrated Agent Architecture for Software Defined Radio. PhD thesis, Royal Institute of Technology, Sweden (2000) 11. Krishnamurthy, S., Chandrasekaran, R., Mittal, N., Venkatesan, S.: Brief announcement: Synchronous distributed algorithms for node discovery and configuration in multi-channel cognitive radio networks. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 572­574. Springer, Heidelberg (2006) 12. Krishnamurthy, S., Thoppian, M., Kuppa, S., Chanrasekaran, R., Venkatesan, S., Mittal, N., Prakash, R.: Time-efficient layer-2 auto-configuration for cognitive radios. In: Procedings of the International Conference on Parallel and Distributed Systems (PDCS), pp. 459­464 (2005) 13. Alon, N., Bar-Noy, A., Linial, N., Peleg, D.: A lower bound for radio broadcast. Journal of Computer and System Sciences 43(2), 290­298 (1992) 14. Bar-Yehuda, R., Goldreich, O., Itai, A.: On the time-complexity of broadcast in multi-hop radio networks: an exponential gap between determinism and randomization. Journal of Computer and System Sciences 45(1), 104­126 (1992) 15. Czumaj, A., Rytter, W.: Broadcasting algorithms in radio networks with unknown topology. In: Proceedings of the 44th Symposium on Foundations of Computer Science (FOCS), pp. 492­501 (2003) 16. Kowalski, D.R., Pelc, A.: Time of deterministic broadcasting in radio networks with local knowledge. SIAM Journal on Computing 33(4), 870­891 (2004) 17. Diks, K., Pelc, A.: Almost safe gossiping in bounded degree networks. SIAM Journal on Discrete Mathematics 5(3), 338­344 (1992) 18. Kranakis, E., Krizanc, D., Pelc, A.: Fault-tolerant broadcasting in radio networks. Journal of Algorithms 39(1), 47­67 (2001) 19. Tur´ an, P.: On an extremal problem in graph theory. Matematicko Fizicki Lapok 48 (1941) 20. Komlos, J., Greenberg, A.: An asymptotically fast non-adaptive algorithm for conflict resolution in multiple access channels. Transactions on Information Theory 31(2), 302­306 (1985) 21. Bonis, A.D., Gasieniec, L., Vaccaro, U.: Optimal two-stage algorithms for group testing problems. SIAM Journal on Computing 34(5), 1253­1270 (2005) 22. Indyk, P.: Explicit constructions of selectors and related combinatorial structures, with applications. In: Proceedings of the 13th Symposium on Discrete Algorithms (SODA), pp. 697­704 (2002) 23. de Caen, D.: Extension of a theorem of Moon and Moser on complete subgraphs. Ars Combinatoria 16, 5­10 (1983) 24. Sidorenko, A.F.: What we know and what we do not know about Tur´ an numbers. Graphs and Combinatorics 11(2), 179­199 (1995) 25. Miller, M.J., Vaidya, N.H.: Leveraging channel diversity for key establishment in wireless sensor networks. Technical report, U. of Illinois at Urbana-Champaign (December 2005)

The Space Complexity of Unbounded Timestamps
Faith Ellen1 , Panagiota Fatourou2 , and Eric Ruppert3
1 2

University of Toronto, Canada University of Ioannina, Greece 3 York University, Canada

Abstract. The timestamp problem captures a fundamental aspect of asynchronous distributed computing. It allows processes to label events throughout the system with timestamps that provide information about the real-time ordering of those events. We consider the space complexity of wait-free implementations of timestamps from shared read-write registers in a system  of n processes. We prove an  ( n) lower bound on the number of registers required. If the timestamps are elements of a nowhere dense set, for example the integers, we prove a stronger, and tight, lower bound of n. However, if timestamps are not from a nowhere dense set, this bound can be beaten; we give an algorithm that uses n - 1 (single-writer) registers. We also consider the special case of anonymous algorithms, where processes do not have unique identifiers. We prove anonymous timestamp algorithms require n registers. We give an algorithm to prove that this lower bound is tight. This is the first anonymous algorithm that uses a finite number of registers. Although this algorithm is wait-free, its step complexity is not bounded. We also present an algorithm that uses O(n2 ) registers and has bounded step complexity. Keywords: timestamps, shared memory, anonymous, lower bounds.

1

Introduction

In asynchronous systems, it is the unpredictability of the scheduler that gives rise to the principle challenges of designing distributed algorithms. One approach to overcoming these challenges is for processes to determine the temporal ordering of certain events that take place at different locations within the system. Examples of tasks where such temporal information is essential include implementing first-come first-served processing of jobs that arrive at different locations in the system and knowing whether a locally cached copy of data is up-to-date. Temporal information about the scheduling of events can also be used to break symmetry, e.g., the first process to perform some step can be elected as a leader. If processes communicate via messages or shared read-write registers, it is impossible for them to determine the exact temporal ordering of all events. However, timestamps provide partial information about this ordering in such systems. A timestamp algorithm allows processes to ask for labels, or timestamps,
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 223­237, 2007. c Springer-Verlag Berlin Heidelberg 2007

224

F. Ellen, P. Fatourou, and E. Ruppert

which can then be compared with other timestamps. Timestamps have been used to solve several of the most fundamental problems in distributed computing. Examples include mutual exclusion [17] (and the more general k -exclusion problem [2]), randomized consensus [1], and constructing multi-writer registers from single-writer registers [13, 19, 22]. Timestamps have also been employed in anonymous systems as building blocks for implementations of wait-free atomic snapshots and other data structures [12]. Despite the central importance of the timestamp problem, its complexity is not well-understood. In this paper, we present the first study on the number of registers required for wait-free implementations of timestamps. The history of timestamps begins with Lamport [18], who defined a partial ordering on events in a message-passing system; one event "happens before" another if the first could influence the second (because they are by the same process or because of messages sent between processes). He defined a logical clock, which assigns integer timestamps to events such that, if one event happens before another, it is assigned a smaller timestamp. There is no constraint on the relationship between timestamps assigned to other pairs of events. Fidge and Mattern [11, 20] introduced the notion of vector clocks, where timestamps are vectors of integers rather than integers. Two vectors are compared component-wise: one vector is smaller than or equal to another when each component of the first is smaller than or equal to the corresponding component of the second. Their vector clock algorithms satisfy the property that one event gets a smaller vector than another if and only if it happens before the other event. This property is not possible to ensure using integer timestamps, because concurrent events may need to be assigned incomparable vectors. Charron-Bost [5] proved that the number of components required by a vector clock is at least the number of processes, n. In message-passing algorithms, the timestamps reflect the partial order representing (potential) causal relationships. In shared-memory systems, we are concerned, instead, with the real-time ordering of events. The simplest shared-memory timestamp algorithm uses single-writer registers [17]. To get a new timestamp, a process collects the values in all the single-writer registers and writes one plus the maximum value it read into its single-writer register. This value is its new timestamp. Dwork and Waarts [8] described a vector timestamp algorithm that uses n single-writer registers. To obtain a new timestamp, a process increments its register and collects the values in the registers of all other processes. It returns the vector of these n values as its timestamp. These timestamps can be compared either lexicographically or in the same way as in the vector clock algorithm. Attiya and Fouren [3] gave a vector timestamp algorithm that is considerably more complicated. It uses an unbounded number of registers but has the advantage that the number of components in the timestamp (and the time required to obtain it) is a function of the number of processes running concurrently. Guerraoui and Ruppert [12] described an anonymous wait-free timestamp algorithm, but the number of registers used and the time-complexity of getting

The Space Complexity of Unbounded Timestamps

225

a timestamp increases without bound as the number of labelled events increases. Thus, their algorithm is not bounded wait-free. In all the above algorithms, the size of timestamps grows without bound as the number of labelled events increases. This is necessary to describe the ordering among an unbounded number of non-concurrent events. For some applications, one can restrict the events about which order queries can be made, for example, only the most recent event by each process. This restriction allows timestamps to be reused, so they can be of bounded size. This restricted version of timestamps is called the bounded timestamp problem. In contrast, the general version of the problem is sometimes called the unbounded timestamp problem. Israeli and Li [14] gave a bounded timestamp algorithm, assuming timestamps are only generated by one process at a time. Dolev and Shavit defined and solved the problem allowing multiple processes to obtain timestamps concurrently [6]. This and other known implementations of bounded concurrent timestamps [7, 8, 13, 15] are quite complex, as compared to unbounded timestamps. It is known that bounded timestamp algorithms must use  (n) bits per timestamp [14]. In contrast, unbounded timestamp algorithms can use timestamps whose bit lengths are logarithmic in the number of events that must be labelled. Thus, if the number of events requiring timestamps is reasonable (for example, less than 264 ), timestamps will easily fit into one word of memory. The work on the bounded timestamp problem is of great interest and technical depth. However, since bounded timestamp algorithms are complicated and require long timestamps, the unbounded version is often considered more practical. This paper focusses exclusively on the unbounded timestamp problem. 1.1 Our Contributions

In this paper, we study the number of read-write registers needed to implement timestamps. We present both upper and lower bounds. For our upper bounds, we give wait-free algorithms. The lower bounds apply even if algorithms must only satisfy the weaker progress property of obstruction-freedom. Our most general lower bound shows that any timestamp algorithm must use  ( n) registers. Previously known wait-free algorithms use n registers. We show how to modify one of these algorithms to use n - 1 registers. Some existing timestamp implementations use timestamps drawn from a nowhere dense set. Intuitively, this means that between any two possible timestamps, there are a finite number of other timestamps. For this restricted case, we show that any such implementation must use at least n registers, exactly matching known implementations. Interestingly, our lower bound can be beaten by using timestamps from a domain that is not nowhere dense, namely, pairs of integers, ordered lexicographically. We also prove matching upper and lower bounds for anonymous systems, where processes do not have unique identifiers and are programmed identically. We give a wait-free algorithm using n registers, whereas previous algorithms used an unbounded number. We also provide another, faster anonymous algorithm. It uses O(n2 ) registers and a process takes O(n3 ) steps to obtain a timestamp.

226

F. Ellen, P. Fatourou, and E. Ruppert

We prove a tight lower bound of n for the number of registers required for an anonymous timestamp implementation. This establishes a small but interesting space complexity separation between the anonymous and general versions of the timestamp problem, since n - 1 registers suffice for our algorithm, which uses identifiers. Lower bounds for anonymous systems are interesting, in part, because they provide insight for lower bounds in more general systems [9, 10]. Guerraoui and Ruppert [12] used timestamps as a subroutine for their anonymous implementation of a snapshot object. Plugging in our space-optimal anonymous timestamp algorithm yields an anonymous wait-free implementation of an m-component snapshot from m + n registers. This is the first such algorithm to use a bounded number of registers. Similarly, if our second anonymous timestamp algorithm is used, we obtain an anonymous wait-free snapshot implementation from O(m + n2 ) registers where each Scan and Update takes O(n2 (m + n)) steps. This is the first bounded wait-free anonymous snapshot implementation.

2

The Model of Computation

We use a standard model for asynchronous shared-memory systems, in which a collection of n processes communicate using atomic read-write registers. We consider only deterministic algorithms. If processes have identical programmes and do not have unique identifiers, the algorithm is called anonymous; otherwise, it is called eponymous [21]. An execution of an algorithm is a possibly infinite sequence of steps, where each step is an access to a shared register by some process, followed by local computation of that process. The subsequence of steps taken by each process must conform to the algorithm of that process. Each read of a register returns the value that was most recently written there (or the initial value of the register if no write to it has occurred). If P is a set of processes, a P -only execution is an execution in which only processes in P take steps. A solo execution by a process p is a {p}-only execution. We use  ·  to denote the concatenation of the finite execution  and the (finite or infinite) execution  . A configuration is a complete description of the system at some time. It is comprised of the internal state of each process and the value stored in each shared register. A configuration C is reachable if there is an execution from an initial configuration that ends in C . In an execution, two operation instances are called concurrent if neither one ends before the other begins. We consider processes that may fail by halting. An algorithm is wait-free if every non-faulty process completes its tasks within a finite number of its own steps, no matter how processes are scheduled or which other processes fail. A stronger version of the wait-freedom property, called bounded wait-freedom, requires that the number of steps be bounded. A much weaker progress property is obstruction-freedom, which requires that each process must complete its task if it is given sufficiently many consecutive steps. In our algorithms, each register need only be large enough to store one timestamp. For our lower bounds, we assume that each register can hold arbitrarily large amounts of information. In our algorithms, we use the convention that

The Space Complexity of Unbounded Timestamps

227

shared registers have names that begin with upper-case letters and local variables begin with lower-case letters. If R is a set or array of registers, we use Collect(R) to denote a read of each register in R, in some unspecified order. Our lower bounds use covering arguments, introduced by Burns and Lynch [4]. We say a process p covers a register R in a configuration C if p will write to R when it next takes a step. A set of processes P covers a set of registers R in C if |P| = |R| and each register in R is covered by exactly one process in P . If P covers R, a block write by P is an execution in which each process in P takes exactly one step writing its value.

3

The Timestamp Problem

A timestamp implementation provides two algorithms for each process: GetTS and Compare. GetTS takes no arguments and outputs a value from a universe U . Elements of U are called timestamps. Compare takes two arguments from U and outputs a Boolean value. If an instance of GetTS, which outputs t1 , finishes before another instance, which outputs t2 , begins, then any subsequent instances of Compare(t1 , t2 ) and Compare(t2 , t1 ) must output true and false, respectively. Thus, two non-concurrent GetTS operations cannot return the same timestamp. Unlike the bounded timestamp problem, Compare can compare any previously granted timestamps, so U must be infinite. This definition of the timestamp problem is weak, which makes our lower bounds stronger. It is sufficient for some applications [12], but it is too weak for other applications. For example, consider the implementation of atomic multi-writer registers from single-writer registers [13, 19, 22]. Suppose readers determine which value to return by comparing timestamps attached to each written value to find the most recently written value. If two writers write different values concurrently, and two readers later read the register, the readers should agree on which of the two values to return. To handle this kind of application, we can define a stronger version of the timestamp problem which requires that, for each pair t and t , all Compare(t, t ) operations in the same execution must return the same value. A static timestamp algorithm is one that satisfies a still stronger property: for each pair, t and t , the Compare(t, t ) always returns the same result in all executions. Static timestamp algorithms have the nice property that Compare queries need not access shared memory. The algorithms we present in this paper are all static. The lower bounds in Sections 4.1 and 7 apply even for non-static implementations. A natural way to design a static timestamp algorithm is to use timestamps drawn from a partially ordered universe U , and answer Compare queries using that order; Compare(t1 , t2 ) returns true if and only if t1 < t2 . A partially ordered set U is called nowhere dense if, for every x, y  U , there are only a finite number of elements z  U such that x < z < y . The integers, in their natural order, and the set of all finite sets of integers, ordered by set inclusion, are nowhere dense. Any set of fixed-length vectors of integers, where x  y if and only if each component of x is less than or equal to the corresponding component

228

F. Ellen, P. Fatourou, and E. Ruppert

of y is too. However, for k  2, the set of all length-k vectors of integers, ordered lexicographically, is not nowhere dense. Another desirable property is that all timestamps produced are distinct, even for concurrent GetTS operations. In eponymous systems, this property is easy to satisfy by incorporating the process's identifier into the timestamp generated [17]. In anonymous systems, it is impossible, because symmetry cannot be broken using registers.

4

Eponymous Lower Bounds

We prove lower bounds on the number of registers needed to implement timestamps eponymously. First, we give the most general result of the paper, proving  that  ( n) registers are needed. Then, we prove a tight lower bound of n if the timestamps are chosen from a partially ordered set that is nowhere dense. 4.1 A General Space Lower Bound

We use a covering argument, showing that, starting from a configuration where some registers are covered, we can reach another configuration where more registers are covered. The following lemma allows us to do this, provided the original registers are covered by three processes each. The complement of a set of processes S is denoted by S . Lemma 1. Consider any timestamp algorithm. Suppose that, in a reachable configuration C , there are three disjoint sets of processes, P1 , P2 , and Q that each cover the set of registers R. Let Ci be the configuration obtained from C by having the processes in Pi do a block write, i , for i = 1, 2. Then for all disjoint sets S1  P2  Q and S2  P1  Q, with some process not in S1  S2 , there is an i  {1, 2} such that every Si -only execution starting from Ci that contains a complete GetTS writes to a register not in R. Proof. Suppose there exist disjoint sets S1  P2  Q and S2  P1  Q, an S1 only execution 1 from C1 and an S2 -only execution 2 from C2 that both write only to registers in R, and q  / S1  S2 . Also suppose 1 and 2 contain complete instances of GetTS, I1 and I2 , that return t1 and t2 , respectively. Let  be an execution starting from C that begins with a block write to R by Q, followed by a solo execution in which q performs a complete instance of Compare(t1 , t2 ). Then, 1 · 1 · 2 · 2 ·  and 2 · 2 · 1 · 1 ·  are valid executions starting from C that are indistinguishable to q . Hence, in both, q returns the same result for Compare(t1 , t2 ). This is incorrect, since I1 precedes I2 in 1 · 1 · 2 · 2 ·  , but I2 precedes I1 in 2 · 2 · 1 · 1 ·  . Theorem 2. Every obstruction-free timestamp algorithm for n processes uses more than 1 2 n - 1 registers. Proof. First, we show that at least one register is required. To derive a contradiction, suppose there is an implementation that uses no shared registers. Let

The Space Complexity of Unbounded Timestamps

229

 and  be solo executions of GetTS by different processes, p and q , starting from the initial configuration. Suppose they return timestamps t and t . Let  be a solo execution of Compare(t, t ) by p immediately following . Since  ·  ·  is indistinguishable from  ·  ·  to p, it must return the same result for Compare(t, t ) in both. However, it must return true in  ·  ·  and false in  ·  ·  . This is a contradiction. This suffices to prove the claim for n  4. For the remainder of the proof, we assume that n  5. Consider any timestamp algorithm that uses r > 0 registers. To derive a contradiction, assume r   1 2 n - 1. We show, by repeated applications of Lemma 1 that it is possible to reach a configuration where all r registers are covered by three processes each. One further application of Lemma 1 will then show that some process must write to some other register, to produce the desired contradiction. We prove the following claim by induction on k . Claim: For k = 1, . . . , r, there is a reachable configuration with k registers each covered by r - k + 3 processes. Base case (k = 1): Let p1 , p2 and q be any three processes. Applying Lemma 1 with initial configuration C , R = , P1 = P2 = Q = , S1 = {p1 }, and S2 = {p2 } proves that the solo execution of GetTS by either p1 or p2 must write to some register. Thus, all except possibly one process must write to a register during a solo execution of GetTS starting from C . Consider an execution consisting of the concatenation of the longest write-free prefixes of n - 1 of these solo executions. In the resulting configuration, there are n - 1 processes covering registers. Since there are r registers and n - 1  (2r)2 > r(r + 1), there is some register that is covered by at least r + 2 = r - k + 3 processes. Induction Step: Let 1  k  r - 1 and suppose the claim is true for k . Let C be a reachable configuration in which there is a set R of k registers that are each covered by r - k + 3  3 processes. Let P1 , . . . , Pr-k+3 be disjoint sets that each cover R with |Pi | = k for all i. Divide the n-(r -k +3)k processes not in P1 · · ·Pr-k+3 into two sets, U1 and U2 , each containing at least (n - (r - k + 3)k )/2 processes. Let S1 = P1  U1 and S2 = P2  U2 . Then S1  P2  P3 and S2  P1  P3 are disjoint. Since |P3 | = k  1, there is a process q  P3 - (S1  S2 ). For i = 1, 2, let Ci be the configuration obtained from C by having the processes in Pi do a block write. By Lemma 1, there exists i  {1, 2} such that every Si -only execution starting from Ci that contains a complete GetTS writes to a register not in R. Let m = |Si |. We inductively define a sequence of solo executions 1 , 2 , . . . , m by each of the processes of Si such that 1 · 2 · · · m is a legal execution from Ci that does not write to any registers outside R and each process covers a register not in R. Let 1  j  m. Assume that 1 , . . . , j -1 have already been defined and satisfy the claim. Consider the Si -only execution  = 1 · 2 · · · j -1 ·  from Ci , where  is a solo execution by another process pj  Si that contains a complete GetTS operation. Then  must include a write by pj to a register outside R during . Let j be the prefix of  up to, but not including, pj 's first write outside of R. This has the desired properties.

230

F. Ellen, P. Fatourou, and E. Ruppert

Let C be the configuration reached from Ci by performing the execution 1 ·2 · · · m . Then at C , each process in Si covers one of the r -k registers not in R and |Si |  k + (n-(r -k +3)k )/2  ((2r)2 -(r -k +1)k )/2 > (r -k )(r -k +1), since 2r > 2r - k, r - k + 1 > 0. Thus, by the pigeonhole principle, some register R not in R is covered by at least r - k + 2 processes. Let R = R  {R}. Each register in R is covered by one process from each of P3 , . . . , Pr-k+3 and P3-i . Thus, each of the k + 1 registers in R is covered by r - k + 2 processes in the configuration C , proving the claim for k + 1. By induction, there is a reachable configuration in which all r registers are covered by three processes each. By Lemma 1, there is an execution in which a process writes to some other register. This is impossible. The first paragraph of the proof also shows that, if a timestamp algorithm uses only single-writer registers, then at most one process never writes and, hence, at least n - 1 single-writer registers are necessary. 4.2 A Tight Space Lower Bound for Static Algorithms Using Nowhere Dense Universes

We now turn to the special case where timestamps come from a nowhere dense partial order, and Compare operations can be resolved using that order, without accessing shared memory. The following theorem provides a tight lower bound, since it matches a standard timestamp algorithm [17]. Theorem 3. Any static obstruction-free timestamp algorithm that uses a nowhere dense partially ordered universe of timestamps requires at least n registers. Proof. We prove by induction that, for 0  i  n, there is a reachable configuration Ci in which a set Pi of i processes covers a set Ri of i different registers. Then, in configuration Cn , there are processes poised at n different registers. Base Case (i = 0): Let C0 be the initial configuration and let P0 = R0 = . Inductive Step: Let 1  i  n. Assume Ci-1 , Ri-1 and Pi-1 satisfy the claim. If i = 1, let p be any process. Otherwise, let p  Pi-1 . Consider an execution  that starts from Ci-1 with a block write by the processes in Pi-1 to the registers of Ri-1 , followed by a solo execution by p in which p completes its pending operation, if any, and then performs GetTS, returning some timestamp t. Let q be a process not in Pi-1  {p}. We show that a solo execution by q , starting from Ci-1 , in which it performs an infinite sequence of GetTS operations must eventually write to a register not in Ri-1 . Let tj be the timestamp returned by the j 'th instance of GetTS by q in this solo execution. Then tj < tj +1 for all j  1. Since {j  N | t1 < tj < t} is finite, there exists j  N such that tj < t. Suppose that q does not write to any register outside Ri-1 during the solo execution,  , of j instances of GetTS, starting from Ci-1 . Then  ·  is indistinguishable from  to p, so p returns t as the result of its last GetTS in  · . Therefore, tj < t. This contradicts the definition of j , so q must write outside Ri-1 . Consider the solo execution of q starting from Ci-1 until it first

The Space Complexity of Unbounded Timestamps

231

Code for process pi (for 1  i  n - 1): GetTS t  max(Collect(R)) + 1 R[i]  t return (t, 0)

Code for process pn : GetTS t  max(Collect(R)) if t > oldt then c  0 c c+1 oldt  t return (t, c)

Fig. 1. An eponymous algorithm using n - 1 registers

covers some register R outside Ri-1 . Let Ci be the resulting configuration. Then Pi = Pi-1  {q } and Ri = Ri-1  {R} satisfy the claim for i. Jayanti, Tan and Toueg proved that linearizable implementations of perturbable objects require at least n - 1 registers [16]. Roughly speaking, an object is perturbable if some sequence of operations on the object by one process must be visible to another process that starts executing later. General timestamps do not have this property. However, the proof technique of [16] can be applied to the special case considered in Theorem 3 (even though timestamps are not linearizable). The proof technique used in Theorem 3 is similar to theirs, but is considerably simpler, and gives a slightly stronger lower bound. Although our improvement to the bound is small, it is important, since it proves a complexity separation, showing that using nowhere dense sets of timestamps requires more registers than used by the algorithm of the next section.

5

An Eponymous Algorithm

In this section, we show that there is a simple wait-free eponymous algorithm that uses only n - 1 single-writer registers, which is optimal. The timestamps generated will be ordered pairs of non-negative integers, ordered lexicographically. This shows that the lower bound in Sect. 4.2 is not true for all domains. The algorithm uses an array R[1..n - 1] of single-writer registers, each initially 0. Processes p1 , . . . , pn-1 use this array to collaboratively create the first component of the timestamps by the simple method [17] discussed in Sect. 1. The second component of any timestamp they generate is 0. The last process, pn , reads the registers of the other processes to determine the first component of its timestamp, and produces the values for the second component of its timestamp on its own. Process pn does not write into shared memory. The implementation of GetTS is presented in Figure 1. In the code for pn , oldt and c are persistent variables, initially 0. Compare((t1 , c1 ), (t2 , c2 )) returns true if and only if either t1 = t2 and c1 < c2 or t1 < t2 . The value stored in each component of R does not decrease over time. So, if two non-concurrent Collects are performed on R, the maximum value seen by the later Collect will be at least as big as the maximum value seen by the earlier Collect. Theorem 4. Figure 1 gives a timestamp algorithm using n - 1 registers with step complexity O(n).

232

F. Ellen, P. Fatourou, and E. Ruppert

Proof. Suppose an instance, I1 , of GetTS returns (t1 , c1 ) before the invocation of another instance I2 of GetTS, returns (t2 , c2 ). We show that Compare((t1 , c1 ), (t2 , c2 )) returns true. We consider several cases. Case 1: I1 and I2 are both performed by pn . It follows from the code that pn generates an increasing sequence of timestamps (in lexicographic order): each time pn produces a new timestamp, it either increases the first component or leaves the first component unchanged and increases the second component. Case 2: pn performs I2 but some process pi = pn performs I1 . During I2 , the value pn sees when it reads R[i] is at least t1 , so t2  t1 . Furthermore, c2  1 > 0 = c1 . Case 3: I2 is not performed by pn . Then t1 was the value of some component of R some time before the end of I1 (because it was either read by pn while performing I1 , or was written by another process while performing I1 ). The value of this component of R is at least t1 when I2 reads it, so t2  t1 + 1. In all three cases, a Compare((t1 , c1 ), (t2 , c2 )) will return true, as required. Since R has n - 1 components, the step complexity of GetTS is O(n).

6

Anonymous Algorithms

We present two new anonymous timestamp algorithms. The first uses n registers and, as we shall see in Sect. 7, it is space-optimal. However, this algorithm, like Guerraoui and Ruppert's algorithm [12], is not bounded wait-free. The second algorithm uses O(n2 ) registers, but it is bounded wait-free. It is an open question whether there is a bounded wait-free algorithm that uses O(n) registers. 6.1 A Wait-Free Algorithm Using n Registers

The first algorithm uses an array A[1..n] of registers, each initially 0. The timestamps are non-negative integers. Before a process returns a timestamp, it records it in A so that subsequent GetTS operations will see the value and return a larger one. We ensure this by having a process choose its timestamp by reading all timestamps in A and choosing a larger one. The anonymity of the algorithm presents a challenge, however. In a system with only registers, two processes running in lockstep, performing the same sequence of steps, have the same effect as a single process: there is no way to tell these two executions apart. Even the two processes themselves cannot detect the presence of the other. Consider an execution where some process p takes no steps. We can construct another execution where p runs as a clone of any other process q , and p stops taking steps at any time, covering any register that q wrote to. Thus, at any time, a clone can overwrite any value written in a register (except the first such value) with an older value. In the timestamp algorithm, if the value t chosen by one process and recorded in A is overwritten by values smaller than t, another process that begins performing GetTS after the value t has been chosen could again choose t as a timestamp, which would be incorrect. To avoid this problem, we ensure that the evidence of a timestamp cannot be entirely overwritten after GetTS returns it. We say that a value v is established

The Space Complexity of Unbounded Timestamps GetTS t  max(Collect(A)) + 1 for i  1..M (t) for j  1..n if A[j ] < t then A[j ]  t end for end for return(t) Fig. 2. A wait-free anonymous timestamp algorithm using n registers

233

in configuration C if there exists a shared register that, in every configuration reachable from C , contains a value larger than or equal to v . (Note that, if a value larger than v is established, then v is also established.) Once a value v is established, any subsequent GetTS can perform a Collect of the registers and see that it should return a value greater than v . Thus, our goal is to ensure that values are established before they are returned by GetTS operations. The algorithm, shown in Fig. 2, uses several measures to do this. The first is having processes read a location before writing it and never knowingly overwrite a value with a smaller value. This implies a value in a register is established whenever there are no processes covering it, poised to write smaller values. This measure alone is insufficient: if p writes to a register between q 's read and write of that register, q may overwrite a larger value with a smaller one. However, it limits the damage that a process can do. Another measure is for GetTS to record its output in many locations before terminating. It also writes to each of those locations repeatedly, using a larger number of repetitions as the value of the timestamp gets larger. The number of repetitions, M (t), that GetTS uses to record the timestamp t, is defined recursively by M (1) = 1 and M (t) = t-1 n(n - 1) i=1 M (i) for t > 1. Solving this recurrence yields M (t) = n(n - 2 1)(n -n+1)t-2 for t > 1. The Compare(t1 , t2 ) algorithm simply checks whether t1 < t2 . Correctness follows easily from the following lemma. Lemma 5. Whenever GetTS returns a value t, the value t is established. Theorem 6. Figure 2 gives a wait-free anonymous timestamp algorithm using n registers. When GetTS returns t, it performs (n2t-1 ) steps. Thus, the algorithm is waitfree, but not bounded wait-free. In an execution with k GetTS operations, all timestamps are at most k , since GetTS can choose timestamp t only if another (possibly incomplete) GetTS has chosen t - 1 and written it into A. Each of the n registers must contain enough bits to represent one timestamp. 6.2 A Bounded Wait-Free Algorithm Using O (n2 ) Registers

The preceding algorithm is impractical because of its time complexity. Here, we give an algorithm that runs in polynomial time and space. As in the preceding algorithm, timestamps are non-negative integers and a process chooses

234

F. Ellen, P. Fatourou, and E. Ruppert GetTS t  max(max(Collect(A), t) + 1 row  t mod (2n - 1) for i  1..n A[row, i]  t if max(Collect(A))  t + n - 1 then return(t) end for return(t)

Fig. 3. A bounded wait-free anonymous timestamp algorithm using O(n2 ) registers

a timestamp that is larger than any value recorded in the array A. However, now, A[0..2n - 2, 1..n] is a two-dimensional array of registers and the method for recording a chosen value in A is quite different. Before a process p returns a timestamp t, it writes t into the entries of one row of the array, chosen as a function of t. A careful balance must be maintained: p should not write too many copies of t, because doing so could overwrite information written by other, more advanced processes, but p must write enough copies to ensure that t is not expunged by other, less advanced processes. Process p attempts to write t into all entries of one row, but stops writing if it sees value t + n - 1 or larger anywhere in the array. We show that, if this occurs, then another process q has already returned a timestamp larger than t. (In that case, q will have already ensured that no future GetTS will ever return a value smaller than its own timestamp, so p can safely terminate and return t.) This avoids the problem of p writing too many copies of t. To avoid the problem of p writing too few copies of t, the rows are chosen in a way that ensures that one value cannot be overwritten by another value unless those two values are sufficiently far apart. This ensures that other processes will terminate before obliterating all evidence of the largest timestamp written in A. The algorithm is presented in Fig. 3. In addition to the shared array A, each process has a persistent local variable t, initialized to 0. Again, Compare(t1 , t2 ) is performed by simply checking whether t1 < t2 . We remark that, if a value v > 0 is written into A, then v - 1 appeared in A earlier. The correctness of the algorithm follows easily from the lemma below. Lemma 7. Whenever GetTS returns a value t, the value t is established. Proof. We prove the lemma by induction on the number of return events. Base case: If no return events have occurred, the lemma is vacuously satisfied. Induction step: Let k > 0. Assume that, at each of the first k - 1 return events, the returned value is established. Consider the configuration C just after the k th return event, in which process p returns t. We show t is established in C by considering two cases, depending on the termination condition that p satisfies. Case 1: Suppose p returns t because it saw some value m  t + n - 1 in A. Some process wrote m before p read it. It follows that each of the values t, t +1, t +2, . . . , t + n - 1, . . . , m appeared in A at some time during the execution

The Space Complexity of Unbounded Timestamps

235

before C . For 1  i  n - 1, let pi be the process that first wrote the value t + i into A. These processes do not include p, since p returns t at configuration C . If all of these n - 1 processes are distinct, then no process will ever write a value smaller than t after C , so t is established. Otherwise, by the pigeonhole principle, pi = pj for some i < j . Process pi must have completed the instance of GetTS that first wrote t + i before it began the instance of GetTS that first wrote t + j . The former instance returns t + i, so the value t + i is established when it is returned, by the induction hypothesis. Thus, in C , the value t + i is established and, hence, so is the value t. Case 2: Suppose p terminates after it has completed all n iterations of the loop. If t < 2n - 1, in the first loop iteration of the GetTS that returns t, p writes t into A[t, 1]. No value smaller than t can ever be written there, so t is established. Now assume t  2n - 1. The values t - 1, t - 2, . . . , t - n were written into A prior to the completion of p's first Collect. For 0  i < n, let pi be the process that first wrote the value t - n + i into A. If pi = p for some i, then p returned t - n + i before starting the instance of GetTS that returned t, and the value t - n + i is established, by the induction hypothesis. Otherwise, by the pigeonhole principle, we must have pi = pj for some 0  i < j < n. When process pi first wrote t - n + i, it returns t - n + i, that value is established, by the induction hypothesis. In either case, some value greater than or equal to t - n is established by the time that p completes its first Collect. Hence, t - n is also established. We show no process writes values smaller than t in row t mod (2n - 1) more than once after p's first write of t. Suppose not. Let q be the process that first does a second such write. Suppose the first such write by q writes the value t1 < t and the second writes the value t2 < t. Then t1  t - (2n - 1), since t1 mod (2n - 1) = t mod (2n - 1) and t1 < t. Similarly, t2  t - (2n - 1). When q performs Collect just after it writes t1 , it sees a value t - n or larger in A, since t - n is established. Furthermore, t - n  (t1 + 2n - 1) - n = t1 + n - 1 and the loop terminates. So, when q writes t2 , that write is part of a different instance of GetTS. Again, when q performs Collect in the first line of that instance of GetTS, it must see a value t - n or larger, since t - n is established. Thus, t2  t - n + 1, contradicting the fact that t2  t - 2n. Thus, when p returns t, it has written the value t into all n entries of row t mod (2n-1) of A and at most n-1 of those copies are subsequently overwritten by smaller values. So, t is established. The worst-case running time of GetTS is O(n3 ), since each Collect takes (n2 ) steps. Timestamps are bounded by the number of GetTS operations invoked, and each register must be large enough to contain one timestamp. Theorem 8. Figure 3 gives a wait-free anonymous timestamp algorithm using O(n2 ) registers with step complexity O(n3 ).

236

F. Ellen, P. Fatourou, and E. Ruppert

7

A Tight Space Lower Bound for Anonymous Algorithms

The anonymous timestamp algorithm given in Sect. 6.1 uses n registers. In it, a process may write its timestamp value to each of the n registers. Intuitively, this is done to ensure that other processes, which could potentially cover n - 1 of the registers, cannot overwrite all evidence of the timestamp. Here, we sharpen this intuition into a proof that at least n registers are required for anonymous timestamp algorithms. This applies to obstruction-free implementations of timestamps (and therefore to wait-free implementations). Lemma 9. Let n  2. In any anonymous obstruction-free timestamp implementation for n processes, a solo execution of k  n instances of GetTS, starting from an initial configuration, writes to at least k different registers. Proof. Suppose not. Consider the smallest k such that there is a solo execution of k  n instances of GetTS by a process p, starting from an initial configuration, which writes to a set R of fewer than k different registers. Let  be the prefix of this execution consisting of the first k - 1 instances of GetTS. By definition of k , it writes to at least k - 1 different registers. Thus, |R| = k - 1 and R is the set of registers written to during . Let C be the configuration immediately after the last write in  (or the initial configuration, if there are no writes in ). We define another execution  . First, add clones of p to execution , such that one clone continues until just before p last writes to each register in R. Let q be the last of these clones to take a step. (If R is empty, then let q be any process other than p.) Then p performs one more instance of GetTS after those it performed in . Let t be the value returned by this operation. Note that p only writes to registers in R. Next, let the clones do a block write to R. Let C be the configuration immediately after the block write. Finally, q runs solo to complete its operation, if necessary, and then does one more GetTS. Each register has the same value in configurations C and C and p's state in C is the same as q 's state in C . Thus, q 's steps after C will be identical to p's steps after C , and q 's last GetTS will also return t. This is a contradiction, since that operation begins after p's last GetTS, which also returned t, ended. Theorem 10. Any n-process anonymous obstruction-free timestamp algorithm uses at least n registers. Acknowledgements. We thank Rachid Guerraoui for helpful discussions. Funding was provided by the Natural Sciences and Engineering Research Council of Canada and by the Scalable Synchronization Group at Sun Microsystems.

References
[1] Abrahamson, K.: On achieving consensus using a shared memory. In: Proc. 7th ACM Symposium on Principles of Distributed Computing, pp. 291­302 (1988) [2] Afek, Y., Dolev, D., Gafni, E., Merritt, M., Shavit, N.: A bounded first-in, firstenabled solution to the l-exclusion problem. ACM Transactions on Programming Languages and Systems 16(3), 939­953 (1994)

The Space Complexity of Unbounded Timestamps

237

[3] Attiya, H., Fouren, A.: Algorithms adapting to point contention. Journal of the ACM 50(4), 444­468 (2003) [4] Burns, J., Lynch, N.: Bounds on shared memory for mutual exclusion. Information and Computation 107(2), 171­184 (1993) [5] Charron-Bost, B.: Concerning the size of logical clocks in distributed systems. Information Processing Letters 39(1), 11­16 (1991) [6] Dolev, D., Shavit, N.: Bounded concurrent time-stamping. SIAM Journal on Computing 26(2), 418­455 (1997) [7] Dwork, C., Herlihy, M., Plotkin, S., Waarts, O.: Time-lapse snapshots. SIAM Journal on Computing 28(5), 1848­1874 (1999) [8] Dwork, C., Waarts, O.: Simple and efficient bounded concurrent timestamping and the traceable use abstraction. Journal of the ACM 46(5), 633­666 (1999) [9] Fatourou, P., Fich, F.E., Ruppert, E.: Time-space tradeoffs for implementations of snapshots. In: Proc. 38th ACM Symposium on Theory of Computing, pp. 169­178 (2006) [10] Fich, F., Herlihy, M., Shavit, N.: On the space complexity of randomized synchronization. Journal of the ACM 45(5), 843­862 (1998) [11] Fidge, C.: Logical time in distributed computing systems. Computer 24(8), 28­33 (1991) [12] Guerraoui, R., Ruppert, E.: Anonymous and fault-tolerant shared-memory computing. Distributed Computing. A preliminary version appeared in Distributed Computing. In: 19th International Conference, pp. 244­259, 2006 (to appear) [13] Haldar, S., Vit´ anyi, P.: Bounded concurrent timestamp systems using vector clocks. Journal of the ACM 49(1), 101­126 (2002) [14] Israeli, A., Li, M.: Bounded time-stamps. Distributed Computing 6(4), 205­209 (1993) [15] Israeli, A., Pinhasov, M.: A concurrent time-stamp scheme which is linear in time and space. In: Proc. 6th Int. Workshop on Distributed Algorithms, pp. 95­109 (1992) [16] Jayanti, P., Tan, K., Toueg, S.: Time and space lower bounds for nonblocking implementations. SIAM Journal on Computing 30(2), 438­456 (2000) [17] Lamport, L.: A new solution of Dijkstra's concurrent programming problem. Communications of the ACM 17(8), 453­455 (1974) [18] Lamport, L.: Time, clocks and the ordering of events in a distributed system. Communications of the ACM 21(7), 558­565 (1978) [19] Li, M., Tromp, J., Vit´ anyi, P.M.B.: How to share concurrent wait-free variables. Journal of the ACM 43(4), 723­746 (1996) [20] Mattern, F.: Virtual time and global states of distributed systems. In: Proc. Workshop on Parallel and Distributed Algorithms, pp. 215­226 (1989) [21] Mavronicolas, M., Michael, L., Spirakis, P.: Computing on a partially eponymous ring. In: Proc. 10th International Conference on Principles of Distributed Systems, pp. 380­394 (2006) [22] Vit´ anyi, P.M.B., Awerbuch, B.: Atomic shared register access by asynchronous hardware. In: Proc. 27th IEEE Symposium on Foundations of Computer Science, pp. 233­243. IEEE Computer Society Press, Los Alamitos (1986)

Approximating Wardrop Equilibria with Finitely Many Agents
Simon Fischer , Lars Olbrich , and Berthold V¨ ocking
Dept. of Computer Science, RWTH Aachen, Germany {fischer,lars,voecking}@cs.rwth-aachen.de

Abstract. We study adaptive routing algorithms in a round-based model. Suppose we are given a network equipped with load-dependent latency functions on the edges and a set of commodities each of which is defined by a collection of paths (represented by a DAG) and a flow rate. Each commodity is controlled by an agent which aims at balancing its traffic among its paths such that all used paths have the same latency. Such an allocation is called a Wardrop equilibrium. In recent work, it was shown that an infinite population of users each of which carries an infinitesimal amount of traffic can attain approximate equilibria in a distributed and concurrent fashion quickly. Interestingly, the convergence time is independent of the underlying graph and depends only mildly on the latency functions. Unfortunately, a direct simulation of this process requires to maintain an exponential number of variables, one for each path. The focus of this work lies on the distributed and efficient computation of the adaptation rules by a finite number of agents. In order to guarantee a polynomial running time, every agent computes a randomised path decomposition in every communication round. Based on this decomposition, agents remove flow from paths with high latency and reassign it proportionally to all paths. This way, our algorithm can handle exponentially large path collections in polynomial time.

1

Introduction

We consider routing problems in the Wardrop model. We are given a network equipped with non-decreasing latency functions mapping flow on the edges to latency. For each of several commodities a fixed flow rate has to be routed from a source to a sink via a collection of paths. A flow vector is said to be at Wardrop equilibrium if for all commodities the latencies of all used paths are minimal with respect to this commodity. Whereas such equilibria can be formulated as
Supported by DFG grant Vo889/1-3 and by DFG through German excellence cluster UMIC at RWTH Aachen. Supported by the DFG GK/1298 "AlgoSyn" Supported in part by the the EU within the 6th Framework Programme under contract 001907 (DELIS) and by DFG through German excellence cluster UMIC at RWTH Aachen.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 238­252, 2007. c Springer-Verlag Berlin Heidelberg 2007

Approximating Wardrop Equilibria with Finitely Many Agents

239

convex programs (under some mild assumptions on the latency functions) and can thus be solved by centralised algorithms in polynomial time, in this work, we study distributed algorithms to compute Wardrop equilibria. A common interpretation of the Wardrop model is that flow is controlled by an infinite number of selfish agents each of which carries an infinitesimal amount of flow. In [12] it was shown that in this setting such a population approaches Wardrop equilibria quickly by following a simple round-based load-adaptive rerouting policy. This policy, called the replication policy, is executed by all agents in parallel and proceeds in the following way. Each agent samples another agent at random and, if this improves the latency, migrates to this agent's path with a probability that increases with the latency gain. In this setting, a natural goal is to reach approximate equilibria in the following bicriterial sense. We say that a flow is at  - -equilibrium if at most an -fraction of the flow utilises paths whose latency exceeds the average latency of their commodity by more than a  -fraction of the overall average latency. Remarkably, the number of rounds to reach an approximate equilibrium in this sense is independent of the size and the topology of the underlying graph and chiefly depends on the approximation parameters and the elasticity of the latency functions. In this work, we consider a different setting, in which the flow is controlled by a finite number of agents only, each of which is responsible for the entire flow of one commodity. Each agent has a set of admissible paths among which it may distribute its flow. To be able to represent exponentially large collections of paths we assume that these are represented by an arbitrary DAG connecting the source and the sink of the agent. Each agent aims at balancing its own flow such that the jointly computed allocation will be at Wardrop equilibrium. Let us remark that agents do not aim at minimising the overall latency of their flow, but seek to minimise the maximum latency of their commodity. Unfortunately, the replication policy does not yield a feasible distributed algorithm in this setting directly. Simulating an infinite number of agents each of which chooses one out of the given collection of paths would require maintaining one variable for each path and computing a quadratic number of migration rates between pairs of paths. As the number of paths may be exponential in the size of the network this approach is rendered computationally infeasible. We present two approaches to circumvent this problem. Our first approach exploits the fact that, for a simplified variant of the replication policy, the updates of the edge flows can be expressed in a way that merely uses the edge flow variables themselves (rather than the path flow variables). Thus, the updates can be computed in polynomial time. Unfortunately, the convergence time of this variant is only pseudopolynomial in the latency functions since it depends on the maximum slope of the latency functions. Since the original replication policy cannot be expressed in this compact way, we consider a second approach to achieve convergence in a polynomial number of communication rounds. Consider a collection of paths for one of the commodities. In a first step, our algorithm samples a polynomial number of paths with probability proportional to their flow. We thus obtain a randomised path

240

S. Fischer, L. Olbrich, and B. V¨ ocking

decomposition. We consider paths in this decomposition with above-average latency. From such paths, a fraction of the flow is removed and reallocated proportionally among all admissible paths. If this is done carefully, oscillations can be avoided, and a potential function argument ensures convergence towards Wardrop equilibria. Thus, we achieve essentially the same convergence rates as in the setting with an infinite number of agents and keep the computation time of one communication round polynomial. Altogether, we can compute approximate Wardrop equilibria in polynomial time. 1.1 Related Work

The game theoretic traffic model considered in this paper was introduced by Wardrop [19]. Many aspects of Wardrop equilibria have been studied, the most prominent being the degradation of performance due to the selfish behaviour, called the price of anarchy [18] as well as the inverse, the increase of the maximum latency incurred to an agent due to optimal routing [17]. It has also been shown that the price of anarchy can be decreased by imposing taxes on the edges [9,14]. Cominetti etal. consider the price of anarchy in a model with finitely many agents aiming at minimising their average latency [10]. For solving the corresponding classical goal of finding a minimum cost multicommodity flow, several algorithms are known. For an overview see, e. g., [1] and [7]. An efficient distributed steepest-descent algorithm for solving multicommodity flow problems with linear latency functions has been presented recently in [3]. In [2], a stateless algorithm for this problem is presented. It is also known that the problems of finding an optimal allocation and finding a Wardrop equilibrium are essentially equivalent. Under mild conditions on the latency functions, a flow at Wardrop equilibrium with respect to so-called marginal-cost latency functions is optimal with respect to the original latency functions, see e. g. [5] and [18]. Several authors (e. g. [4,8]) consider dynamic routing from an online-learning perspective. Awerbuch and Kleinberg [4] present an algorithm for the online shortest path problem in an end-to-end feedback model. Blum etal. [8] show that approximate Wardrop equilibria defined in a similar way can be attained if the agents follow no-regret algorithms. Their bounds on the convergence time depend polynomially on the regret bounds and network size and depend pseudopolynomially on the maximum slope of the latency functions. The problem of load-balancing has also been studied in various discrete settings for networks of parallel links. For the case of identical links, both sequential [15] and concurrent distributed algorithms were considered [6]. Even-Dar et al. [11] consider distributed algorithms for load balancing on links with speeds using sampling rules which depend pseudopolynomially on the speed of the links. The rerouting policy upon which our algorithms are based was introduced in [13] and [12]. It was shown that an infinite number of agents executing this policy can attain a Wardrop equilibrium quickly in a concurrent setting.

Approximating Wardrop Equilibria with Finitely Many Agents

241

2
2.1

Model and Problem Statement
Wardrop's Traffic Model

We consider Wardrop's traffic model originally introduced in [19]. We are given a graph G = (V, E ) with non-decreasing differentiable latency functions e : R0  R0 . Furthermore, we are given a set of commodities [k ] = {1, . . . , k } specified by source-sink-pairs (si , ti )  V × V , a directed acyclic subgraph Gi of G connecting si and ti and flow demands ri  R0 . The total demand is r = i[k] ri , and we normalise r = 1 for simplicity. Let Pi denote the admissible paths of commodity i, i. e., all paths connecting si and ti in Gi , and let P = i[k] Pi . We may assume that the sets Pi are disjoint and define iP to be the unique commodity to which path P belongs. A non-negative path flow vector (fP )P P is feasible if it satisfies the flow demands P Pi fP = ri for all i  [k ]. We denote the set of all feasible flow vectors by F . A path flow vector (fP )P P induces an edge flow vector f = (fe,i )eE,i[k] with fe,i = P Pi :eP fP . The total flow on edge e is fe = f . Furthermore, for v  V and i  [k ], the total flow of comi[k] e,i modity i through node v is fv,i = (u,v )E f(u,v ),i = (v,w )E f(v,w ),i for v  / {si , ti } and fsi ,i = fti ,i = ri . The latency of an edge e  E is given by e (fe ) and the latency of a path P is given by the sum of the edge latencies P (f ) = eP e (fe ). Finally, the weighted average latency of commodity i  [k ] is given by Li (f ) = eE e (f ) · (fe,i /ri ) and the overall average latency is L(f ) = eE e (f ) · fe /r. We drop the argument f of (·) and L(·) whenever it is clear from the context. A flow vector in this model is considered stable when no fraction of the flow can improve its sustained latency by moving unilaterally to another path. This implies that all used paths must have the same minimal latency. Unused paths may have larger latency. Definition 1 (Wardrop equilibrium). A feasible flow vector f is at Wardrop equilibrium if for every commodity i  [k ] and paths P1 , P2  Pi with fP1 > 0 it holds that P1 (f )  P2 (f ). It is well-known that Wardrop equilibria are exactly those allocations that minimise the following potential function introduced in [5]: (f ) =
eE 0 fe e (u) du

.

This potential precisely absorbs progress: If an infinitesimal amount of flow dx is shifted from path P to Q , thus improving its latency by ( P - Q ), the potential decreases by ( P - Q ) dx. We will make use of this fact frequently. The minimum potential is denoted by  = minf F (f ). Every flow vector f with (f ) =  is then at Wardrop equilibrium. We assume that  is positive. The case that  = 0 can be treated by adding virtual offsets to the latency functions. For a detailed treatment see [12].

242

S. Fischer, L. Olbrich, and B. V¨ ocking

Let us remark, that the problems of computing a Wardrop equilibrium and computing a flow minimising L are equivalent. It is sufficient to replace the latency functions e by so-called marginal-cost latency functions he (x) = (x · e (x)) = e (x) + x · e (x). If for all e  E , x · e (x) is convex, then Wardrop equilibria with respect to (he )eE minimise L [5,18]. The algorithms presented in this paper will compute approximate equilibria in the following bicriterial sense. Definition 2 ( - -equilibrium). Consider a flow vector f and let Pi = {P  Pi | P (f ) > Li (f ) +  L(f )} denote the set of  -expensive paths. A flow vector is at a  - -equilibrium if i[k] P P  fP  .
i

This definition of approximate Wardrop equilibria requires that almost all flow utilises paths with a latency that is close to the average of their own commodity. A similar definition of approximate Nash equilibria is used, e. g., in [8]. 2.2 Elasticity of Latency Functions

Our algorithms take the steepness of the latency functions into account when deciding how much flow to shift from one path to another. In [12] it was shown that the critical parameter in this setting is not the slope but the elasticity. Definition 3. For any positive differentiable function (x ) ticity of at x is d(x) = x· (x ) . : R0  R0 , the elas-

In other words, the elasticity of a function is bounded from above by d if the (absolute) slope at any point is at most by a factor of d larger than the slope of the line connecting the origin and the point (x, (x)). Note that a polynomial with positive coefficients and degree d has elasticity at most d, hence, elasticity can be considered as a generalisation of the degree of such a polynomial. The function a · exp( x), x  [0, 1] has maximum elasticity . 2.3 Implicit Path Decomposition

Wardrop equilibria are defined with respect to path flows. Our algorithms, however, will make use only of the edge flow vectors, which do not determine a vector of path flows uniquely. However, in a DAG, an edge flow vector (fe )eE induces a natural vector of path flows by starting with the flow injected at the source, and splitting the flow at each node v such that the set of paths containing the outgoing edge e receives a flow proportional to fe . Since the decomposition for one commodity i  [k ] is independent of the flow of other commodities, we can omit the index i for simplicity. Definition 4. Consider any edge flow vector (fe )eE (for some commodity i). For any path P = (v1 , . . . , vl ) let
l- 1

~ f P = fv1 ·
j =1

f(vj ,vj+1 ) . fvj

Approximating Wardrop Equilibria with Finitely Many Agents

243

It is easily verified by induction on the distance from the source that this is ~ actually a valid flow decomposition of (fe )eE , i. e., fe = P e f P. 2.4 Distributed Computation Model

Our algorithms operate in the following setting. Agents operate in a synchronous, round-based fashion. We assume that there is a billboard via which the agents are able to share information. On this billboard, each agent can observe the edge flows of its own commodity and the latency values of the paths it uses. Agents know an upper bound d on the elasticity of the latency functions, but they do not know the latency functions themselves. However, it is easily possible to extend our algorithm such that it does not rely on the knowledge of a bound on the elasticity. In every round an agent can update the edge flows of its own commodity on the billboard. These updates become visible to all agents only in the next round. All agents execute the same algorithm in parallel. Therefore, in the descriptions of our algorithms, we may omit the index for the commodity, i. e., fe refers to the flow fe,i of commodity i on edge e.

3

A Pseudopolynomial Algorithm

Our first approach works by simulating the replication policy presented in [12]. We will see that this can be done in polynomial time although this policy operates on an exponential number of paths. 3.1 The Replication Policy

Let us start by introducing the replication policy formally. We consider an infinite population of agents each of which controls an infinitesimal amount of flow which it assigns to a path. In each round agents may migrate their flow from the current path to another one. Consider an agent in commodity i  [k ] currently using path P  Pi . Whenever activated, it performs two steps. 1. Sampling. Sample another path Q where the probability to sample any path Q equals fQ /ri . 2. Migration. There are two cases: (a) Q  P . In this case, the agent stays with its old path. (b) Q < P . The agent migrates to the sampled path Q with probability  · ( P - Q ) for some constant  > 0 to be determined later. Altogether, we can characterise our policy by specifying the rate of agents migrating from one path P  Pi to another path Q  Pi with Q (f ) < P (f ) within one round. This rate can be obtained by multiplying the probabilities speciefied in steps (1) and (2) with the volume of agents using path P . For this rate we obtain fQ · ( P - Q) P Q =  · f P · ri

244

S. Fischer, L. Olbrich, and B. V¨ ocking

if Q < P and P Q = 0 otherwise. Thus, we can compute a sequence of flow vectors (fP (t))P P generated by this policy by summing over all paths Q: fP (t + 1) = fP (t) +
QPi

QP -
QPi

P Q
Q

= fP (t) +  fP
QPi

fQ ( ri
P)

-

P)

= fP (t) +  fP (Li - 3.2 Convergence Towards Equilibria

.

(1)

For the time being assume that agents are migrating in a continuous fashion as described by the above rules. Then, an infinitesimal amount of flow dx migrating from a path P to another path Q improving its latency from P to Q causes the potential  to reduce by ( P - Q ) dx. Since we only accept migrations that improve the latency, this implies that the potential always decreases which in turn implies convergence towards a Wardrop equilibrium by Lyapunov's direct method if all paths are used in the initial flow. However, in our concurrent roundbased model, flow is not shifted continuously, but in finite chunks. Thus, if these chunks are chosen too large, overshooting and oscillation effects may occur. This issue can be resolved by choosing the migration rate in step 2b of the replication policy carefully. In [13] it was shown that if we choose  = (1/ max ) small enough with max = max max e (f ) ,
P P f F eP

convergence towards Wardrop equilibria can be guaranteed. We may assume that max > 0 since otherwise all latency functions are constant and our problem can be solved trivially by assigning the entire flow to the path with lowest latency. Theorem 1 ([13,12]). If  = (1/ max ) sufficiently small, the replication policy given by Equation (1) with initial flow f (0) = f 0 converges towards a War0 drop equilibrium if fP > 0 for all P  P . Furthermore, the number of rounds in which the flow is not at a  - -equilibrium is O 1 · 2 2
max min

· ln

(f 0 ) 

.

One may observe that the ratio between maximum slope and minimum latency used in this theorem depends on the scale by which we measure flow. This scale, however, is fixed since we have normalised the total flow demand to be r = 1. 3.3 Simulating the Replication Policy

By a naive application of Theorem 1 we can compute a sequence of flow vectors (f (t))t0 according to Equation (1) to obtain approximate Wardrop equilibria.

Approximating Wardrop Equilibria with Finitely Many Agents

245

However, this approach is rendered computationally intractable by the fact that there may be an exponential number of variables fP . In the following, we describe an algorithm that computes the iterative change ~ described rates of the edge flows according to the implicit flow decomposition f in the preceding section. To that end, we show that the change rates of the edge flows fe can be expressed solely in terms of edge flows and edge latencies (i. e., without explicit reference to the fP variables). It suffices to know the weighted average latencies of all paths containing e defined as Le =
P e

fP · fe

P

Recall that we have fixed a commodity here, so we may drop the index i. Lemma 1. Consider an edge flow vector (fe (t))eE and its path decomposition ~(t), and let f ~(t + 1) denote the flow generated by the replication policy in Equaf ~(t). Finally, let fe (t + 1) = ~ tion (1) from f P e fP (t + 1). Then, fe (t + 1) = fe (t) +  · fe · (L - Le ) . Proof. Let f = f (t) and f = f (t + 1). By definition of fe , fe - fe =
P e

fP - fP =  ·
P e

fP · (L -

P)

=  · fe · L -

P e

fP fe

P

,

where the last term equals Le . In order to obtain the value of Le , we implicitly compute the path decomposition ~, i. e., for every edge e we compute the flow caused by paths containing e on f edge e . This is done by Algorithm SimulatedReplication (Algorithm 1) in time O (m) for every edge e  E . Since there are m edges, each iteration can be performed in time O m2 . Algorithm 1. SimulatedReplication() (executed by all commodities in parallel; (fe )eE denotes the edge flows vector of commodity i)
1: for all edges e  E do 2: sort all edges (v, w) in the subgraph reachable from e topologically (v,w) (u,v ) ~ ~ = (u,v)E f · 3: compute total flow of all paths containing e and (v, w) f e e 4: reverse all edges and repeat steps 2 and 3 for edges between e and s
fe f(v,w) fv

5: compute Le = e fee e 6: fe  fe +  · fe · (L - Le ) with  = 1/ max 7: end for 8: replace (fe )eE on the billboard with (fe )eE

246

S. Fischer, L. Olbrich, and B. V¨ ocking

Corollary 1. The sequence of flow vectors computed by Algorithm SimulatedReplication converges towards the set of Wardrop equilibria. Furthermore, the number of rounds in which the flow is not at a  - -equilibrium with respect ~, is bounded by to f O 1 · 2 2
max min

· ln

(f 0 ) 

,

where f 0 is the initial flow vector. Each iteration takes time O m2 . Proof. Lemma 1 implies that the edge flow vector computed by the algorithm equals the edge flow vector obtained by applying the replication policy given by ~)P P . Combining this with the upper Equation (1) to the path decomposition (f bounds on the convergence time given in [12], the claim follows.

4

The Polynomial Time Algorithm

The migration probability specified for step 2b of the replication policy can get very small since the latency difference P - Q may become small in relation to  if  is chosen constant. This causes the algorithm to obtain only a pseudopolynomial convergence time depending on the maximum slope of the latency functions. In this section we present an approach that gets rid of this dependence. To this end, we choose the amount of flow removed from a path proportional to its relative deviation ( P - LiP )/ P from the average and the reciprocal of the elasticity d to obtain a polynomial number of communication rounds. Whereas in the preceding section the amount of flow removed or added to a path within one round could be expressed in a nice closed form as  · fP · (L - P ) (Equation (1)), this is now no longer possible. To compute flow updates in polynomial time we use a randomised flow decomposition. First we sample a path at random according to the implicit path ~, i. e., the probability to sample path P is f ~ decomposition f P /riP . Since the length of a path is bounded by n this is possible in time n log n by representing adjacent nodes and their flows in a binary tree. Now, the path is assigned a certain flow volume fP . For the time being, assume that we assign the entire bottleneck flow to P . Then, if P has latency above LiP , we remove a portion of x =  fP ·
P

- LiP d P

of its flow and distribute it proportionally among all admissible paths, i. e., after removing a flow of x from path P , the flow on every edge e  E is increased by (fe,i /ri ) · x. Why does this process decrease the potential quickly? As long as we are not at a  - -equilibrium, the probability of sampling a  -expensive path is at least . In this case, the latency gain and thus the potential gain per flow unit will be ~ large and proportional to f P . If we sample only a single path, we may in fact

Approximating Wardrop Equilibria with Finitely Many Agents

247

assign the entire bottleneck flow to it. We can lower bound the probability that this bottleneck flow is not too small (Lemma 2). To increase the potential gain we repeat this process several times. Doing this, we can no longer assign the entire bottleneck to a path since it may happen that an edge is sampled several times. Hence, we assign at most a (1/ log m) fraction of the bottleneck while at the same time sampling T = m log m paths rather than only a single one. It thus becomes unlikely that an edge becomes empty along the way if its flow is O (1/m). In order to achieve the same result for edges with larger flow, we limit the amount of flow consumed in one step to O (1/(m log m)). More precisely, let e = min 1 fe , 7 m log m 7 log m .

We start with an empty decomposition. In a round in which path P is sampled we increase fP by e where e is a bottleneck edge in P . We say that an edge is alive if the overall flow assigned to paths containing e is at most fe - e (i. e. it can be sampled one more time without having our decomposition exceeding the flow of e). Our algorithm terminates as soon as there are any edges that are not alive. The final algorithm RandomisedBalancing(d) is described in Algorithm 2. Under the assumption that the latency functions are constant, we can thus show that the potential decreases in every round by a factor that only depends on and  , and the elasticity d (Lemma 5). We furthermore show that due to our careful migration rate the potential gain with respect to the true latency functions is still at least half of the potential gain with respect to constant latencies (Lemma 4). Finally, we show that the expected potential gain implies a bound on the time to reach a minimum potential (Lemma 6). Altogether, this yields the following upper bound for our algorithm. Theorem 2. The sequence of flow vectors computed by Algorithm RandomisedBalancing converges towards the set of Wardrop equilibria. Furthermore, the expected number of rounds in which the flow is not at a  - -equilibrium with ~, is bounded by respect to f O d
3 2

log

(f0 ) 

,

if d is an upper bound on the elasticity of the latency functions. The computation time of each round is bounded by O (n log n · m log m). We present the proof after establishing the necessary lemmas. Note that our algorithm can be easily modified for the case that the elasticity of the latency functions is not known to the algorithm in advance. 4.1 Randomised Decomposition

Our algorithm generates a randomised flow decomposition using a sampling pro~. In this section, we lower bound the probability that the bottlecess based on f neck flows of the sampled paths are not too small. Furthermore, we show that the flow removed from every edge is at most fe with high probability.

248

S. Fischer, L. Olbrich, and B. V¨ ocking

Algorithm 2. RandomisedBalancing(d) (executed by all commodities in parallel; (fe )eE denotes the edge flows vector of commodity i)
1: for T = m log m times do ~ P 2: sample a path P where P [P ] = f ri  3: let e denote the bottleneck edge of P ; let fP = e 4: if P > Li then P -Li 5: reduce the flow on all edges e  P by fP = fP · 4 d P 6: if for any e  P , e is not alive then 7: abort loop and continue in line 11 8: end if 9: end if 10: end for e · P Pi fP 11: increase the flow on all edges e  E proportionally by f ri

Lemma 2. Consider a flow vector f of volume 1 and a set of paths P with ~ ~ P  P  mineP fe  2 m  2 . P P fP = . Then, PP f Proof. We consider a scaled flow vector which supports only paths in P . fP = P P 0 P  /P .
~ f P

Observe that the total volume of f is 1 again, hence PP f [P = Q] = PP f ~ [P = Q | P  P ]. Now, P P f ~ P  P  min fe 
eP

2m

= P P f ~ [ P  P ] · P P f ~ min fe 
eP

1 | P P 2m (2)

= · P P f

min fe 
eP

1 ,  2m 2

where the first equality uses the definition of f and the second one uses the above observation. Now, let d(x, y ) denote the number of edges of a shortest path connecting x and y . We can show that P [e = (v, w)  P ] = fe by induction on d(s, v ). ~. Now, assume that the statement This holds for d(s, v ) = 0 by definition of f holds for all edges (u, v ) with d(s, u) = k and consider an edge e = (v, w) with d(s, v ) = k + 1. fe P [(u, v )  P ] · P [e  P ] = P [v  P ] · P [e  P | v  P ] = fv
(u,v )

=
(u,v )

fe f(u,v) · = fe . fv

With E = {e  E | fe  1/(2 m)}, P [P e:eE]
eE

P [e  P ] 
eE

fe 

1 |E |  . 2m 2

Substituting this into Equation (2) yields our desired bound.

Approximating Wardrop Equilibria with Finitely Many Agents

249

We now consider a sequence of T = m log m rounds. Observe that e is an upper bound on the flow removed from a path containing e by our algorithm, since for the bottleneck edge e , e = mineP {e }. The flow on e may decrease to below zero only if it is contained in the sampled path at least fe /e times. In the following we show that this is unlikely. Lemma 3. With probability 1 -o(1), after a sequence of T = m log m iterations, all edges are still alive. Proof. In the proof of Lemma 2 we have seen that the probability to hit edge e in one round equals fe . Let the random variable X denote the number of hits in T rounds. We have E [X ] = T fe . An edge is alive if X  fe /e - 1. There are two cases: 1. fe <
1 m

implying e = fe /(7 log m). Then, P X> fe - 1 = P X > E [X ] · e  P X > E [X ] ·
6

1 7 - fe m T fe 6 fe m

 2-E[X ]· fe m = m-6 . The first inequality is the definition of T and e and uses our assumption that fe · m < 1, and the second inequality is Chernoff's inequality (which asserts that P [X  r · E [X ]]  2-r·E[X ] for r  6 for a random variable X that is the sum of 0-1 random variables, see [16]). 1 implying e = 1/(7 T ). This case can be treated similarly. 2. fe  m In both cases, the probability that edge e is not alive at the end of a sequence of T iterations is bounded by m-6 . Using a union bound, the probability that at least one edge does not survive is at most m-5 and consequently the probability that all edges survive the sequence is at least 1 - m-5 . 4.2 Lower Bounding the Potential Gain

We use a potential function argument to prove convergence. In order to show that our algorithm avoids oscillations, we consider the potential gain achieved within one round. We show that this potential gain is at least half of the potential gain that would occur if latencies values were fixed at the beginning of a round. A second lemma shows that, in expectation, the potential decreases by a factor in every round, as long as we are not yet at an approximate equilibrium. Lemma 4. Let d denote an upper bound on the elasticity of the latency functions. For a flow vector f consider a flow vector f generated by Algorithm RandomisedBalancing(d) (Algorithm 2) with positive probability. For any P  P let fP denote the amount of flow removed from path P . Then, (f ) - (f )  1 P P ( P (f ) - LiP ) · fP . 2 · Due to space limitations, we defer the proof to the full version.

250

S. Fischer, L. Olbrich, and B. V¨ ocking

Lemma 5. Assume that f is a flow that is not at  - -equilibrium and let the random variable f denote a flow generated by our algorithm. Then, E [(f )]  (f ) · 1 - 
3 2

 d

.

Proof. For the time being, assume that the latency functions are constant. By Markov's inequality, the total volume of flow in commodities with Li > 2 · L/ is at most /2. We consider only commodities with Li  2 · L/ . In total, at least a flow volume of utilises  -expensive paths and there is still at least a volume of /2 left in the commodities we consider. Consider such a commodity i  [k ] and denote the flow volume using  -expensive paths in this commodity by i . Consider any iteration satisfying the precondition that all edges are alive. Let P denote the path sampled by the algorithm. By Lemma 2, the probability that P  Li +  L and the minimum edge flow along P is at least i /(2 m) is at least i /(2 ri ) (we have to scale the flow of this commodity by a factor 1/ri to make it a unit flow). The amount of flow removed from this path by our algorithm is 1 i P - Li i  · ·  2 m 7 log m 4 d P 113 d m log m where we have used that P  Li +  L and Li  2 L/ . The latency gain of this path is then at least  L and since this event happens with probability i /(2 ri ) the expected virtual potential gain of such a path is then at least 2 L . 226 d ri m log m
2 i

By Lemma 3 the probability that in this iteration all edges are alive is 1 - o(1) and the expected potential gain computed above is independent of this event. Summing up over all T = m log m iterations and all commodities, the total expected virtual potential gain of one round is at least (1 - o(1)) ·
i[k] 3 2 2  L . L  (1 - o(1)) · 226 d ri 226 d 2 i

For the last inequality we have used the Cauchy-Schwarz Inequality which asserts  2 b2 that for two vectors (ai ) and (bi ), i a2 i ( i . Using ai = i / ri i ai b i ) /  and bi = ri yields the result. This implies the claim since L is an upper bound on  and Lemma 4 ensures that the true potential gain with respect to the real latency functions is at least half of the potential gain with respect to the constant latency functions. 4.3 From Expected Potential Gain to Expected Stopping Time

The preceding section has shown that in every round the potential decreases by a factor in expectation. Intuitively, this implies an expected running time that is logarithmic in this factor and the initial values. This intuition is made precise by the following lemma.

Approximating Wardrop Equilibria with Finitely Many Agents

251

Lemma 6. Let X0 , X1 , . . . denote a sequence of non-negative random variables and assume that for all i  0 E [Xi | Xi-1 = xi-1 ]  xi-1 ·  for some constant   (0, 1). Furthermore, fix some constant x  (0, x0 ] and let  be the random variable that describes the smallest t such that Xt  x . Then, E [ | X0 = x0 ]  x0 2 log(1/) · log x . We defer the proof to the full version. Finally, we can proof our main result. Proof (Proof of Theorem 2). Let f0 , f1 , . . . denote a sequence of flow vectors generated by Algorithm 2. Lemma 5 implies that E [(ft+1 ) | (ft ) = ]   · 1 - 
3 2

 d

.

Thus, the sequence ((ft ))t0 satisfies the conditions of Lemma 6 and the expected time until (ft ) reaches its minimum  implying that ft is a  - equilibrium is 2 log 1-
3

d

2

-1

log

(f0 ) 

=O

d
3 2

log

(f0 ) 

,

our desired bound. One path can be sampled in time O (n log n), the bottleneck edge can be found in time O (n), and the flow update can be computed in time O (n). Altogether, at most T = m log m iterations have to be computed. Finally, the removed flow can be reinserted in time O (n).

5

Open Problems

Our algorithm works by redistributing flow of overloaded paths. To identify such paths we face the subproblem of finding a flow decomposition that assigns much flow to paths with high latency. In our algorithm we have used a randomised path decomposition to achieve this goal. It is not obvious whether this randomisation can be avoided, and, in fact, naive deterministic approaches like longest path first decompositions fail. In the long run, our algorithm converges towards the set of Wardrop equilibria. A weakness of our notion of approximate equilibria, however, is the fact that the average latency may be arbitrarily far away from the minimum latency. As an alternative, one could also consider deviations from the minimum latency rather than from the average latency. It is unclear whether convergence towards approximate equilibria in this sense can be guaranteed in polynomial time. Furthermore, it would be desirable to design specialised (not necessarily distributed) algorithms to compute (exact) Wardrop equilibria that improve upon the standard solution via convex programming.

252

S. Fischer, L. Olbrich, and B. V¨ ocking

References
1. Ahuja, R.K., Magnanti, T.L., Orlin, J.B.: Network flows: Theory, algorithms and applications. Prentince-Hall, Englewood Cliffs (1993) 2. Awerbuch, B., Khandekar, R.: Greedy distributed optimization of multi-commodity flows. In: Proc. 26th Ann. Symp. on Principles of Distributed Computing (PODC) (2007) 3. Awerbuch, B., Khandekar, R., Rao, S.: Distributed algorithms for multicommodity flow problems via approximate steepest descent framework. In: Proc. 18th Ann. Symp. on Discrete Algorithms (SODA) (2007) 4. Awerbuch, B., Kleinberg, R.D.: Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In: Proc. 36th Ann. Symp. on Theory of Comput. (STOC), pp. 45­53 (2004) 5. Beckmann, M., McGuire, C.B., Winsten, C.B.: Studies in the Economics of Transportation. Yale University Press, New Haven and London (1956) 6. Berenbrink, P., Friedetzky, T., Goldberg, L.A., Goldberg, P., Hu, Z., Martin, R.: Distributed selfish load balancing. In: Proc. 17th Ann. Symp. on Discrete Algorithms (SODA) (2006) 7. Bertsekas, D.P.: Network Optimization: Continuous and Discrete Models. Athena Scientific (1998) 8. Blum, A., Even-Dar, E., Ligett, K.: Routing without regret: On convergence to Nash equilibria of regret-minimizing algorithms in routing games. In: Proc. 25th Ann. Symp. on Principles of Distributed Computing (PODC), pp. 45­52. ACM, New York (2006) 9. Cole, R., Dodis, Y., Roughgarden, T.: How much can taxes help selfish routing? In: Proc. 4th Conf. on Electronic Commerce, pp. 98­107 (2003) 10. Cominetti, R., Correa, J.R., Moses, N.E.S.: Network games with atomic players. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006. LNCS, vol. 4052, pp. 525­536. Springer, Heidelberg (2006) 11. Even-Dar, E., Mansour, Y.: Fast convergence of selfish rerouting. In: Proc. 16th Ann. Symp. on Discrete Algorithms (SODA), pp. 772­781 (2005) 12. Fischer, S., R¨ acke, H., V¨ ocking, B.: Fast convergence to Wardrop equilibria by adaptive sampling methods. In: Proc. 38th Symposium on Theory of Computing (STOC), pp. 653­662. ACM, New York (2006) 13. Fischer, S., V¨ ocking, B.: Adaptive routing with stale information. In: Aguilera, M.K., Aspnes, J. (eds.) Proc. 24th Symp. on Principles of Distributed Computing (PODC), pp. 276­283. ACM, New York (2005) 14. Fleischer, L.: Linear tolls suffice: New bounds and algorithms for tolls in single source networks. In: D´ iaz, J., Karhum¨ aki, J., Lepist¨ o, A., Sannella, D. (eds.) ICALP 2004. LNCS, vol. 3142, pp. 544­554. Springer, Heidelberg (2004) 15. Goldberg, P.W.: Bounds for the convergence rate of randomized local search in a multiplayer, load-balancing game. In: Proc. 23rd Symp. on Principles of Distributed Computing (PODC), pp. 131­140. ACM, New York (2004) 16. Hagerup, T., R¨ ub, C.: A guided tour of Chernoff bounds. Information Processing Letters 33, 305­308 (1990) 17. Roughgarden, T.: How unfair is optimal routing? In: Proc. 13th Ann. Symp. on Discrete Algorithms (SODA), pp. 203­204 (2002) ´ How bad is selfish routing? J. ACM 49(2), 236­259 18. Roughgarden, T., Tardos, E.: (2002) 19. Wardrop, J.G.: Some theoretical aspects of road traffic research. In: Proc. of the Institute of Civil Engineers, Pt. II, vol. 1, pp. 325­378 (1952)

Energy and Time Efficient Broadcasting in Known Topology Radio Networks
Leszek G¸ asieniec1 , , Erez Kantor2, , Dariusz R. Kowalski1, , David Peleg2 , , and Chang Su1
Department of Computer Science, The University of Liverpool, Liverpool L69 3BX, UK {leszek,darek,suc}@csc.liv.ac.uk Department of Computer Science and Applied Mathematics, The Weizmann Institute of Science, Rehovot, 76100 Israel {erez.kantor,david.peleg}@weizmann.ac.il
1

2

Abstract. The paper considers broadcasting protocols in radio networks with known topology that are efficient in both time and energy. The radio network is modelled as an undirected graph G = (V, E ) where |V | = n. It is assumed that during execution of the communication task every node in V is allowed to transmit at most once. Under this assumption it is shown that any radio broadcast protocol requires  D +  ( n - D ) transmission rounds, where D is the diameter of G. This lower bound is complemented with an efficient construction of a deterministic protocol that accomplishes broadcasting in D + O( n log n) rounds. Moreover, if we allow each node to transmit at most k times, the lower bound D +  ((n - D)1/(2k) ) on the number of transmission rounds holds. We also provide a randomised protocol that accomplishes broadcasting in D + O(kn1/(k-2) log2 n) rounds. The paper concludes with a discussion of several other strategies for energy efficient radio broadcasting and a number of open problems in the area.

1
1.1

Introduction
Background

This paper concerns the study of simultaneously energy and time efficient communication protocols under an abstract model of radio networks, where uniform transmitting and receiving devices form a set of nodes V in an undirected graph G = (V, E ) of size |V | = n. Two nodes v, w  V are neighbours in G, i.e., there is an edge (v, w) in E , whenever v and w can communicate (i.e., send and receive messages) directly with each other. Nodes that are not connected by edges must communicate via intermediate nodes. We consider synchronous networks, where the processing and transmission speeds of nodes are uniform across the entire
Supported in part by the Royal Society grant Algorithmic and Combinatorial Aspects of Radio Communication, IJP - 2006/R2. Supported in part by grants from the Minerva Foundation and the Israel Ministry of Science.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 253­267, 2007. c Springer-Verlag Berlin Heidelberg 2007

254

L. G¸ asieniec et al.

network. Communication is performed in rounds. During any round, each node can be either in a transmitting mode or in a receiving mode, meaning that a node cannot both transmit and receive messages during the same round. Moreover, if a node v transmits in a given round, the message is delivered to all its neighbours. However, a node w in a receiving mode in a given round will receive the message from its transmitting neighbour v if and only if v is the only transmitting neighbour of w in this round. The efficiency of communication protocols in synchronous networks is often expressed as the time (the number of rounds) required to accomplish the task. In this paper, apart from the time complexity we are also interested in another important efficiency measure, namely the energy efficiency. We consider strategies for efficient radio communication in the context of the broadcasting task. In the broadcasting problem, a distinguished node s in the network, referred to as the source node, has a message that has to be distributed to all other nodes in the network. Energy efficient radio broadcasting was mostly studied in the context of geometric networks, where the network nodes are embedded into 2-dimensional plane. In particular, the goal in the energy efficient broadcast tree problem is to find a transmission graph that minimizes the total power consumption and contains a directed spanning tree rooted at the source node s. The problem is known to be NP-hard [8] and if the distance function is arbitrary, it has no logarithmic factor approximation unless P = N P [16]. For the Euclidean distance model, Wan et al. [22] and with improved reasoning Klasing et al. [17] argued that the algorithm that computes a minimal spanning tree for the set of nodes yields the approximation ratio 12.15. The approximation ratio was further reduced to 7.6 by Flammini et al. [10] and later to 6.33 by Navarra [21]. Recently Amb¨ uhl [1] showed that the minimum spanning tree yields an approximation ratio 6, which is as far as one can go with this approach, in view of the lower bound presented in [22]. In our model of radio communication spatial information is not available, thus there is a need for an alternative definition of energy consumption. Since the network nodes are uniform, it is natural to assume that transmissions performed by every node cost exactly the same. Moreover, we are interested in balancing the energy consumption at the nodes. This would serve to avoid energy consumption bottlenecks at some wireless nodes, especially if they operate on limited power sources, e.g., batteries, and may thus help to prolong the operational lifetime of the entire system. With this goal in mind, we consider energy efficient strategies in which every node is allowed to transmit at most once during the execution of any communication task, in our case the broadcasting procedure. We refer to such energy efficient strategies as 1-shot protocols. Such a strategy for energy efficient radio communication was very recently studied in the context of broadcasting and gossiping in radio networks of random topology, see [5]. Research on time efficient broadcasting in known topology radio networks, where an entire schedule of node transmissions can be precomputed in advance, was initiated in [6]. In this paper Chlamtac and Weinstein provided a broadcast schedule with the running time O(D log2 n), where D is diameter of the network.

Energy and Time Efficient Broadcasting

255

Later an  (log2 n) time lower bound was proved for the family of radius 2 graphs [3]. While it was known for quite a while [4] that for every n-vertex radio network of diameter D there exists a deterministic broadcasting schedule of length O(D log n + log2 n), an appropriate efficient construction for such a schedule was proposed only very recently in [18]. Another type of a broadcast schedule requiring D + O(log5 n) rounds is due to Gaber and Mansour, see [11]. Elkin and Kortsarz in [9] presented deterministic constructions of broadcasting schedules of length D + O(log4 n) for arbitrary graphs and D + O(log3 n) for planar graphs. The existential proof that the optimal D + O(log2 n) broadcast schedule is feasible was given by Gasieniec et al. in [12]. Explicit constructions log3 n of broadcasting schedules operating in O(D + log2 n) and D + O( log log n ) rounds can be found in [19] and [7] respectively. 1.2 Our Results

In this paper we focus on simultaneously time and energy efficient broadcasting protocols in radio networks with known topology. In  Section 2 we show that any 1-shot radio broadcast protocol requires D +  ( n - D) rounds of transmission, where D is the diameter of G. In Section 3 we provide an efficient construction of a deterministic protocol that accomplishes broadcast ing in D + O( n log n) rounds. Section 4 contains results in the model where each node is allowed to transmit at most k times. We prove a lower bound D +  ((n - D)1/(2k) ) and we design a randomised protocol that accomplishes broadcasting in D + O(kn1/(k-2) log2 n) rounds. Finally in Section 5 we discuss several other strategies for energy efficient radio broadcasting and state a number of open problems in the area. All algorithms presented in the paper are deterministic and constructible in polynomial time.

2

 A D +  ( n - D ) Lower Bound for 1-Shot Broadcasting

In this section we show that there exist radio networks in which every 1-shot broadcasting strategy requires at least D +  ( n - D) rounds of transmissions. Specifically, we show that for any positive integer n there exists an n -node bi partite graph on which any 1-shot broadcasting protocol requires  ( n) communication rounds. Consider the binomial graph B (x) = ({r}  U  L, E ), where {r}, U and L are disjoint, |U | = x, |L| = y , and y = x 2 . The singleton set {r} is intended as the source while the set U (respectively, L) forms the upper (resp., lower) layer of B (x). The nodes in U are labelled by the integers 1 to x and the nodes in L by unordered pairs {a, b} such that 1  a < b  x. The node r is connected to all the nodes in U , and a node in L labelled by {a, b} is connected to exactly two nodes a and b in U. For example, see structures of B (3) and B (4) in Figures 1(a) and 1(b) respectively. In the first step, the message is transmitted by r to reach all the nodes in U . Our analysis concerns the process by which the message is disseminated from the nodes of U to the nodes of L.

256

L. G¸ asieniec et al.

The proof is based on the observation that in any 1-shot broadcasting protocol, exactly one node from the upper layer U is permitted to transmit in each round. Consider the first round of any broadcasting procedure. Assume first that two nodes a and b in U decide to transmit simultaneously. Their shared neighbour {a, b} in the lower layer L gets neither of the messages due to collision. Moreover, this node will not receive any other messages in the future since its only neighbours already transmitted and they are allowed to do this only once. This proves that multiple transmissions in the first round are not allowed. Now assume that during the first round a single node a from U transmits and all neighbours of a in L receive the broadcast message. Removing a from U and all its neighbours from L, we obtain a smaller binomial graph B (x - 1). Thus the next round of the broadcasting protocol must again consist of a transmission from a single node in U . This argument is repeated until a binomial graph B (1) with no edges between the layers U and L is obtained. This leads to the conclusion that any 1-shot broadcasting protocol in a binomial graph B (x) requires x - 1 consecutive transmission rounds. Since n = x + y + 1 for y = x 2 , the number  of communication rounds required by any 1-shot broadcasting procedure is  ( n). Theorem 1. There exists a radio network of size  n and diameter D in which any 1-shot broadcasting protocol requires D +  ( n - D) transmission rounds.  Proof. The lower bound of D + ( n - D) can be derived directly from the lower bound for 1-shot broadcasting strategies in binomial bipartite graphs. Consider a graph formed by attaching a path P of length D - 2 to the node r of the binomial bipartite graph B (x), where x is the largest integer satisfying D - 2 + x(x + 1)/2  n, and placing the broadcasting source at the far (from B (x)) end of the path P . The time required by any 1-shot broadcasting procedure includes D - 2 rounds to move the broadcast message along P and  ( n - (D - 2)) additional rounds to inform every node in the  lower layer of B (x). The lower bound D - 2 +  ( n - (D - 2)) = D +  ( n - D) follows.

3

 1-Shot Broadcasting in D + O( n log n) Rounds

For ease of presentation, we first provide a 1-shot radio broadcasting strategy for all n-node bipartite graphs (with  the message already available at all nodes of the upper layer), consisting of O( n) transmission rounds. This result is then combined with the new ranking  scheme given in Section 3.2, and admits 1-shot radio broadcasting in D + O( n log n) rounds for arbitrary undirected graphs. 3.1 Broadcasting in Bipartite Graphs

Consider a bipartite graph B = (U  L, E ) with the upper layer U and the lower layer L. In what follows we assume that U forms a minimal covering set (MCS) of L, see [13], i.e., that the removal of any node from U , along with all edges incident to it, isolates some nodes in L. Define another graph G = (U, E ) on the basis of B as follows. For every node v  L of degree at least two, pick two

Energy and Time Efficient Broadcasting

257

a)

r

B(3)
1 2 3

U

L

b)

{2,3} r

{1,3}

{1,2}

G'
permissible independent

B(4)

1

2

1

2

3

4 4 3

U

not independent not permissible

L
{1,2} {2,3} {1,3} {1,4} {2,4} {3,4}

Fig. 1. (a) the structure of B (3);(b) the graph G based on B (4)

arbitrary neighbours of v in U , and add to E an edge between them. Note that by this construction, |E |  |L|. For example, a graph G defined on the basis of the layers U and L in the binomial graph B (4) is presented in Figure 1(b). A set of nodes in an undirected graph is independent if no two nodes in it are directly connected by an edge. For a subset Q  U , let I (Q)  L denote the set containing all nodes in L having exactly one neighbour in Q. We say that a subset Q  U in the bipartite graph B is permissible if simultaneous transmissions from Q inform a subset I (Q)  L and the removal of all nodes in Q from U , along with their incident edges, does not isolate nodes in L \ I (Q). Lemma 1. An independent set of nodes Q in the graph G forms a permissible set in B. Proof. By the definition of I (Q), it suffices to show that removing all nodes in Q from the upper layer U , along with their incident edges, does not isolate any nodes in L \ I (Q). Indeed, consider an arbitrary node v  L with neighbour set A in B . If v has no neighbour in Q, then after the removal, v is still connected to some node in U \ Q. If v has exactly one neighbour in Q, then v  I (Q), hence v / L \ I (Q). Finally, suppose v is connected to at least two nodes in Q. In this case, v must also have another neighbour outside of Q. To see this, note that by construction, the graph G includes an edge between some two nodes of A. Hence if A  Q, then Q fails to be independent in G , leading to contradiction. The following fact, related to the efficient construction of large independent sets, was proved in the context of parallel computing in [14].

258

L. G¸ asieniec et al.

Lemma 2. In a graph G = (U, E ), where |U | = x and |E | = y , one can construct in time polynomial in x + y an independent set Q  U such that x2 |Q|  2y +x . Combining Lemmas 1 and 2 we conclude: Lemma 3. In a bipartite graph B (U L, E ), where |U | = x, |L| = y and x  x2 there exists a permissible transmission set of size at least 2y +x .  y,

We are ready to state the main theorem for 1-shot radio broadcasting strategies in bipartite graphs. Theorem 2. In any n-node bipartite graph, the broadcast message can be dis tributed from the upper layer to the lower layer in O( n) rounds using a 1-shot broadcasting strategy. Proof. Assume that U  U is a subset of nodes in the upper layer that forms a minimal covering set of the lower layer L, where |U | = x and |L| = y. Note  that if x2  2y + x then nodes from U can transmit sequentially in time x = O( n) + x. We first since x2  2y + x < 2(y + x) = 2n. Otherwise, assume that x2 > 2y   show how to reduce the size of the upper layer to y log y in O( n) rounds,   and later how to perform further reductions to obtain a set of size y in O( y) additional rounds. After the reduction process is accomplished, the  nodes still  present in the upper layer transmit sequentially in at most y = O( n) rounds. Assume first that after some number of rounds of the reduction process the size  of the upper layer x is still larger than y log y , where the size of the lower layer is y  y . By Lemma 3, the current set U (after the removal of nodes and edges in the construction process) contains a permissible transmission set of size at x )2 least 2( y +x . We can assume that x  y (the upper layer constitutes a minimal covering set) and consequently that there exists a permissible transmission set )2 of size at least (x 3y . In this case, after one round of transmissions the size of
2

) x the upper layer is reduced to x = x - (x = x (1 - 3 3y y ). Since we assumed   y log y y  x > y log y and also y  y , it follows that x  x (1 - 3y ) = x (1 - log 3 y ). This means that the size of the upper layer can be  reduced in one round by 3 y log y a fraction of 1 - 3y and consequently in at most log y rounds by a constant  fraction, for as long as x > y log y. Thus repeating this reduction process  for O(log y ) times, the size of the upper layer becomes smaller than y log y .  The total number of rounds in the entire reduction process is O( y ), where  y = O( n).  After the size of the upper layer is reduced to y log y , the remaining reduction process is split into 0  i  log log y stages, where during stage i, the size x of  y log y y log y the upper layer U is reduced from at most 2i to at most 2i+1 . Consider an arbitrary stage i, where y  x is the size of the lower layer. By Lemma 3, U 2 contains a permissible transmission set of size at least 2yx+x . Since x  y  y

and x 

 y log y 2i+1

we conclude that there exists a permissible transmission set

Energy and Time Efficient Broadcasting

259

of size

x2 3y

(

 y log y 2 2i+1 ) /3y



log2 y . 22(i+2)

To move from stage i to stage i + 1 one

2

has to remove from the upper layer U at most

y round we know how to remove at least 2log 2(i+2) nodes, the number of rounds   2 y log y 2i+3 y y required to move to stage i + 1 is bounded by 2i+1 / 2log 2(i+2) = log y . Thus the  log log y 2i+3 y total number of rounds of all log log y stages is bounded by i=0 log y =     8 y log log y i 2 = O( y ), and y = O( n). i=0 log y ·

y log y 2i+1

nodes. Since during each

Finally, when the size of the upper layer U becomes smaller than remaining nodes accomplish broadcasting via sequential transmissions. 3.2 Broadcasting in Arbitrary Graphs

 n, the

In this section we introduce a new tree ranking scheme that enables 1-shot broadcasting  protocols in an arbitrary n-node graph G = (V, E ) of diameter D, in D + O( n log n) rounds. The most time-efficient radio broadcasting algorithms in known graphs use the concept of tree ranking, see e.g., [12,19], where the ranks are computed for especially designed BFS spanning tree rooted in the source node s. The algorithms use two types of transmissions. Fast transmissions are performed along paths in the tree containing nodes with the same rank. Slow transmissions are designed to move instances of the broadcast message between nodes with different ranks at neighbouring BFS levels. In this type of radio broadcasting algorithms, most nodes are involved in transmissions of both types. Note, however, that in the setting used in this paper, each node is allowed to transmit at most once, which means that the concepts of fast and slow transmissions have to be unified. We therefore propose a new ranking scheme in which the rank of a node corresponds to the unique number of a round when the node transmits. The new rank is a combination of two types of ranks, external and internal. External ranks. The external rank of every node in the network is computed on the basis of the ranking mechanism proposed in the context of gatheringbroadcasting BFS spanning trees [12]. The nodes in the spanning tree get ranks according to a simple principle. All leaves are assigned the rank 1 and each internal node calculates its rank by looking at the maximum rank among its children. If the maximum rank m occurs in only one child, then the rank of the parent is also set to m; otherwise, the parent gets the rank m + 1. It is known that the rank of the root s in a gathering-broadcasting spanning tree of size n is at most log n. The spanning tree constructed in [12] has also an important property that at any BFS level d, if two nodes v and w as well as their respective (disjoint) parents p(v ) and p(w) share the same rank, then there are no edges in the network G between nodes p(v ) and w as well as between p(w) and v. This property allows simultaneous (collision free) transmissions from the two parents towards their children. Note that nodes having the same rank in the spanning tree form a collection of disjoined paths leading towards the root of the tree. We refer to these paths as chains. The bottom end (from the root) of each path is called a tail of the chain. Note that some chains can contain only singleton nodes.

260

L. G¸ asieniec et al.

For example, all leaves are tails in their chains. Now, if the rank (as defined in [12]) of a node is l, then the node is assigned the external rank lex = 2l - 1 if it is the tail of some chain; otherwise, it gets the external rank 2l. Thus the external rank of the root s in the spanning tree is at most 2 log n. We refer to new shorter paths based on external ranks as channels. Internal ranks. The system of internal ranks is computed on the basis of external ranks and the broadcasting scheme for bipartite graphs provided in Section 3.1. Let d(v ) be the distance between a node v and the root s. Network nodes with the same distance d(·) form BFS layers in G. Assume that all nodes in the network obtained the external rank from the range 1, . . . , 2 log n. The set of network nodes V is partitioned into channels, where Pi,j denotes j th channel containing nodes with rank i. Each channel Pi,j constitutes a supernode in the upper layer of the internal rank bipartite graph BIR (i), for i = 1, . . . , 2 log n. The bottom layer of each BIR (i) is formed of all nodes in the network. A supernode Pi,j is connected to a node w in the lower layer of BIR (i) if there exists a node v  Pi,j such that (v, w)  E, d(w) - d(v ) = 1, and lex (v ) > lex (w). Note that each graph BIR (i), for i = 1, . . . , 2 log n, has at most 2n vertices, since there are at most n supernodes (they are disjoint subsets of nodes from graph G) and at most n nodes in the lower layer of graph BIR (i). In each BIR (i) we apply the broadcasting scheme from Section 3.1, which allocates to each supernode Pi,j in the upper layer number k (of a transmission round) from the range  a unique  1, . . . , f (n) = O( 2n) = O( n). In fact, the number k defines the internal rank lin (v ) for any node v present in the supernode Pi,j . The examples of internal and external ranking can be found in Figure 2. Combined ranking scheme. The new ranking scheme provides a rank to each node v  V based on its BFS layer d(v ), as well as its internal and external ranks, lin (v ) and lex (v ). More precisely, for any node  v  V, where 0  d(v )  D, 1  lex (v )  2 log n and 0  lin (v ) < f (n) = O( n), define the delay factor of v as  (v ) = 2 log n - lex (v ). The new rank of the node v is then l(v ) = d(v ) + 3[ (v )f (n) + lin (v )] . (1) Essentially, the new combined rank l(v ) corresponds to the transmission time of the node v . In other words, in the new broadcasting scheme the node v transmits only once, in round l(v ). This means, e.g., that the  running time of the new broadcasting scheme is trivially bounded by D + O( n log n). What is left to be shown is that the broadcasting scheme works correctly, i.e., that each network node receives the broadcast message on time (before its transmission round). The intuition behind the definition of l(v ) is as follows. The summand d(v ) is necessary since in any case, node v cannot receive the source message faster than its distance from the source. Then, node v may need to wait for some number of rounds in order to avoid collisions, which is expressed by the second summand. The reason for the factor 3 is that in order to avoid collisions between the transmissions of v and those of nodes at distance 2 from it, we allow only nodes of distances 3, 6, 9. . . . from node v to transmit simultaneously with it. To avoid collisions with nodes on the same BFS layer but with a different external

Energy and Time Efficient Broadcasting
d

261

V0
3

edges in G but not in BFS
V2
2

V1
d+1 2

old rank chains longer than 1
V5
2

V3
d+2 1 2

V4

d+3

V6

V7
2 2

V8
1

V9
d+4 1

V10 V11 V12
1 1 1

V13
1

V14
d+5 1

tails

V0
d 5

V1
d+1 4

V2
4

new rank channels
V5
4

V3
d+2 1

V4 P41 P31
4

P42

d+3

V6

3

V7

3

P32 V13
1

V8
1

V9
d+4 1

V10 V11 V12
2

P21

1

1

V14
d+5 1

channels
P11

BIR (3)

P31

P32

BIR (4)

P41

P42

V0

... ...

V8 V9 V10 V11

V12 V13

V14

V6

V3

V7 V8 V0

... ...

V2 V4 V5 V9

... ... V14

Fig. 2. The old ranks, new external ranks and bipartite graphs BIR (3) and BIR (4)

ranking, node v waits for  (v ) "time windows" of f (n) rounds each. Having done this, only nodes with the same external rank as v may interrupt its transmissions, which is dealt with by waiting an additional lin (v ) rounds. The formal analysis of correctness of the algorithm follows. The proof of the following lemma is deferred to the full version of the paper.

262

L. G¸ asieniec et al.

Lemma 4. Each network  node v with 0  d(v )  D, 1  lex (v )  2 log n and 0  lin (v ) < f (n) = O( n) receives the broadcast message prior to the round l(v ) = d(v ) + 3[ (v )f (n) + lin (v )]. The following theorem holds. Theorem 3. In every radio network of size n and  a diameter D there exists a 1-shot broadcasting protocol that runs in D + O( n log n) rounds Proof. Recall that each node v is scheduled to transmit only once during time l(v ) = d(v )+3[ (v )f (n)+ lin (v )], where 0  d(v )  D,  (v ) = 2 log n - lex (v ), 1   lex (v )  2 log n and 0  lin (v ) < f (n) = O( n). Lemma 4 ensures that each node receives the broadcast message before its transmission time. The time complexity follows directly from the new ranking scheme, where the running time is bounded by the ranking of nodes at the  BFS level farthest from the source node s, thus it is not more than D + O( n log n).

4

k-Shot Protocols

A natural extension of 1-shot strategies is a model in which each node in the network can transmit at most k times. We show here that under this assumption, both deterministic and randomised radio broadcasting requires  (n1/(2k) ) transmission rounds in bipartite graphs, and that this lower bound can be nearly matched by a randomised algorithm. These results could also be generalised for networks with a diameter D, in the same fashion as in Sections 2 and 3.2, resulting in the lower bound D +  (n1/(2k) ) and the upper bound D + O(kn1/(k-2) log n). Please note that due to the space limit almost all proofs in this section are deferred to the full version of the paper. 4.1 Lower Bound

The lower bound argument mimics the proof for 1-shot protocols. Theorem 4. There exist bipartite graphs of size n in which any k -shot broadcasting scheme requires  (n1/(2k) ) transmission rounds. Using the same argument as in the proof of Theorem 1 we come to the following conclusion. Corollary 1. There exist bipartite graphs of size n and diameter D in which any k -shot broadcasting scheme requires D +  ((n - D)1/(2k) ) rounds. Moreover, it follows that in order to guarantee fast broadcast in bipartite graphs, namely, in O(polylog n) rounds (or in general graphs in D + O(polylog n) rounds), log log n  log n 2 log n = log n. The same an  ( log log n )-shot protocol must be used, since n bounds hold also for randomised protocols.

Energy and Time Efficient Broadcasting

263

4.2

Randomised k-Shot Protocol

Consider a graph G = (U, L, E ). Assume that n  8 (otherwise there exists a log n direct constant length schedule) and 5  k < 5+log log n + 2. Note that for k  4 one can use efficient deterministic 1-shot protocol described in section 3. And log n for k  5+log log n + 2 the broadcast protocol proposed in this section runs in at most O(log3 n/ log log n) rounds, i.e., almost matches the lower bound  (log2 n) for unbounded energy broadcast, see [3]. We say that an event holds with high probability if the probability is at least 1 - n-c/k , for some constant c  1. We compute first an MCS for |U |  n/2  |L| and then we define a k -shot protocol RandBroadcast(k ) as follows. Let a = k - 2. Note that 3  a < log n 5+log log n . The protocol proceeds in a epochs, where each epoch (apart from the first one) is formed of T = 64n1/a log n consecutive rounds. The first epoch is different and it is executed in (a + 1) · T rounds. We define also a sequence of numbers pi , for 2  i  a, where pi = 16 log n n(i-1)/a and pi = p2 ,
pi i-1 j =2 (1-pj )

i = 2, , otherwise.

Applying the inequality a < a, and consequently pi < (p2 ) following fact holds.

log n 5+log log n i-1

we get pi < 1/2, for every 2  i 
i j =1 (1

< (1/2)i-1 <

- pj ). Therefore the

Fact 5. 0 < pi  pi  1 for every 2  i  a. A pseudocode of the k-shot protocol for a node v  U is presented below. Algorithm RandBroadcast(k ): 1. Epoch 1: ­ Select, one by one, uniformly at random (with repetitions allowed) a + 1 integers from the set {1, 2, . . . , (a + 1) · T }; Let 1 (v ) be the first selected integer. ­ During the period {1, 2, . . . , (a +1) ·T } transmit in rounds with the index corresponding to selected integers. 2. A random bit selection: ­ Select a random bit 2 (v ), set to 1 with probability p2 and to 0 otherwise. ­ For i = 3 to a do: (i) If j (v ) = 0 for every 2  j  i - 1, then select a random bit i (v ), set to 1 with probability pi and to 0 otherwise. 3. For i = 2 to a do: // iterating epochs i = 2, . . . , a If i (v ) = 1 then ­ select uniformly at random an integer 1  i (v )  T , and ­ transmit in round (a + i - 1)T + i (v ).

264

L. G¸ asieniec et al.

Lemma 5. For every v  U , 1. i (v ) = 1 with probability pi , for every 2  i  a; and a 2. j =2 j (v )  1 with probability 1. Note that every node v transmits at most once throughout epochs i = 2, ..., a, and in epoch 1 it performs at most a + 1 additional transmissions. Hence obtain the following corollary. Corollary 2. RandBroadcast(k ) is a k -shot protocol with probability 1. The algorithm runs in time O(k ) at each node. Thus the total number of transmission rounds is bounded by (a + 1) · T + (a - 1) · T = O(kn1/(k-2) log n). We prove here that the k-shot protocol performs radio broadcasting with high probability. We start with two technical observations referring to placing balls in bins. Given x balls and y bins, consider a process in which each ball is placed uniformly and independently in a random bin. This process is used to model random selection of transmitting rounds, where nodes (balls) choose their transmission rounds (bins) randomly. We say that an event is good when more than x/2 bins are occupied, i.e., which where more than x/2 different rounds are selected by a node. Let P be the probability of a good event. Note that a good event admits existence of a round selected by exactly one node among all considered subset of nodes. (This is due to the fact that at least one bin is occupied by exactly one ball). Lemma 6. P  1 -
x/2 j =1 e·(x/2) y x -j

.

Proof. Let j be the number of occupied bins. We have
x/2

P =1-
j =1 x/2

y j

j y

x

x/2

 1-
j =1 x -j

e·y j

j

j · y

x

x/2

= 1-
j =1

ej ·

j y

x -j

1-
j =1

e · (x/2) y

.

Lemma 7. If 8 log n  x  3T /8 and y  T , then with probability at least 1 - 3/n3 there exists a bin with exactly one ball. Proof. It is sufficient to prove that P  1 - 1/n3 , since if the number of occupied bins is larger than x/2 then there is a bin containing exactly one ball. Using Lemma 6 for x balls and y = T bins,
x/2

P 1-
j =1

e · (x/2) y
x/2

x -j

x -1

 1-
= x/2

3eT /16 T  1 - 3/n3 .

1-

3e 16

·

1 3e 3e  1 - 3 · 16 1 - 16

(8 log n)/2

Energy and Time Efficient Broadcasting

265

Lemma 8. Every node w  L gets the source message with high probability. Therefore the following theorem holds. Theorem 6. Algorithm RandBroadcast(k ) is a k -shot protocol and it accomplishes radio broadcasting in O(kn1/(k-2) log n) rounds with high probability. Note that the ranking scheme proposed in Section 3.2 can be used in conjunction with k -shot protocols. I.e., any O(f (n))-time k -shot broadcasting scheme for bipartite graphs admits D + O(f (n) log n) broadcast in arbitrary graphs with the diameter D. In particular, the k -shot randomised protocol RandBroadcast(k ) for bipartite graphs can be extended in polynomial time into a protocol for completing broadcast on an arbitrary D-hop radio network in D +O(kn1/(k-2) log2 n) rounds, with high probability. More precisely, given a arbitrary graph G(V, E ) with diameter D, we construct bipartite graphs BIR (i) as in section 3.2, and for each node v we compute k internal ranks lin (1, v ), lin (2, v ), . . . , lin (k, v ), instead of one internal rank lin (v ), using RandBroadcast(k ) algorithm in place of 1-shot deterministic algorithm. Note that lin (j, v ), for 1  j  k , are now from the range 1, . . . , f (k, n), where f (n, k ) = O(kn1/(k-2) log n) due to theorem 6. Thus combining the concept of external ranks with newly obtained k different internal ranks (there are at most k different rounds in which node v transmits) in the same fashion as in formula (1) in section 3.2 we get the following corollary. Corollary 3. There exists a polynomial time constructable k -shot randomised broadcasting protocol that runs in D + O(kn1/(k-2) log2 n) rounds in every radio network of size n and a diameter D, with high probability.

5

Further Discussion

This paper presents a new broadcasting scheme that performs the communication task under assumption that each node can transmit at most once. It turns out that there is a clear distinction between the model with bounded and unbounded number of transmissions. In the unbounded model the lower bound and the upper bound is known to be D + (log2 n) in view of [3] and  [12] while in n - D). Note the model with unique transmissions the lower bound is D +  (  also that our new 1-shot broadcasting scheme requires D + O( n log n) rounds of communication. This leaves an interesting open problem on the exact complexity of 1-shot broadcasting strategies in radio networks with known topology. Broadcasting with bounded number of transmissions at each node. In the more general case where each node can transmit up to k times during a broadcasting process, it is shown that radio broadcasting requires  (n1/(2k) ) transmission rounds in bipartite graphs, and D + ((n-D)1/(2k) ) in graphs of diameter D. These complexities can be nearly matched by randomised algorithms, however the exact complexity of k -shot broadcasting in bipartite and arbitrary radio networks with known topology remains open (the gap is nearly n1/k ). In particular, a randomised k -shot broadcasting algorithm needs D + O(kn1/(k-2) log2 n)

266

L. G¸ asieniec et al.

rounds to succeed in any D-hop network, with high probability. Constructing, in polynomial time, an efficient k -shot deterministic protocol is another problem that remains open. Broadcasting with the minimum number of transmissions. Another possible strategy for energy efficient radio broadcasting is to minimise the total number of transmissions, without targeting the best possible running time of a broadcasting procedure. In this model one can provide almost immediately an approximate solution based on the efficient computation of minimal connected dominating sets with logarithmic approximation ratio in general graphs [15] and constant approximation ratio in unit disk graphs, see e.g. [2]. An interesting related open problem is to look for trade-offs between the total number of transmissions and the broadcasting time.

Acknowledgments
We would like to thank Andrzej Lingas for valuable discussions on algorithmic issues related to the main themes of this paper.

References
1. Amb¨ uhl, C.: An optimal bound for the MST algorithm to compute energy efficient broadcast trees in wireless networks. In: Caires, L., Italiano, G.F., Monteiro, L., Palamidessi, C., Yung, M. (eds.) ICALP 2005. LNCS, vol. 3580, pp. 1139­1150. Springer, Heidelberg (2005) 2. Amb¨ uhl, C., Erlebach, T., Mihalak, M., Nunkesser, M.: Constant-factor approximation for minimum-weight (connected) dominating sets in unit disk graphs. In: D´ iaz, J., Jansen, K., Rolim, J.D.P., Zwick, U. (eds.) APPROX 2006 and RANDOM 2006. LNCS, vol. 4110, pp. 3­14. Springer, Heidelberg (2006) 3. Alon, N., Bar-Noy, A., Linial, N., Peleg, D.: A lower bound for radio broadcast. J. Computer and System Sciences 43, 290­298 (1991) 4. Bar-Yehuda, R., Goldreich, O., Itai, A.: On the time complexity of broadcasting in radio networks: an exponential gap between determinism and randomization. In: Proc. 5th Symposium on Principles of Distributed Computing(PODC), pp. 98­107 (1986) 5. Berenbrink, P., Cooper, C., Hu, Z.: Energy efficient randomised communication in unknown adhoc networks. In: Proc. 19th ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), pp. 250­259 (2007) 6. Chlamtac, I., Weinstein, O.: The wave expansion approach to broadcasting in multihop radio networks. IEEE Trans. on Communications 39, 426­433 (1991) 7. Cicalese, F., Manne, F., Xin, Q.: Faster centralised communication in radio networks. In: Asano, T. (ed.) ISAAC 2006. LNCS, vol. 4288, pp. 339­348. Springer, Heidelberg (2006) 8. Clementi, A.E.F., Crescenzi, P., Penna, P., Rossi, G., Vocca, P.: On the complexity of computing minimum energy consumption broadcast subgraphs. In: Ferreira, A., Reichel, H. (eds.) STACS 2001. LNCS, vol. 2010, pp. 121­131. Springer, Heidelberg (2001)

Energy and Time Efficient Broadcasting

267

9. Elkin, M., Kortsarz, G.: Improved broadcast schedule for radio networks. In: Proc. 16th ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 222­231 (2005) 10. Flammini, M., Navarra, A., Klasing, R., Perennes, S.: Improved approximation results for the minimum energy broadcasting problem. In: Proc. DIALM-POMC Workshop on Foundations of Mobile Computing, pp. 85­91 (2004) 11. Gaber, I., Mansour, Y.: Centralised broadcast in multihop radio networks. J. Algorithms 46(1), 1­20 (2003) 12. Gasieniec, L., Peleg, D., Xin, Q.: Faster communication in known topology radio networks. In: Proc. 24th Annual ACM Symposium on Principles of Distributed Computing (PODC), pp. 129­137 (2005) 13. Gasieniec, L., Potapov, I., Xin, Q.: Efficient gossiping in known radio networks. In: Kralovic, R., S´ ykora, O. (eds.) SIROCCO 2004. LNCS, vol. 3104, pp. 173­184. Springer, Heidelberg (2004) 14. Goldberg, M., Spencer, T.: An efficient parallel algorithm that finds independent sets of guaranteed size. SIAM J. of Discrete Mathematics 6(3), 443­459 (1993) 15. Guha, S., Khuller, S.: Approximation algorithms for connected dominating sets. Algorithmica 20(4), 374­387 (1998) 16. Guha, S., Khuller, S.: Improved methods for approximating node-weighted Steiner trees and connected dominating sets. Information and Computation 150, 57­74 (1999) 17. Klasing, R., Navarra, A., Papadopoulos, A., Perennes, S.: Adaptive broadcast consumption (abc), a new heuristic and new bounds for the minimum energy broadcast routing problem. Networking, 866­877 (2004) 18. Kowalski, D.R., Pelc, A.: Centralised deterministic broadcasting in undirected multi-hop radio networks. In: Jansen, K., Khanna, S., Rolim, J.D.P., Ron, D. (eds.) RANDOM 2004 and APPROX 2004. LNCS, vol. 3122, pp. 171­182. Springer, Heidelberg (2004) 19. Kowalski, D.R., Pelc, A.: Optimal deterministic broadcasting in known topology radio networks. Distributed Computing 19(3), 185­195 (2007) 20. Mitzenmacher, M., Upfal, E.: Probability and Computing. Cambridge University Press, Cambridge (2005) 21. Navarra, A.: Tighter bounds for the minimum energy broadcasting problem. In: Proc. 3rd International Symposium on Modeling and Optimisation in Mobile, AdHoc and Wireless Networks, pp. 313­322 (2005) 22. Wan, P.J., Calinescu, G., Li, X.Y., Frieder, O.: Minimum-energy broadcast routing in static ad hoc wireless networks. In: Proc. 20th Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM), pp. 1162­1171 (2001)

A Distributed Algorithm for Finding All Best Swap Edges of a Minimum Diameter Spanning Tree
Beat Gfeller1 , Nicola Santoro2, and Peter Widmayer1
1

Institute of Theoretical Computer Science, ETH Zurich, Switzerland {gfeller,widmayer}@inf.ethz.ch 2 School of Computer Science, Carleton University, Ottawa, Canada santoro@scs.carleton.ca

Abstract. Communication in networks suffers if a link fails. When the links are edges of a tree that has been chosen from an underlying graph of all possible links, a broken link even disconnects the network. Most often, the link is restored rapidly. A good policy to deal with this sort of transient link failures is swap rerouting, where the temporarily broken link is replaced by a single swap link from the underlying graph. A rapid replacement of a broken link by a swap link is only possible if all swap links have been precomputed. The selection of high quality swap links is essential; it must follow the same objective as the originally chosen communication subnetwork. We are interested in a minimum diameter tree in a graph with edge weights (so as to minimize the maximum travel time of messages). Hence, each swap link must minimize (among all possible swaps) the diameter of the tree that results from swapping. We propose a distributed algorithm that efficiently computes all of these swap links, and we explain how to route messages across swap edges with a compact routing scheme.

1

Introduction

For communication in computer networks, often only a subset of the available connections is used to communicate at any given time. If all nodes are connected using the smallest number of links, the subset forms a spanning tree of the network. Depending on the purpose of the network, there is a variety of desirable properties of a spanning tree. We are interested in a Minimum Diameter Spanning Tree (MDST), i.e., a tree that minimizes the largest distance between any pair of nodes, thus minimizing the worst case length of any transmission path. The importance of minimizing the diameter of a spanning tree has been widely recognized (see e.g. [2]); essentially, the diameter of a network provides a lower bound on the computation time of most algorithms in which all nodes participate.
We gratefully acknowledge the support of the Swiss SBF under contract no. C05.0047 within COST-295 (DYNAMO) of the European Union and the support of the Natural Sciences and Engineering Research Council of Canada.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 268­282, 2007. c Springer-Verlag Berlin Heidelberg 2007

A Distributed Algorithm for Finding All Best Swap Edges of a MDST

269

One downside of using a spanning tree is that a single link failure disconnects the network. Whenever the link failure is transient, i.e., the failed link soon becomes operational again, the best possible way of reconnecting the network is to replace the failed link by a single other link, called a swap link. Among all possible swap links, one should choose a best swap w.r.t. the original objective [5,6,7,8], that is in our case, a swap that minimizes the diameter of the resulting swap tree. Note that the swap tree is different from a minimum diameter spanning tree of the underlying graph that does not use the failed link. The reason for preferring the swap tree to the latter lies in the effort that a change of the current communication tree requires: If we were to replace the original MDST by a tree whose edge set can be very different, we would need to put many edges out of service, many new edges into service, and adjust many routing tables substantially -- and all of this for a transient situation. For a swap tree, instead, only one new edge goes into service, and routing can be adjusted with little effort (as we will show). Interestingly, this choice of swapping against adjusting an entire tree even comes at a moderate loss in diameter: The swap tree diameter is at most a factor of 2.5 larger than the diameter of an entirely adjusted tree [6]. In order to keep the required time for swapping small, for each edge of the tree, a best swap edge is precomputed. We show in the following that this distributed computation of all best swaps has the further advantage of gaining efficiency (against computing swap edges individually), because dependencies between the computations for different failing edges can be exploited. Related Work. Nardelli et al. [6] describe a centralized (i.e.,  non-distributed) algorithm for computing all best swaps of a MDST in O(n m) time and O(m) space, where the given underlying communication network G = (V, E ) has n = |V | vertices and m = |E | edges. For shortest paths trees, an earlier centralized algorithm has been complemented by a distributed algorithm [7] using totally different techniques for finding all best swap edges for several objectives [3,4], with either O(n) messages of size O(n) (i.e., a message contains O(n) node labels, edge weights, etc.) each, or O(n ) short messages with size O(1) each, where n denotes the size of the transitive closure of the tree, where edges are directed away from the root. In a so-called preprocessing phase of this algorithm, some information is computed along with the spanning tree construction using O(m) messages. A distributed algorithm for computing a MDST in a graph G(V, E ) in an asynchronous setting has O(n) time complexity (in the standard sense, as explained in Section 3) and uses O(nm) messages [2]. However, no efficient distributed algorithm to compute the best swaps of a MDST had been found to date. Our Contribution. In this paper, we propose a distributed algorithm for computing all best swaps of a MDST using no more than O(max{n , m}) messages of size O(1) each. The size of a message denotes the number of atomic values that it contains, such as node labels, edge weights, path lengths etc., and n is the size of the transitive closure of the MDST with edges directed away from a center of the tree. Both n and m are very natural bounds: When each subtree triggers as many messages as there are nodes in the subtree, the size of the

270

B. Gfeller, N. Santoro, and P. Widmayer

transitive closure describes the total number of messages. Furthermore, it seems inevitable that each node receives some information from each of its neighbours in G, across each potential swap edge. Our algorithm runs in O( D ) time (in the standard sense, as explained in Section 3), where D is the hop-length of the diameter path of G; note that this is asymptotically optimal. The message and time costs of our algorithm are easily subsumed by the costs of constructing a MDST distributively using the algorithm from [2]. Thus, it is cheap to precompute all the best swaps in addition to constructing a MDST initially. Just like the best swaps algorithms for shortest paths trees [3,4], our algorithm (like many fundamental distributed algorithms) exploits the structure of the tree. This tree, however, is substantially different in that it requires a significantly more complex invariant to be maintained during the computation: We need to have just the right collection of pieces of paths available so that on the one hand, these pieces can be maintained efficiently, and on the other hand, they can be composed to reveal the diameter at the corresponding steps in the computation. Furthermore, we propose a compact routing scheme for trees which can quickly and inexpensively adapt routing when a failing edge is replaced by a best swap edge. Notably, our scheme does not require an additional full backup table, but assigns a label of c log n bits to each node (for some small constant c); a node of degree  stores the labels of all its neighbours (and itself), which amounts to c log n bits per node, or mc log n bits in total. Given this labelling, knowledge of the labels of both adjacent nodes of a failing edge and the labels of both adjacent nodes of its swap edge is sufficient to adjust routing. In Section 2, we formally define the distributed all best swaps problem. Section 3 states our assumptions about the distributed setting and explains the basic idea of our algorithm. In Section 4, we study the structure of diameter paths after swapping, and we propose an algorithm for finding best swaps. The algorithm uses information that is computed in a preprocessing phase, described in Section 5. Our routing scheme is presented in Section 6. Section 7 concludes the paper.

2

Problem Statement and Terminology

A communication network is a 2-connected, undirected, edge weighted graph G = (V, E ), with n = |V | vertices and m = |E | edges. Each edge e  E has a non-negative real length w(e). The length |P| of a path P is the sum of the lengths of its edges, and the distance d(x, y ) between two vertices x, y is the length of a shortest path between x and y . The hop-length P of a path P is the number of edges that P contains. Throughout the paper, we are only dealing with simple paths. Given a spanning tree T = (V, E (T )) of G, let D(T ) := d1 , d2 , . . . , dk denote a diameter of T , that is, a longest path in T (see Fig. 1). Where no confusion arises, we abbreviate D(T ) with D. Furthermore, define the center dc of D as a node such that the lengths of DL := d1 , d2 , . . . , dc and DR := dc , dc+1 , . . . , dk satisfy |DL |  |DR | and have the smallest possible difference |DL | - |DR |. The set of neighbours of a node z (excluding z itself) in G and in T is written as NG (z ) and NT (z )  NG (z ), respectively.

A Distributed Algorithm for Finding All Best Swap Edges of a MDST
dc dc-1 dc+1 T dc-1 dc p(x) e z dc+1 x dk Tx

271

T

dk d1 VL VC VR d1

u z f

Fig. 1. A minimum diameter spanning tree T

Fig. 2. A swap edge f = (z, z ) for e = (x, p(x))

Let T be rooted at dc , and let, for each node x = dc , node p(x) be the parent of x and C (x) the set of its children. Furthermore, let Tx = (V (Tx ), E (Tx )) be the subtree of T rooted at x, including x. Let VL (L stands for "left") be the set of nodes in the subtree rooted at dc-1 , VR the set of nodes in the subtree rooted at dc+1 , and VC all other nodes. Now, the removal of any edge e = (x, p(x)) of T partitions the spanning tree into two trees Tx and T \Tx (see Fig. 2). Note that T \Tx does not contain the node x. A swap edge f for e is any edge in E \E (T ) that (re-)connects Tx and T \Tx, i.e., for which T \{e}  {f } =: Te/f is a spanning tree of G\{e}. Let S (e) be the set of swap edges for e. A best swap edge for e is any edge f  S (e) for which |D(Te/f )| is minimum. A local swap edge of node z for some failing edge e is an edge in S (e) adjacent to z . The distributed all best swaps problem for a MDST is the problem of finding for every edge e  E (T ) a best swap edge (with respect to the diameter). Throughout the paper, let e = (x, p(x)) denote a failing edge and f = (z, z ) a swap edge, where z is a node inside Tx , and z a node in T \Tx.

3

Algorithmic Setting and Basic Idea

In our setting, nodes have unique identifiers that possess a linear order. Further, let each node know its own neighbours in T and in G, and for each neighbour the length of the corresponding edge. We assume port-to-port communication between neighbouring nodes. The distributed system of nodes is totally asynchronous. Each message sent from some node to one of its neighbours eventually arrives (there is no message loss). As usual, we define the asynchronous time complexity of an algorithm as the longest possible execution time assuming that sending a message requires at most one time unit. Furthermore, nodes do not need to know the total number of nodes in the system (although it is easy to count the nodes in T using a convergecast). 3.1 The Basic Idea

Our goal is to compute, for each edge of T , a best swap edge. A swap edge for a given failing edge e = (x, p(x)) must connect the subtree of T rooted at x to the

272

B. Gfeller, N. Santoro, and P. Widmayer

part of the tree containing p(x). Thus, a swap edge must be adjacent to some node inside Tx . If each node in Tx considers its own local swap edges for e, then in total all swap edges for e are considered. Therefore, each node inside Tx finds a best local swap edge, and then participates in a minimum finding process that computes a (globally) best swap edge for e. The computation of the best local swap edges is composed of three main phases: In a first preprocessing phase, a root of the MDST is chosen, and various pieces of information (explained later) are computed for each node. Then, in a top-down phase each node computes and forwards some "enabling information" (explained later) for each node in its own subtree. This information is collected and merged in a third bottom-up phase, during which each node obtains its best local swap edge for each (potentially failing) edge on its path to the root. The efficiency of our algorithm will be due to our careful choice of the various pieces of information that we collect and use in these phases. To give an overview, we now briefly sketch how each node computes a best local swap edge. First observe that after replacing edge e by f , the resulting diameter is longer than the previous diameter only if there is a path through f which is longer than the previous diameter, in which case the path through f is the new diameter. In this case, the length of the diameter equals the length of a longest path through f in the new tree. For a local swap edge f = (z, z ) connecting node z  V (Tx ) and z  V \V (Tx ), such a path consists of (i) a longest path inside T \Tx starting in z , (ii) edge f , and (iii) a longest path inside Tx starting in z . Part (i) is computed in a preprocessing phase, as described in Section 5. Part (ii) is by assumption known to z , because f is adjacent to z . Part (iii) is inductively computed by a process starting from the root x of Tx , and stopping in the leaves, as follows. A path starting in z and staying inside Tx either descends to a child of z (if any), or goes up to p(z ) (if p(z ) is still in Tx ) and continues within Tx \Tz . For the special case where z = x, node x needs to consider only the heights of the subtrees rooted at its children. All other nodes z in Tx additionally need to know the length of a longest path starting at p(z ) and staying inside Tx \Tz . This additional enabling information will be computed by p(z ) and then be sent to z . Once the best local swap edges are known, a best (global) swap edge is identified by a single minimum finding process that starts at the leaves of Tx and ends in node x. To compute all best swap edges of T , this procedure is executed separately for each edge of T . This approach will turn out to work with the desired efficiency: Main Theorem. All best swap edges of a MDST can be computed in an asynchronous distributed setting with O(max{n , m}) messages of constant size, and in O( D ) time. We will prove this theorem in the next sections, by proving that the preprocessing phase can be realized with O(m) messages, and after that the computation of all best swap edges requires at most O(n ) additional messages.

A Distributed Algorithm for Finding All Best Swap Edges of a MDST

273

This algorithm requires that each node knows which of its neighbours are children and which neighbour is its parent in T . Although this information is not known a priori, it can be easily computed in a preprocessing phase, during which a particular diameter and a root of T are chosen.

4

How to Pick a Best Swap Edge

In our distributed algorithm, we compute for each (potentially) failing edge the resulting new diameter for each possible swap edge candidate. This approach can be made efficient by exploiting the structure of diameter path changes, as described in the following. 4.1 The Structure of Diameter Path Changes

For a given failing edge e, let Pf be a longest path in Te/f that goes through swap edge f for e. Then, we have the following: Lemma 1. The length of the diameter of Te/f is |D(Te/f )| = max{|D(T )|, |Pf |}. Proof. Let T1 and T2 be the parts into which T is split if e is removed. It is easy to see that (1) |D(Te/f )| = max{|D(T1 )|, |D(T2 )|, |Pf |}. Since T is a MDST, we have |D(Te/f )|  |D(T )|. Because T1 and T2 are contained in T , |D(T1 )|  |D(T )| and |D(T2 )|  |D(T )|. (3) (2)

If |Pf |  |D(T )|, it is clear that |Pf | is a largest term in (1), so the claim holds. On the other hand, if |Pf | < |D(T )|, then either T1 or T2 must contain a diameter of length exactly |D(T )| (otherwise, either (2) or (3) would be violated). Thus, the claim holds also in this case. That is, for computing the resulting diameter length for a given swap edge f = (z, z ) for e, we only need to compute the length of a longest path in Te/f that goes through f . For node z in the subtree Tx of T rooted in x, and z outside this subtree, such a path Pf consists of three parts. To describe these parts, let L(H, r) denote a longest path starting in node r and staying inside the graph H . The first part is a longest path L(T \Tx, z ) in T \Tx that starts in z . The second part is the edge f itself. The third part is a longest path L(Tx , z ) starting in z and staying inside Tx . This determines the length of a longest path through f as |Pf | = |L(Tx , z )| + w(f ) + |L(T \Tx, z )|.

274

B. Gfeller, N. Santoro, and P. Widmayer

4.2

Distributed Computation of |L(Tx , z )|

For a given failing edge e = (x, p(x)), each node z in Tx needs its |L(Tx , z )| value to check for the new diameter when using a swap edge. This is achieved by a distributed computation, starting in x. As x knows the heights of the subtrees of all its children (from the preprocessing), it can locally compute the height of its own subtree Tx as |L(Tx , x)| = maxqC (x) {w(x, q ) + height(Tq )}, where C (x) is the set of children of x. For a node z in the subtree rooted at x, a longest simple path either goes from z to its parent and hence has length |L(Tx \Tz  {z }, z )|, or goes into the subtree of one of its children and hence has length |L(Tz , z )| (see Fig. 3). The latter term has just been described, and the former can be computed by induction by the parent r of z and can be sent to z . This inductive step is identical to the step just described, except that z itself is no candidate subtree for a path starting at r in the induction. In total, each node r computes, for each of its children q  C (r), the value of |L(Tx \Tq  {q }, q )| = w(q, r) + max |L(Tx \Tr  {r}, r)|,
sC (r ),s=q

max

{w(r, s) + height(Ts )} ,

and sends it to q , where we assume that the value |L(Tx \Tr  {r}, r)| was previously sent to r by p(r). A bird's eye view of the process shows that each node z first computes |L(Tx , z )|, and then computes and sends |L(Tx \Tq  {q }, q )| for each of its children q  C (z ). Computation of the |L(Tx , z )| values finishes in Tx 's leaves. Note that a second value will be added to the enabling information if (x, p(x))  D, for reasons explained in the next section. 4.3 Distributed Computation of |L(T \Tx , z )|

In the following, we explain how z can compute |L(T \Tx, z )| for a given swap / D, we show below that edge f = (z, z ). In case the failing edge e = (x, p(x))  the information obtained in the preprocessing phase is sufficient. For the sake of clarity, we analyze two cases separately, starting with the simpler case. Case 1: The removed edge e is not on the diameter. For this case, we know from [6] that at least one of the longest paths in T \Tx starting from z contains dc . If z  VL , we get a longest path from z through dc by continuing on the diameter up to dk , and hence we have |L(T \Tx , z )| = d(z , dc ) + |DR |. If z is in VC or VR , some longest path from z through dc continues on the diameter up to d1 , yielding |L(T \Tx, z )| = d(z , dc ) + |DL |. Remarkably, in this case |L(T \Tx , z )| does not depend on the concrete failing edge e = (x, p(x)), apart from the fact that (z, z ) must be a swap edge for e. Case 2: The removed edge e is on the diameter. We analyze the case e  DL , and omit the symmetric case e  DR . If z  VL or z  VC , we know from [6] that again, one of the longest paths in T \Tx starting at z contains dc .

A Distributed Algorithm for Finding All Best Swap Edges of a MDST

275

Thus, for z  VL we are in the same situation as for the failing edge not on the diameter, leading to |L(T \Tx, z )| = d(z , dc ) + |DR |. For z  VC , after dc a longest path may continue either on DR , or continue to nodes in VL . In the latter case, the path now cannot continue on DL until it reaches d1 , because edge e lies on DL . Instead, we are interested in the length of a longest path that starts at dc , proceeds into VL , but does not go below the parent p(x) of x on DL ; let us call this length (p(x)). As announced before, we include the (p(x)) value as a second value into the enabling information received by p(x); then, we get |L(T \Tx, z )| = d(z , dc ) + max{|DR |, (p(x))}. It remains to consider z  VR . For this case (see Fig. 4), we know (from [6]) that at least one of the longest paths in T \Tx starting at z passes through the node u closest to z on D(T ). After u , this path may either continue on DR up to dk , or continue through dc going inside VC or VL (without crossing e = (x, p(x))), or continue towards dc only up to some node di on Dr , going further on non-diameter edges inside VR . It remains to show how the length of a longest path of this last type can be found efficiently. We propose to combine three lengths, in addition to the length of the path from z to u . The first is the length of a longest path inside VR that starts at dk ; let us call this length R . In general, this path goes up the diameter path DR for a while, and then turns down into a subtree of VR , away from the diameter, at a diameter node that we call R (see Fig. 4). Given R , the distance from u to R , and the distance from R to dk , the desired path length of an upwards turning path inside VR is d(z , u )+ d(u , R )+ R - d(dk , R ). Note that while it may seem that R needs to lie above u on DR , this is not really needed in our computation, because the term above will not be largest (among all path choices) if R happens to be below or at u . In total, we get |L(T \Tx, z )| = max {d(z , dk ), d(z , dc ) + (p(x)), d(z , u ) + d u , R + R - d dk , R .1
dc x p(z ) z Tz Tx \Tz d1 Tx \Tz  {z } Tx di p(x) x z f z dc+1 R R u dk

Fig. 3. Illustration of the tree Tx \Tz {z }

Fig. 4. Computing |L(T \Tx , z )| if e  DL , z  VL and z  VR

All of these path length computations can be carried out locally with no message exchanges, if the constituents of these sums are available locally at a node. We will show in the next section how to achieve this in an efficient preprocessing phase.
1

Recall that in the definition of (p(x)), paths inside VC starting from dc are also considered.

276

B. Gfeller, N. Santoro, and P. Widmayer

4.4

The BestDiamSwap Algorithm

For a given edge e = (x, p(x)) that may fail, each node z in the subtree Tx rooted at x executes the following steps: (i) Wait for the enabling information from the parent (unless x = z ), and then compute |L(Tx , z )|. Compute the enabling information for all children and send it. (ii) For each local swap edge f = (z, z ), compute |L(T \Tx , z )| as described in Section 4.3. (iii) For each local swap edge f = (z, z ), locally compute |D(Te/f )| = max{|D(T )|, |L(Tx , z )| + w(f ) + |L(T \Tx , z )|}.
 and store the resulting new Among these, choose a best swap edge flocal  diameter as |D(Te/flocal )|. (iv) From each child q  C (z ), receive the node label of a best swap edge   )|. Pick a best swap edge and its resulting diameter |D(Te/fq candidate fq   )|. Comcandidate fb among these, i.e., choose b := arg minqC (z) |D(Te/fq   and flocal , and define fbest as the edge pare the resulting diameter of fb achieving the smaller diameter (or any of them if their length is equal), and its diameter as |D(Te/fbest )|. (v) Send the information fbest , |D(Te/fbest )| to the parent.

The above algorithm computes the best swap edge for one (potentially) failing edge e, based on the information available after the preprocessing phase. In order to compute all best swap edges of T , we execute this algorithm for each edge of T independently. Analysis of the Algorithm. We now show that the proposed algorithm indeed meets our efficiency requirements: Theorem 1. After preprocessing, executing the BestDiamSwap algorithm independently for each and every edge e  E (T ) costs at most O(n ) messages of size O(1) each, and O( D ) time, using a "Farthest-to-Go" queuing policy [1]. Proof. Correctness follows from the preceding discussion. Preprocessing ensures that all precomputed values (such as |L(T \Tx , z )|) defined for the other end z of a candidate swap edge are available locally at z . As to the message complexity, consider the execution of the BestDiamSwap algorithm for one particular edge e = (x, p(x)). Starting in node x  V \{dc }, each node in Tx sends a message containing the "enabling information" (i.e., L(Tx \Tq , q ) and possibly (p(x))) containing O(1) items to each of its children. Furthermore, each node in Tx (including finally x) sends another message with size O(1) up to its parent in the minimum finding process. Hence, two messages of size O(1) are sent across each edge of Tx , and one message is sent across e. Thus, the computation of a best swap for e requires 2 · |E (Tx )| + 1 = 2 · |V (Tx )| - 1 messages. The number of messages exchanged for computing a best swap edge for each and every edge (x, p(x)) where x  V \{dc } is x (2 · |V (Tx )| - 1) = 2n - (n - 1).

A Distributed Algorithm for Finding All Best Swap Edges of a MDST

277

As to the time complexity, note that the best swap computation of a single edge according to the BestDiamSwap algorithm requires at most O( D ) time. Now note that this algorithm can be executed independently (and thus concurrently) for each potential failing edge: In this fashion, each node x in T sends exactly one message to each node in Tx during the top-down phase. Symmetrically, in the bottom-up phase, each node u in T sends exactly one message to each node on its path to the root. The crucial point here is to avoid that some of these messages block others for some time (as only one message can traverse a link at a time). Indeed, one can ensure that each message reaches its destination in O( D ) time as follows. A node z receiving a message with destination at distance d from z forwards it only after all messages of the protocol with a destination of distance more than d from z have been received and forwarded. By induction over the distance of a message from its destination, it is easily proven that this "Farthest-to-Go" queuing policy allows each message to traverse one link towards its destination after at most one time unit of waiting. Thus, the O( D ) time complexity also holds for the entire algorithm. Instead of sending many small messages individually, we can choose to sequence the process of message sending so that messages for different failing edges are bundled before sending (see also [3,4] for applications of this idea). This leads to an alternative with fewer but longer messages: Corollary 1. After preprocessing, the distributed all best swaps problem can be solved using O(n) messages of size O(n) each, and O( D ) time.

5

The Preprocessing Phase

The preprocessing phase serves the purpose of making the needed terms in the sums described in the previous section available at the nodes of the tree. In the preprocessing phase, a diameter D of T is chosen, and its two ends d1 and dk as well as its center dc are identified. This can be done essentially by a convergecast, followed by a broadcast to distribute the result (see e.g. [9]); we omit the details. Hence, after preprocessing exchanges O(n) messages, each node knows the information that is requested in (A) and (C ) below. It is crucial that during preprocessing, each node obtains enough information to later carry out all computational steps to determine path components (i), (ii) and (iii). More precisely, each node gets the following global information (the same for all nodes): (A) The endpoints d1 and dk of the diameter, the length |D| of the diameter, and the lengths |DL | and |DR |. (B) The length R of a longest path starting in dk that is fully inside Tdc+1 , together with the node R on D where such a path leaves the diameter. Figure 5 illustrates such a longest path R . Moreover, the distance d(R , dc ) must be known. Symmetrically, the length L of a longest path starting in d1 that is fully inside Tdc-1 , with the corresponding node L and distance d(L , dc ) are required.

278

B. Gfeller, N. Santoro, and P. Widmayer

In addition, each node z obtains the following information that is specific for z : (C) (D) (E) (F) (G) (H) For each child q  C (z ) of its children, the height Tq of q 's subtree. Is z on the diameter D, yes or no. The distance d(z, dc ) of z to dc . The identification of the parent p(z ) of z in T . To which of VL , VC and VR does z belong. If z  / D, the closest predecessor u of z on the diameter; the distance d(u, dc ) from u to dc . (I) If z is on the left (right) diameter DL (DR ), with z = di , the length (di ) of a longest path in T starting at dc and neither containing the node dc+1 (dc-1 ) nor the node di-1 (di+1 ) (see Fig. 5). (J) For each of the neighbours z of z in G, which of VL , VC and VR contains z ; the distance d(z , dc ) from z to dc ; the nearest predecessor u of z on D, the distance d(u , dc ).

Computing the Additional Information. Recall that the first preprocessing part ends with a broadcast that informs all nodes about the information described in (A) and (C ). The second part of the preprocessing phase follows now. A node z receiving the message about D can infer from the previous convergecast whether it belongs to D itself by just checking whether the paths from z to d1 and dk go through the same neighbour of z . Information (E ) is obtained by having the center node send a "distance from dc " d(dc , d ) message to both neighbours dc+1 and dc-1 on D, which is forwarded and updated on the diameter. This information is used by the diameter nodes for computing (di ), required in (I ). The center initiates the inductive computation of (di ): ­ (dc ) is the depth of a deepest node in VC . ­ For each dj , 1  j < c, (dj ) = max{(dj +1 ), d(dc , dj ) + h2 (dj )}, h2 being the height of a highest subtree of dj apart from the diameter subtree. ­ For each dj , c < j  k , (dj ) = max{(dj -1 ), d(dc , dj ) + h2 (dj )}. In order to compute L and R as required in (B ), we define (di ) for each node di on DL as the length of a longest path starting in d1 that is fully inside Tdi , together with the node (di ) on DL where such a path leaves the diameter. For di on DR , the definition is symmetric. We then have L = (dc-1 ) and R = (dc+1 ). The inductive computation of (di ) is started by d1 and dk , and then propagated along the diameter: ­ (d1 ) = (dk ) = 0; ­ for each dj , 1 < j < c, (dj ) = max{(dj -1 ), d(d1 , dj ) + h2 (dj )}; ­ for each dj , c < j < k , (dj ) = max{(dj +1 ), d(dk , dj ) + h2 (dj )}. Along with (dj ), (dj ) and d((dj ), dc ) can be computed as well. The computation stops in dc , which receives the messages ((dc-1 ), (dc-1 ), d((dc-1 ), dc )) = (L , L , d(L , dc )) and ((dc+1 ), (dc+1 ), d((dc+1 ), dc )) = (R , R , d(R , dc )). Altogether, this second preprocessing part operates along the diameter and takes O( D(T ) ) = O(n) messages.

A Distributed Algorithm for Finding All Best Swap Edges of a MDST
dc dc-1

279

T (di )

dc+1 R R

T e Tx dk s i z x f p(x) z d

di d1

Fig. 5. Definition of (di ), R and R

Fig. 6. Only some nodes need to know about failure of edge e = (x, p(x))

Distributing the Information. When the computation of (L , L , d(L , dc )) and (R , R , d(R , dc )) completes in dc , the center packs these values plus the values |DL | and |DR | into one message M . It adds the appropriate one of the labels "VL ","VR " and "VC " to M , before forwarding M to dc-1 , dc+1 and any other neighbour of dc in T and then flooding the tree. Additionally, M contains the "distance from dc " information which is updated on forwarding, such that all nodes know their distance to the center2 . When M is forwarded from a node u  D to a node not on D, it is extended by the "distance from u" information, which is also updated on forwarding. In addition, d(u, dc ) is appended to M . Finally, if node z receives M from node v , then z learns that v is its parent. At the end of this second part of the preprocessing phase, each node z sends a message M to each of its neighbours z in G\T . Note that this is the only point in our solution where messages need to be sent over edges in G\T . M contains d(z , dc ) and exactly one of { "z  VL ", "z  VC " , "z  VR " }, whichever applies. Furthermore, let u be the nearest ancestor of z on D; the distance d(u , dc ) is also appended to M . As a consequence, after each node has received its version of the message M , the information stated in (B), (E), (F), (G), (H) is known to each node. Furthermore, each node that has received M from all its neighbours in G knows the information stated in (J). The distribution of this information requires O( D(T ) ) time and O(m) messages. Let us summarize. Lemma 2. After the end of the two parts of the preprocessing phase, which requires O( D ) time, all nodes know all information (A) - (J ), and O(m) messages have been exchanged. Recognizing Swap Edges Using Labels. A node v  Tx must be able to tell whether an incident edge f = (v, w) is a swap edge for e = (x, p(x)) or not. We achieve this by the folklore method of numbering nodes in two ways, a preorder traversal and a reverse preorder traversal. After this, a node can decide in constant time whether an edge is a swap edge. For details, see [3,4].
2

The nodes on D already have that information at this point, but all other nodes still require it.

280

B. Gfeller, N. Santoro, and P. Widmayer

6

Routing Issues

A natural question arises concerning routing in the presence of a failure: After replacing the failing edge e by a best swap edge f , how do we adjust our routing mechanism in order to guide messages to their destination in the new tree Te/f ? And how is routing changed back again after the failing edge has been repaired? Clearly, it is desirable that the adaptation of the routing mechanism is as fast and inexpensive as possible. Existing Approaches. The simplest routing scheme uses a routing table of n entries at each node, which contains, for each possible destination node, the link that should be chosen for forwarding. This approach can be modified to allow swaps by storing additional n entries for the swap links at each node [3]. In [5] a scheme is proposed that stores only one swap entry, at the cost of choosing suboptimal swap edges. All these approaches require O(n2 ) routing entries in total. In the following, we propose to use a compact routing scheme for arbitrary trees (shortest paths, minimum diameter, or any other) which requires only  entries, i.e. c log n bits, at a node of degree  , thus n entries or mc log n bits in total, which is the same amount of space that the interval routing scheme of [10] requires. The header of a message requires c log n bits to describe its destination. Our Routing Scheme. Our routing scheme for trees is based on the labelling  : V  {1, . . . , n}2 described in the end of Section 5. Note that  allows to decide in constant time whether a is in the subtree of b (i.e., a  Tb ) for any two given nodes a and b. Basic Routing Algorithm: A node s routes message M with destination d as follows: (i) If d = s, M has arrived at its destination. (ii) If d  / Ts , s sends M to p(s). (iii) Otherwise, s sends M to the child q  C (s) for which d  Tq . This algorithm clearly routes each message directly on its (unique) path in T from s to d. Before describing the adaptation in the presence of a swap, observe that a node s which receives a message M with destination d can locally decide whether M traverses a given edge e = (x, p(x)): edge e is used by M if and only if exactly one of s and d is in the subtree Tx of x, i.e., if (s  Tx ) = (d  Tx ). Thus, it is enough to adapt routing if all nodes are informed about a failing edge (and later the repair) by two broadcasts starting at its two incident nodes (the points of failure). However, the following lemma shows that optimal rerouting is guaranteed even if only those nodes which lie on the two paths between the points of failure and the swap edge's endpoints are informed about failures, which allows "piggybacking" all information for routing adjustment on the first message arriving at the point of failure after the failure occurred. Lemma 3. Let e = (x, p(x)) be the failing edge, and f = (z, z ) the best swap of e, where z is in Tx and z in T \Tx, as shown in Figure 6. If all nodes on the path from x to z know that e is unavailable and that f = (z, z ) is a best swap

A Distributed Algorithm for Finding All Best Swap Edges of a MDST

281

edge, then any message originating in s  Tx will be routed on the direct path from s to its destination d. Symmetrically, if all nodes on the path from p(x) to z know about e and f , then any message originating in s  T \Tx will be routed on the direct path from s to its destination d. Proof. Let M be any message with source s  Tx . If d  Tx , then trivially M will be routed on its direct path, because it does not require edge e. If d  T \Tx, consider the path PT from s to d in T , and the path PTe/f from s to d in Te/f . Consider the last common node i of PT and PTe/f in Tx . The path composed of the paths x, . . . , i , i, . . . , z is exactly the unique path in T from x to z , so node i lies on that path. Obviously, M will be routed on the direct path towards d up to i. As i lies on the path from x to z , it knows about the failure and the swap, and will route M towards z . The lemma assumes that any node on the path from i to z also knows about the swap. Thus, such nodes will route M on the direct path to z . At z , M will be routed over the swap edge f , and from z on M is forwarded on the direct path from z to d. Given Lemma 3, we propose the following "lazy update" procedure for informing nodes about an edge failure: Algorithm Swap: If an edge fails, no action is taken as long as no message needs to cross it. As soon as a message M which should be routed over the failing edge arrives at the point of failure, information about the failure and its best swap is attached to message M , and M is routed towards the swap edge. On its way, all nodes which receive M route it further towards the swap edge, and remember for themselves the information about the swap. Observation (Adaptivity). After one message M has been rerouted from the point of failure to the swap edge, all messages originating in the same side of T as M (with respect to the failing edge) will be routed to their destination on the direct path in the tree (i.e., without any detour via the point of failure). If a failing edge has been replaced by a swap edge, then all nodes which know about that swap must be informed when the failure has been repaired. Therefore, a message is sent from the point of failure to the swap edge (on both sides if necessary), to inform these nodes, and to deactivate the swap edge.

7

Discussion

We have presented a distributed algorithm for computing all best swap edges for a minimum diameter spanning tree. Our solution is asynchronous, requires unique identifiers from a linearly ordered universe (but only for tiebreaking to determine a center node), and uses O( D ) time and O(max{n , m}) small messages, or O(n) messages of size O(n). It remains an open problem to extend our approach to subgraphs with other objectives; for instance, can we efficiently compute swap edges for failing edges in a spanner?

282

B. Gfeller, N. Santoro, and P. Widmayer

References
1. Andrews, M., Awerbuch, B., Fern´ andez, A., Leighton, T., Liu, Z., Kleinberg, J.: Universal-stability results and performance bounds for greedy contentionresolution protocols. J. ACM 48(1), 39­69 (2001) 2. Bui, M., Butelle, F., Lavault, C.: A Distributed Algorithm for Constructing a Minimum Diameter Spanning Tree. Journal of Parallel and Distributed Computing 64, 571­577 (2004) 3. Flocchini, P., Enriques, A.M., Pagli, L., Prencipe, G., Santoro, N.: Point-of-failure Shortest-path Rerouting: Computing the Optimal Swap Edges Distributively. IEICE Transactions on Information and Systems E89-D(2), 700­708 (2006) 4. Flocchini, P., Pagli, L., Prencipe, G., Santoro, N., Widmayer, P.: Computing All the Best Swap Edges Distributively. In: Higashino, T. (ed.) OPODIS 2004. LNCS, vol. 3544. pp. 154­168. Springer, Heidelberg (2005) 5. Ito, H., Iwama, K., Okabe, Y., Yoshihiro, T.: Single Backup Table Schemes for Shortest-path Routing. Theoretical Computer Science 333(3), 347­353 (2005) 6. Nardelli, E., Proietti, G., Widmayer, P.: Finding All the Best Swaps of a Minimum Diameter Spanning Tree Under Transient Edge Failures. Journal of Graph Algorithms and Applications 5(5), 39­57 (2001) 7. Nardelli, E., Proietti, G., Widmayer, P.: Swapping a Failing Edge of a Single Source Shortest Paths Tree Is Good and Fast. Algorithmica 35(1), 56­74 (2003) 8. Di Salvo, A., Proietti, G.: Swapping a Failing Edge of a Shortest Paths Tree by Minimizing the Average Stretch Factor. In: Kralovic, R., S´ ykora, O. (eds.) SIROCCO 2004. LNCS, vol. 3104. Springer, Heidelberg (2004) 9. Santoro, N.: Design and Analysis of Distributed Algorithms. Wiley Series on Parallel and Distributed Computing. Wiley, Chichester (2007) 10. Santoro, N., Khatib, R.: Labelling and Implicit Routing in Networks. The Computer Journal 28(1), 5­8 (1985)

On the Message Complexity of Indulgent Consensus
Seth Gilbert1 , Rachid Guerraoui1, and Dariusz R. Kowalski2
I&C School Of Computer & Communication Sciences EPFL, 1015 Lausanne, Switzerland seth.gilbert@epfl.ch, rachid.guerraoui@epfl.ch 2 Department of Computer Science, University of Liverpool Liverpool L69 3BX, UK d.r.kowalski@csc.liv.ac.uk
1

Abstract. Many recommend planning for the worst and hoping for the best. In this paper we devise efficient indulgent consensus algorithms that can tolerate crash failures and arbitrarily long periods of asynchrony, and yet perform (asymptotically) optimally in well-behaved, synchronous executions with few failures. We present two such algorithms: In synchronous executions, the first has optimal message complexity, using only O(n) messages, but runs in superlinear time of O(n1+ ). The second has a message complexity of O(n polylog(n)), but has an optimal running time, completing in O(f ) rounds in synchronous executions with at most f failures. Both of these results improve significantly over the most message-efficient of previous indulgent consensus algorithms which have a message complexity of at least  (n2 ) in well-behaved executions.

1 Introduction
As in many other fields, it is considered good computing practice to plan for the worst and hope for the best. In the context of distributed computing, this typically translates into devising algorithms that, on the one hand tolerate process failures and arbitrarily long periods of asynchrony, whilst on the other hand, are particularly effective under best-case conditions, namely, few failures and synchrony. Such best-case conditions are usually considered frequent in practice and it makes sense to optimize algorithms with these conditions in mind. In this paper, we explore this idea in the context of consensus [1, 2] in a system of n processes of which a minority can fail by crashing. Given a set of n crash-prone processes, each with initial value vi ; each process needs to decide an output satisfying: (1) (agreement) every process decides the same value; (2) (validity) if a process decides value v , then v is the initial value for some process; (3) (termination) every correct process eventually decides. The question we ask is how "efficient" can a consensus algorithm be when the system is synchronous and f  n/2 - 1 failures actually occur, if the algorithm needs to tolerate arbitrarily long periods of asynchrony. Consensus algorithms that tolerate arbitrarily long periods of asynchrony include [3, 4, 5, 6, 7, 8]: they have been called indulgent [9]; indulgent consensus is impossible when there are more than a minority of crash failures [3].
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 283­297, 2007. c Springer-Verlag Berlin Heidelberg 2007

284

S. Gilbert, R. Guerraoui, and D.R. Kowalski

Message Complexity Alg. 1 (Section 4): Alg. 2 (Section 5): O(n) O(n log 6 n)

Round Complexity O(n1+ ) O (f )

Fig. 1. Message and round complexity of the two algorithms presented in this paper. Both refer to synchronous executions in which there are no more than f  t failures.

Addressing this question requires defining what it means for a consensus algorithm to be "efficient." Usually, this is measured in terms of rounds of communication needed for processes to reach a decision (see, e.g., [10]). There indeed exists an indulgent consensus protocol that reaches a decision in O(f ) (in fact, f + 2) rounds when the system is synchronous and f processes fail [11]. This algorithm, and in fact all indulgent consensus algorithms that are optimized for synchronous periods (e.g., [12,4,8]), exchange  (n2 ) messages: all processes send messages to all processes in every round. (In fact, most use at least (n2 f ) messages.) This pattern of full message exchange is a key subprotocol underlying those algorithms, and is used to detect synchrony and adapt the decision time to the actual number of failures. It is natural to ask whether such a pattern is necessary and whether (n2 ) messages really need to be exchanged. In other words, is it possible to devise an indulgent consensus protocol that reaches a decision in O(f ) rounds when the system is synchronous and no more than f processes fail, while exchanging fewer than (n2 ) messages? If the algorithm does not need to tolerate asynchrony, then the answer is yes [11]: [13] presented a protocol that uses O((f + 1)n) messages and [14] later demonstrated that O(n + f n ) messages are sufficient. However, it is not immediately obvious whether similar results can be achieved if the algorithm must tolerate periods of asynchrony. Clearly, during such a period processes could have divergent views: some may believe the system to be synchronous whereas others may not; some may observe only a small number of failures and hence believe it safe to decide, while others may not. In algorithms with a pattern of full message exchange, these inconsistencies are easy to resolve. The key difficulty in constructing an efficient algorithm that tolerates asynchrony is devising messageefficient techniques for producing a consistent view of the (a)synchrony of the world. Results We present in this paper two indulgent consensus algorithms (see Table 1). Both tolerate a minority of the processes failing, and output a decision when the system becomes stable. When the system is synchronous, both algorithms guarantee good performance, both in terms of message-efficiency and round complexity. Each is (asymptotically) optimal in a different sense. The first guarantees optimal message complexity--O(n) messages--in synchronous executions, and terminates in O(n1+ ) rounds. The second is adaptive: it guarantees optimal round complexity--O(f ) rounds--in a synchronous execution with no more than f  t failures, and has a message complexity of O(n log6 n). The key idea in both algorithms is to simulate an efficient synchronous consensus algorithm, while at the same time detecting asynchrony. If the execution is synchronous,

On the Message Complexity of Indulgent Consensus

285

then efficient performance is achieved. If the execution is not synchronous, however, the processes synchronize their view of the world via message-efficient gossip, and eventually fall-back to a less efficient consensus protocol that can better tolerate the uncertain synchrony. In the case of the second algorithm, which is adaptive, the simulation of the efficient synchronous protocol is more involved: different processes may complete the simulation at different times and (again) with different views of the world. In messageexpensive algorithms, this is easy to resolve, as typically all processes decide within one round of each other due to nodes flooding their decision prior to termination. In our case, the combination of adaptivity and possible asynchrony complicated the matters. Throughout the simulation, processes must efficiently determine whether any processes have already produced a decision which is clearly difficult because a process cannot distinguish a failed process from one whose messages are delayed. The solution, again, is through careful use of efficient gossip protocols to synchronize the status of the processes prior to deciding. Interestingly, both our algorithms can be viewed as generic transformations from synchronous consensus (and gossip) protocols to partially synchronous consensus protocols. Thus future improvements in synchronous algorithms will result immediately in improved indulgent consensus algorithms. Previous and Related Work The problem of consensus was first introduced by Pease, Shostak and Lamport [1]. Fisher, Lynch and Paterson [2] showed that consensus is unsolvable in an asynchronous system in which even one process can crash. Thus research on consensus has often focused on synchronous and partially synchronous models of computation. In a seminal paper [3], Dwork, Lynch, and Stockmeyer introduced a model of eventual synchrony in which clock skew and message delivery eventually stabilize at some unknown point in the execution. This is the model we adopt in this paper. They showed in [3] that consensus can be solved in the eventually synchronous model if and only if n  2t + 1, where t is the tolerable number of crash failures. In [9], Guerraoui coins the term "indulgent" to describe algorithms that can tolerate arbitrarily long periods of asynchrony. Fisher and Lynch [15] showed that a synchronous solution to consensus requires t +1 rounds, where t is the tolerable number of failures. Dolev and Strong [16] introduce the idea of early stopping, or adaptive, consensus protocols, and Lamport and Fischer [17] show that it is possible to terminate in only f + 2 rounds in executions with f < t failures. Dolev, Reischuk and Strong [18] show that at least min(f + 2, t + 1) rounds are necessary. In the context of indulgent consensus, Dutta and Guerraoui [11] show that at least t + 2 rounds are required, even in a synchronous execution. There has been a significant amount of recent work on optimizing the running time of consensus in failure-free executions; see, for example [12, 19, 8]. In a synchronous setting, it is relatively straightforward to observe that there is an  (n) lower bound on the message complexity of fault-tolerant synchronous consensus. Dwork, Halpern and Waarts [20] found a solution with O(n log n) messages but exponential time. Finally, Galil, Mayer and Yung [14] developed an algorithm with O(n) messages, thus showing that this is the optimal message-complexity. The drawback of

286

S. Gilbert, R. Guerraoui, and D.R. Kowalski

their solution is that it runs in superlinear time O(n1+ ), for any fixed 0 <  < 1. Galil, Mayer and Yung [14] also found an adaptive solution with O(n + f n ) communication complexity, for any 0 <  < 1. Chlebus and Kowalski reduced the number of messages to O(n log2 n) for consensus in case n - t =  (n) [21], and recently they developed an adaptive algorithm that tolerates up to n - 1 crashes and achieves O(n log5 n) message complexity [22]. The message complexity of consensus when no failures actually occur, was studied by Amdur, Weber and Hadzilacos [23] and by Hadzilacos and Halpern [24], and results in the following fact which implies that O(n) message complexity is optimal, regardless of the actual number of failures: Fact 1 (Amdur, Weber and Hadzilacos [23]). The message complexity of every (eventually)-synchronous consensus protocol is at least  (n), even in failure-free executions. Roadmap In Section 2 we describe the eventually synchronous model, and in Section 3 we define a series of building blocks, synchronous protocols that will be used in the construction of our algorithms. In Section 4, we describe our first algorithm which guarantees optimal message complexity (in synchronous executions). In Section 5, we describe our second algorithm which is adaptive and guarantees optimal round complexity (in synchronous executions). We outline the proof of correctness in Section 6. In Section 7, we describe instantiations of the building blocks from Section 3, which allows us to analyze the performance of our algorithms in Section 8. We conclude in Section 9.

2 System Model
In this section we describe a basic system model for a partially synchronous (or eventually synchronous) system, as in [3]. The model is defined by three parameters that are known a priori: n, the number of processes,  , an eventual bound on clock skew, and d, an eventual bound on message delay. There is also a stabilization time, referred to as GST, that is unknown. We say that an execution is synchronous if GST= 0. In more detail, we consider a system consisting of n message-passing processes, each of which has a unique identifier from the set [n] = {1, 2, . . . , n}. Each process is capable of communicating directly with all other processes: prior to GST, messages may be arbitrarily delayed; after GST, every message is delivered within d time. Each process has a local clock, and after GST the clock skew of every process is bounded by  , i.e., eventually the ratio of the rates of two processes' clocks is at most  . We assume that up to t < n/2 processes may crash, and that processes do not restart or recover. We say that a process is correct if it does not crash. We do not assume reliable multicast: if a process crashes while sending a message to multiple recipients, then an arbitrary subset of the recipients may receive the message. We are specifically interested in the performance of algorithms in synchronous executions. We say that an algorithm A solves consensus by time  in the presence of f failures if for every synchronous execution with no more than f failures, every node has decided by time  . We say that algorithm A has message complexity  if for every synchronous execution, the total number of messages sent is no more than .

On the Message Complexity of Indulgent Consensus

287

3 Building Blocks Protocols
We construct our protocol out of three synchronous building blocks: synchronous consensus, synchronous gossip, and synchronous wake-up. We also use one eventuallysynchronous building block, a consensus protocol. In this section, we describe each of these building blocks, and enumerate their properties. In Section 7 we describe how each building block can be implemented from existing protocols. Synchronous Consensus Protocol. The first basic building block, SynchConsensus, is a protocol that solves consensus in synchronous executions. The protocol guarantees the following properties: (1) Agreement: In every synchronous execution, all decision values are the same. (2) Unconditional validity: In every execution (synchronous or otherwise), every decision is the initial value of some process. (3) Termination: In every synchronous execution, every process eventually decides and terminates. The second property, unconditional validity, is the only guarantee in an execution that is not synchronous. For every 0  f  t, define  cons (f ) to be the earliest round in which every execution of SynchConsensus with no more than f failures terminates. (This is of particular relevance when the consensus protocol is adaptive.) Synchronous Conditional Gossip. The second building block is a protocol Gossip(k ) that solves the conditional gossip problem in synchronous executions with no more than k failures. It is called "conditional" since its guarantees only hold when there are  k failures. Each process begins the gossip protocol with a rumor vi . The protocol satisfies the following: (1) Completion: In every synchronous execution with at most k  f < n failures, every non-failed process eventually receives a rumor from every non-failed process; and (2) Unconditional validity: In every execution (synchronous or otherwise), every rumor received is the initial value of some process. For every 0  f  t, define  gossip (f ) to be the earliest round in which every execution of Gossip(f ) with no more than f failures terminates. Synchronous Conditional Wake-Up. The third building block is a protocol WakeUp(k ) that solves the conditional wake-up problem. In conditional wake-up, initially, some subset S of the processes are designated awake, while the rest are designated asleep. The goal of conditional wake-up is that if every process is initially asleep, i.e., S = , then every process remains asleep and no messages are sent by any process. Conversely, if S = , then every non-failed process wakes up. Again, it is referred to as "conditional" since its guarantees only hold when there are  k failures. In more detail, the protocol guarantees the following: (1) Completion: In every synchronous execution with at most k  f < n failures, if S = , then eventually the protocol terminates and every process concludes that it is awake. (2) Validity: In every synchronous execution, if S = , then every process remains asleep and no messages are sent. For every 0  f  t, define  wakeup (f ) to be the earliest round in which every execution of WakeUp(f ) with no more than f failures terminates. Partially-Synchronous Consensus Protocol. The final building block is an arbitrary eventually-synchronous consensus protocol PartSynchConsensus; it guarantees the usual properties of consensus: agreement, validity, and termination. There are a variety of protocols that satisfy these requirements, including, for example, [5, 3].

288

S. Gilbert, R. Guerraoui, and D.R. Kowalski

4 Indulgent Consensus
In this section we present our first indulgent consensus protocol. When instantiated using the appropriate building-block protocols, the result is an (asymptotically) messageoptimal algorithm. (See Theorem 3.) The main idea is to first simulate an efficient synchronous consensus protocol SynchConsensus (see Section 7.1), and then determine whether it has completed successfully. If so, then each process can decide that value and terminate; otherwise processes run a fall-back partially synchronous consensus protocol that is not as message efficient. The main difficulty, then, is correctly detecting when an execution is synchronous without sending too many messages. 4.1 Simulating Synchronous Rounds Each process simulates synchronous rounds in the standard manner based on message delay d and clock skew  . Recall that in a synchronous execution, at time  the clock at every process i is in the range [(1 -  ), (1 +  ) ]. Let  = (1 +  )/(1 -  ). The first simulated round r1 ends at time d/(1 -  ) according to the local clock at each process. r -1 j d Simulated round r ends for each process at time:  sim (r) = 1- j =0  according  to the local clock of that process. In a synchronous execution, every message sent at the beginning of round r according to the local clock of the sending process is received by the end of round r according to the local clock of the receiving process. 4.2 Protocol Description The protocol proceeds in four phases: (1) Agreement Phase, (2) Locking Phase, (3) Decision Phase, (4) Fall-back Phase. When the protocol begins, the proposal for process i is stored in ei , its estimate. Process i also maintains a variable statusi that indicates its current status. Initially status i = proposal, indicating that the estimate is the initial value. As the status advances during the protocol to higher levels, it never returns to a lower level. 1. Agreement Phase. In the first phase, the processes together simulate the consensus protocol SynchConsensus for  cons (t) rounds. If the execution is, in fact, synchronous, then for each correct process the consensus simulation will output a decision; all such decisions will agree. If the execution is not synchronous, then the consensus protocol may not terminate, or may output different decisions at different processes. If a process discovers that its simulation reaches a decision, then this decision is stored as its estimate ei , and its status is advanced to candidate. Notice that processes do not decide on the value output by SynchConsensus at this time. The agreement phase continues until  cons (t) rounds have been simulated (where t < n/2 is the maximum tolerated number of failures). If the simulated consensus protocol SynchConsensus has not terminated, then the simulation is halted. In this case, any process that has not decided will (eventually) enter the fall-back phase. 2. Locking Phase. In the second phase, the processes together simulate the synchronous conditional gossip protocol Gossip(t) for  gossip (t) rounds. Each process i uses ei and status i as its initial rumor. Thus, in a synchronous execution, every non-failed

On the Message Complexity of Indulgent Consensus

289

process receives the rumors of all other non-failed processes. At the end of the phase, process i advances its status under the following conditions: (1) it has received a rumor from at least n/2 + 1 processes that have a status of candidate, locked, or decided for some value v ; (2) estimate ei = v ; and (3) status i = proposal or candidate. In this case, process i updates its status to locked. We will argue (Lemma 1) that at most one value is locked in an execution. 3. Decision Phase. In the third phase, the processes repeat the (synchronous) conditional gossip protocol Gossip(t) for  gossip (t) further (synchronous) rounds. Each process i again uses ei and status i as its initial rumor. At the end of the third phase, process i advances its status under the following conditions: (1) it receives a rumor from at least n/2 + 1 processes that have a status of locked or decided for same value v ; and (2) estimate ei = v ; and status i = locked. In this case, process i updates status i = decided, and decides ei . If all the processes have decided, then from this point on no further messages are broadcast, and the protocol is considered to be terminated. 4. Fall-back Phase. In the final phase, if any process has not yet decided, then the processes all resort to the fall-back consensus protocol PartSynchConsensus. This phase occurs only in executions that are not synchronous. The synchronous round simulation is abandoned at this point. The first step in the fall-back phase is to collect the final status of all the other processes. This proceeds as follows: (1) Each process i that has not yet decided in the previous phase sends a fall-back message to every other process: fallback, ei , status i . (2) Any process that receives a fall-back message enters the fall-back phase and, if it has not already done so, immediately sends a fall-back message fallback, ej , status j to every process, even if it has previuosly decided. (3) When a process i receives n/2 + 1 fall-back messages, it determines if there are any locked values. That is, if any fall-back message contains status locked, then process i sets its estimate ei to the value of that message. Since every message is eventually delivered, and since a majority of processes are correct, it is easy to see that if any correct process begins the fall-back phase, then eventually every process receives n/2 + 1 fall-back messages. Thus, if any value has been locked by a majority of the processes during the initial three phases, then each process executing the fall-back phase will adopt that value as its estimate. Every process that has received a fall-back message then executes PartSynchConsensus, where process i uses estimate ei as its proposal. Eventually PartSynchConsensus produces a decision, and each process decides this value and terminates.

5 Adaptive Indulgent Consensus
In this section we show how to modify the protocol presented in Section 4 to develop an adaptive indulgent consensus protocol. Recall that the protocol in Section 4 begins by simulating the consensus protocol SynchConsensus in the agreement phase. If SynchConsensus is adaptive, it terminates early in synchronous executions with few failures. The goal of this section is to detect when the SynchConsensus simulation has terminated. This detection is accomplished by pausing the consensus simulation every so often and executing a variant of the locking and decision phases, using conditional gossip primitives designed for f  t failures. Before resuming the consensus

290

S. Gilbert, R. Guerraoui, and D.R. Kowalski

protocol simulation, we execute a conditional wake-up protocol: if some processes have decided and other have not yet decided, this protocol wakes the processes that have already decided so that they can continue with the (simulated) consensus protocol. We divide the agreement phase into log n epochs numbered from 0 to log n - 1. Epoch x has length O(2x ), and simulates O(2x ) rounds of SynchConsensus. The epochs are structured such that by the end of epoch x, the system has finished executing round  cons (2x ) of SynchConsensus; thus in a synchronous execution with  2x failures, SynchConsensus completes by the end of epoch x. (Notice that there are < 2log n-1 = n/2 failures.) In more detail, each epoch consists of four phases: (1) Wake-up: waking the processes that have already decided; (2) Agreement: simulating some rounds of the consensus protocol; (3) Locking: execute conditional gossip and determine if any of the values can be locked, and (4) Deciding: execute conditional gossip and determine if any of the values can be decided. If, at the end of log n epochs a process has not yet decided, then it enters the fall-back phase. 1. Waking the Processes. At the beginning of epoch x, there are three possibilities: all the processes have decided, none have decided, or some have decided and some have not. In this last case, a problem might occur if some processes have decided, and thus stopped participating voluntarily in future epochs, while others have not yet decided and need to continue the protocol. The first step in epoch x, then, is to execute WakeUp(2x): each process that has decided is initially asleep and each process that is undecided is initially awake. This step takes  wakeup (2x ) rounds, and guarantees that if the execution is synchronous and there are no more than 2x failures, then every process is awake. 2. Agreement. The second step in epoch x is to simulate some rounds of the consensus protocol SynchConsensus, continuing from the last round simulated in the previous epoch. In epoch 0, the processes simulate the first  cons (1) rounds of the protocol. In epoch x > 0, the process simulate rounds  cons (2x-1 ) + 1, . . . ,  cons (2x ) of the consensus protocol. If a process has decided in an earlier epoch, then it continues to execute the simulation of SynchConsensus only if it was awoken in the wake-up step, and if there are further rounds to simulate. As in Section 4, each process i maintains two variables: ei , its estimate, and status i , its status. Initially, ei is process i's proposal, and status i = proposal. If process i discovers that its simulated consensus protocol has decided value v , and if process i has status equal to proposal, then process i sets ei = v and advances status i = candidate. If an execution is synchronous and has fewer than 2x failures, then the simulated consensus protocol will terminate for all non-failed processes by the end of epoch x. 3. Locking. The third step in epoch x is to simulate the conditional gossip protocol Gossip(2x) for  gossip (2x ) (simulated) rounds, with ei and status i as the rumor for process i. This step is equivalent to the locking phase described in Section 4, except that Gossip(2x) is executed, instead of Gossip(t). If at the end of the locking phase, process i has received rumors from at least n/2 + 1 processes that all have value ei as a candidate, locked, or decided, and if status i = proposal or candidate, then process i

On the Message Complexity of Indulgent Consensus

291

locks value ei . If a process has decided in an earlier epoch, then it executes the gossip only if it was awoken in the wake-up step; otherwise, it remains silent. 4. Deciding. The fourth step in epoch x is to again together simulate Gossip(2x) for  gossip (2x ) (simulated) rounds, again with ei and status i as the rumor for process i. This step is equivalent to the deciding phase described in Section 4, except that Gossip(2x) is executed, instead of Gossip(t). If at the end of the deciding phase, process i has received rumors from at least n/2 + 1 processes that have all locked or decided value ei , and if status i = decided, then process i decides value v . As in the previous step, if a process has decided in an earlier epoch, then it executes the gossip protocol only if was awoken in the wake-up step; otherwise, it remains silent. Fall Back. If, at the end of all log n epochs, any process has not yet decided, then it enters the fall-back phase, as described in Section 4, sending and collecting fall-back messages, and running PartSynchConsensus.

6 Analysis
In this section we provide an outline of the proof that the protocol presented in Section 5 guarantees agreement, validity, and termination. Performance results are given in Section 8. We begin by showing that in every execution, there is at most one value that is decided. The key lemma, in this case, is that at most one value is locked during a locking phase. Notice that this does not depend in any way on the agreement property of SynchConsensus which only holds in synchronous executions. Lemma 1. In every execution, there is at most one value v such that ei = v and status i = locked for any i. Proof. Assume for the sake of contradiction that i and j have locked two distinct values v and v (possibly in two different epochs). This implies that each received rumors during a locking phase from a majority of processes indicating that v and v , respectively, were candidate, locked, or decided values. Thus, some process k (in the intersection of the two majorities) must have at one point had value v as a candidate, locked, or decided value and at another point value v as a candidate, locked, or decided value. But a process never changes its estimate after it has become a candidate, implying a contradiction. Lemma 2. In every execution, there is at most one value v that is decided. Proof. Suppose for contradiction that processes i and j decide two different values v and v . There are three cases: (Case 1) Both decide prior to the fall-back phase: This contradicts Lemma 1, as prior to the fall-back phase, a process only decides a value that has been previously locked. (Case 2) Both decide during the fall-back phase: This contradicts the agreement property of PartSynchConsensus, which guarantees that at most one value is decided. (Case 3) One (say, i) decides v prior to the fall-back phase and one (say, j ) decides v during the fall-back phase: We argue that every process k begins the fall-back phase with initial value v . Process i decides prior to the fall-back phase only if it receives gossip messages indicating that a majority of processes have

292

S. Gilbert, R. Guerraoui, and D.R. Kowalski

locked value v . In the first step of the fall-back phase, process k receives fall-back messages from a majority of the processes. Since a process never changes its estimate once it is locked (prior to the fall-back phase), we can conclude that process k receives a message indicating that value v has been locked. Since there is at most one locked value, by Lemma 1, we conclude that process k adopts value v as its proposal in the fall-back phase. Since every process proposes value v in the fall-back phase, the validity of PartSynchConsensus implies that every non-failed process decides v , resulting in a contradiction. Next, it follows immediately from the unconditional validity of SynchConsensus and Gossip(t), and from the validity of PartSynchConsensus, that the decision is valid: Lemma 3. If v is decided in some execution, then for some process i, initially ei = v . Finally, it is easy to see that, due to the fall-back protocol, the protocol eventually terminates in all executions: Lemma 4. In all executions, every process eventually decides and stops sending messages.

7 Implementing the Three Synchronous Building Blocks
In this section we describe efficient implementations of the building-block protocols described in Section 3. 7.1 Implementing SynchConsensus This section describes two synchronous consensus protocols, both derived from prior work. The first is adaptive and uses O(n log6 n) messages; the second uses a superlinear number of rounds but has optimal O(n) message complexity. Adaptive Synchronous Consensus. In this section, we outline the construction of an adaptive synchronous consensus protocol that is message efficient. We proceed in three steps: we start with a synchronous binary consensus developed in [22]; then we construct a multivalue consensus protocol; finally we transform the resulting protocol into an adaptive protocol. In each step, the challenge is to not increase the asymptotic running time and message complexity too much. Efficient binary synchronous consensus. In [22], Chlebus and Kowalski introduce a binary, message-efficient consensus protocol that tolerates up to n - 1 failures, decides in time O(n) and sends O(n log5 n) point-to-point messages. From binary to multivalue consensus. While the protocol presented in [22] is for binary consensus, it can be readily modified to efficiently support multivalue consensus. Typically, binary consensus protocols are translated into multivalue consensus protocols by agreeing on each bit one at a time. In order to achieve unconditional validity and to avoid increasing time complexity above (n), a slightly different approach is needed.

On the Message Complexity of Indulgent Consensus

293

We construct a binary tournament tree and use binary consensus to navigate the tree. This results in a synchronous multivalue consensus protocol that runs in O(n) times and O(n log6 n) message complexity. Adaptive synchronous consensus Chlebus and Kowalski show in [22] how to transform a message-efficient, synchronous consensus protocol into an adaptive message-efficient synchronous consensus protocol, with an (additive) additional O(n log4 n) message complexity. The end result is a synchronous, adaptive, message-efficient, that is having O(n log6 n) message complexity, consensus protocol SynchConsensus that guarantees unconditional validity: Proposition 1. There exists a synchronous multivalue consensus protocol with message complexity O(n log6 n) and round complexity O(f ) in executions with  f failures. Message-Optimal Synchronous Consensus. In this section we outline the construction of a message-optimal synchronous consensus protocol that uses only O(n) messages and runs in times O(n1+ ) for every 0 <  < 1. We begin by describing a protocol that solves the Interactive Consistency problem, a stronger variant of consensus in which processes agree not simply on a single value, but rather on a vector of decision values, including one for each correct process1 . Formally, the IC problem is defined as follows: each process i begins with an initial value vi , and outputs a decision vector Di such that the following properties are satisfied: (1) Agreement: In every synchronous execution, the decision vector Di of all processes is the same. (2) Unconditional validity: In every execution (synchronous or otherwise), if Di is the decision vector of process i, then D[j ]i is either the initial value of process j or . (3) Conditional validity: If the execution is synchronous and j is correct, then D[j ]i = . (4) Termination: Eventually every process outputs a decision vector Di and terminates. In [14], there is a synchronous protocol that efficiently solves the checkpoint problem, a variant of IC. In particular, the checkpoint problem requires each process i to output a set of processes Pi (rather than a set of values) where every correct process is in the set Pi , and every process in Pi is non-failed at the beginning of the execution. We claim that every synchronous checkpoint protocol can be transformed into a synchronous algorithm for IC: Lemma 5. If A solves synchronous checkpoint in  rounds with message complexity , then there exists a synchronous protocol A that solves IC in  rounds with message complexity . We conclude from Lemma 5, along with the checkpoint protocol from [14]: Proposition 2. There exists a synchronous multivalue consensus protocol with message complexity O(n) and round complexity O(n1+ ), for any 0 <  < 1. 7.2 Implementing Gossip(k) In this section, we describe two synchronous (conditional) gossip protocols. The first terminates in O(k ) rounds and uses O(n log4 n) messages, while the second uses a superlinear number of rounds but has O(n) message complexity.
1

It is interesting to notice that IC cannot be solved in a partially synchronous model [9]. We depend on the IC protocol only in synchronous executions, and hence there is no contradiction.

294

S. Gilbert, R. Guerraoui, and D.R. Kowalski

Adaptive Conditional Gossip. In [22], Chlebus and Kowalski present a gossip protocol tolerating up to n - 1 failures that has message complexity O(n log4 n) and completes in O(log3 n) rounds. When k  log3 n, the running time is O(k ), as desired. When k  log3 n, we resort to a simpler two-round protocol in order to guarantee termination time O(k ): in the first round, each process sends its rumor to processes [1, . . . , k + 1]; in the second round, processes [1, . . . , k + 1] send all the received rumors to all the other processes. Notice that this satisfies the conditional completion property, as there are at most k failures, and is message efficient, as it requires at most 2(k + 1)n = O(n log3 n) messages when k < log3 n. Proposition 3. For all f < n, there exists a synchronous conditional gossip protocol with message complexity O(n log4 n) and round complexity O(f ) in executions with at most f failures. Message-Efficient Conditional Gossip. Recall from Section 7.1, there exists a protocol solving Interactive Consistency in O(n1+ ) rounds with O(n) messages. Notice that any solution to Interactive Consistency is also a solution to gossip, as each process outputs a set of initial values from every correct process. We thus conclude: Proposition 4. There exists a synchronous conditional gossip protocol with message complexity O(n) and round complexity O(n1+ ) in executions, for any 0 <  < 1. 7.3 Implementing WakeUp(k) The conditional wake-up problem is quite close to the conditional gossip problem; the primary difference is that processes initially designated to be asleep must not send any messages at least until they have received a message from a process that was initially awake. Thus, from the point of view of the gossip algorithm, a sleeping process can be treated as faulty until it is awoken. Thus the wake-up problem can be solved using any synchronous gossip protocol that satisfies the following additional Polling Property: in every execution, for every faulty process i there is some process j that, prior to failing, sends a message to i. Nearly every "reasonable" gossip protocol, including the one described in Section Section 7.2, has this property. The simple two-round protocol (when k  log3 n)) also clearly has this property. We conclude: Proposition 5. For all f  t, there exists a synchronous conditional wake-up protocol that has message complexity O(n log4 n) and round complexity O(f ) in executions with at most f failures.

8 Performance Analysis
In this section, we analyze the efficiency of the two algorithms. We begin with the adaptive protocol from Section 5 where SynchConsensus is instantiated by the consensus protocol posited by Proposition 1, Gossip(k ) is instantiated by the gossip protocol posited by Proposition 3, and WakeUp(k ) is instantiated by the wake-up protocol posited by Proposition 5.

On the Message Complexity of Indulgent Consensus

295

Lemma 6. For every synchronous execution with no more than f  t failures, every process decides by time O(f ), terminating prior to the beginning of the fall-back phase. Proof. If f = 1, consider epoch x = 0; otherwise, consider epoch x such that 2x-1 < f  2x . There are two possibilities at the beginning of epoch x: either some process has already decided in an earlier epoch, or no process has decided in an earlier epoch. By the conditional guarantee of the wake-up protocol, however, in either case every non-failed process awakes to participate in epoch x. Next, by the adaptivity property of SynchConsensus, we can conclude that the simulated consensus protocol has output a decision at each non-failed process by the end of the agreement step of epoch x. Thus every non-failed process has status either a candidate, locked, or decided. Since the simulated consensus protocol guarantees agreement, every process with status candidate has the same value. Since every value that is locked or decided was previously a candidate, we can conclude that every process in fact has the same value. In the locking step of epoch x, since there are no more than 2x failures, the conditional gossip ensures that each non-failed process receives rumors from a majority of processes, all of which have value v as candidate, locked, or decided. We can thus conclude that at the end of the locking step, every non-failed process has either locked or decided value v . Similarly, in the decision step of epoch x, since there are no more than 2x failures, the conditional gossip ensures that each non-failed process receives rumors from a majority of processes. We can thus conclude that at the end of the decision phase, every process has decided value v . From this point on, no process sends any further messages. Thus we conclude that by the end of epoch x, every non-failed process has terminated. Finally, we calculate the total running time through the end of epoch x. First, simulating SynchConsensus through the end of epoch x requires O( cons (2x )) rounds, which by the choice of SynchConsensus is O(2x ) rounds. Next, notice that for every epoch y  x, each process executes two instances of Gossip(2y ) and one instance of WakeUp(2y ); these instances take time O( gossip (2y )) and O( wakeup (2y )), respectively, which are both, by assumption, O(2y ). Thus, for each epoch y , the wake-up, locking, and decision phases cost O(2y ) rounds, and hence when summed from epoch 0 to epoch x result in a running time of O(2x ) rounds. Thus the total running time to the end of epoch x, in terms of synchronous rounds, is O(2x ) = O(f ), implying a termination time of  sim (O(f )) = O(f ). We next argue that the resulting protocol is message efficient: Lemma 7. In every synchronous execution, the processes send O(n log6 n) messages. Proof. During the entire simulation of SynchConsensus, the processes collectively send O(n log6 n) messages. In each epoch x, each (non-failed) process executes two instances of Gossip(x) and one instance of WakeUp(x); each such instance uses O(n log4 n) messages, resulting in O(n log6 n) messages total. By Lemma 6, we conclude that each process decides no later than the final epoch, as desired. Thus we conclude:

296

S. Gilbert, R. Guerraoui, and D.R. Kowalski

Theorem 2. There exists an adaptive indulgent consensus protocol with message complexity (n log6 n) and running time O(f ) in synchronous executions with no more than f failures. We next briefly examine the performance of the protocol presented in Section 4, where SynchConsensus is instantiated by the protocol posited by Proposition 2 and Gossip(k ) is instantiated by the gossip protocol posited by Proposition 4. Since the structure of the protocol is identical to that of one epoch of the adaptive protocol, we conclude (much as in Section 6, and omitted to avoid redundancy and save space) that the protocol solves the gossip problem and eventually terminates: Theorem 3. There exists an indulgent consensus protocol with message complexity (n) and a running time O(n1+ ) in synchronous executions.

9 Discussion and Open Questions
We have shown how to implement efficient indulgent consensus algorithms in an eventually-synchronous network. In fact, the algorithms described, with minor modifications, tolerate an even less well-behaved environment. First, even if messages are lost prior to GST, both algorithms continue to behave correctly, as long as each process that has entered the fall-back phase repeats its fall-back message until a decision is reached. Second, even if the bounds d and  are incorrect, both algorithms continue to solve ^ and  ^; consensus as long as the network eventually stabilizes for some (unknown) d only the message-efficiency is sacrificed. Third, in synchronous executions both algorithms can tolerate more failures, in fact, up to n - 1 failures, as long as no more than a minority fail in executions that are not synchronous. One major open question raised by this paper is whether there exists a protocol that is both optimal in message complexity and linear in round complexity. The answer is unknown even for synchronous networks. Another question is whether it is possible to achieve better message complexity in an adaptive algorithm. For some values of f (when f is much smaller than n), it is possible to use an alternative instantiation of the building blocks derived from [14] to achieve a somewhat better message complexity, while terminating in O(f ) time.

References
1. Pease, M., Shostak, R., Lamport, L.: Reaching agreement in the presence of faults. Journal of the ACM 27(2), 228­234 (1980) 2. Fisher, M., Lynch, N., Paterson, M.: Impossibility of distributed consensus with one faulty process. Journal of the ACM 32(2), 374­382 (1985) 3. Dwork, C., Lynch, N., Stockmeyer, L.: Consensus in the presence of partial synchrony. Journal of the ACM 35(2), 288­323 (1988) 4. Chandra, T., Toueg, S.: Unreliable failure detectors for reliable distributed systems. Journal of the ACM 43(2), 225­267 (1996) 5. Lamport, L.: The part-time parliament. ACM Transactions on Computer Systems 16(2), 133­ 169 (1998)

On the Message Complexity of Indulgent Consensus

297

6. Mostefaoui, A., Raynal, M.: Solving consensus using chandra-toueg's unreliable failure detectors: A general quorum-based approach. In: Jayanti, P. (ed.) DISC 1999. LNCS, vol. 1693, pp. 49­63. Springer, Heidelberg (1999) 7. Guerraoui, R., Raynal, M.: The information structure of indulgent consensus. IEEE Transactions on Computers 53(4), 453­466 (2004) 8. Schiper, A.: Early consensus in an asynchronous system with a weak failure detector. Distributed Computing 10(3), 149­157 (1997) 9. Guerraoui, R.: Indulgent algorithms (preliminary version). In: Proceedings of the 19th Symposium on Principles of Distributed Computing (PODC), pp. 289­297 (2000) 10. Lynch, N.: Distributed Algorithms. Morgan Kaufman, San Francisco (1996) 11. Dutta, P., Guerraoui, R.: The inherent price of indulgence. In: Proceedings of the 21st Symposium on Principles of Distributed Computing (PODC), pp. 88­97 (2002) 12. Lamport, L.: Fast paxos. Technical Report MSR-TR-2005-12, Microsoft (2005) 13. Chandra, T., Toueg, S.: Time and message efficient reliable broadcasts. In: van Leeuwen, J., Santoro, N. (eds.) Distributed Algorithms. LNCS, vol. 486, pp. 289­303. Springer, Heidelberg (1991) 14. Galil, Z., Mayer, A., Yung, M.: Resolving message complexity of byzantine agreement and beyond. In: Proceedings of the 36th Symposium on Foundations of Computer Science (FOCS), pp. 724­733 (1995) 15. Fisher, M., Lynch, N.: A lower bound for the time to assure interactive consistency. Information Processing Letters (IPL) 14(4), 183­186 (1982) 16. Dolev, D., Strong, H.: Requirements for agreement in a distributed system. Technical Report RJ 3418, IBM Research, San Jose, CA (March 1982) 17. Lamport, L., Fisher, M.: Byzantine generals and transaction commit protocols. Unpublished (April 1982) 18. Dolev, D., Reischuk, R., Strong, H.R.: Early stopping in byzantine agreement. Journal of the ACM 37(4), 720­741 (1990) 19. Charron-Bost, B., Schiper, A.: Improving Fast Paxos: being optimistic with no overhead. In: Proceedings of the 12th Pacific Rim International Symposium on Dependable Computing (PRDC), pp. 287­295 (2006) 20. Dwork, C., Halpern, J., Waarts, O.: Performing work efficiently in the presence of faults. SIAM Journal on Computing 27(5), 1457­1491 (1998) 21. Chlebus, B., Kowalski, D.: Gossiping to reach consensus. In: Proceedings of 14th Symposium on Parallel Algorithms and Architectures (SPAA), pp. 220­229 (2002) 22. Chlebus, B., Kowalski, D.: Robust gossiping with an application to consensus. Journal of Computer and System Science 72(8), 1262­1281 (2006) 23. Amdur, S., Weber, S., Hadzilacos, V.: On the message complexity of binary agreement under crash failures. Distributed Computing 5(4), 175­186 (1992) 24. Hadzilacos, V., Halpern, J.: Message-optimal protocols for byzantine agreement. Mathematical Systems Theory 26(1), 41­102 (1993)

Gathering Autonomous Mobile Robots with Dynamic Compasses: An Optimal Result
Taisuke Izumi, Yoshiaki Katayama, Nobuhiro Inuzuka, and Koichi Wada
Nagoya Institute of Technology Gokiso-cho, Showa-ku, Nagoya, Aichi 466-8555, Japan {t-izumi,katayama,inuzuka,wada}@nitech.ac.jp

Abstract. Let consider n autonomous mobile robots that can move in a two dimensional plane. The gathering problem is one of the most fundamental tasks of autonomous mobile robots. In short, given a set of robots with arbitrary initial locations, gathering must make all robots meet in finite time at a point that is not predefined. In this paper, we study about the feasibility of gathering by mobile robots that have absolute error dynamic compasses. While the direction of each local coordinate system is fixed in usual systems, the dynamic compass model allows the angle difference between a local coordinate system and the global coordinate system to vary with time in the range of [0, ]. This paper proposes a semi-synchronous gathering algorithm for n robots with (/2 - )-absolute error dynamic compasses, where is an arbitrary small constant larger than zero. To the best of our knowledge, the proposed algorithm is the first one that considers both inaccurate compass models and more than two robots. We also show the optimality of our algorithm. It is proved that for any   /2, there is no algorithm to gather two robots with -absolute error dynamic compasses.

1

Introduction

In recent years, cooperations of a large number of autonomous mobile robots have received much attention. Because of its interesting features, such as inexpensiveness, fault tolerance, and high-level parallelism, many researchers study about them from several kinds of aspects. In particular, the algorithmic issues of autonomous mobile robots are actively studied in the literature of the distributed computing. In most of algorithmic studies about autonomous mobile robots, a robot is modeled as a point in a plane, and its capability is quite weak. It is usually assumed that robots are oblivious (no memory to record past situations), anonymous and uniform (no IDs to distinguish two robots, and all robots run an identical algorithm). In addition, it is also assumed that each robot has no explicit direct means of communication. Typically, the communication between two robots is done in the implicit way that each robot observes the environment, which includes the position of other robots. From practical aspects, such weak capabilities of robots are favorable in the point of cost and implementability. However, too-weak robot systems are hard to accomplish the task to be
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 298­312, 2007. c Springer-Verlag Berlin Heidelberg 2007

Gathering Autonomous Mobile Robots with Dynamic Compasses

299

completed. Thus, revealing "the weakest" capability of robot systems to accomplish a given task is one of worthwhile challenges in the theoretical studies of autonomous mobile robots. This paper is also an exploration of such weakest capabilities. The problem considered in this paper is gathering, which is one of the most fundamental tasks of autonomous mobile robots. In short, given a set of robots with arbitrary initial locations, gathering must make all robots meet in finite time at a point that is not predefined. Because of its simplicity, the gathering problem is actively studied before now: Many researchers tackle this problem, and show a number of possibility/impossibility results under the different assumptions [1,2,4,5,7,10,11,12,13,14]. In particular, we focus on the disagreement of local coordinate systems. As we mentioned, robots implicitly communicate with each other by observing environments. Then, the observation of each robot is done in terms of its local coordinate system, and thus it differentiates the capability of robots how local coordinate systems agree with each other. The seminal paper by Suzuki and Yamashita [13] shows that even two oblivious robots cannot achieve gathering if there is no agreement about their local coordinate systems. On the other hand, it also shows that it is possible to gather any number of robots if all of their local coordinate systems are consistent. These two results yield an interesting question, "how much agreement is necessary to accomplish gathering task?" The study to answer this question is independently and concurrently originated by two papers [6,11]. Both of two papers quantify the agreement level of local coordinate systems by the angle difference for the global coordinate system (Figure 1), and show that, in asynchronous systems, two-robot gathering is possible if the differences of two robots' coordinate systems are bounded by a certain degree. The upper bounds proposed by the above two papers are /4, and it is improved to /3 by Katayama et al. [7]. More recently, this upper bound is drastically improved to  - , where is an arbitrary small constant larger than zero [14]. While these results assume that the differences of local coordinate systems are fixed constants, the paper by Katayama et al. [7] also considers the dynamic compass model, which allows the difference of a local coordinate system to vary with time. It also shows that it is possible to gather two robots with dynamic compasses in asynchronous systems if the maximum angle difference of each local coordinate systems is at most /4. This paper has also the same research direction as the above papers. The contribution of this paper is as follows: This paper assumes the Suzuki-Yamashita model (SYm), which is also known as the semi-synchronous systems [13]. In typical algorithmic robot models, an execution of a robot consists of consecutive cycles. One cycle includes the observation of environments, local computation, and movement. While fully-asynchronous systems assume no bound on the time length of one cycle, semi-synchronous ones assume that each robot works under synchronized rounds. Each robot can execute one cycle in one round. However, different from fully-synchronous systems, every robot does not necessarily execute one cycle in every round. Only a subset of all robots, which is determined

300

T. Izumi et al.

Active Robot0 Robot1 Inactive Robot2 (1) Semi-Synchronous system Robot0 Robot1 Robot2

(2) Asynchronous system One cycle

Fig. 1. A local coordinate system with difference

Fig. 2. Two timing models

by the scheduler, executes one cycle in each round (Figure 2). We present an algorithm that gathers n robots having the dynamic compasses whose maximum difference is at most /2 - , where is an arbitrary small constant. The most important contribution of our result is to consider n robots. To the best of our knowledge, our gathering algorithm is the first one considering both the disagreement of local coordinate systems and more than two robots. We present a comparison between existing algorithms and ours in Figure 3. The n-robot algorithm is designed in a constructive way: We first design the algorithm for gathering two robots having dynamic compasses whose maximum difference is /2 - . Then, slightly modifying the two-robot algorithm, we obtain a conditional nrobot algorithm, where "conditional" implies that the algorithm correctly works only when the initial configuration satisfies a certain condition. To remove the condition, we further design an algorithm that reduces any configuration to one satisfying the condition. The general n-robot algorithm is obtained by combining these two algorithms. We also show that there is no algorithm that gathers two robots having the dynamic compasses whose maximum difference is larger than or equal to /2 in SYm. This impossibility results implies that our algorithm is optimal in the sense of the angle difference of local coordination systems. The rest of this paper is organized as follows. In Section 2, we introduce the robot model, dynamic compass model, and several necessary definitions and notations. In Section 3, we present a two-robot gathering algorithm, which is the basis of the n-robot one. In Section 4, we explain the construction of the n-robot gathering algorithm which is obtained from the two-robot algorithm in Section 3. The impossibility result is shown in Section 5. Section 6 addresses the relation between our model and another inaccurate compass models.

2
2.1

The System Model
Robots with Dynamic Compasses

The robot system considered in this paper is the Suzuki-Yamashita model (SYm), which is also known as the semi-synchronous robot model [13]. The system consists of a set of autonomous mobile robots R = {R1 , R2 , · · · , Rn }. One robot is

Gathering Autonomous Mobile Robots with Dynamic Compasses Timing Assumption Souissi et al. [11] Async. Imazu et al. [6] Async. Katayama et al. [7] Async. Katayama et al. [7] Async. Yamashita et al. [14] Async. This paper SemiSync. Compass Angle #Robots Model Difference Fixed /4 2 Fixed /4 2 Fixed /3 2 Dynamic /4 2 Fixed -  2 Dynamic /2 -  n

301

 : Optimal Upper Bound Async. : Asynchronous Systems SemiSync. : Semi-Synchronous Systems Fig. 3. The comparison of this paper's algorithm

modeled as a point located on a two-dimensional space. To specify the location of each robot consistently, we introduce the global Cartesian coordinate system. In addition, we also introduce the discrete global time 0, 1, 2, . . .. Notice that these global entities are introduced only for ease of explanations, and thus each robot cannot be aware of them. Each interval [t, t + 1] is called a round. The global coordinate where a robot Ri stays at time t is denoted by Ri (t). Throughout this paper, any coordinate is represented by a vector. To denote vectors, we use boldfaced characters. For a vector V, |V| and (V) denote the length and the polar angle of V (i.e., the value d and  (0   < 2 ) satisfying V = d(cos , sin )T ). The robots are anonymous and oblivious. That is, each robot has no identifier distinguishing itself and others, and cannot explicitly remember the history of its execution. In addition, no device for direct communication is equipped. Cooperations of robots are done in an implicit manner: Each robot has a sensor device to observe the environment (i.e., the positions of all robots). The observation of an environment is represented as the set of points on the local coordinate system that the observer has. The local coordinate system of a robot is the Cartesian coordinate system whose origin is the current position of the robot. Local coordinate systems have only weak (or no) agreement on their unit lengths and the directions of x-axis and y -axis. A compass model defines the agreement level of local coordinate systems. In this paper, we consider the -absolute error dynamic compass model, which is first introduced by Katayama et al. [7].1 The -absolute error model allows each robot to have a local coordinate system that is counterclockwisely tilted from the global coordinate system by a degree less than or equal to .2 The dynamism of compasses implies that the tilt angle of
1

2

While the original paper defines two kinds of dynamic compass models, called semidynamic compass and full-dynamic compass, we do not distinguish them and simply use the term "dynamic compass" because the original two compass models are defined on the fully-asynchronous model (Corda ) [8,9] and they are equivalent in SYm. The original paper defines the range of tilt angle in -absolute error model as [-/2, /2]. However, this paper defines it as [0, ] for ease of presentations. Both of two definitions are equivalent.

302

T. Izumi et al.

each local coordinate system can vary with time. The tilt angle of Ri 's local coordinate system in round t is called the compass difference of Ri at t, and is denoted by i (t) (0  i (t)  ). The compass configuration (t) in round t is the n-tuple whose i-th entry is i (t). The compass model considered in this paper also allows each robot has a different unit length. For any robot Ri , the ratio of the unit length in Ri 's coordinate system to that in the global coordinate system is called the scaling ratio of Ri , and is denoted by sci . For each robot Ri , we define the observation function Z(i,t) (p) that transforms a global coordinate p to the coordinate in terms of Ri 's local coordination system at t. Formally, the observation function Z(i,t) (p) of Ri at time t is defined as follows: Z(i,t) (p) = 1 sci cos i (t) sin i (t) - sin i (t) cos i (t) (p - Ri (t))

-1 The inverse function of Z(i,t) is denoted by Z( i,t) . The configuration C (t) at time t is defined as the n-tuple of global coordinates (R1 (t), R2 (t), . . . , Rn (t)). We also define the point set P (t) as a set of global coordinates {Ri (t)|i  [1, · · · , n]} without multiplicity. In this paper, we assume that each robot cannot detect the multiplicity of robots. That is, robots cannot distinguish the configuration where two or more robots are located at a point and one where only one robot is located at the same point. It implies that if a robot observes the environment it cannot obtain the current configuration, but the current point set. We also define the observation function Z(i,t) over all point sets, i.e., Z(i,t) (P (t)) = {Z(i,t) (p)|p  P (t)}.

2.2

Algorithms, Executions, and the Gathering Problem

At any round t(t = 0, 1, 2, · · ·), each robot is either active or inactive. If a robot is active at t, it observes the environment, and computes the destination point of the movement performed in round t. Therefore, an algorithm is defined as a function  that maps any point set to a vector that represents the destination. Since each robot observes an environment in terms of its local coordinate systems, the current point set P (t) does not directly passed to  as an input. Actually, in activation of robot Ri at time t, the conversion of P (t) by the observation function Z(i,t) is passed to the algorithm. By the same reason, the output of the algorithm -1 is also converted by the inverse function Z( i,t) . Therefore, if a robot Ri is active -1 at t, the global coordinate where Ri stays at t + 1 is Z( i,t) ( (Z(i,t) (P (t)))). Since this paper considers only deterministic algorithms, the configuration C (t + 1) is determined by the previous configuration C (t), the set of active robots in round t, and the compass configuration in round t. Thus, we can describe an execution of the system as an alternating sequence C (0), ( (0), (0)), C (1), ( (0), (0)), C (2), · · ·, where  (t) is the set of active robots in round t. In this paper, we only consider fair executions, i.e., infinite executions where every robot becomes active infinitely many times. The gathering problem must ensure that all robots eventually meet at a point that is not

Gathering Autonomous Mobile Robots with Dynamic Compasses

303

R (t+1)

2

R2(t)

|V|

(t)
R (t)

y x
The global coord. sys.

R (t)

1

-2

1

|V|

R (t+1)

1

Fig. 4. The illustration of the algo- Fig. 5. The illustration of the roundabout move rithm

predefined, beginning from any configuration. Formally, we say that an algorithm A solves the gathering problem if for any fair execution of A, there exists a time te such that |P (t)| = 1 holds for any t > te .

3
3.1

The Two-Robot Algorithm
Algorithm GatherTwoRobots

In this section, we first show a gathering algorithm GatherTwoRobots for two robots with (/2 - )-absolute error dynamic compasses, where is an arbitrary small constant satisfying 0 < < /2. The algorithm GatherTwoRobots consists of three types of moves, Approach, Wait, and Roundabout. The approach is a movement that the robot moves to the point of the other robot. The wait is a movement that the robot has no movement actually. Letting a robot R observe the other at the coordinate V in its local coordinate system, it performs approach move if  < (V)  3/2 + , and performs wait move if 0 < (V)  /2 + . Otherwise, the robot R performs roundabout move: The destination of R's roundabout move is the coordinate |V|(cos((V)+  - 2 ), sin((V)+  - 2 )) (i.e., the robot R moves to the direction at angle (V) +  - 2 by length V). We illustrate the behavior of our algorithm in Figure 4 and 5. The correctness of our algorithm derives from two key properties: The first one is that any movement decreases the angle formed by the global Y -axis and the line passing through the positions of two robots unless gathering is achieved. Then, it is guaranteed that two robots eventually observe each other in the directions near the global north and south respectively 3 if they does not gather. The second key idea is that if two robots observe each other in the direction near the global north or south, they necessarily performs approach-move and waitmove respectively, regardless of their difference. Then, gathering is achieved. We show the correctness of the algorithm GatherTwoRobots. We define a vector AB(t1 , t2 ) as B(t2 ) - A(t2 ), where A, B  {R1 , R2 } and t1 and t2 are global times. In particular, let R1 R2 (t, t) = R(t) for short. Without loss of generality,
3

The global north/south imply the positive/negative direction of the global Y -axis.

304

T. Izumi et al.

we assume /2 - < (R1 R2 (0))  3/2 - (i.e., the names R1 and R2 are assigned in the manner satisfying this assumption). Lemma 1. If /2 - at t or later. < (R(t))  /2 + holds, no roundabout move occurs

Proof. The robots R1 and R2 observe each other in the direction (R(t)) - 1 (t) and (R(t)) +  - 2 (t) in terms of their local coordinate systems respectively. Then, we obtain 0 < (R(t)) - 1 (t)  /2 - and  < (R(t)) +  - 2 (t)  - + 3/2. These imply that R1 and R2 can perform only wait and approach move respectively. 2 This lemma yields the following corollary because simultaneous wait of two robots never occurs. Corollary 1. If /2 - < (R(t))  /2+ holds, two robots eventually gather. Lemma 2. For any t, 2  (R(t)) - (R(t + 1))  achieved at t + 1. holds unless gathering is

Proof(sketch). If two robots respectively perform approach and wait, the gathering clearly achieved. Since simultaneous wait of two robots never occurs, we need to consider only the cases a robot R1 performs roundabout. Figure 6 shows all of such cases. The illustrations (a), (b), and (c) respectively indicate the cases where the other robot R2 performs wait, approach, roundabout. In any case, we can see (R(t)) decreases by such that   2 holds. This implies that the lemma holds. 2 Theorem 1. The algorithm GatherTwoRobots achieves gathering of two robots with (/2 - )-absolute error dynamic compasses. Proof. By Lemma 2,the value (R(t)) gradually decreases with the progress of time t, and eventually (R(t))  (/2 - , /2 + ] holds at a certain time t. Then, by Corollary 1, gathering is achieved. 2

4

The n-Robot Algorithm

This section provides a gathering algorithm for n robots with (/2 - )-absolute error dynamic compasses. The n-robot algorithm consists of two sub-algorithms. The first one, called GatherNRobots, is obtained from the algorithm GatherTwoRobots. It achieves gathering of n robots under the assumption that the point set of the initial configuration has a unique longest-distance segment(LDS): The LDS at t is the maximum-length segment of all defined by any pair of points in P (t). Notice that two or more segments become LDSs because of the equality of their lengths. If the initial configuration has two or more LDSs, the algorithm GatherNRobots does not work correctly. To handle such initial configuration, we use the second algorithm ElectOneLDS, which works as a preprocessor of

Gathering Autonomous Mobile Robots with Dynamic Compasses
R (t+1)

305

2


R (t+1)

2


: Line R (t+1)R (t+1)

2

R (t)

2

1

2

(=R2(t))

R (t)

2

: Line R (t)R (t) R (t)

1

R (t)

1

2

1

2



R (t+1)

2

(=R (t))

1

2


R1(t+1) (c)

2



R (t+1)

1

R (t+1)

1

(a)

(b)

Fig. 6. The proof of Lemma 2

GatherNRobots. The objective of ElectOneLDS is to make the system reach a configuration where (1) a unique LDS is elected, or (2) gathering is achieved. Consequently, the combination of GatherNRobots and ElectOneLDS becomes an n-robot gathering algorithm that works correctly from any arbitrary initial configuration. 4.1 Gathering Under a Unique Longest-Distance Segment

In this subsection, we first introduce the algorithm GatherNRobots that achieves gathering under the assumption of a unique LDS. The LDS at time t is denoted by l(t) if it is uniquely determined. Let pla (t) and plb (t) be two endpoints of l(t). The vector plb (t) - pla (t) is denoted by l(t). Then, without loss of generality, we assume /2 - < (l(t))  3/2 - . For a time t when the LDS is uniquely determined, we define Rl(t) = {Ri  R|Ri (t) = pla (t) or Ri (t) = plb (t)}. Intuitively Rl(t) is the set of robots that stay at one of the end points of the LDS. If Rl(t) = R, we say that the configuration C (t) (or the point set P (t)) is 2-gathered. The algorithm GatherNRobots has the algorithm GatherTwoRobots as its subroutine, and its key idea is almost same as that of GatherTwoRobots: The algorithm GatherNRobots first gathers all robots at the endpoints of the unique LDS. Then, by the same scheme as GatherTwoRobots, it gradually reduces the angle (l(t)) to /2. If (l(t)) is near /2 and all robots are gathered at two endpoints of l(t), all robots perform approach move or wait move. Thus, gathering is achieved. In the followings, we briefly explain the behavior of GatherNRobots: If the current configuration is not 2-gathered, the algorithm first makes the configuration 2-gathered. More precisely, if a robot Ri  Rl(t) observes and recognizes that the current configuration is not 2-gathered, it moves to either pla (t) or plb (t). All robots in Rl(t) wait until the current configuration becomes 2-gathered. If the current configuration becomes 2-gathered, the behavior of each robot follows the algorithm GatherTwoRobots. The robots decrease the angle (l(t)) by counterclockwisely rotating the LDS. Then, even if all robots are gathered at the

306

T. Izumi et al.

Three robots
p

lb(t)

l

(t)
p

Two robots

la(t)

One performs approach, and another one performs roundabout
p

The configuration C(t)

lb(t+1)
l

(t+1) (t)
p

p

(=

lb(t+1) p (t)) lb

l

(t)
p

p

lb(t)
l

l

(t+1)

la(t)

la(t)

One performs roundabout

p

la(t+1)

One performs roundabout
p

la(t+1)

(a) An Example of three-point cases

(b) An Example of four-point cases

Fig. 7. The proof of Lemma 4

endpoints of the LDS before the rotation, robots may stay at three or more points at the configuration after the rotation (notice that two robots at a same point can behave differently because their compass difference is not same). However, even in the case where the robots stay at three or more points, it is guaranteed that a LDS l(t + 1) satisfying  (l(t)) - (l(t + 1))  2 is uniquely determined. We present a proof outline for the correctness of GatherNRobots. A number of proofs are omitted because of lack of space. The following lemma clearly holds. Lemma 3. Let C (t) be a configuration that has a unique LDS, but that is not 2-gathered. Then, the system reaches to a 2-gathered configuration having the same LDS as C (t). Lemma 4. Let C (t) be a 2-gathered configuration. Then, the system eventually reaches to the configuration C (t ) such that gathering is achieved in C (t ) or  (l(t)) - (l(t ))  2 holds. Proof(sketch). If no roundabout move occurs, the system eventually achieves gathering because any approach movement is performed (1) only by ones at pla (t) and the destination is plb (t), or (2) only by ones at plb (t) and the destination is pla (t). Thus we only have to consider the case where a roundabout move eventually occurs. Assume that a roundabout move occurs in round t - 1 (t > t). Then, the point set P (t ) consists of two, three or four points (Figure 7). In the case of two points we can show the lemma by the same way as Lemma 2. In the

Gathering Autonomous Mobile Robots with Dynamic Compasses

307

case of three points or four points, the configuration is not 2-gathered. However, in both cases, the point set P (t ) has a unique LDS l(t + 1). In addition, by the same argument as the proof of Lemma 2, we can show 2  (l(t)) - (l(t ))  holds. Thus, the lemma holds. 2. The following lemma is a simple extension of Lemma 1 for n-robot systems, and can be proved by the same way as the proof of Lemma 1. Lemma 5. If /2 -  (l(t))  /2 + holds, no roundabout move occurs at t or later. By combining Lemmas 3, 4, and 5, we can obtain the following theorem. The proof is almost same as that for Theorem 1. Theorem 2. If the point set of the initial configuration C (0) has a unique LDS, the algorithm GatherNRobots achieves gathering of n robots with (/2 - )absolute error dynamic compasses. In addition to the above main theorem, we also show the following subtheorem, which guarantees that the behavior of GatherNRobots and one of ElectOneLDS do not conflict (the proof is omitted). Theorem 3. Let C (t) be the configuration having a unique LDS. Then, any configuration following C (t) has a unique LDS unless it achieves gathering. 4.2 Election of a Unique LDS

This section provides the algorithm ElectOneLDS that reduces any configuration to one where a unique LDS is elected or gathering is achieved. To explain the behavior of ElectOneLDS, we first introduce the several notations and definitions. For a point set P (t), we define H (t) as its convex hull. The number of edges constituting H (t) is denoted by #H (t). A convex hull H (t) is symmetric if all edges constituting the convex hull H (t) have a same length.4 The convex hull that is not symmetric is asymmetric. We say that the configuration C (t) is contractable if (1) its convex hull H (t) is symmetric and any point in P (t) is a vertex of H (t) or the center of gravity of H (t), or (2) H (t) is asymmetric and any point in P (t) is a vertex of H (t). Let Ri be the robot such that its coordinate Ri (t) is a vertex of H (t). Then, we define the left neighbor of Ri at t as the next vertex of Ri (t) in the counterclockwise traversal on H (t)'s border. We also define the right neighbor in the same fashion. Then the segments between Ri and its right/left neighbors are called the left/right arm of Ri at t respectively. The algorithm ElectOneLDS works only when the current point set has two or more LDS. By Theorem 3, once the system reaches to the configuration having a unique LDS, it is guaranteed that ElectOneLDS never works again. In addition, the algorithm GatherNRobots works only when a unique LDS is elected. These imply that the algorithms ElectOneLDS and GatherNRobots do not disturb each other, and thus their composition is possible.
4

Notice that, while it is not essential, a symmetric convex hull is not necessarily a regular polygon. For example, a rhomboid is symmetric.

308

T. Izumi et al.

Contracting Robots Contracting Robots

(a) Asymmetric small

Contracting Robots

(b) Gathering
The shortestlength edge LDS Unique LDS

(c) Symmetric small

Fig. 8. Edge contractions

Fig. 9. ments

Symmetry-breaking

move-

Roughly speaking, the behavior of ElectOneLDS is that (1) if the current configuration is not contractable, the algorithm reduces it to a contractable one, and (2) if the current configuration is contractable, the algorithm carries forward the election of a unique LDS. More precisely, in the first case, each robot that does not stay at a vertex of H (t) (or stay at neither a vertex of H (t) nor H (t)'s center of gravity when H (t) is symmetric) moves to a certain vertex of H (t). Then, the convex hull eventually becomes contractable. In the second case, the algorithm decreases the number of edges #H (t) until the point set has a unique LDS. Notice that this scheme necessarily elects a unique LDS because the convex hull H (t) eventually becomes a segment when #H (t) = 2. The number of edges #H (t) is decreased by contracting a shortest-length edge in H (t): A contracting robot are defined as one whose left arm is a shortest-length edge in H (t) and the right arm is not the shortest.If a contracting robot observes and recognizes that the current configuration is contractable, it moves the coordinate of its left neighbor. The contracting robots move to their left neighborhood points, and consequently their left arms are contracted (Figure 8). Then, during the contraction, all non-contracting robots do not move. The shortest-edge contraction correctly works if the current convex hull H (t) is asymmetric. However, if H (t) is symmetric, all edges in H (t) is shortest, and thus no edge contraction occurs. To handle this case, we introduce another contraction scheme (symmetry-breaking movement): If a robot recognizes that the current configuration is contractable but the corresponding convex hull is symmetric, it moves to H (t)'s center of gravity. Then, if all robots simultaneously moves to the center of gravity, gathering is clearly achieved. Otherwise, the number of edges constituting the convex hull eventually decreases (Figure 9). We can show that the following theorem (The proof is omitted for lack of space). This theorem implies that the composition of ElectOneLDS and GatherNRobots becomes a gathering algorithm for n robots with (/2 - )-absolute error dynamic compasses. Theorem 4. The algorithm ElectOneLDS correctly elects a unique LDS unless gathering is achieved.

Gathering Autonomous Mobile Robots with Dynamic Compasses

309

5

Impossibility Result

In this section, we show that there is no gathering algorithm for two robots with -absolute error dynamic compass when   /2. Throughout the following proofs, we suppose for contradiction that there exists a gathering algorithm for two robots with -absolute error dynamic compasses (  /2), which is denoted by A. We first give the definition of termination vector, which is an important notion to prove the impossibility. Definition 1 (Termination vector). The vector V = Z(1,t) (R1 R2 (t)) is a termination vector of A if R1 (t) = R1 (t + 1) holds in the execution C (t), ({R1 }, (0, 0)), C (t + 1). Intuitively, a termination vector V is one such that even if an active robot observing the other at the coordinate V in terms of its local coordinate system, it does not change the position. The following lemma is a fundamental lemma about gathering algorithms for robots with erroneous compasses. Lemma 6. Letting V1 and V2 be any two termination vectors of the algorithm A, |(V1 ) - (V2 )| <  -  holds. Proof. Suppose |(V1 ) - (V2 )|   -  for contradiction. Without loss of generality, we assume (V1 ) > (V2 ). Let  = (V1 )-(V2 ). Then, we consider the initial configuration C (0) as follows: ­ The robot R1 is located at the origin of the global coordinate system. The local coordinate system of R1 is identical to the global one. ­ The robot R2 is located at the point represented by V1 . The scaling ratio sc2 is |V1 |/|V2 |. Now we consider the execution E beginning from C (0) such that 2 (t) =  -  ( ) and 1 (t) = 0 holds for any t. Then, since Z(1,t) (R1 R2 (t, t)) = V1 and Z(2,t) (R2 R1 (t, t)) = V2 hold for any t, both R1 and R2 do not move at all, and thus gathering is not achieved. It is a contradiction. 2 Let C (t) be a configuration. We say that C (t) is 2-movement if there exist two tilt angles 1 , 2  {0, /2} such that two robots changes their position in the one-round execution E = C (t), ({R1 , R2 }, (1 , 2 )), C (t + 1). Lemma 7. Any configuration C (t) is 2-movement. Proof. Let V = R1 R2 (t, t) for short. We first consider the execution E = C (t), ({R1 , R2 }, (0, 0)), C (t + 1). If both R1 and R2 moves in E , the C (t) is 2-movement. Thus, we only consider the case where at least one robot does not change its position in E . From Lemma 6, either V or -V is not termination vector. Thus, one robot changes the position in E . Without loss of generality, we assume that R2 is such one. Then, we consider the execution E = C (t), ({R1 , R2 }, (/2, 0)), C (t + 1). From Lemma 6, the vector Z(1,t) (V) in E is not termination vector because |(Z(1,t) (V)) - (V)| = /2 holds in E . It follows that two robots change their positions in the execution E . That is, C (t) is 2-movement. 2

310

T. Izumi et al.

Lemma 8. There exists an infinite execution of A where gathering is not achieved and both R1 and R2 become active infinitely-many times. Proof. Let C (t) be a configuration. From lemma 7, there exists an execution E = C (t), ({R1 , R2 }, (1 , 2 )), C (t + 1), where R1 and R2 change their positions. Then, we consider two executions E1 = C (t), ({R1 }, (1 , 2 )), C1 (t +1) and E2 = C (t), ({R2 }, (1 , 2 )), C2 (t + 1). If C (t + 1) achieves gathering, both C1 (t + 1) and C2 (t + 2) do not achieve gathering. In contrast, if either E1 or E2 achieves gathering, E does not achieve gathering. This implies that for any configuration C , there exists one-round execution beginning from C where (1) gathering is not achieved, and (2) arbitrary one of R1 and R2 is active. It follows that there exists infinite fair execution where gathering is not achieved. 2 This lemma implies the main theorem. Theorem 5. There is no gathering algorithm for two robots with -absolute error dynamic compasses when   /2.

6

Discussion

More recently, Cohen and Peleg introduced another model of erroneous compasses [3], where each robot may observe another robot at a position slightly different from the actual one. In this section, we explain the relation between the above model and our model. The model by Cohen and Peleg introduces two accuracy measurements 0 and 0 for angle and distance respectively. In this model, each robot R1 at the coordinate d(cos , sin ) can be observed by other robots as it stays a coordinate d (cos  , sin  ) such that d  [d - 0 , d + 0 ] and   [ - 0 ,  + 0 ] holds. The model restricting by 0 = 0 (say Mrr- ) may seem to be equivalent to our model with 0 -absolute error dynamic compasses. However, those two models inherently different and incomparable. In our model, while the coordinate system within each robot is agreed (i.e., in each cycle of a robot, the observation phase and move phase uses a same coordinate system), the coordinate systems between two or more robots are not agreed. On the other hand, the model Mrr- has no agreement between observation phases and move phases in each cycle, however, the tilt angle of each local coordinate system is the same as the global one (and thus all local coordinate systems are agreed with each other). A typical example that differentiates the ability of these compass models is as follows: Let us consider the system of two robots. In our model, it is possible that a robot moves to the position of another robot, which is impossible in Mrr- because the robot cannot detect the exact location of another one. In contrast, in the model Mrr- , it is possible that two robot moves to a same direction (e.g., the global north). However, such movement is impossible in our model because tilt angles of local coordinate systems can be different. It should be noted that the original paper of dynamic compass models [7] introduces two different classes of dynamic compass models, semi-dynamic compass model and full-dynamic compass model. The semi-dynamic compass is one

Gathering Autonomous Mobile Robots with Dynamic Compasses

311

considered in this paper. The full-dynamic compass is more weaker model of semi-dynamic one, which allows compass differences to vary during a cycle, i.e., the compass difference can be disagreed between the observation phase and the move phase within a cycle. This implies that the full-dynamic compass model is also weaker than Mrr- .

Acknowledgment
This work is supported in part by the Japan Society for the Promotion of Science: Grant-in-Aid for Scientific Research on Priority Area 'New Horizons in Computing(C08)' and Grant-in-Aid for Young Scientists(B) (19700058).

References
1. Agmon, N., Peleg, D.: Fault-tolerant gathering algorithms for autonomous mobile robots. SIAM Journal of Computing 36(1), 56­82 (2006) 2. Cieliebak, M., Flocchini, P., Prencipe, G., Santoro, N.: Solving the robots gathering problem. In: Baeten, J.C.M., Lenstra, J.K., Parrow, J., Woeginger, G.J. (eds.) ICALP 2003. LNCS, vol. 2719, pp. 1181­1196. Springer, Heidelberg (2003) 3. Cohen, R., Peleg, D.: Convergence of autonomous mobile robots with inaccurate sensors and movements. In: Durand, B., Thomas, W. (eds.) STACS 2006. LNCS, vol. 3884, pp. 549­560. Springer, Heidelberg (2006) 4. D´ efago, X., Gradinariu, M., Messika, S., Parv´ edy, P.R.: Fault-tolerant and selfstabilizing mobile robots gathering. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 46­60. Springer, Heidelberg (2006) 5. Flocchini, P., Prencipe, G., Santoro, N., Widmayer, P.: Gathering of asynchronous robots with limited visibility. Theoreical Computer Science 337(1-3), 147­168 (2005) 6. Imazu, H., Itoh, N., Inuzuka, K.Y.N., Wada, K.: A gathering problem for autonomous mobile robots with disagreement in compasses. In: Proc. of First Workshop on Theoretical Computer Science in Izumo, pp. 43­44, 2005 (in Japanese) 7. Katayama, Y., Tomida, Y., Imazu, H., Inuzuka, N., Wada, K.: Dynamic compass models and gathering algorithm for autonomous mobile robots. In: Proc. of 14th International Colloquium on Structural Information and Communication Complexity (SIROCCO). LNCS, vol. 4474, pp. 274­288 (2007) 8. Prencipe, G.: CORDA: distributed coordination of a set of autonomous mobile robots. In: Proc. of Fourth European Research Seminar on Advances in Distributed Systems(ESRADS) (2001) 9. Prencipe, G.: Distributed Coordination of a Set of Autonomous Mobile Robots. PhD thesis, University of Pisa (2002) 10. Prencipe, G.: On the feasibility of gathering by autonomous mobile robots. In: Pelc, A., Raynal, M. (eds.) SIROCCO 2005. LNCS, vol. 3499, pp. 246­261. Springer, Heidelberg (2005) 11. Souissi, S., D´ efago, X., Yamashita, M.: Gathering asynchronous mobile robots with inaccurate compasses. In: Shvartsman, A.A. (ed.) OPODIS 2006. LNCS, vol. 4305, pp. 333­349. Springer, Heidelberg (2006)

312

T. Izumi et al.

12. Souissi, S., D´ efago, X., Yamashita, M.: Using eventually consistent compasses to gather oblivious mobile robots with limited visibility. In: Datta, A.K., Gradinariu, M. (eds.) SSS 2006. LNCS, vol. 4280, pp. 471­487. Springer, Heidelberg (2006) 13. Suzuki, I., Yamashita, M.: Distributed anonymous mobile robots: Formation of geometric patterns. SIAM Journal of Computing 28(4), 1347­1363 (1999) 14. Yamashita, M., Souissi, S., D´ efago, X.: Tight bound on the gathering of obliviousmobile robots with inconsistent compasses. Unpublished (2007)

Compact Separator Decompositions in Dynamic Trees and Applications to Labeling Schemes
Amos Korman1 , and David Peleg2 ,
1

Information Systems Group, Faculty of IE&M, The Technion, Haifa 32000, Israel pandit@tx.technion.ac.il 2 Department of Computer Science and Applied Mathematics, The Weizmann Institute of Science, Rehovot 76100, Israel david.peleg@weizmann.ac.il

Abstract. This paper presents an efficient scheme maintaining a separator decomposition representation in dynamic trees using asymptotically optimal labels. In order to maintain the short labels, the scheme uses relatively low message complexity. In particular, if the initial dynamic tree contains just the root, then the scheme incurs an O(log 3 n) amortized message complexity per topology change, where n is the current number of nodes in the tree. As a separator decomposition is a fundamental decomposition of trees used extensively as a component in many static graph algorithms, our dynamic scheme for separator decomposition may be used for constructing dynamic versions to these algorithms. The paper then shows how to use our dynamic separator decomposition to construct rather efficient labeling schemes on dynamic trees, using the same message complexity as our dynamic separator scheme. Specifically, we construct efficient routing schemes on dynamic trees, for both the designer and the adversary port models, which maintain optimal labels, up to a multiplicative factor of O(log log n). In addition, it is shown how to use our dynamic separator decomposition scheme to construct dynamic labeling schemes supporting the ancestry and NCA relations using asymptotically optimal labels, as well as to extend a known result on dynamic distance labeling schemes. Keywords: Distributed algorithms, dynamic networks, routing schemes, graph decompositions, informative labeling schemes.

1

Introduction

Background: A distributed representation scheme is a scheme maintaining global information using local data structures (or labels). Such schemes play an extensive and sometimes crucial role in the fields of distributed computing and communication networks. Their goal is to locally store useful information about
Supported in part at the Technion by an Aly Kaufman fellowship. Supported in part by grants from the Israel Science Foundation and the Israel Ministry of Science and Art.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 313­327, 2007. c Springer-Verlag Berlin Heidelberg 2007

314

A. Korman and D. Peleg

the network and make it readily and conveniently accessible. As a notable example, the basic function of a communication network, namely, message delivery, is performed by its routing scheme, which in turn requires maintaining certain topological knowledge. Often, the performance of the network as a whole may be dominated by the quality of the routing scheme and the accuracy of the topological information. Representation schemes in the static (fixed topology) setting were the subject of extensive research (e.g., [1,5,6,9,13,15]). The common measure for evaluating a static representation scheme is the label size, i.e., the maximum number of bits used in a label. In this paper, a representation scheme with asymptotically optimal label size is termed compact. The more realistic (and more involved) distributed dynamic setting, where processors may join or leave the network or new connections may be established or removed, has received much less attention. Clearly, changes in the network topology may necessitate corresponding changes in the representation. Consequently, in the distributed dynamic setting, an update protocol is activated where the topology change occurs, and its goal is to update the vertices, by transmitting messages over the links of the underlying network. Ideally, the update protocol maintains short labels using only a limited number of messages. In this paper we consider representation schemes in dynamic trees, operating under the leaf-dynamic tree model, in which at each step, a leaf may either join or leave the tree. We consider the controlled dynamic model, which was also considered in [3,16], in which the topological changes do not occur spontaneously. Instead, when an entity wishes to cause a topology change at some node u, it enters a request at u, and performs the change only after the request is granted a permit from the update protocol. The controlled model may be found useful in Peer to Peer applications and in other popular overlay networks. See [16] for more details and motivations regarding the controlled model. In this paper, we present several dynamic representation schemes, which are efficient in both their label size and their communication complexity. Specifically, if the initial tree contains only the root, then all our dynamic schemes incur O(log3 n) amortized message complexity, per topological change. We first present a compact representation scheme of a separator decomposition in dynamic trees, and then use this basic scheme to derive compact labeling schemes supporting the ancestry and NCA relations on dynamic trees. In addition, we present dynamic routing schemes which have optimal label size up to O(log log n) multiplicative factor, for both the adversary and the designer port models. Finally, we show how to use our dynamic separator decomposition to extend a known result on dynamic distance labeling schemes. Related work: An elegant and simple compact labeling scheme was presented in [13], for supporting the ancestry relation on static n-node trees using labels of size 2 log n. Applications to XML search engines motivated various attempts to improve the constant multiplicative factor in the label size, see [1,2]. Static compact labeling schemes were presented for two types of NCA relations on trees. For the id-based NCA relation (which is the type of NCA relation we consider in this paper), a static labeling scheme was developed in [19] using labels

Compact Separator Decompositions in Dynamic Trees

315

of (log2 n) bits on n-node trees. A static labeling scheme supporting the labelbased NCA relation using labels of (log n) bits on n-node trees was presented in [5]. In addition, [5] gave a survey on applications and previous results concerning NCA queries on trees, in both the distributed and centralized settings. Labeling schemes for routing on static trees were investigated in a number of papers until finally optimized in [9,10,24]. For the designer port model, in which the designer of the scheme can freely enumerate the port numbers of the nodes, [9] shows how to construct a routing scheme using labels of size O(log n) on n-node trees. In the adversary port model, where the port numbers are fixed by an adversary, it is shown how to construct a routing scheme using labels of size O(log2 n/ log log n) on n-node trees. In [10] it is shown that both label sizes are asymptotically optimal. Independently, a routing scheme for trees of label size (1 + o(1)) log n was given in [24] for the designer port model. Dynamic data structures for trees have been studied extensively in the centralized model, cf. [7,22,21]. For comprehensive surveys on centralized dynamic graph algorithms see [11,20]. A survey on popular link state dynamic routing protocols (e.g. OSPF) can be found in [23]. Compared to our dynamic routing schemes, these routing schemes are more robust on weaker dynamic models, such as ones which allow spontaneous faults, however, their message complexity is higher. The controlled model is presented in [3], which also establishes an efficient dynamic controller that can operate in the leaf-increasing tree model, where the only topology change allowed is that a leaf joins the tree. This controller can, in particular, be used to maintain a constant approximation of the number of nodes in the (leaf-increasing) tree, using O(n log2 n) messages. In [16] an extended controller was derived for the controlled model, which can operate under both insertions and deletions of both leaves and internal nodes. In particular, that controller can be used to efficiently maintain a constant approximation of the number of nodes in the dynamic tree, undergoing both deletions and additions of nodes, using low message complexity. Specifically, the approximation scheme incurs O(n0 log2 n0 )+ O( j log2 nj ) messages, where n0 is the initial tree size, and nj is the size of the tree immediately after the j 'th topology change. (Note, that if the initial tree contains just the root, then this complexity can be considered as O(log2 n) amortized message complexity, per topology change). A dynamic routing scheme in the leaf-increasing tree model was given in [4] using identities of size O(log2 n), database size O( log3 n) (where  is the maximum degree in the tree) and amortized message complexity O(log n). Dynamic distance labeling schemes on trees were presented in [17,18] for the serialized model, in which it is assumed that the topology changes are spaced enough so that the update protocol can complete its operation before the next topology change occurs. Two dynamic  -approximate distance labeling schemes (in which given two labels, one can infer a  -approximation to the distance between the corresponding nodes) were presented in [17]. The first scheme applies to a model in which the tree topology is fixed but the edge weights may change, and the second applies to a model in which the only topological event that

316

A. Korman and D. Peleg

may occur is that an edge increases its weight by one. The amortized message complexity of the first scheme depends on the local density parameter of the underlying graph and the amortized message complexity of the second scheme is polylogarithmic. Both schemes have label size O(log2 n + log n log W ) where W denotes the largest edge weight in the tree. Two general translation methods for extending static labeling schemes on trees to the dynamic setting were considered in [18] and [14], for the serialized model. Both approaches fit a number of natural functions on trees, such as ancestry relation, routing, NCA relation etc. The translation methods incur overheads (over the static scheme) in both the label size and the message complexity. Specifically, the method of [14] yields dynamic compact labeling schemes, although the amortized message complexity is high, namely, O(n ). On the other hand, the label sizes of the dynamic labeling schemes in [14], which use polylogarithmic amortized message complexity, have a multiplicative overhead factor of O(log n/ log log n) over the optimal size. Our contributions: In this paper we consider a dynamic tree T operating under the leaf-dynamic tree model and the controlled model, and present several efficient dynamic schemes for T . All our schemes incur O(n0 log3 n0 ) + O( j log3 nj ) messages, where n0 is the initial number of nodes and nj is the number of nodes immediately after the j 'th topology change. Note, that if the initial tree contains just the root, then the amortized message complexity is O(log3 n), per topological change, where n is the current number of nodes in T . We first present an efficient protocol maintaining a compact (i.e., with optimal label size) separator decomposition representation in T . Let us note that, the general translation method of [14] may also yield such a dynamic compact scheme, however, their resulted scheme uses high amortized message complexity, namely, O(n ). Our basic dynamic separator scheme is then used in order to construct several other dynamic labeling schemes for the dynamic tree T , which improve known results. Specifically, we present dynamic compact labeling schemes supporting the ancestry and the NCA relations, and we establish routing schemes for both the designer and the adversary port models, which use optimal label size up to a multiplicative O(log log n) factor. For any of the above mentioned functions f , the best known label size for dynamic labeling schemes supporting f , that use polylogarithmic amortized message complexity, has a multiplicative overhead of O(log n/ log log n) over the optimal label size. In addition, the best known amortized message complexity for dynamic compact labeling schemes supporting f is O(n ). Finally, we show that our dynamic separator decomposition can also be used to allow the dynamic distance labeling schemes of [17] to operate under a more general dynamic model. In addition to allowing the edges of the underlying tree to change their weight, the extended dynamic model allows also leaves to be added to or removed from the tree. The extended scheme incurs an extra O(n0 log3 n0 ) + O( j log3 nj ) additive factor to the message complexity.

Compact Separator Decompositions in Dynamic Trees

317

Paper outline: For clarity of presentation, in the extended abstract we consider only the serialized model, and defer the modifications required for operating under the controlled model. We first assume the leaf-increasing tree model. The adaptation to the leaf-dynamic model is done according to the method described in [16] (the idea is to ignore deletions, maintain an estimate to the number of topological changes and initialize the tree every (n) topological changes). Also, due to lack of space, this extended abstract contains mainly intuition regarding the construction of the dynamic separator decomposition and its applications to dynamic ancestry and routing labeling schemes, in the leaf-increasing model. The formal description and analysis of the separator decomposition construction, as well as the description and analysis of the other applications and the adaptation to the leaf-dynamic model, will appear in the full paper. Our separator scheme for the leaf-increasing tree model is based on an adaptation of our static scheme (described in Section 3). The adaptation requires a number of components whose tasks are maintaining estimates on the sizes of the various subtrees managed in the decomposition, manipulating and reorganizing these subtrees, and maintaining the corresponding labels and topological data. These components are Protocol Shuffle, Protocol Maintain W and Protocol Dyn Sep. Generally speaking, Protocol Maintain W is used to allow each separator v to maintain a constant approximation to the number of nodes in the the subtree T  (v ) for which v was chosen as a separator. Whenever the size of T  (v ) grows by some constant factor, the main protocol Dyn Sep invoke Protocol Shuffle on T  (v ) which calculates a new separator decomposition on T  (v ) which is consistent with the global separator decomposition. The correct operation of each of these protocols relies on the assumption that certain properties hold at the beginning of their execution, and in turn, each of these components guarantees that certain properties hold upon their termination. Hence the correctness proof of the entire algorithm depends on establishing an intricate set of invariants and showing that these invariants hold throughout the execution.

2

Preliminaries

Our communication network model is restricted to tree topologies. Let T be a tree rooted at vertex r and let T (v ) denote the subtree of T rooted at v . For every vertex v  T , let D(v ) denote the depth of v , i.e., the unweighted distance between v and the root r. For a non-root vertex v , denote by p(v ) its parent in the tree. The ancestry relation is defined as the transitive closure of the parenthood relation. Define the weight of v , denoted  (v ), as the number of vertices in T (v ), i.e.,  (v ) = |T (v )|. Let n denote the number of vertices in the tree, i.e., n =  (r). The ports at each node (leading to its different neighbors) are assigned unique port-numbers. The enumeration of the ports at a node v is known only to v . For every two numbers a < b, let [a, b) denote the set of integers a  i < b. For every integer q  -1 let Iq = [3 · 2q+2 , 3 · 2q+3 ) and for every m  n and m m m m -1  q  log m, let Jq (m) = [ 2q +1 , 2q ) and let Jq (m) = [ 2q+1 , 2q-1 ). In other words, Jq (m) = Jq (m)  Jq-1 (m).

318

A. Korman and D. Peleg

Separator decomposition: We first define a separator decomposition of a tree T recursively as follows. At the first stage we choose some vertex v in T to be the level-1 separator of T . By removing v , T breaks into disconnected subtrees which are referred to as the subtrees formed by v . Each such subtree is decomposed recursively by choosing some vertex to be a level-2 separator, etc. Let T subtrees be the collection of all subtrees obtained by the resulting partitioning, on all levels of the recursion. Note that the trees on each level are disjoint but the entire collection contains overlapping trees. Moreover, in this partitioning, each vertex v in T belongs to a unique subtree Tl (v )  T subtrees on each level l of the recursion, up to the level l(v ) in which v itself is selected as the separator. The subtrees T = T1 (v ), T2 (v ), · · · , Tl(v) (v ) are referred to as the ancestor subtrees of v . Define the separator tree T sep to be the tree rooted at the level-1 separator of T , with the level-2 separators as its children, and generally, with each level j + 1 separator as the child of the level j separator above it in the decomposition. For a vertex v in T , let sj (v ) denote the level-j separator of v , i.e., the ancestor of v in T sep at depth j . We associate each vertex v with the subtree T  (v ) = Tl(v) (v ) for which v is chosen as its separator. If v is a level j separator, then T  (v ) is referred to as a level j subtree. (See Figure 1.)

T
11 00

sep

T
11 00 00 11

v 11 00 w1 0 0 1
0 1 1 0 0 1 00 11 00 11 x 00 11

u
11 00 11 00

z r0 00 11 00 1 11 0 1
11 00 11 00 1 0 1 0

11 00 00 11

11 00 11 00

T 1 (v)

00 11 00 11 00 11 0000 1111 000 111 1111 0000 000 111 0000 1111 000 111 0000 1111 000 0 111 1 0000 1111 000 111 0 1 0000 1111 000 111 1 0 0 1111 1 0000 1111 000 111 1 0 00 11 0000 00000 11111 00 11 0000 1111 00000 11111 00 1111 11 0000 1111 00000 11111 00 11 0000 00000 11111 00 1111 11 0000 1111 00000 11111 00 11 0000 00000 11111 00 11 00 11 11 00 0 11 1 00 11 0000 1111 00000 11111 00 00 11 11 00 0 1 00000 11111 00 00 11 0 11 1 0 1 0 1 0 1 0 1 00 11 2 0 1 00 11 0 1 0 1 00000 11111 00 11 0 1 0 1 00000 11111 0 1 00000 11111 0 00000 1 11111 0 1 00000 1 11111 0 00000 11111 0 1 00000 11111 0 1 0 1 00000 11111 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

r

u

v x

z

T (v)

w

Fig. 1. In the depicted tree T , rooted at r , the node v is the level-1 separator of T . By deleting v , T breaks into T 1 (v ) and T 2 (v ). Similarly, w is the separator of T 1 (v ) and u is the separator of T 2 (v ), therefore w and u are the children of v in T sep . By deleting u, T 2 (v ) breaks into three subtrees, one of which contains z as its separator. We have T  (v ) = T , T  (w) = T 1 (v ) and T  (u) = T 2 (v ).

For 1/2   < 1, a  -separator of T is a vertex v whose removal breaks T into disconnected subtrees of at most  |T | vertices each. It is a well known fact that every tree has a  -separator (even for  = 1/2), and that one can recursively partition the tree by  -separators. Such a decomposition is termed  -separator decomposition. It is easy to see that the depth of the corresponding separator tree T sep is O(log |T |). In the special case where  = 1/2, the separator node is called a perfect separator, and the decomposition is called a perfect separator decomposition.

Compact Separator Decompositions in Dynamic Trees

319

Representations for separator decompositions: One may define a distributed representation for separator decompositions in trees in various ways. For our purposes, we define a separator decomposition representation as follows. Each vertex v in a tree T is given a label L(v ) so that the following hold. 1. Each vertex has a unique label, i.e., L(u) = L(v ) for every two vertices u, v  T . 2. Given the label L(v ) of some vertex v and an integer 1  i  l(v ), one can extract the label L(u) where u is the level-i separator of v . Note that by the first requirement, the maximum number of bits in a label in any n-node tree is  (log n) for any separator decomposition representation. The functions: We consider the following functions F on pairs of vertices u, v of a rooted tree. (a) Routing: F (u, v ) is the port number at u leading to the next vertex on the (shortest) path from u to v . (b) Ancestry relation: if u is an ancestor of v in the tree then F (u, v ) = 1, otherwise, F (u, v ) = 0. (c) NCA relation: assuming each vertex z has a unique identifier id(z ) (encoded using O(log n) bits), F (u, v ) is the identifier id(z ) of the nearest common ancestor (NCA) z of u and v , i.e., the common ancestor of both u and v of maximum depth. Labeling schemes: An F -labeling scheme  = M , D is composed of the following components: 1. A marker algorithm M that given a tree, assigns a label L(v ) to each vertex v in the tree. 2. A polynomial time decoder algorithm D that given the labels L(u) and L(v ) of two vertices u and v in the tree, outputs F (u, v ). We note that in our schemes, the labels given to the vertices may contain several fields. In order to distinguish between the different fields of some label one can use an additional label L (v ) for v , which has the same number of bits as L(v ) and whose 1's mark the locations where the fields of L(v ) begin. Clearly, adding L (v ) does not increase the asymptotic label size. The dynamic models: The following types of topology changes are considered. Add-leaf: A new vertex u is added as a child of an existing vertex v . Remove-leaf: A leaf u of a tree is deleted. Subsequent to a topology change, both relevant nodes u and v are informed of it. When a new edge is attached to a node v , the corresponding port at v is assigned (either by an adversary or by v ) a unique port-number (i.e., at any time, the port numbers at v are distinct), encoded using O(log n) bits. In this paper, all our results, except for our results on routing, concern the weak adversary model in which an adversary can freely select and change the port numbers at any node (as long as they remain disjoint at that node). Our

320

A. Korman and D. Peleg

dynamic routing schemes consider the following two port models. In the designer port model, each node v is allowed at any time, to freely select and change the port numbers on its incident ports (as long as they remain disjoint) and in the adversary port model, the port numbers at each node are fixed by an adversary (once the adversary assigns a port number, the number remains unchanged). Various dynamic models are considered in the literature. In the leaf-increasing tree model, cf. [4,14,18], the only topology change allowed is that a leaf joins the tree, and in the more general leaf-dynamic tree model, cf. [3,14,18], leaves can either be added to or removed from the tree. All the results in this paper apply for the leaf-dynamic tree model. After every topological change, an update protocol U is activated in order to maintain the labels L(v ) of the vertices v to fit the requirements of the corresponding problem. As mentioned before, in the context of distributed networks, the messages are sent over the edges of the underlying graph. For simplicity of presentation, in this extended abstract, we analyze our protocols assuming the serialized model (e.g., [18,17,14]) in which the topological changes occur in a serialized manner and are sufficiently spaced so that the update protocol has enough time to complete its operation in response to a given topological change, before the occurrence of the next change. This model allows us to concentrate on the combinatorial aspects of the problem without considering asynchrony issues. Let us remark, however, that our schemes can operate under the weaker controlled model (considered also in [16,3], see [16] for more details and motivations). In this model, when a topological change  wishes to occur at vertex v , a request R to perform  arrives at v . Vertex v performs the topological change  only when the request R is granted a permit from the update protocol. It is guaranteed that every request to perform a topological change is eventually granted a permit. Moreover, in the leaf-increasing model, our dynamic schemes can operate under the weak uncontrolled model in which the topological changes may occur in rapid succession or even concurrently. Correctness, however, is required only at quiet times, i.e., times for which all updates concerning the previous topological changes have occurred. (It can easily be shown that no dynamic compact separator decomposition scheme, can be expected to operate correctly also in non-quiet times). Due to lack of space, the analysis of our schemes under the weaker models is deferred to the full paper. For a static scheme  on n-node trees, model, we are interested in the following complexity measures. The label size, LS (, n), is the maximum number of bits in L(v ) taken over any vertex v . The message complexity, MC (, n), is the maximum number of messages (of size O(log n)) sent by a distribute algorithm assigning the labels of  . For a dynamic scheme  , operating in the leaf-increasing model, the above definitions are taken over all scenarios where n is the final (and maximum) number of nodes in the tree. In the leaf-dynamic tree model, instead of measuring the message complexity in terms of the maximal number of nodes in the scenario, we use more explicit time references employing the notation (n1 , n2 , . . . , nt ) where nj is the size of the tree immediately after the j 'th topological event takes place.

Compact Separator Decompositions in Dynamic Trees

321
Sep

3

The Static Separator Representation Scheme Stat

Let us first note that a static compact separator representation scheme is implicitly described in [12]. However, we were not able to extend that scheme to the dynamic scenario. Instead, in this section we present a new static compact separator decomposition representation scheme Stat Sep (which is in some sense a relaxation of the scheme in [12]), which we find easier to extend to the dynamic scenario. Scheme Stat Sep enjoys label size (log n) and message complexity O(n log n). Recall that in a  -separator decomposition of the tree T , each node v is a separator of some level. Given a  -separator decomposition, a simple way of constructing a representation for it is to assign each vertex a disjoint identity and then to label each vertex by the list of identities of v 's ancestors in T sep . However, this simple scheme has label size O(log2 n). In order to reduce the label size to O(log n) we exploit the liberty of choosing the labels of the separators. As in the simple scheme described above, our marker algorithm assigns each vertex v a different label Lsep (v ) containing l(v ) fields. However, in contrast to the simple sep scheme mentioned above, for any 1 < l  l(v ), the l'th field Lsep (v ) l (v ) of L does not contain the identity of the level-l separator of v . Instead, it contains the binary representation of a number proportionate to |Tl-1 (v )|/|Tl (v )|. Moreover, the label of the level-k separator of v is the prefix of Lsep (v ) containing the first k fields in Lsep (v ). Informally, these properties are achieved in the following manner. Define the labels Lsep (v ) of the separators v by induction on their level. The label of the level-1 separator is set to be 0 . Assume that we have defined the labels of all the level-(l - 1) separators. For each level-(l - 1) separator v , we now define the labels of its children v1 , v2 , · · · in T sep as follows. For each k , vk is first assigned a unique number (vk ) (in the sense that if k = g then (vk ) = (vg )) such that (vk )  [3 · 2q+2 , 3 · 2q+3 ) iff |T  (v )|/2q+1 < |T  (vk )|  |T  (v )|/2q , or in other words, (vk )  Iq iff |T  (vk )|  Jq (|T  (v )|). Note that for each q , there could be at most 2q+1 children vk of v in T sep such that |T  (v )|/2q+1 < |T  (vk )|  |T  (v )|/2q . Therefore, the interval Iq = [3 · 2q+2 , 3 · 2q+3 ) contains sufficiently many integers so that every separator vk satisfying |T  (vk )|  Jq (|T  (v )|) can be issued a distinct integer in Iq . For every k , after assigning each vertex vk a number (vk ) as described above, the label of vk is set to be the concatenation Lsep (vk ) = Lsep (v )  (vk ). The fact that for each k , (vk ) is unique is used to show that the labels are disjoint. Note that the label of a level-l separator u can be considered as a sequence of l fields sep Lsep (u) = Lsep 1 (u)  · · ·  Ll (u). Moreover, for each 1  j < l , the label of the sep level-j separator of u is simply Lsep 1 (u)  · · ·  Lj (u). In addition, for 1  j  l , sep the j + 1'st field Lj +1 (u) is proportionate to |Tj (u)|/|Tj +1 (u)|. This property is used to show that the label size is O(log n). In order to implement Stat Sep by a distributed protocol, when the separator v wishes to assign a unique value (vk )  Iq to one of its children (in T sep ), it somehow needs know which values it had already assigned in the range Iq . For this purpose, for every -1  q  log n , v maintains a counter cq (v ) counting the number of values (vk )  Iq that were already assigned by it. Whenever v

322

A. Korman and D. Peleg

wishes to assign a new value (vk )  Iq , it simply selects 3 · 2q+2 + cq (v ) and then raises cq (v ) by 1. The fact that (vk ) indeed belongs to Iq follows from the following invariant, which holds throughout the execution at every vertex v . Counters invariant at v : For every -1  q  log n , the set of currently assigned values in Iq is a prefix of Iq , namely, [3 · 2q+2 , 3 · 2q+2 + cq (v ) - 1]. The formal description and analysis of the distributed Protocol Stat Sep(T ), which is initiated at the root of a given tree T and assigns each vertex v the label Lsep (v ), are deferred to the full paper. Lemma 1. Protocol Stat Sep(T ) constructs a compact separator decomposition on a static n-node tree T using O(n log n) messages.

4

Protocol Shuffle

Protocol Shuffle is invoked in the dynamic scenario on subtrees T  T subtrees that are suspected to violate some balance properties required in order to maintain the compact separator decomposition on the whole tree T . The goal of Protocol Shuffle(T ) is to recompute a separator decomposition representation on T while keeping it consistent with the global separator decomposition  representation on T . Specifically, we assume that each separator v keeps 0 (v ),  the number of vertices in T (v ) after the last application of Protocol Shuffle on a subtree containing T  (v ). Let v be a level-l separator and let T 1 (v ), T 2 (v ), · · · be the subtrees formed by v . Let T be one of those subtrees, w.l.o.g. T = T 1 (v ). The correct operation of Protocol Shuffle relies on the fact that throughout the dynamic scenario, the following invariants are maintained for every level-l separator v . The balance invariants: B1: For every vertex u  T  (v ), if Lsep l+1 (u)  Iq for some q then |Tl+1 (u)|   Jq (0 (v )).   B2: |T  (v )|  [0 (v ), 5 4 · 0 (v )]. The growth property:  (v ) Just before Protocol Shuffle is invoked on T  (v ), we have |T  (v )|   · 0 where  = 5/4. Protocol Shuffle(T ) is conceptually composed of three stages. In the first stage, all the labels in T are initialized to be Lsep (v ) (which contains l fields). At the second stage, the l + 1'st field of the labels in T , (T ), is initialized so  (v )/|T | and disjoint from (T i ) for every i > 1. At that it is proportionate to 0 the third stage, Protocol Stat Sep is invoked on T to initialize the following (i.e., the l + 2'nd, l + 3'rd, etc) fields of the labels in T according to a perfect separator decomposition of T . It is relatively easy to implement the first and third stages of Protocol Shuffle(T ). Let us now describe informally how Protocol Shuffle implements the second stage. In order for the new assigned value new (T ) to be proportionate  to 0 (v )/|T |, it may need to be in some different interval Iq than before. We

Compact Separator Decompositions in Dynamic Trees

323

use the counters cq (v ) (described in the previous section) to count the number of values in Iq that were already assigned. When v wishes to assign T a new value new (T )  Iq , it selects the value 3 · 2q+2 + cq (v ) and then raises cq (v ) by 1. However, in contrast to Protocol Stat Sep, the counters invariant is not necessarily maintained. Instead, the fact that 3 · 2q+2 + cq (v )  Iq results from the following more involved argument. After applying S , the last Protocol Shuffle to be applied on a subtree containing v , cq (v ) was relatively small. Let T be one of the subtrees T i (v ) that received a new value in Iq after S was invoked and let S be the Shuffle protocol applied on T after which T received this value. By combining the balance invariant B2 (for the separator of T ) with the growth property (for T ), we obtain that the number of vertices that have joined T from the time S was invoked until the time S  was invoked is proportionate to 0 (v )/2q . On the other hand, by B2, the total   number of nodes joining T (v ) from the time S was invoked is at most 0 (v )/4. Combining these two observations, we obtain that the number of subtrees that received a new value in Iq after S was invoked is small enough to guarantee that 3 · 2q+2 + cq (v )  Iq . The formal description of Protocol Shuffle as well as its analysis are deferred to the full paper, where we show the following Lemma. Lemma 2. MC (Shuffle(T )) = O(|T | log |T |).

5

Protocol Maintain W

The goal of Protocol Maintain W is to allow each separator v in the dynamically growing tree to maintain a constant approximation to the number of nodes in T  (v ). In [3] and [16], they show how to allow the root to maintain a constant approximation to the number of nodes in a growing tree, using O(n log2 n) messages, where n is the final (and maximum) number of nodes in the growing tree. Let us denote such a protocol by Protocol Weight Watch. In this section, for each separator v , we consider T  (v ) as rooted at v . Protocol Maintain W simply invokes Protocol Weight Watch on T  (v ), for each separator v . Therefore, each vertex u participates in l(u) applications of Protocol Weight Watch, one for each subtree Ti (u). This can be implemented easily assuming each node u knows, for each 1  i < l(u), the port number leading to its parent in Ti (u) and the port numbers leading to its children in Ti (u). This assumption can be removed by slightly modifying protocol Shuffle. Note that if a  -separator decomposition is maintained then every vertex u participates in l(u) = O(log n) concurrent executions of Protocol Weight Watch. We therefore obtain the following lemma. Lemma 3. Assuming the leaf-increasing tree model, Protocol Maintain W allows each vertex v in the dynamic tree to maintain a constant approximation to the number of nodes in T  (v ). Moreover, if a  -separator decomposition is maintained at all times then MC (Maintain W, n) = O(n log3 n).

324

A. Korman and D. Peleg

6

Dynamic Separator Decomposition

We now briefly sketch Protocol Dyn Sep, whose goal is to maintain a compact separator decomposition representation in the leaf-increasing model. Protocol Dyn Sep uses Protocol Maintain W as a subroutine and from time to time invokes Protocol Shuffle on different subtrees. Therefore, the correctness of Protocol Dyn Sep depends on the correctness of Protocol Maintain W and on the fact that the Shuffle properties are maintained whenever a Shuffle protocol is invoked. However, these are only guaranteed assuming that the balance invariants and the growth property are maintained when the Shuffle protocols take place and assuming that a  -separator decomposition is maintained at all times. Protocol Dyn Sep guarantees these assumptions by invoking Protocol Shuffle(T  (v )) whenever the number of vertices in some T  (v ) grows by some constant factor. This is implemented as follows. Every vertex v keeps the value  (v ) which is the number of vertices in T  (v ) after the last Shuffle proto0 col on a subtree containing v . Whenever the counter  ~  (v ) (which is used by    ~ (v )  5/4 · 0 (v ), vertex v invokes v in order to estimate |T (v )|) satisfies   Protocol Shuffle(T (v )). The formal description of Protocol Dyn Sep and the proof of the following theorem are deferred to the full paper. Theorem 1. Assuming the leaf-increasing tree model, Protocol Dyn Sep maintains a compact separator decomposition using O(n log3 n) messages.

7

Applications: Dynamic Labeling Schemes for Trees

In this section we describe the ideas behind our dynamic labeling schemes, all of which use O(log3 n) amortized message complexity. In this extended abstract we sketch the improved dynamic ancestry and routing schemes. The formal description and analysis of these applications as well as the formal description and analysis of the NCA labeling schemes and the extended distance labeling are deferred to the full paper. We begin with sketching the ideas behind our dynamic compact ancestry labeling schemes. Improved ancestry labeling schemes on dynamic trees: We first introduce a new static compact labeling scheme, Stat Anc = MSA , DSA , supporting the ancestry relation, and then show how to extend it to the dynamic setting. Scheme Stat Anc uses the separator decomposition representation obtained by Scheme Stat Sep . For every two vertices v and u, let s(v, u) denote the NCA of v and u in T sep . Scheme Stat Anc is based on the fact that a vertex v is an ancestor of a vertex u iff v is an ancestor of s(v, u) and u is a descendant of s(v, u). The label L(v ) given by the marker algorithm MSA to a vertex v is composed of two sublabels, namely, the separation sublabel, Lsep (v ), and the relative sublabel, Lrel (v ). The separation sublabel Lsep (v ) is the label given to v by the scheme Stat Sep . The relative sublabel Lrel (v ) is composed of l(v ) fields. The j 'th field of Lrel (v ) contains two bits indicating whether sj (v ), the level-j separator of v , is an ancestor of v in T , descendant of v in T or neither.

Compact Separator Decompositions in Dynamic Trees

325

Given two labels L(v ) and L(u), of two vertices v and u, one can extract the level i of s(v, u) using the corresponding separation sublabels and then find whether in T , v is an ancestor of s(v, u) and u is a descendant of s(v, u), using the i'th field of the corresponding relative sublabels. In the dynamic scenario, the separation sublabels are maintained using the dynamic scheme Dyn Sep . Throughout the dynamic scenario, whenever a vertex v is assigned a new level j separator, the j 'th field in its relative sublabel is updated appropriately, according to whether v is an ancestor (or a descendant) of this separator. By Theorem 1 we therefore obtain the following theorem. Theorem 2. Assuming the leaf-increasing model, Scheme Dyn a compact ancestry labeling scheme using O(n log3 n) messages.
Anc

maintains

Dynamic routing labeling schemes: We now sketch our dynamic routing schemes rout which have optimal label size up to a multiplicative factor of O(log log n). I.e., the label size of rout is O(log n · log log n) for the designer port model, and O(log2 n) for the adversary port model. Let v be some vertex, and let l(v ) be level for which v was chosen as a separator. For each 1  i  l(v ), let si (v ) be the i'th separator of v . The label of v given by rout is composed of three sublabels. The first is the separator sublabel Lsep (v ) which is the label given to v by Dyn Sep (recall that Lsep (v ) contains l(v ) fields). The second and third sublabels are the port-to-separator sublabel Lto-sep (v ) and the port-from-separator sublabel Lf rom-sep (v ). Each of these sublabels also con-sep (v ), is the port tains l(v ) fields. The i'th field in Lto-sep (v ), namely Lto i number leading from v to the next vertex on the shortest path connecting v and rom-sep si (v ). The i'th field in Lf rom-sep (v ), namely Lf (v ), is the port number i leading from si (v ) to the next vertex on the shortest path connecting si (v ) and v . By slightly modifying Protocol Shuffle, we can ensure that whenever Protocol Dyn Sep updates the i'th field in Lsep (v ), the i'th fields in the sublabels Lto-sep (v ) and Lf rom-sep (v ) are also updated appropriately. Given the labels L(u) and L(v ) of two vertices u and v , the port number leading from u to the next vertex on the shortest path connecting u and v is determined as follows. If Lsep (u) is a prefix of Lsep (v ) and Lsep (u) contains i rom-sep fields, then u = si (v ) and therefore the desired port number is Lf (v ). If, i sep sep on the other hand, L (u) is not a prefix of L (v ) then let i be the last index sep such that Lsep i (u) = Li (v ). In this case, the i'th separator of u, si (u), must be on the path connecting u and v and must be different than u. Therefore, the -sep (u). desired port number is Lto i Scheme rout is clearly a correct dynamic routing schemes. Let us now analyze its label size. First, for each vertex v , the separator sublabel Lsep (v ) contains O(log n) bits. Both the port-to-separator sublabel Lto-sep (v ) and the port-fromseparator sublabel Lf rom-sep (v ) contain O(log n) fields, where each such field contains a port number. Recall that it is assumed that each port number is encoded using O(log n) bits. It follows that in the adversary port model, the label size of Scheme rout is O(log2 n). Let us now consider the designer port model and describe the method by which each vertex u chooses its port numbers, so that the label size of Scheme rout is O(log n · log log n). Let E sep (u) be the

326

A. Korman and D. Peleg

set of edges leading from u to the next vertex on the shortest path connecting u and one of its ancestors in T sep . Since u has l(u) = O(log n) such ancestors, E sep (u) contains O(log n) edges. For each edge e  E sep (u), vertex u chooses a unique port number in the range {1, 2, · · · , l(u)}. Therefore, each such port number can be encoded using O(log log n) bits. We therefore immediately get that for every vertex v , the port-to-separator sublabel Lto-sep (v ) can be encoded using O(log n · log log n) bits. We now describe the method by which each vertex u chooses its remaining port numbers, i.e., the port numbers of the edges not in E sep (u). For each such edge e, let T i (u) be the corresponding subtree formed by u. The corresponding port number at u is set to be the number l(u) + (T i (u)), where (T i (u)) is the number given to T i (u) by Protocol Dyn Sep. We therefore obtain that the port numbers incident to u are disjoint. For a fixed vertex v and rom-sep (v ) is leading from si (v ) to x, the next i  l(v ), the port number Lf i vertex on the shortest path connecting si (v ) and v . If the edge (si (v ), x) belongs rom-sep to E sep (si (v )) then Lf (v ) is encoded using O(log log n) bits. Otherwise, i rom-sep the number of bits in Lf (v ) is O(log log n) plus the number of bits used i to encode the i + 1'st subfield in Lsep (v ). Therefore, the number of bits used to encode Lf rom-sep (v ) is at most O(log n · log log n)+ O(log n) = O(log n · log log n). We therefore obtain the following theorem. Theorem 3. Assuming the leaf-increasing model, Scheme rout is a correct dynamic routing scheme that uses O(n log3 n) messages. Moreover, the labels it produces are of optimal length, up to a multiplicative factor of O(log log n). I.e., the label size of rout is O(log2 n) for the adversary port model, and O(log n·log log n) for the designer port model.

References
1. Abiteboul, S., Alstrup, S., Kaplan, H., Milo, T., Rauhe, T.: Compact Labeling Scheme for Ancestor Queries. SIAM J. Computing 35(6), 1295­1309 (2006) 2. Abiteboul, S., Kaplan, H., Milo, T.: Compact Labeling Schemes for Ancestor Queries. In: Proc. 12th ACM-SIAM Symp. on Discrete Algorithms. ACM Press, New York (2001) 3. Afek, Y., Awerbuch, B., Plotkin, S.A., Saks, M.: Local Management of a Global Resource in a Communication. J. ACM 43, 1­19 (1996) 4. Afek, Y., Gafni, E., Ricklin, M.: Upper and Lower Bounds for Routing Schemes in Dynamic Networks. In: Proc. 30th Symp. on Foundations of Computer Science, pp. 370­375 (1989) 5. Alstrup, S., Gavoille, C., Kaplan, H., Rauhe, T.: Nearest Common Ancestors: A Survey and a new Distributed Algorithm. Theory of Computing Systems 37, 441­ 456 (2004) 6. Alstrup, S., Rauhe, T.: Small induced-universal graphs and compact implicit graph representations. In: Proc. 43'rd IEEE Symp. on Foundations of Computer Science. IEEE Computer Society Press, Los Alamitos (2002) 7. Cole, R., Hariharan, R.: Dynamic LCA Queries on Trees. SIAM J. Computing 34(4), 894­923 (2005)

Compact Separator Decompositions in Dynamic Trees

327

8. Eppstein, D., Galil, Z., Italiano, G.F.: Dynamic Graph Algorithms. In: Atallah, M.J. (ed.) Algorithms and Theoretical Computing Handbook, ch. 8. CRC Press, Boca Raton, USA (1999) 9. Fraigniaud, P., Gavoille, C.: Routing in Trees. In: Orejas, F., Spirakis, P.G., van Leeuwen, J. (eds.) ICALP 2001. LNCS, vol. 2076, pp. 757­772. Springer, Heidelberg (2001) 10. Fraigniaud, P., Gavoille, C.: A space lower bound for routing in trees. In: Proc. 19th Int. Symp. on Theoretical Aspects of Computer Science, pp. 65­75 (March 2002) 11. Feigenbaum, J., Kannan, S.: Dynamic Graph Algorithms. In: Handbook of Discrete and Combinatorial Mathematics. CRC Press, Boca Raton, USA (2000) 12. Gavoille, C., Katz, M., Katz, N.A., Paul, C., Peleg, D.: Approximate Distance Labeling Schemes. In: 9th European Symp. on Algorithms, pp. 476­488 (August 2001) 13. Kannan, S., Naor, M., Rudich, S.: Implicit Representation of Graphs. SIAM J. on Descrete Math. 5, 596­603 (1992) 14. Korman, A.: General Compact Labeling Schemes for Dynamic Trees. In: Proc. 19th Symp. on Distributed Computing (September 2005) 15. Korman, A.: Labeling Schemes for Vertex Connectivity. In: Proc. 34th Int. Colloq. on Automata, Languages and Prog (ICALP) (July 2007) 16. Korman, A., Kutten, S.: Controller and Estimator for Dynamic Networks. In: Proc. 26th ACM Symp. on Principles of Distributed Computing. ACM Press, New York (2007) 17. Korman, A., Peleg, D.: Labeling Schemes for Weighted Dynamic Trees. In: Proc. 30th Int. Colloq. on Automata, Languages & Prog. (July 2003) 18. Korman, A., Peleg, D., Rodeh, Y.: Labeling Schemes for Dynamic Tree Networks. STACS 2002 37(1), 49­75 (2004) Special Issue of STACS'02 papers. 19. Peleg, D.: Informative Labeling Schemes for Graphs. Theoretical Computer Science 340, 577­593 (2005) Special Issue of MFCS'00 papers 20. Peterson, L.L., Davie, B.S.: Computer Networks: A Systems Approach. Morgan Kaufmann, San Francisco (2007) 21. Schieber, B., Vishkin, U.: On finding Lowest Common Ancestors: Simplification and Parallelization. SIAM J. Computing 17(6), 1253­1262 (1988) 22. Sleator, D.D., Tarjan, R.E.: A Data Structure for Dynamic Trees. J. Computer & System Sciences 26(1), 362­391 (1983) 23. Tanenbaum, A.S.: Computer Networks. Prentice-Hall, Englewood Cliffs (2003) 24. Thorup, M., Zwick, U.: Compact Routing Schemes. In: Proc. 13th ACM Symp. on Parallel Algorithms and Architecture, pp. 1­10. ACM Press, New York (2001)

On the Communication Surplus Incurred by Faulty Processors
Dariusz R. Kowalski1 and Michal Strojnowski2 ,
1

Department of Computer Science, The University of Liverpool, UK darek@csc.liv.ac.uk 2 Instytut Informatyki, Uniwersytet Warszawski, Poland stromy@mimuw.edu.pl

Abstract. We study the impact of faulty processors on the communication cost of distributed algorithms in a message-passing model. The system is synchronous but prone to various kinds of processor failures: crashes, message omissions, (authenticated) Byzantine faults. One of the basic communication tasks, called fault-tolerant gossip, or gossip for short, is to exchange the initial values among all non-faulty processors. In this paper we address the question if there is a gossip algorithm which is both fault-tolerant, fast and communication-efficient? We answer this question in affirmative in the model allowing only crash failures, and in some sense negatively when the other kinds of failures may occur. More precisely, in an execution by n processors when f of them are faulty, each non-faulty processor contributes a constant to the message complexity, each crashed processor contributes (f  ) ( > 0 could be an arbitrarily small constant independent from n, f but dependent on the algorithm), each omission (or authenticated Byzantine) processor contributes (t), and each--even potential--Byzantine failure results in additional (n) messages sent.

1

Introduction

Communication tasks, like broadcast, multicast or gossip, are among fundamental algorithmic problems in distributed computing. All of them have fast and fault-tolerant solutions in a synchronous message-passing system. However still not much is known about the communication complexity incurred by faulty processors while performing a communication task. In this paper we address this issue for a gossip problem. A fault-tolerant gossip requires that all non-faulty processors learn initial values (called rumors) of all other non-faulty processors. We study deterministic solutions terminating in constant time, which is asymptotically the best time complexity we can achieve for this problem, and tolerating different kinds of failures: crashes, message omissions, authenticated Byzantine and Byzantine faults. Gossip algorithms are important tools for solving decision problems (e.g., consensus [3,4,11]) and cooperation problems (e.g., performing tasks [12]). The additional motivation for studying constant-time message-efficient algorithms comes
Supported in part by KBN Grant N206 001 32/00924 and COST Action 295.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 328­342, 2007. c Springer-Verlag Berlin Heidelberg 2007

On the Communication Surplus Incurred by Faulty Processors

329

from the crucial property that reducing the number of messages increases the stability of many underlying networking systems (see e.g., [5]). More precisely, substantially reducing the number of messages in the system is more important for efficient queuing and processing of messages than decreasing latency by a constant number of rounds. We denote the number of processors in the system by n, the upper bound on the number of failures (in other words, the number of potential failures) by t < n, and the real number of failures that occur during the execution of the algorithm by f  t. Parameters n, t are known to all processors. Algorithm is called t-resilient if it solves the problem correctly unless the number of failures exceeds t. The naive fault-tolerant gossip solution, in which each processor sends its rumor to all other processors in the first round, suffers from the fact that every processor, even non-faulty one, sends n - 1 =  (n) messages. Another popular algorithm, based on the set of leaders (see e.g., [10,13]), requires that firstly each processor sends its rumor to all the leaders, and then every leader sends back the combined message to all other processors. This approach however is inefficient if the number of leaders, and hence the number of tolerated failures, is big. For example, an algorithm tolerating up to t failures should choose at least t + 1 leaders, which yields  (t) messages per each, again even non-faulty, processor. Our goal is to improve the message complexity of deterministic fault-tolerant time-efficient gossip algorithms, wherever it is possible, and show the actual impact of faulty processors to the message complexity of the gossip problem. 1.1 Our Contribution

A straightforward lower bound for the message complexity of a gossip problem is  (n), since each processor must send its value at least once. In the model without failures, this lower bound is matched by the algorithm with one leader, described above. If crashes or even more malicious failures are allowed, this algorithm no longer works since the leader may crash and every other processor receives no message. We show that indeed every crash failure induces additional cost of  (t ) messages for any constant-time t-reliable gossip algorithm, where  > 0 could be an arbitrarily small constant which depends on the algorithm. This distinguishes between the models with crash failures and without failures, for f > n1- . On the other hand, we design a deterministic gossip algorithm where each crash failure contributes O(f  ) to the message complexity, for any constant  > 0. This can not be achieved against omission failures, since each such failure contributes (t) to the message complexity (we show both a lower bound and a deterministic algorithm that matches it). The same holds for authenticated omission failures. Finally, we show that Byzantine failures are the most costly ones, since each even potentially Byzantine fault results in additional (n) message complexity. Similarly as above, we show a lower bound and refer to the algorithm described in [8] that matches this bound. Table 1 summarizes the results.

330

D.R. Kowalski and M. Strojnowski

Table 1. Message complexity incurred by different kinds of failures.  > 0 is a constant that depends on the algorithm. The results without a reference are obtained in this paper. Kind of failures Lower bound Upper bound additional cost per crash  (f  ) O (f  ) additional cost per omission/auth.-Byzantine  (t ) O (t ) additional cost per potential Byzantine failure  (n) O(n) [8]

Another important contribution of this paper is a general framework for faulttolerant gossip algorithms, which is used to obtain efficient gossip algorithms in every considered model of failures. It is based on two classes of generic subroutines: (adaptive) mix and request. Using specific combinations of these subroutines allows to handle efficiently different kinds of failures. 1.2 Previous and Related Work

There is a vast body of literature concerning fault-tolerant gossip in different settings, here we present only the most relevant results. Dolev and Reischuk [8] introduced a simple constant-time t-resilient gossip algorithm with O(n) messages per each potentially Byzantine failure. Chlebus and Kowalski [3] developed a t-resilient gossip algorithm against crash failures, where t < cn for a positive constant c < 1, working in O(log2 n) rounds with message complexity O(polylog n) per each crash during the execution. That result was extended over all values t < n by Georgiou et al. [12] who presented a gossip algorithm working in time O(log2 n) and with O(n ) communication cost per each crash, for any constant 0 <  < 1. Recently, Chlebus and Kowalski [4] developed a solution tolerating up to n - 1 crashes in time O(log3 n) and with O(n log4 n) message complexity. Note that apart from Dolev and Reischuk's [8], those results are not in constant number of rounds, therefore they can not be directly compared with or transformed into constant-time gossip solutions. In the more restricted model of static crashes, Diks and Pelc [7] solved a gossip problem with cn faults (for any constant 0 < c < 1) in time O(log n) and with O(n) messages. Book [13] presents some aspects of fault-tolerant solutions of the gossip problem in general networks. Related problems of gathering and spreading information in shared-memory system were also broadly studied, but from slightly different perspective, see e.g., [6]. For the purpose of our algorithms we define graphs with specific fault-tolerant properties. Different kinds of graphs with expansion properties were studied before in the context of fault-tolerant communication in a message-passing system [3,4,7,12], networks in general (for references see e.g., [2]) and shared memory [6]. Graphs defined in this paper have not been studied before. We compare the results obtained for the gossip problem with another fundamental problem in fault-tolerant distributed computing, which is a consensus problem. Fisher and Lynch [9] showed that a synchronous solution requires f + 1 rounds. This proves that consensus--as a decision problem-- is more complex

On the Communication Surplus Incurred by Faulty Processors

331

than the gossip problem from the perspective of time complexity. For crash failures, Galil, Mayer and Yung [11] designed a consensus algorithm working in time O(f ) and with communication complexity overhead O(t ) per each crash, for any constant  > 0. Their solution was based on a diffusion tree. This result has been recently improved by Chlebus and Kowalski [4] who decreased the communication complexity overhead to O(polylog n) per each crash. That result compared with the result from this paper emphasizes another difference between gossip and consensus problems: for time-optimal algorithms, the communication impact of a crash is much smaller for consensus than for gossip. Regarding the other types of faults, the additional message complexity incurred in a consensus algorithm by an omission or an authenticated Byzantine failure is (t), and (n) by a Byzantine failure, as shown by Dolev and Reischuk [8]. Surprisingly, for these kinds of failures the results for gossip and consensus problems are the same. 1.3 Organization of the Paper

Basic definitions and notations are given in Section 2. Section 3 provides the main framework for efficient gossip algorithms against all considered types of failures. The following three sections present the results for the gossip problem in considered models of failures: Section 4 for crash failures, Section 5 for omission and authenticated Byzantine failures, and Section 6 for Byzantine failures. A discussion in Section 7 concludes the paper. The missing proofs, due to the lack of space, are deferred to the full version of the paper.

2

Technical Preliminaries

In this paper we consider a synchronous message-passing model with failures, as described in [1,15]. Processors and communication. A distributed system considered in this paper is synchronous, with all processors having access to a global clock. There are n processors, each with a unique integer name (identifier) in set P = {1, . . . , n}. Parameter n is known to all processors, in the sense that it is a part of code of an algorithm. Processors communicate by sending messages. Clock cycles are partitioned into rounds during which every processor can receive all the messages delivered at the beginning of this round, perform any finite computation (with exception of breaking cryptographic protocols used in the authenticated Byzantine case), and send point-to-point messages to any subset of processors. A message sent in a round is delivered to its destination at the beginning of the next round, unless the sender or the receiver is faulty; in that case a message could be lost (see the next paragraph for description of the models of failures). Processor failures. We consider the classic model of processor failures (see i.e., [1,15] for detailed description). To avoid much formalism, we use the notion of an adversary who incurs failures into the system. The adversary is adaptive

332

D.R. Kowalski and M. Strojnowski

in the sense that it knows the algorithm and may cause failures during the execution of the algorithm. A processor failure is permanent, that is once a processor becomes faulty it remains faulty till the end of the computation. The total number of processor failures in any execution is bounded by t < n. The real number of failures occurring during an execution is denoted by f  t. We consider the following types of failures. Crash failures: after crashing a processor stops its activity, in particular it stops sending and receiving any messages till the end of the computation. When a processor crashes while sending a message to a subset of processors, the message can be delivered only to some of them, depending on the choice of the adversary. General omission failures: a faulty processor may omit to send or receive any message. More precisely, the adversary has a choice for every message sent by or to a faulty processor if it is received or not. Although the number of faulty processors is bounded by t, there is no limit on the number of omitted messages during the execution. Note that this model is more severe than one with crashes since here no processor can infer a failure of any other by lack of messages from it (since it might be faulty itself). This model also differs from one in which particular faulty links can drop off messages (see [14] for details and further references). Byzantine failures: a faulty processor may behave in any malicious way desired by the adversary. In particular, it may avoid sending messages or send messages with malicious content. In authenticated Byzantine model there is a restriction that each processor may sign any part of a message with its identifier, and such messages may not be forged (only relied unchanged to other processors or be omitted by a faulty processor). From the point of view of performing a communication task, an authenticated Byzantine processor is similar to an omission-prone processor [15]. Fault-tolerant gossip problem. We are given a set of n processors. Initially each of them has a distinct piece of information, called a rumor. A t-resilient gossip solution must satisfy the following requirements in the presence of any pattern of at most t failures, and under the assumption that all processors start in the same round: Correctness: Each non-faulty processor learns the rumors of all non-faulty processors; Termination: Each non-faulty processor terminates its protocol. We assume that an algorithm (code) executed by each processor is the same, and that each processor knows at the beginning values n and t, as well as its own unique identifier i  {1, 2, . . . , n} and its own unique data rumori . Complexity measures. In this paper we consider constant-time algorithms, i.e., terminating in a constant number of rounds. We measure their message complexity, that is, the total number of point-to-point messages sent by processors before termination. We compute this complexity as a function of parameters n, t and f . Note that message complexity is also an upper bound on the number of received messages.

On the Communication Surplus Incurred by Faulty Processors

333

3

Algorithmic Framework

In this section we define a basic framework for our gossip algorithms. This includes a specification of local data structures, content of messages and update procedures based on received messages, communication patterns and subroutines. An adaptation of this framework to particular model of failures may require additional data structures; this will be addressed while describing specific algorithms. 3.1 Local Memory, Messages and Updates

Local memory. A processor p stores the following structures: Rumorsp [1 . . . n] : array of known processor's rumors. A field Rumorsp [q] contains a rumor of processor q , or one of the special values faulty or unknown. Initially Rumorsp [p] = rumorp , and all remaining fields contain value unknown. Activep [1 . . . n] : array of processors' activities. A field Activep [q] contains value unknown (initial) or active (the meaning of this value depends on particular algorithm and will be made clear later). Inf ormedp [1 . . . n] : array of known processors' statuses in the gossip task. A field Inf ormedp [q] contains value unknown (initial) or informed (which means that processor q has collected all required rumors). All the arrays are gradually filled in during the execution. In particular, array Rumorsp is filled in with rumors or values faulty for processors that are known to be faulty. When processor p has no value unknown in the array Rumorsp , is sets Inf ormedp [p] = inf ormed, and is called informed. If all non-faulty processors are informed, the gossip is solved (however some of them may not terminate yet). Messages and memory update. For simplicity, we use only one format of messages. It contains the local data structures of the sender. At the beginning of each round, every processor receives messages and updates its local data structures by overwriting values unknown with newly received values. 3.2 Communication Graphs

Before describing subroutines used in our algorithms, we need to define three classes of graphs which will be used in subroutines to determine the communication pattern. In each subroutine we identify the processors with nodes of the used graph, and we assume that processors send messages only to their neighbors in this graph. All graphs presented below are undirected, and nodes/processors usually communicate both ways along the adjacent edges (unless stated otherwise). For a graph G = (V, E ) and a subset of nodes B  V , we denote the set of all neighbors of B in graph G by NG (B ). Constructions and probabilistic proofs of existence of the following graphs are deferred to the full version of the paper.

334

D.R. Kowalski and M. Strojnowski

Distributor Communicator Each sufficiently big set of workers (grey) After removing any set of nodes (black) has a big set of neighboring leaders (black) there remains a big set with small (only some edges displayed) diameter (grey) (edges not displayed)

Distributors. A distributor is a bipartite unbalanced expander graph with degree  and expansion rate  4 (for small subsets) on the bigger side. Formally, we say that a bipartite graph G = (W, L, E ), where W, L are disjoint sets of nodes, is a (n, x, )-distributor iff it satisfies the following properties: n ; (a) |W | = n, |L| = 2x (b) maximum degree of a node in set W is at most ; 4f n (c) for every f < 2 x2 , every set Y  W of size  has more than f neighbors. Set W is called a set of workers and set L is called a set of leaders. Theorem 1. There exists a (n, x, )-distributor, for any n,   4 and x  8. Communicators. A communicator is a graph in which every large subset of nodes contains a large subgraph with small diameter. We say that graph G = (L, E ) with n nodes and of degree  is a (n, x, )-communicator, iff
nx there exists set C  B of size bigger For each set B  L of size m  6 m than 2 , such that the subgraph of G induced by set C has diameter at most 2 logx n.

Theorem 2. There exists a (n, x, )-communicator, for any n,  and x  2 log n. Adaptive communicators. An adaptive communicator is a graph in which removal of a small set of nodes never detaches a bigger set from the main connected part. Formally, graph G = (L, E ) of n nodes and degree  is called a (n, x, )-adaptive-communicator iff

On the Communication Surplus Incurred by Faulty Processors

335

For each f  n x and set B  L of size n - f there exists set C  B of size at least n - 2f , such that the subgraph of G induced by C has diameter at most 2 log n. Theorem 3. There exists a (n, x, )-adaptive-communicator, for any n, x  6 and   36. Note that a complete graph with n nodes is a (n, x, n - 1)-(adaptive)-communicator, for any 1  x  n. 3.3 Subroutines

Using the classes of graphs defined above, we design three simple subroutines that are used in our algorithms. Each subroutine gets the following input parameters: a set of leaders L, parameters x and . For a given subroutine and its input parameters L, x, , there is a fixed corresponding graph, which is, a (|L|, x, )-distributor for subroutine DistributedRequest(L, x, ), a (|L|, x, )communicator for subroutine M ixing (L, x, ), and a (|L|, x, )-adaptive communicator for subroutine AdaptiveM ixing (L, x, ). In particular, it means that for a given subroutine and parameters L, x, , each processor knows its neighboring processors in the corresponding graph. Based on the input and the properties of the corresponding graph, the exact running time of a subroutine can be computed, as it will be described later for each kind of subroutine. Therefore if processors start the same subroutine all in the same round, they will also finish it simultaneously, which is important for the coordination of processors' actions in the course of the algorithm. DistributedRequest(L, x, ), where   4. The aim of this subroutine is to gather a large number of processors' rumors in the leaders. Recall that all processors running this subroutine know the same fixed (n, x, )-distributor graph G = (L, W, E ), where L is a pre-defined set of leaders and W is the set of all processors. In the first round of the subroutine every processor from set L requests its neighbors in G. During the second round every requested processor replies for all the requests (by sending a message to each processor from which it has received a request), and in the third round the answers are received and processed by the leaders. We will use three variants of this subroutine, depending on if a leader p  L wants to request all its neighbors in graph G, or only those neighbors whose rumors are unknown (those q for which Rumorsp [q] = unknown), or only uninformed neighbors (those q for which Inf ormedp [q] = unknown). Summarizing the complexity, the subroutine works in 3 rounds, with O(n) message complexity. Mixing(L, x, ). The purpose of this subroutine is to exchange knowledge among the leaders. Recall again that all processors running this subroutine know the same (|L|, x, )-communicator graph G = (L, E ), where L is a pre-defined set of leaders given as a part of the input. Every processor in L keeps sending a message to all its neighbors in G during 2 logx n subsequent rounds.

336

D.R. Kowalski and M. Strojnowski

This subroutine works in 2 logx n rounds, and processors send O(|L| logx n) point-to-point messages in total. We will use this subroutine only for x being a polynomial in n, and in that case the subroutine takes a (fixed) constant number of rounds and has O(|L|) message complexity. AdaptiveMixing(L, x, ), where   36. This procedure is identical to subroutine M ixing (L, x, ), with the only difference that instead of a communicator graph, a fixed (|L|, x, )-adaptive-communicator is used. This subroutine works in 2 log n rounds and generates O(|L| log n) pointto-point messages. It will be applied only for  being a polynomial in n, and in that case the subroutine lasts a (fixed) constant number of rounds and has O(|L|) message complexity.

4

Crash Failures

Theorem 4. Every t-crash-resilient gossip algorithm terminating in constant time sends  (n + f 1+ ) messages when f  t crashes occur during the execution, for some constant  > 0. I.e., each faulty processor contributes  (f  ). Proof. Consider a t-resilient gossip algorithm A. The lower bound  (n) is obvious from the fact that in the execution without failures each processor must send at least one message. We show the lower bound  (f 1+ ). Let c be a constant upper bound on the time complexity of algorithm A. We prove the existence of an admissible execution with f crashes, for any 1  f  t, in which algorithm A 1+ 1 sends more than f 2 messages, where  = c+1 . We consider only a nontrivial  case when f > 2. Assume the contrary, that the message complexity M of algorithm A is smaller 1+ than f 2 . We show a strategy of the adversary in which it crashes f processors in a way that algorithm A turns out to be incorrect. The strategy is as follows: in each round the adversary crashes each processor that receives at least f  messages in this round. By the assumption, the number of such processors in the f < f whole execution is at most M f  < 2 , thus only 2 processors are crashed in that way. In order to be consistent with the assumption that the adversary fails f processors during the execution, at the end of the last round c the adversary additionally crashes some of the remaining processors in order to have the total number of crashes equal to f ; we call them last-minute crashes and define later. We denote the whole execution by E and the part of the execution before lastminute crashes by E  . Note that E is an admissible execution and, by the fact that time complexity is at most c, all messages received in E are also received during E  . It remains to prove that execution E violates correctness of gossip algorithm A, which means that there is a non-crashed processor that has not got all the rumors of non-crashed processors by the end of round c. Consider the partial execution E  . A straightforward inductive argument shows that since each noncrashed processor receives at most f  - 1 messages in any round i, it knows at

On the Communication Surplus Incurred by Faulty Processors

337

f  rumors by the end of round j  c. Consequently, it knows at most f = f c = f c/(c+1) = f 1- < f 2 < f - rumors at the end of the partial execution E  . Still there are f - last-minute crashes to be done by the adversary at the very end of the execution E . Therefore, if the adversary chooses a processor p that is non-faulty in E  and in the last-minute it crashes all processors which rumors have been collected by p (there are less than f - of them), p knows no rumor of non-faulty processor at the end of the whole execution. Additionally the adversary does some other last-minute crashes (arbitrarily selected), still keeping processor p alive, to assure that the total number of crashes during execution E is exactly f . Thus E violates correctness of the gossip algorithm. most
c i=1

j i=1 

Observe that the lower bound does not depend on parameter t. Moreover, a large number of crashes, e.g., n/3, increases the message complexity by polynomial factor, comparing to the executions without failures. We now show how to solve the gossip task in the presence of crash failures, in constant number of rounds and with O(n + f 1+ ) message complexity, for any given  > 0. Our algorithm GosCrash() is composed of two parts. Part I (see Figure 1) attempts to solve the gossip using a small number 2n1-/6 of leaders. It uses O(n) messages, independently of the number of failures. If most of the leaders are non-crashed, precisely f < n1-/3 , after this part the gossip is completed and all processors are informed. Otherwise, all processors that are uninformed after Part I perform Part II (see Figure 2), which costs additional O(1) rounds and O(f 1+ ) messages in total. Algorithm initialization and control. For a given constant parameter 0 < /6   1, let k = 6 }. We define L (the set of leaders) to  and s = max{2, n be the set of the first 2n/s processors. Recall that P stands for the set of all processors. Each processor first runs Part I of the algorithm. If, after Part I, it is still not informed then it executes Part II. Otherwise, it waits during the period corresponding to the execution of Part II, and it only replies to all messages received during this period (by sending back all its local data as described in Section 3). Part I. In this part we first run k + 1 gathering P hases, parameterized by i = 0, . . . , k , in order to gather the rumors by the leaders, and then we perform k + 1 informing P hases, parameterized by i = 0, . . . , k , in order to spread gathered information from the leaders to all non-faulty processors. The processes of gathering and informing are symmetric to each other, in the sense that they both use the same communication pattern and the only difference is that in the gathering process the goal is to request and get the answer, while in the informing part the aim is to send the data and then get the confirmation. Therefore we give a pseudo-code for a generic phase, see Figure 1, pointing our clearly all the places where both processes differ, that is, where either unknown processors (in gathering phases) or uninformed processors (in informing phases) are considered. Parameter i is responsible for limiting the number of requests sent during P hase(i); more precisely, si is the upper bound on the number of requests arriving at a single processor. Intuitively, we start with a

338

D.R. Kowalski and M. Strojnowski % a generic phase can be either gathering or informing

Generic P hase(i) (code for processor p):

1. Run AdaptiveMixing(L, 6, s) 2. If there are at most 8n/si values unknown in Rumorsp (in gathering phase) or Inf ormedp (in informing phase), set Activep [p] = active and reset the rest of the array Activep . If not, reset the whole array Activep 3. Run AdaptiveMixing(L, 6, s) 4. If Activep [p] = active and there are more than |L|/2 values active in Activep , set Activep [p] = active and reset the rest of the array Activep . If not, reset the whole array Activep 5. Run AdaptiveMixing(L, 6, s) 6. If Activep [p] = active and there are more than |L|/2 values active in Activep , run DistributedRequest(L, s, si ), requesting unknown neighbors (in gathering phase) or uninf ormed neighbors (in informing phase) 7. After receiving any replies, set Rumorp [q] = f aulty for every unknown processor q requested in the previous line Algorithm GosCrash, Part I: ­ For i := 0 to k run gathering P hase(i) ­ For i := 0 to k run informing P hase(i)

Fig. 1. Algorithm GosCrash, Part I

small number of requests, and increase this number gradually with the consecutive phases in order to assure that each processor will be asked eventually by some other processor. We briefly describe a gathering P hase(i); an informing phase--as we argued-- is analogous. In line (1) the leaders exchange their knowledge running AdaptiveMixing(L, 6, s). Any leader that knows enough rumors, that is all but at most 8n/si , considers itself active in line (2). Only an active processor will have a chance to send requests later in line (6). In lines (3) to (5), every active leader perform two additional AdaptiveMixing(L, 6, s), in order to check if it has sufficiently many leaders (at least 3n/si-1 ) in distance 2k , and to exchange its knowledge with other leaders with this property. The reason why it performs adaptive mixing twice is that the information exchange between two such leaders is in practice made via some intermediate leader, which is in distance 2k from both of them. In line (6) only the leaders that in both predeceasing runs of adaptive mixing (lines (3) and (5)) had at least 3n/si-1 leaders in distance at most 2k perform DistributedRequest(L, s, si ). This protocol guarantees that any two requesting leaders exchanged their knowledge while executing lines (3) to (5). As we will show in the analysis, this also gives a desired upper bound on the number of requested processors, and therefore on the total number of requests (contributing substantially to the message complexity). On the other hand, the property of subroutine DistributedRequest(L, s, si ) assures that at least 1/s fraction of rumors that are unknown to active leaders does not receive any request from them, providing there are sufficiently many active leaders. Finally, every

On the Communication Surplus Incurred by Faulty Processors Generic P hase(i, j ) (code for processor p):

339

1. Run Mixing(P , s, si ) 2. If (i = j ) or there are at most 3n/sj -2 values unknown in Rumorsp (in gathering phase) or Inf ormedp (in informing phase), set Activep [p] = active and reset the rest of the array Activep . If not, reset the whole array Activep 3. Run Mixing(P , s, si ) 4. If Activep [p] = active and there are more than 3n/si-1 values active in Activep , set Activep [p] = active and reset the rest of the array Activep . If not, reset the whole array Activep 5. Run Mixing(P , s, si ) 6. If Activep [p] = active and there are more than 3n/si-1 values active in Activep , send requests to unknown neighbors (in gathering phase) or uninf ormed neighbors (in informing phase) in the (n, s, sj )-communicator 7. After receiving any replies, process them and make updates; set Rumorp [q] = f aulty for every unknown processor q requested in the previous line Epoch(i): ­ For j := i to k run gathering P hase(i, j ) ­ For j := i to k run informing P hase(i, j ) Algorithm GosCrash, Part II: ­ For i := 1 to k run Epoch(i)

% a generic phase can be either gathering or informing

Fig. 2. Algorithm GosCrash, Part II

processor that was requested in line (6) answers in line (7), and therefore the number of unknown rumors to the active leaders decreases by fraction 1/s (again, providing there are sufficiently many active leaders). Part II. Part II contains k Epochs, each being a slightly modified version of Part I, handling a different range of the number of failures. The main difference from Part I is that different graphs for mixing and requests are used, due to the fact that subsequent epochs handle larger and larger number of crashes, trading them for the increasing communication cost. More precisely, we will show that for each i, Epoch(i) costs O(n1+/3 ) messages, and successfully solves gossip whenever the number of correct processors is at least 6n/si-1 . In each Epoch, consecutive P hases work analogously to P hases in Part I, with the similar role of corresponding lines in the pseudo-code; therefore we skip an informal description and refer directly to Figure 2 for details. 4.1 Analysis of Algorithm GosCrash

Correctness of the algorithm follows from the fact that in the first line of Epoch(k ) of Part II, uninformed processors send messages to all their neighbors in (n, s, n - 1)-communicator (which is a complete graph), and receive the answers. After this line all processors are informed.

340

D.R. Kowalski and M. Strojnowski

Time complexity is a constant, which can be seen directly from the construction: k is a constant, each used subroutine lasts O(1) rounds, and each line of the code of Part I and Part II lasts a constant number of rounds. We now analyze the message complexity. The following lemmas describe the progress and the message complexity of the algorithm after Part I and Part II. Lemma 1. If f  n1-/3 then after Part I all non-faulty processors are informed. Lemma 2. The number of messages sent during Part I is O(n). Lemma 3. If there are at least 6n/si-1 non-faulty processors at the end of Epoch(i) of Part II, then all of them are informed by that time. Lemma 4. Each epoch of Part II contributes O(n1+/3 ) to the message complexity. Lemma 5. The number of messages sent during Part II is O(f 1+ ). Combining Lemmas 2 and 5 we obtain the final result. Theorem 5. Each crash failure contributes O(f  ) to the message complexity of algorithm GosCrash.

5

Omission and Authenticated Byzantine Failures

Since an omission-faulty processor may, for example, omit all messages starting from some given round, omission failures are at least as hard to handle as crashes. In this section we prove that from the point of view of the gossip they are substantially more severe. In particular, each omission-faulty processor contributes (t) to the message complexity of the gossip problem. The following lower bound holds for all t-omission-reliable gossip algorithms. Theorem 6. Every t-omission-resilient gossip algorithm has message complexity  (n + f t), that is, each faulty processor contributes  (t). We present an algorithm, called GosOmission, based on the generic framework described in Section 3. It uses subroutines DistributedRequest and AdaptiveMixing. The main difference between this algorithm and algorithm GosCrash is that in case of omission failures it might be not enough to send a request once to every processor. The lack of answer might as well indicate that the sender is faulty, not necessarily the receiver as it was for crashes. Thus in order to confirm that some processor is faulty, at least t + 1 messages must be sent to it, each 5 by different processor. Let L be the set of the first 2n 6 processors, T be the set of the first min{3t + 1, n} processors, and P be the set of all processors. Figure 3 presents the pseudo-code of the algorithm. A detailed description and the analysis are deferred to the full version of the paper. Theorem 7. GosOmission is a constant-time omission-resilient gossip algorithm with message complexity O(n + f t), that is, O(t) per each faulty processor. Using the simulation from [15], our algorithm can be adapted to tolerate authenticated Byzantine faults with the same asymptotic time and message complexity.

On the Communication Surplus Incurred by Faulty Processors

341

1. 2. 3. 4. 5.

6. 7. 8. 9. 10.

Run DistributedRequest(L, n 6 , 8) 1 1 Run AdaptiveMixing(L, n 6 , n 6 ) 1 Run DistributedRequest(L, n 6 , 8) 1 2 Run AdaptiveMixing(P , 6, n 3 ); a processor that knows more than n - 2n 3 rumors only answers the requests Each processor in T that knows at least n - 4t rumors sends requests to all unknown processors; requested processors reply in the next round, and the answers are delivered in the second next round 1 Run DistributedRequest(L, n 6 , 8) 1 1 Run AdaptiveMixing(L, n 6 , n 6 ) 1 Run DistributedRequest(L, n 6 , 8) 1 Run AdaptiveMixing(P , 6, n 3 ); an informed processor only answers the requests Finalizing phase: every uninformed processor sends requests to all processors in T ; requested processors reply in the next round, and the answers are delivered in the second next round

1

Fig. 3. Algorithm GosOmission

6

Byzantine Failures

From the message complexity point of view, the Byzantine failures are indeed the most severe failures among the considered ones. Theorem 8. Every potentially-Byzantine processor contributes  (n) to the message complexity of a t-Byzantine-resilient gossip algorithm. The proof of Theorem 8 is deferred to the full version of the paper. On the other hand, the matching upper bound was established in [8]. Theorem 9. [8] There is a constant-time t-Byzantine-resilient gossip algorithm for which each potential Byzantine failure results in additional O(n) messages, for every 0 < t < n.

7

Conclusions and Applications

In this paper we analyzed the impact of different kinds of failures to the message complexity of a constant-time gossip problem. In particular we showed that crashes cost more messages than non-faulty processors, however still polynomially less than more severe types of failures like omissions and (authenticated) Byzantine. There are several additional properties of the gossip algorithms designed in this paper, apart that they are fast, fault-tolerant and generate minimum required number of messages. The algorithms designed to handle crashes and omissions also provide to each non-faulty processor some extra information about each faulty processor: its rumor or a special value faulty. The extra information provided by

342

D.R. Kowalski and M. Strojnowski

the algorithm for authenticated Byzantine failures is slightly weaker: a message sent by the faulty processor at some point of the execution or a special value faulty. There are several open questions related to the efficient fault-tolerant gossip. A tradeoff between time and message complexity in the model with crash failures is one of the intriguing problems (compare this work with [3,4]). Another open direction is to analyze fast fault-tolerant gossip in general network topologies.

References
1. Attiya, H., Welch, J.: Distributed Computing. John Willey & Sons, West Sussex, England (2004) 2. Capalbo, M.R., Reingold, O., Vadhan, S.P., Wigderson, A.: Randomness conductors and constant-degree lossless expanders. In: Proc. of 34th ACM Symposium on Theory of Computing (STOC), pp. 659­668 (2002) 3. Chlebus, B.S., Kowalski, D.R.: Robust gossiping with an application to consensus. Journal of Computer and System Sciences 72, 1262­1281 (2006) 4. Chlebus, B.S., Kowalski, D.R.: Time and communication efficient consensus for crash failures. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 314­328. Springer, Heidelberg (2006) 5. Chlebus, B.S., Kowalski, D.R., Rokicki, M.A.: Adversarial queuing on the multipleaccess channel. In: Proc. of 25th ACM Symposium on Principles of Distributed Computing (PODC), pp. 92­101 (2006) 6. Chlebus, B.S., Kowalski, D.R., Shvartsman, A.A.: Collective asynchronous reading with polylogarithmic worst-case overhead. In: Proc. of 36th ACM Symposium on Theory of Computing (STOC), pp. 321­330 (2004) 7. Diks, K., Pelc, A.: Optimal adaptive broadcasting with a bounded fraction of faulty nodes. Algorithmica 28(1), 37­50 (2000) 8. Dolev, D., Reischuk, R.: Bounds on information exchange for Byzantine Agreement. Journal of ACM 32(1), 191­204 (1985) 9. Fischer, M., Lynch, N.: A lower bound for the time to assure interactive consistency. Information Processing Letters 14(4), 183­186 (1982) 10. Fujita, S., Yamashita, M.: Optimal group gossiping in hypercubes under circuit switching model. SIAM J. on Computing 25(5), 1045­1060 (1996) 11. Galil, Z., Mayer, A., Yung, M.: Resolving message complexity of Byzantine agreement and beyond. In: Proc. of 36th IEEE Symposium on Foundations of Computer Science (FOCS), pp. 724­733 (1995) 12. Georgiou, C., Kowalski, D.R., Shvartsman, A.A.: Efficient gossip and robust distributed computation. In: Fich, F.E. (ed.) DISC 2003. LNCS, vol. 2848, pp. 224­ 238. Springer, Heidelberg (2003) 13. Hromkovic, J., Klasing, R., Pelc, A., Ruzicka, P., Unger, W.: Dissemination of information in communication networks: broadcasting, gossiping, leader election, and fault-tolerance. In: Theoretical Computer Science. EATCS Series. Springer, Heidelberg (2005) 14. Lynch, N.: Distributed Algorithms. Morgan Kaufmann, San Francisco (1996) 15. Neiger, G., Toueg, S.: Automatically increasing the fault-tolerance of distributed systems. Journal of Algorithms 11, 374­419 (1990)

Output Stability Versus Time Till Output
(Extended Abstract)
Shay Kutten1 , and Toshimitsu Masuzawa2
1 2

Dept. of Industrial Engineering & Management, The Technion, Haifa 32000, Israel Graduate School of Information Science and Technology, Osaka University, Japan

Abstract. Consider a network whose inputs change rapidly, or are subject to frequent faults. This is expected often to be the case in the foreseen huge sensor networks. Suppose, that an algorithm is required to output the majority value of the inputs. To address such networks, it is desirable to be able to stabilize the output fast, and to give guarantees on the outputs even before stabilization, even if additional changes occur. We bound the instability of the outputs (the number of times the output changes) of majority consensus algorithms even before the final stabilization. We show that the instability can be traded off with their time adaptvity (how fast they are required to stabilize the output if f faults occurred). First, for the extreme point of the trade-off, we achieve instability that is optimal for the class of algorithms that are optimal in their output time adaptivity. This is done for various known versions of majority consensus problem. The optimal instability for this case is  (log f ) and is shown to be O(log f ) for most versions and O(log n) in some cases. Previous such algorithms did not have such a guarantee on the behaviour of the output before its final stabilization (and their instability was  (n)). We also explain how to adapt the results for other points in the trade off. The output stabilization in previous algorithms was adaptive only if the faults ceased for O(Diam) time. An additional result in this paper uses adaptations of some previous tools, as well as the new tools developed here for bounding the instability, in order to remove this limitation that is undesirable when changes are frequent.

1

Introduction

Consider an action that is to be taken according to some value measured by sensors composing a network. The measurements at some of the sensors may be different (possibly, because of measurements inaccuracies, or because of faults). To overcome that, the network computes a majority consensus. An outside action may be taken according to this consensus. For example, travelers in the woods may consult the sensor near them to decide whether to unfold a tent, since a
This research was supported in part by a grant for research on sensor networks from the Israeli Ministry of Science and Technology.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 343­357, 2007. c Springer-Verlag Berlin Heidelberg 2007

344

S. Kutten and T. Masuzawa

storm is coming, or whether to fold the tent and continue the walk in the case that the rain has gone. A fast answer is sometimes crucial. However, if one insists that a warning always come as fast as possible, then some false warnings are unavoidable. For example, if all the near-by sensors predict a storm, then the initial answer must be that a storm is about to break. If those near by sensors are a small minority, then the answer changes eventually. The false alarm may have consumed some resources (e.g., the effort of unfolding and then re-folding). An external system using the output may even act incorrectly if the output changes too frequently. For example, various machines, if turned on and off too often, would break. Note, that changes in the output may be unavoidable even if the travelers are willing to wait. The inputs of the sensors may change, for example, when a chance of a storm is indeed increasing. Moreover, the inputs change not all at the same time. Hence, at some point, the travelers should decide that they are willing to act upon the best answer the network can give them at present. The stability (or instability) of the output for a distributed consensus was discussed in [1,2]. It is the number of times the output may change when the input changes. The output time complexity, or the output stabilization time, in self stabilizing systems1 , is the time it takes for the system to start outputting the correct and final output, following input changes (e.g. introduced by faults). These two measures of complexity seem, intuitively, related. (For example, the number of changes in the output of a node cannot be larger than the time complexity). Still, they were discussed only separately in the literature. The stability problem was studied also outside the setting of distributed systems, in the context of mechanical engineering, see e.g. [11]. Note, that instability cannot be avoided altogether. In every setting of the Consensus problem, some inputs dictate a certain output value, while some others dictate a different one. For any other set of inputs, the algorithm enjoys the freedom to decide the value to output. In [1], they explore how the stability is increased (the instability is decreased) as a function of this freedom. In this paper, we explore the way the freedom along another dimension influences the stability. In the Majority Consensus problem, the output value must be that of the majority (as opposed to cases studied in [1]). However, (a different kind of) freedom exists in this system too: if there are changes in the inputs of some f nodes (or any other changes by f state faults), then the output is permitted to be incorrect (i.e., different than the majority of the inputs) for some finite time. We note that allowing such a freedom is unavoidable in distributed systems where inputs may change. This is because it takes some time for a changed input in one node to be communicated to the other nodes. Such a freedom is especially assumed in the context of self stabilization [18] (except for the case of a very limited set of tasks [8] that does not include the task of Consensus). In optimal time adaptive systems [16,12], this unavoidable output freedom is restricted to the unavoidable duration, as a function of the number of faults. For
1

Self stabilization and stability are two different notions. The use of both may be confusing, but we chose not to change terms that are common in the literature.

Output Stability Versus Time Till Output

345

Majority Consensus, if f faults occur, the output is required to stabilize to the correct final value in O(f ) time. Definition 1. Assume, that the outputs of all the network nodes is stable (will not change unless the inputs are changed) at some time t0 . Assume further, that a set of faults and changes occur at some time tf > t0 . The instability of an algorithm is the number of times the output will change in the worst case starting from tf and until the output eventually stabilizes (if it ever does) or until further faults or input changes occur. Several time adaptive algorithms exist for the Persistent Value Problem [16,12] and the Majority Consensus with Persistence Problem [21]. For these algorithms, the instability for an optimal time adaptive algorithm was  (n). This means that the output of a node could change every time unit until the final stabilization. Some algorithms give (different kinds of) guarantees for outputs even before the final stabilization, but these guarantees apply only for non- faulty nodes, and the algorithms were not time adaptive. See, e.g. [15]. Self stabilizing algorithms in general have been criticized for not giving much guarantee for the value of the outputs before stabilization. This is especially problematic in networks that rarely stabilize. Hence, reducing the instability (especially of faulty nodes) below O(n) can be viewed as a step in the right direction for such network. Another result the can be obtained using our methods for and with fast changing networks as a motivation, is time adaptivity even when additional faults occur before stabilization. Main results: We address the problems of (One Time) Majority Consensus, Persistent Value (and the related "Majority Consensus with Persistence"), and Repeated Majority Consensus. On the negative side, it is easy to show that no algorithm for these problems that is asymptotically optimal in its time adaptivity can have instability that is better than  (log f ). This is the case even if the algorithm is not required to self stabilize. We then present algorithms that both have optimal time adaptivity and have O(log f ) instability for non-faulty nodes, and O(log f ) (for some cases) or O(log n) (for other cases) for faulty nodes. That is, our algorithms are asymptotically optimal in their output stability for the class of optimal time adaptive algorithms. We then show how to generalize the results, such that if the time complexity is allowed to grow beyond O(f ), the instability shrinks below  (log f ). The instability of our algorithms in most cases is adaptive too, that is, it does not depend on n, but only on f . Additional results: The proofs of the following additional results are deferred because of space considerations, and do not appear in this extended abstract. While previous algorithms for the Persistent Value Problem self stabilized in any case (as do the algorithms in the current paper), they were time adaptive only when the faults occurred in one batch at a time, and no additional faults occurred until the eventual full state stabilization (not just until output stabilization). This happens in  (n) time. Our results can be proved for a more realistic model, where faults can occur at any time, and not just in batches.

346

S. Kutten and T. Masuzawa

As tools for the our algorithms, we had to design a building block broadcast module (Protocol ABC) that is both error confined ([15]) and time adaptive ([12]). In contrast, the error confined tool of [15] was not time adaptive, while the time adaptive tool of [12,19] was not error confined. We also had to add to the tool a snap stabilizing (see [8]) action we term Cancel.

2

Model, Definitions, and Some Very Related Work

The system is modeled as a fixed undirected connected graph G = (V, E ), where |V | = n. Nodes represent processors and edges represent bi-directional communication links. Every node has a unique identity ID that cannot be changed by faults. For the sake of this extended abstract, we assume that the network is synchronous, even though methods to translate protocols such as ours to asynchronous networks are known, such as in [19,21]. The distance between two nodes u, v  V , denoted dist(u, v ), is the minimum number of edges in a path connecting them. Given a node v  V , let Ballv (r) = {u  V | dist(v, u)  r} be the ball of radius r around v . The radius of the network around a node v , denoted Radiusv , is the minimal r such that Ballv (r) = V . RRadiusv is the first power of 2 larger than or equal to Radiusv . Diam (the diameter) is the maximum over all v  V of Radiusv . For the purpose of saving in memory only, we assume that the diamter is bounded by M axDiam. For simplicity of exposition, we assume often that RRadiusv = Radiusv . In the extended abstract, we assume that the topology of the network is known (this assumption is lifted in the full paper, see a short discussion in Section 6 below). Definition 2. A state corrupting fault is an action that alters the state arbitrarily in some subset of nodes. We terms these node faulty. For simplicity, we assume that after a fault, each node is in a legal local state (otherwise, the node can detect the fault). Our approach in modeling faults is similar to the one in [9,22]. The model of state-corrupting faults is implicit in the work of Dijkstra about self stabilization [18]: put in our terminology, a system is called self-stabilizing if after some arbitrary state-corrupting faults occur (possibly, hitting all nodes) the system starts behaving correctly eventually. (Issues arising from different definitions of self stabilization are discussed in [10]). See the full paper for more detailed definitions. We find it convenient to define two sets state , output of correct behaviors. For defining output , we assume that every node has a part of its state called the output. Moreover, only assignment to this variable are external actions of the protocol. Hence, only such actions show in behaviors in this case. The specific legal behavior output is given in the definition of every problem to be solved. We say that output determines output stabilization. A protocol is time adaptive for the output stabilization if the system starts behaving correctly after a time which depends only on f , the number of faults (rather than on n). In other words, the output stabilization time is O(g (f )) for some function g (f ). The problems we solve is defined next.

Output Stability Versus Time Till Output

347

Definition 3. The One Time Majority Consensus problem: Every node has an input that can be changed only once and only by the environment and an output variable read by the environment. (In the extended abstract, we assume that b is binary). For every node, it is required that, Eventual Agreement- the output stabilizes to the majority value of the inputs eventually. To unify the discussion, we consider the majority value as the correct one, and nodes with the minority input as faulty. A second problem- the Persistent Value Problem, is defined below. Definition 4. The Persistent Value problem [16,12] (somewhat rephrased): A value b was given exactly once by the environment to every node (the same b to all the nodes) before the faults started. (In the extended abstract, we assume that b is binary). This value was stored in each node in a storage variable. To be compatible with previous papers dealing with the persistent value problem, we name this storage variable the input variable. This (and every other) variable can be changed by faults, or by the node. Every node also has an output variable read by the environment. It is required that: Persistence: it is required that both the output and the input of every node stabilize to b eventually. We note that persistence implies eventual agreement. The algorithm we present here solves only the requirements for the output, while the requirement for the input variable is solved by the Input Correction Module taken from [12] (the current algorithm and the above module are executed as co-routines). We note that we termed the storage variable the input since that storage variable is the input for the algorithm module designed in the current paper (though it may be changed by the other module, the one taken from [12]). The properties of the Input Correction Module are listed in Subsection 5.2. Note, that designing the Output Stabilization Module for the Persistent Value problem is a harder task than designing the One Time Majority Consensus, since our algorithm must take into account the fact that its input may be changed not only by the environment (at some time tf the faults occur), but also (later) by the Input Correction Module. In Repeated Majority Consensus, changes may continue to occur. The time and the number of changes are counted from the last time the network was stable. Additional very related work: The distinction between output stabilization and state stabilization is used and discussed in a number of papers [4,16,12,28,3,5]. Fast stabilization of output variables has been demonstrated in a number of algorithms [7,6,24,25,16,17,26,19,5] and some general methods to achieve time adaptivity [12,26]. In [20], the importance of output stability for practical Internet protocols was emphasized and obtained using time adaptivity methods.

348

S. Kutten and T. Masuzawa

3

Lower Bound

We establish a lower bound for the instability of a protocol given that the output of the protocol is required to stabilize (to the correct value) as fast as possible (asymptotically). The requirement about the fast output stabilization is used heavily in the proof. Furthermore, we see in this section that relaxing this requirement leads to weakening the lower bounds. This establishes the lower bound side of the trade off between output time and instability. We note that the lower bound holds even for synchronous networks, and even for algorithms that are not required to self stabilize. Theorem 1. The instability of any deterministic asymptotically optimal time adaptive protocol for the Persistent Value Problem, or for the Majority Consensus Problem, is  (log f ). The formal proof are deferred to the full paper. Informally, it considers a line network with v as the first node in the line. In the first, say, X time units, v receives only votes (broadcasts of input values) from the nearest X nodes. If all of them vote some value b1 , then v cannot distinguish between this case and the case that f = 0. To be time adaptive, v must start outputting b1 within some constant time C . (We then choose X = C ). Next, assume that out of the C 2 closest to v , the majority (C 2 - C ) vote b0 . Now, v must change its output to b0 to be time adaptive (since it may be the case that f = C ). Next, we consider the C 3 nodes closest to v . This argument is carried forward to show  (log f ). A problem with initial algorithmic ideas: At first glance, the proof of the lower bound seems to suggest an algorithm: (1) collect votes (broadcasted input values of the other nodes), (2) after changing the output, do not change the output again, before the number of votes received is grows by a factor of some C . This would have implied a logarithmic instability in the case that no vote may change or may be corrupted by a fault. Unfortunately, it is possible that votes arriving at v (by broadcasts) cease to arrive, or change their value. This can be caused by faults at the sources of such votes, or at nodes on the route from them to v , or by the action of the algorithm that corrects the faults.

4

A Building Block: Error-Confined and Adaptive Broadcast

As mentioned at the end of Section 3, changes in a vote received at v increase the instability. Some such changes cannot be avoided, since they represent real changes in the inputs. We use a tool that avoids some changes- informally, it allows a node s to broadcast its input such that if s and a recipient v are not faulty, no node on the way from s to v can change the value received at v (though the protocol may fail to deliver any value sometimes). This is termed an error confined broadcast in [15]. See the exact properties of this protocol,

Output Stability Versus Time Till Output

349

ABC (Adaptive Broadcast with Confinement, in Theorem 2 (the definitions of the broadcast task and of error confinement appear in the appendix). As compared in "Additional Results" above, these properties of ABC are a combination of those of the tools used in previous papers [16,12,21,8]. Still, if the algorithms of those papers are modified to use our tool, instead of their own tools, their instabilities would still be high. However, the ABC tool proved useful for our algorithms and may prove useful by itself in the future. Definition 5. The value of the broadcast of a node s received in a node v is authentic if it was indeed communicated by s (rather than a value resulting from a corruption in some channel or some intermediate node) 2dist(v, s) time earlier. Operation Cancel performed by v on the ABC of s causes the value of s received at v to become undefined (). Moreover, if v starts again receiving s's broadcast, then the value received is authentic, and was broadcast by s after the last Cancel of v (unless v itself suffered another fault meanwhile). (The motivation for this operation is similar to that of snap stabilization. [8]). The detailed description of protocol ABC and the proof of the following theorem are deferred to the full paper. They do not use cryptographic assumptions (but alternative implementations that do use cryptographic assumptions may save in communication complexity). Theorem 2. Consider any node v . Let tf be the last time the faults occurred. Let tb be the time that s started broadcasting a value b. Finally, let tCancel be either the last time v performed Cancel on the broadcast of s, or the last time that the value of the broadcast of s at v became  (whichever came later). Below, if some of these times ti is undefined, then max{ti , tj , tk } = max{tj , tk }. 1. Speed: As long as the value b broadcast by the source node s does not change, Protocol ABC of s at v outputs b starting at time max{tf , tb , tCancel } + 2dist(s, v ). 2. Time adaptivity: (even for faulty nodes): At any time t such that t  max{tf + f, tCancel }, the output value (for s's broadcast) is either authentic or is undefined (equal to ). 3. Error Confinement (for non-faulty nodes): Let tf (v) be the last time that a fault hit node v . Assume, that the vote of some s changed in v from some b =  after time tf (v) . Then, it first changes to . In addition, starting from that time, the output value (for s's broadcast in v ) is either authentic or is . Finally, if the value does become authentic (after changing first to  then is stays authentic, unless additional faults occur.

5

Instability Upper Bounds

Theorem 3. There exists a self stabilizing protocol for Majority Consensus, such that if the local states of f of the nodes are changed arbitrarily, then ­ Time Adaptivity: The output values are restored everywhere in O(f ) time; ­ Instability: The instability in each process is O(log f ).

350

S. Kutten and T. Masuzawa

Theorem 4. There exists a protocol for the Persistent Value Problem such that if the local states of f < n/2 of the nodes are changed arbitrarily, then Time Adaptivity is achieved, and, in addition, ­ Instability: The instability is O(log f ) for non- faulty nodes and O(log n), for faulty nodes . ­ Complete state stabilization occurs in O(Diam) time units. ­ There is no change in the input of any correct process. Note, that the requirement that f < n/2 in the statement of Theorem 4 is required by the Input Correction Module of [12], to ensure persistence (not stability, nor stabilization). We note that these are the best possible output- and statestabilization times even when the instability is allowed to be higher [12]. However, if the time is allowed to be higher then the instability can be smaller. The protocols claimed in the theorems are presented in two subsections below. 5.1 Stable Adaptive Majority Algorithm (Theorem 3)

The algorithm for this easier problem is given in figure 1. Its informal description is given in the full paper. In short, each node broadcasts (using ABC) its input and collects the values broadcasted by the others. The node outputs the majority of the votes it receives, but only from a ball of radius Scanned around

Broadcast (using algorithm ABC) inputv ; Receive every arriving broadcast of other nodes;

(* possibly,  *)

Let NearestUndef = min{dist(s, v ) | valuev [s] = }; (* valuev [s] is s's vote as received in v *) If NearestUndef = Undefined then (* Received some  *) Let Reduced  max{integer i|2i < NearestUndef }. (* Reduce to exclude  *) Set Majority to the majority value in Ballv (2Scanned ); Suspects  {w  Ballv (2Scanned ) |  = (valuev [w] = Majority Wait  min{Wait - 1, |Suspects|}; (* Delay reducing Scanned *) If Wait  0 and Reduced < Scanned then (* without violating adaptivity. *) Scanned  min{Reduced, Scanned }; Cancel the broadcast of every node outside Ballv (Scanned); Wait  |Ballv (2Scanned )|. If for every node u in Ballv (2Scanned+1 ) the arriving valuev [s] is not  then Scanned  min{log RRadiusv , Scanned + 1}; Wait  |Ballv (2Scanned )|; Cancel the broadcast of every node outside Ballv (2Scanned ). Set output to the majority value in Ballv (2Scanned ).

Fig. 1. Stable Adaptive Majority Consensus Algorithm (with adaptive instability)

Output Stability Versus Time Till Output

351

it. The algorithm decreases Scanned, carefully, to exclude votes it suspects their authenticity, and increases Scanned, carefully, when it guesses that votes in a larger ball are authentic. It is easy to demonstrate that changing the algorithm so that it would change Scanned more "drastically" (e.g. would restart from Scanned = 0 every time a vote changes) would either not be adaptive, or would have high instability, or both. For example, the event that some vote becomes  can happen f times. Had the algorithms restarted to Scanned = 0 each time some vote became , the instability could have grown to  (f ). Had there been only increases (the last "If" statement), it seems easy to prove the O(log n) upper bound (maybe also the O(log f )). Bounding the number of decreases in the radius (the first "If" statement) is somewhat harder. Still harder, is bounding the number of decreases to be O(log f ) rather than O(log n) even in a node v that is faulty. Intuitively, all the votes that a faulty node v "believes" it received, it may have not received actually (which means that they are not authentic in v ), so the effect on the instability at v is as if there were n faults. The "wait" mechanism in the code is intended to allow non- authentic votes at v to disappear before decreasing Scanned. The node cannot wait too much, however, since this would have caused it not to be time adaptive. Hence, the node waits as much as possible given a lower bound ( |Suspects|) it computes for the number of faults. This Wait method bounds the number of times Scanned is decreased, even in a faulty node. Lemma 1. The instability of the Stable Adaptive Majority Algorithm of Figure 1 is O(log f ). The proof of the lemma is deferred to the full paper. We bring here only the most interesting case (that uses the Wait mechanism). This is the case that the value of Scanned is reduced at some time 0 < Cf . The main reason this case is interesting, since this is the case that votes in v may not be authentic (they become either authentic or  after O(f ) time). This can have an effect on v that is similar to a number of faults that is larger than f , which makes the proof of O(log f ) instability harder. Let v 's output after the reduction be some b1 . By the selection of the size of Wait, the number of votes received at v for b1 (= b1 ) must be some X0  Cf . Now, consider the next reduction at time 1 , that flips the value of the output to b1 . The number of votes for b1 may have increased by some X1 nodes who were not counted among the X0 above. (Some, or all of the old X0 may have changed their vote too.) Note, that these X1 nodes voted b1 right after 0 (and not , nor b1 ). By Item 3 of Theorem 2, the votes of these X1 nodes at 1 are authentic. Hence, their votes will not change, by the same theorem. If X1 > Cf then the next flipping reduction (to output b1 ) is delayed Cf time, and there are no more changes in v 's output, as shown above. Hence, X1 < Cf . Let us now consider the next flipping reduction (to output b1 ) at 2 . As in the previous argument, there may now be some new Y2 nodes who did not support b1 in the previous flipping reduction, but do support b1 now, and Y2 < Cf . So far, we established that the number of supporters of b1 , as well as the number of supporters of b1 are smaller than Cf . From the code, it is easy to

352

S. Kutten and T. Masuzawa

see that the new value of Scanned is selected such that there are no  voters in Ballv (2Scanned ). Hence, at that point, Scanned  2Cf . The number of additional reductions possible before the next increase is O(log f ). The lemma then follows from the proof for the increases in the value of Scanned (which follows immediately from the fact that Cancel is performed when the value of Scanned changes, and from Theorem 2; details are deferred to the full paper). The following lemma that bounds the "damage" of the Wait mechanism. Lemma 2. The Stable Adaptive Majority Algorithm of Fig. 1 is time adaptive. The proof of Lemma 2 bears similarities to those of [12,13]. The differences result from the following three mechanisms used in the current algorithm: (a) the algorithms in the previous paper outputs the majority of all the arriving votes, while here the algorithm outputs just those in a certain ball; (b) the current algorithm outputs the majority (in a ball) only when there are non-  votes from all the nodes (in the ball); (c) the Wait here may cause v to wait before changing the output to that of the majority. Correspondingly, the current proof needs to show the following: (a) the radius of the ball indeed reaches a size that is larger than 2f fast enough (so the majority of the votes in the radius are of correct nodes); (b) the radius of the ball is not too large (since otherwise, authentic votes from all the nodes it in may not be received fast enough; (c) the Wait mechanism does not delay the final output longer than O(f ). The proof is deferred to the full paper. Intuitively, within O(f ) time all the non-authentic votes disappear by Theorem 2 and authentic votes arrive from all the nodes in the ball of radius O(f ) around v . If Scanned starts "small" after the faults, then it can be increased to more than O(2f + 1), since the  values disappear from that ball in O(f ) time. More than f authentic votes in that ball imply a correct output. If Scanned starts large, when the non- authentic votes disappear (in O(f ) time) the majority it receives is correct. If v receives "many" non-  votes, then the output is correct. Otherwise, Scannedv is reduced. Finally, when non- authentic votes disappear, |Suspects|  f and hence, Wait  f . 5.2 Stable Time Adaptive Self Stabilized Persistent Value

(The proof of Theorem 4): We now move to deal with problems where the input may change not just as a result of faults. In particular, solutions for the the Persistent Value Problem have two modules. One, the Input Correction module, maintains (and changes) the storage (input) value (see Definition 4). Here, we only replace the second module- the Output Stabilization Module. The latter uses the above storage value as its input. We assume the use of the Input Correction Module introduced in [12]. The following is assumed for that Module (and proven in [12], given a module that stabilizes the output in O(f ) time): If f < n/2 then the Input Correction Module never changes the value of a nonfaulty node. It changes the value of an incorrect node at most twice: the last of these changes is from the incorrect value to the correct one. The algorithm presented in this section is a modification of the algorithm of Figure 1. Recall, that each decrease in Scanned in that algorithm may cause

Output Stability Versus Time Till Output

353

the output to change. An additional down side to decreasing Scanned is that if |Ballv (2Scanned )| becomes smaller than 2f , then the output may be incorrect (since the majority in the ball may be the faulty nodes) late after the algorithm was supposed to stabilize. This is why the algorithm of Figure 1 would not have been adaptive had it been used for the Persistent Value Problem. (Recall, that here a vote arriving at v may become , and then change value, after  (n) time; we do not want that to cause a reduction in Scanned). The new algorithm does not cut the value of Scanned every time an arriving broadcast changes its value. Instead, Scanned is reduced only when a constant fraction of the nodes in the ball change their mind. In addition, the algorithm attempts to output in the new ball the same value it outputted the previous time (if any) it used for that ball. Only if a significant number of votes changed in the new ball (from that last time) the output is not the output used the last time that new ball was used. This reduces the instability. Additional informal explanations, as well of the proofs of the following claim, are deferred to the full paper. The pseudo code appears in Figure 2. Claim. For any i, the second time (after the faults ended, and after the first increase in Scanned after the faults) that Scanned = i + 1, all the votes received at v are authentic. Lemma 3. If each input changes only a constant number of times (starting from some time t) in the Stable Persistent Value Algorithm, then the number of times Scanned = i gets assigned the value i is bounded by a constant. Proof Sketch: First, we claim that the number of times Scanned can be reduced from some i + 1 to i is bounded by a constant. Let s be the node such that the change in its ABC broadcast vote caused v to cut Scanned from i + 1 to i. First consider the case that the changed vote had not been authentic. By Claim 5.2, this can happen at most twice per value of Scanned. Now, assume that Scanned is reduced because the number of authentic votes i+1 )| (or below 1 for OldOutput(i + 1) is below 1 4 |Ballv (2 4 |Ballv (RRadius)|). The first sub-case is when Scanned < log RRadiusv . The first time Scanned is reduced from i + 1 after the faults, the value of OldOutput(i + 1) could be one that was set by the adversary (remember the setting of self stabilization). However, in later times this value is one that was set by the algorithm, since we are computing the instability at a period when there are no additional faults. Hence, and by Claim 5.2, every time (starting from the second time) Scanned = is increased to i + 1, the real majority of the inputs in Ballv (2i+1 ) is the value assigned to OldOutput(i + 1). This means that at the k th time Scanned is reduced from i +1 (k  3), at least a quarter of the nodes in Ballv (2i+1 ) changed their input since the (k - 1)th time. However, by the properties of the input correction module, a node can change its input at most twice after the faults (if it is a faulty node, otherwise, it cannot change its value at all). Hence, the number of times Scanned can be reduced from i + 1 to i is a constant. The second sub-case is when Scanned is reduced from log RRadiusv to log RRadiusv - 1. (The proof is similar to the proof of the previous sub-case.)

354

S. Kutten and T. Masuzawa

Now, consider the case that Scanned is set to i by an increase (and not by a reduction). To be increased again to i, the value of Scanned must first be reduced to i - 1, since a reduction is performed to a consecutive value. Thus, the proof follows from the proof for reductions. Finally, notice that v may change its output only when it changes the value of Scanned. Since the number of different values for Scanned is log RRadius, this leads only to an instability of O(log RRadius) which is O(log n). By a somewhat more precise analysis we obtain the following improved result. In most of the cases, the proof of the following lemma resembles that of Lemma 3. The main difference is in the case that a decrease in Scanned is due to unauthentic votes that disappear. This can happen only at a faulty node v . A sketch of the proof of the next lemma, highlighting the differences between this proof and that of Lemma 3 is deferred to the full paper, together with the proof of Lemma 5 (which bears similarities to the proof of Lemma 2). Theorem 4 follows from the two lemmas bellow and from the assumptions on the Input Correction Module. Lemma 4. The instability of the algorithm of Figure 2 is O(log f ) for a nonfaulty node, and O(min{log n, f }) for a faulty nodes.

Broadcast the input value and receive every arriving broadcast of other nodes (*possibly, undefined ()*). Do while Scanned > 0 and

( (Scanned < log RRadius

v

or (Scanned = log RRadiusv and N umV otes(OldOutput(Scanned) < or

and N umV otes(OldOutput(Scanned) < 1 |Ballv (2Scanned )|) 4
1 |Ballv (2Scanned )| 2

+ 1)

OldOutput(Scanned) =  OldOutput(Scanned)  ; Scanned  Scanned - 1; Cancel the broadcast of every node outside Ballv (2Scanned ). If Scanned = 0 then OldOutput(Scanned)  output  input; else output  OldOutput(Scanned). |Ballv (2i )| ; Let i > Scanned be the smallest for which b = |N umV otes(b) > 1 2 (* If i is not undefined then *) Scanned  i; Set output to the majority value in Ballv (Scanned); OldOutput(Scanned)  output; Cancel the broadcast of every node outside Ballv (2Scanned ).

)

Fig. 2. Stable Persistent Value Algorithm: actions at node v . NumVotes(b) is the number of votes v is currently receiving by ABC from nodes in Ballv (2Scanned ) for the value b.

Output Stability Versus Time Till Output

355

Lemma 5. The Stable Persistent Value Algorithm Algorithm of Figure 2 is time adaptive. 5.3 Repeated Faults, Majority Consensus with Persistence, and Repeated Majority Consensus

Previous algorithms for the Persistent Value problem assumed (for the sake of obtaining time adaptivity) that all the faults occurred in one batch, and another batch may occur only after full state stabilization. A useful property of the algorithm of Figure 2 is that this assumption is not necessary. Indeed, we did not use it in the proofs. Given that, it is not difficult to change that algorithm to solve the problem of Majority Consensus with Persistence, and using that solution to solve also the Repeated majority Consensus. We omit the changes required to solve these problems from the extended abstract.

6

Conclusion and Future Work

As claimed above, if the algorithms are allowed to be less adaptive, it is easy to change them to have a lower instability, to match the more generalized lower bound of Section 3. In the extended abstract, we assumed that the topology of the network is known to every node in advance. To lift this assumption, a node needs to detect that some broadcasts it receives are claimed to be arriving from nodes that do not actually exist. We deffer this in the full paper. We studied the instability for the case that the freedom was in the time till stabilization. It may be interesting to combine that with the freedom to decide what is correct, as studied in [1]. This may be especially interesting since it was demonstrated in [2] that multiple possible input values complicate their problem, while this does not seem the case here. We studied instability in the context of Consensus (and in the context of Persistence). Instability is expensive in other contexts as well. For example, when a network changes, the routing changes. Instability in the routing tables causes routed messages to loop. It is hoped that the understanding gained here will prove useful for increasing the stability for other problems. A common criticisms against self stabilizing algorithms is that they do not provide much guarantees on the output of nodes until the stabilization. The current paper provides some such guarantees. It would be interesting to find which additional such guarantees are possible.

References
1. Dolev, S., Rajsbaum, S.: Stability of long-lived consensus. In: JCSS (2003) 2. Davidovitch, L., Dolev, S., Rajsbaum, S.: Consensus continue? Stability of multivalued continuous consensus! In: GETCO 2004, Amsterdam, pp. 21­24 (October 4, 2004)

356

S. Kutten and T. Masuzawa

3. Dolev, S., Gouda, M., Schneider, M.: Memory requirements for silent stabilization. In: PODC 1996, pp. 27­34 (1996) 4. Awerbuch, B., Kutten, S., Mansour, Y., Patt-Shamir, B., Varghese, G.: Time optimal self-stabilizing synchronization. In: STOC 1993, San Diego, CA, pp. 652­661 (1993) 5. Ghosh, S., Gupta, A., Herman, T., Pemamraju, S.V.: Fault-containing selfstabilizing algorithms. In: PODC 1996 (1996) 6. Ghosh, S., He, X.: Fault-containing self-stabilization using priority scheduling. IPL 73(3-4), 145­151 (2000) 7. Ghosh, S., Pemmaraju, S.V.: Tradeoffs in fault-containing self-stabilization. WSS, 157­169 (1997) 8. Bui, A., Datta, A.K., Petit, F., Villain, V.: State-optimal snap-stabilizing PIF in tree networks. WSS, 78­85 (1999) 9. Lynch, N.: Distributed Algorithms. Morgan Kaufmann, San Francisco (1996) 10. Burns, J.E., Gouda, M.G., Miller, R.E.: Stabilization and Pseudo-Stabilization. Distributed Computing 7(1), 35­42 (1993) 11. Dayan, J., Kobett, J.J., Kutten, M.: Control of Surge Drums by Different Types of D. D. C. Algorithms. IJT 10(4) (1972) 12. Kutten, S., Patt-Shamir, B.: Stabilizing Time Adaptive Protocols. TCS 220(1), 93­111 (1999) 13. Kutten, S., Patt-Shamir, B.: Adaptive Stabilization of Reactive Tasks. In: Lodaya, K., Mahajan, M. (eds.) FSTTCS 2004. LNCS, vol. 3328. Springer, Heidelberg (2004) 14. Afek, Y., Kutten, S., Yung, M.: The Local Detection Paradigm and its Applications to Self Stabilization. TCS 186(1­2), 199­230 (1997) 15. Azar, Y., Kutten, S., Patt-Shamir, B.: Distributed Error Confinement. In: PODC 2003, Boston (July 2003) 16. Kutten, S., Peleg, D.: Fault-local distributed mending. J. of Alg. 30, 144­165 (1999) 17. Kutten, S., Peleg, D.: Tight Fault Locality. SIAM J. on Comp. 30(1), 247­268 (2000) 18. Dijkstra, E.W.: Self-stabilizing systems in spite of distributed control. CACM 17(11), 643­644 (1974) 19. Afek, Y., Bremler, A.: Self-stabilizing unidirectional network algorithms by powersupply. In: SODA (1997) 20. Bremler-Barr, A., Afek, Y., Schwarz, S.: Improved BGP Convergence via Ghost Flushing. In: INFOCOM 2003 (2003) 21. Burman, J., Herman, T., Kutten, S., Patt-Shamir, B.: Time-Adaptive Majority Consensus. In: Anderson, J.H., Prencipe, G., Wattenhofer, R. (eds.) OPODIS 2005. LNCS, vol. 3974. Springer, Heidelberg (2006) 22. Breitling, M.: Modeling faults of distributed, reactive systems. In: Joseph, M. (ed.) FTRTFT 2000. LNCS, vol. 1926, pp. 58­69. Springer, Heidelberg (2000) 23. Fischer, M.j., Lynch, N.A., Paterson, M.S.: Impossibility of distributed consensus with one faulty process. J. ACM 32(2), 374­382 (1985) 24. Arora, A., Zhang, H.: LSRP: Local Stabilization in Shortest Path Routing. In: DSN 2003, pp. 139­148 (2003) 25. Zhang, H., Arora, A.: Guaranteed fault containment and local stabilization in routing. Computer Networks 50(18) 26. Afek, Y., Dolev, S.: Local Stabilizer. J. Par. & Dist. Comput. 62(5), 745­765 (2002) 27. Dolev, S., Herman, T.: SuperStabilizing Protocols for Dynamic Distributed Systems. C.J.TCS 1997 4 (1997)

Output Stability Versus Time Till Output

357

28. Parlati, G., Yung, M.: Non-exploratory self-stabilization for constant-space symmetry-breaking. In: van Leeuwen, J. (ed.) ESA 1994. LNCS, vol. 855, pp. 26­ 28. Springer, Heidelberg (1994)

A

Appendix

Definitions for Broadcast with Error Confinement Definition 6. A protocol P is said to be an error-confined protocol for task  if for any execution with behavior  (possibly, containing a fault) there exists a legal behavior  of  such that (1) For each non-faulty node v , v = v . (2) For each faulty node v , there exists a suffix v of v and a suffix v of v such that v = v . The main point in the definition above is that the behavior of non-faulty nodes must be exactly as in the specification: only faulty nodes may have some period (immediately following the fault) in which their behavior does not agree with the specification. The broadcast task is defined as follows. Broadcast (BCAST Input actions: inps (b), done at node s  V , for b in some set D. Node s is called the source. Output actions: outp(b), required at every node v  V , where b  D  {}. Legal behaviors: There is at most one inps action. Each node v outputs outp() in each step up to some point, and then it outputs outp(b) in each step, where b is the value input by the inp action.

A Distributed Maximal Scheduler for Strong Fairness
Matthew Lang and Paolo A.G. Sivilotti
Department of Computer Science and Engineering The Ohio State University, Columbus OH, USA, 43210-1277 {langma,paolo}@cse.ohio-state.edu

Abstract. Weak fairness guarantees that continuously enabled actions are executed infinitely often. Strong fairness, on the other hand, guarantees that actions that are enabled infinitely often (but not necessarily continuously) are executed infinitely often. In this paper, we present a distributed algorithm for scheduling actions for execution. Assuming weak fairness for the execution of this algorithm, the schedule it provides is strongly fair. Furthermore, this algorithm is maximal in that it is capable of generating any strongly fair schedule. This algorithm is the first strongly-fair scheduling algorithm that is both distributed and maximal.

1

Introduction

An action system models a distributed systems as a set of actions, each of which is either enabled or disabled. A fairness assumption controls the selection of actions from this set for execution. For example, weak fairness requires that an action that is enabled continuously be selected while enabled infinitely often. Strong fairness, on the other hand, requires that an action that is enabled infinitely often (but perhaps not continuously) be selected while enabled infinitely often. Weak fairness is useful because of the minimal assumption it makes and the simple scheduling algorithm required to implement it: Select every action infinitely often. Strong fairness, on the other hand, is useful for simplifying the design of synchronization and communication protocols since it rules out the starvation of actions that are repeatedly enabled. While weak fairness reflects an asynchronous and independent scheduling of individual actions, strong fairness reflects some scheduling coordination to rule out certain pathological traces. The advantages of both models can be achieved by constructing a strongly-fair scheduler on top of an assumption of weak fairness. A program is correct if it can exhibit only behaviors permitted by its specification. A correct program is maximal [4] if it can exhibit all behaviors permitted by its specification. Maximal programs are important for testing component-based systems because they prevent a component implementation from providing unnecessarily deterministic behavior and, in this way, masking errors in its clients. For example, if a scheduling algorithm is not maximal, it is incapable of generating some traces that are otherwise possible under the corresponding fairness
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 358­372, 2007. c Springer-Verlag Berlin Heidelberg 2007

A Distributed Maximal Scheduler for Strong Fairness

359

assumption. These traces are no longer observable behaviors for the system built on top of such a limited scheduler. In this paper, we present a strongly-fair scheduler, layered on top of a weak fairness assumption. This algorithm is distributed: it does not maintain a global set of enabled actions and it permits concurrent selection of independent actions. Furthermore, this algorithm is maximal: any trace that satisfies strong fairness is a possible behavior of the scheduler. To our knowledge, this is the first stronglyfair scheduler that is both distributed and maximal.

2
2.1

Maximality and Fairness
Maximality

A program is maximal if it is capable of generating any behavior permitted by a specification [5,12]. This notion is similar to bisimulation [11,13]. However, bisimulation involves relating artifacts with similar mathematical representations, while maximality relates a program text to a formal specification. Proving the maximality of a program P with respect to a specification S is carried out in three stages. Firstly, one defines a set of specification variables mentioned by S and derives properties of traces of these variables from S . Next, one shows that an arbitrary trace   |S | satisfying these properties is a possible execution of an instrumented version P of P (chronicle correspondence ). Finally, one proves that every fair execution of P corresponds to a fair execution of P (execution correspondence ). Since  is a possible execution of P and every execution of P corresponds to a possible execution of P ,  is a possible execution of P . Hence, any trace in S is a possible execution of P . Constructing P is carried out by adding new variables, assignments to new variables within existing actions, guards to existing actions, and actions that assign to only new variables. These additions ensure that safety properties of P are safety properties of P . The new variables typically include read-only chronicle variables that encode the trace  and auxiliary variables (e.g., variables that encode the current point in the computation). Proving chronicle correspondence, requires showing that the execution of P follows a given trace  . Proving execution correspondence requires showing that (i) each added guard in P is infinitely often true and (ii) the truth of each added guard is preserved by the execution of every other action in P . These properties ensure that each action is infinitely often executed in a state where the additional guard is true. Thus, every weakly-fair execution of P corresponds to a weaklyfair execution of P . 2.2 Fairness

Consider the following UNITY [2] program:

360

M. Lang and P.A.G. Sivilotti

Program f airness var b : boolean x, y : int initially b assign A: true - x := x + 1 B: b - y := x

b := ¬b

Action A is always enabled. It increments x and sets b to ¬b . Action B is enabled in states where b is true and assigns to y the value of x . Weak fairness requires that every action be selected infinitely often. Under this assumption, the fairness program satisfies: the safety property (i) x increases by at most one in each step, and the progress properties (ii) x eventually increases and (iii) b eventually changes value. More formally: (i) x = k next |x-k |  1 , (ii) x = k ; x = k , and (iii) b = k ; b = k . Progress properties (ii) and (iii) follow from the fact that action A is infinitely often executed in a state where it is enabled. The only property involving y is one of safety: at each step of the computation, y either remains the same or changes to the value of x . Since action B may never be selected while enabled, no progress properties for y can be proven. For example, consider the sequence of actions: A, B, A, A, B, A, A, B, . . . . This schedule is weakly fair since all actions are selected infinitely often, but B never executes from an enabled state and so y never changes value. Strong fairness, on the other hand, requires that any action that is infinitely often enabled be selected while enabled infinitely often. Under strong fairness, the fairness program satisfies the same properties as it did under weak fairness. In addition, the program also satisfies new properties, including the progress property y increases eventually 2.3 Maximality and Scheduling Assertions should be as strong as possible and must hold in every possible program execution. A maximal scheduler ensures that the strongest properties we prove using the program text and a notion of fairness are the strongest properties of the actual system behavior. A non-maximal scheduler eliminates possible executions and therefore allows us to assert stronger properties that hold on only a subset of possible program executions. To illustrate, consider a non-maximal strongly-fair scheduler that allows an action to be disabled at most twice before being scheduled for execution in a state in which it is enabled. This scheduler is correct--actions which are infinitely often enabled are infinitely often executed in a state in which they are enabled. However, the scheduler clearly generates a small subset of possible correct schedules. If we schedule the program fairness using this scheduler, we see action A can execute at most four times before action B must execute in a state in which it is enabled. This allows us to prove much stronger properties about y , for example: x - y  4 is a program invariant.

A Distributed Maximal Scheduler for Strong Fairness

361

Although we can now assert a stronger program property, this is undesirable, for instance, in the case of testing. If one were to test the program fairness composed with such a non-maximal scheduler, one may be led to believe that x - y  4 is indeed an invariant of the system. In fact, it would be impossible to design a test case to expose the fact that it is not.

3
3.1

Specification
Description of the System

The system is comprised of a set of processes, each comprised of two components-- a client layer and a scheduler layer. Clients can be enabled, and an enabled client can be granted a lock. Holding a lock allows the client to access some resource, perform some action(s), or otherwise modify the system state, including enabling or disabling other clients. When a client modifies system state, it simultaneously increments its own count and releases the lock it holds. The scheduler layer manages locks. If a process is infinitely often enabled, the scheduler ensures that it is infinitely often granted a lock. We say two processes u and v are neighbors if u or v 's client can affect the other's enabledness. If the scheduler guarantees that no two neighboring processes simultaneously hold a lock, the client layer guarantees that held locks are eventually relinquished. The composed system generates a strongly-fair schedule--if a process is infinitely often enabled, it infinitely often changes its count. 3.2 Formal Specification of the Strong Fairness Problem

The system is comprised of a set of processes, P . All processes have access to a symmetric neighbor relation N  P 2 . We define N (u, v ) if u or v can affect the other's enabledness.1 . Each process u  P has boolean variables u.enabled and u.lock representing that process being enabled and holding a lock, respectively. A third variable, u.count , is the number of times action u has executed. Since the execution of actions is atomic, there is no state in which an action is executing. Consequently, we require that when an action u executes, u.count is incremented. 3.3 Client Layer Specification

The client layer is responsible for execution of the action associated with a process. Intuitively, a client is "idle" until it is granted a lock. When granted a lock, the client eventually executes its action and increments its count, releasing the lock. The specification for client u is:
1

This neighbor relation is irreflexive, it is never the case N (u, u) . This is not to say that a process cannot enable/disable itself by executing its action; this is captured in the specification. The irreflexitivity of N only simplifies presentation.

362

M. Lang and P.A.G. Sivilotti

(  v : v = u : constant v.lock ) (  v : v = u : constant v.count ) (  v, b : ¬N (u, v ) : constant v.enabled = b ) (  v, b, a, k : N (u, v ) : stable ¬u.lock  u.count = k  v.enabled = b  u.enabled = a ) (  v, b, a, k : N (u, v ) : u.lock  u.count = k  v.enabled = b  u.enabled = a unless ¬u.lock  u.count = k + 1 ) Hypothesis: invariant (  v : N (u, v ) : ¬(u.lock  v.lock ) ), invariant u.lock  u.enabled Conclusion: u.lock ; ¬u.lock

(C0) (C1) (C2) (C3)

(C4)

(C5)

Properties (C0)­(C2) ensure that clients can modify only the enabledness of neighbors. Property (C3) ensures that a lock is necessary for a client to act. Propery (C4) ensures that count is incremented and enabledness of neighbors affected only with the release of a lock. Property (C5) is a conditional property; if the scheduling layer maintains the properties that neighbors do not hold locks simultaneously and that only enabled clients hold locks, the client layer guarantees that a lock is eventually relinquished. The mutual exclusion property and the invariant in the hypothesis of (C5) are important; neighboring processes are permitted to modify the enabledness of their neighbors. If two neighboring processes u and v simultaneously hold locks, a process, say u , may execute its action and disable the other. Then v is not guaranteed to become re-enabled and execute its action, releasing the lock. 3.4 Scheduler Layer Specification

This layer schedules actions for execution by granting client processes locks-- when a client process holds a lock it is free to execute its associated action. constant u.count stable u.lock invariant u.lock  u.enabled invariant (  v : N (u, v ) : ¬(u.lock  v.lock ) ) Hypothesis: true ; u.enabled, C0, C2, C3, C4, C5, Conclusion: true ; u.lock (S0) (S1) (S2) (S3) (S4)

Properties (S0) and (S1) ensure that the scheduling layer does not modify the count, nor revoke a lock once granted. Property (S2) ensures that locks are granted only to enabled processes, while property (S3) ensures that neighbors do not hold locks simultaneously. Property (S4) is a conditional property that captures the notion of strong fairness. If a correct client process is infinitely often enabled, the scheduler infinitely often grants the process a lock.

A Distributed Maximal Scheduler for Strong Fairness

363

3.5

Composed Specification

Given the client and scheduler specifications, the composed specification of the system satisfies the strong fairness property: if a process is infinitely often enabled, it infinitely often increases its execution count. Formally, client scheduler satisfies: Hypothesis: true ; u.enabled Conclusion: u.count = k ; u.count = k + 1

4

Algorithm for Scheduling Layer

Solving the strong fairness scheduling problem entails providing an algorithm that satisfies the specification of the scheduling layer from the previous section. In addition, our goal is for this algorithm to be maximal with respect to the composed specification. The challenge in designing a strongly fair scheduler lies in limiting concurrency--no correct scheduler can always allow processes sharing a mutual neighbor to concurrently hold locks. As an illustration, consider the system with P = {x, y, z } and { x, y , x, z } representing N . Suppose y and z are enabled while x is disabled. A scheduler that always allows processes sharing a mutual neighbor to concurrently hold locks permits both y and z to acquire locks. Now suppose y executes its action, leaving x and y both enabled. Since z still holds its lock and N (x, z ) , x may not acquire a lock. Now suppose z executes its action, disabling x but leaving z enabled. The system is now in back in the state where y and z are enabled while x is disabled. A scheduler that always allows processes with a mutual neighbor to concurrently hold locks allows this sequence of events to repeat continually, resulting in a schedule where x is infinitely often enabled but never executed. We overcome this challenge by bounding the number of times a process allows its neighbors to hold locks concurrently. Although unintuitive, this will not affect the maximality of our solution: our scheduler will be capable of generating any schedule satisfying the strong fairness property. Furthermore, any correct algorithm satisfying the strong fairness scheduler specification can be viewed as a refinement of our algorithm. 4.1 Scheduler Design

In order to ensure the mutual exclusion property S3 , we associate with each pair of neighboring processes u, v a shared lock token, tok (u, v ) . A process may only be granted a lock if it holds all of its shared tokens. A process u also stores a read-only boolean array, u.en , storing the enabledness of its neighbors. A process v notifies a neighbor u of its enabledness by assigning to u.en[v ] . To ensure progress, each process u is has a height, u.ht , representing its priority. A process is higher-priority than another if it has greater height. We require a process's height to be unique among its neighbors. Ties in priority

364

M. Lang and P.A.G. Sivilotti

between non-neighbors are broken by a static order on processes, say by process id. We will call lock tokens shared with higher-priority neighbors high tokens and lock tokens share with lower-priority neighbors low tokens. A process only changes its priority after it has executed its action and released a held lock, at which point it lowers its height by a nondeterministically chosen finite but unbounded amount. A process which has released a lock holds all of its tokens until it lowers its height, at which point it gives up all its high tokens. Processes always release tokens to higher-priority neighbors (high neighbors ). An enabled process does not relinquish tokens to lower priority neighbors (low neighbors ) and, in order to limit concurrency while still ensuring progress, a disabled process releases at most one low token. In order to ensure there are no wait-cycles, a disabled process u releases a low token only to its highest priority low neighbor, v . If u.en[w] holds later for some higher-priority low neighbor w , u retrieves the shared token from v by assigning true to v.en[u] . It is guaranteed to eventually receive the token as processes always relinquish high tokens. In addition, process u includes a boolean variable u.gate . If u.gate is true, u is free to exchange tokens with its neighbors or grant itself a lock. When u grants itself a lock, it sets u.gate to false. Upon releasing a lock, the process sets u.gate to true, lowers its height, and releases its high tokens. The following predicates are associated with a process u : ­ u.sendtok.v for all neighbors v of u . u.sendtok.v is true if a process u should send its shared token to process v . u.sendtok.v is true if v is a high neighbor of u and either u.en[v ] or ¬u.enabled . u.sendtok.v is true when v is a low neighbor of u and v is the highest-priority among all low neighbors of u , w = v , for which u.en[w] = true . u.sendtok.v  tok (u, v ) = u  ( ( u.ht < v.ht  (¬u.enabled  u.en[v ]))  ( u.ht > v.ht  u.en[v ]  (  w : N (u, w)  w.ht < u.ht : tok (u, w) = u )  v.ht = ( Max w : N (u, w)  w.ht < u.ht  u.en[w] : w.ht ))) ­ u.maylock . u.maylock is true if u is enabled and holds all its tokens. u.maylock  u.enabled  (  v : N (u, v ) : tok (u, v ) = u ) ­ u.retr.v for all neighbors v of u . u.retr.v is true if u has granted a low token to v and now some higher low neighbor of u is enabled. u.retr.v  tok (u, v ) = v  (  w : N (u, w)  u.en[w] : v.ht < w.ht < u.ht ) Figure 1 shows this implementation of u 's scheduler layer. Actions Uu,v and Tu,v are understood to be quantified across all neighbors v of u .

A Distributed Maximal Scheduler for Strong Fairness Program SFu var u.enabled, u.gate, u.lock : bool u.ht : integer u.en : array of bool initially (  v : N (u, v ) : u.ht = v.ht ) ¬u.lock u.gate assign true - v.en[u] := u.enabled  u.retr.v Uu,v u.sendtok.v  u.gate - tok(u, v ) := v Tu,v u.maylock  u.gate - u.lock := true; Lu u.gate := false ¬u.lock  ¬u.gate - u.gate := true; Du u.ht :=? st u.ht < u.ht  (  v : N (u, v ) : u.ht = v.ht ); ( v : N (u, v )  u.ht < v.ht : tok(u, v ) := v ) Fig. 1. Maximal Strong Fairness Scheduling Algorithm

365

Action Uu,v updates v.en[u] by assigning true if u.enabled or u.retr.v and assigns false otherwise. Action Tu,v sends a token to v if u is free to exchange tokens and u.sendtok.v is true. Action Lu grants a lock to process u and stops further communication by setting u.gate to false. Finally, action Du frees u to exchange tokens with neighbors, lowers its height by a finite but unbounded amount, and releases u 's high tokens. Du is enabled only after a process has relinquished a lock and executed its action. Note: In the algorithm SF , we assume that a process can read the height of its neighbors. In practice, this information can be encoded on shared tokens as differences in height, and by storing locally the height of the (unique) low neighbor holding a token.

5

Correctness of SFu

Properties (S0), (S1), and (S2) follow directly from the program text. Property (S3) is satisfied since a process must hold all its shared tokens to grant itself a lock and a process does not relinquish its tokens while it holds a lock. The progress property (S4) (that an infinitely often enabled process holds a lock infinitely often) requires a more thorough treatment. In the interest of space, however, we only sketch the key proof ideas here. The complete proof is available in [9]. In order to prove (S4), we show: (i) the system is free from deadlock, (ii) a process with no higher priority neighbors that becomes enabled eventually acquires a lock, (iii) a continually enabled process eventually is granted a lock, and finally (iv) an infinitely often enabled process eventually is granted a lock. Part (i) follows from the acyclicity of the partial order of priorities. The remaining parts rely on the identification of a metric. We define u.M to be the sum of the difference in height between u and all processes with higher priority than u that are reachable from u by following the neighbor relation through

366

M. Lang and P.A.G. Sivilotti

higher-priority processes. More formally, we define the set u.ab = u.abn is defined by recursion: u.ab0 = { v | u.ht < v.ht  N (u, v ) } u.abi+1 = { v | (  w : w  u.abi : N (v, w)  u.ht < w.ht ) }

u.abn where

Then u.M = ( v : v  u.ab : v.ht - u.ht ) . By definition, u.M is bounded below by zero when u.ab =  and u has no higher-priority neighbors. Furthermore, u.M is non-increasing unless u acquires a lock and lowers its height. To show the progress property, we demonstrate that if u.M = k and u is infinitely often enabled, eventually either u.M < k or u.lock . Since u.M is bounded below and non-increasing unless u acquires a lock, eventually u acquires a lock.

6

The Maximality of SF

Since maximality is noncompositional, we use the rely-guarantee style proof outlined in [10] as a template. This method for proving the maximality of composed systems involves stipulating that other processes in the system satisfy certain properties beyond their formal specification and proving the maximality of the composed system using these properties. These additional properties entail that the client process our system is composed with is maximal and can be constrained in a way to establish its maximality. In the interest of space and clarity, we only present the intuition behind the proof of maximality in this section. The interested reader should refer to [9] for a thorough proof of maximality of SF . In this section we reverse the priority relation described in Section 4 to clarify presentation and allow the reader to maintain an intuition about the behavior of the constrained system. In Section 4 a process was higher priority if it had a greater height and processes lowered their priority by lowering their height. In this section, we will reverse this--a process has higher priority if it has a lesser height, thus a process lowers its priority by increasing its height. 6.1 Proving the Maximality of SF

In order to prove SF is a maximal implementation of the strong-fairness specification, we need to show that any trace satisfying the strong-fairness specification is a possible trace of SF . In order to accomplish this, we create a constrained program SF from SF that accepts as input any trace  satisfying the strongfairness specification. We then show that at each point i in the trace  , the state of the system is exactly that of i . This establishes  as a possible execution of SF . Next we need to show that any fair execution of SF corresponds to a fair execution of SF . Then, since any trace  satisfying the specification of the strong fairness problem is a possible execution of SF , any trace satisfying the strong fairness problem is a possible execution of SF .

A Distributed Maximal Scheduler for Strong Fairness

367

However, this simple view is not quite complete. Since we want to show that any schedule of action executions is a possible behavior of the composed system, we need to stipulate that the client process composed with SF satisfies some additional requirements. Namely, we require that this client process can be constrained to produce client which, when composed with SF , can take the "steps" in the computation that  dictates. i.e., if at some point i in  some process u is to execute and enable/disable itself or its neighbors, client can compute this step. The additional requirements are that the client process is maximal, client satisfies the safety properties of the client specification, and that client is created in a way that ensures the correspondence between executions of client and the client process. In order to compute  , we introduce a variable p shared by client and SF that marks the current point in the trace (i.e., p ). We then prove (i) it is invariant that the current state is p and (ii) the point p eventually increases. It follows that  is a possible execution of SF client . 6.2 A Strong Fairness Trace

Let  be a stutter-free sequence of tuples  = 0 , 1 . . . representing the state of processes in an execution satisfying the strong-fairness specification. i = E, C i is a tuple containing two arrays, Ei and Ci , representing the enu abledness and count of processes in state i . That is, Ei = true if u.enabled u in i and Ci = k if u.count = k in i .  is stutter-free in that each tuple in the sequence differs from the previous by at least one element, unless the execution is in a state of quiescence (each processes is disabled forever). Since  is a correct trace of the strong fairness scheduling problem, it obeys certain properties. Namely, it satisfies the following: in subsequent states in  , at most one process changes count (by incrementing it by one) and if a process changes enabledness, a process must change count. Also, if a process is infinitely often enabled in the trace, it infinitely often changes its count. Given a trace  , we create an isomorphic trace  by inserting a stutteringstate in between every i and i+1 . That is, 0 = 0 and i+1 is i if i is even and is (i+1)/2 if i is odd. 6.3 Requirements of clientu

We require that a client process u can be constrained to produce clientu . The requirements on clientu are as follows: ­ clientu is produced from the client process by only adding new variables, assignments to new variables, and new guards referencing new and existing program variables. Furthermore, if random assignments in the client process are replaced with deterministic assignments, we require that the assigned value satisfy the predicate on the random assignment. These requirements ensure that clientu satisfies the safety properties of the client process. ­ The additional guards of clientu are infinitely often true and the enabledness of each guard is preserved by the execution of any other action in the system.

368

M. Lang and P.A.G. Sivilotti

u ­ At each point p in the computation, it is invariant that u.enabled = Ep u and u.count = Cp . ­ clientu does not assign to  and only changes p by at most one. u ­ If SFu ensures u holds a lock at a point p = k in the trace where Ck = u Ck+1 (i.e., u executes its action), clientu guarantees that p is incremented and the lock is released.

These requirements on the client process ensure that client will compute the transitions dictated by  . It is then the obligation of SF to ensure that processes hold locks when  dictates u executes its action and increments its count. 6.4 The Constrained Program SFu

In the constrained program SFu we introduce the following objects not found in SFu : the input trace  and the point p , a function u.next to compute the next point at which process u executes its action and increments its count, a predicate u.done to indicate whether or not u increments its count again after the current point in the computation, and a predicate u.quiet which indicates whether or not u is enabled after the current point in the computation. Formally, u.quiet , u.done , and u.next are defined as the following.
u ) u.quiet  (  i : i  p : ¬Ei u u u.done  (  i : i  p : Ci = Ci +1 ) u u u.next = ( Min i : i  p : Ci = Ci +1 ) if ¬u.done u ( Min i : i  p : (  j : j  i : ¬Ej 

(  v : v = u : v.ht = j ) ) ) otherwise Figure 2 shows the instrumented program. The key property that follows from this instrumentation is that a process u 's height corresponds to the next point in the computation when u increments its count. At that point, u is the highest priority enabled process among its neighbors (i.e., lowest height ). Any process with a higher priority (lower height) than u at that point is in a state of quiescence. If a process u has executed for the last time, we set its height to be after the last point in the trace that it is enabled. This ensures that any process that executes and enables/disables u will be higher priority than u until u is quiescent. Such a point is guaranteed to exist by the assumption that the process has executed for the last time; if no such point exists, the process must be infinitely often enabled (and therefore execute again). The motivation for the introduction of stutter states in  is to ensure that a process that never executes again can be assigned a unique height. If  were stutter-free, it is not guaranteed that such a point exists.

A Distributed Maximal Scheduler for Strong Fairness Program SFu var u.enabled, u.gate, u.lock : bool u.ht : integer u.en : array of bool initially p = 0 u u.enabled = Ep ¬u.done  u.ht = u.next u ) u.done  u.ht  min i (  j : i  j : ¬Ej (  v : N (u, v ) : u.ht = v.ht ) ¬u.lock u.gate assign Uu,v true - true - v.en[u] := u.enabled  u.retr.v Tu,v true - u.sendtok.v  u.gate - tok(u, v ) := v Lu (u.ht = p  ¬u.done)  u.quiet - u.maylock  u.gate - u.lock := true; u.gate := false Du true - ¬u.lock  ¬u.gate - u.gate := true; u.ht := u.next; ( v : N (u, v )  u.ht < v.ht : tok(u, v ) := v ) Q i = i+1 - p := p + 1 Fig. 2. Constrained Strong Fairness Scheduling Algorithm

369

A key invariant of SFu is that if ¬u.done and u.gate hold, u.ht = u.next . SFu inherits the safety properties of SFu as guards are only strengthened and existing program variables are not assigned to, except for the replacement of the random assignment to u.ht with a deterministic assignment. However, at the point of the assignment to u.ht , u.next > u.ht and is unique by definition of u.next and the properties of  . 6.5 Proof Sketch of the Maximality of SF

There are two main obligations to dispatch: (i) SF client computes  and (ii) every fair execution of SF client corresponds to a fair execution of the original system. u u  u.count = Cp is an invariant of (i) is proved by showing u.enabled = Ep the system and p = k ; p = k + 1 . (ii) requires showing that the truth of each additional guard in the system is preserved by the execution of any other action and that each additional guard is infinitely often true. Then each additional guard is executed infinitely often in a state where it is true, corresponding to a fair execution of the original program. Proving the invariant: The invariant in (i) is initially true by the initially predicates in SFu . Also, each action of SF maintains the invariant as no action assigns to the trace, u.enabled , or u.count and the only action that assigns to

370

M. Lang and P.A.G. Sivilotti

p only increments p in a stuttering state. Thus, since the invariant is also a property of clientu , it is an invariant of the composed system. Proving p = k ; p = k + 1 : There are two cases to consider -- the case where the current point is a stuttering-state, in which action Q increments p , and a non-stuttering state. In a non-stuttering state, there exists some process u such u u that Cp = Cp +1 . It was a requirement on clientu that if u holds a lock in such a state, clientu eventually increments p . It is the responsibility of SF to ensure that in such a state process u eventually acquires a lock. At that point in the computation u.next = p and ¬u.done holds. Without loss of generality, assume u.gate holds as well, so by the invariant of SFu , u.ht = p . Also, by the way height is assigned u is the highest priority process among all its neighbors that are enabled. So u eventually acquires all its tokens and acquires a lock. Proving the stability of additional guards: Since clientu is required to satisfy this property, it suffices to show that the guard of Lu is not falsified by any action of SFu . It is easy to see that the only actions which might affect the truth of the additional guard of Lu are Q , which assigns to p , and Du , which assigns to u.ht . Since u.quiet is stable, neither Q nor Du can falsify it. Now, if u.ht = p  ¬u.done hold, it is implied by the invariant that p = p+1 , so Q is disabled in such a state. If action Du is enabled, ¬u.lock  ¬u.gate holds. Then since ¬u.lock  ¬u.gate holds, clientu must have released a lock and incremented the point, which implies u.ht < p . So if Lu is enabled, Du is not. Proving additional guards are infinitely often true: Again, since this was a requirement of clientu , we only need consider the guard of Lu . Now, since u.quiet is stable and if u.done ever holds, eventually u.quiet holds, it suffices to show that u.ht = p  ¬u.done is infinitely often true if ¬u.quiet is an invariant of the trace. Assuming ¬u.quiet is an invariant of the trace, ¬u.done is an invariant of the trace as well. Now, if ¬u.gate holds at any point in the computation, it must be the case ¬u.lock holds as well and both continue to hold until eventually Du is executed. The execution of Du in an enabled state ensures u.gate holds. Then the invariant of SFu dictates that u.ht = u.next and, since u.next  p and p = k ; p = k + 1 , eventually u.ht = p . Thus, the additional guard of Lu is infinitely often true. The Maximality of SF : The preceding arguments establish that any trace  satisfying the strong-fairness specification is a possible execution of SF composed with a client process meeting the requirements described. It follows that SF is a maximal strongly-fair scheduler.

7

Discussion

Fairness is a well-researched and developed notion in existing literature, both in terms of interaction fairness [1] and in terms of selection of actions in nondeterministic guarded command programs [8]. Although a large body of work

A Distributed Maximal Scheduler for Strong Fairness

371

surrounds fairness issues, our algorithm is unique in that it is the first solution for strongly-fair scheduling of atomic actions that is both maximal and distributed. In [7], Karaata gives a distributed self-stabilizing algorithm for the stronglyfair scheduling of atomic actions under weak fairness. A key property of the algorithm is that an action u can disable another action v at most twice before action v must execute, and therefore this algorithm is not maximal. In addition, although there is no notion of a "lock," the algorithm precludes two processes with a shared neighbor from both having the "right" to execute their actions. Although this does not affect the possible schedules the algorithm can generate, it does limit the algorithm from being generalized to a situation where the mutual exclusion property of the strong-fairness specification can benefit processes (e.g., processes perform some computation before releasing the lock and affecting their neighbors). Then the concurrency of non-neighboring processes holding locks is a valuable property. Karaata's algorithm has the advantage of being self-stabilizing, whereas ours does not. Also, Karaata provides a brief message complexity analysis of the algorithm while we make no claims regarding the message complexity of our algorithm. In [6], Joung develops a criterion for implementability of fairness notions for multiparty interactions. If a fairness notion fails to meet the criterion, then no deterministic scheduling algorithm can meet the fairness requirement in an asynchronous system. In the general case, both strong interaction fairness and strong process fairness fail to meet the criterion. The dining philosophers problem proposed by Dijkstra [3] is superficially similar (as also pointed out in [7]) to the strong-fairness problem in that one can map the state ¬u.enabled to thinking, u.enabled  ¬u.lock to hungry, and u.enabled  u.lock to eating. However, in the dining philosophers problem, a process becomes hungry autonomously, not as a result of the behavior of other processes in the system. Furthermore, processes remain hungry until the arbitration layer affects a change in state to eating. The possibility for processes to affect the enabledness of neighboring processes adds complexity to the strong fairness scheduling problem. For example, a solution to the dining philosophers problem can maintain an invariant that if a process holds a request from a neighbor, that neighbor is hungry. No corresponding invariant can be shown for a solution to the strong-fairness problem without synchronization between a process and its neighbor's neighbors.

8

Conclusions

In this work we presented a formal specification of the distributed strong fairness scheduling problem and described a maximal solution SF to the problem. The importance of a maximal scheduling algorithm was discussed in detail in Section 2, making the maximality of the SF algorithm a key contribution of the work. The maximality of SF also implies that any correct implementation of the strong-fairness specification is a refinement of the SF algorithm in that any correct algorithm's behavior is a subset of the behavior of SF .

372

M. Lang and P.A.G. Sivilotti

References
1. Apt, K.R., Francez, N., Katz, S.: Appraising fairness in distributed languages. In: POPL '87: Proceedings of the 14th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pp. 189­198. ACM Press, New York (1987) 2. Chandy, K.M., Misra, J.: Parallel Program Design: A Foundation. Addison-Wesley, Reading, Massachusetts (1988) 3. Dijkstra, E.W.: Hierarchical ordering of sequential processes. Acta Informatica 1(2), 115­138 (1971) 4. Joshi, R., Misra, J.: Maximally concurrent programs. Formal Aspects of Computing 12(2), 100­119 (2000) 5. Joshi, R., Misra, J.: Toward a theory of maximally concurrent programs. In: Proceedings of PODC '00, pp. 319­328 (2000) 6. Joung, Y.-J.: On fairness notions in distributed systems, part I: A characterization of implementability. Information and Computation 166, 1­34 (2001) 7. Karaata, M.H.: Self-stabilizing strong fairness under weak fairness. IEEE Trans. Parallel Distrib. Syst. 12(4), 337­345 (2001) 8. Lamport, L.: Fairness and hyperfairness. Distrib. Comput. 13(4), 239­245 (2000) 9. Lang, M., Sivilotti, P.A.G.: A distributed maximal scheduler for strong fairness. Technical Report OSU-CISRC-7/07-TR61, The Ohio State University (July 2007) 10. Lang, M., Sivilotti, P.A.G.: The maximality of unhygienic dining philosophers. Technical Report OSU-CISRC-5/07-TR39, The Ohio State University (May 2007) 11. Milner, R.: Communication and concurrency. Prentice-Hall, Inc., Upper Saddle River, NJ, USA (1989) 12. Misra, J.: A Discipline of Multiprogramming: Programming Theory for Distributed Applications. Springer, New York (2001) 13. Park, D.: Concurrency and automata on infinite sequences. In: Proceedings of the 5th GI-Conference on Theoretical Computer Science, London, UK, pp. 167­183. Springer, Heidelberg (1981)

Cost-Aware Caching Algorithms for Distributed Storage Servers
Shuang Liang1 , Ke Chen2 , Song Jiang3 , and Xiaodong Zhang1
1

The Ohio State University, Columbus, OH 43210, USA 2 University of Illinois, Urbana, IL 61801, USA 3 Wayne State University, Detroit, MI 48202, USA

Abstract. We study replacement algorithms for non-uniform access caches that are used in distributed storage systems. Considering access latencies as major costs of data management in such a system, we show that the total cost of any replacement algorithm is bounded by the total costs of evicted blocks plus the total cost of the optimal off-line algorithm (OPT). We propose two off-line heuristics: MIN-d and MIN-cod, as well as an on-line algorithm: HD-cod, which can be run efficiently and perform well at the same time. Our simulation results with Storage Performance Council (SPC)'s storage server traces show that: (1) for off-line workloads, MIN-cod performs as well as OPT in some cases, all is at most three times worse in all test case; (2) for on-line workloads, HD-cod performs closely to the best algorithms in all cases, and is the single algorithm that performs well in all test cases, including the optimal on-line algorithm (Landlord). Our study suggests that the essential issue to be considered be the trade-off between the costs of victim blocks and the total number of evictions in order to effectively optimize both efficiency and performance of distributed storage cache replacement algorithms.

1

Introduction

Widely used distributed storage systems have two unique features: storage device heterogeneity and multi-level caching management. In a typical multi-level heterogeneous distributed storage system, I/O buffer caches are installed at hierarchical levels. Access latencies to data blocks are no longer a constant due to non-uniform access times caused by heterogeneous storage devices and hierarchical caching. This adds another dimension to the management of distributed storage caches, which is a significant impact factor to the system performance. However, most existing replacement algorithms in practice focus on minimizing miss rate as the single metric for performance optimization, treating access latency as a constant. For example, recent studies on replacement algorithms such as 2Q, ARC, LIRS, and MQ mainly aim to improve the traditional LRU heuristic1 , which consider only block recency or balance both recency and frequency
1

A brief overview of these algorithms is available in [1].

A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 373­387, 2007. c Springer-Verlag Berlin Heidelberg 2007

374

S. Liang et al.

to reduce miss rate. These algorithms may not be suitable to manage caches of variable access latencies in distributed storage systems. The replacement problem for caches with non-uniform access latencies can be modeled by the weighted caching problem, which can be solved off-line in O(kn2 ) time by reduction to the minimal cost flow problem [2], where k is the cache size and n is the number of total requests. However, this optimal algorithm is resource intensive in terms of both space and time for real-world system workloads, particularly when k and n are large. As an example, for a sequence of only 16K requests and a buffer cache of as small as 1.5 MBytes, the best known implementation of the minimum-cost flow algorithm [3] takes more than 17 GBytes of memory and multiple days to run on a dual-core 2.8GHz SMP Xeon server. Therefore, as current workloads and cache capacity continue to scale up, it becomes too unrealistic to timely make optimal replacement schedules. In face of this problem, we study replacement algorithms for non-uniform access latency caches. Similar to previous studies, we use variable cost to model the non-uniform access latency in order to improve the efficiency and performance of replacement algorithms. In general, our model can be used for a distributed storage system with other non-uniform features, such as non-uniform energy consumption per access by instantiating costs as energy access consumption for different blocks to minimize total the energy consumption. We show that for any replacement algorithm, the total access cost is bounded by the total cost of the evicted blocks (see Section 3) of the replacement algorithm plus the total access cost of the optimal algorithm (OPT). Therefore, the key to design variable-cost cache replacement algorithms lies in the trade-off between the number of evictions and the cost of victim blocks. Based on this principle, we propose two off-line algorithms: MIN-d and MIN-cod. Specifically, we take the variable cost consideration into MIN ­ the optimal replacement algorithm for uniform caches [4]. We found that choosing replacement victims based on the ratio of cost and forward distance (see Section 3.1) is effective for minimizing the total costs. Using this heuristic, we also propose an on-line replacement algorithm HD-cod, which adaptively selects victims among blocks of largest recency from different cost groups. We have evaluated the performance of the proposed algorithms with Storage Performance Council's storage server traces [5] by comparing our algorithms with OPT, Landlord [6] ­ a theoretically optimal on-line cost-aware algorithm , and other well-known cost-unaware replacement algorithms such as LRU, LFU, MRU, and Minimal Cost First (MCF). The results demonstrate that the proposed algorithms can be executed efficiently. Among all the algorithms, MIN-cod performs best in all cases, whose total cost is the same as OPT in some cases, and is at most four times of the lower bound of OPT in all cases. MIN-d performs similarly to MIN-cod when the cost distribution is small. In on-line scenarios, HD-cod performs close to the best algorithms in all cases and is the single algorithm that performs well in all test cases.

Cost-Aware Caching Algorithms for Distributed Storage Servers

375

2

Preliminaries

The weighted caching problem [6] is defined as follows. Given a request sequence of data blocks:r1 , ..., rn , each block has a cost (or weight) cost(r). For a cache of size k , upon each request r, if the requested block is not in cache(a miss), it is fetched in with cost(r). At the same time, one block in the cache is replaced to make space for r. If the requested block is already in cache, then no cost is involved. The goal is to minimize the total cost to serve the request sequence. An algorithm for the problem which assumes prior knowledge of the complete request sequence is an off-line algorithm. If an algorithm only knows the current and past requests in the sequence, then it is an on-line algorithm. OPT: An Optimal Off-Line Algorithm. Chrobak et al. [7] gave an optimal off-line algorithm for the weighted caching problem by reducing it to the minimal cost maximum flow problem [2]. Since the minimum-cost maximum flow problem can be solved in O(kn2 ) time and the reduction step takes only O(n2 + k ) time, the problem can be solved in O(kn2 ) in total. Therefore, this optimal offline algorithm is resource intensive, especially when k and n are large. For example, for a request sequence of one million blocks, the flow network has around one thousand billion arcs to process and needs several terabytes storage, which is well beyond the capability of current off-the-shelf servers. 2.1 MIN: An Off-Line Algorithm for Uniform Cost Cases

Belady's MIN algorithm [4] is based on the assumption of a uniform block access cost. It always replaces the block to be requested furthest in the future. Belady proved that MIN can minimize the total number of misses, thus minimizing the total cost when the access cost to each block is uniform. Compared with OPT, MIN is much more efficient ­ it can be implemented in O(n log k ) time and O(n) space. In a variable-cost cache, MIN's replacement decision can be far from optimal, since the furthest blocks can carry high fetch costs and lead to subsequent high miss penalties. Next we analyze its performance in variable-cost cases. Definition 1. A cache configuration is the set of (distinct) blocks resident in a given cache C . Given a request sequence S and an initial cache configuration cfg(C), let tc(Alg, cfg(C), S ) denote the total fetch costs incurred by an algorithm Alg; and let tf (Alg, cfg(C), S ) denote the total number of fetches (misses). Let S be a request sequence, and cfg(C) be an initial cache configuration. Let costmin = minrS cost(r) and costmax = maxrS cost(r) be the minimal fetch cost and the maximum fetch cost among all requests, respectively. it is easy to verify the following relationship. tf (MIN, cfg(C), S )  costmin  tc(OPT, cfg(C), S )  tc(MIN, cfg(C), S )  tf (MIN, cfg(C), S )  costmax

376

S. Liang et al.

When costmax /costmin is sufficiently small, MIN works pretty well; indeed, tc(MIN, cfg(C), S ) is bounded together with tc(OPT, cfg(C), S ) ­ the optimal cost by a narrow range. Actually, if a cache-resident block with an access cost of costmin is to be referenced furthest in the future, it is the optimal victim candidate for replacement. However, it is not easy to choose victims when the furthest-to-be-referenced block has a non-minimal cost. Therefore, a new heuristic is needed to choose victims efficiently and accurately.

3

Cost-Aware Cache Replacement

A storage cache is a fully associative cache. It has two kinds of misses: cold miss and capacity miss [8]. Therefore, any replacement algorithm's total fetch cost can be divided into cold-miss cost and capacity-miss cost. Since cold miss is compulsory, it is the same for any algorithm including OPT, which means the cold-miss cost is no greater than the total cost of OPT. Therefore, we have the following observation. Definition 2. Given a request sequence S , on each fetch of any replacement algorithm, a block is evicted if it is replaced and is to be requested later in the remaining request sequence. Observation 1. Let S be a request sequence, cfg(C) be an initial cache configuration, Alg be a replacement algorithm. Let v1 , v2 , . . . , vm be the sequence of blocks evicted by Alg when it serves S . It holds that tc(Alg, cfg(C), S )  tc(OPT, cfg(C), S ) + m 1 cost(vi ). Observation 1 shows that the key to design replacement algorithms for a weighted cache is the trade-off between the cost of eviction victims and the total number of m evictions to minimize 1 cost(vi ). If a replacement algorithm is wise in choosing replacement victims such that no eviction is needed, then the total cost involved is the same as OPT. Otherwise, it performs at most m 1 cost(vi ) worse than OPT. In addition, unlike competitive analysis involving some unknown constant, observation 1 shows that a concrete upper bound of extra cost compared with OPT can be determined by simply adding the costs of all the evicted blocks, which can be implemented with negligible overhead in real systems. Such an upper bound is useful for evaluating the cache efficiency of a replacement algorithm. On the other hand, it can also be used to calculate the lower bound of OPT so as to estimate its total cost by deducting the eviction costs from the total costs of tested replacement algorithms. m Aimed at minimizing 1 cost(vi ), next we propose two off-line algorithms and one on-line algorithm. 3.1 MIN-d Algorithm

Our first algorithm, MIN-d, is an extension of MIN. It chooses the minimal-cost block from the d + 1 furthest blocks (d  0) as victim rather than choose the one

Cost-Aware Caching Algorithms for Distributed Storage Servers

377

Input: request sequence S and initial cache configuration cfg(C). for each request r in S do if r  / cfg(C) then Let Q  cfg(C) be the set of d + 1 resident cache blocks having the largest forward distances. Let b  Q be the block having the smallest cost among Q. Replace b and read block r . Update cfg(C) and the forward distances of resident cache blocks. Fig. 1. MIN-d Algorithm

furthest block without consideration of costs. In particular, when d = 0, MIN-d is MIN. Before giving the details of MIN-d, depicted in Figure 1, we make the following definition. Definition 3. Given a block r resident in the cache, the forward distance of r, denoted by fwd(r), is the number of distinct accesses from the current position to the next access of r in the request sequence. For example, suppose that the current cache configuration is {r1 , r2 , r3 }, and the remaining request sequence (that has not be served) is r2 , r4 , r5 , r4 , r3 , r1 . Then we have fwd(r1 ) = 4, fwd(r2 ) = 0, and fwd(r3 ) = 3. We claim that MIN-d's total number of misses is small, if d is relatively small compared to the cache size. Actually, the extra number of misses for MIN-d k -1 can be at most n  ln k- d-1 , where k is the cache size, n is the number of total requests. Bound of Miss Count for MIN-d. In what follows, we fix a request sequence S of length n. For simplicity of exposition, we assume that the ith request of S occurs at time i. Let S [i, j ] be the subsequence of S consisting of the requests at time from i to j (inclusive). Given a set H and two elements x, y , the notation H - x + y refers to the set (H \ {x})  {y }. Definition 4. Let seq(t, b) be the first occurrence of a block b in the sequence S after time t; if b is not requested in S after time t then let seq(t, b) = . For example, if S = {a, b, a, d, c, b}, then seq(4, b) = 6 and seq(3, a) = . Definition 5. When S is being served by a given cache C , the cache configuration of C changes only when a miss occurs. Let curi (C ) be the time when the ith miss occurs (namely, if the rth request of S incurs the ith miss, then curi (C ) = r). Let cfgi (C ) be the cache configuration of C after i misses have occurred. For example, cfg0 (C ) is the initial cache configuration of C . Let HSi (C ) = {seq(curi (C ), b) =  | b  / cfgi (C )}. In words, if a block b is not in the cache configuration cfgi (C ) and if b is requested after curi (C ), then the first occurrence of b after curi (C ) is in HSi (C ). Note that the next cache miss after curi (C ), namely curi+1 (C ), would occur at the earliest time in HSi (C ).

378

S. Liang et al.

Let H1 and H2 be two sets of positive integers, we write H2  H1 if |H2  [1, x]|  |H1  [1, x]|, for any x  1. For example, {1, 2, 4, 5, 7}  {2, 3, 5, 6}. Claim 1. (i) Let H1 and H2 be the (nonempty) set after removing the smallest element from H1 and H2 , respectively. If H2  H1 then H2  H1 . (ii) Let H1 = H1 + h1 and H2 = H2 + h2 . If H2  H1 and h2  h1 , then H2  H1 . Lemma 1. Given a request sequence S , let C1 and C2 be two caches such that |C1 | = d + 1 + |C2 | and cfg0 (C2 )  cfg0 (C1 ). If MIN-d is used for C1 and MIN is used for C2 , then curr (C2 )  curr (C1 ) and HSr (C2 )  HSr (C1 ), for each r  0 (satisfying HSr (C1 ) = ). Proof. By induction on r. When r = 0, it is easy to verify that the claim holds. Assume the claim holds for r = i. Next we show that the claim holds for r = i +1. Let z1 = curi+1 (C1 ) and z2 = curi+1 (C2 ). By definition, z1 and z2 are the earliest times in HSi (C1 ) and HSi (C1 ), respectively. This immediately implies z2  z1 , since HSi (C2 )  HSi (C1 ). It remains to prove HSi+1 (C2 )  HSi+1 (C1 ).
C1 with MIN-d

z1
C2 with MIN

t1 t2

z2

Fig. 2. Here, z1 = curi+1 (C1 ) is the time when the (i + 1)th miss occurs in C1 . At time z1 , the block h1 is fetched into C1 and v1 is the victim block. The time t1 is the first occurrence of v1 (in S ) after z1 . The numbers z2 and t2 are defined similarly on C2 . Note that z1  z2 , but t1 may be smaller than t2 .

Let h1 be the block being requested (in S ) at time z1 , and v1 be the victim block (in C1 ) replaced by MIN-d at the same time. Let h2 be the block being requested at time z2 , and v2 be the victim block (in C2 ) replaced by MIN at the same time. Let t1 = seq(z1 , v1 ) and t2 = seq(z2 , v2 ). See Figure 2. For simplicity, we focus on the cases when t1 =  and t2 = , and omit the other cases (when t1 =  or t2 = ) since they are similar. Consider the difference between HSi (C1 ) and HSi+1 (C1 ). It is easy to see that z1 is the earliest time in HSi (C1 ), and it is not in HSi+1 (C1 ). Also note that t1 is in HSi+1 (C1 ) but not in HSi (C1 ). All other elements in HSi (C1 ) remains unchanged in HSi+1 (C1 ). Therefore, it holds that HSi+1 (C1 ) = HSi (C1 ) - z1 + t1 . Similarly, we have HSi+1 (C2 ) = HSi (C1 ) - z2 + t2 . (2) (1)

Cost-Aware Caching Algorithms for Distributed Storage Servers

379

1. t1  t2 . By Claim 1 (i), we have HSi (C1 ) - z1  HSi (C2 ) - z2 , since z1 and z2 are the earliest times in HSi (C1 ) and HSi (C1 ), respectively. Now, by Claim 1 (ii), it is easy to verify HSi+1 (C1 )  HSi+1 (C2 ), since t2  t1 . 2. t1 < t2 . We need to show that |HSi+1 (C2 )  [1, x]|  |HSi+1 (C1 )  [1, x]|, for any integer x  1. (i) x  z1 . We have HSi+1 (C1 )  [1, x] =  (recall that z1 is the earliest time in HSi (C1 ), which implies that the earliest time in HSi+1 (C1 ) is larger than z1 ). The claim trivially follows. (ii) z1 < x < t1 . Note that z2  z1 and t1 < t2 . By Eq. (1) and Eq. (2), we have |HSi+1 (C1 )  [1, x]| = |HSi (C1 )  [1, x]| - 1 and |HSi+1 (C2 )  [1, x]| = |HSi (C2 )  [1, x]| - 1. The claim immediately follows, by the induction hypothesis |HSi (C1 )  [1, x]|  |HSi (C2 )  [1, x]|. (iii) x  t1 . Note that HSi+1 (C1 )  [1, z1 ] =  and HSi+1 (C2 )  [1, z2 ] = , by similar arguments to (i). As such, it suffices to prove |HSi+1 (C1 )  [z1 + 1, x]|  |HSi+1 (C2 )  [z2 + 1, x]| . (3)

Let B1 be the set of distinct blocks in S [z1 + 1, x], and R1  B1 be the set of distinct blocks in S [z1 + 1, x] that are in cfgi+1 (C1 ). Similarly, let B2 be the set of distinct blocks in S [z2 + 1, x], and R2  B2 be the set of distinct blocks in S [z2 + 1, x] that are in cfgi+1 (C2 ). Now, notice that the LHS of Eq. (3) is the number of distinct blocks in S [z1 + 1, x] that are not in cfgi+1 (C1 ), which is equal to |B1 |-|R1 |, and the RHS of Eq. (3) is the number of distinct blocks in S [z2 + 1, x] that are not in cfgi+1 (C2 ), which is equal to |B2 | - |R2 |. Therefore, we need to prove that |B1 | - |R1 |  |B2 | - |R2 | . By the MIN-d algorithm, v1 is one of the d +1 furthest block to be requested at time z1 . Therefore, |R1 |  |C1 | - (d + 1) = |C2 |. On the other hand, |R2 |  |C2 |. It follows that |R1 |  |R2 |. Furthermore, by the definition, B1  B2 , since z1  z2 . This implies that |B1 |  |B2 |. It thus follows that |B1 | - |R1 |  |B2 | - |R2 |, as required. The following corollary is straightforward: Corollary 1. Let S be a request sequence, C1 and C2 be two caches such at |C1 | = |C2 | + d +1 and cfg0 (C1 )  cfg0 (C2 ). It holds that tf (MIN-d, cfg(C1 ), S ) <= tf (MIN, cfg(C2 ), S ). Due to the limit of space for presentation, we omit the proofs of the following lemma and theorem. Details can be found in [9]. Lemma 2. Let S be a given request sequence and C1 and C2 be two caches such that |C1 | = |C2 | + 1 and cfg(C1 )  cfg(C2 ). The number of fetches by MIN on C2 is at most n/ |C2 | larger than the number of fetches by MIN on C1 . That is, tf (MIN, cfg(C2 ), S )  tf (MIN, cfg(C1 ), S ) + n/ |C2 |. The following theorem follows from Corollary 1 and Lemma 2.

380

S. Liang et al. Input: request sequence S and initial cache configuration cfg(C). for each request r in S do if r  / cfg(C) then (b) Let b  cfg(C) be the resident block having the smallest Cod value cost fwd(b) in current cache configuration cfg(C). If there is a tie, choose b as the one with the largest forward distance. Replace b and read block r . Update cfg(C) and the forward distances of resident cache blocks. Fig. 3. MIN-cod Algorithm

Theorem 1. Let S be a request sequence of length n and C be a cache of size k -1 k . We have tf (MIN-d, cfg(C), S )  tf (MIN, cfg(C), S ) + n ln k- d-1 , namely, the k -1 MIN-d algorithm performs at most n ln k-d-1 more fetches than MIN. 3.2 MIN-cod Algorithm The MIN-d algorithm takes block cost into consideration for replacement decisions without significantly increasing the number of misses. However, it is conservative in nature as the scope of the candidate victim blocks is small (d + 1 furthest blocks). In reality, it is possible that some blocks to be accessed recently are much cheaper than the d + 1 furthest blocks such that evicting those near blocks to keep those expensive blocks despite of more misses is still beneficial. Obviously, MIN-d cannot make efficient decisions in these cases, thus its performance is limited, especially when the cost differences among the blocks are large. Therefore, we propose an algorithm that more aggressively pursues an optimal trade-off between the number of evictions and block costs by considering every block as a potential replacement candidate. As described in Figure 3, the algorithm MIN-cod makes replacement decisions based on the ratio of the cost over forward distance (Cod) among all the resident blocks in cache. If two blocks have the same ratio, the block with a larger forward distance is chosen. Clearly, if a block has the minimal cost among all resident blocks and is the furthest block, MIN-cod will replace it upon a miss, which is necessary for OPT too. However, if one block has a smaller cost and a shorter forward distance than another block, then it is unclear which one is a better victim block to reduce the total cost. Note that the number of evictions for keeping a block is closely related with its forward distance. Assuming keeping a block is beneficial, then it must not be evicted before its next request, otherwise the sooner it is evicted the better so as to save space for other blocks. Since the space for this block is occupied from the current request to the block's next request, keeping a block can be viewed as effectively reducing the cache size by one during this period. Based on the reasoning of Lemma 2, it is not difficult to know that the upper bound of extra misses caused for keeping this block in comparison to keeping a nearer block is roughly in proportion to its forward distance. Therefore, the cost/f wd essentially represents the minimal average cost savings per extra miss. The Cod heuristic chooses to replace the furthest block that generates the smallest saving.

Cost-Aware Caching Algorithms for Distributed Storage Servers

381

The Running Times of MIN-d and MIN-cod. A naive implementation of MIN-d and MIN-cod, on each request, scans the resident blocks in cache to find the victim and update fwd values. Therefore, the total execution times of both algorithms are O(nk ). There are two observations that can lead to a faster implementation for MIN-d, which uses only O(n log k ) time. First, MIN-d only requires to maintain the relative forward distances among the blocks to choose victims from. Second, after serving a new request, the cache configuration changes by only one element, and at most one resident cache block changes its relative forward distance. Therefore, if we use a priority queue to maintain the (relative) forward distances of resident blocks, we only need log k time for processing each request, resulting a total running time of O(n log k ). In real systems, the number of different fetch costs of blocks is relatively small, because only a limited number of different storage devices and levels exist in a system. Therefore, by keeping the resident blocks in binary trees of different costs, MIN-cod only needs to compare the blocks of the largest (relative) forward distance within each tree to find the right victim, whose total execution steps are in proportion to the number of different trees, thus can be considered as O(1). Since the maintenance of each binary tree needs O(log k ) time, the overall running time is bounded by O(n log k ), which is much faster than OPT. 3.3 An On-Line Algorithm HD-cod

The off-line algorithms assume complete knowledge of future requests, which is not always realistic in practice. In this section, we present HD-cod, an on-line algorithm based on MIN-cod. In on-line algorithms, we can only estimate the forward distance of a resident block. To this end, we use the recency of a resident cache block b as the estimated forward distance of b. (Recency is a concept borrowed from the well-known LRU replacement algorithm.) More specifically, the recency of b is the difference between its current request sequence number and the request sequence number of the last request of b. It is widely recognized that the LRU replacement algorithm, which estimates the forward distance of a block by its recency, works well for most workloads with strong temporal locality. However, it performs poorly for workloads with weak locality such as those with looping or random access patterns, where a recent access of a block does not indicate its re-access is near. These observations suggest that different considerations of forward distance can be used when evaluating Cod value of each block for the choice of replacement victims. In HD-cod, we use fcost wd to evaluate each block, where  is a workload dependent parameter in [0, 1]. For workloads with LRU-like temporal locality,  approaches 1 because the forward distance estimation is accurate, so that the Cod heuristic is appropriate. For non-LRU-like workload,  approaches 0 because the estimation of forward distance is inaccurate and the forward distance becomes less relevant, so that the replacement decision can be more dependent on the cost.

382

S. Liang et al.

To determine the workload type, HD-cod maintains an LRU queue (ordered by recency) for all the resident blocks. It divides the queue into multiple contiguous regions of a fixed size, which is a system run-time parameter and usually small. HD-cod traces the hit count in each recency region to generate the hit density curve of the workloads, so that  can be set dynamically based on the locality feature. Therefore the algorithm is called Hit Density(HD)-cod. On each replacement decision, HD-cod walks through the regions to calculate . (More details can be found in [9]). Since HD-cod maintains each queue using LRU order whose overhead is very small, its time complexity is O(n).

4

Evaluation

Methodology: We evaluate our proposed algorithms through trace-driven simulation. We compare the performance of our proposed algorithms with the optimal off-line algorithm OPT, representative non-cost-aware algorithms including MIN, Least Recently Used (LRU), Most Recently Used (MRU), Most Frequently Used (LFU) as well as cost-aware algorithms including Landlord and Minimal Cost First (MCF). Landlord is an optimal on-line cost-aware caching algorithm [10,6]. Upon each replacement, it chooses the block with the minimal residual cost as victim, and decreases each resident block's residual cost by this minimal value. Then upon each hit, the block's residual cost is updated by a value that is between current value and its original cost. It is proved in [6] that Landlord is a k -competitive algorithm, hence an optimal on-line replacement algorithm. It is also a generalization of the GreedyDual algorithm [11], which is studied in WWW-proxy cache management. The traces used in our experiments are production storage I/O traces from Storage Performance Council (SPC) [5] ­ a vendor-neutral standards body. They include both OLTP application I/O and search engine I/O. The OLTP traces are with strong temporal locality, i.e. repeated accesses to the same block, if any, are usually separated by a small (compared with cache size in blocks) count of accesses to other blocks. And the OLTP traces also include a significant portions of concurrent sequential accesses due to both the nature of server workloads and OLTP itself. The search engine traces comprise mostly random accesses, which are mostly non-sequential accesses with weak temporal locality, i.e. repeated accesses to the same block, if any, are usually separated by a large (compared with cache size in blocks) count of accesses to other blocks. Due to the resourceintensive nature of the OPT algorithm, which makes replaying the complete trace computationally intractable on our system, we split an entire trace into smaller traces by the ASU (Application Specific Unit) field of each request, so that logically related requests are grouped in the same trace file. We randomly generate the cost for each block based on two cost distributions. One cost distribution spans a wide range with differences as large as 70,000 times, the other spans a small range with differences at most 3 times. Specific values used and their distributions can be found in [9].

Cost-Aware Caching Algorithms for Distributed Storage Servers

383

9 8 Performance Ratio over OPT 7 6 5 4 3 2 1 0 OLTP1 OLTP2

Performance Ratio over OPT

LRU LFU MRU MIN MCF LANDLORD MIN-d MIN-cod

7 6 5 4 3 2 1 0

16.22

24.32

LRU LFU MRU MIN MCF LANDLORD MIN-d MIN-cod

OLTP3

OLTP1

OLTP2

OLTP3

(a)Small Cost Distribution

(b)Large Cost Distribution

Fig. 4. Total cost comparison of three workloads between OPT and different algorithms

Impact of Cost-Aware Replacement: We compare OPT with both existing and our proposed replacement algorithms to evaluate the impact of weighted cache on replacement performance. In this experiment, we set one eighth of the working set size as the cache size. Figure 4 shows the performance ratio of various algorithms over OPT for three small workloads using two different cost distributions. Overall, a significant performance degradation is observed using non-cost-aware replacement algorithms. For example, the optimal non-cost-aware algorithm MIN is 182% worse than OPT for OLTP2 using large cost distribution, so is LRU 537% worse. In contrast, the cost-aware algorithms including Landlord, MIN-d, and MIN-cod perform better almost in all these cases. The only exception is for the small cost distribution, MIN performs closer to OPT than Landlord, since in these scenarios, miss count is more important in the trade-off for overall performance. The results also show that the extent of the performance degradation is related with the workload itself. As we can see, OLTP2 is much more sensitive to the cost-awareness of the algorithms than the other two, because the other two traces have very few reused blocks. Miss Rate of MIN-d: We measure the miss rate of both OLTP and WebSearch workloads. The OLTP workload has a working set of around 300K blocks, while the WebSearch workload has a working set of around 480K . The results show that the miss rate does not increase noticeably until d is larger than 6% of the cache size. Based on this empirically result, in the following experiments, we set d to be 1/16 of the cache size. Off-line Algorithm Results: Figure 5 shows the experiment results comparing off-line algorithms (MIN-d and MIN-cod) with existing off-line algorithm MIN. We also include the comparison with on-line algorithms: MCF and Landlord for two reasons: a) MIN and MCF represent two ends on the trade-off between miss count and cost. MIN considers only misses in replacement decisions, while MCF considers only cost; b)since Landlord is the state-of-the-art cost aware replacement algorithm, the inclusion of it gives us an idea on the benefit of complete request knowledge to variable-cost cache replacement.

384
60

S. Liang et al.
600 550 500 Total Cost 450 400 350 300 250
10 50% 25% 12.50% 6.25% Percentage of Working Set
20 10 70 60 50 Total Cost 40 30

Millions

50

MIN MCF LANDLORD MIN-d MIN-cod

MIN MCF LANDLORD MIN-d MIN-cod

Millions

Billions

MIN MCF LANDLORD MIN-d MIN-cod

Total Cost

40

30

20

200 50% 25% 12.50% 6.25% Percentage of Working Set

50%

25%

12.50%

6.25%

(a) OLTP4-small
Billions

(b) OLTP4-large
Millions MIN MCF LANDLORD MIN-d MIN-cod

(c) OLTP5-small
Billions 900 MIN MCF LANDLORD MIN-d MIN-cod

Percentage of Working Set

800 700 600

MIN MCF LANDLORD MIN-d MIN-cod
Total Cost

70

60

800

Total Cost

500 400 300 200 50% 25% 12.50% 6.25% Percentage of Working Set

40

Total Cost
50% 25% 12.50% 6.25%

50

700

600

30

500

20 Percentage of Working Set

400 50% 25% 12.50% 6.25% Percentage of Working Set

(e) OLTP5-large
Millions 300 250 200 Total Cost 150 100 50 0 50% 25% MIN MCF LANDLORD MIN-d MIN-cod

(f) WebSearch1-small
Trillions 3 MIN MCF LANDLORD MIN-d MIN-cod

(g) WebSearch1-large

2.5

Total Cost

2

1.5

1

0.5
12.50% 6.25%

50%

25%

12.50%

6.25%

(h) WebSearch2-small

Percentage of Working Set

(i) WebSearch2-large

Percentage of Working Set

Fig. 5. Comparison of off-line cost aware algorithms using different cache sizes

Overall, MIN-cod consistently performs the best for all workloads. It reduces up to 62% of the total cost of MIN and up to 50% of Landlord. Other algorithms perform differently depending on the workloads. For example, as expected, MINd performs close to MIN-cod for the small cost distribution; however it is significantly worse than MIN-cod for the large distribution due to its scope limitation of victim candidates. Compared with Landlord, MCF performs poorly for the OLTP workloads, while it performs better for the Websearch workloads. Using the lower bound measured, it also shows that when the cache size is 50% of the workload, in seven out of the eight cases MIN-cod performs the same as OPT, while for one of the WebSearch workloads, MIN-cod has a total cost 20% larger than the lower bound of OPT. In all cases, using the bound reported, we can guarantee that MIN-cod's performance is at most four times of optimal. The above results show that the Cod heuristic works well with all the workloads tested. Since MIN-cod balances the cost of evicted blocks and the number of misses the replacement decision can cause, it is effective for a cost-aware replacement algorithm. On-line Algorithm Results: Figure 6 compares HD-cod with online algorithms: LRU, LFU, MRU, MCF, and Landlord. Overall, HD-cod performs very close to

Cost-Aware Caching Algorithms for Distributed Storage Servers
Millions
Billions

385

70 60

Total Cost

Total Cost

50 40 30 20 10

700 600 500 400 300 200

Total Cost

LRU LFU MRU MCF LANDLORD HD-cod

1000 900 800

LRU LFU MRU MCF LANDLORD HD-cod

Millions

80

1100

80 70 60 50 40 30 20 10 0 LRU LFU MRU MCF LANDLORD HD-cod 50% 25% 12.50% 6.25%

50%

25%

12.50%

6.25%

50%

25%

12.50%

6.25%

(a) OLTP4-small
Billions

Percentage of Working Set

(b) OLTP4-large
Millions
Billions

Percentage of Working Set

(c) OLTP5-small
1200 LRU LFU MRU MCF LANDLORD HD-cod

Percentage of Working Set

2800 2400 2000

Total Cost

1600 1200 800 400 0

Total Cost

50

Total Cost

LRU LFU MRU MCF LANDLORD HD-cod

70

60

LRU LFU MRU MCF LANDLORD HD-cod

1000

800

600

40

400

30
50% 25% 12.50% 6.25%

200

50%

25%

12.50%

6.25%

50%

25%

12.50%

6.25%

(e) OLTP5-large
Millions 300 LRU LFU MRU MCF LANDLORD HD-cod

Percentage of Working Set

(f) WebSearch1-small
Trillions 5 LRU LFU MRU MCF LANDLORD HD-cod

Percentage of Working Set

(g) WebSearch1-large

Percentage of Working Set

250

4

Total Cost

150

Total Cost
25% 12.50% 6.25%

200

3

2

100

1

50 50% Percentage of Working Set

0 50% 25% 12.50% 6.25% Percentage of Working Set

(h) WebSearch2-small

(i) WebSearch2-large

Fig. 6. Comparison of on-line cost aware algorithms using different cache sizes

the best algorithm in each test scenario. Specifically, for the OLTP workload, HD-cod performs comparably as Landlord, yet reduces up to 68% of the total cost of MCF; for the WebSearch workload, HD-cod performs similarly as MCF, yet reduces up to 15% of the total cost of Landlord. The above results show that for on-line cost-aware replacement algorithms where the forward distance of a block is not known, the balance between the two metrics (cost and forward distance) needs to be conducted adaptively based on the workloads. Since the Landlord algorithm only considers future accesses as with LRU-like locality, it does not perform well for workloads with the nonLRU-like locality. Since HD-cod detects workload characteristics by tracing the hit density, it adapts itself to behave more like MCF when temporal locality is weak and to behave more like Landlord when temporal locality is strong. Therefore, HD-cod performs close to the best algorithm in all test cases.

5

Related Work

Previous work on cost-aware cache replacement includes both theoretical results and system studies. Young [12] studied the weighted caching problem and proposed an on-line algorithm ­ GreedyDual. Cao et al. [11] studied WWW-proxy

386

S. Liang et al.

caching and proposed GreedyDual-size to incorporate file size into replacement decision. Landlord [6] is a generalization of both GreedyDual and GreedyDualsize. Chrobak et al. [7] proposed on-line algorithms: Rotate and Balance. All these algorithms are proved to be k -competitive, thus theoretically optimal. GreedyDual-size has also been demonstrated to be effective experimentally using simulation experiments on Web proxy traces. Although competitive analysis provides a bound for approximation algorithms, the bound is usually too loose to be attractive. For example, k in the above results is the cache size in blocks, which is at the magnitude of millions with current technology; and it keeps increasing as the technology evolves. Although experimental results are provided to demonstrate the effective of GreedyDual-size [3], it focuses on the variance of document size rather than the access cost of uniform-sized block in storage system. Therefore, they compare with popular replacement algorithms such as LRU, LFU and other size-aware algorithms. Forney et al. [10] studied partitionbased cache management scheme for heterogeneous storage devices and proposed to use equal device wait time as a metric for dynamic cache allocation, which is orthogonal to our studies. Compared with previous work, our work demonstrates that the upper-bound of performance degradation of any replacement algorithm over OPT can be determined by keeping track of the costs of evicted blocks, which is more practical and meaningful than the k -competitive result. We also point out that the key design issue for efficient cost-aware replacement algorithms is to make an effective trade-off between victim blocks' cost and the total number of evictions. Our Cod heuristic which is based on the principle is demonstrated to outperform previous cost-aware and cost-unaware algorithms in real storage server traces simulations.

6

Conclusions

We have proposed both off-line and on-line algorithms that have performance comparable to the optimal replacement algorithm (OPT) measured by the total cost, yet are much faster to run in practice. The algorithms' design is guided by the following findings of ours. The performance of any replacement algorithm is deviated from OPT by at most the cost of evicted blocks, such that the key to design cost-aware replacement algorithm is to trade-off the number of evictions and the cost of victim blocks. Our work provides analytical bases for buffer cache management in distributed storage systems. We will further understand the implications of our study in our experimental system research.

References
1. Jiang, S., Zhang, X.: Making LRU friendly to weak locality workloads: A novel replacement algorithm to improve buffer cache performance. IEEE Trans. on Comp. 54(8), 939­952 (2005) 2. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms. MIT Press, Cambridge (2001)

Cost-Aware Caching Algorithms for Distributed Storage Servers

387

3. Goldberg, A.V.: An efficient implementation of a scaling minimum-cost flow algorithm. J. Algorithms 22(1), 1­29 (1997) 4. Belady, L.: A study of replacement algorithms for virtual storage computers. IBM Sys. J. 5, 78­101 (1966) 5. Storage Performance Council: SPC I/O Traces. Available at http://www.storageperformance.org 6. Young, N.E.: On-line file caching. In: Proc. 9th Annu. ACM-SIAM sympos. Discrete algorithms, pp. 82­86. ACM Press, New York (1998) 7. Chrobak, M., Karloff, H., Payne, T., Vishwanathan, S.: New results on server problems. SIAM J. Discret. Math. 4(2), 172­181 (1991) 8. Patterson, D.A., Hennessy, J.L.: Computer architecture: a quantitative approach. Morgan Kaufmann, San Francisco (1990) 9. Liang, S., Zhang, X., Jiang, S.: Cost-aware caching algorithms for distributed storage servers. Technical Report OSU-CISRC-3/07-TR23 (2007) 10. Forney, B., Arpaci-Dusseau, A.C., Arpaci-Dusseau, R.H.: Storage-aware caching: Revisiting caching for heterogeneous storage systems. In: Proc. 1st USENIX Sympos. File and Storage Tech., pp. 61­74 (2002) 11. Cao, P., Irani, S.: Cost-aware WWW proxy caching algorithms. In: Proc. 1997 Usenix Sympos. on Internet Tech. Sys. (1997) 12. Young, N.E.: The k-server dual and loose competitiveness for paging. Algorithmica 11(6), 525­541 (1994)

Push-to-Pull Peer-to-Peer Live Streaming
Thomas Locher, Remo Meier, Stefan Schmid, and Roger Wattenhofer
Computer Engineering and Networks Laboratory (TIK), ETH Zurich, 8092 Zurich, Switzerland {lochert,remmeier,schmiste,wattenhofer}@tik.ee.ethz.ch

Abstract. In contrast to peer-to-peer file sharing, live streaming based on peer-to-peer technology is still awaiting its breakthrough. This may be due to the additional challenges live streaming faces, e.g., the need to meet real-time playback deadlines, or the increased demands on robustness under churn. This paper presents and evaluates novel neighbor selection and data distribution schemes for peer-to-peer live streaming. Concretely, in order to distribute data efficiently and with minimal delay, our algorithms combine low-latency push operations along a structured overlay with the flexibility of pull operations. The protocols ensure that all peers are able to obtain the required data blocks of a live stream in time, and that due to the loop-free dissemination paths, the overhead is low.

1

Introduction

Currently, we are witnessing an explosion of online video content provided on websites such as YouTube 1 . It is likely that in the near future, the Internet will also revolutionize television. Due to its scalability, peer-to-peer (p2p) technology is an appealing paradigm for providing live TV broadcasts over the Internet. Live p2p streaming is not only an active field of research, but there are already commercial products emerging, e.g., JumpTV 2 , PPLive 3 , SopCast 4 , among others, which provide television to thousands of viewers. Live streaming faces several challenges that are not encountered in other p2p applications such as file sharing. The streaming content is required to be received with respect to hard real-time constraints, and data blocks that are not obtained in time are dropped, resulting in a reduced playback quality. Additionally, a live broadcast ought to be received by all users simultaneously and with minimal delay. Moreover, as video streams often already demand a high transmission rate themselves, it is of paramount importance that the overhead caused by redundant transmissions of the protocol itself be minimized. Yet another crucial property of any successful live streaming system is its robustness to peer dynamics: It is likely that peers join and leave the system continuously and concurrently, called churn.
1 2 3 4

See See See See

http://www.youtube.com/ http://www.jumptv.com/ http://www.pplive.com/ http://www.sopcast.com/

A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 388­402, 2007. c Springer-Verlag Berlin Heidelberg 2007

Push-to-Pull Peer-to-Peer Live Streaming

389

While there already exist several solutions both in literature and in practice, many of these systems fail to take all the aforementioned criteria into account. This may partly be explained by the fact that some of the optimization goals are inherently antagonistic. For example, a low delay can be achieved by having each peer immediately forward all incoming data blocks to its neighboring peers (pushing ). Unfortunately, such a naive solution results in a significant overhead, as a peer may receive the same block repeatedly from different neighbors. Alternatively, peers could request missing blocks explicitly. This scheme is referred to as pulling since all peers have to initiate the transmission of data blocks towards themselves. While a pull-based approach circumvents the problem of receiving duplicates, it comes at the cost of intolerable latencies, as notifications and requests have to be sent back and forth. Hence, there is a trade-off between overhead and efficiency. This paper presents and evaluates novel data distribution mechanisms which combine the benefits of pull-based approaches with the advantages of push-based approaches. In our mechanism, a fresh data block is quickly pushed to a welldefined set of peers. Due to the structured, prefix-based neighbor selection policy, this can be achieved without any redundant transmissions. The remaining peers which have not received the blocks in this initial phase use the pull mechanism to distribute the new data block amongst themselves. We have implemented the algorithms presented in this paper in our own peerto-peer live streaming system Pulsar.5 Apart from real-world tests such as the broadcast of the IPTPS 2007 conference, we have performed extensive simulations of our protocol. According to our emulations with up to 100,000 peers (using the real code base), the system scales well as the network topology has a low diameter and guarantees small round trip times due to the latency-aware choice of neighbors. The proposed push-to-pull data dissemination policy is efficient: The time required from the moment a fresh data block becomes available at the source until it has reached virtually all peers is around 1,500 ms in a overlay consisting of 10,000 peers, and having 100,000 peers instead of 10,000 incurs a moderate additional delay of less than 250 ms. Finally, the Pulsar system tolerates a large fraction of peers crashing simultaneously without entailing any underflows at the remaining peers, i.e., all packets arrive before their playback deadline. This indicates that our protocols also perform well in dynamic environments. The remainder of this paper is organized as follows. After reviewing related work in Section 2, the design of our protocol is presented in Section 3, followed by its evaluation in Section 4. In Section 5, the paper concludes.

2

Related Work

Although it has been expected that one-to-many broadcast would be offered through IP multicast since the early 1990s, it is not used in practice at all due to its limited support by the Internet Service Providers (ISPs). An attractive
5

See http://www.getpulsar.com/

390

T. Locher et al.

alternative to native IP multicast is to use a peer-to-peer network overlay built on the application layer to distribute the content. Existing peer-to-peer approaches are mainly categorized according to the topology maintained among the peers or, equivalently, the neighbor selection algorithms the peers employ. Simple multicast systems are based on overlay trees [2,4,13]. Trees have the advantage that the topology is simple, once it is constructed the overhead is small in a static setting, and there are no duplicates as every peer receives its data blocks from its sole parent. However, there are rather serious drawbacks which render such systems inefficient. For example, resources are wasted as all the leaves of such a tree do not contribute anything to the system. Moreover, inner nodes having two or more children need to upload at least at twice the bitrate of the stream. This means that high-quality video streams cannot be transmitted unless one can guarantee that all inner nodes have a lot of spare upload capacity. Finally, the fragile tree structure is not resilient to any kind of node failures or churn. In order to overcome these problems, systems have been proposed to split the content of the stream into several disjoint stripes and disseminate this information along multiple disjoint trees. SplitStream [1] is a prominent example which uses multiple description coding (MDC) [3] to split the stream into different stripes in order to distribute them on several trees. Multiple description coding allows for the reconstruction of the original stream using any subset of the stripes. As each peer is also required to be an inner node in one of the trees, this approach solves the single tree's problem of having a large fraction of free-riding leaf peers. The CoopNet [7] approach is similar in that it also uses MDC and multiple trees; however, its goal is merely to complement the traditional client-server model as opposed to completely replace it. In this protocol, the server handles all the join requests and centrally manages all trees which limits the system's scalability. MDC is still an active research effort and no implementations for practical use are available. In addition, the overhead of multiple description coding harms the system's efficiency which may raise concerns whether multiple description coding is currently suitable for this kind of application. While maintaining several trees improves the robustness of a system, each tree can break individually, and the overhead potentially increases as more trees have to be repaired continuously (and concurrently). Various systems using other approaches to cope with the shortcomings of tree-based topologies have also been presented. The Bullet [6] system uses a mesh on top of an arbitrary tree overlay in order to increase robustness. The additional links introduced by the mesh increase robustness by reducing the dependency of peers on their parents. The stream is split into disjoint blocks and distributed within the tree. Only as many blocks are sent to children as bandwidth is available, and missing blocks are then localized and retrieved using the mesh. However, the encoding of blocks, the duplicates, the requests for missing blocks, and the tree maintenance entail a substantial overhead in Bullet. ChunkySpread [12] strives to redeem the shortcomings of tree-based topologies by providing more efficient protocols to build and repair trees. By adding a

Push-to-Pull Peer-to-Peer Live Streaming

391

"weak" tit-for-tat model and locality awareness, additional important aspects of peer-to-peer live streaming are considered. The overhead of any tree-based protocol is generally large as the trees have to be repaired and the topology maintained. This is particularly true if there is a lot of churn in the network. Another disadvantage of trees is its lack of control over selfish peers: It is difficult to enforce that peers actually forward the data blocks to their designated children. Due to these inherent problems, a lot of research has also focused on tree-less protocols. Since a rigidly structured overlay requires permanent maintenance, care has to be taken not to burden the individual peers. Therefore, unstructured overlays have been favored over structured overlays, and various protocols based on unstructured overlays have been proposed, e.g., CoolStreaming/DONet [15], Chainsaw [8] and GridMedia [14], all published in 2005. CoolStreaming/DONet makes a strong case for a data-centric design of the overlay, which means that the availability of data at certain nodes must steer the content dissemination, in contrast to having the predefined overlay dictate the data flow. Chainsaw and Gridmedia also follow this paradigm and mainly differ in the number of stored links to other peers, block sizes, buffer lengths, etc.--generally, parameters which have an impact on the overlay's robustness and overhead. Typically, in unstructured overlays, peers have to notify neighboring peers about available blocks of data, and peers that are interested in obtaining these blocks must explicitly request them before any data is exchanged, because there is no structure in the network that could be used to disseminate data. Note that this scheme has the disadvantage that notifying peers and subsequently requesting data blocks potentially results in long delays before any data is exchanged. Our approach differs from all these protocols in that it uses a structured overlay, based on a prefix-routing neighbor selection policy [9,10]. This policy guarantees a logarithmic diameter, robustness to massive crash failures and churn, and it also ensures that the entire network remains connected. At the same time, the protocol uses the flexibility of this neighbor selection scheme to take latency and bandwidth considerations into account when building up and maintaining the routing tables. Our mechanism further uses novel push algorithms tailored specifically for prefix-routing-based topologies to quickly disseminate the content to a fraction of all peers, thereby significantly reducing the delay experienced in other pull-based protocols. The benefits of push-to-pull strategies are wellknown in theory, e.g., in the context of efficient rumor spreading [5]. Hence, this push-to-pull-based technique possesses the advantages of the pull-based schemes and in addition has the efficiency of push-based algorithms.

3

Push-to-Pull Protocol

This section presents the design of our protocol for peer-to-peer live streaming. It is based on two concepts: First, the protocol defines the overlay structure, i.e., it specifies how peers are to select their neighboring peers. The overlay is inspired by the structured topologies of distributed hash tables (DHTs) which

392

T. Locher et al.

guarantee connectivity and a logarithmic diameter. The flexibility of the neighbor selection strategy is used to account for additional factors which influence performance, for instance, bandwidth requirements and latency constraints. The topology aims at being resilient to churn and massive correlated failures. Second, the protocol specifies how data is distributed in the overlay network. Concretely, the protocol advocates the data-driven streaming paradigm, and introduces a novel combination of fast pushing operations and robust pull operations. 3.1 Overlay and Neighbor Selection

The proposed overlay consists of an unstructured and a structured part. Initially, a peer is assigned a random set of neighbors by a network entry point. Over time, a refinement process takes place as peers learn about other peers from their neighbors and add them to their routing table depending on the following criteria: Since peers strive to maintain several connections to close-by peers, new neighbors are continuously accepted based on the latency measured to these peers. While truly random networks are known to have desirable properties, constraining the choice of neighbors to peers that are close-by may lead to clusters and consequently threaten the efficiency or even the connectivity of the overlay. Therefore, our protocol uses d-bit peer identifiers in order to build a DHT-like topology (of course, without the data storage semantics). These identifiers can be used for prefix-routing, as links to neighbors are stored for different shared prefix lengths. Let  denote the number of bits that can be fixed at a peer to route any message to an arbitrary destination. For i = {0, , 2, 3, . . .}, a peer chooses, if possible, 2 - 1 neighbors whose identifiers are equal in the i most significant bits and differ in the subsequent  bits by one of 2 - 1 possibilities. For random bit strings, this ensures an expected logarithmic network diameter and peer degree. Similarly to DHTs based on prefix-routing, our solution has the advantage over more rigid DHT structures such as Chord [11] that there is a large choice of neighbors for short prefixes, which means that an optimizing secondary criterion can be used to pick neighbors. For example, as the identifiers of roughly half of all peers start with 0, any of those peers can be used as the routing table entry for this prefix, while about one fourth of all peers are suitable for the prefix 00 etc. This freedom is used in our protocol to choose peers according to their latency (locality awareness), but also in order to construct different push mechanisms as described in the following section. 3.2 Pushing and Pulling Data

The prime objective of the pushing component is to quickly distribute a data block to a certain number of peers, in order to fuel the subsequent pull-based exchanges. As we have argued before, such a mechanism is needed due to the long delays of purely pull-based approaches; the pushing phase brings the data block into the vicinity of virtually all peers. In this section, various aspects of pushing data blocks to neighbors are discussed. In particular, we present two concrete algorithms where each of these

Push-to-Pull Peer-to-Peer Live Streaming

393

algorithms has its own merits. The first algorithm, denoted by ALG 1 , is simple, robust, and has a low overhead; it needs fewer neighbors per peer and deals better with heterogenous bandwidths. However, it cannot guarantee that the push mechanism reaches a considerable share of all peers and specific care has to be taken to make sure that no duplicates can occur. The second algorithm, ALG 2 , is more sophisticated: All the peers can be reached without the use of the pulling mechanism, and there are provably no retransmissions. Note that a loop-free transmission implies that data is distributed on induced spanning trees, which are generally not comparable to structures where the overlay graph consists of one or more trees which must be used to disseminate data. Our graph is still hypercubic, and, in accordance with the data-driven streaming paradigm, each packet can theoretically induce a different tree on which it is broadcast. Due to the simplicity and robustness of ALG 1 , it is better suited for dynamic environments and also in settings where peers may act selfishly. As we will show in Section 4, in order to boost the dissemination process it suffices to push fresh data blocks to a subset of all peers. This implies that the lack of guarantee that many peers can be reached using this push mechanism is not a severe limitation. Nevertheless, the ability of ALG 2 to efficiently push new blocks to practically all peers may be preferable in various scenarios. For example, there may be situations where one wants to precisely control the fraction of peers reached by the pushing operation only. In a more stable network or a network where incentives are of no concern, more peers should be reached by pushing blocks for efficiency reasons, so that only a small number of pulls are necessary to distribute the new block among the remainder of the peers. u In the following, let, for two peers u and v with identifiers bu 0 . . . bd-1 and v v u v th b0 . . . bd-1 , where bi and bi , denote the i bit of their respective identifiers, v u v (u, v ) = k if bu j = bj for all j  {0, . . . , k - 1} and bk = bk . Furthermore, let Nv be the set of all neighboring peers of v . We first present ALG 1 and discuss its properties. Let  again denote the number of bits that the prefix routing algorithm fixes at each hop. The source selects 2 peers from its routing table, if possible, such that the identifiers of any two peers differ in at least one bit of the first  bits. A new block is pushed to these peers along with the information that they must only forward the block to peers with which they share the first  bits of their identifiers. Recursively, upon receiving such a push message with the specified prefix length  that they must not modify, a recipient selects 2 peers that share the prefix of length  with itself and that differ in at least one bit between the ( + 1)st and the ( +  )th bit and so on. This straightforward approach to pushing on prefix-based overlays has an obvious shortcoming: Assume that  = 2 and that the source peer has the identifier consisting of only zeros. It will push the block, among others, to a peer whose identifier starts with 00 which will in turn forward the block to a peer whose identifier starts with 0000. This peer might then forward the block back to the source again, as the identifier of the source also starts with 000000. Such loops can occur on all paths. If a peer v pushes the block solely to all the 2 - 1 peers that differ in at least one bit from the identifier of v itself, there are no

394

T. Locher et al.

duplicates; however, this reduction would cut off entire branches of peers which could never benefit from the push mechanism. A viable solution to the duplicates problem is to include a list L of critical predecessors of the induced spanning tree. Only the peer identifiers having a prefix of length  +  in common are sent along. The push message any peer v receives contains the parent p in the induced spanning tree, the fixed prefix length  , and the list of critical predecessors L.6 The parent is potentially a critical peer for one of the children, and therefore it is added to the list L. Afterwards, using the local subroutine getChildren, the l  2 children are selected from the routing table for which it holds that they all share a prefix of length at least  with peer v itself, the identifiers of any two of those children differ in at least one of the following  bits, and they do not occur in the list L. In the next step, the lists Lj of critical predecessors are created for all children. Note that any critical predecessor pi is added to at most one list Lj , and only if it is still critical for this child vj , i.e., (vj , pi )   +  . The source v0 pushes data blocks containing the parameters p := v0 ,  :=  , and L :=  to its children. This push strategy ALG 1 is summarized in Algorithm 1. Algorithm 1. ALG 1 : push(p, , L) at peer v .
1: 2: 3: 4: 5: 6: 7: L := L  {p} {v1 , . . . , vl } := getChildren(v, , L) for all pi  L do j := arg maxj {1,...,l} (vj , pi ) if (vj , pi )   +  then Lj := Lj  {pi } fi od for j = 1, . . . l do send push(v,  + , Lj ) to vj od

It is easy to see that ALG 1 is indeed loop-free, and that the expected length  1 1 of the list L is bounded by j =2 (2 )j = 2 (2 -1) which is less than one entry. However, the worst-case length of the list is log(n)/ . Another shortcoming of this algorithm is that it is likely that not all peers can be reached, because once a peer is reached that only has connections to peers that are in the list L for a certain prefix, this branch of the tree is cut off. ALG 2 avoids these problems by modifying the topology and using a different routing scheme. For simplicity, we present the neighbor selection strategy and the push algorithm for the case  = 1. In order to use ALG 2 , the peers must store links to a totally different set of neighbors: A peer v with the identifier v v v v v v bv 0 . . . bd-1 stores links to peers whose identifiers start with b0 b1 . . . bi-1 bi bi+1 v v v v v and b0 b1 . . . bi-1 bi bi+1 for all i  {0, . . . , d - 2}. For example, the peer with the identifier 0000 has to maintain connections to peers whose identifiers start with the prefixes 10, 11, 010, 011, 0010, and 0011. Pseudo-code for the algorithm is given in Algorithm 2.
6

For simplicity, as the data contained in the push messages does not have any influence on the push procedures, it is omitted in our notation.

Push-to-Pull Peer-to-Peer Live Streaming

395

Algorithm 2. ALG 2 : push(, vc ) at peer v .
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: S := {v  Nv | (v , v )   + 1} v , v ) v ~S choose v1  S : (v1 , v )  (~ if v1 =  then send push( (v1 , v ), v ) to v1 fi if vc =  then choose v2  Nv : (v2 , vc ) =  + 1 if v2 =  then v2 := getNext(v ) from vc fi if v2 =  then send push( (v2 , vc ), vc ) to v2 fi else choose v2  Nv : (v2 , v ) =  if v2 =  then send push( + 1, vc ) to v2 fi fi

The parameters are again the length  of the prefix that is not to be modified, and at most one critical predecessor vc . If  = 1, any node v tries to forward the push message to two peers v1 and v2 . The procedure is called at the source v0 with arguments  := 0 and vc := , resulting in the two push messages push(1, v0 ) to v1 and push(1, ) to v2 . The peer v1 is chosen locally such that the prefix its identifier shares with the identifier of v is the shortest among all those whose shared prefix length is at least  + 1. This value (v1 , v ) and v itself are the parameters included in the push message to peer v1 , if such a peer exists. The second peer is chosen similarly, but with respect to vc and not v itself. If no suitable peer is found in the routing table, the peer vc is inquired for a candidate using the subroutine getNext which is described in Algorithm 3. Algorithm 3. getNext(vs ) at peer v
1: S := {v  Nv | (v , v ) > (vs , v )} v , v ) v ~S 2: choose vr  S : (vr , v )  (~ 3: send vr to vs

This step is required because node v cannot deduce from its routing table whether a peer v2 with the property (v2 , vc )   + 1 exists. In the special case when vc = , v2 is chosen locally, if possible, such that (v2 , v ) =  . In Figure 1, an example spanning tree resulting from the execution of ALG 2 is depicted. As mentioned earlier, ALG 2 has the property that, at least in a static setting where peers neither join nor leave the network, all peers can be reached. Due to churn, any real overlay can never be considered static. However, this static property implies that this pushing procedure is expected to reach a large number of peers even if some peers appear and disappear during the push phase. Theorem 3.1 In a static overlay, the push algorithm ALG 2 has the following properties: (a) It does not induce any duplicate messages (loop-free), and (b) all peers are reached (complete).

396

T. Locher et al.

v0 0000
(1,v0 ) (1,0 )

v1 0101
(2,v0 ) (2,v1 ) (2,v2 )

v2 1010
(2,0 )

v3 0010

v4 0110 v6 100 1

v7 1101

Fig. 1. The spanning tree induced by a push message initiated at peer v0 is shown. The fixed prefix is underlined at each peer, whereas prefixes in bold print indicate that the parent peer has been constrained to push the packet to peers with these prefixes.

Proof. Throughout the proof, we will use the fact that u, v, w : (u, v ) =  and (v, w) =  implies that (u, w) = min(,  ) which we will refer to as Fact (1). (a) Loop-free: If a peer v receives a push message  and forwards it to other peers which in turn forward the message and so on, let Cv () denote the set of peers that are reached recursively. We first show that if any peer v forwarding push messages  and  to two peers v and v , these peers will subsequently forward the message to disjoint sets of peers, i.e., Cv ( )  Cv ( ) = . Then, we will show that peers never send messages back to predecessors. Let v be the peer receiving the push message and let v denote the prefix length that peer v can no longer modify. As in the description of the algorithm, the two peers the message is forwarded to are v1 and v2 . Let further vp denote the peer that sent the message to v . In order to prove that disjoint sets are constructed, it suffices to show that (v, v1 )  v + 1 and (v, v2 ) = v at any peer v . The first inequality follows immediately from the algorithm. If vc =  then (v, v2 ) = v also follows by definition. Therefore we can assume in the following that vc = . If vp = vc we have that (vp , v2 )  v + 1, as v was chosen from S according to this criterion. It further holds that (v, vp ) = v because the parameter v that p sends to v in the message is precisely (v, vp ). According to Fact (1), we get that (v2 , v ) = v . Similarly, if vp = vc , it holds that (vc , v2 )  v + 1, due to the fact that either vp found peer v in its routing table, implying that (vc , v2 ) = v + 1, or the procedure getNext has been invoked which by definition means that (vc , v2 ) > v + 1. As p sends the value (v, vc ) to v , it holds at peer v that v = (v, vc ), again leading us to the conclusion that (v, v2 ) = v . This concludes the proof that the resulting peer sets are always disjoint. Since peers might forward push messages back to a predecessor, we cannot yet conclude that no duplicates are produced. Let v (0) ; v (1) ; . . . ; v (k) , where k  v , denote the path from peer v (0) := v back to the source v (k) . Note that the value  steadily increases downwards, implying that v(0) > v(1) > . . . > v(k) . Let us first assume that vc =  on the entire path. If vc = v (1) then it holds that (v, v (1) ) = v according to the algorithm. In the case vc = v (1) , then by

Push-to-Pull Peer-to-Peer Live Streaming

397

definition (v, vc ) = v and as (v (1) , vc ) = v(1) < v , we get that in all cases (v, v (1) )  v . Inductively, the same argument can be applied to the maximal prefix length between v (1) and v (2) which is bounded by v(1) etc. Using Fact (1), we have that (v, v (i) )  v for all 1  i  k . If for some i  {0, . . . , k - 1} a peer  is reached that received vc =  from v (i +1) , it holds according to the definition of ALG 2 that all other peers closer to the source on this path also received vc = . This entails that (v (k) , v (k-1) ) = 0, (v (k-1) , v (k-2) ) = 1 and so on, down to   (v (i ) , v (i +1) ) = k - i  v . Applying the same inductive argument as before, we can conclude that (v, v (j ) )  v also for all j > i if such an i exists. Since the first v bits are not changed at peer v when forwarding the message to other peers, it is impossible for v to send the push message to a predecessor as all predecessors' identifiers differ in at least one bit among the first v bits, which concludes the proof that no duplicates can occur and the resulting structure is a spanning tree. (b) Complete: It remains to be shown that all peers are reached using this procedure. Using (v, v1 )  v + 1 and (v, v2 ) = v at any peer v , it follows that, when forwarding push messages, the current prefix is extended with a 0 and a 1, and the value  is increased. Note that care has to be taken only if identifiers with certain prefixes do not exist. If no peer v1 such that (v1 , v ) =  + 1 exists, the next bit can be tried by choosing v1 such that (v1 , v ) =  + 2 and so on. Given that v1 is chosen among all peers in S such that the (v1 , v ) is minimal, it is guaranteed that no peer is left out. Similarly, if there is no peer v2 such that (v2 , vc ) = v , the next bit is tried by calling the function getNext at peer vc which chooses v2 the same way as peer v chooses v1 . This means that prefixes are only left out if no peer's identifier has this particular prefix and thus every peer can be reached. Observe that at any time, at most one predecessor is critical and has to be included in a push packet. A disadvantage of ALG 2 , compared to ALG 1 , is that peers have to maintain twice as many connections to other peers. Since all peers ideally communicate regularly with all their neighbors, it is best to keep the set of neighboring peers small. However, both algorithms are not sufficient to quickly disseminate data to all peers in dynamic environments such as the Internet. Due to the perpetual arrival and departure of peers, which results in inaccurate routing tables, only a certain fraction of all peers are effectively reached through pushing. Thus, a second mechanism has to be used where peers having received new data blocks notify their neighbors about the corresponding sequence numbers. A peer can then obtain data blocks it is interested in by explicitly requesting them from a neighbor (pull operation ). Hence, a data block is never forwarded twice to the same peer, and there are no redundant transmissions. The initial distribution of new data blocks through pushing ensures that almost every peer has at least one other peer in its vicinity that offers the missing data block.

398

T. Locher et al.

4

Evaluation

Our protocol has been evaluated in several respects. We have performed extensive emulations (simulations using the real code base of the Pulsar system in a simulated network) with up to 100,000 peers on a single Core2 Quad personal computer with 4GB of RAM. Our emulation results have also been confirmed in tests on PlanetLab. Finally, a real-world beta test has shown that the protocol manages to cope well with the peculiarities of the Internet and to distribute the content reliably. Due to space constraints, we only present results concerning the key concepts introduced in this paper, namely the neighbor selection and the push- and pull-based data dissemination policy. 4.1 Topology and Neighbor Selection

First, we have evaluated the properties of the streaming topology itself. We have streamed data over a network of 100 to 100,000 peers and counted the total number of hops taken by each data packet. Figure 2 shows that, as expected, the hypercubic structure induced by the neighbor selection results in a logarithmic network diameter. As described in Section 3, the flexibility of our topology allows for locality awareness, i.e., for the choice of close peers as neighbors. This indeed helps to reduce the round trip times significantly compared to a random neighbor selection strategy, as Figure 3 clearly suggests. Figure 3 depicts the number of neighbors that the average peer maintains for any given round trip time. In this emulation, peers are distributed uniformly on a square with a minimum delay of 10 ms and maximum delay of 200 ms which corresponds to the square's diagonal.

Fig. 2. Number of hops taken by each data packet to reach the destination peer. The network diameter scales logarithmically with the total number of peers.

Fig. 3. Effect of locality awareness with 10,000 peers: The average round trip times to all neighboring peers are significantly smaller in the network constructed using our protocol than in a network where neighbors are chosen at random

Push-to-Pull Peer-to-Peer Live Streaming

399

4.2

Push-to-Pull Data Distribution

Figure 4 compares the two push strategies ALG 1 and ALG 2 introduced in Section 3 with a pull-only strategy like the one adopted by Chainsaw. The figure shows that, compared to pull-only protocols, pushing considerably speeds up the distribution of new data blocks and thereby reduces the playback delay. Once a sufficient number of peers have received a block, the remainder of the peers can retrieve the fresh data block using the pull mechanism with a moderate additional overhead. It is evident from Figure 4 that ALG 1 is almost as fast as ALG 2 , although only about one third of all peers obtain the new data block through pushing, while almost all peers are reached using ALG 2 in this test.

Fig. 4. Time required until the push strategies ALG 1 , ALG 2 , and a pull-only strategy reach a given fraction of all peers in a network of 10,000 peers

Figure 5 depicts the percentage of all data packets received through pushing for both algorithms ALG 1 and ALG 2 for increasing network sizes. Independent of the chosen algorithm, less packets are received through pushing as the network grows. This decline is due to the increased chance of branches of the distribution trees being cut off, because of inaccurate routing tables, before a substantial number of peers is reached. As expected, the fraction of pushed packets decreases much more rapidly when ALG 1 is used. However, it is sufficient to reach only a fraction of all peers in the pushing phase, as the subsequent pull operations can be performed efficiently and with a small additional delay. Note that both pushing strategies greatly benefit from the locality awareness which not only decreases the chances of packet loss but also allows the use of short timeouts for acknowledgments. A second test studies the scalability of the ALG 1 pushing algorithm. Figure 6 indicates that the network scales well with the number peers, as exponentially more peers merely results in a linear increase of the delays. Moreover, all peers experience a delay of not more than 1.5 seconds.

400

T. Locher et al.

Fig. 5. As the network grows, less data is received in the pushing phase. The fraction of data obtained through pushing decreases considerably faster when algorithm ALG 1 is used.

Fig. 6. Given an exponential increase of the number of peers, the delays increase only linearly

Fig. 7. Effect of 50% simultaneous random crashes in a network of 5,000 peers. "One alive" shows the percentage of prefixes for which at least one connection is present, while "All alive" depicts the percentage of prefixes for which all connections are still alive. In both cases, already after 3 seconds, the peers are again fully connected.

4.3

Robustness to Churn

The high connectivity of our hypercubic network topology and the flexible choice of neighbors allows to build up and fix routing tables quickly. Several scenarios have been considered in which a large fraction of peers leaves simultaneously. It turns out that it is easy to maintain the topology and to recover even from such massive concurrent network changes. Figure 7 shows a network where a random set of 50% of the 5,000 peers leave simultaneously. A severe network failure is assumed where all the peers crash without notice (no "leave message"). For each prefix stored in the routing table, a peer maintains roughly 2 to 3 connections to other peers whose identifiers match the specific prefix. Immediately after the network failure, for approximately 80% of the stored prefixes, at least one connection to a peer that is still alive is retained. After roughly 3 seconds, the routing table is again almost completely repaired. The figure also depicts the percentage of prefixes for which all

Push-to-Pull Peer-to-Peer Live Streaming

401

connections are still alive. This short loss of connectivity is only due to the lack of a proper leave message. In case disconnecting peers are able to send a leave message, which is certainly the normal case, the network is hardly affected if as many as 50% of the peers leave, and the prefix connectivity does not drop noticeably, as peers immediately search for suitable replacements. Due to the fast repairing process, our system also copes well with membership changes occurring continuously over time.

5

Conclusions

Given the growing number of radio stations and TV channels available online, peer-to-peer live streaming is able to overcome the limitations of traditional, centralized approaches, and it enables content providers to both increase playback quality and to reduce costs. Thus, the p2p paradigm has the potential to democratize the streaming world in that it enables everyone to broadcast her own media content--similarly to how the world wide web revolutionized the distribution of information more than a decade ago: Nowadays, everyone can publish her thoughts on her own blog or website at virtually no cost. By combining pull-based and push-based techniques, our push-to-pull protocol for live streaming achieves high efficiency and robustness, both essential features of a reliable p2p streaming service. As a second central ingredient, our protocol makes use of the lessons learnt from distributed hash tables by structuring the overlay topology while still maintaining a large degree of flexibility. The resulting system is locality-aware and has a guaranteed logarithmic diameter. Moreover, it enables the source to push new data blocks to speed up data dissemination. Having a push mechanism allows to reduce the notification frequency, which leads to a substantially smaller overhead.

References
1. Castro, M., Druschel, P., Kermarrec, A.-M., Nandi, A., Rowstron, A., Singh, A.: SplitStream: High-bandwidth Content Distribution in a Cooperative Environment. In: Kaashoek, M.F., Stoica, I. (eds.) IPTPS 2003. LNCS, vol. 2735, Springer, Heidelberg (2003) 2. Chu, Y., Rao, S., Zhang, H.: A Case For End System Multicast. In: Proc. Int. Conference on Measurement and Modeling of Computer Systems (SIGMETRICS), pp. 1­12 (2000) 3. Goyal, V.K.: Multiple Description Coding: Compression Meets the Network. IEEE Signal Processing Magazine 18(5), 74­93 (2001) 4. Jannotti, J., Gifford, D.K., Johnson, K.L., Kaashoek, M.F., O'Toole, J.W.: Overcast: Reliable Multicasting with an Overlay Network. In: Proc. 4th Symposium on Operating System Design and Implementation (OSDI) (2000) 5. Karp, R., Schindelhauer, C., Shenker, S., Vocking, B.: Randomized Rumor Spreading. In: Proc. 41st Annual Symposium on Foundations of Computer Science (FOCS). IEEE Computer Society Press, Los Alamitos (2000)

402

T. Locher et al.

6. Kosti, D., Rodriguez, A., Albrecht, J., Vahdat, A.: Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh. In: Proc. 19th ACM Symposium on Operating Systems Principles (SOSP), pp. 282­297. ACM Press, New York (2003) 7. Padmanabhan, V.N., Sripanidkulchai, K.: The Case for Cooperative Networking. In: Druschel, P., Kaashoek, M.F., Rowstron, A. (eds.) IPTPS 2002. LNCS, vol. 2429, pp. 178­190. Springer, Heidelberg (2002) 8. Pai, V., Tamilmani, K., Sambamurthy, V., Kumar, K., Mohr, A.: Chainsaw: Eliminating Trees from Overlay Multicast. In: Castro, M., van Renesse, R. (eds.) IPTPS 2005. LNCS, vol. 3640, Springer, Heidelberg (2005) 9. Plaxton, C.G., Rajaraman, R., Richa, A.W.: Accessing Nearby Copies of Replicated Objects in a Distributed Environment. In: Proc. 9th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), pp. 311­320. ACM Press, New York (1997) 10. Rowstron, A., Druschel, P.: Pastry: Scalable, Decentralized Object Location, and Routing for Large-Scale Peer-to-Peer Systems. In: Proc. International Conference on Distributed Systems Platforms (Middleware), pp. 329­350 (2001) 11. Stoica, I., Morris, R., Karger, D., Kaashoek, F., Balakrishnan, H.: Chord: A Scalable Peer-To-Peer Lookup Service for Internet Applications. In: Proc. ACM SIGCOMM Conference, pp. 149­160. ACM Press, New York (2001) 12. Venkataraman, V., Francis, P., Calandrino, J.: Chunkyspread: Multi-tree Unstructured Peer-to-Peer. In: Proc. Int. Workshop on Peer-to-Peer Systems (IPTPS) (2006) 13. Wang, W., Helder, D.A., Jamin, S., Zhang, L.: Overlay Optimizations for End-host Multicast. In: Networked Group Communications (2002) 14. Zhang, M., Tang, Y., Zhao, L., Luo, J.-G., Yang, S.-Q.: Gridmedia: A Multi-Sender Based Peer-to-Peer Multicast System for Video Streaming. In: IEEE Int. Conference on Multimedia and Expo (ICME), pp. 614­617. IEEE Computer Society Press, Los Alamitos (2005) 15. Zhang, X., Liu, J., Li, B., Yum, Y.: CoolStreaming/DONet: A Data-Driven Overlay Network for Peer-to-Peer Live Media Streaming. In: Proc. Annual IEEE Conference on Computer Communications (INFOCOM), pp. 2102­2111. IEEE Computer Society Press, Los Alamitos (2005)

Probabilistic Opaque Quorum Systems
Michael G. Merideth1 and Michael K. Reiter2
1 2

Carnegie Mellon University, Pittsburgh, PA, USA University of North Carolina, Chapel Hill, NC, USA

Abstract. Byzantine-fault-tolerant service protocols like Q/U and FaB Paxos that optimistically order requests can provide increased efficiency and fault scalability. However, these protocols require n  5b + 1 servers (where b is the maximum number of faults tolerated), owing to their use of opaque Byzantine quorum systems; this is 2b more servers than required by some non-optimistic protocols. In this paper, we present a family of probabilistic opaque Byzantine quorum systems that require substantially fewer servers. Our analysis is novel in that it assumes Byzantine clients, anticipating that a faulty client may seek quorums that maximize the probability of error. Using this as motivation, we present an optional, novel protocol that allows probabilistic quorum systems to tolerate Byzantine clients. The protocol requires only one additional round of interaction between the client and the servers, and this round may be amortized over multiple operations. We consider actual error probabilities introduced by the probabilistic approach for concrete configurations of opaque quorum systems, and prove that the probability of error vanishes with as few as n > 3.15b servers as n and b grow.

1

Introduction

For distributed systems consisting of a large number of servers, a Byzantinefault-tolerant replication algorithm that requires all servers to communicate with each other for every client request can be prohibitively expensive. Therefore, for large systems, it is critical that the protocol have good fault scalability [1]--the property that performance does not (substantially) degrade as the system size is increased--by avoiding this communication. Byzantine-fault-tolerant service protocols must assign a total order to requests to provide replicated state machine semantics [2]. To minimize the amount of communication between servers, protocols like Q/U [1] and FaB Paxos [3] use opaque quorum systems [4] to order requests optimistically. That is, servers independently choose an ordering, without steps that would be required to reach agreement with other servers; the steps are performed only if servers choose different orderings. Under the assumption that servers independently typically choose the same ordering, the optimistic approach can provide better fault scalability in the common case than protocols like BFT [5], which require that servers perform steps to agree upon an ordering before choosing it [1]. However, optimistic protocols have the disadvantage of requiring at least 5b + 1 servers to
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 403­419, 2007. c Springer-Verlag Berlin Heidelberg 2007

404

M.G. Merideth and M.K. Reiter

tolerate b server faults, instead of as few as 3b + 1 servers, and so they cannot tolerate as many faults for a given number of servers. In this paper, we present probabilistic opaque quorum systems (POQS), a new type of probabilistic quorum system [6], in order to increase the fraction of faults that can be tolerated by an optimistic approach from fewer than n/5 to as many as n/3.15. A POQS provides the same properties as the strict opaque quorum systems used by, e.g., Q/U and FaB Paxos, but is probabilistic in the sense that quorums are not guaranteed to overlap in the number of servers required to ensure safety. However, we prove that this error probability is negligible for large system sizes (for a given ratio of b to n). Application domains that could give rise to systems of such scale include sensor networks and edge services. Byzantine clients are problematic for all probabilistic quorum systems because the combination of high fault tolerance and low probability of error that can be achieved is based on the assumption that clients choose quorums uniformly at random (and independently of other quorums and the state of the system, e.g., the values held by each server, and the identities of faulty servers). This can be seen in our results that show: (i) that probabilistic opaque quorum systems can tolerate up to n/3.15 faults (compared with less than n/5 faults for strict opaque quorum systems) assuming that all quorums are selected uniformly at random, but that the maximum fault tolerance drops to n/4.56 faults if Byzantine clients are allowed to choose quorums according to their own goals; and (ii) that to achieve a specified error probability for a given degree of fault tolerance, substantially more servers are required if quorums are not selected uniformly at random. Therefore, we present a protocol with which we constrain clients to using pseudo-randomly selected access sets (sets of servers contacted in order to find quorums, c.f., [7]) of a prescribed size. In the limit, we can set the sizes of access sets to be the sizes of quorums, thereby dictating that all clients use pseudo-randomly selected quorums, and providing a mechanism that guarantees, in practice, the behavior of clients that is assumed by probabilistic quorum systems. However, as shown in Section 4.3, the notion of restricted access sets allows us a range of options in trading off the low error probability and high fault tolerance of completely random quorum selection, for the guaranteed singleround access provided when there is an available quorum (one in which all servers respond) in every access set. Our contributions are as follows: ­ We present the first family of probabilistic opaque quorum system constructions. For each construction, we: (i) show that we are able to reduce the number of servers below the 5b + 1 required by protocols that use strict opaque quorums, (ii) prove that it works with vanishing error probability as the system size grows, and (iii) evaluate the characteristics of its error probability over a variety of specific system sizes and configurations. ­ We present the first analysis of a probabilistic quorum system that accounts for the behavior of Byzantine clients. We anticipate that a faulty client may choose quorums with the goal of maximizing the error probability, and show the effects that this may have.

Probabilistic Opaque Quorum Systems

405

­ We present an access-restriction protocol that allows probabilistic quorum systems to tolerate faulty clients with the same degree of fault tolerance as if all clients were non-faulty. One aspect of the protocol is that servers work to propagate the values of established writes to each other in the background. Therefore, we provide analysis, unique to opaque quorum systems, of the number of servers that must propagate a value for it to be accepted by another server.

2

Related Work

Strict Opaque Quorum Systems. Opaque Byzantine quorum systems were introduced by Malkhi and Reiter [4] in two variants: one in which the number of non-faulty servers in a quorum is at least half of the quorum, and the other in which the number of non-faulty servers represents a strict majority of the quorum. The first construction makes it unnecessary for the client to know the sets of servers of which the system can tolerate failure (hence the term `opaque'), while the second construction additionally makes it possible to create a protocol that does not use timestamps. The paper also proves that 5b is the lower bound on the number of servers for the first version; simply changing the inequality to a strict inequality proves 5b + 1 is the lower bound for the second. In this paper, we are concerned with the second variant. The constraints on strict opaque quorums have also been described in the context of consensus and state-machine-replication protocols, e.g., the Q/U [1] and FaB Paxos [3] protocols, though not explicitly as opaque quorums. AbdEl-Malek et al. [1] provide generic (not just threshold) opaque quorum system constraints that they prove sufficient for providing state-machine replication semantics where both writes and reads complete in a single (pipelined) phase when there is no write­write contention. Martin and Alvisi [3] use an opaque quorum system of acceptors in FaB Paxos, a two-phase consensus protocol (with a designated proposer) and three-phase state-machine-replication protocol requiring at least 5b + 1 servers. Probabilistic Quorum Systems. Table 1. Minimum servers needed for probaA Probabilistic Quorum System bilistic and strict quorum variants (PQS), as presented by Malkhi et prob. strict presented al. [6], can provide better availOpaque 3.15b + 1 5b + 1 Here ability and fault tolerance than [6] Masking 2.62b + 1 4b + 1 provided by strict quorum sys[6] tems; Table 1 compares proba- Dissemination b + 1 3b + 1 bilistic quorums with their strict quorum counterparts.1 Malkhi et al. provide constructions for dissemination and masking quorums, and prove properties of load and availability for these constructions. They do not address
1

The 2.62b lower bound for masking quorums is not shown in [6], but can be quickly derived using our results from Section 4.

406

M.G. Merideth and M.K. Reiter

opaque quorum systems, or the effects of concurrent or Byzantine writers; we address each of these. In addition, in Section 4, we borrow analysis techniques from [6], but our analysis is more general in the sense that clients are not all assumed to communicate only with quorums of servers. We also use a McDiarmid inequality [8] in our technical report [9] for bounding the error probability; this provides a simpler bounding technique for our purposes than do the Chernoff bounds used there. The technique that we present in Section 5 for restricting access to limited numbers of servers should be applicable to the constructions of Malkhi et al. equally well. Other Work. Signed Quorum Systems [10] and k -quorums [11,12] also weaken the requirements of strict quorum systems but use different techniques; our technical report [9] has a more detailed discussion. There has been work on strict quorum systems that can tolerate Byzantine clients (e.g., [13,14]) but this is fundamentally unconcerned with the way in which quorums are chosen because such choices cannot impact the correctness of strict quorum systems.

3

System Model and Definitions

We assume a system with a set U of servers, |U | = n, and an arbitrary but bounded number of clients. Clients and servers can fail arbitrarily (i.e., Byzantine [15] faults). We assume that up to b servers can fail, and denote the set of faulty servers by B , where B  U . Any number of clients can fail. Failures are permanent. Clients and servers that do not fail are said to be non-faulty. We allow that faulty clients and servers may collude, and so we assume that faulty clients and servers all know the membership of B (although non-faulty clients and servers do not). We make the standard assumption that nodes are computationally bound such that they cannot subvert the effectiveness of cryptographic primitives. Throughout the paper, we use San Serif font to denote random variables, uppercase ITALICS for set-valued constants, and lowercase italics for integervalued constants. 3.1 Behavior of Clients

We abstractly describe client operations as either writes that alter the state of the service or reads that do not. Informally, a non-faulty client performs a write to update the state of the service such that its value (or a later one) will be observed with high probability by any subsequent operation; a write thus successfully performed is called "established" (we define established more precisely below). A non-faulty client performs a read to obtain the value of the latest established write, where "latest" refers to the value of the most recent write preceding this read in a linearization [16] of the execution. Therefore, we define the correct value for the read to return to be the value of this latest established write; other values are called incorrect. We assume that the read and write operations by non-faulty clients take the following forms:

Probabilistic Opaque Quorum Systems

407

­ Writes: To perform a write, a non-faulty client selects a write access set Awt  U of size awt uniformly at random and attempts to inform all servers in Awt of the write value. Formally, the write is established once all nonfaulty servers in some set Qwt  Awt of size qwt  awt servers have accepted this write. (Intuitively, an access set is a set of servers contacted in order to find a live quorum, c.f., [7].) We refer to qwt as the write quorum size; to any Qwt  U of that size as a write quorum; and to Qwt = {Qwt  U : |Qwt | = qwt } as the write quorum system. ­ Reads: To perform a read, a non-faulty client selects a read access set Ard of size ard uniformly at random and attempts to contact each server in Ard to learn the value that the server last accepted. We denote the minimum number of servers from which a non-faulty client must receive a response to complete the read successfully by qrd  ard . We refer to qrd as the read quorum size; to any Qrd  U of that size as a read quorum; and to Qrd = {Qrd  U : |Qrd | = qrd } as the read quorum system. In a read operation, we refer to each response received from a server in Ard as a vote for a read value. We assume that votes for two read values that result from any two distinct write operations are distinguishable from each other, even if the corresponding write values are the same (this is discussed in Section 5). The read operation discerns the correct value from these votes in a protocol-specific way. It is possible in an optimistic protocol such as Q/U [1], for example, that the (at least qrd ) votes may reflect a write operation but not provide enough evidence to determine whether that write is established. In this case, the reader may itself establish, or repair, the write value before returning it, to ensure that a subsequent reader returns that value, as well (which is necessary to achieve linearizability). In such a protocol, the reader does so by copying its votes for that value to servers, in order to convince them to accept that write. For this reason, the correctness requirements for POQS discussed in Section 4 treat not only the number of votes that a non-faulty reader observes for the correct value, but also the number of votes that a faulty client can gather for a conflicting value. A conflicting value is a specific type of incorrect value characterized by the property that a non-faulty server would accept either it or the correct value, but not both. Two values may conflict because, e.g., they both bear the same timestamp, or are "conditioned on" the same established write in the sense used in Q/U. We assume that this timestamp or similar information can be used to distinguish older (stale) values from newer values. Enabling a faulty client to obtain sufficiently many votes for a conflicting value would, e.g., enable it to convince other non-faulty servers to accept the conflicting value via the repair protocol, a possibility that must be avoided for correctness. Consequently, an error is said to occur when a non-faulty client fails to return the correct value or a faulty client obtains sufficiently many votes for a conflicting value. This definition (or specifically "sufficiently many") will be made more precise in Section 4.4. The error probability then refers to the probability of an error when the client (non-faulty or faulty) reads from a read access set Ard chosen uniformly at random. While we cannot force a faulty client to choose

408

M.G. Merideth and M.K. Reiter

Ard uniformly at random, in Section 5 we demonstrate an access protocol that enables a faulty client to assemble votes for a value that can be verified by servers (and hence, e.g., to perform a repair in Q/U) only if Ard was selected uniformly at random, which is good enough for our purposes. So, from here forward, we restrict our attention to read access sets chosen in this way. 3.2 Communication

The communication assumptions we adopt are common to prior works in probabilistic [6] and signed [10] quorum systems: we assume that each non-faulty client can successfully communicate with each non-faulty server with high probability, and hence with all non-faulty servers with roughly equal probability. This assumption is in place to ensure that the network does not significantly bias a non-faulty client's interactions with servers either toward faulty servers or toward different non-faulty servers than those with which another non-faulty client can interact. Put another way, we treat a server that can be reliably reached by none or only some non-faulty clients as a member of B . This assumption enables us to refine the read protocol of Section 3.1 in a straightforward way so that non-faulty clients choose read quorums from an access set uniformly at random. (More precisely, a faulty server can bias quorum selection away from quorums containing it by not responding, but this decreases the error probability, and so we conservatively assume that non-faulty clients select read quorums at random from their access sets.) However, because a write is, by definition, established once all of the non-faulty servers in any write quorum within Awt have accepted it, the write quorum at which a write is established contains all servers in Awt  B ; i.e., only the the non-faulty servers within the write quorum are selected uniformly at random by a non-faulty client. The access-restriction protocol of Section 5 requires no communication assumptions beyond those of the probabilistic quorums it supports.

4

Probabilistic Opaque Quorum Systems

In this section, we present a family of probabilistic opaque quorum systems. We begin by reviewing the properties of strict opaque quorum systems [4]. Define the following functions (where Qrd and Qwt are as defined in Section 3.1): correct(Qrd , Qwt ) conflicting(Qrd , Qwt ) : |(Qrd  Qwt ) \ B | : |(Qrd  B )  (Qrd \ Qwt )| (1) (2)

correct(Qrd , Qwt ) returns the number of non-faulty servers in the intersection of a pair of read and write quorums, while conflicting(Qrd , Qwt ) returns the other servers in the read quorum, all of which may return a conflicting value in some protocol execution. Let a read operation return a value that receives at least r votes. Then, the consistency property for strict opaque quorum systems is as follows:

Probabilistic Opaque Quorum Systems

409

O-Consistency : Qrd  Qrd , Qwt  Qwt : correct(Qrd , Qwt )  r > conflicting(Qrd , Qwt ). (3)

The property states that the number of non-faulty servers in the intersection of any read quorum and write quorum must represent a majority of the read quorum. Because of this and the fact that newer values can be distinguished from older values, the correct value--which, by definition, is established by being written to all of the non-faulty servers in a write quorum--can be distinguished from other values, even if some non-faulty servers (and all faulty servers) present conflicting or stale values. At a high level, O-Consistency guarantees: P1 No two conflicting writes are both established. P2 Every read observes sufficiently many votes for the correct value to identify it as such. P3 No (non-faulty or faulty) reader obtains votes for a conflicting value sufficient to repair it successfully. Given that the stated assumptions of a strict opaque quorum system hold, the system behaves correctly. In contrast to this, probabilistic opaque quorum systems (POQS) allow for a (small) possibility of error. Informally, this can be thought of as relaxing O-Consistency so that a variant of it holds for most--but not all--quorums. To ensure that the probability of an error happening is small, POQS are designed so that P1, P2, and P3 hold with high probability. In the remainder of this section, we model the worst-case behavior of faulty clients (Section 4.1); derive a constraint (PO-Consistency, Section 4.2) that determines the maximum fraction of faulty servers that can be tolerated (Section 4.3) by POQS; and prove that the error probability goes to zero as n (and b) is increased if this constraint is satisfied (Section 4.4). 4.1 Behavior of Faulty Clients

Because a faulty client can behave arbitrarily, we examine the way that a faulty client should choose quorums to maximize the chance of error. Throughout this section, let Awt denote a write access set from which Qwt (a quorum used for an established write) is selected by a faulty client, let Awt be a write access set used for a conflicting write by a faulty client, and let Ard be a read access set from which Qrd , a read quorum, is selected by a faulty client. Again, we assume that Awt , Awt , and Ard are selected uniformly at random, an assumption that can be enforced using the protocol of Section 5. A faulty client can increase the error probability with a write in one of two ways: (i) by establishing a write at a write quorum that contains as many faulty servers as possible, or (ii) by performing the write of a conflicting value in a way that maximizes the number of non-faulty servers that accept it, i.e., by writing to all of Awt \ Qwt . Since a faulty client may perform both such writes, we assume that this client has knowledge of Awt and Awt simultaneously. However, it is important to note that a faulty client does not have knowledge of the read

410

M.G. Merideth and M.K. Reiter

Fig. 1. The preference (1st, 2nd, 3rd) a faulty client gives to a server when choosing (a) Qwt , or (b) Qrd

access set Ard used by a non-faulty client--or specifically the non-faulty servers within it, i.e., Ard \ B --and so Qwt is chosen independently of Ard \ B .2 Figure 1(a) shows the preferences that a faulty client gives to servers when choosing Qwt to do both (i) and (ii). Goal (i) requires maximizing |Qwt  B | to maximize the probability that P1 or P2 is violated; hence, first preference is given to the servers in Awt  B in a write. Goal (ii) requires minimizing |(Qwt  Awt ) \ B | to maximize the probability that P1 or P3 is violated; hence, the servers in (Awt  Awt ) \ B are avoided to the extent possible. A faulty client can increase the probability that P3 is violated by choosing a read quorum with the most faulty servers and non-faulty servers that share the same conflicting value. Figure 1(b) shows the preferences that a faulty client gives to servers to do so. Because a faulty client can collude with the servers in B , it can obtain replies from all servers in B that are also in Ard , i.e., the servers in Ard  B . It can also wait for responses from all of the non-faulty servers in Ard with the conflicting value, i.e., those in Ard  (Awt \ Qwt ). Only after receiving all such responses, and only if these responses number fewer than qrd , must it choose responses from servers with other values. 4.2 Probabilistic Constraint

In this section, we present PO-Consistency, a constraint akin to O-Consistency specified in terms of expected values for POQS. As detailed below, let MinCorrect be a random variable for the minimum number of non-faulty servers that report the correct value in a randomly chosen read quorum taken by a non-faulty client. (Recall that an error is caused by MinCorrect being too small only for reads performed by a non-faulty client.) Also, let MaxConflicting be a random variable for the maximum number of servers that report a conflicting value in a read quorum taken from a randomly chosen read access set by a faulty client that seeks to maximize MaxConflicting. (Recall that an error is caused by MaxConflicting
2

More precisely, with the access protocol in Section 5, Ard can be hidden unless, and until, that read access set is used for repair, at which point it is too late for faulty clients to choose Qwt so as to induce an error in that read operation.

Probabilistic Opaque Quorum Systems

411

being too large even if the client is faulty.) Then the consistency property for POQS is: PO-Consistency : E [MinCorrect] > E [MaxConflicting] . (4)

As shown in Section 4.4, PO-Consistency allows us to choose a threshold, r, for the number of votes used to determine the result of a read operation, while ensuring that the error probability vanishes as we increase n (and b). We now derive expressions for MinCorrect and MaxConflicting. Recall that B is the set of up to b faulty servers. Let Awt be a randomly chosen write access set, and let Ard be a randomly chosen read access set. As stated in the system model, a write to Awt is established once it has been accepted by all of the non-faulty servers in any Qwt , a write quorum within Awt . Therefore, we conservatively assume that the number of faulty servers in Qwt is: MalWrite = |Awt  B |. (5)

Here, Awt is a random variable taking on a write access set chosen uniformly at random from Awt . Qwt also contains qwt - MalWrite non-faulty servers, not necessarily chosen at random, in addition to the MalWrite faulty servers. Let Cwt represent these non-faulty servers: Cwt = Qwt \ B, |Cwt | = qwt - MalWrite, (6) (7)

where Qwt is a random variable taking on the write quorum at which the write is established, and Cwt is a random variable taking on the set of non-faulty servers within this write quorum. Then, the number of non-faulty servers that return the correct value in a read quorum selected by a non-faulty client is, MinCorrect = |Qrd  Cwt |, (8)

where Qrd is a random variable taking on a read quorum chosen uniformly at random from Ard , itself chosen uniformly at random from Ard . A faulty client may select its read quorum, Qrd , to maximize the number of votes for a single conflicting value in an attempt to invalidate P3. Therefore, as described in Section 4.1, the client first chooses all faulty servers in Ard . The number of such servers is, Malevolent = |Ard  B |. (9)

The faulty client also chooses the non-faulty servers that vote for the conflicting value that is most represented in Ard ; these servers are a subset of (Ard \ (Cwt  B )). This conflicting value has an associated write access set Awt chosen uniformly at random from Awt , and no vote from a non-faulty server not in Awt will be counted among those for this conflicting value (because votes for any two write operations are distinguishable from each other as discussed in

412

M.G. Merideth and M.K. Reiter

Section 3.1). Let Awt be a random variable taking on Awt . Then, the number of non-faulty servers in Ard that vote for this conflicting value is, Conflicting = |Ard  (Awt \ (Cwt  B ))|. (10) A faulty client can choose all of these servers for Qrd . Therefore, since the sets of servers measured by Malevolent and Conflicting are disjoint (the former consists solely of faulty servers; the latter solely of non-faulty servers), the maximum number of instances of the same conflicting value that a faulty client will select for Qrd is, MaxConflicting = Malevolent + Conflicting. 4.3 Minimum System Sizes (11)

In this section, we consider PO-Consistency under various assumptions concerning the sizes of access sets and quorums in order to derive the maximum fraction of faults that can be tolerated with decreasing error probability as a function of n (and b). Our primary result is Theorem 1 which provides an upper bound on b for which PO-Consistency holds. It is derived using the expectations of MinCorrect and MaxConflicting (derived in our technical report [9]) that are computed using the worst-case behavior of faulty clients presented in Section 4.1. Theorem 1. PO-Consistency holds iff b< (ard qwt n - 2ard awt n + a2 wt ard + qrd qwt n)n . 2 2 n ard - ard awt n + awt ard + qrd awt n

As shown in Section 4.4, a construction exhibits decreasing error Table 2. Lower bounds on n for various conprobability in the limit with in- figurations creasing n if PO-Consistency holds. n> =n =n-b = n - 2b Therefore, the remainder of this 3.15b ard qrd awt qwt section is concerned with interpretqrd awt qwt 3.83b ard ing the inequality in Theorem 1. 4.00b awt ard qrd qwt Our analysis of this inequality is ard awt qwt qrd 4.08b given in Table 2 and shows that the 4.56b ard awt qrd qwt best bounds are provided when: (i) 4.73b awt ard qwt qrd both types of quorums are as large 5.49b ard qrd awt qwt as possible (while still ensuring an 6.07b ard qrd awt qwt available quorum), i.e., qrd = qwt = 6.19b ard awt qrd qwt n - b; and (ii), given (i), that access sets as small as possible. Our technical report [9] provides a more detailed analysis including an inequality for systems with no Byzantine clients. 4.4 Bounding the Error Probability

Suppose a read operation always returns a value that receives more than r votes, where E [MaxConflicting]  r < E [MinCorrect]. Then, the error probability, , is

Probabilistic Opaque Quorum Systems

413

= Pr(MaxConflicting > r  MinCorrect  r). Theorem 2 states that if r is chosen so that E [MinCorrect] - r = (n) and r - E [MaxConflicting] = (n)

(12)

(13)

then decreases as a function of n, assuming that the ratio of each of b, ard , qrd , awt , and qwt to n remains constant. For example, r can be set equal to (E [MaxConflicting] + E [MinCorrect])/2. Theorem 2. Let MinCorrect, MaxConflicting, and r be defined as above (so POConsistency holds) and let the ratio of each of b, ard , qrd , awt , and qwt to n be fixed. Then, = 2/e (n) + 2/e (n) .

5

Access-Restriction Protocol

Our analysis in the previous sections assumes that all access sets are chosen uniformly at random by all clients--even faulty clients. Therefore, here we present an access-restriction protocol that is used to enforce this. Recall from Section 3.1 that the need for read access sets to be selected uniformly at random is motivated by repair. As such, protocols that do not involve repair may not require this access-restriction protocol for read operations. Our protocol must balance conflicting constraints. First, a client may be forced to discard a randomly chosen access set--and choose another--because a given access set (of size less than b servers more than a quorum) might not contain an available quorum. However, in order to support protocols like Q/U [1] that use opaque quorum systems for single-round writes, we cannot require additional rounds of communication for each operation. This precludes, for example, a protocol in which the servers collectively choose an access set at random and assign it to the client for every operation. As such, a client must be able to choose from multiple access sets without involving the servers for each. Yet, a faulty client should be prevented from discarding access sets in order to choose the one that has the highest probability of causing an error given the current system state. In addition, we should ensure that a faulty client does not benefit from waiting for the system state to change in order to use a previously chosen access set that becomes more advantageous as a result of the change. In our protocol, the client obtains one or more random values, each called a Verifiable Random Value (VRV), with the participation of non-faulty servers. Each VRV determines a unique, verifiable, ordered sequence of random access sets that the client can use; the client has no control over the sequence. To deter a client from discarding earlier access sets in the sequence for potentially more favorable access sets later in the sequence, the protocol imposes an exponentially increasing cost (in terms of computation) for the ability to use later access sets.

414

M.G. Merideth and M.K. Reiter

The cost is implemented as a client puzzle [17]. We couple this with a facility for the propagation of the correct value in the background so that any advantages for a faulty client in the current system state are reduced if the client chooses to delay performing the operation while it explores later access sets. Finally, to deter a client from waiting for the system state to change, we tie the validity of a VRV (and its sequence of access sets) to the state of the system so that as execution proceeds, any unused access sets become invalid. 5.1 Obtaining a VRV

In order to get an access set, the client first must obtain a VRV from the servers. Servers implement a metering policy, in which each server responds to a request for a VRV only after a delay. The delay varies, such that it increases exponentially with the rate at which the client has requested VRVs during some recent interval of time--i.e., a client that has not requested a VRV recently will receive a VRV with little or no delay, whereas a client that has recently requested many VRVs will receive a VRV after a (potentially significant) delay. To offload work from servers to clients (e.g., for scalability), the servers can make it relatively more expensive (in terms of time) to ask for and receive a new VRV than to compute a given number of access sets (potentially for multiple operations) from a single VRV, using the mechanisms described below. The VRV is characterized by the following properties: ­ It can be created only with the consent of non-faulty servers; ­ Its validity is tied to the state of the system, in the sense that as the system state evolves (possibly merely through the passage of time), eventually the VRV is invalidated; ­ While it is valid, any non-faulty server can verify its validity and so will accept it. The VRV must be created with the consent of non-faulty servers because otherwise faulty servers might collude to issue multiple VRVs to a faulty client with no delay. Therefore, l, the number of servers required for the issuance of a VRV, must be at least b + 1. However, of the non-faulty servers in the system, only those among the (at least l - b) used to issue a VRV will impose additional delay before issuing an additional VRV. Therefore, to minimize the time to get an additional VRV, a faulty client avoids involving servers that have issued VRVs recently. This strategy maximizes the number of VRVs to which the non-faulty server contributing to the fewest VRVs has contributed. Thus, once k VRVs have been issued, all n - b non-faulty servers have contributed to the issuance of at least k (l - b)/(n - b) of these k . Since all non-faulty servers have contributed to at least this many VRVs, and the delay is exponential in this number, the time T (k ) required for a client to obtain k VRVs is: T (k ) =  exp k (l - b) n-b

Probabilistic Opaque Quorum Systems

415

In practice, T (k ) for a client decays during periods in which that client does not request additional VRVs, so that a client that does not request VRVs for a period can obtain one with small delay. The validity of the VRV (and its sequence of access sets) is tied to the state of the system so that as execution proceeds, any unused access sets become invalid. To implement this, the replication protocol may provide some piece of data that varies with the state of the system--the Object History Set in Q/U [1] is an example of this--with which the servers can compute a VRV, but, in the absence of a suitable value from the protocol, the VRV can include a timestamp (assuming that the non-faulty servers have roughly synchronized clocks). The VRV consists of this value together with a digital signature created using a (l, n)-threshold signature scheme (e.g., [18]), i.e., so that any set of l servers can together create the signature, but smaller sets of servers cannot. The signature scheme must be strongly unforgeable [19], meaning that an adversary, given a VRV, is not able to find other valid VRVs. This is necessary because otherwise a faulty client would be able to generate variations of a valid VRV until finding one from which to select an access set that causes an error (see below). 5.2 Choosing an Access Set

As motivated above: (i) the VRV determines a sequence of valid access sets; and (ii) a client puzzle must make it exponentially harder to use later access sets in the sequence than earlier ones. In addition, it is desirable for our protocol to satisfy the following requirements: ­ Each VRV must determine only a single valid sequence of access sets. This is to prevent a faulty client from choosing a preferred sequence. ­ The puzzle solutions must be easy to verify, so that verification costs do not limit the scalability of the system in terms of the number of requests. ­ There must be a solution to each puzzle. Otherwise a non-faulty client might be unable to use any access set. ­ No server can know the solution to the puzzle beforehand due to the Byzantine fault model. Otherwise, a faulty client could avoid the exponential work by asking a faulty server for the solution. In our protocol, the sequence of access sets is determined as follows. Let v be a VRV, let g be a hash function modeled as a random oracle [20], and let access set be a deterministic operation that, given a seed value, selects an access set of the specified size from the set of all access sets of that size in a uniform fashion. Let the first seed, s1 , be g(v ), and the i'th seed, si , be g(si-1 ). Then the i'th access set is access set(si ). Our technical report [9] contains an example specification of access set. In order to use the i 'th access set, the client must solve a puzzle of suitable difficulty. This puzzle must be non-interactive [21] to avoid additional rounds of communication. There are many suitable candidate puzzle functions [21].

416

M.G. Merideth and M.K. Reiter

5.3

Server Verification

Upon receiving a write request for the i 'th access set, each non-faulty server in the chosen access set must verify that it is a member of the access set; for a repair request it must verify that the relevant votes are from servers in the access set of the read operation that gave rise to the repair. In addition, in either case, before accepting the value, each server must verify that the VRV is valid, that the access set corresponds to the i 'th access set of the sequence, and that the client has provided a valid solution to a puzzle of the appropriate difficulty level to use the i 'th access set. While the client can obtain additional access sets from the VRV, each access set used is treated as a different operation by servers as stated in Section 3.1; e.g., a write operation using one access set, and then using another access set, is treated as two different writes,3 so that a faulty client cannot "accumulate" more than awt servers for its operation through the use of multiple write access sets. 5.4 Background Propagation

As described above, servers work to propagate the values of established writes to each other in the background. Our main contribution in this area is our analysis of the threshold number of servers that must propagate a value for it to be accepted by another server. While related Byzantine diffusion protocols (e.g., [22]) use the number b +1, we require a larger number because opaque quorum systems allow that some non-faulty servers may accept conflicting values. We assume an appropriate propagation algorithm (e.g., a variant of an epidemic algorithm [23] such as [22]). At a high level, a non-faulty server has two responsibilities. First, having accepted a write value and returned a response to the client, it periodically informs other servers that it has accepted the value. Second, if it has not yet accepted a value upon learning that a threshold number, p, of servers have accepted the value, it accepts the value. Faulty servers are all assumed to have access to any conflicting value directly without propagation, so we assume no additional constraints on their behavior. Lemma 1. Let n < 2qwt - 2b and p = n - qwt + b + 1. Then an established value will be accepted and propagated by at least p non-faulty servers, and no conflicting value can be propagated by p servers (faulty or non-faulty). For example, if qwt = n - b and n > 4b we set p = 2b + 1. Since the established value will be accepted by at least p non-faulty servers, it will propagate. No conflicting value will propagate. If the conditions of Lemma 1 do not hold, we must allow for some probability of error during propagation. We set p so that it is between the expectations of the minimum number of non-faulty servers that accept an established write (PCorrect), and the maximum number of servers that propagate a conflicting value (PConflicting).
3

Typically, a Byzantine-fault-tolerant write protocol must already be resilient to partial writes, which is how these writes using different access sets might appear to the service.

Probabilistic Opaque Quorum Systems

417

Lemma 2. PO-Consistency  E [PCorrect] > E [PConflicting]. Lemma 2 shows that we can set p as described for any system in which POConsistency holds.

6

Evaluation

In this section, we analyze error probabilities for concrete system sizes. In addition to validating our results from Section 4, this shows that an access restriction protocol like that of Section 5 can provide significant advantages in terms of worst-case error probabilities.
105 105 n=4.66b+1 n=5.00b+1 10 min n
4

103

min n 10-4

n=3.25b+1 n=3.93b+1 n=4.10b+1 n=4.66b+1 n=5.00b+1

10

4

103

102

102

101 -2 10

10-3 error probability

101 -2 10

10-3 error probability

10-4

(a) restricted reads and writes (ard = qrd , awt = qwt )

(b) unrestricted (ard = n, awt = n)

Fig. 2. Number of servers required to achieve given calculated worst-case error probability

Figure 2 plots the total number of nodes required to achieve a given calculated error probability for two configurations that tolerate faulty clients where qwt = qrd = n - b: the restricted configuration (ard = qrd , awt = qwt ) and the unrestricted configuration (ard = n, awt = n). Since the unrestricted configuration (Figure 2(b)) does not require the access-restriction protocol of Section 5, yet yields better maximum ratios of b to n than the other configurations listed in Table 2 in which qwt = awt - b or qrd = ard - b from Section 4.3, we do not evaluate the error probabilities for those configurations here. In all cases, the error probabilities are worst-case in that they reflect the situation in which all b nodes are in fact faulty. For each configuration, we provide plots for different ratios of n to b, ranging from the maximum b for a given configuration, to n = 5b + 1, as a comparison with strict opaque quorum systems. Our technical report [9] provides details of our calculations, as well as calculations for additional configurations. In the figure, we see that to decrease the worst-case error probability, we can either keep the same function of b in terms of n while increasing n, or hold n fixed while decreasing the number of faults the system can tolerate. In addition, we see that configurations that tolerate a larger b also provide better error probabilities

418

M.G. Merideth and M.K. Reiter

for a given b. Overall, we find that our constructions can tolerate significantly more than b = n/5 faulty servers, while providing error probabilities in the range of 10-2 to 10-4 for systems with fewer than 50 servers to hundreds of servers. Coupled with the dissemination of correct values between servers (off the critical path), as described in Section 5.4, the error probability decreases between writes.

7

Conclusion

First, have presented probabilistic opaque quorum systems (POQS), a new type of opaque quorum system that we have shown can tolerate up to n/3.15 Byzantine servers (compared with n/5 Byzantine servers for strict opaque quorum systems) with high probability, while preserving the properties that make opaque quorums useful for optimistic Byzantine-fault-tolerant service protocols. Second, we have presented an optional, novel access-restriction protocol for POQS that provides the ability for servers to constrain clients so that they use randomly selected access sets for operations. With POQS, we expect to create probabilistic optimistic Byzantine fault-tolerant service protocols that tolerate substantially more faults than current optimistic protocols. While strict opaque quorums systems may be more appropriate for smaller systems that require no chance of error, a POQS can provide increased fault tolerance for a given number of nodes, with a worst-case error probability that is bounded and that decreases as the system scales. Acknowledgments. This work was partially supported by NSF grant CCF0424422.

References
1. Abd-El-Malek, M., Ganger, G.R., Goodson, G.R., Reiter, M.K., Wylie, J.J.: Faultscalable Byzantine fault-tolerant services. In: Symposium on Operating Systems Principles (October 2005) 2. Schneider, F.B.: Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Computing Surveys 22(4), 299­319 (1990) 3. Martin, J.P., Alvisi, L.: Fast Byzantine consensus. IEEE Transactions on Dependable and Secure Computing 3(3), 202­215 (2006) 4. Malkhi, D., Reiter, M.: Byzantine quorum systems. Distributed Computing 11(4), 203­213 (1998) 5. Castro, M., Liskov, B.: Practical Byzantine fault tolerance and proactive recovery. ACM Transactions on Computer Systems 20(4), 398­461 (2002) 6. Malkhi, D., Reiter, M.K., Wool, A., Wright, R.N.: Probabilistic quorum systems. Information and Computation 170(2), 184­206 (2001) 7. Bazzi, R.A.: Access cost for asynchronous Byzantine quorum systems. Distributed Computing 14(1), 41­48 (2001) 8. McDiarmid, C.: Concentration for independent permutations. Combinatorics, Probability and Computing 11(2), 163­178 (2002) 9. Merideth, M.G., Reiter, M.K.: Probabilistic opaque quorum systems. Technical Report CMU-CS-07-117, CMU School of Computer Science (March 2007)

Probabilistic Opaque Quorum Systems

419

10. Yu, H.: Signed quorum systems. Distributed Computing 18(4), 307­323 (2006) 11. Aiyer, A.S., Alvisi, L., Bazzi, R.A.: On the availability of non-strict quorum systems. In: Fraigniaud, P. (ed.) DISC 2005. LNCS, vol. 3724, pp. 48­62. Springer, Heidelberg (2005) 12. Aiyer, A.S., Alvisi, L., Bazzi, R.A.: Byzantine and multi-writer k-quorums. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 443­458. Springer, Heidelberg (2006) 13. Liskov, B., Rodrigues, R.: Tolerating Byzantine faulty clients in a quorum system. In: International Conference on Distributed Computing Systems (2006) 14. Cachin, C., Tessaro, S.: Optimal resilience for erasure-coded Byzantine distributed storage. In: International Conference on Dependable Systems and Networks (2006) 15. Lamport, L., Shostak, R., Pease, M.: The Byzantine generals problem. ACM Transactions on Programming Languages and Systems 4(3), 382­401 (1982) 16. Herlihy, M., Wing, J.: Linearizability: A correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems 12(3), 463­492 (1990) 17. Juels, A., Brainard, J.: Client puzzles: A cryptographic countermeasure against connection depletion attacks. In: Network and Distributed Systems Security Symposium, pp. 151­165 (1999) 18. Shoup, V., Gennaro, R.: Securing threshold cryptosystems against chosen ciphertext attack. Journal of Cryptology 15(2), 75­96 (2002) 19. An, J.H., Dodis, Y., Rabin, T.: On the security of joint signature and encryption. In: Knudsen, L.R. (ed.) EUROCRYPT 2002. LNCS, vol. 2332, pp. 83­107. Springer, Heidelberg (2002) 20. Bellare, M., Rogaway, P.: Random oracles are practical: A paradigm for designing efficient protocols. In: Conference on Computer and Communications Security, pp. 62­73 (1993) 21. Jakobsson, M., Juels, A.: Proofs of work and bread pudding protocols. In: Communications and Multimedia Security, pp. 258­272 (1999) 22. Malkhi, D., Mansour, Y., Reiter, M.K.: Diffusion without false rumors: On propagating updates in a Byzantine environment. Theoretical Computer Science 299(1­ 3), 289­306 (2003) 23. Demers, A., Greene, D., Hauser, C., Irish, W., Larson, J., Shenker, S., Sturgis, H., Swinehart, D., Terry, D.: Epidemic algorithms for replicated database maintenance. In: Principles of Distributed Computing, pp. 1­12 (August 1987)

Detecting Temporal Logic Predicates on Distributed Computations
Vinit A. Ogale and Vijay K. Garg
Parallel and Distributed Systems Laboratory, Dept. of Electrical and Computer Engineering, University of Texas at Austin {ogale,garg}@ece.utexas.edu

Abstract. We examine the problem of detecting nested temporal predicates given the execution trace of a distributed program. We present a technique that allows efficient detection of a reasonably large class of predicates which we call the Basic Temporal Logic or BTL. Examples of valid BTL predicates are nested temporal predicates based on local variables with arbitrary negations, disjunctions, conjunctions and the possibly (EF or ) and invariant(AG or 2) temporal operators. We introduce the concept of a basis, a compact representation of all global cuts which satisfy the predicate. We present an algorithm to compute a basis of a computation given any BTL predicate and prove that its time complexity is polynomial with respect to the number of processes and events in the trace although it is not polynomial in the size of the formula. We do not know of any other technique which detects a similar class of predicates with a time complexity that is polynomial in the number of processes and events in the system. We have implemented a predicate detection toolkit based on our algorithm that accepts offline traces from any distributed program.

1

Introduction

In large distributed programs it is often desirable to have a formal guarantee that the program output is correct. One approach is to model check the entire program with respect to the given specification. This is impractical even for most moderately complex programs. For many applications, predicate detection offers a simple and efficient alternative over model checking the entire program. Predicate detection involves verifying the execution trace of a distributed program with respect to a given property (for example, violation of mutual exclusion). The correctness properties or the predicates, which enable us to formally define a correct execution, can have temporal implications. In scientific computing, it may be vital to verify that the result of a computation was valid, and if it was invalid due to a rare `chance' bug, the program can
Supported in part by the NSF Grants CNS-0509024, Texas Education Board Grant 781, SRC Grant 2006-TJ-1426, and Cullen Trust for Higher Education Endowed Professorship.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 420­434, 2007. c Springer-Verlag Berlin Heidelberg 2007

Detecting Temporal Logic Predicates on Distributed Computations

421

be re-executed. Predicate detection provides a formal guarantee on the validity of the computation (assuming that the specifications are correct). If the specifications can be expressed in a supported logic then verification of the traces requires a comparatively insignificant overhead (polynomial in the number of processes and events) using the algorithm discussed in this paper. Note that this approach is obviously not useful for critical real time applications where it is essential that all runs be correct. A distributed computation, i.e., the execution trace of a distributed program, can either be modeled as a total order, or as a partial order on the set of events in the computation. Representing the computation as a total order can mask some of the bugs in other possible consistent interleavings. A partial order, in contrast, captures all the possible causally consistent interleavings. In this paper we use a partial order representation based on Lamport's happened before relation [1]. The drawback of using a partial order model is that the number of global states of the computation is exponential in the number of processes. This makes predicate detection a hard problem in general [2,3]. A number of strategies like symbolic representation of states and partial order reduction have been explored to tackle the state explosion problem [4,5,6,7,8,9,10]. In this paper, we present a technique to efficiently detect all temporal predicates that can be expressed in, what we call, Basic Temporal Logic or BTL. An example of a valid BTL predicate would be a property based on local predicates and arbitrarily placed negations, disjunctions and conjunctions along with the possibly() and invariant(2) temporal operators (the EF and AG operators defined in [11]). Our algorithm is based on computing a basis which is a compact representation of the subset of the computational lattice containing exactly those global states (or cuts) that satisfy the predicate. In general, it is hard to efficiently compute a basis for an arbitrary predicate. We utilize the fact that the set of global states of a computation forms a distributive lattice and restrict the predicates to BTL formulas. The basis introduced in this paper is a union of smaller sets of cuts called semiregular structures. Note that, without any restrictions on the predicate formula, predicate detection is NP-complete with respect to the formula size, and for arbitrary predicates the time complexity could be exponential in the formula size. However, if the input formula is in a `DNF like' form after pushing in negations, our technique detects it in polynomial time with respect to the formula size. To summarize, this paper makes the following contributions: ­ We introduce the concept of a basis and discuss representations of stable, regular and semiregular predicates. ­ We present an algorithm to efficiently compute the basis for BTL predicates. This enables detection of BTL predicates in O(2k .|E |.n) time, where k is the number of operators in the predicate, E is the set of events in the computation and n is the number of processes. To the best of our knowledge, there is no other known technique that can detect nested temporal predicates

422

V.A. Ogale and V.K. Garg

containing disjunctions or negations with a time complexity that is polynomial in n and |E |. ­ We discuss the implementation of our algorithm and compare it with existing approaches like using SPIN [12] and POTA [13] to detect predicates in distributed programs. Our tool, BTV (Basis based Trace Verifier), can analyze traces in a compatible format generated by any distributed program. Note that currently known approaches, like slicing [14] or model checking of traces, for detecting a similar class of predicates, are inefficient and require exponential time with respect to the number of processes. The remainder of the paper is organized as follows: Section 2 discusses related work and section 3 explains in detail, the model and notation used in the paper. Section 4 introduces the concept of a basis of a computation with respect to a predicate and presents the main algorithm. In section 5, we present the complexity analysis of our algorithm. We follow that with an example and a short description of our implementation of a predicate detection toolkit based on the algorithm in this paper.

2

Related Work

A number of approaches for checking computations using temporal logic have been published. Temporal Rover [15], MaC [16] and JPaX [17] are some of the available tools. Many of the tools are based on total ordering of events and hence cannot be directly compared to our approach. These tools can miss potential bugs which would be detected by partial order representations. JMPaX [18] is based on a partial order model and supports temporal properties but its time complexity is exponential in the number of processes in the computation. Another available option to verify computation traces is to use a model checking tool like SPIN [12,19]. The computation trace needs to be converted to the SPIN input computation and verification takes exponential time in the number of processes. Computational slicing [14] based approaches can efficiently detect regular predicates. POTA [13] is such a partial order based tool which uses computational slicing to detect predicates. POTA guarantees polynomial time complexity only if the predicate can be expressed in a subset of CTL [11] called Regular CTL or RCTL [20]. Disjunctions and negations are not allowed in RCTL. If POTA is used with a logic that allows disjunctions or negations (like BTL), it uses a model checking algorithm to explore the reduced state space. Hence the asymptotic time complexity using POTA is exponential in the number of processes when the predicate contains disjunctions. Table 1 compares the time complexities of SPIN, POTA and our algorithms implemented in the BTV tool.

3

Model and Notation

This paper uses basic lattice theory constructs that are formally defined in the technical report [21]. We assume a loosely coupled, message-passing,

Detecting Temporal Logic Predicates on Distributed Computations Table 1. Time complexities (n = number of processes)

423

SPIN POTA BTV RCTL exponential in n polynomial in n polynomial in n BTL exponential in n exponential in n polynomial in n asynchronous system model. A distributed program consists of n sequential programs P1 , P2 , . . . , Pn . A computation is a single execution of such a program. A distributed computation ( E,  ) is modeled as a partial order on the set of events E , based on the happened before relation () [1]. The size of the computation is the total number of events, |E |, in the computation. Definition 1. (Consistent Cut) A consistent cut C is a set of events in the computation which satisfies the following property: if an event e is contained in the set C , then all events in the computation that happened before e are contained in C . e1 , e2  E : (e2  C )  (e1  e2 )  e1  C . In figure 1(i) the set {e1 , f1 } is a consistent cut, while {e1 , e2 } is not. In the following discussion, we mean `consistent cut' whenever we simply say `cut'. For notational convenience, we simply mention the maximal elements on each process that are elements of the cut to represent that cut. For example, the cut {e1 , e2 , f1 , f2 , f3 } is written as {e2 , f3 }. The set of all consistent cuts in a computation is denoted by C . This set, C , forms a distributive lattice [22] (also called the computational lattice) under the less than equal to relation defined as follows. Definition 2. Cut C1 is less than or equal to cut C2 if and only if, C1  C2 . A cut C , in a computation E , satisfies a predicate P if the predicate is true in the global state represented by the cut. This is denoted by (C, E ) |= P or simply C |= P where the context is clear. The join of two cuts is simply defined as their union, and the meet of two cuts corresponds to the intersection of those two cuts. Figure 1 shows a computation and the distributive lattice formed by all the consistent cuts in the computation. Birkhoff's representation theorem [22] states that a distributive lattice can be completely characterized by the set of its join irreducible elements. Join irreducibles are elements of the lattice that cannot be expressed as the join of any two elements.1 For example, in figure 1(ii), cuts {}, {f1}, {f2 }, {f3 }, {e1 , f1 }, {e2 , f1 }, {e3 , f1 } are join irreducible. The cut, {e1 , f2 } is not join irreducible because it can be expressed as the join of cuts {f2 } and {e1 , f1 }. The initial cut is the least cut, i.e., the empty set {} and the final cut is the greatest cut, i.e, the set of all events E , in the computational lattice.
1

Commonly, the bottom element is not considered to be a join irreducible element. However, in this paper, for notational convenience, we include the bottom element (the initial cut {}) in the set of join irreducible elements.

424

V.A. Ogale and V.K. Garg

f

Fig. 1. A computation and the lattice of its consistent cuts

Detecting a predicate in a distributed computation is determining if the initial cut of the computation satisfies the predicate. Definition 3. (Join-closed, Meet-closed and Regular Predicates) A predicate P is join-closed if all cuts that satisfy the predicate are closed under union. i.e., (C1 |= P  C2 |= P )  (C1  C2 ) |= P . Similarly a predicate P is meet-closed if all the cuts that satisfy the predicate are closed under intersection. A predicate is regular if it is join-closed and meetclosed. If cuts C1 and C2 satisfy a regular predicate, then by definition, C1  C2 and C1  C2 also satisfy that predicate. For example, the predicate "No process has the token and the token in not in transit" is regular. All conjunctions of local predicates are regular. A predicate is stable if, once it becomes true, it remains true [23]. A stable predicate is always join-closed. Definition 4. A predicate P is stable, if C1 , C2  C : C1 |= P  C1  C2  C2 |= P . Some examples of stable predicates are loss of a token, deadlocks, and termination. Figure 2 depicts examples of the cuts satisfied by meet-closed, join-closed, regular and stable predicates.

4

Basis of a Computation

We now introduce the concept of a basis of a computation. Informally, a basis is an exact compact representation of the set of cuts which satisfy the predicate. Definition 5. (Basis) Given a computational lattice C , corresponding to a computation E , and a predicate P , a subset S [P ] of C is a basis of P if

Detecting Temporal Logic Predicates on Distributed Computations

425

Fig. 2. Predicates

1. (Compactness) The size of S [P ] is polynomial in the size of computation that generates C . 2. (Efficient Membership) Given any cut (global state) C  C , there exists a polynomial time algorithm that takes S [P ], E and C as inputs and determines if (C, E ) |= P . We denote the basis with respect to a predicate P as S [P ]. Given a predicate P , a cut C belongs to a basis S [P ], if C satisfies that predicate. i.e., C  S [P ]  C |= P . Note that direct enumeration of all the states satisfied by a predicate is, in general, not a basis since determining if a cut is a member of that set could take exponential time. For a simple example of an basis, consider a class of predicates, such that the cuts satisfying a predicate in that class form an ideal in the computational lattice. (An ideal is a sublattice that contains every cut that is less than the maximal cut in the sublattice.) A basis, for such a class of predicates, is just the maximal cut of the ideal. It can be efficiently determined if a cut C  Cp by checking if the cut is less than or equal to the maximal cut. Computational slicing, introduced in [14], is a technique to compute an efficient predicate structure for regular predicates. Definition 6. (Slice) The slice slice[P ] of a computation with respect to a predicate P is the poset of the join irreducible consistent cuts representing the smallest sublattice that contains all consistent cuts satisfying P . Though the number of consistent cuts satisfying the predicate may be large, the slice of a predicate can be efficiently represented by the set of the join irreducible cuts in the slice. Slicing is the operation of computing the slice for the given predicate.

426

V.A. Ogale and V.K. Garg

When the predicate is regular, the computed slice represents exactly those cuts that satisfy the predicate. Given the slice with respect to a predicate, it is possible to efficiently detect if a cut satisfies that predicate. Therefore, a slice is an efficient basis for regular predicates. However, using slicing for predicate detection of non-regular predicates can take exponential time. In the remainder of this paper, we explore a technique to compute a basis for a more general class of predicates, that we call BTL, which can have arbitrary negations, disjunctions, conjunctions and the temporal possibly() operator. Since a BTL predicate can be non-regular, a slice of a BTL predicate is not a valid basis. One naive approach to compute a predicate structure is to maintain a set of slices instead of a single slice. Though this is polynomial in the number k of processes n, it results in a large number of slices (O(n2 )), where k is the size of the predicate. In this paper, we introduce a semiregular structure which can efficiently represent a more general class than regular predicates. A BTL predicate can be represented by using a set of semiregular structures.

ËØ

Ð

ÈÖ

Ø

c1 c2
Á Ð Û Ø Ñ Ü ÙØ

c1

Á

Ð ÛØ

Ñ Ü

ÙØ

c2

Fig. 3. Representing stable predicates

We start off by looking at the representation of a stable predicate. Figure 3 shows an example of a stable predicate. The set of states satisfying a stable predicate can be considered to be the union of a set of filters of the computational lattice. Thus, a stable predicate can be represented by the set of minimal cuts that satisfy the predicate. Another representation is to identify a set of ideals, I = {I1 , I2 , . . .} of the computational lattice such that all the cuts satisfying the stable predicate are contained in the complement of I I I . The stable predicate in figure 3 can be represented by two ideals as seen in the figure. We use the set of ideals representation in this paper for computational efficiency while dealing with BTL predicates.

Detecting Temporal Logic Predicates on Distributed Computations

427

Definition 7. (Stable Structure) Given a stable predicate P and the computational lattice C , a stable structure is the set of ideals I such that a cut satisfies P iff it does not belong to any of the ideals in I . Therefore, C |= P  ¬(C  I I I ). A cut C is said to belong to the stable structure if C does not belong to I I I . Note that, any ideal is uniquely and efficiently represented by its maximal cut. In the remainder of this paper we use I to represent a set of ideals representing the stable predicate and simply maxCuts to denote the set containing the maximal cut from each ideal in I . Note that, this representation is not a basis since, the set of ideals could be very large in general. However, we see later, that this leads to an efficient representation when the predicate is expressed in BTL. 4.1 Semiregular Predicates and Structures

The conjunction of a stable predicate and a regular predicate is called a semiregular predicate and is more expressive than either of them. Definition 8. P is a semiregular predicate if it can be expressed as a conjunction of a regular predicate with a stable predicate. We now list some properties of semiregular predicates. 1. All regular predicates and stable predicates are semiregular. This follows from the definition of semiregular predicates since true is a stable and regular predicate. 2. Since regular and stable predicates are join-closed, it follows that their conjunction, a semiregular predicate, is also join-closed. However not all joinclosed predicates are semiregular. Figure 4 shows a join-closed predicate that is not semiregular. 3. Semiregular predicates are closed under conjunction, i.e., if P and Q are semiregular then P  Q is semiregular. 4. If P is a semiregular predicate then P and 2P are semiregular. If P is semiregular, P has a unique maximal cut, say Cmax and P is an ideal of the lattice that contains all cuts less than or equal to Cmax . We now present an alternative characterization of a semiregular predicate that offers a different insight into the structure of the cuts satisfying such a predicate. Lemma 1. Predicate P is semiregular iff ­ P is join-closed, i.e, C1 |= P  C2 |= B  (C1  C2 ) |= P and ­ The meet of two cuts that satisfy P is C , and C does not satisfy P , then any cut smaller than C does not satisfy P . i.e., (C1  C2 ) |= P  (C  (C1  C2 ) : ¬(C |= P )). A few examples of semiregular predicates are listed below.

428

V.A. Ogale and V.K. Garg
{e3 , f3 } {e2 , f3 } {e1 , f3 }

{e3 , f2 } {e2 , f2 } {e1 , f2 } {e3 , f1 }

{f3 } {f2 } {f1 }

{e2 , f1 }

{e1 , f1 }

Predicate is true

{}

Fig. 4. A join-closed predicate may not be semiregular

­ All processes are never red concurrently at any future state and process 0 has the token. That is P = ¬( redi )  token0 . ­ At least one process is beyond phase k (stable) and all the processes are red. We now define a representation for semiregular predicates. Definition 9. (Semiregular Structure) A semiregular structure, g , is represented as a tuple ( slice, I ) consisting of a slice and a stable structure, such that the predicate is true in exactly those cuts that belong to the intersection of the slice and the stable structure. Hence C  g  (C  slice)  ¬(C  I I I ). Note that, a cut is contained in a semiregular structure if it belongs to the slice and the stable structure in the semiregular structure. The maximal cut in a semiregular structure is the maximal cut in the slice if the semiregular structure is nonempty. We see later that any BTL predicate can be expressed as a basis consisting of a union of semiregular structures. A semiregular structure enables us to easily handle predicates of the form ¬P . Such a predicate can be represented by n slices or by a single stable structure or a semiregular structure. We use this in our algorithms and prove that it is possible to compute an efficient basis representation for any BTL predicate. 4.2 Logic Model (BTL)

In this section formally define Basic Temporal Logic (BTL), such that any predicate expressible in BTL can be efficiently detected using the algorithm presented later in this paper. The atomic propositions in BTL are local predicates, i.e., properties that depend on a single process in the computation. Local predicates and their negations are regular predicates. Let AP be the set of all atomic propositions. Given the set of all consistent cuts, C , of a computation, a labeling

Detecting Temporal Logic Predicates on Distributed Computations

429

function  : C  2AP assigns to each consistent cut, the set of predicates from AP that hold in it. The operators  and  represent the boolean conjunction and disjunction operators as usual, ¬ represent the negation of a predicate and we define the possibly () temporal operator (called EF in [4]). Definition 10. If C is the set of all consistent cuts of the computation, then P holds at consistent cut C , if and only if, there exists C  C such that P is true at C and C  C . The formal BTL syntax is given below. Definition 11. A predicate in BTL is defined recursively as follows: 1. l  AP , l is a BTL predicate 2. If P and Q are BTL predicates then P  Q, P  Q, P and ¬P are BTL predicates We formally define the semantics of BTL. ­ ­ ­ ­ ­ (C, E, ) |= l  l  (C ) for an atomic proposition l (C, E, ) |= P  Q  C |= P and C |= Q (C, E, ) |= P  Q  C |= P or C |= Q (C, E, ) |= P  C  C : (C  C and C |= P ) (C, E, ) |= ¬P  ¬(C |= P )

We use (C, E ) |= P or simply C |= P in the rest of the discussion when E and  are obvious from the context. Note that, the AG operator in CTL [4] can be written as ¬¬ in BTL. 4.3 Algorithm

We present an algorithm to compute a basis for any predicate expressed in BTL. The computed basis consists of a set of semiregular structures such that a cut belongs to the basis if it belongs to any semiregular structure in that set. Definition 12. Given a BTL predicate P , we define a representation S of the predicate that consists of a set of semiregular structures such that C |= P  (g  S : C  g ). We assume that the input predicate has negations pushed in to the local predicates or the  operators. In the following discussion, we often treat ¬ as single operator. We see later that our algorithm returns an efficient predicate structure which allows polynomial time detection of the predicate. Each semiregular structure, g , is represented as a tuple slice, maxCuts where g.slice is the slice in g and g.maxCuts is the set of cuts corresponding to the ideals representing the stable structure. The use of ideals instead of filters is very important and results in the 2k bound (see theorem 2) on the size of the stable structure. (The stable structures calculated by the algorithm could require nk filters to represent it.)

430

V.A. Ogale and V.K. Garg

/*The input predicate Pin has all negations pushed - inside to the  operator or to the atomic propositions */ /* each semiregular structure is represented as a tuple slice, maxCuts - where maxCuts is the set of maximal cuts - of the ideals I representing the stable structure */ function getBasis(Predicate Pin ) output: S [Pin ], a set of semiregular structures Case 1. (Base case: local predicates) : Pin = l or Pin = ¬l S [Pin ] := { slice(P ), {} } Case 2. Pin = P  Q S [P ] := getBasis(P ); S [Q] = getBasis(Q); S [Pin ] := {S [P ]  S [Q]}; Case 3. Pin = P  Q S [P ] := getBasis(P ); S [Q] = getBasis(Q); S [Pin ] := gp S [P ],gq S [Q] {( gp .slice  gq .slice, gp .maxCuts  gq .maxCuts )}; Case 4. Pin = P S [P ] := getBasis(P ); S [Pin ] := gS [P ] { (g.slice), {} }; Case 5. Pin = ¬P S [P ] := getBasis(P ); /* sliceorig is the original computation */ S [Pin ] := { sliceorig , gS [P ] {maxCutIn(g.slice)} }; Remove all empty semiregular structures from S [Pin ]; return S [Pin ]

Fig. 5. Computing a basis

Figure 5 outlines the main algorithm to compute a basis of the computation for any BTL predicate. For predicate detection, we simply check if the initial cut of the computation is contained in the computed basis. To determine if a cut is contained within the basis, we need to examine if it belongs to any semiregular structure in the basis. A basis is nonempty if the predicate is true in any consistent cut of the computation. Note that, in case we need to check whether a predicate P is true at any cut in the computation (and not just the initial cut), we can either apply our algorithm on the predicate P or alternatively apply the algorithm on P and check if the returned basis is nonempty. The algorithm computes the basis by recursively processing the predicate inside out. ­ The base case is a local predicate. Note that, the negation of a local predicate is also local. We know that for each atomic proposition li , slice[li ] can be computed in polynomial time. Efficient algorithms to compute slice[li ] (or slice[¬li ]) when the atomic propositions are local predicates, can be found in [14]. The basis of a local predicate has a single semiregular structure that consists of a slice and an empty set of ideals. (A local predicate and its

Detecting Temporal Logic Predicates on Distributed Computations

431

­

­

­

­

negation are regular predicates and hence a slice is an efficient basis for such predicates). The second case handles disjunctions. If the input predicate Pin is of the form P  Q the basis is the structure containing all the cuts in S [P ] and S [Q] and is obtained by computing the union of the sets S [P ] and S [Q]. When the input predicate is of the form P  Q, the resultant basis is the pairwise intersection of each semiregular structure in S [P ] and S [Q]. Each semiregular structure consists of a slice and a stable structure. The intersection of two semiregular structures, say gp and gq , is the tuple gp .slice  gq .slice, gp .stable structure  gq .stable structure . The grafting algorithm described in [14] describes a technique to compute the intersection of two slices. Since we use ideals to represent stable structures, the intersection of the stable structures is represented by the union of the sets gp .maxCuts and gq .maxCuts. The fourth case in the algorithm handles predicates of the form Pin = P . S [P ] is the union of a set of semiregular structures. The resultant basis is obtained by computing g for each g in S [P ] and taking the union. Note that g is equivalent to (g.slice) and the algorithm for EF of a regular predicate in [20] can be used to determine (g.slice). Since ¬P is stable, the basis corresponding to ¬P contains a single semiregular structure g . The slice in this semiregular structure is the original computation while the ideals are represented by the maximal cuts of the slice in each of the semiregular structures that belong to S [P ]. In this case, it becomes clear that using the `set of ideals representation' for stable structures is more efficient. The number of ideals is guaranteed to be k if S [P ] had k semiregular structures. Using another representation like maintaining a set of filters would have resulted in expensive operations since the number of filters could be nk in this case.

After each step, the algorithm checks if any of the semiregular structures are empty and discards the empty semiregular structures. A semiregular structure is empty, if the maximal element of the slice is less than or equal to each cut in g.maxCuts. It can be seen that the structure returned by our algorithm contains exactly those cuts which satisfy the input predicate. We show in section 5 that the number of semiregular structures and the number of ideals required to represent the stable structures returned by our algorithm is polynomial in n. This enables us to check whether a cut belongs to the structure in polynomial time and hence the structure is efficient.

5

Complexity Analysis

The time taken by the algorithm in figure 5 depends on the number of ideals representing the stable structure in each semiregular structure and the total number of semiregular structures in the resultant basis (the size of the basis). The proofs for most the results in this section are presented in the technical

432

V.A. Ogale and V.K. Garg

report [21] due to space constraints. We first present a result on the bound on the size of computed basis. Theorem 1. The basis S [P ] computed by the algorithm in Figure 5 for a BTL predicate P with k operators has at most 2k semiregular structures. This leads to the following theorem. Theorem 2. The total number of ideals |I | in the basis computed by the algorithm in Figure 5 for a BTL predicate P is at most 2k . The time required to compute the conjunction of two slices with respect to  is O(|E |n) [14]. It takes O(|E |n) time to compute the slice with respect to the  operator. Theorem 3. The time complexity of the algorithm in figure 5 is polynomial in the number of events (|E |) and the number of processes (n) in the computation. The algorithm simplifies the predicate by computing the basis one operator at a time. Hence, if there are k operators in all, it requires k steps to compute the basis for the entire predicate. Theorem 1 states that after the lth operator is processed at most 2l new semiregular structures are generated. The generation of each semiregular structure takes less than or equal to |E |n time. The time required to generate all the semiregular structures is 2l .|E |n. The algorithm compares each ideal to the maximal cut of a slice to check if the semiregular structure is empty. There are at most 2l semiregular structures (theorem 1) which implies that there are no more than 2l slices (since each semiregular structure contains exactly one slice). The total number of ideals is less than or equal to 2l (theorem 2). Since comparing two cuts requires O(n) time, it takes (2l + 2l )n time to check which semiregular structures are empty. Hence the time required to process the lth operator is 2l .(|E |n) + n(2l+1 ) , i.e, 2l+1 .n.(2|E | + 1)) l+1 .n.(2|E | + 1) = O(2k |E |n). Therefore the total time required is lk =1 2 If the input predicate is in a `DNF-like' form then predicate detection is even more efficient (polynomial in k ). Theorem 4. If the input predicate has conjunctions only over regular predicates, then the size of the predicate structure and the total number of ideals |I |, is at most k . Since conjunctions are allowed over regular predicates, the resulting predicate is regular and can be represented by exactly one semiregular predicate with no ideals.

6

Implementation

We have implemented a toolkit to verify computation traces generated by distributed programs. This toolkit accepts offline execution traces as its input.

Detecting Temporal Logic Predicates on Distributed Computations

433

We used a Java implementation of the distributed dining philosophers algorithm from [24] and checked for errors in the system. We injected faults in the traces and verified the traces using, both, our toolkit and POTA [13]. Note that, for predicates containing disjunctions, POTA reduces the computation size and uses SPIN [19] to check for predicate violations. The POTA-SPIN combination performs well in some runs (when the slice generated is lean or empty) but it runs out of memory when the number of processes is increased, especially when configured to list all predicate violations. BTV, as expected, scales well and we could use it to verify computations with large number of processes. Our implementation (including the Java source code) can be downloaded from our laboratory website. Note that the toolkit relies on offline traces and hence it is not necessary for the program that is being tested to be implemented in Java. It can be used with any arbitrary distributed program that outputs a compatible trace. The toolkit includes a utility to convert traces from the POTA trace format.

7

Conclusions

We conclude that it is possible to efficiently detect nested temporal predicates containing disjunctions and negations (along with conjunctions and ). We have introduced the notion of a semiregular structure and have presented techniques to efficiently compute an efficient basis given any BTL predicate. This has many practical applications which require verification of traces. Apart from ensuring the validity of runs, the technique discussed in this paper is also useful in distributed program debuggers. Since the computed basis contains exactly all states where the predicate holds, we can use it to pinpoint the faults in the program. One useful extension of this work would be an online version of the algorithm which could be used to control distributed programs by changing their behavior at runtime if faults are detected.

References
1. Lamport, L.: Time, clocks, and the ordering of events in a distributed system. Communications of the ACM 21(7), 558­565 (1978) 2. Stoller, S.D., Schneider, F.B.: Faster possibility detection by combining two approaches. In: Proc. of the 9th International Workshop on Distributed Algorithms, Le Mont-Saint-Michel, France, pp. 318­332. Springer, Heidelberg (1995) 3. Garg, V.K.: Elements of Distributed Computing. Wiley & Sons, Chichester (2002) 4. McMillan, K.L.: Symbolic Model Checking. Kluwer Academic Publishers, Dordrecht (1993) 5. Godefroid, P.: Partial-Order Methods for the Verification of Concurrent Systems. LNCS, vol. 1032. Springer, Heidelberg (1996) 6. Valmari, A.: A stubborn attack on state explosion. In: Clarke, E., Kurshan, R.P. (eds.) CAV 1990. LNCS, vol. 531, pp. 156­165. Springer, Heidelberg (1991) 7. Peled, D.: All from one, one for all: On model checking using representatives. In: Courcoubetis, C. (ed.) CAV 1993. LNCS, vol. 697, pp. 409­423. Springer, Heidelberg (1993)

434

V.A. Ogale and V.K. Garg

8. Stoller, S.D., Unnikrishnan, L., Liu, Y.A.: Efficient Detection of Global Properties in Distributed Systems Using Partial-Order Methods. In: Emerson, E.A., Sistla, A.P. (eds.) CAV 2000. LNCS, vol. 1855, pp. 264­279. Springer, Heidelberg (2000) 9. Stoller, S.D., Liu, Y.: Efficient symbolic detection of global properties in distributed systems. In: Emerson, E.A., Sistla, A.P. (eds.) CAV 2000. LNCS, vol. 1855, pp. 264­279. Springer, Heidelberg (2000) 10. Esparza, J.: Model checking using net unfoldings. Science Of Computer Programming 23(2), 151­195 (1994) 11. Clarke, E.M., Emerson, E.A.: Design and synthesis of synchronization skeletons using branching-time temporal logic. In: Logic of Programs, Workshop, London, UK, pp. 52­71. Springer, Heidelberg (1982) 12. Holzmann, G.: The model checker SPIN. IEEE transactions on software engineering 23(5), 279­295 (1997) 13. Sen, A., Garg, V.K.: Partial order trace analyzer (POTA) for distributed programs. In: Proceedings of the Third International Workshop on Runtime Verification (RV) (2003) 14. Mittal, N., Garg, V.K.: Slicing a distributed computation: Techniques and theory. In: Welch, J.L. (ed.) DISC 2001. LNCS, vol. 2180, pp. 78­92. Springer, Heidelberg (2001) 15. Drusinsky, D.: The temporal rover and the ATG rover. In: Havelund, K., Penix, J., Visser, W. (eds.) SPIN Model Checking and Software Verification. LNCS, vol. 1885, pp. 323­330. Springer, Heidelberg (2000) 16. Kim, M., Kannan, S., Lee, I., Sokolsky, O., Viswanathan, M.: Java-MaC: A runtime assurance tool for Java programs. In: Runtime Verification 2001. ENTCS, vol. 55 (2001) 17. Havelund, K., Rosu, G.: Monitoring Java programs with Java PathExplorer. In: Runtime Verification 2001. ENTCS, vol. 55 (2001) 18. Sen, K., Rosu, G., Agha, G.: Detecting errors in multithreaded programs by generalized predictive analysis of executions. In: Steffen, M., Zavattaro, G. (eds.) FMOODS 2005. LNCS, vol. 3535, Springer, Heidelberg (2005) 19. Holzmann, G.: The Spin Model Checker. Addison-Wesley Professional, Reading (2003) 20. Sen, A., Garg, V.K.: Detecting temporal logic predicates in distributed programs using computation slicing. In: 7th International Conference on Principles of Distributed Systems (2003) 21. Ogale, V., Garg, V.K.: Predicate detection. In: Technical report TR-PDS-2007-001 (2007), available at http://maple.ece.utexas.edu/TechReports/2007/TR-PDS-2007-001.ps 22. Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order. Cambridge University Press, Cambridge, UK (1990) 23. Chandy, K.M., Lamport, L.: Distributed snapshots: Determining global states of distributed systems. ACM Transactions on Computer Systems 3(1), 63­75 (1985) 24. Hartley, S.: Concurrent Programming: The Java Programming Language. Oxford University Press, Oxford (1998)

Optimal On-Line Colorings for Minimizing the Number of ADMs in Optical Networks
(Extended Abstract)
Mordechai Shalom1,3 , Prudence W.H. Wong2 , and Shmuel Zaks1 ,
Department of Computer Science, Technion, Haifa, Israel {cmshalom,zaks}@cs.technion.ac.il Department of Computer Science, The University of Liverpool, Liverpool, UK pwong@csc.liv.ac.uk 3 Tel-Hai Academic College, Upper Galilee, 12210, Israel cmshalom@telhai.ac.il
1

2

Abstract. We consider the problem of minimizing the number of ADMs in optical networks. All previous theoretical studies of this problem dealt with the off-line case, where all the lightpaths are given in advance. In a real-life situation, the requests (lightpaths) arrive at the network on-line, and we have to assign them wavelengths so as to minimize the switching cost. This study is thus of great importance in the theory of optical networks. We present an on-line algorithm for the problem, and show . We show that this result is best possible its competitive ratio to be 7 4 in general. Moreover, we show that even for the ring topology network . We there is no on-line algorithm with competitive ratio better than 7 4 show that on path topology the competitive ratio of the algorithm is 3 . This is optimal for this topology. The lower bound on ring topology 2 does not hold when the ring is of bounded size. We analyze the triangle for it. The analyzes of the upper topology and show a tight bound of 5 3 bounds, as well as those for the lower bounds, are all using a variety of proof techniques, which are of interest by their own, and which might prove helpful in future research on the topic.

1
1.1

Introduction
Background

Optical wavelength-division multiplexing (WDM) is today the most promising technology that enables us to deal with the enormous growth of traffic in communication networks, like the Internet. A communication between a pair of nodes is done via a lightpath, which is assigned a certain wavelength. In graph-theoretic terms, a lightpath is a simple path in the network, with a color assigned to it. Given a WDM network G = (V, E ) comprising optical nodes and a set of fullduplex lightpaths P = {p1 , p2 , ..., pN } of G, the wavelength assignment (WLA)
This research was partly supported by the EU Project "Graphs and Algorithms in Communication Networks (GRAAL)" - COST Action TIST 293.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 435­449, 2007. c Springer-Verlag Berlin Heidelberg 2007

436

M. Shalom, P.W.H. Wong, and S. Zaks

task is to assign a wavelength to each lightpath pi . Most of the studies in optical networks dealt with the issue of assigning colors to lightpaths, so that every two lightpaths that share an edge get different colors. When the various parameters comprising the switching mechanism in these networks became clearer, the focus of studies shifted, and today a large portion of the studies concentrates on the total hardware cost. The key point here is that each lightpath uses two Add-Drop Multiplexers (ADMs), one at each endpoint. If two adjacent lightpaths, i.e. lightpaths sharing a common endpoint, are assigned the same wavelength, then they can use the same ADM. Because ADMs are designed to be used mainly in ring and path networks in which the degree af a node is at most two, an ADM may be shared by at most two lightpaths. The total cost considered is the total number of ADMs. A more detailed technical explanation can be found in [GLS98]. Lightpaths sharing ADMs in a common endpoint can be thought as concatenated, so that they form longer paths or cycles. These paths/cycles do not use any edge e  E twice, for otherwise they cannot use the same wavelength which is a necessary condition to share ADMs. 1.2 Previous Work

Minimizing the number of ADMs in optical networks is a main research topic in recent studies. The problem was introduced in [GLS98] for the ring topology. An approximation algorithm for ring topology with approximation ratio of 3/2 was presented in [CW02], and was improved in [SZ04, EL04] to 10/7 + and 10/7, respectively. For general topology [EMZ02] describe an algorithm with approximation ratio of 8/5. The same problem was studied in [CFW02] and an algorithm with an approximation ratio of 3 2 + was presented. This algorithm is further analyzed in [FSZ06b]. The problem of on-line path coloring is studied in earlier works, such as [LV98]. The problem studied in these works has a different objective function, namely the number of colors. All previous theoretical studies on the problem of minimizing the number of switches dealt with the off-line case, where all the lightpaths are given in advance. In a real-life situation, the requests (lightpaths) arrive at the network on-line, and we have to assign them wavelengths so as to minimize the switching cost. An on-line algorithm is said to be c-competitive if for any sequence of lightpaths, the number of ADMs used is at most c times that used by the optimal offline algorithm (see [BEY98]). 1.3 Our Contribution

We present an on-line algorithm with competitive ratio of 7 4 for any network topology. We prove that no algorithm has a competitive ratio better than 7 4 even if the topology is a ring. We show that the same algorithm has a competitive ratio of 3 2 in path topologies, and that this is also a lower bound for on-line algorithms in this topology.

Optimal On-Line Colorings for Minimizing the Number of ADMs

437

The lower bound on ring topology does not hold when the ring is of a bounded size. We study the triangle topology, and show a tight bound of 5 3 for the competitive ratio on this topology, using another algorithm. The analyses of the upper bounds, as well as those for the lower bounds, use a variety of proof techniques, which are of interest on their own, and which might prove helpful in future research on the topic. In Section 2 we describe the problem and some preliminary results. The algorithm and its competitive analysis are presented in Section 3. In Section 4 we present lower bounds for the competitive ratio of the problem on general topology, ring and path topologies. In Section 5 we present tight bounds for triangle networks. We conclude with discussion and open problems in Section 6. Some proofs are sketched in this Extended Abstract; for full details the reader is referred to [SWZ07].

2

Preliminaries

An instance  of the problem is a pair  = (G, P ) where G = (V, E ) is an undirected graph and P is a set of simple paths in G. In an on-line instance, the graph G is known in advance and the set P of paths is given on-line. In this case we denote P = {p1 , p2 , ..., pN } where pi is the i-th path of the input and Pi = {pj  P |j  i} consists of the first i paths of the input. In this work we need a number of notions introduced in [FSZ06a]. ­ The paths p, p  P are conflicting or overlapping if they have an edge in common. This is denoted as p p . The graph of the relation is called the conflict graph of (G, P ). ­ A proper coloring (or wavelength assignment) of P is a function w : P  N, such that w(p) = w(p ) whenever p p . ­ A valid chain (resp. cycle) of  = (G, P ) is a path (resp.cycle) formed by the concatenation of distinct paths pi0 , pi1 , ..., pik-1  P that do not go over the same edge twice. Note that the paths of a valid chain (resp. cycle) constitute an independent set of the conflict graph. ­ A solution S of an instance  = (G, P ) is a set of valid chains and valid cycles of P such that each p  P appears in exactly one of these sets. Note that w is a proper coloring if and only if for any color c  N, w-1 (c) is an independent set in the conflict graph. In the sequel we introduce the shareability graph, which together with the conflict graph constitutes another (dual) representation of the instance . In the sequel, except one exception, we will use the dual representation of the problem. ­ The shareability graph of an instance  = (G, P ), is the edge-labelled multigraph G = (P, E ) such that there is an edge e = (p, q ) labelled u in E if and only if p q , and u is a common endpoint of p and q in G. ­ A valid chain (resp. cycle) of G is a simple path pi0 , pi1 , ..., pik-1 of G , such that any two consecutive edges in the path (resp. cycle) have distinct labels

438

M. Shalom, P.W.H. Wong, and S. Zaks

and its node set is properly colorable with one color (in G), or in other words constitutes an independent set of the conflict graph. ­ The sharing graph of a solution S of an instance  = (G, P ), is the following subgraph G,S = (P, ES ) of G . Two lightpaths p, q  P are connected with an edge labelled u in ES if and only if they are consecutive in a chain or cycle in the solution S , and their common endpoint is u  V . We will usually omit the index  and simply write GS . d(p) is the degree of node p in GS . Example: Let  = (G, P ) be the instance in the left side of Figure 1. Its shareability graph G is the graph at midle. In this instance P = {a, b, c, d}, and it constitutes the set of nodes of G . The edges together with their labels are E = {(b, c, u), (a, c, w), (a, b, x), (a, d, x)}, because a and b can be joined in their common endpoint x, etc.. Note that, for instance (b, d, x)  / E , because although b and d share a common endpoint x, they can not be concatenated, because they have the edge (x, u) in common. The corresponding conflict graph is at the right side of the figure. It has the same node set and one edge, namely (b, d). The paths b, d  P are conflicting because they have a common edge, i.e. (u, x).
x w
d x a a w x d

a d
t u

b c
c

b u

c

b

v

Fig. 1. A sample input

Note that the edges of the conflict graph are not in E . This immediately follows from the definitions. Note also that, for any node v of G , the set of labels of the edges adjacent to v is of size at most two. Valid chains and cycles of G correspond to valid chains and cycles of the instance . In the above example the chain a, d which is the concatenation of the paths a and d in the graph G, corresponds to the simple path a, d in G and the cycle a, b, c which is a cycle formed by the concatenation of three paths in G corresponds to the cycle a, b, c in G . Note that no two consecutive labels are equal in this cycle. On the other hand the paths b, a, d can not be concatenated to form a chain, because this would require the connection of a to both b and d at node x. The corresponding path b, a, d in G is not a chain because the edges (b, a) and (a, d) have the same label, namely x. S = {(d, a, c), (b)} is a solution with two chains. The sharing graph of this solution has two edges (d, a) and (a, c). Note that for a chain of size at most two, the distinct labelling condition is satisfied vacuously, and the independent set condition is satisfied because no edge of G can be an edge of the conflict graph. We define i  {0, 1, 2} , Di (S ) = {p  P |d(p) = i} and di (S ) = |Di (S )|.
def def

Optimal On-Line Colorings for Minimizing the Number of ADMs

439

Note that d0 (S ) + d1 (S ) + d2 (S ) = |P | = N. An edge (p, q )  ES with label u corresponds to a concatenation of two paths with the same color at their common endpoint u. Therefore these two endpoints can share an ADM operating at node u, thus saving one ADM. We conclude that every edge of ES corresponds to a saving of one ADM. When no ADMs are shared, each path needs two ADM's, a total of 2N ADMs. Therefore the cost of a solution S is cost(S ) = 2 |P | - |ES | = 2N - |ES | . The objective is to find a solution S such that cost(S ) is minimum, in other words |ES | is maximum. The following definitions and Lemma appeared in [FSZ06b], we repeat them here for completeness. Given a solution S , d(p)  2 for every node p  P . Therefore, the connected components of GS are either paths or cycles. Note that an isolated vertex is a special case of a path. Let PS be the set of the connected components of GS that are paths. Clearly, |ES | = N - |PS |. Therefore cost(S ) = 2N - |ES | = N + |PS |. Let S  be a solution with minimum cost. For any solution S we define (S ) =
def

d0 (S ) - d2 (S ) - 2 |PS  | . N

Lemma 2.1 For any solution S , cost(S ) = cost(S  ) + 1 2 N (1 + (S )). Proof. Clearly |ES  | = N - |PS  |. On the other hand 2 |ES | is the sum of the degrees of the nodes in GS , namely 2 |ES | = d1 (S ) + 2d2 (S ) = N - d0 (S ) + d2 (S ). We conclude: cost(S ) - cost(S  ) = |ES  | - |ES | = N - |PS  | - = N d0 (S ) - d2 (S ) - 2 |PS  | 1 + = N 2 2 2 N - d0 (S ) + d2 (S ) 2 d0 (S ) - d2 (S ) - 2 |PS  | 1+ N

3

Upper Bounds

In this section we first describe an on-line algorithm, show that it is 7/4-competitive on any network topology and 3/2-competitive on path topology. 3.1 Algorithm ONLINE-MINADM

In a general network, when the lightpaths are given one-by-one, we adopt a simple coloring procedure. Basically, a new lightpath with endpoints u and v looks for free ADM at its endpoints. If there are two of the same color, then it first tries to make a cycle with the existing lightpaths, and if this is impossible then it makes a path. If there are free ADMs (at one endpoint, or at both endpoints but of different colors), then it tries to connect to any of them. Otherwise - when there is no free ADM - it is assigned a new color.

440

M. Shalom, P.W.H. Wong, and S. Zaks

When we attempt to color some lightpath pi , a color  is said to be feasible for pi , if there is no other lightpath with the same color and overlapping with pi . In other words  is feasible for pi , if we can assign w(pi ) =  and w is a proper coloring for Pi . When a lightpath pi with endpoints ui and vi arrives, ­ If there exists a chain of lightpaths with the same color  with endpoints ui , vi and  is feasible for pi then, assign w(pi ) = . ­ Otherwise, If there exists a chain of lightpaths with the same color  with one endpoint from {ui , vi } and  is feasible for pi then, assign w(pi ) = . ­ Otherwise, assign w(pi ) =  , where  is an unused color. Note that, as in the last clause the algorithm resorts to an unused color, it will never construct two chains with the same color. Therefore in the first clause, the algorithm necessarily closes a cycle. Algorithm ONLINE-MINADM is obviously correct: w is a proper coloring for Pi , because if pi is colored by one the first two cases, then it is checked by the algorithm for feasibility, otherwise w(pi ) is assigned an unused color, therefore no other path, in particular no path pj conflicting with pi may have w(pj ) = w(pi ). In this and the following section we prove the following theorem. Theorem 3.1 Algorithm ONLINE-MINADM is optimal for ­ general topology, with competitive ratio of 7 4, ­ ring topology, with competitive ratio of 7 , 4 ­ path topology, with competitive ratio of 3 2. 3.2 Analysis for General Topology
7 4.

Lemma 3.1 The competitive ratio of ONLINE-MINADM is at least

Proof. Let G be a cycle of three nodes V = {v1 , v2 , v3 }, E = {e1 , e2 , e3 } where e1 = (v1 , v2 ), e2 = (v2 , v3 ), e3 = (v3 , v1 ) and let P = {p1 , p2 , p3 , p4 } where p1 = (e3 ), p2 = (e1 ), p3 = (e2 , e3 ), p4 = (e1 , e2 ). The optimal solution assigns w(p1 ) = w(p4 ) = 1 and w(p2 ) = w(p3 ) = 2 , and uses 4 ADMs. Recall that ONLINEMINADM receives the paths of the input one at a time. It assigns w(p1 ) = 1 , then w(p2 ) = 1 because 1 is feasible for p2 , then w(p3 ) = 2 because 1 is not feasible for p3 and finally w(p4 ) = 3 , because neither 1 nor 2 are feasible for p4 . It uses 7 ADM's in total. In the sequel S is a solution returned by the ONLINE-MINADM and S  is an optimal solution. Lemma 3.2 The competitive ratio of ONLINE-MINADM is at most
7 4.

Proof. We direct each edge of GS  , such that each path becomes a directed path and each cycle becomes a directed cycle. The direction chosen for every  - path (resp. cycle) is arbitrary. Let G S  be the digraph obtained by this process.

Optimal On-Line Colorings for Minimizing the Number of ADMs

441

Unless otherwise stated, din (p) and dout (p) denote the in and out degrees of p  - in G S  , respectively. Clearly, p  P , din (p)  1 and dout (p)  1. The following  - definitions refer to G S  :  - LAST  is the set of nodes that do not have successors in G S  , namely LAST  = {p  P |dout (p) = 0} . Note that |LAST  | = |PS  |. The functions N ext and P rev  are defined as expected: N ext (resp. P rev  )  - maps a node p to the next (resp. previous) node in G S  whenever such a node exists, namely: N ext : P \ LAST   P and N ext (p) is the unique node u such that there is an edge from p to u in -  G S  . P rev  = N ext -1 . With these definitions in hand, we partition D0 (S ) into the sets A, B, C and D using the following classification procedure : Given a path p  D0 (S ), if p  LAST  then p is in A and fA (p) = p. Otherwise, there is a node q = N ext (p), we decide according to the degree of q in S : if it has degree 2, then p is in B and fB (p) = q , if it has degree 1, then p is in C and fC (p) = {p, q }, otherwise q has degree 0, then p is in D. It is also immediate from the description that fA : A  LAST  , fB : B  D2 (S ) and fC : C  2P . We first show that D = . Assume, by contradiction that p  D for some p  D0 (S ). Then there is q  D0 (S ) such that q = N ext (p), therefore (p, q )  ES   E . ONLINE-MINADM assigned unique colors to each of p and q . Assume without loss of generality that q comes later than p in the input sequence. p is assigned a unique color, therefore it is the only element in its chain. Then w(p) is feasible for q . Then the algorithm should assign w(q ) = w(p), a contradiction. fA (p) = p, therefore it is a one-to-one function, i.e. |A|  |LAST  | = |PS  |. fB (p) = N ext (p). N ext is one-to-one, therefore fB is one-to-one, i.e. |B |  |D2 (S )| = d2 (S ). We will now show that the sets fC (p) are disjoint. Note that fC (p) = {p, q } / D0 (S ). Assume that fC (p)  fC (p ) = . Let fC (p) = where p  D0 (S ) and q  {p, q } and fC (p ) = {p , q }. Then either p = p or q = q . In the latter case q = N ext (p) = N ext (p ) = q , then p = p . In both cases, we have p = p . We conclude that if p = p , fC (p)  fC (p) = . As the sets fC (p), have exactly 2 elements, we conclude that |C |  N 2. We have d0 (S ) = |D0 (S )| = |A| + |B | + |C | + |D|  |PS  | + d2 (S ) + N 2 . Then (S ) =
def def

1 d0 (S ) - d2 (S ) - 2 |PS  |  . N 2

Substituting this in Lemma 2.1 and recalling that cost(S  )  N we get 1 7 1 3 Cost(S )  Cost(S  ) + N (1 + ) = Cost(S  ) + N  Cost(S  ). 2 2 4 4

442

M. Shalom, P.W.H. Wong, and S. Zaks

3.3

Analysis for Path Topology

Lemma 3.3 ONLINE-MINADM is 3 2 -competitive in path topology. Proof. Let V = {v1 , v2 , ...} be the nodes of the path from left to right, and i (resp. i ) be the set of paths having vi as their right (resp. left) endpoint. It is well known that the number of ADMs used by an optimal solution is i max {|i | , |i |}. In an optimal solution, at each node vi , exactly min {|i | , |i |} pairs of paths are assigned one color per pair. In fact these pairs constitute a maximum matching M Mi of the complete bipartite graph (i , i , i × i ). The solution saves |M Mi | = min {|i | , |i |} ADMs at node vi , in other words ES  = i M Mi . Note that every matching of a complete bipartite graph can be augmented to a maximum matching. Let S  be an optimal solution, such that the matching in each node is obtained by augmenting the matching done by S to a maximum matching, i.e. ES  ES  . We will now define a function f : (ES  \ ES )  ES . Let e = (pi , pj )  ES  \ ES . e  ES  = i M Mi . Let e  M Mk . Assume without loss of generality that i < j , i.e. path pi appears before pj in the input. As e  / ES , none of pi , pj are paired with any path at node vk . Therefore when pj appears in the input w(pi ) is feasible for pj , if it is not assigned color w(pi ), this can be only because it is assigned color w(pj ) = w(pi ), for some i < j . Let the common node of pj and pi be vk . Then e = (pj , pi )  ES  . We define f (e) = e . Note that e is defined uniquely because there can not be a third path except pj and pi getting the same color and ending at node vk . Necessarily k = k , because we know that pj is not paired at node vk . We claim that f is one-to-one. Assume, by contradiction that there is some e = e, such that f (e ) = e . Then e  ES  , therefore e  M Mk for some node vk . By the construction of f , k is the other endpoint of pi . Let e = (pi , pi ). By the discussion in the previous paragraph, symmetrically it follows that j < i , a contradiction. Therefore f is one-to-one, i.e.  |ES  | - |ES | = |ES  \ ES |  |ES |, thus |ES |  1 2 |ES |.  S | We conclude as follows. Cost(S ) - Cost(S ) = |ES  | - |ES |  |E2  N 2  Cost(S  ) , therefore: 2 3 Cost(S )  Cost(S  ). 2

4
4.1

Lower Bounds
General Topology

Lemma 4.1 There is no deterministic on-line algorithm with competitive ratio <7 4. Proof. Assume ALG is a deterministic on-line algorithm, with competitive ratio . We show that   7 4 . For colors we use numbers 1, 2, .... The color assigned to a lightpath a by ALG is denoted by w(a). We use the network depicted in

Optimal On-Line Colorings for Minimizing the Number of ADMs

443

A E F

B D G

C

A
y

B E
1 2

C

D G H

H

F

K

M
Fig. 2. Proof of Lemma 4.1

K

M

Figure 2. The first lightpath in the input is EFG. Without loss of generality, assume w(EF G) = 1. The second lightpath in the input is. First assume w(BDG) = 1. In this case if lightpath EABDG arrives, we have w(EABDG) = 2, then when lightpath GFEAB arrives we have w(GF EAB ) = 3. ALG thus uses 7 ADMs, while it is easy to see the an optimal solution can use only 4 ADMs, thus   7 4, a contradiction. Hence, w(BDG) = 2. When the third lightpath in the input y =BAE arrives The situation is as depicted in the right side of the figure. It is clear that w(y ) = 3, since otherwise 7  6 3 > 4 , a contradiction. Thus w(y ) = 1 or w(y ) = 2. ­ case a: w(y ) = 1 Let z =EFKMHG be the next lightpath in the input sequence. Clearly w(z ) = 1. Hence w(z ) = 2 or w(z ) = 3. If w(z ) = 2, when lightpaths GFEAB, EABDG, BDGFE and EABCDG arrive, we get w(GF EAB ) = 3, w(EABDG) 7 = 4, w(BDGF E ) = 5, w(EABCDG) = 6, and  = 14 8 = 4 , a contradiction. 7 In the case w(z ) = 3 for u=EABDCHG we have w(u) = 4, and   9 5 > 4, a contradiction. ­ case b: w(y ) = 2 Let z =BDCHG. Clearly w(z ) = 2. Hence w(z ) = 1 or w(z ) = 3. If w(z ) = 1, when lightpaths EABDG, GFEAB, GKFEAB, and EFGDB arrive, we have w(EABDG) = 3, w(GF EAB ) = 4, w(GKF EAB ) = 5, w(EF GDB ) = 6, 7 and   14 8 = 4 , a contradiction. In the case w(z ) = 3, for u=GHMKFEAB 7 we have w(u) = 4. Then   9 5 > 4 , a contradiction. 4.2 Ring Topology

The result in the previous Lemma can be proven, though asymptotically even for ring topologies. Lemma 4.2 No deterministic on-line algorithm has a competitive ratio better than 7/4, even for the ring topology. Sketch of Proof. We first give the inutitive ideas behind the adversary. Suppose we divide the ring into four segments R1 , R2 , R3 and R4 . The adversary first requests lightpaths R1 and R3 .

444

M. Shalom, P.W.H. Wong, and S. Zaks

­ If the on-line algorithm assigns the same color to them, we then request two lightpaths (R2 , R3 , R4 ) and (R4 , R1 , R2 ). The on-line algorithm uses 8 ADMs while the offline algorithm can use 4 ADMs. ­ If the on-line algorithm assigns different colors to them, we then request R2 . If the on-line algorithm assigns a third color to R2 , we further request R4 making the on-line algorithm using at least 7 ADMs and the offline algorithm using 4 ADMs only. The only problematic case for the adversary is that the on-line algorithm assigns R1 and R2 with the same color and R3 using a different color. In this case, the adversary requests two lightpaths (R2 , R3 , R4 ) and (R3 , R4 , R1 ). Neither of these can share ADMs with existing lightpaths. The on-line algorithm uses 7 ADMs plus 2 ADMs for R3 while the offline algorithm uses 4 ADMs plus 2 ADMs for R3 . The adversary then repeats the process for k times such that the on-line algorithm uses 7k + 2 ADMs and the offline algorithm uses 4k + 2 ADMs. This for any > 0. The crucial point in gives a competitive ratio at least 7 4 - repeating the process is to ensure later arrival lightpaths cannot share ADMs with lightpaths in previous iterations. This can be done by careful division of the ring and shifting of the division in every iterations. The details can be found in [SWZ07]. 4.3 Path Topology

Lemma 4.3 For any > 0, there is no ( 3 2 - )-competitive deterministic algorithm for path topology. Proof. We prove using the following adversary. Let G be a path with 2k nodes u1 , v1 , u2 , v2 , ..., uk , vk (see Figure 3). Let ALG be any deterministic algorithm. The value of k will be determined later.

c2
b '1
b1
u1
a1

v1

a2

u2

v2

u3

a3

v3

uk

ak
vk

Fig. 3. Proof of Lemma 4.3

The adversary works in two phases. In the first phase the input is a1 , a2 , ..., ak where i, ai = (ui , vi ). In the second phase the input depends on the decisions made by ALG during the first phase. For every 1  i < k , if w(ai ) = w(ai+1 ) then the input contains two paths bi = (u1 , ui+1 ) and bi = (vi , vk ), otherwise the input contains one path ci = (vi , ui+1 ).

Optimal On-Line Colorings for Minimizing the Number of ADMs

445

Let 0  x  k - 1 be the number of times w(ai ) = w(ai+1 ) is satisfied. Then w(ai ) = w(ai+1 ) is satisfied k - 1 - x times. During the first phase the algorithm uses 2k ADMs, one for each node. For the paths bi and bi , let  = w(ai )(= w(ai+1 )).  is not feasible neither for bi nor for bi . Then the algorithm assigns other colors to bi and bi , and it uses 4 ADMs, for a total of 4x ADMs. For the path ci , let  = w(ai ) and  = w(ai+1 )(= ), coloring ci with one of these colors ALG uses one ADM, otherwise it uses 2 ADMs. Therefore for the paths ci , ALG uses at least k - 1 - x ADMs. Summing up, we get that ALG uses at least 2k + 4x + (k - 1 - x) = 3(k + x) - 1 ADMs. On the other hand the following solution is possible. For any consecutive paths ci , ci+1 , ..., ci+j color such that w(bi-1 ) = w(ai ) = w(ci ) = w(ai+1 ) = w(ci+1 ) = ... = w(ci+j ) = w(ai+j +1 ) = w(bi+j +1 ). This solutions use 2k + 2x ADM's, one ADM at each ui , vi , x additional ADMs at u1 , and x additional ADMs at vk . k +x ) - 1 3 1 Therefore the competitive ratio of ALG is at least 3( 2(k+x) = 2 - 2(k+x)  3 1 1 > 0 we can choose k > 2 , so that the competitive ratio of 2 - 2k . For any ALG is bigger then 3 2 - .

5

Triangle Topology

In the previous sections we have shown that algorithm ONLINE-MINADM has an optimal competitive ratio, in general topologies, ring and path topologies. In this section we show an example of topology for which ONLINE-MINADM is not optimal. Note that the proof of Lemma 3.1 implies that ONLINE-MINADM is 7 4 -competitive in the triangle topology. We will show in this section a tight bound of 5 3 for this topology. Note that the lower bound proof for ring networks requires the ring to be of unbounded size. The proof will not hold for rings of a bounded size. In this section we show that this lower bound does not hold for triangles, and give an optimal algorithm for this topology. Lemma 5.1 There is no on-line algorithm with competitive ratio < angle topology.
5 3

for tri-

Proof. Consider a triangle with edge set {e1 , e2 , e3 }. We will use the following adversary. Release two lightpaths each of length 1, on edges e1 and e2 . If w(e1 ) = w(e2 ), then we continue as in Lemma 3.1, namely release two lightpaths of length 2 each {(e2 - e3), (e1 , e3 }, and we get a competitive ratio of 7/4 > 5/3. Otherwise w(e1 ) = w(e2 ), w.l.o.g. assume w(e1 ) = 1, w(e2 ) = 2. Release a / {1, 2} then the competitive ratio is 6/3 = 2 > lightpath on edge e3 . If w(e3 )  5/3, otherwise w.l.o.g w(e3 ) = 1. In this case we have w(e1 ) = w(e3 ) = 1 using 3 ADMs, w(e2 ) = 2 using 2 ADMs, for a total of 5 ADMs. The competitive ratio is 5/3. For the triangle topology, let us name the three edges in the triangle network e1 , e2 , and e3 . There are only six types of lightpaths, namely, (e1 ), (e2 ), (e3 ),

446

M. Shalom, P.W.H. Wong, and S. Zaks

(e1 , e2 ), (e2 , e3 ) and (e1 , e3 ). For any lightpath p, we say that p is length-i if it contains i edges. There are only length-1 and length-2 lightpaths in a triangle topology. We now present another algorithm ONLINE-TRIANGLE and show that it is 5/3-competitive for triangle topology. Roughly speaking, the algorithm gives highest priority to a pair of length-2 and length-1 lightpaths to share the same color whenever possible. For length-1 lightpaths, we have seen in the lower bound of ONLINE-MINADM in Lemma 3.1 that, if an on-line algorithm always colors two adjacent length-1 lightpaths with the same color, the competitive ratio of the algorithm is at least 7 4 . To overcome this barrier, when a length-1 lightpath, say pi = (e1 ), arrives, ONLINE-TRIANGLE does not always color pi with an adjacent length-1 lightpath using the same color. However, if we color three length-1 lightpaths on a cycle each with a different color, this will result in a competitive ratio of 2. Therefore, if there are two lightpaths pj = (e2 ) and pk = (e3 ) with different colors, then ONLINE-TRIANGLE should color pi with either of these colors if it is feasible. We formalize this concept by "marking" the three lightpaths to represent they are grouped together and should not be further considered when other length-1 lightpaths arrive. Formally, the algorithm runs as follows. When a request of lightpath pi with endpoints ui and vi arrives, 1. In case pi is length-2, ­ If there exists a length-1 (marked or unmarked) lightpath with color  with endpoints ui , vi , and  is feasible for pi , then assign w(pi ) = . ­ Otherwise, assign w(pi ) =  , where  is an unused color. 2. In case pi is length-1, ­ If there exists a length-2 lightpath with color  with endpoints ui , vi , and  is feasible for pi , then assign w(pi ) = . ­ Otherwise, if there exists a valid chain of two unmarked length-1 lightpaths with different colors 1 and 2 with endpoints ui , vi , and 1 or 2 is feasible for pi (w.l.o.g. assume 1 is feasible), then assign w(pi ) = 1 and mark all three lightpaths involved. ­ Otherwise, assign w(pi ) =  , where  is an unused color. For example, suppose P = {p1 , p2 , · · · , p7 } where pi is, in order, (e1 ), (e2 ), (e3 ), (e2 ), (e1 ), (e3 ), (e1 , e3 ). Then ONLINE-TRIANGLE will first assign w(p1 ) = 1 , w(p2 ) = 2 , w(p3 ) = 1 and mark all three p1 , p2 and p3 . Next, we assign w(p4 ) = 3 because there is no unmarked lightpath available. We further assign w(p5 ) = 4 and w(p6 ) = 3 . Finally, we assign w(p7 ) = 2 because p7 and p2 form a cycle. To analyze the performance of ONLINE-TRIANGLE, we first observe how lightpaths are colored in an optimal solution. The proof of the following lemma follows immediately from the definitions. Lemma 5.2 The optimal solution S  always colors (e1 , e2 ) and (e3 ) with the same color if possible and similarly for the two other symmetric cases. Any

Optimal On-Line Colorings for Minimizing the Number of ADMs

447

remaining length-2 lightpath is colored a distinct color. If there are some length1 lightpaths remained after this, cycles of three length-1 lightpaths are colored the same color; followed by chains of two length-1 lightpaths with same color and finally remaining length-1 lightpaths with distinct colors. It can be verified such coloring uses the minimum number of ADMs. We then compare S and S  as follows. We first give a rough idea before formally prove it in Lemma 5.3. Roughly speaking, in S , a length-2 lightpath can always share ADM with a length-1 lightpath unless the length-1 lightpath has been marked with the same color with some other length-1 lightpath. In this case, S  also has to use extra ADMs for these this length-1 lightpath, therefore, making S  use a comparable number ADMs as S in total. As mentioned before, ONLINETRIANGLE does not always color adjacent length-1 lightpaths using same color to avoid the 7 4 lower bound. Furthermore, there is no marked cycle of length-1 lightpaths all with different color; for any marked cycle, S  uses at least 3 ADMs for such cycle while S uses at most 5, which is indeed the worst case leading  to the 5 3 -competitive ratio. Also for the case S is able to color two length-1 lightpaths with the same color while S has to use two different colors, this only gives a ratio of 4 3 . Precisely, we prove the competitive ratio in the following lemma giving more details. Lemma 5.3 ONLINE-TRIANGLE is
5 3 -competitive

in the triangle topology.

Sketch of Proof. Consider the solution S , the lightpaths can be partitioned into five disjoint sets according to how they are colored. We start with defining the set A whose edges will not be included in later sets and similarly for other sets. Let A be the set of cycles containing a length-1 lightpath and a length-2 lightpath with the same color; B be the set of length-2 lightpaths with distinct color; C be the set of marked cycles containing two same colored length-1 lightpaths and a third different colored one (excluding those later share color with a length-2 lightpath and thus included in A); D be the set of marked chains containing two same colored length-1 lightpaths (excluding those in A or C ); and E be the set of remaining length-1 lightpaths. In the example given above, A contains p7 and p2 ; C contains p4 , p5 and p6 ; D contains p1 and p3 ; B and E are empty. We denote |A|, |B |, |C |, |D|, and |E | by a, b, c, d and e, respectively. Note that cost(S ) = 2a + 2b + 5c + 3d + 2e. We consider four cases depending on the set B . Case 1: B is empty, in other words, every length-2 lightpath is colored the same color as a length-1 lightpath; this is actually the same as in S  . For length-1 lightpaths, by Lemma 5.2, S  colors all possible cycle of 3 lightpaths in the same color using 3 ADMs, then chains of 2 lightpaths with same color using 3 ADMs, and finally 1 lightpath with its own color using 2 ADMs. S needs at most 5, 4 and 2 ADMs for each of S 5 the cases, respectively. Therefore, S   3. Case 2: B contains all three types of length-2 lightpaths . In this case, both C and E must be empty, otherwise, ONLINE-TRIANGLE would have colored some lightpath p in B with the same color as the corresponding lightpath in C or E , then p should be in A instead. In this case S  outperform S by grouping

448

M. Shalom, P.W.H. Wong, and S. Zaks

lightpaths in B with lightpaths in D. Even if so, there are still 2d - b length-1 lightpaths left unpaired in D. So S  uses at least 2a + 2b + 2d - b ADMs while S 3 S uses 2a + 2b + 3d ADMs. Then, S   2. In Case 3, B contains two types of length-2 lightpaths only; w.l.o.g., assume they are (e1 , e2 ) and (e2 , e3 ). In Case 4, B contains one type of length-2 lightpaths only; . For these two cases, we can employ a similar argument as in Cases S 5 1 and 2 and show that S   3 . The full details can be found in [SWZ07].

6

Conclusion and Possible Improvements

In this paper we presented an on-line algorithm with competitive ratio of 7 4 for any network topology, and proved that no algorithm has a competitive ratio better than 7 4 , even if the topology is a ring. We showed that the same algorithm has a competitive ratio of 3 2 in path topologies, and that this is also a lower bound for any on-line algorithm on this topology. The lower bound on ring topology does not hold when the ring is of a bounded size; we showed an optimal bound of 5 3 for the competitive ratio for the triangle topology, using a different algorithm. The analyses of the upper bounds, as well as those for the lower bounds, are all using a variety of proof techniques, which are of interest by their own, and which might prove helpful in future research on the topic. Our bounds pertain to deterministic on-line algorithms. It may be interesting to explore probabilistic algorithms and obtain similar bounds. Following our study, it might be interesting to determine the exact complexity of the on-line problem for tree topologies, as a function of some parameter of the tree, and of networks (e.g., rings or paths) of bounded size. An important extension is to consider the on-line version of the problem when grooming is allowed; in graphtheoretic terms, this amounts to coloring the paths so that at most g of them are crossing any edge, and where each ADM can serve up to g paths that come from at most two of its adjacent edges (see [GRS98, ZM03]). Another direction of extension is to the case where more involved switching functions are under consideration.

References
Borodin, A., El-Yaniv, R.: Online Computation and Competitive Analysis. Cambridge University Press, Cambridge (1998) [CFW02] C alinescu, G., Frieder, O., Wan, P.-J.: Minimizing electronic line terminals for automatic ring protection in general wdm optical networks. IEEE Journal of Selected Area on Communications 20(1), 183­189 (2002) [CW02] C alinescu, G., Wan, P.-J.: Traffic partition in wdm/sonet rings to minimize sonet adms. Journal of Combinatorial Optimization 6(4), 425­453 (2002) [EL04] Epstein, L., Levin, A.: Better bounds for minimizing sonet adms. In: 2nd Workshop on Approximation and Online Algorithms (September 2004) [EMZ02] Eilam, T., Moran, S., Zaks, S.: Lightpath arrangement in survivable rings to minimize the switching cost. IEEE Journal of Selected Area on Communications 20(1), 172­182 (2002) [BEY98]

Optimal On-Line Colorings for Minimizing the Number of ADMs

449

[FSZ06a] Flammini, M., Shalom, M., Zaks, S.: On minimizing the number of adms tight bounds for an algorithm without preprocessing. In: Erlebach, T. (ed.) CAAN 2006. LNCS, vol. 4235, Springer, Heidelberg (2006) [FSZ06b] Flammini, M., Shalom, M., Zaks, S.: On minimizing the number of adms in a general topology optical network. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 459­473. Springer, Heidelberg (2006) [GLS98] Gerstel, O., Lin, P., Sasaki, G.: Wavelength assignment in a wdm ring to minimize cost of embedded sonet rings. In: INFOCOM'98. 17th Annual Conference of the IEEE Computer and Communications Societies, pp. 69­ 77. IEEE Computer Society Press, Los Alamitos (1998) [GRS98] Gerstel, O., Ramaswami, R., Sasaki, G.: Cost effective traffic grooming in wdm rings. In: INFOCOM'98. 7th Annual Conference of the IEEE Computer and Communications Societies. IEEE Computer Society Press, Los Alamitos (1998) [LV98] Leonardi, S., Vitaletti, A.: Randomized lower bounds for online path coloring. In: 2nd International Workshop on Randomization and Approximation Techniques in Computer Science, pp. 232­247 (1998) [SWZ07] Shalom, M., Wong, P.W., Zaks, S.: Optimal on-line colorings for minimizing the number of adms in optical networks. In: Technion, Faculty of Computer Science, Technical Report CS-2007-14 (July 2007) [SZ04] Shalom, M., Zaks, S.: A 10/7 + approximation scheme for minimizing the number of adms in sonet rings. In: First Annual International Conference on Broadband Networks, pp. 254­262 (October 2004) [ZM03] Zhu, K., Mukherjee, B.: A review of traffic grooming in wdm optical networks: Architecture and challenges. Optical Networks Magazine 4(2), 55­64 (2003)

Efficient Transformations of Obstruction-Free Algorithms into Non-blocking Algorithms
Gadi Taubenfeld
The Interdisciplinary Center, P.O. Box 167, Herzliya 46150, Israel tgadi@idc.ac.il http://www.faculty.idc.ac.il/gadi/ Abstract. Three well studied progress conditions for implementing concurrent algorithms without locking are, obstruction-freedom, non-blocking and wait-freedom. Obstruction-freedom is weaker than non-blocking which, in turn, is weaker than wait-freedom. While obstruction-freedom and non-blocking have the potential to significantly improve the performance of concurrent applications, wait-freedom (although desirable) imposes too much overhead upon the implementation. In [5], Fich, Luchangco, Moir, and Shavit have presented an interesting transformation that converts any obstruction-free algorithm into a waitfree algorithm when analyzed in the unknown-bound semi-synchronous model. The FLMS transformation uses n atomic single-writer registers, n atomic multi-writer registers and a single fetch-and-increment object, where n is the number of processes. We define a time complexity measure for analyzing such transformations, and prove that the time complexity of the FLMS transformation is exponential in the number of processes n. This leads naturally to the question of whether the time and/or space complexity of the FLMS transformation can be improved by relaxing the wait-freedom progress condition. We present several efficient transformations that convert any obstruction-free algorithm into a non-blocking algorithm when analyzed in the unknown-bound semi-synchronous model. All our transformations have O(1) time complexity. One transformation uses n atomic singlewriter registers and a single compare-and-swap object; another transformation uses only a single compare-and-swap object which is assumed to support also a read operation.

1
1.1

Introduction
Motivation

Three well studied progress conditions for implementing concurrent algorithms without locking are, obstruction-freedom, non-blocking and wait-freedom. An algorithm is wait-free if it guarantees that every process will always be able to complete its pending operations in a finite number of its own steps. An algorithm is non-blocking if it guarantees that some process will always be able to complete its pending operation in a finite number of its own steps. An algorithm is obstruction-free if it guarantees that a process will be able to complete its pending operations in a finite number of its own steps, if all the other processes "hold still" long enough (that is, in the absence of interference from other processes).
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 450­464, 2007. c Springer-Verlag Berlin Heidelberg 2007

Efficient Transformations of Obstruction-Free Algorithms

451

Clearly, obstruction-freedom is weaker than non-blocking which, in turn, is weaker than wait-freedom. The term lock-free algorithms refers to algorithms that do not use locking in any way. Wait-free, non-blocking and obstruction-free algorithms are by definition lock-free algorithms.1 Advantages of using lock-free algorithms are that they are not subject to deadlocks or priority inversion, they are resilient to process crash failures (no data corruption on process failure), and they do not suffer significant performance degradation from scheduling preemption, page faults or cache misses. While non-blocking and obstruction-freedom have the potential to significantly improve the performance of concurrent applications, and can be used in place of using locks in various cases, wait-free synchronization (although desirable) imposes too much overhead upon the implementation. Wait-free algorithms are often very complex and memory consuming, and hence considered less practical than non-blocking algorithms. Furthermore, starvation can be sometimes efficiently handled by collision avoidance techniques such as exponential backoff. Requiring implementations to satisfy only obstruction-freedom can significantly simplify the design of concurrent algorithms, as it eliminates the need to ensure progress under contention. However, since obstruction-free algorithms do not guarantee progress under contention, they may suffer from livelocks. Various contention management techniques have been proposed to efficiently improve progress of obstruction-free algorithms under contention. Existing lock-free contention managers, which allow processes to run without interference long enough until they can complete their operations, do not provide full guarantee to ensure progress in all cases. While obstruction-free algorithms are easier to design and are efficient in various cases, it is most desirable that a lock-free implementation do satisfy the stronger non-blocking progress condition. Hence the importance of designing efficient transformations that automatically convert any obstruction-free algorithm into a non-blocking algorithm. Such transformations should not affect the behaviour of the original (obstruction-free) algorithm in uncontended cases, or in executions where the contention management technique used is effective. The focus of this paper is on the design of such transformations in the unknown-bound semi-synchronous model, where it is assumed that there is an unknown upper bound on memory access time. All practical systems satisfy the unknown-bound assumption. 1.2 Results

In [5], Fich, Luchangco, Moir, and Shavit have presented an interesting transformation that converts any obstruction-free algorithm into a wait-free algorithm when analyzed in the unknown-bound semi-synchronous model. The FLMS transforma1

In the literature, the terms lock-free and non-blocking are sometimes used as synonymous, or even with opposite meaning to the way they are defined here. As suggested in [16], it is useful to distinguish between algorithms that do not require locking (i.e., lock-free algorithms) and those that actually satisfy the non-blocking progress condition.

452

G. Taubenfeld

tion uses n atomic single-writer registers, n atomic multi-writer registers and a single fetch-and-increment object. We start by defining a time complexity measure for analyzing such transformations, and prove that the time complexity of the FLMS transformation is exponential in the number of processes n. Then, we present several efficient transformations that convert any obstruction-free algorithm into a non-blocking algorithm when analyzed in the unknown-bound semi-synchronous model. All our transformations have O(1) time complexity. One of transformation uses n atomic single-writer registers and a single compare-and-swap object; another transformation uses only a single compare-and-swap object which is assumed to support also a read operation. 1.3 Related Work

Our work is based on the transformation presented in [5]. A comprehensive discussion of wait-free synchronization is given in [8]. In [11], the concept of a non-blocking data structure is introduced. The notion of obstruction-freedom is introduced in [9]. Contention management is discussed in [6,10,13]. The importance of the unknown-bound semi-synchronous model in the context of shared memory systems was first investigated in [1]. In [12,14], indulgent algorithm are investigated in semi-synchronous shared memory systems. The interested reader will find in [15] a pedagogical description of several families of semi-synchronous and timing-based algorithms. Message-passing algorithms for partially synchronous systems were presented in various papers [3,4]. In [7], the weakest failure detectors that allow boosting an obstruction-free implementation into a wait-free or a non-blocking implementation have recently been identified (eventual prefect failures detector [2] is the weakest to implement a wait-free contention manager, and   is the weakest to implement a nonblocking contention manager).

2

The Computational Model

The system is made up of n processes, denoted p1 , . . . , pn , which communicate via shared objects. It is assumed that any number of processes may crash. A process that crashes stops its execution in a definitive manner. The possibility and complexity of synchronization in a distributed environment depends heavily on timing assumptions. We focus on a semi-synchronous shared-memory model of computation which provides a practical abstraction of the timing details of concurrent systems. In this model, it is assumed that there is an unknown upper bound on the time it takes a process to execute one step and, in particular, on the time it takes to execute a step which involves access the shared memory. This assumption is inherently different from the asynchronous model where no such bound exists. In the semi-synchronous model a process can delay itself explicitly by executing a statement delay (d), for some constant d. Executing the statement delay(d) by a process p delays p for at least d time units before it can continue, and

Efficient Transformations of Obstruction-Free Algorithms

453

there is some (unknown) upper bound (as a function of d) on the time a correct process can be delayed (for example, this bound can be 2d). A key idea in designing algorithms for the semi-synchronous model is that a process can delay itself for increasingly longer periods, and by doing so it can ensure that eventually other processes will take "enough" steps during one of these waiting periods. The appeal of the semi-synchronous model lies in the fact that while it abstracts from implementation details, it is a better approximation of real concurrent systems compared to the asynchronous model, as all practical (shared memory) systems satisfy the unknown-bound assumption. Furthermore, it enables to obtain more efficient solutions. We point out that the semi-synchronous model, as defined here, is similar (but not identical) to a model where it is assumed that there is an unknown bound on the ratio of the maximum time and minimum time between the steps of the processes. That is, some unknown bound exists on the relative execution rates of any two processes in the system. In such a model, the delay statement is simply implemented by counting steps. All our results and algorithms apply also to this variant of the unknown bound semi-synchronous model. Lock-free algorithms usually require the use of powerful atomic operations such as compare-and-swap (CAS). A CAS operation takes three parameters: a shared register r, and two values: old and new. If the current value of the register r is equal to old, then the value of r is set to new and the value true is returned; otherwise r is left unchanged and the value false is returned. We consider three types of shared objects: (1) Atomic register ­ a shared register that supports atomic read and write operations; (2) Compare-and-swap object ­ a shared object that supports an atomic CAS operation; (3) Compareand-swap/read object ­ a shared object that supports both atomic CAS and atomic read operations. The FLMS transformation, and our transformations are all black box transformations: A transformation does not change anything in the original obstructionfree algorithm, it only adds code to ensure that a stronger progress condition is satisfied. Thus, a transformed algorithm performs the original algorithm on the original shared objects and does not apply any other steps to these objects. Below we define a time complexity measure for analyzing such transformations. Let T be a transformation that converts an arbitrary obstruction-free algorithm, denoted ALG, into a non-blocking or a wait-free algorithm. Enabled Process: A process is enabled in a given finite run of transformation T if its next step is a step of the original obstruction-free algorithm ALG, or if its last step was a step of ALG. Being enabled corresponds to holding a lock (i.e., being in the critical section) in lock-based algorithms. We notice that being enabled is not exactly like being in a critical section since exclusiveness is not guaranteed. Time Complexity: The time complexity of transformation T, is the maximum number of steps which involve access to the shared memory that a process may

454

G. Taubenfeld

need to take until it becomes enabled since the last time some process has been enabled. Or, if no process has been enabled yet, since the beginning of the execution. This definition corresponds to the way time complexity is usually defined in lock-based (mutual exclusion) algorithms. In lock-based algorithms, time complexity is usually measured by counting the amount of "work" (time units) a winning process, a process that gets to enter its critical section, may need to do (wait) since the last time some process has released its critical section. Since all our algorithms require very few accesses to shared memory locations, the definition does not distinguish between different types of shared memory accesses. In a different context, it would make sense to distinguish between relatively cheap operations like reads and writes to more expensive operations like compare-and-swap. Remark: The new complexity measure is a special case of the following more general new complexity measure for synchronization algorithms, which might be interesting in its own right. Given an algorithm, denoted SYNC, let us divide its steps into three disjoint groups, 1. group A ­ the group of synchronization steps; 2. group B ­ the group of real work steps; 3. group C ­ the group of inexpensive steps. In mutual exclusion algorithms, A may include the steps in the entry section, B the steps in the critical section, and C all other steps. For a transformation that converts an arbitrary obstruction-free algorithm, ALG, into a non-blocking or a wait-free algorithm, A may include all the step which involve access to the shared memory, B all the step of ALG, and C all other steps. A process is enabled in a given finite run of SYNC if its next step is a step from B or if its last step was a step from B . Next we define two possible complexity measures, 1. The maximum number of steps from group A that a process may need to take until it becomes enabled since the last time some process has been enabled. Or, if no process has been enabled yet, since the beginning of the execution. 2. The longest time interval where no process is enabled, assuming there is an upper bound of one time unit for step time and no lower bound. The first measure generalizes the one used in this paper, the second measure is called system response time in the context of mutual exclusion algorithms. Other variants of these measures can be obtained by generalizing corresponding measures for lock-based algorithms (see [15], Section 1.4).

3

The Time Complexity of the FLMS Transformation

We prove that the time complexity of the FLMS transformation is at least exponential in the number of processes n. This exponential bound holds even when

Efficient Transformations of Obstruction-Free Algorithms

455

the transformation is executed in a fault-free environment. As already mentioned, the transformation converts any obstruction-free algorithm into a wait-free algorithm when analyzed in the unknown-bound semi-synchronous model, and uses n atomic single-writer registers, n atomic multi-writer registers and a single atomic fetch-and-increment object. In the following, we describe only the part of the FLMS transformation that is needed for proving the time complexity bound. When a process pi notices that there is contention, it begins to participate in a strategy to ensure progress, called the panic mode. This strategy is as follows: using an atomic fetch-and-increment object, process pi first acquires a timestamp, and initializes an atomic multiwriter register, denoted T [i], with the value of its timestamp. Then pi searches for the minimum timestamp by scanning the array T [1..n]. (Initially all entries of the T array are set to .) During the search, all timestamps that are not , but are larger than the minimum timestamp pi has observed so far, are replaced by . If process pi determines that it has the minimum timestamp then pi becomes enabled. If process pi determines that some other process, say pk , has the minimum timestamp, pi waits for some time (the amount of time waited is not relevant here), and then checks the status of pk . If pi does not notice (after checking some shared register) that pk has taken steps while pi was waiting, pi overwrites pk 's timestamp by setting T [k ] to . Then pi restarts executing the strategy to ensure progress (i.e., go to the beginning of the panic mode) using its original timestamp (i.e., pi uses the same timestamp from the previous round). Similarly, if after it waits, pi notices that T [k ] = , then pi also restarts executing the strategy to ensure progress using its original timestamp. The above partial description of the FLMS transformation is sufficient for proving its exponential time complexity. Theorem 1. The time complexity of the FLMS transformation is exponential, in the number of processes n. Proof. Consider a finite run  where: (1) all the n processes have just started to execute the strategy to ensure progress (i.e, the panic mode); (2) each process has chosen a timestamp such that process pi has chosen timestamp i, for all i  {1, ..., n}; and (3) all the entries of the T array are still set to their initial value . For every i  {1, ..., n - 1}, let Ri denotes the maximum number of times, that process pn has to scan the array T [1..n] starting from (the end of) run  before pn becomes the first enabled process, assuming that only the i + 1 processes pn ,...,pn-i may take steps in an extension of  . We prove by induction that Ri  2i for every i  {1, ..., n - 1}, which would imply that the time complexity is of order 2n-1 × n. Actually, we prove by induction the following (stronger) claim:

456

G. Taubenfeld

For every i  {1, ..., n - 1}, there is an extension i of  where: 1. pn has performed 2i scans of T in i . 2. Only the i + 1 processes pn ,...,pn-i have taken steps in the extension i of  . 3. pn is enabled in i , and no process is enabled in any strict prefix of i which extends  . 4. For every j  {1, ..., n - 1}, process pj is (again) at the beginning of the (code of the) panic mode in i , with timestamp j and T [j ] = . 5. In the last steps in i process pn has scanned the array T [1..n]. We  denote by i the prefix of i which result from omitting this last single scan of T by pn . We notice that the existence of run i implies that Ri  2i . When i = 1, 1 is constructed as follows: we first let process pn-1 set T [n - 1] to n - 1. Then, we run pn alone. Process pn searches for the minimum timestamp by scanning the array T [1..n] once, and determines that process pn-1 has the minimum timestamp. Then, process pn delays itself for some time and then checks the status of pn-1 . Since pn-1 has taken no steps while pn was waiting, pn overwrites pn-1 's timestamp by setting T [n - 1] to . Then pn restarts executing the strategy to ensure progress (i.e., go to the beginning of the panic mode) using its original timestamp. Next, pn sets T [n] to n, searches again for the minimum timestamp by scanning T , determines that it has the minimum timestamp and becomes enabled. Since pn has scanned T twice in 1 , we get, R1  21 . (1)

When i = 2, 2 is constructed as follows: we first repeat the extension from the previous case and stop just before the last scan on T by pn (i.e., the extension  1 ). Then we let process pn-2 set T [n - 2] to n - 2, and let both pn-1 and pn scan T (notice that so far the number of scans of pn equals 21 as in 1 ). Both determine that process pn-2 has the minimum timestamp, each one delays itself for some time and then checks the status of pn-2 . Since pn-2 has taken no steps while pn-1 and pn were waiting, they overwrite pn-2 's timestamp by setting T [n - 2] to . Then they restart executing the strategy to ensure progress. At this point, from process pn-1 and process pn point of view, they are back at a situation similar to the one at run  . So, we repeat the construction from the previous case of i = 1 (in which the number of scans of pn equals 21 ). Since pn has scanned T four times in 2 , we get, R2  22 . (2)

Induction hypothesis: we assume that a run i-1 exists and prove that run i exists. We consider now the general case where i + 1 processes participate. We first repeat the extension from the case when only i processes participate and stop  just before the last scan on T by pn (i.e., the extension i -1 ). Then we let process pn-i set T [n - i] to n - i, and let the i processes pn-i+1 through pn

Efficient Transformations of Obstruction-Free Algorithms

457

scan T (notice that so far the number of scans by pn equals 2i-1 as in i-1 ). All the processes determine that process pn-2 has the minimum timestamp, they delay themselves for some time and then check the status of pn-i . Since pn-i has taken no more steps, they overwrite pn-i 's timestamp by setting T [n - i] to . Then they restart executing the strategy to ensure progress. At this point, from processes pn-i+1 through pn point of views, they are back at a situation similar to run  . So, we repeat the construction from the i - 1 case in which only i processes participate (in which the number of scans of pn equals 2i-1 ). Since pn has scanned T 2 × 2i-1 times in i , we get, Ri  2i . (3)

Thus, from the construction of run n-1 where all the n processes participate, we get, Rn-1  2n-1 . (4) We have proved that there is an extension of  where the number times, that process pn has to scan the array T [1..n] before it becomes the first enabled process is at least 2n-1 . Each such scan involves n accesses to shared memory location. Thus, we conclude that the time complexity the FLMS transformation is at least of order 2n-1 × n.

4

The Main Transformation

We now present our main transformation. It has O(1) time complexity, and uses n atomic single-writer registers and a single compare-and-swap object which supports also a read operation. The other transformations, presented later, are variants of this transformation. One important strength of all the transformations is their simplicity. To understand how the transformation works, let us start by assuming a fault-free model in which no process ever crashes. In such a model, we can design a simple transformation by using a single (mutual exclusion) lock. To avoid interference between different operations, a process performs steps of the original obstruction-free algorithm, denoted ALG, only inside its critical section (after it has acquired the lock), within which the process is guaranteed exclusive access with no interference to the original algorithm shared objects. Using a single lock to prevent interference between different operations of ALG may degrade the performance, as it enforces processes to wait for a lock to be released, and thus, does not allow several processes with non-interfering operations to proceed concurrently. Furthermore, when there is no contention, acquiring the lock introduces additional overhead. To overcome these limitations, before a process tries to acquire the lock, it first tries to complete its operation of ALG without holding the lock. If there is no contention or if the contention manager is effective the process will complete its operation without any overhead. Otherwise if the process, after taking many steps, does not succeed in completing its operation, it tries to acquire the lock.

458

G. Taubenfeld

Of course, as a result of such an approach, a process that is already holding the lock may experience interference. However, either some process will manage to complete its operation (without holding the lock), or this interference will vanish after some finite time. Going back to our original model where processes may crash, using locks is problematic as a process may crash while holding the lock, preventing all other processes from ever completing their operations. Resolving this problem, is the main difficulty in designing efficient transformations, and is done as follows: The winner ­ the process that is currently holding the lock ­ is required to increment a (single-writer) counter, denoted W [winner ] every few steps, of ALG. A process p that fails to acquire the lock, reads the value of the winner's counter and delays itself for W [winner ] time units. Then, p checks W [winner ] again, and if the value was updated p delays itself again, and so on. Otherwise, if W [winner ] has not been changed, p assumes that the winner has crashed and releases the lock. Releasing the lock by a process p, which is not the winner, is a very delicate issue, since the winner might be alive but very slow, and as a result: (1) the winner will notice that the lock has been released although it is interested in holding it further; (2) we might end up with two or more processes holding the lock at the same time; and (3) process p might be suspended just before releasing the lock, and may release the lock at some unexpected time later on. We address these problems as follows: when a winner process, say pi , notices that the lock has been released pi tries to acquire the lock again. However, before doing so, pi waits long enough so that other processes that have mistakenly concluded that pi has crashed, will have enough time to release the lock (again) before pi tries to acquire it again. Ensuring that eventually at most one correct process will hold the lock, has to do with the fact that the value of the counter of a winning process W [winner ] keeps on increasing over time. Thus, forcing processes that fail to acquires the lock to delay themselves for increasingly longer periods, and eventually ­ by the unknown-bound assumption, the waiting time is long enough to guarantee that only one process will hold the lock and that some process will complete its operation of ALG. The code of our main transformation, Transformation 1, is given in Figure 1. Transformation 1 converts an arbitrary obstruction-free algorithm, denoted ALG, which may include a contention manager, into a non-blocking algorithm. Process pi first tries to execute X steps (for some predetermined constant X ) of the original obstruction-free algorithm ALG (line 1). If pi succeeds to complete its operation, it returns (line 2), otherwise pi tries to acquire the lock. The lock is implemented by a compare-and-swap object, named T . T = 0 means that the lock is free, T = i means that process pi has acquired the lock. So process pi tries to acquire the lock by setting the value of T to i (line 5). If pi succeeds it tries to complete its operation by taking steps of the original algorithm ALG (lines 6 ­ 12). Every X such steps pi increments its counter W [i] by 1. It continues doing so until it either completes its operation and releases the lock (line 9) or finds that it is no longer holding the lock (line 12).

Efficient Transformations of Obstruction-Free Algorithms

459

shared T : CAS/read object, initially 0 /* "the lock" */ W [1..n]: array of atomic single-writer registers /* initial values immaterial */ local /* initial values immaterial */ winner : ranges over {0, ..., n}; wait : integer; b : boolean invoke(op ) 1 execute up to X steps of ALG /* ALG is the original algorithm 2 if op is completed then return response fi 3 W [i] := 1 /* contention possible ­ set initial delay 4 repeat /* tries to execute op without interference 5 if CAS (T, 0, i) then /* tries to acquires the "lock" 6 repeat /* pi is enabled 7 execute up to X steps of ALG /* original algorithm 8 if op is completed then 9 CAS (T, i, 0) /* release "lock" 10 return response 11 else W [i] := W [i] + 1 fi /* increase delay 12 until read (T ) = i /* equivalent to ¬CAS (T, i, i) 13 delay (2 × W [i]) /* flash out processes waiting in lines 16­22 14 else /* loser 15 winner := read (T ) /* tricky to imp. efficiently using CAS only 16 if winner = 0 then /* "lock" is captured by winner 17 repeat /* wait for the winner to proceed 18 wait := W [winner ] /* delay time 19 delay (wait ) /* wait as requested 20 b := read (T ) = winner /* b := CAS (T, winner , winner ) 21 until wait = W [winner ]  ¬b /* winner crashed? 22 if wait = W [winner ]  b then CAS (T, winner , 0) fi fi fi /*release 23 until op is completed Fig. 1. Transformation 1. Program for process pi which invokes operation op. */ */ */ */ */ */ */ */ */ */ */ */ */ */ */ */ */ */ */

In line 13, process pi delays itself, so that other processes that may have concluded that pi has crashed (lines 16­21), will have enough time to release the lock (line 22) before pi tries to acquire the lock again. After executing the delay (line 13), process pi tries to acquire the lock again. We notice that pi may decrease the value of W [i] only after it completes its operation of ALG. If pi fails to acquire the lock (line 5), it executes the code at lines 15 ­ 22. It finds out the identity of the winner (line 15), and waits for W [winner ] time units. Then, pi checks W [winner ] again, and if the value has been updated (meaning the winner is alive) pi delays itself again, and so on. pi does so until it notices that either W [winner ] has not been changed or that the lock has been released (line 21). If W [winner ] has not been changed, pi assumes that the winner has crashed, releases the lock (line 22), and tries to acquire the lock (line 5). Theorem 2. Transformation 1 converts any obstruction-free algorithm into a non-blocking algorithm when analyzed in the unknown-bound semi-synchronous model.

460

G. Taubenfeld

Proof. Assume to the contrary that there exists an obstruction-free algorithm, ALG, such that the transformation does not convert ALG into a non-blocking algorithm when analyzed in the unknown-bound semi-synchronous model. Thus, there exists a suffix, 0 , of an infinite run  in which (1) no process succeeds to complete an operation of ALG ; and (2) no process executes line 1 or line 2. Let P denotes the set of all correct processes that do no succeed to complete their operations in 0 . Clearly, there must be at least one process pi  P , which succeeds to capture the "lock" in line 5 infinitely often (that is, its compare-and-swap operation in line 5 is successful infinitely often) and hence it executes the repeat loop in lines 6­12 infinitely often. This implies that the value of W [i] grows without bound in 0 . Thus, there exists a suffix 1 of 0 , in which the value of W [i] is big enough such that immediately after pi executes the delay statement delay (2 × W [i]) in line 13, no correct process is in the middle of executing any of the lines 16 ­ 22 while having its local variable winner set to i. In particular, no process can successfully execute the statement CAS (T, i, 0) in line 22 (without pi taking further steps). Let us denote by r an upper bound on the number of time units required for pi to go through the repeat loop at lines 6 ­ 12 once regardless of the activity of the other processes (such a bound exists by the properties of the unknownbound semi-synchronous model). Let 2 be a suffix of 1 where (1) pi succeeds in capturing the "lock" in line 5, and starts executing the repeat loop at lines 6 ­ 12, (2) W [i]  r, and (3) no correct process is in the middle of executing any of the lines 16 ­ 22 while having its local variable winner set to i. Clearly, in 2 , no process pj will ever be able to successfully executes the statement CAS (T, i, 0) in line 22, because each time pj will execute the delay statement in line 19, pi will go through the loop at least once and increment W [i]. Thus, (1) from that point on the value of T forever equals i, and (2) process pi will never leave the repeat loop at lines 6 ­ 12. Thus, in 2 , every other process that is in the middle of executing the repeat loop at lines 6 ­ 12 will eventually execute line 12 and exits the repeat loop. Thus, there exists a suffix 3 of 2 where pi forever executes the loop at lines 6 ­ 12 alone. This implies that in 3 processes pi will execute its operation on ALG continuously without interference and hence this operation must eventually be completed. A contradiction Theorem 3. Transformation 1 has O(1) time complexity, and uses n atomic single-writer registers and one CAS/read object. Proof. Assume that process pi becomes enabled. Lets examine what is the maximum number of steps which involve access to the shared memory that pi may need to take until it becomes enabled since the last time some process has been enabled. Process pi can becomes enabled in one of three ways: (1) when it starts it execution (line 1); (2) immediately after it succeeds to set the value of the CAS object T to its id (line 5), and (3) starting to execute another round of the repeat loop after executing lines 11 and 12. Option 1 requires 0 steps by pi . Option 3 requires 2 steps by pi since pi was last enabled. So, lets assume that p becomes enabled as a result of option 2. Process pi succeeds in setting the value of T to its id (line 5) only when T = 0. As soon as the value of T is 0 it will

Efficient Transformations of Obstruction-Free Algorithms

461

take pi at most 5 steps (which involve access to the shared memory) to reach line 5. Finally, if T = j and process pj crashes or is slow, it will take pi or some other process at most 8 steps until they set T to 0 in line 22. The result about the space complexity is obvious.

5

Transformation 2: Using a CAS Object with n Atomic Registers

Our second transformation is a modified version of Transformation 1, in which the three read (T ) operations from Transformation 1 (in lines 12, 15, and 20), are
shared T : CAS object, initially 0 /* "the lock" */ W [1..n]: array of atomic single-writer registers /* initial values immaterial */ local /* initial values immaterial */ winner : ranges over {-1, 0, ..., n}; wait , t: integer; b : boolean invoke(op ) 1 execute up to X steps of ALG /* ALG is the original algorithm */ 2 if op is completed then return response fi 3 W [i] := 1 /* contention possible ­ set initial delay */ 4 repeat /* tries to execute op without interference */ 5 if CAS (T, 0, i) then /* tries to acquires the "lock" */ 6 repeat /* pi is enabled */ 7 execute up to X steps of ALG /* original algorithm */ 8 if op is completed then 9 CAS (T, i, 0) /* release "lock" */ 10 return response 11 else W [i] := W [i] + 1 fi /* increase delay */ 12 until ¬CAS (T, i, i) 13 delay (2 × W [i]) /* flash out processes waiting in line 22 */ 14 else /* loser */ 15.1 j := 0; winner := -1 15.2 repeat /* find the winner's id*/ 15.3 if j (mod n) + 1 = i then j := j (mod n) + 1 else j := j + 1 (mod n) + 1 fi 15.4 if CAS (T, j, j ) then winner := j fi /* is j the winner? */ /* "lock" is released ? */ 15.5 if CAS (T, 0, 0) then winner := 0 fi 15.6 until winner = -1 /* winner found */ 16 if winner = 0 then /* "lock" is captured by winner */ 17 repeat /* wait for the winner to proceed */ 18 wait := W [winner ] /* delay time */ 19 delay (wait ) /* wait as requested */ 20 b := CAS (T, winner , winner ) 21 until wait = W [winner ]  ¬b /* winner crashed? */ 22 if wait = W [winner ]  b then CAS (T, winner , 0) fi fi fi /*release */ 23 until op is completed Fig. 2. Transformation 2. Program for process pi which invokes operation op.

462

G. Taubenfeld

type lock : record {id : integer ; W [1..n]: array of integers} shared T : CAS/read object of type lock , initially T.id = 0 /* "the lock" */ local /* initial values immaterial */ temp , temp 1 , temp 2 : of type lock ; winner , wait : integer; b : boolean invoke(op ) 1 execute up to X steps of ALG /* ALG is the original algorithm */ 2 if op is completed then return response fi 3 setW (1) /* set W [i] to 1 */ 4 repeat /* tries to execute op without interference */ 5 if setTid (0, i) then /* tries to acquires the "lock" */ 6 repeat /* pi is enabled */ 7 execute up to X steps of ALG /* original algorithm */ 8 if op is completed then 9 setTid (i, 0) /* release "lock" */ 10 return response 11 else temp := read (T ); setW (temp .W [i] + 1) fi /* increment W [i] */ 12 until temp .id = i /* until pi does not hold the lock */ 13 delay (2 × temp .W [i]) /* flash out processes waiting in lines 16­22 */ 14 else /* loser */ 15 temp := read (T ); winner := temp .id 16 if winner = 0 then /* "lock" is captured by winner */ 17 repeat /* wait for the winner to proceed */ 18 wait := temp .W [winner ] /* delay time */ 19 delay (wait ) /* wait as requested */ 20 temp := read (T ); b := temp .id = winner 21 until wait = temp .W [winner ]  ¬b /* winner crashed? */ 22 if wait = temp .W [winner ]  b then setTid (winner , 0) fi fi fi/*rel.*/ 23 until op is completed function setW (val : integer) 1 repeat 2 temp 1 := read (T ); temp 2 := temp 1 ; temp 2 .W [i] = val 3 until CAS (T, temp 1 , temp 2 ) end /* W [i] := val */

function setTid (old :integer, new :integer) return: boolean /* CAS (T, old , new ) */ 1 temp 1 := read (T ); b := false 2 while temp 1 .id = old do 3 temp 2 := temp 1 ; temp 2 .id = new 4 b := CAS (T, temp 1 , temp 2 ) 5 temp 1 := read (T ) od 6 return(b ) end Fig. 3. Transformation 3. Program for process pi which invokes operation op.

Efficient Transformations of Obstruction-Free Algorithms

463

implemented without using an implicit read operations of T . Transformation 2 has O(1) time complexity, and uses n atomic single-writer registers and a single compare-and-swap object (which does not support a read operation). The code of Transformation 2, is given in Figure 2. The read (T ) operations in lines 12 and 20 are easy to implement, as we are only interested in knowing whether the value of T equals some specific value. Implementing the read (T ) operation in line 15, while preserving the O(1) time complexity of the transformation is slightly more complicated. The easiest way to implement read (T ) is to check, for each value i  {0, ..., n}, whether the operation CAS (T, i, i) returns true. However, such an implementation would increase time complexity of the transformation to O(n). This can be easily fixed. First, we observe that as long as the value of T (in Transformation 1) is different from 0, some process is enabled; and that only steps that are taken while no process is enabled are counted. Thus, after each time we check whether the value of T equals i for i = 0 (by executing CAS (T, i, i)) we check whether the value of T equals 0 (by executing CAS (T, 0, 0)). The final implementation can be seen in lines 15.1 to 15.6. Correctness follows from that of Transformation 1.

6

Transformation 3: Using a Single CAS/Read Object

Our third transformation is also a modified version of Transformation 1, in which the values of the atomic registers W [1..n] are encoded as part of the state of the CAS/read object T . Transformation 3 has O(1) time complexity, and uses a single compare-and-swap/read object (with no atomic registers). Using only a single shared object may degrade the performance, as it forces all processes to reference the same shared memory location. Thus, under contention, the average waiting time to access the shared object would be high. The code of Transformation 3, is given in Figure 3. The correctness of Transformation 3 follows from that of Transformation 1.

7

Discussion

We have introduced a new complexity measure and presented three transformations which are shown very efficient according to this measure. The transformations convert any obstruction-free algorithm into a non-blocking algorithm when analyzed in the unknown-bound semi-synchronous model. As we have shown, the FLMS transformation has exponential time complexity. It is an open question whether achieving wait-freedom must require exponential time complexity when using only atomic registers and fetch-and-increment objects. It would be interesting to find tight bounds also when using other base objects. In particular, in what cases obstruction-free to non-blocking transformations have better time complexity than obstruction-free to wait-free transformations? Are transformations to wait-free implementations are inherently expensive?

464

G. Taubenfeld

References
1. Alur, R., Attiya, H., Taubenfeld, G.: Time-adaptive algorithms for synchronization. SIAM Journal on Computing 26(2), 539­556 (1997) 2. Chandra, T.D., Toueg, S.: Unreliable failure detectors for reliable distributed systems. Journal of the ACM 43(2), 225­267 (1996) 3. Dolev, D., Dwork, C., Stockmeyer, L.: On the minimal synchronism needed for distributed consensus. Journal of the ACM 34(1), 77­97 (1987) 4. Dwork, C., Lynch, N., Stockmeyer, L.: Consensus in the presence of partial synchrony. Journal of the ACM 35(2), 288­323 (1988) 5. Fich, E.F., Luchangco, V., Moir, M., Shavit, N.: Obstruction-free algorithms can be practically wait-free. In: Fraigniaud, P. (ed.) DISC 2005. LNCS, vol. 3724, pp. 78­92. Springer, Heidelberg (2005) 6. Guerraoui, R., Herlihy, M., Pochon, B.: Towards a theory of transactional contention managers. In: Proc. 24th Symposium on Principles of Distributed Computing, pp. 258­264 (2005) 7. Guerraoui, R., Kapalka, M., Kouznetsov, P.: The weakest failure detectors to boost obstruction-freedom. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 376­390. Springer, Heidelberg (2006) 8. Herlihy, M.P.: Wait-free synchronization. ACM Trans. on Programming Languages and Systems 13(1), 124­149 (1991) 9. Herlihy, M.P., Luchangco, V., Moir, M.: Obstruction-free synchronization: Doubleended queues as an example. In: Proc. of the 23rd International Conf. on Dist. Computing Systems, p. 522 (2003) 10. Herlihy, M.P., Luchangco, V., Moir, M., Scherer III, W.N.: Software transactional memory for dynamic-sized data structures. In: Proc. 22nd ACM Symp. on Principles of Distributed Computing, pp. 92­101. ACM Press, New York (2003) 11. Herlihy, M.P., Wing, J.M.: Linearizability: a correctness condition for concurrent objects. ACM Trans. on Programming Languages and Systems 12(3), 463­492 (1990) 12. Raynal, M., Taubenfeld, G.: The notion of a timed register and its application to indulgent synchronization. In: Proc. 19th ACM Symp. on Parallelism in Algorithms and Architectures. ACM Press, New York (2007) 13. Scherer III, W.N., Scott, M.L.: Advanced contention management for dynamic software transactional memory. In: Proc. 24th Symposium on Principles of Distributed Computing, pp. 240­248 (2005) 14. Taubenfeld, G.: Computing in the presence of timing failures. In: Proc. 26th Int'l IEEE Conference on Distributed Computing Systems (ICDCS'06) (2006) 15. Taubenfeld, G.: Synchronization Algorithms and Concurrent Programming. Pearson / Prentice-Hall, p. 423 (2006) ISBN 0-131-97259-6 16. Valois, J.D.: Implementing lock-free queues. In: Proc. of the 7th International Conference on Parallel and Distributed Computing Systems, pp. 212­222 (1994)

Automatic Classification of Eventual Failure Detectors
Piotr Zieli´ nski
Cavendish Laboratory, University of Cambridge, UK piotr.zielinski@cl.cam.ac.uk

Abstract. Eventual failure detectors, such as  or P, can make arbitrarily many mistakes before they start providing correct information. This paper shows that any detector implementable in a purely asynchronous system can be implemented as a function of only the order of most-recently heard-from processes. The finiteness of this representation means that eventual failure detectors can be enumerated and their relative strengths tested automatically. The results for systems with two and three processes are presented. Implementability can also be modelled as a game between Prover and Disprover. This approach not only speeds up automatic implementability testing, but also results in shorter and more intuitive proofs. I use this technique to identify the new weakest failure detector anti - and prove its properties. Anti- outputs process ids and, while not necessarily stabilizing, it ensures that some correct process is eventually never output.

1

Introduction

In purely asynchronous systems, messages between processes can take arbitrarily long to reach their destinations. It is therefore impossible to distinguish a faulty process from a very slow one [8], which causes many practical agreement problems, such as consensus or atomic commit, to be unsolvable [5]. One method of dealing with this impossibility is by equipping the system with failure detectors [3, 11]. A failure detector is an abstract distributed object that processes can query to get information about failures in the system. Different kinds of failure detectors provide different sorts of information, with different reliability guarantees. For example, the eventually perfect detector (P) returns a set of "suspected" processes, and guarantees that eventually it will equal the set of faulty processes. The eventual leader detector ( ) returns a single process, and guarantees that eventually it will keep returning the same correct process. Both P and  are reliable only eventually. They can make mistakes for an arbitrarily long but finite period of time, which is unknown to the application. Such detectors are attractive because algorithms using them are indulgent ; they never fully "trust" the detector, therefore they never violate safety, even if the detector violates its specification [6]. This paper focuses exclusively on such detectors.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 465­479, 2007. c Springer-Verlag Berlin Heidelberg 2007

466

P. Zieli´ nski

Different distributed problems require different failure detectors. A detector is implementable if there is an algorithm that implements it, in a given model. A considerable amount of research has focused on determining the implementability relationships both between problems and failure detectors, and between failure detectors themselves [3, 4, 7, 9, 11]. For example, P can implement  , by outputting the non-suspected process with the smallest id. As a result, every problem solvable with  is solvable with P, but not vice versa [3]. Despite a number failure detectors identified in the literature, no comprehensive exploration of their design space has yet been attempted. As a result, identifying new failure detectors is difficult, and their properties must typically proved from scratch. This paper presents a method that greatly simplifies these tasks: an efficient and fully mechanical procedure for determining the implementability relationship between eventual failure detectors in a system with a given number of processes. The overall strategy to arrive at this result consists of the following steps: ­ Section 2 shows that, under reasonable assumptions, all eventual failure detectors can be completely specified by the list of allowed sets of symbols output infinitely often. For  , this list consists of singleton sets, each containing a single correct process. ­ Section 3 shows that, assuming immediate reliable broadcast, any implementable failure detector can be implemented as a function operating solely on the sequence of past process steps. ­ Section 4 shows that only the order of last occurrences of processes in the above sequence matters. With finitely many possible such orderings, this opens the door to automatic enumeration of failure detectors. ­ Section 5 shows that any failure detector implementable in the immediate reliable broadcast model remains so in the purely asynchronous model. In particular, all results from Section 3 and 4 still apply. ­ Section 6 generalizes the above results to automatically comparing relative strengths of different failure detectors. ­ Section 7 introduces a more intuitive, game-theoretic interpretation of the results from previous sections. It also identifies the weakest non-implementable failure detector anti- , and proves its properties. ­ Section 8 presents the results of automatic enumeration of failure detectors and their relative implementability in a three-process system. Game-solving techniques are used to speed up the search. The theorems and proofs referred to in this paper can be found in the extended version [14].

2

System Model and Failure Detector Specifications

The system consists of a fixed set P = {1, 2, . . . , n} of processes, which communicate using asynchronous reliable channels: messages between correct processes eventually get delivered, but there is no bound on message transmission delay.

Automatic Classification of Eventual Failure Detectors

467

Processes can fail by crashing. In any run, the failure pattern is a function alive(t), which returns the set of non-crashed processes at any given time t  N. Crashed processes do not recover, therefore alive(t)  alive(t + 1). Processes that never crash (C = t alive(t) = ) are called correct, the others are faulty. Runs are fair : correct processes perform infinitely many steps. The system may be equipped with a failure detector. When queried, the detector returns a symbol, for example, a process id ( ) or a set of processes (P). The failure detector history is a function hist(p, t), which gives the symbol returned by the detector at process p at time t. A failure detector specification H is a function that maps each failure pattern alive into a set of allowed functions hist. For example, for  , we have H (alive) = { hist | tN p( 2.1
t

alive(t))

p P t >t hist(p , t ) = p }.

(1)

Failure Detector Assumptions

The standard failure detector specification method [3] described above is very general, but this results in complicated specifications (1). This section simplifies this specification method by making the following assumptions: 1. The detector can behave arbitrarily for any finite amount of time. 2. The set of possible symbols output by the detector is finite. 3. The detector cannot distinguish otherwise indistinguishable runs. For example, a detector that "eventually keeps outputting the process that crashed first" violates Assumption 3: even knowing the entire infinite sequence of system states in a given run is not enough to determine any upper bound on processes' crash times. This is because we cannot distinguish between a process that crashed and one that simply does not take steps. On the other hand, the set of correct processes, provided by P, is deducible from such an infinite sequence of states. Detector P and others are useful because they provide information about the entire infinite run at a finite time. This paper additionally assumes that the detector is querier-independent, that is, function hist depends only on time t, not on the querying process p. I do not list this with other assumptions, because detectors not satisfying this assumption can be emulated by ones that do (Section 3.1, (6)). 2.2 Failure Detector Specification

Assumptions 1­3 allow us to considerably reduce both the space of considered failure detectors as well as the complexity of their descriptions. First, Thm. 3 shows that H depends only on the set C = t alive(t) of correct processes, not on the exact form of alive. This simplifies (1) to H (C ) = { hist | tN pC t >t hist(t ) = p }. (2)

Theorem 4 shows that whether "hist  H(C )" depends only on the set of values that hist(t) takes infinitely often, not on the exact form of hist. Therefore, we

468

P. Zieli´ nski  infset (1) infset (2) infset (12) 1 2 1,2 S P ?P

,

: : : :

only process 1 is correct only process 2 is correct either 1 or 2 faulty no failures

Fig. 1. Specifications infset (C ) for various failure detectors in a system with two processes 1 and 2 (left), and the interpretation of the output symbols (right)

can specify a failure detector as the set infset (C ) of allowed sets of symbols output infinitely often. The description (2) simplifies to infset  (C ) = { {p} | p  C }. In general, infset (C ) = { inf (s1 . . .) | sk = hist(k ), hist  H(C ) }.
def

(3)

(4)

where inf (s1 . . .) = k=1,2,... {sk , sk+1 , . . .} is the set of symbols sk 's that occur infinitely often in s1 . . ., for example, inf (32413512212122 . . .) = {1, 2}. Since a failure detector can behave better than required, S  T  infset (C ) implies S  infset (C ) (Thm. 5). All free set variables in this paper, such as S , T , C in the previous sentence, are implicitly assumed to be non-empty. Examples. Figure 1 shows the specifications of several known detectors, in a two-process system. Detectors  and P have already been introduced. Anonymous ?P eventually detects whether all processes are correct ( ) or not ( ), without revealing the identities of faulty processes. Detector S is similar to P: it also outputs a set of suspected processes, however, S can forever suspect some, but not all, correct processes [3]. Figure 1 represents a set of suspected processes as a vertical bitmap (eg. ), with one entry per process; black entries mean "suspected", white entries "not suspected". For each detector, Figure 1 shows the value of infset (C ) for C = {1}, {2}, {1, 2}. For brevity, sets {a, b, . . .} are abbreviated to ab . . ., non-maximal elements of infset (C ) removed (Thm. 5), and external braces omitted. For example, infset S ({1, 2}) = {{ }, { }, { }, { , }, { , }} = infset S (12) = , . (5)

This de-cluttering convention is used throughout the paper.

3

Implementability in the Immediate Broadcast Model

Our goal is to determine whether a given failure detector, as specified by its infset , is implementable. Sections 3 and 4 will investigate this question in the immediate broadcast model. This model is significantly stronger than the purely asynchronous model, for example, its basic broadcast primitive of implements

Automatic Classification of Eventual Failure Detectors

469

atomic broadcast, which in non-implementable in the asynchronous model [3, 5]. Surprisingly, however, as far as implementability of (eventual) failure detectors is concerned, these two models are equivalent (Section 5). In the immediate broadcast model, all messages are transmitted instantaneously and reliably. Processes take steps in any fair order: correct processes take infinitely many steps, faulty ones finitely many steps. Processes never fail in the middle of a step. 3.1 Failure Detector Implementations

Immediate and reliable broadcast ensures that each process always knows the complete state of the system: the sequence p1 . . . pk of processes that have taken steps until this moment. For example, the state at the end of
        time        

1 2 2 3 2 2 3

is p1 . . . p7 = 2123223. Assuming determinism, all other state information can be inferred from p1 . . . pk (the initial state is fixed). Therefore, the complete state of any algorithm in this model depends only on p1 . . . pk . In particular, any failure detector implementation can be modelled as a function output from sequences of processes p1 . . . pk to output symbols sk . A failure detector sensitive to the identity of the querying process has n functions: output1 , . . . , outputn , one per process. However, these can be transformed into a single, querier-independent function outputting a composite symbol: output(p1 . . . pk ) = [sk1 . . . skn ], where ski = outputi (p1 . . . pk ). (6)

The original detector output at process i is the ski in the composite symbol [sk1 . . . skn ]. Thus, for any failure detector implementation output1 , . . . , outputn , there is a querier-independent detector implementation output that can emulate it. For this reason, this paper focuses on querier-independent detectors. 3.2 Failure Detector Specifications

From (4), an implementation output is consistent with a specification infset iff, for any infinite sequence p1 . . . of processes, we have: inf (s1 . . .)  infset (C ), where sk = output(p1 . . . pk ) and C = inf (p1 . . .). (7) For example, consider a trivial failure detector: infset trivial (C ) = { X | X  C } for all C  P , (8)

which eventually outputs only correct processes. It can be implemented by returning the most recent process to take a step, that is, output(p1 . . . pk ) = pk .

470

P. Zieli´ nski

Similarly, returning the least recent process, for example, output(2123223) = 1, will eventually keep outputting one stable faulty leader, if it exists: infset faulty (C ) = { {p} | p  / C} for all C  P . (9)

(Compare with (3).) By convention, the undefined case C = P allows arbitrary behaviour. For any set X , let perms(X ) be the set of all permutations of elements of X . Let order(p1 . . . pk )  perms(12 . . . n) be obtained from p1 . . . pk by retaining only the last occurrence of each process (eg. order(312233143433131) = 2431)1 . The implementations of failure detectors (8) and (9) can be succinctly written as outputtrivial (p1 . . . pk ) = last element of order(p1 . . . pk ) outputfaulty(p1 . . . pk ) = first element of order(p1 . . . pk ) (10)

Note that both implementations ignore all information in p1 . . . pk , except for order(p1 . . . pk ). Theorem 1 shows that all implementable failure detectors can be implemented this way, with sk = output(p1 . . . pk ) = map(order(p1 . . . pk )) for some function map from perms(12 . . . n) to output symbols. For example, maptrivial (q1 . . . qn ) = qn , mapfaulty (q1 . . . qn ) = q1 . (11)

With a fixed number n of processes, the number such functions map is finite, which enables us to automate implementability testing (Section 8). In any run, as the sequence of steps p1 . . . pk grows, order(p1 . . . pk ) keeps changing. Since faulty processes take finitely many steps, eventually the prefix of order(p1 . . . pk ) consisting of all faulty processes will stabilize, while the rest, consisting of correct processes, will keep changing. Therefore, the implementation map is consistent (7) with the specification infset iff for any order q1 . . . qk of faulty processes { map(q1 . . . qk r1 . . . rn-k ) | r1 . . . rn-k  perms(C ) }  infset (C ), (12)

where C = P \ {q1 . . . qk }. For example, we can show that  is not implementable. To obtain contradiction, assume that it is. By Theorem 1, there is an implementation output (p1 . . . pk ) = map (order(p1 . . . pk )). For any order q1 . . . qn , we must have map (q1 . . . qn ) = qn because qn might be the only correct process (12). In other words, this implementation of  always outputs the last process to take a step. However, if more than one process is correct, the output may never stabilize, violating the properties of  .
1

To ensure that order(p1 . . . pk ) always contains all processes, even if some do not occur in p1 . . . pk , I implicitly prefix each p1 . . . pk with 12 . . . n.

Automatic Classification of Eventual Failure Detectors
function update(q1 . . . qn ) is simulate qn taking a step set map(q1 . . . qn )  failure detector output in the simulation function update(q1 . . . qk<n ) is repeat for each q  / q1 . . . qk do update(q1 . . . qk q ) until (12) holds for q1 . . . qk task construct map is update(), where  is the empty sequence

471

1 2 3 4 5 6 7 8 9 10

Fig. 2. Generating a map for a given failure detector implementation using failure detector outputs in a specially constructed simulated run

4

Order Map Theorem

Section 3 used the fact that any implementable failure detector can be implemented using some function map acting solely on the order of recent process steps. This section proves this theorem. It is important because it restricts the originally infinite number of possible functions output to those induced by one of the functions map, whose number is finite. Theorem 1. Any implementable detector has an implementation of the form output(p1 . . . pk ) = map(order(p1 . . . pk )) for some function map. Proof. Figure 2 presents an algorithm that, for any implementable failure detector, constructs a map that implements it, that is, is consistent (12) with the detector's infset . It takes the algorithm implementing the failure detector, and collects its outputs in a simulated run. This run is constructed by function update(q1 . . . qi ), which also updates map so that (12) holds for all q1 . . . qk starting with q1 . . . qi . Therefore, update() in line 10 produces a map that satisfies (12) for all q1 . . . qk . The implementation of update(q1 . . . qi ) covers two cases. For q1 . . . qn consisting of all processes, update makes the last process step, queries the detector, and sets map(q1 . . . qn ) to its output (lines 1­3). It trivially satisfies (12) because no valid sequence of faulty processes can contain all processes. For shorter q1 . . . qk , function update recursively ensures that (12) holds for all extensions of q1 . . . qk , and then tests whether (12) holds for q1 . . . qk itself. If not, the process is repeated until success (lines 5­8). This cannot go on forever, because update(q1 . . . qk ) makes only processes in C = P \ {q1 . . . qk } take steps. Therefore, any implementable detector will eventually start outputting symbols from some S  infset (C ), passing the test in line 8. Example. Consider the faulty-leader detector (9) implemented by returning the process that took least steps (not the least recent one to step), favouring lower ids to break ties. This results in the following run of the algorithm in Figure 2 on page 471:

472


P. Zieli´ nski
     time      

q1 q2 q3 map

111111112222222222222222233333333333333333 +: line 8 succeeded 222333 11133333 111333 11111222 111222 -: line 8 failed 3 2 3 1 1 3 1 2 2 1 2 1  step (line 2) 1+ 1++ 1+ 1-2+- 2+ 2++ 2-1+ 2+- 3+ 3+++  det output (line 3)

Function update() calls update(1), update(2), and update(3). The recursion in update(1) eventually makes processes 2 and 3 step. In both cases, the detector outputs 1, which results in mappings map(123) = map(132) = 1, which pass the line 8 test in update(12), update(13), and then update(1). Function update(2) encounters more problems. It first calls update(21), which produces map(213) = 1, and then update(23). Function update(23) recursively calls update(231), which produces map(231) = 1. Since 1infset (1), line 8 in update(23) fails and update(231) is called again. It sets map(231) = 2, which passes the test in update(23), but (together with map(213) = 1) fails the test in update(2) because 12infset (13). Calling update(231) and update(213) again results in map(213) = map(231) = 2, which passes the test in update(2). Similarly, update(3) results in map(312) = map(321) = 3. The algorithm in Figure 2 on page 471 has therefore transformed the original least-oftenstepping implementation of (9), into the least-recent-to-step implementation map, highlighted above (11).

5

Implementability in the Asynchronous Model

This section shows that any failure detector implementable in the immediate broadcast model (Sections 3 and 4) remains so in the purely asynchronous model. (The opposite implication is obvious.) This result implies, for example, that for any implementable failure detector, there is a querier-independent, implementable failure detector that can emulate it (Section 3.1). Consider a failure detector implementable in the immediate broadcast model. Section 4 showed that there is a map consistent with it (12), which acts on the process order q1 . . . qn . This process order must satisfy (12): (i) faulty processes precede correct ones, and (ii) the order of faulty processes is fixed. Let Order Oracle be an abstraction that, when queried, outputs an order that eventually satisfies (i) and (ii). It is sufficient to show that Order Oracle is implementable in purely asynchronous settings. As an example, consider a four-process system with only processes 3 and 4 correct. Order Oracle can keep switching between 1234 and 1243 or between 2134 and 2143 in the same run. However, outputting both 1234 and 2134 infinitely often in the same run would violate (ii), and 2314 would violate (i). In the algorithm in Figure 3 on page 473, processes reliably broadcast a message whenever they take a step. Each process keeps track of steps taken by others by storing the highest-numbered step for each process in the vector maxstep. When the algorithm is asked for an order on processes, it returns them in the increasing order of maxstep.

Automatic Classification of Eventual Failure Detectors

473

1 2 3 4 5 6 7 8

maxstep[i]  0 for all processes i when process i takes its k-th step do reliably broadcast "process i, step k" when reliably receive "process i, step k" do maxstep[i]  max {k, maxstep[i]} when queried do return the list of all processes i, ordered wrt increasing maxstep[i] (ties broken deterministically)

Fig. 3. An implementation of Order Oracle in an asynchronous system

This simple algorithm is similar to the heartbeat failure detector [1], with one important difference: it uses reliable broadcast [10] rather than ordinary broadcast. This ensures that not only maxsteps of correct processes keep increasing without limit (i), but also that eventually maxsteps corresponding to faulty processes will be the same at all correct processes (ii). Note that the agreement on the order of faulty processes is only "eventual" in the same sense as reliable broadcast makes correct processes agree on the set of broadcast messages. In particular, it does not contradict FLP [5]. Conclusion. By taking the results from Section 4 and this section together, we can conclude that a failure detector is implementable in the purely asynchronous system iff there is a map consistent (12) with its specification infset .

6

Comparing Relative Strengths of Failure Detectors

This section shows that the theory developed in previous sections allows us not only to mechanically test implementability, but also to compare relative strength of failure detectors. In other words, we can test whether a given failure detector (eg. P) is implementable in the asynchronous system equipped with another detector (eg.  ). For any failure detector S , consider a purely asynchronous system consisting of real processes P and virtual processes RS , one for each possible output of S (its range ). For example, with a two-process P, we have processes P = {1, 2} and RP = { , , }. In general, the set of processes is the disjoint union [P, RS ], in which members of P and RS keep separate identities2 , even if they have identical names (eg. R = P ). The scheduler ensures that virtual processes behave according to the detector specification, that is, the set [C, S ]  [P, RS ] of correct processes satisfies S  infset (C ). (A process is correct iff it takes infinitely many steps.) Given this assumption, real processes p  P can emulate the failure detector by always outputting the most-recently heard-from virtual process s  RS (Thm. 6).
2

Formally, [A1 . . . Ak ] = { (a, i) | a  Ai }. Then, [A, B ]  [A , B ]  A  A B  B .

474

P. Zieli´ nski

To check whether a failure detector S can implement another detector T , we need to test whether T is implementable in the system [P, RS ]. The specification of T in this system is infset T ([C, S ]) = infset T (C ) for all S  infset S (C ). (13)

By convention (9), the undefined cases [C, S ] with S  / infset S (C ) allow arbitrary behaviour: infset T ([C, S ]) = { X | X  RT }. Example 1. Consider a two-process system equipped with ?P (Figure 1). To show that P is implementable in such a system, consider the requirements (13): infset P ([1, ]) = , infset P ([2, ]) = , infset P ([12, ]) = .

To implement P, output if ?P outputs . Otherwise output or , depending whether the most recently heard-from process is 1 or 2. This strategy corresponds to the following map, which satisfies (12): 21 , 2 1, 21, 2 1, 2 1 , 21 12,  12 , 1 2, 12, 1 2, 1 2 , 12  12 , 1 2, 21 , 2 1, 21  12 , 1 2 , 1 2, 21 , 2 1 , 2 1 

Example 2. To show that  cannot implement ?P, consider the requirements ^ = , infset ?P ([1, ^ 1]) = , infset ?P ([2, 2]) infset ?P ([12, ^ 1]) = infset ?P ([12, ^ 2]) = . ^, 2 ^ for  outputs to avoid name collisions with processes 1, 2.) First, (I use 1 112^ 2) = . However, infset ([2, ^ 2]) = infset ([12, ^ 2]) = and (12) imply that map(^ implies map(^ 112^ 2) = , which contradicts map(^ 112^ 2) = .

7

Game-Theoretic Interpretation of Implementability

Two players, YES and NO, play the following game. In the k -th turn, NO chooses a set Ck  P , and YES chooses Sk  infset (Ck ). The sets must satisfy C1  C2  · · · = , and S1  S2  · · · = . The first player unable to make a move loses. Theorem 7 shows that YES has a winning strategy iff the failure detector is implementable. Figure 4 (left) shows the game tree for the two-process  . Each path C1 , S1 , . . . , starting at the root, represents a sequence of moves. For example, "12, ^ 1, 1, ^ 1" is a victory for YES, and "12, ^ 2, 1" for NO. White nodes are wins for YES, black ones for NO. The colour of a node can be easily computed using the minimax algorithm [12]: Ck (resp. Sk ) nodes are black iff all (resp. some) of their children are black. Since C1 = 12 is black, NO has a winning strategy, so the two-process  is not implementable. As Fig. 4 (right) suggests, similar reasoning works for general n > 2 (Thm. 9).

Automatic Classification of Eventual Failure Detectors

475

C1 = S1 = ^ 1

12 ^ 2

C1 = S1 = C2 = 12 S2 = ^ 1 ^ 1 13 ^ 1 23 12 ^ 2 2 2 ^ 2

123 ^ 2 13 23 ^ 2 2 3 ^ 2 12 ^ 3 13 ^ 3 23 ^ 3

C2 = 1 2 1 2 S2 = ^ 1 ^ 2

C3 = 1 2 1 3 S3 = ^ 1 ^ 1

3 1 3 2 ^ 3 ^ 3

Fig. 4. Game trees for  with two processes (left), and three process (right)

root [C1 , S1 ] = 12, ^ 1 12, ^ 2 T1 = [C2 , S2 ] = 1, ^ 1
(a)  , ?P, 2

root 1, 2, 12, 12, 13,

root 23, 123,

2, ^ 2
(b) ?P, P, 2

1,

2,

1,

3,

2,

3,

(c) ?P, P, 3

Fig. 5. Game trees corresponding to implementing detector T in a system equipped with detector S in an n-process systems, for three different (S, T, n)

7.1

Comparing Relative Detector Strengths Using Game Theory

With the modifications described in Section 6, the game-theory approach can also be used to check whether one failure detector S can implement another detector T . Since the system is now equipped with S , player NO chooses [C1 , S1 ]  [C2 , S2 ]  · · · = . YES chooses T1  T2  · · · =  with Tk  infset T ([Ck , Sk ]). We can assume that Sk  infset S (Ck ) and Ck-1  Ck , because otherwise YES could always repeat its previous move, which cannot benefit NO (Lemma 8). With (13), this implies Tk  infset T (Ck ). Figure 5 shows game trees corresponding to implementing detector T in a system equipped with detector S in an n-process systems, for three different (S, T, n). Case (a) shows that  cannot implement ?P in a two-process system. Detector ?P can implement P with two processes (b), but not with three (c). 7.2 Anti- : The Weakest Failure Detector

The anti- failure detector is specified as infset anti- (C ) = { S | C S  P }. (14)

476

P. Zieli´ nski

C1 = S1 = C2 = 12 S2 = 1 2 C3 = 1 2 1 S3 = 12 13 12 3 2 23 12 3 1 12 13

123 13 13 1 3 2 1 3 2 23 13 3 1 12 23 2 1 23 13 23 23 2 3 3 2 3

12 12 12 12 13 13

13 13 23 23 23 23

Fig. 6. Game tree for the three-process anti-

It outputs process ids, and ensures that some correct process id will eventually never be output. Note that the classic  ensures that such an id will eventually always be output. Theorem 10 shows that anti- is not implementable: NO can win by playing C1 = P and then always copying YES's last move Ck+1 = Sk . This strategy corresponds to the black nodes in the three-process anti- game-tree shown in Figure 6 on page 476. In this tree, each Sk -node has exactly one black child; the minimax rule therefore implies that whitening any black node would make the game winnable by YES. In a sense, anti- is therefore a "locally weakest detector". Theorem 12 uses the method from Section 7.1 to prove a stronger result: anti- is the (globally) weakest non-implementable eventual failure detector in the sense that it can be implemented by any non-implementable detector. In particular, anti- is strictly weaker than  , the weakest stable detector [9]: infset  (C ) = { {T } | C = T  P }. (15)

(A detector is stable iff it eventually outputs the same symbol, that is, all infset (C )'s consist of singleton sets only.) As a by-product, this shows that some failure detectors, such as anti- , have no stable equivalents. Anti- is also the weakest detector that solves set agreement [13].

8

Automatic Failure-Detector Discovery Results

Section 6 introduced a mechanical procedure for comparing failure detector strength in a system with a given number of processes. The game-theoretic approach of Section 7 dramatically improved the efficiency by using standard game solving techniques (eg. alpha-beta cutting [12], proof-number search [2]). This section gives a glimpse at the failure detector specification space by enumerating eventual failure detectors and their relationships in systems with two and three processes.

Automatic Classification of Eventual Failure Detectors

477

8.1

Two Processes, All Detectors

This section enumerates and compares all failure detectors with two processes and at most three outputs. The sets infset (1), infset (2), and infset (12) can each take 18 possible values, giving the total of 183 = 5832 failure detectors. Computer testing shows that they all fall into 5 equivalence classes, shown below (left) with several members (right).
implementable

 ?1 ?2 P 1 2 1,2

S

? ?12 ?21 ?P 1 2 1 ,2  ?1 ?2 P

infset (1) infset (2) infset (12)

1 2 12

, - 

equivalent to

The implementability relationship between these classes is
?2

implementable <  < ?1 <  P implementable <  < ?2 <  P
P

implementable

?1

Detectors ?1 and ?2, which eventually detect whether process 1 (resp. 2) is correct, are of incomparable strength. 8.2 Three Processes, Symmetric Detectors

The number of three-process failure detectors with three outputs is 187  6 × 108 . For this reason, this section considers only symmetric failure detectors, which treat all processes equally, that is, do not favour any particular permutation of processes or group of such permutations. Such detectors fall into two categories: (i) those that output process-independent symbols, such as ?P, and (ii) those that output process ids, such as  . There are 6024 such detectors, grouped into 28 equivalence classes shown in Figure 7. Figure 7 contains several known failure detectors, such as  , anti- , and ?P. The strongest detector in Figure 7 on page 478 eventually outputs the number k of correct processes. It is equivalent to P, which it can emulate by suspecting the n - k least recently heard-from processes. The 28 equivalence classes in Figure 7 on page 478 do not contain all symmetric detectors. Detectors that behave as class (i) or (ii), depending on the number of correct processes, form 654 such classes. Allowing non-symmetric detectors and/or more output symbols might increase this number even more. Based on the relatively few failure detectors identified in the literature, such a high number is rather unexpected (and we are only considering systems with three processes here!).

478

P. Zieli´ nski
p pq, r p, q, r qr pq p, q, r qr p, q, r p, q, r pq, pr, qr p, q p, q, r qr pq pq, pr, qr p p, q, r pqr p p, q, r pq, pr, qr pq, pr, qr r p, q, r qr p, q pq, pr, qr 12 1, 3 2, 3 qr r p, q, r p p, q, r p, q, r p pq p, q, r qr p, q p, q, r p r pqr 1, 2 1, 3 2, 3 p p, q



p, q, r 1 1, 2 2 p r pq, pr, qr 1 2 1, 2 1, 2 1 2 1 2 2 p r p, q, r 1 1 2 1 2 1 1 2 3

?P P

implementable anti-
infset (p) = infset (pq ) = infset (pqr) = qr pr, qr pq, pr, qr

Fig. 7. Three-process failure detectors with three outputs

9

Conclusion

This paper investigated the space of eventual failure detectors. The key result is Theorem 1: every implementable detector is a function of the order of recently heard-from processes. By emulating failure detectors with virtual processes corresponding to their outputs, we can use the same technique to compare the strengths of different detectors. Implementability is also equivalent to a winning strategy in a particular twoplayer game. The advantage of this approach is that it has more structure and a more intuitive visual representation. This makes failure detectors easier to analyse, and leads to more succinct, intuitive, and elegant proofs, using existing results from game theory. As an example, this paper identified the weakest eventual failure detector anti- . Every query returns a single process; the detector might not stabilize, but there is a correct process that eventually will never be output. Both approaches produce a finite number of failure detectors, thereby making comprehensive computer search possible. Such a search, applied to three-process detectors with three outputs, generated many known detectors, but also revealed an unexpected richness of non-equivalent failure detector classes. I hope that a similar methodology can be used to explore the space of distributed problems such as consensus, renaming, etc. The benefits of computer search extend to theoretical results as well, many of which would have been difficult to derive without it. For example, the ability of quick implementability verification was very valuable in identifying anti- and proving its properties. I believe that using computer search as a tool for developing

Automatic Classification of Eventual Failure Detectors

479

and testing one's intuition about a problem is a useful and productive technique that should become more popular in distributed-computing research.

References
[1] Aguilera, M.K., Chen, W., Toueg, S.: Heartbeat: A timeout-free failure detector for quiescent reliable communication. In: Mavronicolas, M. (ed.) WDAG 1997. LNCS, vol. 1320, pp. 126­140. Springer, Heidelberg (1997) [2] Allis, L.V.: Searching for Solutions in Games and Artificial Intelligence. PhD thesis, University of Limburg, the Netherlands (September 1994) [3] Chandra, T.D., Hadzilacos, V., Toueg, S.: The weakest failure detector for solving Consensus. Journal of the ACM 43(4), 685­722 (1996) [4] Delporte-Gallet, C., Fauconnier, H., Guerraoui, R., Hadzilacos, V., Kouznetsov, P., Toueg, S.: The weakest failure detectors to solve certain fundamental problems in distributed computing. In: 23rd PODC, St. John's, Newfoundland, Canada, pp. 338­346 (2004) [5] Fischer, M.J., Lynch, N.A., Paterson, M.S.: Impossibility of distributed Consensus with one faulty process. Journal of the ACM 32(2), 374­382 (1985) [6] Guerraoui, R.: Indulgent algorithms. In: Proceedings of the 19th Annual ACM Symposium on Principles of Distributed Computing, pp. 289­298. ACM Press, New York (2000) [7] Guerraoui, R., Kouznetsov, P.: Finally the weakest failure detector for NonBlocking Atomic Commit. Technical Report LPD-2003-005, EPFL, Lausanne, Switzerland (December 2003) [8] Guerraoui, R., Hurfin, M., Most´ efaoui, A., Oliveira, R., Raynal, M., Schiper, A.: Consensus in asynchronous distributed systems: A concise guided tour. In: Krakowiak, S., Shrivastava, S.K. (eds.) Advances in Distributed Systems. LNCS, vol. 1752, pp. 33­47. Springer, Heidelberg (2000) [9] Guerraoui, R., Herlihy, M., Kouznetsov, P., Lynch, N., Newport, C.: On the weakest failure detector ever. In: 26th PODC, Portland, OR, US (August 2007) [10] Hadzilacos, V., Toueg, S.: Fault-tolerant broadcast and related problems. In: Mullender, S. (ed.) Distributed Systems, 2nd edn., ch. 5, pp. 97­146. ACM Press, New York (1993) [11] Raynal, M.: A short introduction to failure detectors for asynchronous distributed systems. ACM SIGACT News 35(1), 53­70 (2005) [12] Russell, S., Norvig, P.: Artificial Intelligence: A Modern Approach. Prentice-Hall, Englewood Cliffs (1995) [13] Zieli´ nski, P.: Anti- : the weakest failure detector for set agreement. Technical Report UCAM-CL-TR-694, Computer Laboratory, University of Cambridge (July 2007) [14] Zieli´ nski, P.: Automatic classification of eventual failure detectors. Technical Report UCAM-CL-TR-693, Computer Laboratory, University of Cambridge (July 2007)

When 3f + 1 Is Not Enough: Tradeoffs for Decentralized Asynchronous Byzantine Consensus
Alysson Neves Bessani, Miguel Correia, Henrique Moniz, Nuno Ferreira Neves, and Paulo Verissimo
LaSIGE, Faculdade de Ci^ encias da Universidade de Lisboa, Portugal

1

Context and Motivation

Recently, we challenged the belief that randomized Byzantine agreement protocols are inefficient, by designing, implementing and assessing the performance of a stack of protocols of that type [3]. That assessment lead us to a set of properties desirable for Byzantine asynchronous binary consensus protocols: (1) Strong validity ­ if all correct processes propose the same value v , the decision is v (values proposed by Byzantine processes are often useless); (2) Asynchrony ­ no time assumptions are made (systems are often prone to arbitrary delays); (3) Decentralization ­ there is no leader (leader elections have a great impact on performance); (4) Optimal resilience ­ n  3f + 1 processes to tolerate f Byzantine (extra processes are costly); (5) Optimal message complexity ­ O(n2 ) (high impact on throughput); (6) Signature freedom (high impact of signatures based on public-key cryptography on the performance); (7) Early decision ­ in "nice" runs the protocol should decide in a few communication steps (good latency in the "normal" case). The main characteristic of the decentralized protocols we are interested in this paper is that they can not require any reliable certificate from a process pi , obtained in phase k or less, in order to justify a message sent in phase k + 1. This is the case because, in our system model, this kind of certificate can only be build with digital signatures (violating signature freedom) or reliable multicast (that can not be executed by all processes maintaining a message complexity O(n2 )). Moreover, given the validity condition we stipulated (1), we require that all correct processes communicate their proposals to each other (a process can not trust another process to correctly communicate its value to a third process, since there are no signatures).

2

The Tradeoff

Is it possible to design such a Byzantine asynchronous binary consensus protocol? The main results in the paper are given by the following theorems:
Work supported by project IST-4-027513-STP (CRUTIAL) and LaSIGE.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 480­481, 2007. c Springer-Verlag Berlin Heidelberg 2007

When 3f + 1 Is Not Enough

481

Theorem 1 (Impossibility result). There is no decentralized algorithm that solves asynchronous binary Byzantine consensus with n  5f , O(n2 ) message complexity and without signatures. Given this impossibility and several other results and protocols already described in the literature, it is possible to define in which conditions a binary decentralized Byzantine consensus protocol can exist: Theorem 2 (Tradeoff ). Decentralized algorithms that solve asynchronous binary Byzantine consensus can be build with and only with: 1. More Processes: n  5f + 1, O(n2 ) message complexity and signature freedom; 2. More Messages: n  3f + 1, O(o) message complexity (n2 < o  n2 f ) and signature freedom; 3. Signatures: n  3f + 1, O(n2 ) message complexity and using signatures. Notice that the bound established by Theorem 2 regarding more messages is not tight: we do not know if it is possible to solve Byzantine consensus without signatures and optimal resilience with message complexity lower than O(n2 f ), but greater than O(n2 ).

3

Discussion

An interesting consequence of the theorems above is that decentralized protocols are inherently more costly in terms of the three properties considered (resilience, message complexity, signature) than leader-based Byzantine consensus protocols. For instance, the CL-BFT state machine replication protocol, that can be trivially adapted to solve consensus, is not subject to the tradeoff in Theorem 2 [2]. However, this protocol does not ensure the strong validity condition that we are interested in and requires synchrony to be able to terminate. Theorem 1 implies that a consensus protocol with all the desired properties listed above can not be designed. However, we developed an improved protocol based on Bracha's Byzantine consensus [1], an algorithm that we believe is close enough to the desired characteristics that we envisage. This protocol improves the original Bracha's protocol in two main points: (1.) its message complexity is O(n2 f ) instead of O(n3 ); and (2.) it can terminate in one communication step if some optimistic conditions hold (no faults and unanimity).

References
1. Bracha, G.: An asynchronous (n - 1)/3 -resilient consensus protocol. In: PODC'84. Proceedings of the 3rd ACM Symposium on Principles of Distributed Computing, pp. 154­162. ACM Press, New York (August 1984) 2. Castro, M., Liskov, B.: Practical Byzantine fault-tolerance and proactive recovery. ACM Transactions Computer Systems 20(4), 398­461 (November 2002) 3. Moniz, H., Neves, N.F., Correia, M., Ver´ issimo, P.: Randomized intrusion-tolerant asynchronous services. In: DSN 2006. Proceedings of the International Conference on Dependable Systems and Networks. LNCS, vol. 4615, pp. 568­577 (June 2006)

On the Complexity of Distributed Greedy Coloring
Cyril Gavoille1 , Ralf Klasing1 , Adrian Kosowski2, and Alfredo Navarra1,3
LaBRI - Universit´ e Bordeaux - CNRS, 351 cours de la Liberation, 33405 Talence, France {gavoille,klasing,navarra}@labri.fr Department of Algorithms and System Modeling, Gda´ nsk University of Technology, Narutowicza 11/12, 80952 Poland kosowski@sphere.pl 3 Dipartimento di Matematica e Informatica, Universit´ a degli Studi di Perugia, Via Vanvitelli 1, 06123 Perugia, Italy navarra@dipmat.unipg.it Abstract. Distributed Greedy Coloring is an interesting and intuitive variation of the standard Coloring problem. It still consists in coloring in a distributed setting each node of a given graph in such a way that two adjacent nodes do not get the same color, but it adds a further constraint. Given an order among the colors, a coloring is said to be greedy if there does not exist a node for which its associated color can be replaced by a color of lower position in this order without violating the coloring property. We provide lower and upper bounds for this problem in Linial's model and we relate them to other well-known problems, namely Coloring, Maximal Independent Set (MIS), and Largest First Coloring. Whereas the best known upper bound for Coloring, MIS, and Greedy Coloring are the same, we prove a lower bound which is strong in the sense that it now makes a difference between Greedy Coloring and MIS.
1

2

We discuss the vertex coloring problem in a distributed network. Such a network consists of a set V of processors and a set E of bidirectional communication links between pairs of processors. It can be modeled by an undirected graph G = (V, E ). We denote n = |V |, m = |E | and for each vertex v define its neighborhood Nv = {u : {u, v }  E } and vertex degree degG v = |Nv |. The set of  = {u  Nv : deg u  deg v }. neighbours of high degree is denoted by Nv To color the vertices of G means to give each vertex a positive integer color value in such a way that no two adjacent vertices get the same color. If at most k colors are used, the result is called a k -coloring. In many practical considerations, such as code assignment in wireless networks [1], it is desirable to minimise the number of used colors. The smallest possible positive integer k for which there exists a k -coloring of G is called the chromatic number (G). This value is bounded from above by  + 1, where  denotes the maximal vertex degree of the graph; consequently, a graph always admits a ( + 1)-coloring.
The research was partially funded by the European projects COST Action 293, "Graphs and Algorithms in Communication Networks" (GRAAL) and, COST Action 295, "Dynamic Communication Networks" (DYNAMO).
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 482­484, 2007. c Springer-Verlag Berlin Heidelberg 2007

On the Complexity of Distributed Greedy Coloring

483

The problems discussed in this paper can be formulated using a local definition: the goal is to achieve a state of the system in which the local state variables associated with each node fulfill certain constraints with respect to the local state variables of its neighbours. Definition 1. The problems are defined by the following constraints on the local variable1 c at vertex v :
( + 1)-Coloring ( COL): Greedy Coloring ( G-COL): Largest-First Coloring ( LF-COL): Maximal Independent Set ( MIS): c(v )  {1, . . . ,  + 1} \ c(Nv ) c(v ) = min {1, . . . ,  + 1} \ c(Nv )
 ) c(v ) = min {1, . . . ,  + 1} \ c(Nv

c(v ) = 0  c(Nv ) = {0}

We provide lower and upper bounds on the deterministic distributed (Linial's model) time complexity of Greedy Coloring (G-COL) with respect to Coloring (COL), Maximal Independent Set (MIS) and Largest First Coloring (LF-COL). A summary of the results is contained in Table 1, where (*) indicates the new results obtained in this paper. In particular, we derive new upper bounds for GCOL and LF-COL and a new lower bound for G-COL. Whereas the upper bounds for the COL, MIS, and G-COL are the same, we prove a strong lower bound in the sense that our lower bound now makes a difference between G-COL and MIS.
Table 1. The time complexity of Greedy Coloring with respect to other well-known problems in the distributed setting. The table can be read also vertically as ( + 1)Coloring  Maximal Independent Set  Greedy Coloring  LF-Coloring. Problem ( + 1)-Coloring (COL) Maximal Independent Set (MIS)  Lower Bound  (log n) [2]
log n log log n

Upper Bound 2O (
 log n)

[3,4]

[5]

[3,4] 2 O( + COL) 2O (
 log n)

 O ( log n)

(*)

Greedy Coloring (G-COL)



log n log log n

(*)

O( + COL) (*) O(2 + log n) [2]  O( n) (*) O( · MIS) [6]

Largest-First Coloring (LF-COL)

  ( n ) [6]

References
1. Battiti, R., Bertossi, A.A., Bonuccelli, M.A.: Assigning codes in wireless networks: bounds and scaling properties. Wireless networks 5(3), 195­209 (1999) 2. Linial, N.: Locality in distributed graph algorithms. SIAM J. Comput. 21, 193­201 (1992)
1

This variable represents the color or the characteristic function of the MIS.

484

C. Gavoille et al.

3. Awerbuch, B., Goldberg, A.V., Luby, M., Plotkin, S.A.: Network decomposition and locality in distributed computation. In: FOCS, pp. 364­369. IEEE Computer Society Press, Los Alamitos (1989) 4. Panconesi, A., Srinivasan, A.: On the complexity of distributed network decomposition. J. Algorithms 20(2), 356­374 (1996) 5. Kuhn, F., Moscibroda, T., Wattenhofer, R.: What cannot be computed locally! In: Chaudhuri, S., Kutten, S. (eds.) PODC, pp. 300­309. ACM Press, New York (2004) 6. Kosowski, A., Kuszner, L.: On greedy graph coloring in the distributed model. In: Nagel, W.E., Walter, W.V., Lehner, W. (eds.) Euro-Par 2006. LNCS, vol. 4128, pp. 592­601. Springer, Heidelberg (2006)

Fault-Tolerant Implementations of the Atomic-State Communication Model in Weaker Networks
Colette Johnen1 and Lisa Higham2
LRI, Univ. Paris-Sud, CNRS, F-91405 Orsay, France colette@lri.fr Computer Science Department, University of Calgary, Canada higham@ucalgary.ca
1

2

There is a proliferation of models for distributed computing, consisting of both shared memory and message passing paradigms. Different communities adopt different variants as the "standard" model for their research setting. Since subtle changes in the communication model can result in significant changes to the solvability/unsolvability or to the complexity of various problems, it becomes imperative to understand the relationships between the many models. The situation becomes even more complicated when additional requirements such as fault-tolerance are added to the mix. This motivates us to determine exactly under what circumstances a program designed for one model and delivering some set of additional guarantees can be converted into an "equivalent" programs for a different model while delivering comparable guarantees. Once these relationships are understood, they can be exploited in system design. Our work addresses this question for networks of processors that communication by locally shared registers. A network that uses locally shared registers can be modelled by a graph where nodes represent processors and there is an edge between two nodes if and only if the corresponding processors communicate directly by reading or writing registers shared between them. Two variants are defined by specifying whether the registers are single-writer/multi-reader and located at the nodes (called state models) or single-writer/ single-reader and located on the edges (called link models). The shared registers used by the communicating processors further distinguishes possible models. Lamport [4] defined three models of single-writer/multireader registers, differentiated by the possible outcome of read operations that overlap concurrent write operations. These three register types, in order of increasing power, are called safe, regular, and atomic. Program design is easier assuming atomic registers rather than weaker registers but the hardware implementation of an atomic register is costlier than the implementation of one of the weaker ones. By specifying either state or link communication, via shared registers that are either regular, atomic, or safe we arrive at six different network models that use
This work was partially supported by the program ACI `security and dependability' FRAGILE and by the NSERC (Canada) Discovery Grant.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 485­487, 2007. c Springer-Verlag Berlin Heidelberg 2007

486

C. Johnen and L. Higham

locally shared registers. For example, the atomic-state model has atomic registers located at the nodes of the network. The other models are named similarly. An algorithm for any one of these networks could provide some fault tolerance. So, we consider a third parameter, namely, wait-freedom, which captures tolerance of stopping failures of components of the network, or self-stabilization, which captures recovery of the network from transient errors of its components. We seek to determine under what conditions it is possible to transform a wait-free (respectively, self-stabilizing) solution to a given problem under one of these models into a wait-free solution (respectively, self-stabilizing) solution under another of the six models. In an earlier paper [2], we proved that a wait-free compiler from atomicstate systems to atomic-link systems requires that if two processors, a and b, each share a register with a third processor c, then a and b must themselves share a register. But, in a network, processors that are not neighbours cannot share a register. A consequence of this essential distinction between networks and globally shared memory systems is the impossibility: "There is no waitfree compiler from atomic-state systems to regular-state systems for the same network graph for any network graph that is not complete". We also presented a self-stabilizing compiler from network graphs where neighbours communicate via atomic-state registers to systems where neighbours communicate via atomiclink registers [2]. This compiler, however, had some shortcomings. It is not a silence-preserving compiler; it requires that each processor in the atomic-state system being implemented executes an atomic-state read operation infinitely often; the implementation of the atomic-state write operation is not wait-free; the implementation of the atomic-state read operation is not even obstructionfree. Furthermore, the proof of correctness failed to characterize the legitimate configurations: instead it only established that all computations of the compiled algorithm are eventually linearizable. Contributions. Our principal result is a self-stabilizing compiler from the atomicstate model to the regular-state model. This compiler is also silent. That is, if, once registers are stabilized, the atomic-state algorithm does not require the participation of neighbours, then the transformed regular-state algorithm also does not require the participation of neighbours. As a consequence, our compiler does not add significant overhead to communication. The code and the selfstabilization proof is presented in a technical report [3]. Our compiler has some additional appealing properties: The size of each shared regular register used by the compiled algorithm is log (M ) + 1 + log (B ) bits where M is the number of processor states of the initial algorithm and B is greater than the network degree. The compiled algorithm has strong progress guarantees. Specifically, the implementation of any write operation is wait-free. The implementation of a read is not wait-free, but it is obstruction-free. For all the remaining relationships (both possibilities and impossibilities) among the four models that use atomic and regular registers under either self-stabilizing and wait-free requirements, we either observe that they have been answered by existing research or we show how they can be derived from combinations of

Fault-Tolerant Implementations of the Atomic-State Communication
State
[2]

487

Link

Atomic
[2] [2] [2] [2]

[1]

Lamport [2] [2] compiler

Regular

[2]

A

B : Self-Stabilizing implementation of A into B A

A

B : Impossibility of Wait-Free implementation

B : Wait-Free and Self-stabilizing implementation

Fig. 1. Transformations between network models

earlier results. Thus, our compiler closes the proposed questions among four of the six models. These results are summarized in the following figure. What remains open is whether or not there is a self-stabilizing compiler from networks (state or link) with regular registers, to the corresponding network with only safe registers. Interestingly, Lamport's construction of single-writer singlereader single-bit regular registers from single-writer single-reader single-bit safe registers [4] fails to be self-stabilizing [1]. We conjecture that this shortcoming can be rectified at the expense of wait-freedom.

References
1. Hoepman, J.H., Papatriantafilou, M., Tsigas, P.: Self-stabilization of wait-free shared memory objects. Journal of Parallel and Distributed Computing 62(5), 818­ 842 (2002) 2. Higham, L., Johnen, C.: Relationships between communication models in networks using atomic registers. In: IPDPS'2006, the 20th IEEE International Parallel & Distributed Processing Symposium. IEEE Computer Society Press, Los Alamitos (2006) 3. Higham, L., Johnen, C.: Self-stabilizing implementation of atomic register by regular register in networks framework. Technical Report 1449, L.R.I (2006) 4. Lamport, L.: On interprocess communication. Distributed Computing 1(2), 77­101 (1986)

Transaction Safe Nonblocking Data Structures
Virendra J. Marathe, Michael F. Spear, and Michael L. Scott
Department of Computer Science, University of Rochester Rochester, NY 14627-0226 USA {vmarathe,spear,scott}@cs.rochester.edu

This brief announcement focuses on interoperability of software transactions with ad hoc nonblocking algorithms. Specifically, we modify arbitrary nonblocking operations so that (1) they can be used both inside and outside transactions, (2) external uses serialize with transactions, and (3) internal uses succeed if and only if the surrounding transaction commits. Interoperability enables seemless integration with legacy code, atomic composition of nonblocking operations, and the equivalent of hand-optimized, closed nested transactions. The key to transaction safety is to ensure that memory accesses of operations called from inside a transaction occur (or appear to occur) if, only if, and when the surrounding transaction commits. We do this by making writes manifestly speculative, with their fate tied to that of the transaction, and by logging reads for re-validation immediately before the transaction commits. (Because correct nonblocking code is designed to tolerate races, additional, intermediate validation is not required.) When called from outside a transaction, operations behave as they did in the original nonblocking code, except that they aggressively abort any transaction that stands in their way. Operations inside a transaction similarly abort transactional peers. They are unaware of nontransactional peers. We provide nonblocking objects with "transaction aware" versions of references and other basic primitive types such as integer, long, etc. These provide Get, Set, and CAS operations, which the programmer uses instead of conventional accesses. If called inside a transaction, Get logs the target location for later validation; Set and CAS speculatively modify the target location. Changes become permanent at transaction commit time. If called outside a transaction, all three operations "clean up" any encountered speculative updates, aborting conflicting transactions if necessary. Given correct nonblocking code, the changes required to create a transaction-safe version are mechanical. To make a type transaction-aware, we must be able to distinguish between real and speculative values. For some types (e.g., pointers in C) we may be able to claim an otherwise unused bit, or use features such as runtime type identification in strongly typed languages such as Java. For others we may use a sentinel value to trigger address-based lookup in a separate metadata table. With support for transaction aware primitives, we expect that construction of transaction safe versions of nonblocking algorithms would require little or no
This work was supported in part by NSF grants CNS-0411127 and CNS-0615139, equipment support from Sun Microsystems Laboratories, and financial support from Intel and Microsoft.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 488­489, 2007. © Springer-Verlag Berlin Heidelberg 2007

Transaction Safe Nonblocking Data Structures
3e+06 MS-Queue 0% txns 10% txns 20% txns 50% txns 100% txns ASTM-Queue 1.8e+06 1.6e+06 1.4e+06 1.2e+06 1e+06 1.5e+06 800000 1e+06 600000 400000 500000 200000 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Threads 0 0 2 4 6 Harris-LL 0% txns 10% txns 20% txns 50% txns 100% txns ASTM-LL

489

2.5e+06

Operations/second

2e+06

8 10 12 14 16 18 20 22 24 26 28 30 32 Threads

Fig. 1. Performance of transaction safe nonblocking objects with varying percentage of transactional and nontransactional invocations (50% inserts and 50% deletes). Experiments on a 16-processor 6800 SunFire cache coherent multiprocessor machine. Comparison with original nonblocking algorithms, and a natural transactional implementation.

additional programming effort, particularly if these primitives are supported in standard libraries. Our preliminary implementation is in the context of the ASTM [3] system, where we extended the AtomicReference Java library class with a transaction aware version TxAtomicRef . We leveraged ASTM's transactional metadata structure (which consists of an indirection object called the locator that determines the current consistent version of the data, and its current writer transaction) to represent speculative values of TxAtomicRef s. We implemented several nonblocking algorithms using TxAtomicRef including Michael and Scott's lock-free queue [4] and Harris' lock-free linked list [1] (results in Figure 1). In all cases we simply replaced the AtomicReferences in the original algorithms with TxAtomicRef s in our constructions. Our results suggest that while transaction safety makes nonblocking data structures somewhat slower, the resulting constructs interoperate smoothly with transactions, and can significantly outperform the natural "fully transactional" alternatives.

References
[1] [2] [3] [4] Harris, T.L.: A Pragmatic Implementation of Non-Blocking Linked-Lists. In: 15th Intl. Symp. on Distributed Computing, Lisboa, Portugal (October 2001) Herlihy, M., Moss, J.E.: Transactional Memory: Architectural Support for LockFree Data Structures. In: 20th Intl. Symp. on Computer Architecture, San Diego, CA (May 1993) Marathe, V.J., Scherer III, W.N., Scott, M.L.: Adaptive Software Transactional Memory. In: 19th Intl. Symp. on Distributed Computing, Cracow, Poland (September 2005) Michael, M.M., Scott, M.L.: Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms. In: 15th ACM Symp. on Principles of Distributed Computing, Philadelphia, PA (May 1996)

Long Live Continuous Consensus
Tal Mizrahi and Yoram Moses
Department of Electrical Engineering, Technion, Haifa, 32000 Israel

Fault-tolerant systems often require a means by which independent processes or processors can arrive at an exact mutual agreement of some kind. The work announced in this note studies the continuous consensus problem, which is a general tool for enabling actions that are performed at the same time at different sites of the system to be consistent with one another (e.g., mutual exlusion, firing squad etc). Suppose that we are interested in maintaining a simultaneously consistent view regarding a set of events E in the system. These are applicationdependent, but will typically record inputs that processes receive at various times, values that certain variables have at a given time, and faulty behavior in the form of failed or inconsistent message deliveries. A continuous consensus (CC) protocol maintains at all times k  0 a core Mi [k ] of events of E at every site i. In every run of this protocol the following three properties are required to hold, for all nonfaulty processes i and j . Accuracy: All events in Mi [k ] occurred in the run; Consistency: Mi [k ] = Mj [k ] at all times k ; and Completeness: If the occurrence of an event e  E is known to process j at any point, then e  Mi [k ] must hold at some time k . A CC protocol can replace the need for initiating separate instances of a consensus protocol. By monitoring a number of events, the protocol can automatically ensure consensus on a variety of issues. Continuous consensus was defined in [2], where it was studied in the crash and sending omissions failure models. We consider the continuous consensus problem in harsher failure models. More interestingly, since continuous consensus is a service that should operate indefinitely, we study it under the bounded failure interval assumption, which is parameterized by a pair (t, T ). The assumption states that at most t processes act in a faulty manner over any interval consisting of T rounds. In particular, it may well be the case that no process behaves correctly throughout an execution. This model was introduced by Castro and Liskov [1], who termed the interval T a Window of vulnerability, and studied protocols for such failure behavior in the context of malicious Byzantine failures. Note that the standard assumption that at most t processes fail overall is represented by the (t, ) assumption. An important distinction between (t, T ) and (t, ) is the fact that correct protocols in this model must provide facilities for recovery of the data of processes that were formerly faulty and have become rehabilitated. Since processes can alternate between being considered faulty and nonfaulty during the course of an execution, we need to refine the specification of the continuous consensus problem slightly. For the purpose of the definition, we consider a process nonfaulty at time k if it behaves correctly in the preceding T rounds, as well as in the following round k + 1.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 490­491, 2007. c Springer-Verlag Berlin Heidelberg 2007

Long Live Continuous Consensus

491

The current work provides the following advances regarding continuous consensus in the different models. ­ We show that for pairs (t, T ) with t  T , no CC protocol can guarantee to ever maintain a nonempty core, even in the crash failure or sending omissions failure model. Interestingly, however, there are "lucky" behaviors of the adversary in such models, in which the core does become nonempty. (In such runs it is possible, for example, to reach simultaneous consensus.) ­ We present an optimum and efficient polynomial-time solution to CC in the crash and (sending) omissions failure models. For every pattern of failures, the core maintained by this protocol at any given time is a superset of the core that any given correct (even computationally unbounded) protocol for CC would provide. Based on the (t, T ) failure assumption, failure information may become outdated, allowing the adversary new powers to fail processes. As a result, the time it takes information to enter the core may vary in both directions (increasing and decreasing) during a given execution. Nevertheless, we show that the core itself is never reduced in size. The protocol we design is a non-trivial modification of the uniform CC protocol presented in [2] under the (t, ) bound assumption. ­ We study CC in the general omissions (see [3]) and authenticated Byzantine models ([4]). We modify a lower bound in [3] to prove that an optimum protocol for CC in these models requires NP-hard computations to be performed between rounds. This news is not detrimental, since optimum solutions are rare in general. For eventual consensus, for example, it has been shown in [3] that no optimum protocol exists at all. We first present continuous consensus protocols for (t, T ) failures of both types in which the information of correct processes enters the core within t + 1 rounds. We then proceed to provide CC protocols that place data in the core faster than that in many runs. To this end, we present two types of protocols. One type consists of computationally efficient protocols that have good behavior in the best case, and more theoretical protocols that make use of an NP oracle and produce good performance much more often. We believe that both the the use of a continuous consensus service and the (t, T )-bounded failures model are worthwhile abstractions that will find many uses in a variety of applications.

References
1. Castro, M., Liskov, B.: Proactive recovery in a Byzantine-fault-tolerant system. ACM Trans. on Computer Systems, 398­461 (2002) 2. Mizrahi, T., Moses, Y.: Continuous consensus via common knowledge, Distributed Computing. An early version appears. In: The Proceedings of TARK X, pp. 236­252 (2005) (to appear, 2008) 3. Moses, Y., Tuttle, M.R.: Programming simultaneous actions using common knowledge. Algorithmica 3, 121­169 (1988) 4. Pease, M., Shostak, R., Lamport, L.: Reaching agreement in the presence of faults. Journal of the ACM 27(2), 228­234 (1980)

Fully Distributed Algorithms for Convex Optimization Problems
Damon Mosk-Aoyama1, Tim Roughgarden1, and Devavrat Shah2
2

Department of Computer Science, Stanford University Department of Electrical Engineering and Computer Science, MIT

1

Abstract. We describe a distributed algorithm for convex constrained optimization in networks without any consistent naming infrastructure. The algorithm produces an approximately feasible and near-optimal solution in time polynomial in the network size, the inverse of the permitted error, and a measure of curvature variation in the dual optimization problem. It blends, in a novel way, gossip-based information spreading, iterative gradient ascent, and the barrier method from the design of interior-point algorithms.

1

Problem Description

We consider an undirected graph G = (V, E ), with V = {1, . . . , n}, in which each node i has a non-negative decision variable xi . Our goal is to solve convex optimization problems of the following form.
n

minimize subject to

f (x) = Ax = b xi  0,
i=1

fi (xi )

(1)

i = 1, . . . , n

We assume that each function fi is twice differentiable and strictly convex, with limxi 0 fi (xi ) <  and limxi  fi (xi ) = . The elements of the m × n matrix A and the vector b  Rm are non-negative. Although we seek a solution to the primal problem (1), instead of directly enforcing the non-negativity constraints, we introduce a logarithmic barrier. For a parameter  > 0, we replace the objective function in (1) by n i=1 (fi (xi ) -  ln xi ), and we remove the constraints xi  0 to obtain a primal barrier problem. The associated Lagrange dual problem is the following optimization problem, for which a feasible solution is any vector   Rm . maximize g () = -bT  +
n i=1

x i >0

inf fi (xi ) -  ln xi + aT i xi

(2)

We assume that the primal barrier problem is feasible; that is, there exists a vector x  Rn with xi > 0 such that Ax = b. As a result, the optimal value of
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 492­493, 2007. c Springer-Verlag Berlin Heidelberg 2007

Fully Distributed Algorithms for Convex Optimization Problems

493

the primal barrier problem is finite, and Slater's condition implies that the dual problem (2) has the same optimal value, and there exists a dual solution  that achieves this optimal value. Since (2) is an unconstrained maximization problem with a strictly concave objective function, the optimal solution  is unique.

2

Algorithm Description

Our approach to solving the primal problem (1) is to apply gradient ascent for the dual barrier problem (2). The algorithm generates a sequence of feasible solutions 0 , 1 , 2 , . . . for (2), where 0 is an initial vector provided as input. To update k-1 to k in an iteration k , the algorithm uses the gradient g k-1 to determine the direction of the difference k - k-1 . For a dual solution , the gradient g () is given by g () = Ax() - b, 1 where the vector x()  Rn is defined by xi () = h- - aT i  . Here, the function i hi is defined as hi (xi ) = fi (xi ) - /xi , and ai is the ith column of the matrix A. To compute a component j  {1, . . . , m} of the gradient, the nodes estimate the sum n i=1 Aji xi () using a distributed summation algorithm by Mosk-Aoyama and Shah (PODC, 2006). This is a randomized algorithm that takes an error parameter 1 as input, and is said to succeed if the output value it produces is within a factor of 1 ± 1 of the actual sum. Based on the formula above for g (), the norm of the gradient measures how far the vector x() is from satisfying the equality constraints in (1). The nodes continue to execute iterations of gradient ascent until the 2 -norm g () goes below a threshold determined by an input error parameter, where  is the current dual solution. At this point, the algorithm terminates with the vector x() as the output. The number of iterations required for the algorithm to terminate can be bounded in terms of the following quantity. R= maxi=1,...,n maxB ( , mini=1,...,n minB ( ,
0 - ) 0 - ) 1 h- i 1 h- i

- aT i  - aT i 

max AT min (AT )

2 2

(3)

Here, B ( , r) = { |  -   r}, and min AT and max AT denote the smallest and largest singular values, respectively, of the matrix AT . Given an error parameter as input, the nodes set the parameters used in the gradient ascent so that, provided that every invocation of the summation subroutine succeeds, the number of iterations executed is bounded as follows. Theorem 1. After O R2 log Ax 0 - b /( b ) iterations, the gradient ascent terminates with a solution x() such that Ax() - b  b . The objective function value of the solution satisfies f (x())  OPT + b  + n, where OPT is the optimal value of the primal program (1). Some of the parameters in the gradient ascent are set by the nodes using quantities, such as R, whose values would be unknown to the nodes. A complete algorithm for computing an approximate solution to the primal problem (1) with high probability can be obtained by adding an outer loop to the algorithm that executes gradient ascent for different possible values of these quantities.

On the Power of Impersonation Attacks
Michael Okun
Weizmann Institute of Science mush@weizmann.ac.il

Background. In the standard message passing models it is assumed that the identity of a sender is known to the receiver. In practice, this often is not the case, due to impersonation attacks by malicious adversaries. Various impersonation attack schemes have been extensively investigated in the context of network security or cryptography, in particular for peep-to-peer and sensor networks [4,5]. Here, we study this problem in the context of distributed computing theory. Consider a set of n processors, p1 , ..., pn , communicating by means of pointto-point message passing between every pair of processors. Assume that the message sender is identified by including its id in the message. For simplicity the communication is assumed to be synchronous. The adversary is an external entity capable of injecting messages with arbitrary content into the network (but it is incapable of preventing the processors from receiving each other's messages). The ids of the processors are assumed to be fixed and known a priori, thus injecting messages that impersonate the real processors is the only way by which the adversary can interfere with the computation. Adversarial behavior of this kind is known as stolen identities Sybil attack [4,5]. For the purpose of formal analysis, the strength of the adversary is quantified by the number of messages it is able to send to each processor in every round. A k -adversary can generate up to k messages for every processor, so that a processor can receive up to n + k messages in a round, instead of just n correct messages. This formulation includes the particular cases of an adversary that in every round can impersonate some specific k processors, or of a system with n + k processors, k of which are Byzantine, capable of sending messages with arbitrary ids and content. When a processor receives several messages tagged by pi , it might be impossible to know which one of them is correct, in which case it is reasonable to drop these messages altogether. If all the messages having a "fake twin" are handled this way, we end up with a synchronous mobile failures model [7] in which the number of transmission failures with respect to every receiver is bounded by k . This is known to be equivalent to the standard asynchronous crash failure model [6]. The opposite direction, however, is not true - the k -adversary model is strictly stronger than the asynchronous model with k failures, because all the messages sent in every round by the processors are received. For example, in the impersonation model each processor is able to compute (in a single round) an upper bound on the input values of all the processors, which is impossible in the asynchronous case even with a single failure. Thus the question of the relative power of the impersonation model remains.
This research was partially supported by Sir Charles Clore fellowship and the Ministry of Defence.
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 494­495, 2007. c Springer-Verlag Berlin Heidelberg 2007

On the Power of Impersonation Attacks

495

Results. To answer the above question we have considered the k -set agreement problem (and consensus in particular) and the renaming problem in the impersonation model. There exists a simple bivalency proof, similar to [1], which shows that deterministic consensus is impossible even in the presence of 1-adversary. For the k -set agreement problem, there exists an algorithm in the presence of (k - 1)adversary, but no deterministic algorithm resilient against a k -adversary. The proof of the latter result uses the combinatorial topology machinery from [3]. For the renaming problem1 there exists a simple order-preserving algorithm resilient against a k -adversary, that has a target namespace of size n + k , which is optimal. In the asynchronous case, the minimum possible size of the target namespace of any order-preserving algorithm resilient to t failures is 2t (n - t + 1) - 1 [2]. Whereas for asynchronous order-preserving renaming, the large target namespace is a result of complete uncertainty about the input values of some processors, in the impersonation model this uncertainty is reduced (eventually the input of each processor is known to belong to a small set of possible values), and as a result the size of the target namespace is significantly smaller. In summary, our results show that the effects of an impersonation attack, mobile failures and the loss of synchrony are very much alike. The subtle difference in the computational power of the models is not evident for k -set agreement. On the other hand, renaming, which is the easier among the coordination problems, reveals that the models are not equivalent.

References
1. Aguilera, M.K., Toueg, S.: Simple Bivalency Proof that t-Resilient Consensus Requires t + 1 Rounds. Inf. Proc. Lett. 71(3-4), 155­158 (1999) 2. Attiya, H., Bar-Noy, A., Dolev, D., Peleg, D., Reischuk, R.: Renaming in an Asynchronous Environment. J. ACM 37(3), 524­548 (1990) 3. Chaudhuri, S., Herlihy, M., Lynch, N.A., Tuttle, M.R.: Tight Bounds for k-set Agreement. J. ACM 47(5), 912­943 (2000) 4. Douceur, J.R.: The Sybil Attack. In: Druschel, P., Kaashoek, M.F., Rowstron, A. (eds.) IPTPS 2002. LNCS, vol. 2429, pp. 251­260. Springer, Heidelberg (2002) 5. Newsome, J., Shi, E., Song, D.X., Perrig, A.: The Sybil Attack in Sensor Networks: Analysis & Defenses. In: Proc. 3rd International Symposium on Information Processing in Sensor Networks (IPSN), pp. 259­268 (2004) 6. Raynal, M., Roy, M.: A Note on a Simple Equivalence between Round-based Synchronous and Asynchronous Models. In: Proc. 11th IEEE Pacific Rim International Symposium on Dependable Computing (PRDC), pp. 387­392 (2005) 7. Santoro, N., Widmayer, P.: Time is Not a Healer. In: Cori, R., Monien, B. (eds.) STACS 89. LNCS, vol. 349, pp. 304­313. Springer, Heidelberg (1989)

1

To avoid a trivial solution, processor ids can be considered as entities that can be tested for equality but not compared, while renaming has to be performed using a unique private input provided to each processor.

Perfectly Reliable and Secure Communication in Directed Networks Tolerating Mixed Adversary
Arpita Patra1, Ashish Choudhary1 , , Kannan Srinathan2 , and Chandrasekharan Pandu Rangan1
Dept of Computer Science and Engineering IIT Madras, Chennai India 600036 {arpita,ashishc}@cse.iitm.ernet.in, rangan@iitm.ernet.in 2 International Institute of Information Technology Hyderabad India 500032 srinathan@iiit.ac.in
1

We characterize Perfectly Secure Message Transmission (PSMT) between two nodes S and R in directed wire model, assuming that n wires are directed from S to R (also termed as top band) and u wires are directed from R to S (also termed as bottom band). A mixed adversary (tb , tf ) controls tb wires in Byzantine fashion and tf in fail-stop fashion among these u + n wires with unbounded computing power. S wishes to send a message m from a finite field F in a perfectly secure manner to R such that the adversary gets no information whatsoever on m, though he has unbounded computing power. Our characterization is the first ever characterization for PSMT considering mixed adversary and reveals more fault tolerance than the existing results [1]. Our protocols terminates in constant number of phases1 , performs polynomial computation and have polynomial communication complexity. The values n, u, tb and tf are system parameters and known publicly. The characterization for the PSMT depending upon the value of u, tb and tf is as follows: Theorem 1. Let G = (V, E ) be a digraph and S, R  V with u wires in the bottom band and n wires in the top band such that the wires in the top band and bottom band are disjoint. Let G be under the influence of a mixed adversary (tb , tf ). Then depending upon the value of u, the following holds: 1. If 1  u  tf , then PSMT is possible iff n = max{3tb + tf +1 - u, 2tb + tf +1}. 2. If tf + 1  u  tb + tf then PSMT is possible iff n = max{3tb + 2tf + 1 - 2u, 2tb + tf + 1}. 3. If u > tb + tf then PSMT is possible iff n  2tb + tf + 1. Due to space constraint, we only consider the first case2 .
The Full Version of this paper is available at [2]. Work Supported by Project No. CSE/05-06/076/DITX/CPAN on Protocols for Secure Communication and Computation Sponsored by Department of Information Technology, Government of India. A phase is a communication from S to R or vice-versa. The complete proof is available at [2].

1 2

A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 496­498, 2007. c Springer-Verlag Berlin Heidelberg 2007

Perfectly Reliable and Secure Communication

497

Proof: Necessity: Any PSMT protocol should also be reliable for which there should exist 2tb + tf + 1 wires in the top band3 . Sufficiency: We design a three phase PSMT protocol which securely sends m from S to R provided there exists n = 2tb + tf + 1 wires Fi , 1  i  n in the top band and u > tb + tf wires Bj , 1  j  u in the bottom band. We summarize the properties of the protocol. Any adversary controlling at most tb + tf wires among n + u wires gets no information about m. If S correctly receives the original conflict graph through at least one wire in the bottom band, then S and R will identify all Fi over which Q(x, i) had been changed during Phase I. The communication complexity of the protocol is O(n4 ) field elements4 .
Phase I: S to R tb tb i j · S selects bivariate polynomial Q(x, y ) = i=0 j =0 rij x y , where rij 's are randomly chosen from F which are independent of m where r00 = Q(0, 0) = m. S evaluates Q(x, y ) at y = 1, 2, . . . , n where each Q(x, i) is a polynomial in x of degree tb and sends over Fi , 1  i  n the polynomial Q(x, i) and the values Q(x, j ), 1  j  n at x = i, denoted by vji . The n tuple [vj1 vj2 . . . vjn ], 1  j  n corresponds to the Reed-Solomon codeword of Q(x, j ), 1  j  n. Phase II: R to S · During Phase I at most tf wires may fail to deliver any values. However, R will receive information over at least 2tb + 1 wires. Let R receives information over Fi1 , Fi2 , . . . , Fi where 2tb + 1    2tb + tf + 1. Suppose R receives over Fik , 1  k   the polynomial Q (x, ik ) and the values vji , 1  j  n. The received codeword (possibly shortened) [vji1 vji2 . . . vji ] can differ from k corresponding the actual (shortened) codeword [vji1 vji2 . . . vji ] in at most tb locations. · R creates a directed graph H = (W , E ), called conflict graph such that W = {Fi1 , Fi2 , . . . , Fi } and (Fij , Fik )  E if Q (ik , ij ) = vij i , 1  ij , ik  . Thus there exists an arc from Fij to Fik in H if the value of Q (x, ij ) received over Fij when evaluated at x = ik does not match the corresponding value vi i received over Fik implying that either Fij or Fik or both are corrupted. Corresponding to each arc (Fij , Fik )  H , R adds a four tuple (Fij , Fik , Q (ik , ij ), vij i ) to a list X . R finally broadcasts the k list X to S through the bottom band. Phase III from S to R · During Phase II, the adversary might block the communication over at most tf wires in the bottom band and change X to some arbitrary list X . In the worst case, S may receive at most tb + 1 different lists. Let S receives distinct lists L1 , L2 , . . . , L through the bottom band, where 1    tb +1. For each such list Lp , S does the following: S creates a fault list denoted by Lpf ault which is initialized to . For each four tuple (Fij , Fik , Q (ik , ij ), vi
? k j ik j k k

) present in the list Lp , S locally checks Q (ik , ij ) = Q(ik , ij )

?

and vij ik = vij i . Depending upon the outcome of the test, S concludes that either R had received incorrect Q(x, ij ) through wire Fij or R had received the value of the polynomial Q(x, ij ) at x = ik incorrectly through wire Fik (or both) and hence accordingly add Fij or Fik (or both) to Lpf ault . After performing the above steps for each received list Lp , S broadcasts to R the pairs (Lp , Lpf ault ).

Message Recovery by R R correctly receives the pairs (Lp , Lpf ault ) and checks for the original X which it had sent during Phase II. Since u > tb + tf , even if during Phase II the adversary had blocked communication over tf wires and changed the original list X to X over tb wires, S will correctly receive X through at least one wire. Hence after Phase III, R always finds the original list X in the received pairs (Lp , Lpf ault ). Let the received pair corresponding to list X be (Lz , Lzf ault ). From Lzf ault , R comes to know the identity of all incorrect Q(x, ij )'s out of the 2tb + 1    2tb + tf + 1 Q(x, i)'s that R had received during Phase I, neglects them, interpolates Q(x, y ) using the remaining Q(x, ij )'s and recovers m = Q(0, 0).

3 4

This is a necessary condition for reliable communication between S and R [2]. Full details can be found in [2].

498

A. Patra et al.

References
1. Desmedt, Y., Wang, Y.: Perfectly secure message transmission revisited. Cryptology ePrint Archive, Report 2002/128 (2002), http://eprint.iacr.org 2. Patra, A., Choudhary, A., Srinathan, K., Rangan, C.P.: Perfectly Reliable and Secure Communication in Directed Networks Tolerating Mixed Adversary. Available at http://www.cs.iitm.ernet.in/ ashishc/ashish pub.html

A Formal Analysis of the Deferred Update Technique
Rodrigo Schmidt1,2 and Fernando Pedone2
1

´ Ecole Polytechnique F´ ed´ erale de Lausanne, Switzerland 2 University of Lugano, Switzerland

Introduction. In the deferred update technique for database replication, a number of database replicas are used to implement a single serializable database interface. Its main idea consists in executing all operations of a transaction initially on a single database replica. Transactions that do not change the database state can commit locally to the replica they executed, but other transactions must be globally certified and, if committed, have their update operations submitted to all replicas. Despite its wide use, we are not aware of any work that explored the inherent limitations and characteristics of deferred update database replication, ours being the first attempt in this direction. We specify a general abstract deferred update algorithm that embraces all the protocols we know of. This general specification allows for a better understanding of the technique, including its requirements and limitations and can be used to ease designing and proving specific protocols. The Deferred Update Abstraction. Due to the space limitation, we present only the general idea of our approach and results in this brief announcement. In our extended technical report [1], we present complete specifications and explain the results in detail. We start with a general serializable database specification, later used to prove our abstraction correct, and gradually move towards an abstract deferred update algorithm. All our specifications have been translated into the TLA+ specification language [2] and model checked. From our initial specification of a serializable database, we formalize the notion of order-preserving serializability, introduced by Beeri et al. in the context of nested transactions [3], for its use in deferred update replication. While previous works assumed replicas should satisfy order-preserving serializability to ensure global serializability, we show that serializability is guaranteed if replicas satisfy the weaker notion of active order-preserving serializability that we introduce. Some multiversion concurrency control mechanisms [4], for example, are active order-preserving but not strict order-preserving; yet, our results show that they can be used in deferred updated protocols. Our specification of serializability also allows us to reason better about the actions required to implement it using a number of internal database replicas. From that, we could specify the atomic actions that abstract the deferred update technique, that is, the actions implemented by all deferred update protocols. The abstract deferred update algorithm we present allows us to isolate the properties
The work presented in this paper has been partially funded by the SNSF, Switzerland (project #200021-170824).
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 499­500, 2007. c Springer-Verlag Berlin Heidelberg 2007

500

R. Schmidt and F. Pedone

of the termination protocol, responsible for certifying and propagating update transactions to the replicas. We specify three safety properties, namely Nontriviality, Stability, and Consistency, and discuss their necessity. This discussion brings out the result that update transactions cannot be propagated to replicas in different orders, even if they operate (read or write) on completely disjunct subsets of the data items, for it can break serializability. This is rather counterintuitive since serializability would allow such transactions to be scheduled in any order. Our properties imply that the termination protocol must ensure something stronger than just the serializability of the update transactions, which means that proving only that does not suffice. By extending the termination protocol with a simple liveness property that ensures propagation of committed transactions, we were able to show that its properties necessarily implement, for committed transactions, the Sequence Agreement problem explained in [5]. Briefly, in the sequence agreement problem, a set of processes agree on an ever-growing sequence of values, built out of proposed ones. This problem is a sequence-based specification of the celebrated Atomic Broadcast problem. Our result implies that implementations of the termination protocol are free to abort transactions, but they must atomically broadcast the transactions they commit. As a consequence, any lower bound or impossibility result for atomic broadcast and consensus applies to the termination protocol. Conclusion. We have formalized the deferred update technique for database replication and stated some intrinsic characteristics and limitations of it. Previous works have only considered new algorithms, with independent specifications, analysis, and correctness proofs. To the best of our knowledge, our work is the first effort to formally characterize this family of algorithms and establish its requirements. Our general abstraction can be used to derive other lower bounds as well as to create new algorithms and prove existing ones correct. Some algorithms can be easily proved correct by a refinement mapping to ours. Others may require an additional effort due to their extra assumptions, but the task seems easier than with previous formalisms. In our experience, we have successfully used our abstraction to obtain interesting protocols and correctness proofs.

References
1. Schmidt, R., Pedone, F.: A formal analysis of the deferred update technique. Technical report, EPFL (2007) 2. Lamport, L.: Specifying Systems: The TLA+ Language and Tools for Hardware and Software Engineers. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA (2002) 3. Beeri, C., Bernstein, P.A., Goodman, N.: A model for concurrency in nested transaction systems. Journal of the ACM 36(2), 230­269 (1989) 4. Bernstein, P., Hadzilacos, V., Goodman, N.: Concurrency Control and Recovery in Database Systems. Addison-Wesley, Reading (1987) 5. Lamport, L.: Generalized consensus and paxos. Technical Report MSR-TR-2005-33, Microsoft Research (2004)

DISC at Its 20th Anniversary (Stockholm, 2006)
Michel Raynal1 , Sam Toueg2, and Shmuel Zaks3
2 1 IRISA, Campus de Beaulieu, 35042 Rennes, France Department of Computer Science, University of Toronto, Toronto, Canada 3 Department of Computer Science, Technion, Haifa, Israel

Prologue
DISC 2006 marked the 20th anniversary of the DISC conferences. We list below the special events that took place during DISC 2006, together with some information and perspectives on the past and future of DISC.

Special 20th Anniversary Events
The celebration of the 20th anniversary of DISC consisted in four main events: invited talks by three of the brightest figures of the distributed computing community, and a panel involving all the people who were at the very beginning of DISC (the abstracts of the invited talks appear independently in these proceedings). ­ An invited talk "Time, clocks and the ordering of my ideas about distributed systems'' by Leslie Lamport. ­ An invited talk "My early days in distributed computing theory: 1979-1982" by Nancy Lynch. ­ An invited talk "Provably unbreakable hyper-encryption using distributed systems" by Michael Rabin. ­ A panel that discussed the contributions of the WDAG/DISC community to distributed computing from a historical perspective. The panelists (Eli Gafni, Jan van Leeuwen, Nicola Santoro, Shmuel Zaks) and the moderator (Michel Raynal) were the members of the program committee of the second DISC (called WDAG at that time), held in Amsterdam. The panel reviewed the status of many contributions to network protocol design and to the understanding of distributed computing in general. It also discussed the possible ways in which DISC may evolve in the future.

Past: A Short History
The Workshop on Distributed Algorithms on Graphs (WDAG) was initiated by Eli Gafni, Nicola Santoro and Jan van Leeuwen in 1985. It was intended to provide a forum for researchers and other interested parties to present and discuss recent results and trends in the design and analysis of distributed algorithms on communication networks and graphs. Then, more than 10 years later, the acronym WDAG was changed to DISC (the international symposium on DIStributed Computing). This change was made to reflect the
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 501­503, 2007. c Springer-Verlag Berlin Heidelberg 2007

502

M. Raynal, S. Toueg, and S. Zaks

expansion from a workshop to a symposium as well as the expansion of the research areas of interest. So, following 11 successful WDAGs, DISC'98 was the 12th in the series. Since 1996 WDAG/DISC has been managed by a Steering Committee consisting of some of the most experienced members of the distributed computing community. The main role of this committee is to provide guidance and leadership to ensure the continuing success of this conference. To do so, the committee oversees the continuous evolution of the symposium's research areas of interest, it forges ties with other related conferences and workshops, and it also maintains contact with Springer-Verlag and other professional or scientific sponsoring organizations (such as EATCS). The structure and rules of the DISC Steering Committee, which were composed by Sam Toueg and Shmuel Zaks, and approved by the participants at the the 1996 WDAG business meeting in Bologna, can be found at http://www.disc-conference.org. This site also contain information about previous WDAG and DISC conferences. The location, program chairs, and proceedings of the WDAG/DISC meetings are summarized in Table 1, and the Steering Committee Chairs are listed in Table 2.
Table 1. The past (and present) Wdag/Disc

Year

Location

Program Chair(s)
N. Santoro and J. van Leeuwen J. van Leeuwen J.-Cl. Bermond and M. Raynal N. Santoro and J. van Leeuwen S. Toueg and P. Spirakis A. Segall and S. Zaks A. Schiper G. Tel and P. Vit´ anyi J.-M. H´ elary and M. Raynal ¨ Babaogl O. u and K. Marzullo M. Mavronicolas and Ph. Tsigas S. Kutten P. Jayanti M. Herlihy J. Welch D. Malkhi F.E. Fich R. Guerraoui P. Fraigniaud S. Dolev A. Pelc

Proceedings
Carleton Scientific LNCS 312 LNCS 392 LNCS 486 LNCS 579 LNCS 647 LNCS 725 LNCS 857 LNCS 972 LNCS 1151 LNCS 1320 LNCS 1499 LNCS 1693 LNCS 1914 LNCS 2180 LNCS 2508 LNCS 2848 LNCS 3274 LNCS 3724 LNCS 4167 This issue

1985 Ottawa 1987 Amsterdam 1989 Nice 1990 Bari 1991 Delphi 1992 Haifa 1993 Lausanne 1994 Terschelling 1995 Le Mont-Saint-Michel 1996 Bologna 1997 Saarbr¨ ucken 1998 Andros 1999 Bratislava 2000 Toledo 2001 Lisbon 2002 Toulouse 2003 Sorrento 2004 Amsterdam 2005 Cracow 2006 Stockholm 2007 Cyprus

Table 2. Steering committee chairs 1996-1998 1998-2000 2000-2002 2002-2004 2004-2007 Sam Toueg Shmuel Zaks Andr´ e Schiper Michel Raynal Alex Shvartsman

DISC at Its 20th Anniversary (Stockholm, 2006)

503

Epilogue, and Future
Together with the whole DISC community, we congratulate DISC for its 20th anniversary. We feel proud to have taken part in this important and successful activity of our research community, and are confident that DISC will continue to play a central role in years to come. We wish to thank all those who contributed over the years to the success of DISC. Each played an essential role, and each forms a vital link in the DISC chain: ­ The local organizers, and their teams, who did everything to ensure a smooth and successful conference, ­ The program committee chairs, program committee members, and external referees, who ensured the high academic level of the conference, ­ The participants of the WDAG and DISC conferences, ­ The steering committee members, ­ The sponsor organizations, for their generous support over the years, and - last but not least ­ All the members of the distributed computing community who submitted papers to WDAG and DISC. We are confident that the DISC community will continue to play a central role within the distributed computing and communication networks research communities for many years to come. H APPY ANNIVERSARY TO DISC!

This photo is from DISC 2005 in Cracow, Poland, and was taken during the banquet at Wierzynek 1364 restaurant (one of the oldest restaurants in Europe). It shows the first five chairs of the DISC steering committee (from left to right: Shmuel Zaks, Alex Shvartsman, Michel Raynal, Andr´ e Schiper and Sam Toueg).

This page intentionally blank

DISC 20th Anniversary: Invited Talk Time, Clocks, and the Ordering of My Ideas About Distributed Systems
Leslie Lamport
Microsoft Corporation 1065 La Avenida Mountain View, CA 94043 U.S.A. http://lamport.org

Abstract. A guided tour through the labyrinth of my thoughts, from the Bakery Algorithm to arbiter-free marked graphs. This exercise in egotism is by invitation of the DISC 20th Anniversary Committee. I take no responsibility for the choice of topic.

A. Pelc (Ed.): DISC 2007, LNCS 4731, p. 504, 2007. c Springer-Verlag Berlin Heidelberg 2007

DISC 20th Anniversary: Invited Talk My Early Days in Distributed Computing Theory: 1979-1982
Nancy Lynch
CSAIL, MIT Cambridge, MA 02139 U.S.A. lynch@theory.csail.mit.edu

Abstract. I first became involved in Distributed Computing Theory around 1978 or 1979, as a new professor at Georgia Tech. Looking back at my first few years in the field, approximately 1979-1982, I see that they were tremendously exciting, productive, and fun. I collaborated with, and learned from, many leaders of the field, including Mike Fischer, Jim Burns, Michael Merritt, Gary Peterson, Danny Dolev, and Leslie Lamport. Results that emerged during that time included space lower bounds for mutual exclusion; definition of the k-exclusion problem, with associated lower bounds and algorithms; the Burns-Lynch lower bound on the number of registers needed for mutual exclusion; fast network-wide resource allocation algorithms; the Lynch-Fischer semantic model for distributed systems (a precursor to I/O automata); early work on proof techniques for distributed algorithms; lower bounds on the number of rounds for Byzantine agreement; definition of the approximate agreement problem and associated algorithms; and finally, the Fischer-LynchPaterson impossibility result for consensus. In this talk, I will review this early work, trying to explain how we were thinking at the time, and how the ideas in these projects influenced later work.

A. Pelc (Ed.): DISC 2007, LNCS 4731, p. 505, 2007. c Springer-Verlag Berlin Heidelberg 2007

DISC 20th Anniversary: Invited Talk Provably Unbreakable Hyper-Encryption Using Distributed Systems
Michael O. Rabin
DEAS Harvard University Cambridge, MA 02138 rabin@deas.harvard.edu

Abstract. Encryption is a fundamental building block for computer and communications technologies. Existing encryption methods depend for their security on unproven assumptions. We propose a new model, the Limited Access model for enabling a simple and practical provably unbreakable encryption scheme. A voluntary distributed network of thousands of computers each maintain and update random pages, and act as Page Server Nodes. A Sender and Receiver share a random key K. They use K to randomly select the same PSNs and download the same random pages. These are employed in groups of say 30 pages to extract One Time Pads common to S and R. Under reasonable assumptions of an Adversary's inability to monitor all PSNs, and easy ways for S and R to evade monitoring while downloading pages, Hyper Encryption is clearly unbreakable. The system has been completely implemented. Modern encryption methods depend for their security on assumptions concerning the intractability of various computational problems such as the factorization of large integers into prime factors or the computation of the discrete log function in large finite groups. Even if true, there are currently no methods for proving such assumptions. At the same time, even if these problems will be shown to be of super-polynomial complexity, there is steady progress in the development of practical algorithms for the solution of progressively larger instances of the problems in question. Thus there is no firm reason to believe that any of the encryptions in actual use is now safe, or an indication as to how long it will remain so. Furthermore, if and when the current intensive work on Quantum Computing will produce actual quantum computers, then the above encryptions will succumb to these machines. At present there are three major proposals for producing provably unbreakable encryption methods. Quantum Cryptography employs properties of quantum mechanics to enable a Sender and Receiver to create common One Time Pads (OTPs) which are secret against any Adversary. The considerable research and development work as well as the funding invested in this effort are testimony to the need felt for an absolutely safe encryption technology. At present Quantum Cryptography systems are limited in range to a few tens of miles, are sensitive to noise or disturbance of the transmission medium, and require rather expensive special equipment. The Limited Storage Model was proposed by U. Maurer. It postulates a public intensive source of random bits. An example would be a satellite or a system of satellites containing a Physical Random Number Generator (PRNG) beaming
A. Pelc (Ed.): DISC 2007, LNCS 4731, pp. 506­508, 2007. c Springer-Verlag Berlin Heidelberg 2007

Provably Unbreakable Hyper-Encryption Using Distributed Systems down a stream a of random numbers, say at the rate of 100GB/sec. S and R use a small shared key, and use those bits and the key to form OTPs which are subsequently employed in the usual manner to encrypt messages. The Limited Storage Model further postulates that for any Adversary or group of Adversaries it is technically or financially infeasible to store more than a fraction, say half, as many bits as there are in a. It was proved by Aumann, Rabin, and Ding and later by Dziembowski-Maurer, that under the Limited Storage Model assumptions, one can construct schemes producing OTPs which are essentially random even for a computationally unbounded (but storage limited) Adversary. The critique of the Limited Storage Model is three-fold. It requires a system of satellites, or other distribution methods, which are very expensive. The above rate of transmission for satellites is right now outside the available capabilities. More fundamentally, with the rapid decline of cost of storage it is not clear that storage is a limiting factor. For example, at a cost of $ 1 per GB, storing the above mentioned stream of bytes will cost about $ 3 Billion per year. And the cost of storage seems to go down very rapidly. The Limited Access Model postulates a system comprising a multitude of sources of random bytes available to the Sender and Receiver. Each of these sources serves as a Page Server Node (PSN) and has a supply ofrandom pages. Sender and Receiver initially have a shared key K. Using K, Sender and Receiver asynchronously in time access the same PSNs and download the same random pages. The Limited Access assumption is that an Adversary cannot monitor or compromise more than a fraction of the PSNs while the Sender or Receiver download pages. After downloading sufficiently many pages, S and are make sure that they have the same pages by employing a Page Reconciliation Protocol. They now employ the common random pages according to a common scheme in groups of, say, 30 pages to extract an OTP from each group. Let us assume that the extraction method is simply taking the XOR of these pages. The common OTPs are used for encryption in the usual manner. A crucially important point is that a Page Server Node sends out a requested random page at most twice, then destroys and replaces it by a new page. Opportunity knocks only twice! Why is this scheme absolutely secure? Assume that we have 5,000 voluntary participants acting as PSNs. Assume that a, possibly distributed, Adversary can eavesdrop, monitor or corrupt (including by acting as imposter) no more than 1000 of these nodes. Thus the probability that in the random choice of the 30 PSNs from which a group of 30 pages are downloaded and XORed, all 30 pages will be known to the Adversary is smaller than (1/5)30, i.e., totally negligible. But if an Adversary misses even one page out of the 30 random pages that are XORed into an OTP then the OTP is completely random for him. The send at most twice, then destroy policy, prevents a powerful Adversary from asking for a large number of pages from each of the PSNs and thereby gain copies of pages common to S and R. The worst that can happen is that, say, S will down load a page P from PSNi and the Adversary (or another user of HyperEncryption) has or will download the same page P from PSNi. When R now requests according to the key K the same page from PSNi, he will not get it. So R and S never have a page P in common if P was also downloaded by a third party. The only consequence of an Adversary's down-loading from too many PSNs is denial of service to the legitimate users of the system. This is a problem for any server system and there are ways of dealing with this type of attack.

507

508

M.O. Rabin What if an Adversary eavesdrops onto the Sender and or Receiver while they are downloading pages from PSNs. Well, S and R can go to an Internet caf or one of those establishments allowing a customer to obtain an Internet connection. They can use a device that does not identify them and download thousands of pages from PSNs within a short time. The salient point is that S and R need not time-synchronize their access to the PSNs. Once S and R have common OTPs, they can securely communicate from their fixed known locations with immunity against eavesdropping or code breaking. The initial key K is continually extended and updated by S and R using common One Time Pads. Each pair of random words from K is used to select a PSN and a page from that PSN only once and then discarded. This is essential for the absolute security of Hyper Encryption. With all these provisions Hyper Encryption in the Limited Access Model also provides Ever Lasting Secrecy. Let us make a worst case assumption that the initial common key K or its later extensions were lost or stolen after their use to collect common random pages from PSNs. Those pages are not available any more as a result of the send only twice and destroy policy. Thus the extracted OTPs used to encrypt messages cannot be reconstructed and the encryption is valid in perpetuity. By contrast, all the existing public or private key encryption methods are vulnerable to the retroactive decryption attack if the key is lost or algorithms come up that break the encryption algorithm. We shall also describe an additional scheme based on the use of search engines for the generation of OTPs and of unbreakable encryption. Our systems were fully coded in Java for distribution as freeware amongst interested users. All the protocols described below are running in the background on the participating computers and impose negligible computational and storage overheads on the host computer.

Author Index

Aiyer, Amitanand S. 7 Alvisi, Lorenzo 7 Angluin, Dana 20 Aspnes, James 20 Awerbuch, Baruch 33 Baldoni, Roberto 48 Bazzi, Rida A. 7 Beauquier, Joffroy 63 Bessani, Alysson Neves 480 Bortnikov, Edward 77 Burman, Janna 92 Chalopin, J´ er´ emie 108 Chen, Ke 373 Chen, Wei 123 Chen, Yu 123 Chockler, Gregory 139 Choudhary, Ashish 496 Cidon, Israel 77 Clement, Julien 63 Correia, Miguel 480 Czygrinow, Andrzej 152 Das, Shantanu 108 Delporte-Gallet, Carole Derbel, Bilel 179 Dolev, Danny 193 Dolev, Shlomi 208 Eisenstat, David 20 Ellen, Faith 223 Fatourou, Panagiota 223 Fauconnier, Hugues 165 Fischer, Simon 238 Freiling, Felix C. 165 Garg, Vijay K. 420 G¸ asieniec, Leszek 253 Gavoille, Cyril 179, 482 Gfeller, Beat 268 Gilbert, Seth 208, 283 Guerraoui, Rachid 139, 208, 283

Ha´ n´ ckowiak, Michal Higham, Lisa 485 Hoch, Ezra N. 193

152

Inuzuka, Nobuhiro 298 Ioannidou, Kleoni 48 Izumi, Taisuke 298 Jiang, Song 373 Johnen, Colette 485 Kantor, Erez 253 Katayama, Yoshiaki 298 Keidar, Idit 77, 139 Klasing, Ralf 482 Korman, Amos 313 Kosowski, Adrian 482 Kowalski, Dariusz R. 253, 283, 328 Kutten, Shay 92, 343 Lamport, Leslie 504 Lang, Matthew 358 Liang, Shuang 373 Liu, Xuezheng 123 Locher, Thomas 388 Lynch, Nancy 505

165

Marathe, Virendra J. 488 Masuzawa, Toshimitsu 343 Meier, Remo 388 Merideth, Michael G. 403 Messika, Stephane 63 Milani, Alessia 48 Mizrahi, Tal 490 Monien, Burkhard 1 Moniz, Henrique 480 Moses, Yoram 490 Mosk-Aoyama, Damon 492 Navarra, Alfredo 482 Neves, Nuno Ferreira 480 Newport, Calvin 208 Ogale, Vinit A. 420 Okun, Michael 494 Olbrich, Lars 238

510

Author Index Spear, Michael F. 488 Srinathan, Kannan 496 Strojnowski, Michal 328 Su, Chang 253 496 Taubenfeld, Gadi 450 Tielmann, Andreas 165 Tiemann, Karsten 1 Toueg, Sam 501 Verissimo, Paulo 480 V¨ ocking, Berthold 238 Wada, Koichi 298 Wattenhofer, Roger 388 Widmayer, Peter 268 Wong, Prudence W.H. 435 Zaks, Shmuel 435, 501 Zhang, Jialin 123 Zhang, Xiaodong 373 Zieli´ nski, Piotr 465

Patra, Arpita 496 Pedone, Fernando 499 Peleg, David 3, 179, 253, 313 Penso, Lucia Draque 165 Rabin, Michael O. 506 Rangan, Chandrasekharan Pandu Raynal, Michel 5, 501 Reiter, Michael K. 403 Rosaz, Laurent 63 Roughgarden, Tim 492 Rozoy, Brigitte 63 Ruppert, Eric 223 Santoro, Nicola 108, 268 Scheideler, Christian 33 Schmid, Stefan 388 Schmidt, Rodrigo 499 Scott, Michael L. 488 Shah, Devavrat 492 Shalom, Mordechai 435 Sivilotti, Paolo A.G. 358

