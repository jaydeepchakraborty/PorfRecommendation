Instructional Module Development System (IMODS)
Srividya K. Bansal

Odesma Dalrymple

Arizona State University
School of Computing, Informatics, Decision Systems
Engineering, Mesa, AZ 85212 USA

University of San Diego
Shiley-Marcos School of Engineering
San Diego, CA USA

srividya.bansal@asu.edu

odesma@sandiego.edu
particularly to STEM faculty. The IMODS is a web-based course
design software that:
1. Guides individual or collaborating users, step-by-step, through
an outcome-based education process as they define learning
objectives, select content to be covered, develop an instruction
and assessment plan, and define the learning environment and
context for their course(s).
2. Contains a repository of current best pedagogical and
assessment practices, and based on selections the user makes
when defining the learning objectives of the course, the
system will present options for assessment and instruction that
align with the type/level of student learning desired.
3. Generates documentation of course design. In the same
manner that an architect’s blue-print articulates the plans for a
structure, the IMODS course design documentation will
present an unequivocal statement as to what to expect when
the course is delivered.
4. Provides just-in-time help to the user. The system will
provide explanations to the user on how to perform course
design tasks efficiently and accurately.
5. Provides feedback to the user on fidelity of the course design.
This will be assessed in terms of the cohesiveness of the
alignment of the course design components (i.e., content,
assessment, and pedagogy) around the defined objectives.

ABSTRACT
To ensure that future generations of engineering, science, and
other technological practitioners are equipped with the required
knowledge and skills to continue to innovate solutions to solve
societal challenges, effective courses or instructional modules that
incorporate best pedagogical and assessment practices must be
designed and delivered. Science, technology, engineering and
mathematics (STEM) educators typically come from STEM
backgrounds and have little or no formal STEM education
training. Their approaches to learning, instruction, and assessment
mimic the experiences they were exposed to as students and are
not necessarily informed by scholarship in the area of how people
learn. The road to effective STEM instruction starts with a wellconceived and constructed plan or curriculum that includes the
tight alignment of content, pedagogical approaches and
assessments, around the learning objectives, and draws upon bestpractices in each of these areas. An information technology (IT)
tool that can guide STEM educators through the complex task of
course design development, ensure tight alignment between
various components of an instructional module, and provide
relevant information about research-based pedagogical and
assessment strategies will be of great value. This demonstration
presents a Web-based software tool called the Instructional
Module Development System (IMODS) that supports these
ventures and broadens the impact and reach of professional
development in the scholarship of teaching and learning,
particularly to STEM faculty.

2. IMODS FRAMEWORK
Outcome-based education (OBE) is an approach where the
product defines the process, i.e., the outcomes that specify what
students should be able to demonstrate upon leaving the system
are defined first, and drive decisions about the content and how it
is organized, the educational strategies, the teaching methods, the
assessment procedures and the educational environment [4], [5].
This is a contrast to the preceding “input-based” model that placed
emphasis on the means as opposed to the end of instruction. OBE
was used as the principal guide for the development of the
IMODS framework. It was chosen for the following reasons: 1)
win-for-all solution – OBE is shown to improve student success,
provides a structure to educators for designing instruction, and
facilitates reporting to external stakeholders in an accountability
education climate; 2) it supports the How People Learn
framework for designing learning environments [6]; 3) growing
adoption of outcome-based program accreditation – Accreditation
boards such as ABET, have moved to an outcome focused model
(what students learned) to assess the quality of programs in
Applied Science, Computing, Engineering, and Engineering
Technology; 4) alignment with other models that are meant to
increase innovation in STEM education – OBE dictates the end
and not the means thereby allowing innovation in instruction. It
also provides an empirical structure to track impact and identify
shortcomings. The IMODS framework adheres strongly to the
OBE approach and treats the course objective as the spine of the
structure. New constructs (not included in the models previously
discussed) are incorporated to add further definition to the

Keywords
Instruction design; Semantic web-based tool; Outcome-based
education; Assessment techniques; Instructional techniques.

1. INTRODUCTION
At many colleges and universities, engagement in scholarly
teaching is becoming a minimum expectation of faculty who are
held accountable for the quality of the learning experienced by
students enrolled in their course(s). These expectations are even
greater for STEM faculty given the national demands for a welltrained STEM workforce [1]. Since education training is not
typically included in the plan of study of most STEM programs,
faculty who graduate with STEM degrees gain their teaching
expertise post-appointment and “on-the-job”. In the absence of
formal training, most faculty can take as much as five years to
truly become proficient teachers, and during that period, it is the
students who are most affected [2]. There is a growing demand
and interest in faculty professional development in areas such as
outcome-based education [3], curriculum design, and pedagogical
and assessment strategies. In response to this demand, a number
of universities have established teaching and learning centers to
provide institution-wide, and sometimes program specific support.
Instructional Module Development System (IMODS) supports
these ventures and broaden the impact and reach of professional
development in the scholarship of teaching and learning,

258

this project. Scrum is an iterative and incremental framework for
managing product development. A sprint (or iteration) is the basic
unit of development in Scrum. The sprint is restricted to a specific
duration, two weeks in case of the IMODS project. Each sprint is
started with a planning meeting. The aim is to define a sprint
backlog where the tasks for the sprint are identified and an
estimated commitment for the sprint goal is made. Each sprint
ends with a sprint review-and-retrospective meeting, where the
progress is reviewed and shown to stakeholders and
improvements for the next sprints are identified. Groovy on Grails
is an open source, full stack, web application framework for the
Java Virtual Machine. It takes advantage of the Groovy
programming language and convention over configuration to
provide a productive and streamlined development experience.
The Groovy/Grails Tool Suite™ (GGTS) provides the best
Eclipse-powered development environment for building Groovy
and Grails applications. Git is a distributed revision control and
source code management (SCM) system with an emphasis on
speed. Grails uses Spring Model–View–Controller (MVC)
architecture as the underlying web application framework. MVC
is a software architecture pattern that separates the representation
of information from the user's interaction with it.
The IMODS framework was applied to design introductory
software engineering courses titled “Software Enterprise I:
Personal Software Process” and “Software Enterprise II: Testing
and Quality” in B.S. in Software Engineering program. Using the
IMODS framework ensured the alignment between various course
elements and thereby ensuring high-quality course design. The
framework supports the checking of alignment between course
assessments and learning objectives. The course assessments are
linked to the performance and criteria elements of the learning.
The framework supports the checking of alignment between
course instructional activities and learning objectives. Course
pedagogical activities are linked to performance and content of
the learning objective as shown in Figure 1.

objective. The work of Robert Mager [7, 8)] informs the IMOD
definition of the objective. Mager identifies three defining
characteristics of a learning objective: Performance – description
of what the learner is expected to be able to do; Conditions –
description of the conditions under which the performance is
expected to occur; and the Criterion – a description of the level of
competence that must be reached or surpassed. For use in the
IMODS framework an additional characteristic was included, i.e.,
the Content – description of the disciplinary knowledge, skill, or
behavior to be attained. The resulting IMODS definition of the
objective is referred to as the PC3 model.

Figure 1: IMODS PC3 Model
The other course design elements (i.e., Content, Pedagogy, and
Assessment) are incorporated into the IMODS framework through
interactions with two of the PC3 characteristics. Course-Content
is linked to the content and condition components of the objective.
The condition component is often stated in terms of pre-cursor
disciplinary knowledge, skills or behaviors. This information,
together with the content defined in the objective, can be used to
generate or validate the list of course topics. Course-Pedagogy is
linked to the performance and content components of the
objective. The types of instructional approaches or learning
activities used in a course should correspond to the level of
learning expected and the disciplinary knowledge, skills or
behaviors to be learned. The content and performance can be used
to validate pedagogical choices. Course-Assessment is linked to
the performance and criteria components of the objective. This
affiliation can be used to test the suitability of the assessment
strategies since an effective assessment, at the very least, must be
able to determine whether the learner’s performance constitutes
competency. Figure 1 shows PC3 model of a learning objective.

4. ACKNOWLEDGMENTS
The authors gratefully acknowledge the support for this project
under the National Science Foundation's Transforming
Undergraduate Education in Science, Technology, Engineering
and Mathematics (TUES) program Award No. DUE-1246139

5. REFERENCES
[1]
[2]
[3]
[4]

3. IMPLEMENTATION OF IMODS

[5]

Different programming platforms and technology options
were considered and appropriate technologies were identified for
the development of the IMODS web application, design of the
back-end database schema, installation and configuration of the
server-side and client-side technologies, and development of the
user interface screens for login, registration, index, and creation of
an instructional module and the connectivity of these web pages
with the backend database. An Agile software development
methodology called Scrum is being used for the development of

[6]
[7]
[8]

259

M. T. Huber and S. P. Morreale, Eds., Disciplinary Styles in the
Scholarship of Teaching and Learning: Exploring Common Ground.
AAHE Publications, 2002.
R. Boice, Advice for new faculty members. Allyn & Bacon, 2000.
G. C. Furman, “Outcome-Based Education and Accountability.,”
Educ. Urban Soc., vol. 26, no. 4, pp. 417–437, 1994.
R. M. Harden, J. R. Crosby, M. H. Davis, and M. Friedman, “AMEE
Guide No. 14: Outcome-based Education: Part 5--From Competency
to Meta-Competency: A Model for the Specification of Learning
Outcomes.,” Med. Teach., vol. 21, no. 6, pp. 546–552, 1999.
W. G. Spady, “Organizing for Results: The Basis of Authentic
Restructuring and Reform.,” Educ. Leadersh., vol. 46, no. 2, pp. 4–
8, 1988.
J. D. Bransford, A. L. Brown, and R. R. Cocking, How people learn.
National Academy Press Washington, DC, 2000.
R. F. Mager, Measuring instructional results, or, Got a match?, 2nd
ed. Belmont, Calif: Pitman Management and Training, 1984.
R. F. Mager, Preparing instructional objectives: a critical tool in the
development of effective instruction, 3rd ed. Atlanta, GA: Center for
Effective Performance, 1997.

Beyond User-to-User Access Control for Online
Social Networks
Mohamed Shehab1 , Anna Cinzia Squicciarini2 , and Gail-Joon Ahn3
1

University of North Carolina at Charlotte, NC, USA
2
The Pennsylvania State University, PA, USA
3
Arizona State University, AZ, USA
mshehab@uncc.edu, acs20@psu.edu, gahn@asu.edu

Abstract. With the development of Web 2.0 technologies, online social networks are able to provide open platforms to enable the seamless
sharing of profile data to enable public developers to interface and extend the social network services as applications (or APIs). At the same
time, these open interfaces pose serious privacy concerns as third party
applications are usually given full read access to the user profiles. Current related research has focused on mainly user-to-user interactions in
social networks, and seems to ignore the third party applications. In
this paper, we present an access control framework to manage the third
party to user interactions. Our framework is based on enabling the user
to specify the data attributes to be shared with the application and at
the same time be able to specify the degree of specificity of the shared
attributes. We model applications as finite state machines, and use the
required user profile attributes as conditions governing the application
execution. We formulate the minimal attribute generalization problem
and we propose a solution that maps the problem to the shortest path
problem to find the minimum set of attribute generalization required to
access the application services.

1

Introduction

The recent growth of social network sites such as Facebook, del.icio.us and Myspace have created many interesting and challenging problems to the research
communities. In social networks users self-organize into diﬀerent communities,
and manage their own proﬁle, as a form of self-expression. Users proﬁles usually
include information such as the user’s name, birthdate, address, contact information, emails, education, interests, photos, music, videos, blogs and many other
attributes. The structure of an example social network proﬁle is depicted in Figure 1(a). Controlling access to the user proﬁle information is a challenging task
as it requires average internet users to act as system administrators to specify
and conﬁgure access control policies for their proﬁles. To control interactions
between users, the user’s world is divided into a trusted and a non-trusted set
of users, referred to as friends and strangers respectively. Furthermore, some
social networks allow users to further partition the set of friends by geographical
L. Chen, M.D. Ryan, and G. Wang (Eds.): ICICS 2008, LNCS 5308, pp. 174–189, 2008.
c Springer-Verlag Berlin Heidelberg 2008


Beyond User-to-User Access Control for Online Social Networks

175

location, social group, organization, or by how well they know them. Users are
provided with group based access control mechanisms that apply access rules on
the diﬀerent groups of friends and strangers. Facebook [6] enables users to create a limited proﬁle and to select which users map to that proﬁle. For example,
a user could share his wedding album with his family members and not with
his colleagues from work. In addition to the issues involved with enabling ﬁne
grain access control for each user proﬁle [5] to control data attributes viewable
by other users, a yet unexplored problem is related to users’ proﬁle access from
entities diﬀerent from other social network users.
With the development of Web 2.0 technologies [20], online social networks are
able to provide open platforms to enable the seamless sharing of proﬁle data to
enable public developers to interface and extend the social network services as
applications (or APIs). For example, Facebook allows anyone to create software
plug-ins that can be added to user proﬁles to provide services based on proﬁle
data. These features have been a great success, the most popular Facebook
applications have around 24 million users as of May 2008, and competing social
networking sites have moved to create their own imitation platforms. However,
although these open platforms enable such advanced features, they also pose
serious privacy risks. Users’ proﬁles in fact have a great commercial value to
marketing companies, competing networking sites, and identity thieves. Data
mining through the development platform can potentially aﬀect more people
than screen scraping, because it exposes information that might otherwise be
hidden.
Applications that are currently added to the users’ proﬁles are given full
read access to all the proﬁles information [6,18]. The user is able to add the

Profile

Friends

Videos

Albums

Friend… Friend Album Album
…
FN
F1
AN
A1

Notes

Events

Video … Video Note … Note Event … Event
E1
V1
VN
EN
N1
NN

Personal
Information
Birth Date

Address

Phone Marital
Number Status

.

(a) Example User Profile Schema

(b) App. Addition Error Message

Fig. 1. Social Networks Profiles and Applications

176

M. Shehab, A.C. Squicciarini, and G.-J. Ahn

application only if he/she agrees to give the application access not only to his
proﬁle, but also to proﬁle data of other users viewable through that user. In
other words, the user enables the application to read information on his behalf.
If the user refuses to grant full read access to the application the installation
process fails. For example, Figure 1(b), shows the error message displayed by the
Facebook platform when the user rejects to give the application full read access
to his proﬁle data. Basically, the application access control model adopted by the
request management module is an all-or-nothing policy. As such, API developers
have access to users’ data, regardless of the actual applications’ needs, leading
to potentially serious privacy breaches. Such information ﬂow is often hidden or
not clear to social network users, who are often not aware of the amount of data
that is actually being disclosed, since they do not really distinguish among social
network users and developers outside the social network boundaries. We believe,
in order to promote healthy development of social networks environments and
protect fundamental individuals’ privacy rights, users should be in control at
any time of their data and be well informed about their usage. Applications
should be given limited privileges to the user proﬁle and only given access to
the smallest set of proﬁle data required to perform their tasks. For example, a
horoscope application should be given access to only the birthday information,
while a fortune cookie application that displays a random daily quote on the
user’s proﬁle should not be given access to any proﬁle data.
Although this issue has been recognized by the media [16,2,4] and by social
network users, to date, no technical solution has been proposed so far. Ideally,
users’ should be able to take advantage of the available applications while still
having a stronger control on their data. The problem is not trivial, in that
it requires new access control models for APIs in social networks, as well as
extending social network applications. Applications should be designed so to be
customized, based on users’ proﬁle preferences and second, users should have
the ability to specify the data that they are willing to reveal. Additionally, users
should be able to use data privacy mechanisms such as generalization to enjoy
the services provided through APIs without having to disclose identifying or
private information.
In this paper we address this issue, by deploying an access control mechanism
for applications in social networks. Our goal is to provide a privacy-enabled
solution that is in line with social network ethics of openness, and does not hinder users’ opportunities of adding useful and entertaining applications to their
proﬁles. Our access control mechanism is based on enabling the user to specify
the data attributes to be shared with the application and at the same time be
able to specify the degree of speciﬁcity of the shared attributes. Enabling such a
mechanism requires application to be developed to accommodate diﬀerent user
preferences. We model applications as ﬁnite state machines, and use the required user proﬁle attributes as conditions governing the application execution.
The challenge the user is faced with is what is the minimum set of attributes and
their minimum generalization levels required to acquire speciﬁc services provided
by the application. In order to address this problem we proposed the weighted

Beyond User-to-User Access Control for Online Social Networks

177

application transition system and formulated the Minimal Attribute Generalization Problem. Furthermore, we propose a solution that maps the problem to
the shortest path problem to ﬁnd the minimum set of attribute generalization
required to access the application services.
The rest of the paper is organized as follows. In Section 2, we provide background information related to Social Network APIs. In Section 3, we introduce
our developer APIs access control framework. In Section 4, we discuss how to
provide customized applications. Section 5 describes the related work. The conclusion and future work are discussed in Section 6.

2

Background on Social Network APIs

With the emergence of new web technologies, and with the establishment of the
Web 2.0, a large number of web sites are exposing their services by providing web
programming interfaces (APIs). For example, Google Web API [12] provides a
programming interface to query web pages through Google from user developed
applications. Several social network web sites have released APIs that allow
developers to leverage and aggregate information stored in user proﬁles and
provide extended social network services. The exposed APIs are basically a set
of web services that provide a limited and controlled view for the application to
interface with the social network site. The social network application architecture
includes three interacting parties namely the user, social network server, and the
third party application server. Figure 2(a), shows the diﬀerent blocks used in a
the social networks architecture. Note that, the application server is able to
connect to social network through the exported web APIs. Furthermore, these
requests are ﬁltered through the request management module which will be
discussed in detail in the next section.
For example, consider an application that recommends stores in your area that
are having sales. In this case, the application requires to retrieve your address,
age, marital status, and gender. The address information is required to be able
User
Browser

Request
Management

Policy
Base

Web
APIs

User
Profiles

(a) Social Network System Architecture

Social Network
Server
1

Request
Profile {Bob}

Application Platform

User Browser

Social Network Platform

2

3

Application
Server
Request
{TESTAPP}
Request {API 1}
Reply {API 1}
.
.
.
Request {API n}
Reply {API n}

4
5

Reply
{TESTAPP}

Reply
Profile {Bob}

(b) Application Interactions

Fig. 2. Social Network Architecture and Application Interactions

178

M. Shehab, A.C. Squicciarini, and G.-J. Ahn

to locate shops in your region, and the other parameters are required to provide
a more focused recommendation. Some other applications would not only require
data from your proﬁle but would also require data from your friends’ proﬁles.
For example, consider an application that projects your friends on an online map
according to the address listed on their proﬁles. This application requires your
address and your friend list, then for each friend it would retrieve their address.
Social networks provide mechanisms for users to customize their proﬁles and
to add applications developed by external developers. The application provides
the customized services by accessing the exported APIs. Figure 2(b), depicts the
interaction stages between the user browser, social network and the third party
developer application. The interaction starts when a user requests an application
AP P (Steps 1-2). The application server interacts with the social network server
by instantiating API calls (Step 3). Upon receiving the responses of the API calls,
the application server compiles and sends a response to the social network which
is forwarded to the requesting user (Steps 4-5). Note that in this model the social
network outsources the application development and execution to an external
third party application server.

3

Developer APIs Access Control Framework

Applications require to access user’s proﬁle data to provide a service customized
to the user’s proﬁle data. In this section we present our approach to enable
ﬁne grain access control [5,21] for developer’s applications, to limit applications’
access only to relevant user’s proﬁle data. We ﬁrst provide some preliminary
deﬁnitions related to applications and API set, then discuss our proposed ﬁne
grain access control framework for API based applications, and then focus on
the relevant phases that characterize our approach.
3.1

Social Network Profiles and Data Sets

For the purpose of our work, the two main components of a social network are
represented by applications and users’ proﬁles.
Users’ Profiles. Users’ proﬁles are modeled as collection of data items that are
uniquely associated to them. Each data item is deﬁned over a ﬁnite domain of
legal values.
Definition 1. (User Proﬁle) A user proﬁle for user i, is characterized by an
attribute vector x = {x1 , . . . , xn }, where attribute xi takes values in a domain
Di , which also includes the null value referred to by ⊥.
Proﬁle data items in our approach can be generalized to increase privacy of users.
A common practice in privacy preservation mechanisms is to replace data records
with suppressed or more general values [24,25] in order to ensure anonymity and
prevent disclosure of sensitive data. A simple disclosure policy can simply suppress an attribute if certain disclosure criterion are met, in this case that is a

Beyond User-to-User Access Control for Online Social Networks

179

all or none policy. A generalization disclosure policy, is accomplished by assigning a disclosed value that is more general than the original attribute value. For
example, the user can make the address information less speciﬁc by omitting
the street and city and revealing just the zip code. Figure 3, shows an example
partial value generalization hierarchy of the address attribute. We assume that
domain Di for a certain data item xi (see Deﬁnition 1) is a partially ordered set
(Dji , ≺), where Dji are the attribute generalizations and ≺ is the ordering operator. In the domain Di the largest element corresponds to the non-generalized
attribute value and the smallest element is the most generalized value which
is the suppressed value ⊥. The domain Di contains li generalization levels, an
attribute generalized to the hth level of generalization is denoted by Dhi , where
0 ≤ h < li . Data attribute generalized to D1i is more general than an attribute
generalized to D2i , D2i ≺ D1i , which implies that D2i discloses more information
than D1i . Given a user proﬁle x, by specifying generalization preferences for each
Level 0

⊥

Level 1

South East

Level 2

North Carolina

Mecklenburg

Level 3

Level 4

Level 5

Matthews

Davidson

South
Charlotte

Charlotte

Mint Hill

Florida

Wake County

Pineville

Huntersville

North
Charlotte

Fig. 3. A partial value generalization hierarchy of the address field

of the proﬁle attributes the user is able to specify a diﬀerent view for each application. The user generalization preferences for an application is deﬁned by the
attribute generalization vector U P = [h1 , . . . , hn ] where hi represents the generalization level Dhi permitted for proﬁle attribute i. Diﬀerent attributes have
diﬀerent disclosure sensitivity, for example some users might regard their home
address more sensitive than their cell phone number. To capture attribute sensitivity, for each proﬁle attribute xi ∈ x the user assigns a sensitivity metric Φi ,
which is speciﬁed for the non-generalized attribute Dlii −1 . Note that the sensitivity of an attribute xi generalized to level hi is proportional to Φi hi . Given a
user generalization preference vector the
U P = [h1 , . . . , hn ], the risk of attribute
disclosure is proportional to Θ(U P ) = ni=1 Φi hi . Note that the function Θ()
provides a mechanism to compare user generalization preferences. The generalization model can be applied not only to the data explicitly mentioned on the
proﬁle in addition it can be applied to the tags and the metadata that are attached to the proﬁle data.

180

M. Shehab, A.C. Squicciarini, and G.-J. Ahn

Applications. The building block for our model is represented by applications.
Applications are composed of a set of API’s which are functions called by the
application.
Definition 2. (Application API Set). Given an application App, the application
API set App.apiset is the set of APIs called by application App, represented as
the set App.apiset = {api1 , . . . , apin }.
For example, consider a horoscope application HoroAP P , illustrated in Figure 4. It calls the “user.get birthday()” and “user.get f riends()” APIs. The
application API set for HoroAP P is HoroAP P.apiset = {user.get birthday(),
user.get f riends()}. From the API calls the set of data set accessed by the
application can be obtained by tracing the data acquired by the called API’s.
For example, consider an API “user.get birthday()”, the proﬁle data accessed
is {prof ile.birthday}. Other APIs involve the processing of several proﬁle data
items for example, consider the API “user.get photos with f riends()”, this API
returns the photos taken with friends. The API performs a join between the user
friends and the user photo album meta data, in this case the data items access
are {prof ile.ablums, prof ile.f riends}. Accordingly, an application can be translated from a set of API calls to a set of data accesses. This set of accessed data
can be then presented to the user to enable the selection of what data items to
expose.
ExampleApplication(){
a = get friends(userid);
...
b = get albums(userid);
...
Query = "SELECT birthday FROM user db
WHERE uid=userid";
c = send query(Query);
...
}

Fig. 4. Example of horoscope application

3.2

The Access Control Framework

Our framework adopts the Principle of Least Privilege [23], which requires that
each principal be accorded the minimum access privileges needed to accomplish
its task. In our context, principals are the application developers, and the application should be awarded access to the minimum set of proﬁle data in order to
provide the requested service. To achieve this goal we present a mechanism that
enables ﬁne grain access control on the proﬁle data. Such a mechanism enables
the application developer to select the data items required by the application
and at the same time enables the user to opt-in or opt-out or generalize each of
the requested data items. Speciﬁcally, our framework is characterized by three
main phases: application registration, to register the application at the social
network server; user application addition, to add the application in a local proﬁle; and application adaptation, within which the application adapts according
to the provided data items. We discuss them in what follows.

Beyond User-to-User Access Control for Online Social Networks

181

Application Registration. The application developers register the application with the social network server. The developers are required to share the
application API calls and the application business state diagram describing the
application process, the details of this requirement will be discussed in following
sections. As part of the registration process, developers need to tag the application, by labeling each API within the application with the set of user’s data
items used by the application. The tags provided during this stage only refer to
the user’s proﬁle data involved and do not include any external output or additional user input that may be required when executing the API. The provided
application information is used to compile an application sheet describing the
data attributes required by the application.
User Application Addition. Once the application is registered with the social
network server, it becomes available for social network users to add to their
social network proﬁles. Upon selecting the application, the application sheet is
presented to the user, who is prompted with the following options for each data
item required by the API: choose to opt-in, opt-out, or generalize. Intuitively, the
user opts-in for the data items he is willing to disclose to the application. If the
user opts-out for some data the application needs to adapt in order to be properly
executed without such input. In case the generalize option is chosen for certain
data item, then the user only accepts the application to employ generalized data
attribute [24,25]. The user selections are input in the user sheet, which indicates
the user access preference for the added application.

<APPSHEET>
<APP id="332198764">
<DESCRIPTION>
<NAME> Horoscope App </NAME>
<INFO> Provide daily horoscope
from www.horoscope.com </INFO>
</DESCRIPTION>
<DATA-GROUP>
<DATA ref="profile.birthday"/>
<DATA ref="profile.gender"/>
<DATA ref="profile.address"/>
</DATA-GROUP>
</APP>
</APPSHEET>

(a) Application Sheet

<USERSHEET>
<APP id="332198764">
<ALLOW>
<DATA-GROUP>
<DATA ref="profile.birthday.day"/>
<DATA ref="profile.birthday.month"/>
</DATA-GROUP>
</ALLOW>
</APP>
</USERSHEET>

(b) User Sheet

Fig. 5. Application and User Sheets

An example of XML encoding for the horoscope application is reported in
Figure 5. In Figure 5(a) we report the application sheet, where birthday, gender
and address are requested. In Figure 5(b) we report the user sheet in case the
user opted to disclose only month and year of birth.
User Application Adaptation. At this stage the user sheet is used to generate a version of the application executable using the input obtained by the proﬁle data items. This phase requires the application to diﬀerentiate provisioning

182

M. Shehab, A.C. Squicciarini, and G.-J. Ahn

according to the permissible data items and their respective generalization levels.
We discuss in the next section how this not trivial task is achieved.

4

Customized Application Service Provisioning

The user sheet provides a mechanism for users to specify generalization preferences on the proﬁle attributes to restrict the data accessible to the application. On the other hand, by enabling attribute generalizations the application
is faced with the problem of missing data, and might not ensure the provisioning of the request service based on the provided data generalizations. To
address this issue we propose that during the application registration phase the
application developer is required to provide the process execution description of
the application. The process execution description describes the interactions between the composed APIs. A candidate process description language standard is
BPEL (Business Process Execution Language for Web Services, also WS-BPEL,
BPEL4WS) [19] which provides a rich vocabulary for expressing composition,
orchestration and coordination of web services to describe the behavior of business processes. Figure 6, shows an example process execution diagram describing
the service invocations and service transitions required by an application that
aggregates the user’s friends’ addresses and projects them on Google Maps. Note
that the transitions are labeled with conditions on the returned API calls. The
web services composition and choreography described by BPEL can be formalized based using ﬁnite state processes (FSP) [8,22,7]. In what follows we deﬁne
the application as a transition system.
Definition 3. (Application Transition System). An application transition system is a tuple T S = (S, Σ, δ), where:
– S is a ﬁnite set of states. The set of states includes a single initial state s0
and a ﬁnite set of ﬁnal states F ⊆ S.
– Σ is the alphabet of operations oﬀered by the service and the data required
by this service.
– δ : S × Σ → S is the transition function that maps states and alphabets to
another state. The transition δ(si , α) = sj , represents that transition from
state si to state sj subject to services and data in α.
The mapping function δ is used to represent the constraints required to transition from one state to another. In this paper, we focus on constraints related
to the required proﬁle data generalization levels requested by the application to
enable the successful transition from a state to another. For example, an application requesting the user’s address through the service get address(), the
application will transition to a diﬀerent state depending on the generalization
level of the returned address attribute. From an application perspective the user
generalization preference vector speciﬁes the permitted attribute generalization
levels, which in turn dictates the set of permissible state transitions. The set of
ﬁnal states represents the diﬀerent service levels provided by the application.

Beyond User-to-User Access Control for Online Social Networks

183

<<Invo ke(get_friends)>>
Get friends o f user “Bob”

L]

lt

su

L
NU
=

<<Inv oke (get_address)>>
Ge t addre ss for ea ch use in

=

Bo b’s frie ndlist

Re

[

L

UL
=N

<<Inv oke(get_address)>>
Ge t a ddre ss o f Bob

t=
u
es

<<Inv o ke (geo_getXY)>>

Get map coordinates on the
map using the address

l

R
]
LL

NU

lt

u
es

R

[

==

<<
(update _map )>>
Update map using the (X Y)
and the user name
Inv oke

,

<<Invo ke (geo _getXY)>>

Get map coordinates on the
map using the address
<<
(update _map)>>
Update map using the (X Y)
and the user name
Inv o ke

,

Fig. 6. Example Application Process

Definition 4. Given an application transition system T S = (S, Σ, δ) and a user
preference vector U P , the reduced application transition system T SUP is deﬁned
as the tuple (SR , ΣR , δR ), where:
– SR = S and ΣR = Σ.
– δR = δ for δ(si , α) = sj where the attributes α satisﬁes the user preference
vector U P .
The reduced application transition system includes only the state transitions
that are permitted by the user preferences. It also indicates the states that are
reachable after the user preferences are applied to the application.
We model the application transition system T S as a directed graph G =
(V, E), where the vertices V represent the states, and the edges E represent the
state transitions. The edges E are labeled with the minimum attribute generalization levels required to enable the state transition. For an edge e ∈ E the edge
label e.h represents the generalization level required for the state transition. For
example, in Figure 7(a) the edge (S0 , S1 ) is labeled with h12 indicating that the
generalization level 2 is required for attribute x1 to enable transition from state
S0 to state S1 . A user preference is said to satisfy a transition if the speciﬁed user
attribute generalization level is greater than or equal to the edge generalization
level. The reduced application transition system is computed by generating a

184

M. Shehab, A.C. Squicciarini, and G.-J. Ahn

graph GR = (VR , ER ), where VR = V and ER ⊆ E includes only the transitions E that satisfy the user preferences. Figure 7(b), shows an example reduced
application transition graph for the user preference vector up = {h11 , h21 , h32 , h41 }
and the original application state diagram in Figure 7(a).
Definition 5. (Application Service Path) Given an application transition instance T S, the path P = {e0 , . . . , en−1 } is sequence of state transitions, where
the edge e0 starts at the initial state s0 and the ending edge en−1 terminating at
a ﬁnal state sn ∈ F . The path generalization vector g(P ) = {e1 .h, . . . , en−1 .h} is
deﬁned as the set of data attribute generalization levels required to traverse this
path.
The Application Service Path represents an instance of an application execution
that starts at the start state s0 and ends at a target ending state sn .
S0

S0

݄ଵଶ
S1

݄ଷଶ

݄ଵଵ
S2

݄଴ଶ

S3

݄ଵଶ

݄ସଷ

Se2

݄଴ଶ
S3

S5

݄ଵ଴

S2

݄ଷଶ

Se1

݄ଵଵ

݄ଵଶ
S4

S5

݄ଵଷ

݄ଷଷ
݄ସଷ

S6

݄ସସ

S1

Se1

݄ଵଵ

S4

݄ଵଷ
݄ଶଷ

݄ଵଵ

݄ଵ଴

݄ଶଷ

݄ଶଷ

݄ଶଷ

S6

݄ଵସ

݄ଵସ

Se3

Se4

Se2

Se5

(a) Application
Diagram

State

Se3

Se4

Se5

(b) Reduced State Diagram, up = {h11 , h21 , h32 , h41 }

S0

‫ݓ‬ଶଵ
S1

‫ݓ‬ଷଶ

‫ݓ‬ଵଵ
S2

‫ݓ‬଴ଶ

S3

‫ݓ‬ଵଶ

‫ݓ‬
‫ݓ‬

ଷ
ସ

‫ݓ‬
‫ݓ‬ଷଶ

S5

‫ݓ‬ଷଷ
‫ݓ‬ସଷ ‫ݓ‬ଶଷ

S6

‫ݓ‬ସସ
Se2

Se1
ଵ
ଵ

S4
ଷ
ଵ

‫ݓ‬ଶଷ

‫ݓ‬଴ଵ

Se3

‫ݓ‬ଵସ
Se4

Se5

(c) Weighted State Diagram, wji = hij ∗ Φi
Fig. 7. Application State Diagram and User Preferences

Beyond User-to-User Access Control for Online Social Networks

4.1

185

Optimal User Application Preferences

In our framework, when trying to install an application, the user speciﬁes an
attribute generalization preferences and a target ﬁnal application state. The
challenge the user is faced with is to identify the minimal attribute generalization preference required to enable the application to successfully terminate to
the requested ﬁnal state. According to the well-known security principle of Least
Privilege [23], which requires that each principal be accorded the minimum access privileges needed to accomplish its task, this translates to the requirement
that an application should be awarded the access to the smallest set of proﬁle attributes at the minimum generalization levels in order to provide the requested
service. Formally, the minimal attribute generalization problem is deﬁned as
follows:
Definition 6. Minimal Attribute Generalization Problem, Given an application
transition instance T S = (S, Σ, δ), and a target ﬁnal state sf ∈ F , determine the
minimal user attribute vector U P ∗ = [h∗1 , . . . , h∗n ] required to enable the successful
transition from the start state s0 to the ﬁnal state sf .
The minimal user attribute vector is the vector that requires the minimum exposure of the user attributes and enables the application to transition to the target
ﬁnal state. Using the graph based application transition model, an application
service path beginning at start state and terminating at the ﬁnal target state
holds the set of generalization levels required to take such a path. The minimal
attribute generalization problem translates to ﬁnding the minimal application
service path from the start state to the target ﬁnal state in a weighted application
transition system deﬁned as follows:
Definition 7. (Weighted Application Transition System). A weighted application transition system T SW = (G, W ) where:
– G is the application transition graph G = (V, E), where V is the set of vertices representing the ﬁnite set of states, and E is the set of edges representing
the state transitions.
– W : E ×Φ → w ∈ + is the edge weight function that maps the edge attribute
generalization labeling E.h and the attribute sensitivity Φ to an edge weight
w.
Given an application service path P = {e0 , . . . , en−1 }, the path length is deﬁned
as follows:
n−1
n−1


Θ(U P ) =
W (ei , Φi ) =
Φi ei .h
i=0

i=0

Given the weighted application transition system and the path length deﬁnition, the minimal attribute generalization problem simply maps to ﬁnding the
shortest path from the start state s0 to the ﬁnal target state sf . The initially

186

M. Shehab, A.C. Squicciarini, and G.-J. Ahn

speciﬁed user preferences are used as an upper limit on the user preferences and
are referred to as the upper limit user preferences U P L = [h0 , . . . , hn ]. Figure 8,
depicts the algorithm used to compute the minimal user attribute preferences
vector. Lines 1-9, initialize the application transition graph to generate the edges
that are not allowed by the speciﬁed user attribute generalization upper limits
buy setting the edge weights to ∞, and the weights of the permitted transitions
using the edge weight function that incorporates both the user attribute sensitivity and generalization level. Lines 10-14, initialize the distance from s0 to
other vertices, where d[u] and pi[u] represent the shortest distance from s0 to u
and the predecessor of u on the shortest path respectively. Lines 15-24, computes
the shortest path from s0 to all the transition states. Lines 25-34, computes the
minimal user preferences vector required to transition from state s0 to the target
ﬁnal state sf .

Algorithm: generate minimal pref erence
Input : Application transition graph G = (V, E),
User upperlimit preferences UP L = [h0 , . . . , hn ], User target state st
Output : User Minimal Attribute Preferences UP ∗
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.

VR ← V
ER ← E
//Generating the reduced graph
for each e ∈ ER
for each h ∈ UP L
if h ≺ e.h
e.w = ∞
else
e.w = Φe.a ∗ e.h
//Initialize distance from s0
for each v ∈ VR
d[v] = ∞
pi[v] = {}
d[s0 ] = 0
//Computing Shortest Path from s0
S = {}
Q ← VR //Priority Queue on d[u]
while Q is not Empty
u = ExtractMin(Q)
S ← S ∪ {u}
for each v ∈ adjacent(u)
if d[v] > d[u] + w(u, v)
d[v] = d[u] + w[u, v]
pi[v] = u
//Tracing Minimal User Preferences from s0 to st
UP ∗ = {}
if d[st ] == ∞
return UP ∗
u = st
do
UP ∗ = (pi[u], u).h ∪ UP ∗
u = pi[u]
while pi[u] 	= s0
return UP ∗

Fig. 8. User Minimal Attribute Preferences Algorithm

Beyond User-to-User Access Control for Online Social Networks

5

187

Related Work

Security and privacy in Social Networks, and more generally in Web 2.0 are
emerging as important and crucial research topics [15,1,14,10]. Several pilot
studies conducted in the past few years have identiﬁed the need for solutions
to address the problem of information leakage networks, based on interpersonal
relationships and very ﬂexible social interactions. Some social networking sites,
such as FaceBook (http://www.facebook.com), have started to develop some
forms of control, however the level of assurance are still limited. For example,
FaceBook allows a user to join various networks (e.g., home university, home
city) and control what information is released to each network. Further, a user
can specify if a particular person should be “limited” from seeing particular
material or blocked entirely from seeing any material. However, there is limited
control over the amount of data API’s can access related to user’s data.
An interesting research proposal has been presented in [11], where a socialnetworking based access control scheme suitable for online sharing is presented.
In the proposed approach authors consider identities as key pairs, and social relationship on the basis of social attestations. Access control lists are employed to
deﬁne the access lists of users. A more sophisticated model has been proposed in
[3]. The authors presented a rule-based access control mechanism for social networks. Such an approach is based on enforcement of complex policies expressed
as constraints on the type, depth, and trust level of existing relationships. The
authors also propose using certiﬁcates for granting relationships authenticity,
and the client-side enforcement of access control according to a rule-based approach, where a subject requesting to access an object must demonstrate that it
has the rights of doing that. However, both in both projects [11,3], the authors
do not consider the issue of API’s in their models, and they do not propose a
method to control API’s access to proﬁle’s personal data.
An ongoing research project is represented by PLOG [13]. The goal of PLOG
is to facilitate access control that is automatic, expressive and convenient. The
authors are interested in exploring content based access control to be applied in
SN sites. We believe this is an interesting direction that we plan on investigating
as extension of Private Box. Another interesting work related to ours is [9]. The
authors present an integrated approach for content sharing supporting a lightweight access control mechanism. HomeViews facilitates ad hoc, peer-to-peer
sharing of data between unmanaged home computers. Sharing and protection
are accomplished without centralized management, global accounts, user authentication, or coordination of any kind. This contribution, although very promising
does not speciﬁcally focus on SNs and thus the proposed solution, although inline with our selective approach to user’s data is complementary to ours.
Some related work has also been conducted with speciﬁc focus on trust relationships in social networks. An important contribution on this topic has been
proposed by [10]. The work introduces a deﬁnition of trust suitable for use in
web-based social networks with a discussion of the properties that will inﬂuence
its use in computation. The authors designed an approach for inferring trust relationships between individuals that are not directly connected in the network.

188

M. Shehab, A.C. Squicciarini, and G.-J. Ahn

Speciﬁcally, they present TrustMail, a prototype email client that uses variations on these algorithms to score email messages in the user’s inbox based on
the user’s participation and ratings in a trust network.
Our idea of transitional states was partly inspired by [17]. The authors propose
a conversation-based access control model that enables service providers to retain
some control on the disclosure of their access control policies while giving clients
some guarantees on the termination of their interactions. Similarly to ours, the
authors represent web service possible conversations as ﬁnite transition systems,
in which ﬁnal states in this context. Many have identiﬁed [14] the need of a new
access control paradigm speciﬁc for so represent those in which the interaction
with the client can be (but not necessarily) ended. We adopt a similar approach
in that we represent possible applications as state machines, and we provide a
labeling technique to enable the comparison of the possible application paths.

6

Conclusions

In this paper we have presented an access control framework for social networks
developer applications that enables users to specify proﬁle attribute preferences
and requires applications to be designed so to be customized based on users’
proﬁle preferences. Our framework provided a privacy-enabled solution that is
in line with social network ethics of openness, and does not hinder users’ opportunities of adding useful and entertaining applications to their proﬁles. We
modeled the applications as ﬁnite state machine with transition labeling indicating the generalization level required to enable application state transitions. We
deﬁned the reduced application transition system that only includes the state
transitions possible with a given user generalization vector. Then we incorporated the user sensitivity metric to generate the weighted applications transition
system.
Furthermore, we formalized the Minimal Attribute Generalization Problem
and presented the Weighted Application Transition System which incorporates
the user attribute sensitivity metric to generated a weighted graph representing
the application state transitions. Using the weighted graph we transformed the
Minimal Attribute Generalization Problem to the shortest path problem and
provided an algorithm that generates the optimal user generalizations vector
that will enable the transition to a target ﬁnal state.

References
1. Acquisti, A., Gross, R.: Imagined communities: Awareness, information sharing,
and privacy on the facebook. In: Danezis, G., Golle, P. (eds.) PET 2006. LNCS,
vol. 4258, pp. 36–58. Springer, Heidelberg (2006)
2. CNET Blog. Exclusive: The next facebook privacy scandal (2008),
http://news.cnet.com/8301-13739 3-9854409-46.html
3. Carminati, B., Ferrari, E., Perego, A.: Rule-based access control for social networks.
In: Meersman, R., Tari, Z., Herrero, P. (eds.) OTM 2006 Workshops (2). LNCS,
vol. 4278, pp. 1734–1744. Springer, Heidelberg (2006)

Beyond User-to-User Access Control for Online Social Networks

189

4. Wahington Chronicle. Study raises new privacy concerns about facebook (2008),
http://chronicle.com/free/2008/02/1489n.htm
5. Damiani, E., Vimercati, S., Paraboschi, S., Samarati, P.: A fine-grained access
control system for XML documents. ACM Transactions on Information and System
Security 5(2), 169–202 (2002)
6. Facebook (2007), http://www.facebook.com
7. Foster, H., Uchitel, S., Magee, J., Kramer, J.: Ltsa-ws: A tool for model-based
verification of web service compositions and choreography, pp. 771–774 (May 2006)
8. Foster, H., Uchitel, S., Magee, J., Kramer, J., Hu, M.: Using a rigorous approach
for engineering web service compositions: a case study, vol. 1, pp. 217–224 (July
2005)
9. Geambasu, R., Balazinska, M., Gribble, S.D., Levy, H.M.: Homeviews: peer-to-peer
middleware for personal data sharing applications. In: SIGMOD Conference, pp.
235–246 (2007)
10. Golbeck, J., Hendler, J.A.: Inferring binary trust relationships in web-based social
networks. ACM Trans. Internet Techn. 6(4), 497–529 (2006)
11. Gollu, K.K., Saroiu, S., Wolman, A.: A social networking-based access control
scheme for personal content. In: Proc. 21st ACM Symposium on Operating Systems
Principles (SOSP 2007) (2007); Work in progress
12. Google Code. Google’s Developer Network, http://code.google.com/
13. Hart, M., Johnson, R., Stent, A.: More content - less control: Access control in the
Web 2.0. Web 2.0 Security & Privacy (2003)
14. Hogben, G.: Security issues and recommendations for online social networks.
ENISA Position Paper N.1 (2007)
15. IEEE. W2SP 2008: Web 2.0 Security and Privacy (2008)
16. Irvine, M.: Social networking applications can pose security risks. Associated Press
(April 2008)
17. Mecella, M., Ouzzani, M., Paci, F., Bertino, E.: Access control enforcement for
conversation-based web services. In: WWW Conference, pp. 257–266 (2006)
18. MySpace (2007), http://www.myspace.com
19. OASIS. OASIS WSBPEL TC Webpage,
http://www.oasis-open.org/committees/tc home.php?wg abbrev=wsbpel
20. O’Reilly, T.: What Is Web 2.0. O’Reilly Network, pp. 169–202 (September 2005)
21. Rizvi, S., Mendelzon, A., Sudarshan, S., Roy, P.: Extending query rewriting techniques for fine-grained access control. In: SIGMOD 2004: Proceedings of the 2004
ACM SIGMOD international conference on Management of data, pp. 551–562.
ACM, New York (2004)
22. Salaun, G., Bordeaux, L., Schaerf, M.: Describing and reasoning on web services
using process algebra, pp. 43–51 (June 2005)
23. Saltzer, J., Schroeder, M.: The Protection of Information in Computer Systems.
Proceedings of the IEEE 63(9), 1278–1308 (1975)
24. Samarati, P., Sweeney, L.: Generalizing data to provide anonymity when disclosing information (abstract). In: PODS ’98: Proceedings of the seventeenth ACM
SIGACT-SIGMOD-SIGART symposium on Principles of database systems, p. 188.
ACM, New York (1998)
25. Sweeney, L.: k-anonymity: a model for protecting privacy. International Journal of
Uncertainty, Fuzziness and Knowledge-Based Systems 10(5), 557–570 (2002)

2017 IEEE 11th International Conference on Semantic Computing

Semantic ETL – State-of-the-art and open research challenges
Jaydeep Chakraborty, Aparna Padki, Srividya K. Bansal
Arizona State University
Mesa, Arizona, USA
{jchakra1, apadki, srividya.bansal}@asu.edu
Abstract— There has been an exponential growth and
availability of data, both structured and unstructured. Massive
amounts of data are available to be harvested for competitive
business advantage, sound government policies, and new insights
in a broad array of applications (including healthcare,
biomedicine, energy, smart cities, genomics, transportation, etc.).
Yet, most of this data is inaccessible for users, as we need
technology and tools to find, transform, analyze, and visualize
data in order to make it consumable for decision-making.
Meaningful data integration in a schema-less, and complex Big
Data world of databases is a big open challenge. This survey
paper presents a holistic view of literature in data integration
and Extract-Transform-Load (ETL) techniques. Limitations and
gaps in existing approaches are identified and open research
challenges are discussed.

existing literature on ETL processes based on various data
integration strategies used and provides a rich discussion of
various aspects of integration. Secondly, it identifies current
open research challenges and sets the stage for future research
agenda. The literature collection process started by querying
prominent scholarly databases in Computer Science e.g.,
Google Scholar, DBLP, ACM, and IEEE Xplore. We also
specifically targeted top conference and journal venues for data
integration and semantic computing such as Mangament of
Data (SIGMOD), Very Large Databases (VLDB), Sementic
Web Conference (ISWC), World Wide Web conference
(WWW), Journal of Data and Knowlede Engineering, Journal
of Data Semantics, etc. and retrieved work published in last 10
years. The rest of the paper is orgranized as follows. Section II
presents traditional data integration approaches. Section III
presents the tradiitonal ETL tools and how they work. Section
IV describes efforts in the area of Semantic ETL. Section V
presents a discussion of the state-of-the art, limitations, and
open research challenges.

Keywords—Data Integration; Semantic ETL; Linked Data;
Semantic Computing.

I.

INTRODUCTION

With the widespread usage of internet, social media,
technology devices and mobile applications, big data is only
getting bigger and the sources of this data, types of format of
data (structured and unstructured) are increasing as well. In
order to be able to utilize this big data, a data integration
process from these aforementioned heterogeneous sources is
imperative. A wide variety of data warehouses exist that stores
these huge datasets to be analyzed and visualized for various
business/research needs. One of the popular approaches to
data integration has been Extract-Transform-Load (ETL) as
shown in previous work [1, 2]. A taxonomy of activities in
ETL and a framework that uses a workflow approach to
design ETL activities has been described.
From the Semantic Computing community, we can draw a
parallel to this process of loading a data warehouse for
publishing linked data to the linked open data cloud [3]. A
Semantic data model (i.e., richer representation of the data to
provide more meaning, context, and relationships) in the
transform phase is the major missing component in the
traditional ETL tools if it has to be used for generating linked
data. This is essential in generating linked data and connecting
it to the linked open data (LOD) cloud. In this paper we
present a detailed survey of research in the area of data
integration using ETL approaches. We present the limitations
of these approaches, their applicability for semantic data, and
discuss open research challenges.
The objective of this survey paper is to deliver a detailed
review of current approaches to data integration, various ETL
tools that are already in use, linked data generation techniques,
semantic-based approaches to ETL and data integration. The
main contribution of this paper is two-fold: firstly it classifies

978-1-5090-4284-5/17 $31.00 © 2017 IEEE
DOI 10.1109/ICSC.2017.94

II. TRADITIONAL DATA INTEGRATION
The task of a data integration system is to provide the user
with a unified view, called global schema, of a set of
heterogeneous data sources. Once the user issues a query over
the global schema, the system carries out the task of suitably
accessing the different sources and assemble the retrieved data
into final answer to the query. In this context, a crucial issue is
the specification of the relationship between the global schema
and the sources, which is called mapping. Traditionally, there
are two approaches for specifying mapping in a data
integration system. The first one, called global-as-view
(GAV), requires that to each element of the global schema a
view over the sources is associated. The second approach,
called local-as-view (LAV), requires that to each source a
view over the global schema is associated [10, 19-21].
One of the popular approaches to data integration has been
Extract-Transform-Load (ETL) as shown by Vassiliasis, et al
[1, 2]. Authors of this work have described taxonomy of
activities in ETL and a framework that uses a workflow
approach to design ETL activities. They used a declarative
database programming language called LDL to define the
semantics of ETL activities. Similarly there are other groups
that have used various other approaches such as UML and
data mapping diagrams for representing ETL activities [22],
quality metrics driven design for ETL [23], and scheduling of
ETL activities [24]. The focus in all of these papers has been
on the design of ETL workflow and not about generating
meaningful/semantic data.
Temporal Data Integration: Data integration efforts have been
towards fixed data sets. But there are some applications that
413

III. TRADITIONAL ETL TOOLS
Extract-Transform-Load (ETL) process that has been used in
computing for integration of data from multiple sources or
applications, possibly from different domains [4]. It refers to a
process in data warehousing that extracts data from outside
sources, transforms it to fit operational needs, which can
include quality checks, and loads it into the end target
database, more specifically, operational data store, data mart,
or data warehouse. The three phases of this process are
described as follows:
• Extract: this is the first phase of the process that involves
data extraction from appropriate data sources. Data is
usually available in flat file formats such as csv, xls, and txt
or is available through a RESTful client.
• Transform: this phase involves the cleansing of data to
comply with the target schema. Some of the typical
transformation activities involve normalizing data,
removing duplicates, checking for integrity constraint
violations, filtering data based on some regular expressions,
sorting and grouping data, applying built-in functions where
necessary, etc. [2]
• Load: this phase involves the propagation of data into a data
mart that serves Big Data.
We have analyzed several traditional ETL tools and described
the open source tools in this section. A feature comparison of
the open-source ETL tools – Clover, Talend, and Pentaho is
presented in Table 1. Other tools that facilitate the ETL
process with similar features, are IBM Infosphere [16],
Microsoft SQL Server Integration Services [17], and
Informatica Powercenter for Enterprise Data Integration [18].
A. Clover ETL
CloverETL [5] is a Java-based data integration ETL platform
for rapid development and automation of data transformations,
data cleansing, data migration and distribution of data into

require temporal data, data that varies over time. Related work
in this area uses a preference aware integration of temporal
data [13]. The authors have developed an operator called
PRAWN (preference aware union), which is typically a final
step in an entity integration workflow. It is capable of
consistently integrating and resolving temporal conflicts in
data that may contain multiple dimensions of time based on a
set of preference rules specified by a user. First, PRAWN
produces the same temporally integrated outcome, modulo
representation of time, regardless of the order in which data
sources are integrated. Second, PRAWN can be customized to
integrate temporal data for different applications by specifying
application-specific preference rules. Third, experimentally
PRAWN is shown to be feasible on both “small” and “big”
data platforms in that it is efficient in both storage and
execution time. Finally, a fundamental advantage of PRAWN
is that it can be used to pose useful temporal queries over the
integrated and resolved entity repository [13].
Benchmarking Data Integration: Although there are a lot of
approaches towards data integration, there is very little
research work done towards benchmarking data integration.
One such effort is TPC-DI [14], the first industry benchmark
for data integration. The TPC-DI benchmark combines and
transforms data extracted from a (fictitious) brokerage firm's
On-Line Transaction Processing (OTLP) system along with
other sources of data, and loads it into a data warehouse. The
source and destination data models, data transformations and
implementation rules have been designed to be broadly
representative of modern data integration requirements [14].
The requirements are characterized by:
• The manipulation and loading of large volumes of data
• Multiple data sources, utilizing a variety of data formats
• Fact & dimension table building & maintenance operations
• A mixture of transformation types including data validation,
key lookups, conditional logic, data type conversions,
complex aggregation operations, etc.
• Initial loading and incremental updates of the DW
Consistency requirements ensuring that the data integration
process results in reliable and accurate data.
On-demand Data Integration: An interesting approach for
ETL is an on demand approach, with a framework named
Lenses [15]. Data curation is a process of selective parsing,
transformation and loading into a new structure. Data analysis
needs to be done on-demand so that only immediately relevant
tasks can be curated. On-demand curation of data is an
approach where data is curated incrementally in response to
specific query requirements. The authors of this work propose
a standalone data processing component that unlike a typical
ETL processing stage that produces a single, deterministic
output, produces a PC-Table (W, P), where W is a large
collection of deterministic databases, so called possible
worlds, all sharing the same schema S, and P is a probability
measure over W. This table defines the set of possible outputs,
and probability measure that approximates the likelihood that
any given possible output accurately models the real world.

Table 1: Comparison of ETL tools
CRITERIA
Engine design

Transformation
Graphs

Extract Load
Transform (ELT)
support
Large scale data
support
Semantic web
support

414

CLOVER
Each
component is
run in a
separate thread
and acts as a
consumer or
producer.
Represented as
XML files and
can be
dynamically
generated

TALEND
All components
are run on a single
thread unless
multi-threaded
environment is
enabled.

PENTAHO
Multi-threaded
with meta-data
driven
approach

Saved as XML
files

Absent

Data
transformation
scripts are
generated. Talend
open Studio is
like a code
generator.
Present

Present

Present

Present

Present

None

Custom
standalone
plugins are
available

None

applications, databases, cloud and data warehouses.
CloverETL is a Java-based ETL tool with open source
components. It is either used in standalone mode – as a
command-line or server application – or embedded in other
applications – as a Java library. CloverETL is accompanied by
the CloverETL Designer graphical user interface available as
either an Eclipse plug-in or standalone application. Data
transformation in CloverETL is represented by a
transformation dataflow, or graph, containing a set of
interconnected components joined by edges. A component can
either be a source (reader), a transformation (reformat, sort,
filter, joiner, etc.) or a target (writer). The edges act as pipes,
transferring data from one component to another. Each edge
has a certain metadata assigned to it that describes the format
of the data it transfers. The transformation graphs are
represented in XML files and can be dynamically generated. A
sample graph in clover ETL is shown below.

also be used for other purposes: Migrating data between
applications or databases, Exporting data from databases to
flat files, Loading data massively into databases, Data
cleansing, and Integrating applications. Every process in PDI
is created with a graphical tool where user specifies what to do
without writing code to indicate how to do it thereby making it
metadata-oriented. PDI can be used as a standalone
application, or it can be used as part of the larger Pentaho
Suite. As an ETL tool, it is the most popular open source tool
available. PDI supports a vast array of input and output
formats, including text files, data sheets, and commercial and
free database engines.
D. Oracle Warehouse Builder
Oracle Warehouse Builder [8] is a single, comprehensive tool
for all aspects of data integration. Warehouse Builder
leverages Oracle Database to transform data into high-quality
information. It provides data quality, data auditing, fully
integrated relational and dimensional modeling, and full
lifecycle management of data and metadata. Warehouse
Builder enables creation of data warehouses, migration of data
from legacy systems, consolidation of data from disparate data
sources, cleaning and transforming data to provide quality
information, and management of corporate metadata. Oracle
Warehouse Builder is comprised of a set of graphical user
interfaces to assist you in implementing solutions for
integrating data. It provides a Design Center to import source
objects, design ETL processes such as mappings, and
ultimately define the integration solution.
E. Hadoop
Apache Hadoop, a framework for distributed processing of
large data sets across clusters of computers using simple
programming models [9], is also an ideal platform to run ETL
jobs. Most of the times, ETL jobs comprise of calculating
aggregate functions like COUNT, SUM, AVG, MAX and
Hadoop is a good fit for it. It can read data from different
sources and then run various jobs to transform the data and
load it into a target source. Hadoop is highly scalable and
flexible; it can deal with huge amount of data, and perform
operations on large-scale. Unlike traditional ETL tools,
Hadoop can load and transform unstructured, structured, or
semi-structured data into the Hadoop Distributed File System
(HDFS). But Hadoop is not good choice for heavily relational
data. As Hadoop does not support ANSI standard, it cannot be
used for Online Transaction Processing (OLTP) jobs but can
be easily used for Online Analytical Processing (OLAP).
Along with Hadoop, there are a number of tools like HIVE,
Sqoop, Pig, Mahout, Impala that can be used for both
relational and non-relational ETL jobs. Due to the scalability
and distributed nature there are number of ETL tools like
Pentaho, Clover ETL are integrating Hadoop into them.

Figure 1: Clover ETL sample workflow

Each component runs in a separate thread and acts either as a
consumer or a producer. This is used to drive data through the
transformation for both simple and complex graphs and makes
the platform extendable by building custom components,
connections etc. Transformation graphs can then be combined
into a job flow, which defines the sequence in which the
individual graphs are executed.
B. Talend Open Studio
Talend [6] is an open source data integration product
developed by Talend and designed to combine, convert and
update data in various locations across business. Talend Open
Studio for Data Integration operates as a code generator,
producing data-transformation scripts and underlying
programs in Java. Its eclipse based GUI gives access to
metadata repository and to a graphical designer. The metadata
repository contains the definitions and configuration for each
job - but not the actual data being transformed or moved. All
of the components of Talend Open Studio for Data Integration
use the information in the metadata repository. It is a powerful
and versatile open source solution that addresses all of an
organization’s data integration needs such as synchronization
or replication of databases, right-time or batch exchanges of
data, ETL for BI or analytics, data migration, complex data
transformation and loading, and basic data quality.
C. Pentaho Data Integration
Pentaho Data Integration (PDI, also called Kettle) [7] is the
component of Pentaho responsible for the Extract, Transform
and Load (ETL) processes. Though ETL tools are most
frequently used in data warehouses environments, PDI can

IV. SEMANTIC ETL
Ontologies have been used as a formal tool for sophisticated
querying and expressing the domain level knowledge at a
high-level of abstraction [25]. A number of approaches
evolved that aimed at automatic ontology generation for

415

different types of data [26]. These approaches could be
classified as Convertors/Translators, Mining-based techniques,
and applications using external knowledge. Convertors or
translators involved mapping data from specific format such
as XML, XSD, UML, etc. into an ontology [27-30]. Text was
annotated with formal ontologies using various mining-based
techniques [31-34]. There were also frameworks built that
used external knowledge or domain-knowledge to produce
ontologies [35]. Clio [36] is an approach that uses SQL
schemas alone to generate the ontological mappings.
The challenge in data integration, be it traditional or
semantic, is that it is a domain-specific process. Hence a
complete automation that ranges across domains is not
available. Given this, a semi-automatic approach where an
intuitive UI is available to user is imperative. Karma [12] is
data integration tool that aims to semi-automatically map
structured sources onto the semantic web. Using Karma,
adding semantics to the data is part of the conversion process,
i.e. from structured to RDF data. The tool learns the model
that has been used to publish RDF data so that the semantic
type need not be added every time. The user sets up the model
by importing ontology into the system. Karma also provides a
SPARQL endpoint to query the published data. PyTransform
module within Karma allows a user to “transform” the
columns as per their business needs using simple UI. The
highlight of this tool is that it builds a graph as semantic types
are set. Visualization helps in determining the correctness of
semantic types. Using these column transformations &
semantic type setting, RDF data is generated.
In ontology-based data integration (OBDI) [37], the global
and local schema (conceptual schema) are expressed in
description logic. As description logic is very much similar to
machine language, it is easier to merge or extract data from
other data sources. To achieve this OWL-DL language is used.
Geodint is a Semantic Web-Based Geographic Data
Integration framework [38] that uses a data warehouse.
Traditionally data warehouses work with a unified schema.
This work shifted to mediated schema and there is a mapping
between original schemas and mediated schema based on two
approaches described in the traditional data integration field Global as view (GAV) and Local as view (LAV). Now this
mapping is based on OBDI-ontology based data integration.
Dataspaces is a research effort based on the idea that
semantic cohesion of a dataspace is increased over time by
different parties providing mappings; the same pay as you go
data integration approach that currently emerges on the Web
of Data. In a similar fashion, on the web of data, web
standards can be used to provide semantics mappings in order
to generate linked data [21]. Traditional data integration
approaches are applicable to relational data models and an
integrated global schema is determined semi-automatically or
through probabilistic schema mapping in case of uncertainty
in schema mappings between data sources [20, 22].
A. Linked Data Integration techniques
Linked Data refers to data published on the Web in machinereadable format, its meaning is explicitly defined, it is linked

to other external data sets, and can in turn be linked to from
external data sets [39]. Linked Data is about using the Web to
connect related data that wasn't previously linked, or using the
Web to lower the barriers to linking data currently linked
using other methods. Study by Erickson, et al [40] presents the
process followed to publish linked open government data.
This work is an important one in the field of Linked Data as it
demonstrates the importance of integrating data from the
government domain by providing a standard process in
building a semantic data model and instances. The work of
open linked government data establishes the importance of
publishing linked data.
We reviewed the work of T2LD, a prototype system for
interpreting tables and extracting entities and relations from
them, and producing a linked data representation of the table’s
contents [41]. The author has harnessed the structure of the
table to convert it into linked data. The author tries to capture
the semantics of a table by mapping header cells to classes,
data cell values to entities and pair of columns to relations
from a given ontology and knowledge base. The table header
and rows are provided as an input to the system. A knowledge
base such as Wikitology [42] is queried. An algorithm is
proposed to predict the class for the column headers. Using
this technique requires a knowledge base that is queried to
predict the classes. Using these results, the cell value is linked
to entities. A relationship is identified between the columns
based on the query results and the prediction algorithm.
Finally, the linked data is generated as an output. The major
drawback of this proposal is the core dependency on the
knowledge base. If there are any unknown entities present in
the table, which does not exist in a knowledge base, linked
data will not be generated.
The short comings of the proposal is addressed in TABEL,
a domain-independent and extensible framework for inferring
the semantics of tables and representing them as RDF Linked
Data [43]. TABEL framework allows for the background
knowledge bases to be adapted and changed based on the
domain. This feature makes TABEL domain independent and
mainly focused on converting tables to linked data. A more
widespread usage of data storage is relational databases. A
comprehensive work of automated mapping generation for
converting databases into Linked Data is discussed by Polfliet
and Ichise [44]. The authors propose a method to
automatically associate attribute names to existing ontology
entities in order to complete the automation of the conversion
of databases. They have used libraries like WordNet and other
java libraries to determine string equality and query a
knowledge base like DBpedia for linking the database
ontology to an existing ontology.
Linked Data Integration Framework (LDIF) [45] employs
the R2R Framework to translate Web data that is represented
using terms from different vocabularies into a single target.
Vocabulary mappings are expressed using the R2R Mapping
Language. LDIF employs the Silk Link Discovery Framework
to find different URIs, which identify the same real-world
entity (http://silkframework.org/).

416

used, which is very much similar to SQL query, we can only
extract data from single data source.
• ETL jobs cannot automated. Manual intervention is required
for all ETL tools. Transformation of data from one source to
another depends on a person who has to be the domain
expert and has to know end-to-end information about each
data source and properties of all relations and their
attributes. Without this domain knowledge, it is impossible
to extract, transform and load.
• Also tools like Hadoop are available that can be used for
complex queries from distributed data sources but they also
require human effort to configure and write logic.
• No support is provided for query optimization. In both
semantic and traditional ETL techniques custom and
complex queries are really hard to execute. In semanticbased integration techniques SPARQL queries can be used.
However SPARQL is quite similar to SQL query and there
is very little room for optimization. As semantic data in
RDF format is flat file structure it is not possible to index it,
which makes SPARQL queries slow.
B. Open Research Challenges
Automation: The main research challenge with ETL
frameworks is that the process cannot be fully automated. It is
really hard to completely get rid of manual intervention. For
example, in Extract, Transform and Load phases a domain
expert needs to define the schema for the data sources. There
is no ETL tool, which can map and define the source schema
or ontology and load the data automatically from the data
source metadata. It is really difficult to train and create such
model that an ETL tool can use and perform extract,
transform, and load operations. Ontology matching: All ETL
frameworks that are available do not have provision to
compare or find similarity or dissimilarity between source and
destination ontologies or schemas. Currently there are some
studies for computing different type of similarities like
Syntactic Similarity, Membership Similarity, Semantic
Similarity but none of them produce prolific results yet.
Data Quality: ETL frameworks are error prone because of
data noise. It is really hard to define rules in ETL framework
for data pruning. There are some ETL tools like Talend, where
we can define business rules. But it needs great amount of
human effort to find the noise and prune it.
Linking data with cloud: The challenge with the Web of data
is to integrate datasets into Linked Open Data cloud that
already comprises of numerous general and domain-specific
vocabularies (thereby serving as global schema). Domainspecific Ontology-based techniques are limited due to one or
more of the following reasons: applies to relational databases;
do not work with diverse datasets; rely on a domain expert for
creation of semantic data model; provide minimal semantic
relationships based on one existing ontology such as Wordnet
or Wikipedia.
Visualization of data model: CloverETL and Pentaho do not
provide any semantic web support. CloverETL provides the
flexibility to develop custom components and RDF conversion
of the integrated data can be possible with the use of API

B. Semantic Data Integration techniques
Datalift platform [46] is one of the earlier work done towards
semantic ETL framework. The process here is lifting raw data
sources to semantic interlinked data sources. The process is not
a traditional ETL one. The user provides structured data
sources as an input to the system. To maintain a uniform
format, all data sources are converted into a raw RDF format,
meaning this conversion does not involve any vocabularies or
entities. Once a unique data format is ready, vocabularies will
be selected to assign meaning to the lifted data. Once the
vocabulary selection is done, the ontology is prepared. This
ontology is now used to map the elements in the RDF file,
forming a fully formal RDF file. The final step in the process
aims at providing links from the newly published dataset to
other datasets already published as Linked Data on the Web.
The interconnection modules give the possibility to achieve
this linkage [46]. Linked Open Vocabulary (LOV) is used in
this platform, to provide functionalities for search and also to
add the generated RDF data to the LOV dataset. Another
unique feature of this system is that it provides an interlinking
module. Due to this, the published datasets can be linked with
other datasets on the web, using sameAs property of OWL.
C. Semantic ETL techniques
A more recent work towards semantic ETL framework is a
python based programmable semantic ETL framework
(SETL) [47]. SETL builds on Semantic Web (SW) standards
and tools and supports developers by offering a number of
powerful modules, classes and methods for (dimensional and
semantic) data warehouse constructs and tasks. Thus it
supports semantic-aware data sources, semantic integration,
and creating a semantic data warehouse, composed of an
ontology and its instances. The approach used here is to
extract the data, transform it to RDF data using domain based
ontology and loading this transformed data to a data
warehouse. Before loading into triple store, external linking of
RDF data is done using semantic web engine, Sindice [48].
Similarity computation in categories of syntactic (compare
alphabets), membership (closeness of attributes), and semantic
(relationship between classes and attributes) is used in a study
to determine the mappings. Based on similarity clustering,
mapping of attributes is done [49].
V.

DISCUSSION

A. Limitations
Current industry-strength ETL tools and research projects
from academia have number of limitations.
• Very few relational joins are supported in standard ETL
tools. Complex inequality join, self-join, aggregate
operations on data cube cannot be achieved because of lack
of variety of join components in ETL tools. ETL tools are
not flexible like SQL-based tools.
• Limited ability is available to extract data from different
sources at same time. It can be achieved using different
reader components available in ETL tools but they are
restricted for complex extract queries.
• Though in most ETL tools ETL query can be created and

417

Apache Anything to triples (Any23). It is a library, a web
service and a command line tool that extracts structured data
in RDF format from a variety of Web documents. The
challenge faced in this approach is that addition of a GUI was
not permitted in CloverETL’s custom component. Allowing a
user to select the appropriate classes and properties to provide
relationships between them through visualization is an
essential part of a semantic ETL.
Multi-Dimensional Data: RDF data can also represent multidimensional data like Data Cube vocabulary. Data Cube
vocabulary is a core foundation, which supports extension
vocabularies to enable publication of other aspects of
statistical data flows or other multi-dimensional data sets. The
multidimensional data can easily be saved and queried. It
follows Statistical Data and Metadata eXchange (SDMX), an
ISO standard for exchanging and sharing statistical data and
metadata among organizations. Within semantic ETL
frameworks, data can be represented in description logic
format. Mapping between local and global schemas is based
on DL formalism where concepts are set of individuals and
role is binary relationship between individuals.
VI.
[1]
[2]
[3]
[4]

[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]

[21] X. L. Dong and D. Srivastava, “Big data integration,” Synthesis
Lectures on Data Management, vol. 7, no. 1, pp. 1–198, 2015.
[22] J. Trujillo and S. Luján-Mora, “A UML-based approach for modeling
ETL processes in data warehouses,” in Conceptual Modeling-ER 2003,
Springer, 2003, pp. 307–320.
[23] S. Luján-Mora, P. Vassiliadis, and J. Trujillo, “Data mapping diagrams
for data warehouse design with UML,” in Conceptual Modeling–ER
2004, Springer, 2004, pp. 191–204.
[24] A. Simitsis, K. Wilkinson, M. Castellanos, and U. Dayal, “QoX-driven
ETL design: reducing the cost of ETL consulting engagements,” in
ACM SIGMOD Conf. on Management of data, 2009, pp. 953–960.
[25] A. Poggi, D. Lembo, D. Calvanese, G. De Giacomo, M. Lenzerini, and
R. Rosati, “Linking data to ontologies,” in Journal on data semantics,
Springer, 2008, pp. 133–173.
[26] I. Bedini and B. Nguyen, “Automatic ontology generation: State of the
art,” PRiSM Laboratory Technical Report. Univ. of Versailles, 2007.
[27] H. Bohring and S. Auer, “Mapping XML to OWL Ontologies.,”
Leipziger Informatik-Tage, vol. 72, pp. 147–156, 2005.
[28] M. Ferdinand, C. Zirpins, and D. Trastour, “Lifting XML schema to
OWL,” in Web Engineering, Springer, 2004, pp. 354–358.
[29] R. Ghawi and N. Cullot, “Building Ontologies from XML Data
Sources.,” in DEXA Workshops, 2009, pp. 480–484.
[30] J. X. W. Li, “Using relational database to build OWL ontology from
XML data sources,” Computational Intelligence, 2007.
[31] B. Biebow, S. Szulman, and A. J. Clément, “TERMINAE: A linguisticsbased tool for the building of a domain ontology,” in Knowledge
Acquisition, Modeling and Management, Springer, 1999, pp. 49–66.
[32] Y. Ding, et al, “Generating ontologies via language components and
ontology reuse,” in Natural Language Processing and Information
Systems, Springer, 2007, pp. 131–142.
[33] H. Hu and D.-Y. Liu, “Learning owl ontologies from free texts,” in Intl.
Conf. on Machine Learning & Cybernetics, 2004, vol.2, pp.1233–1237.
[34] D. I. Moldovan, R. Girju, “Domain-Specific Knowledge Acquisition and
Classification Using WordNet.,” in FLAIRS Conf., 2000, pp. 224–228.
[35] H. Kong, M. Hwang, and P. Kim, “Design of the automatic ontology
building system about the specific domain knowledge,” in of Intl. Conf.
on Advanced Communication Technology, 2006, vol. 2, p. 4–pp.
[36] R. Fagin, et al, “Clio: Schema mapping creation and data exchange,” in
Conceptual Modeling: Fndns & Apps, Springer, 2009, pp. 198–236.
[37] D. Calvanese, et al, “Ontology-based integration of cross-linked
datasets,” in International Semantic Web Conference, 2015, pp.199-216.
[38] Matuszka T., Kiss A. “Geodint: Towards Semantic Web-Based
Geographic Data Integration”, in Intelligent Information and Database
Systems., 2014, vol 8397, Springer.
[39] A Bizer, A Heath, T Berners-Lee, “Linked data-the story so far,” In
Journal of Semantic Services, Interoperability and Web Applications:
Emerging Concepts, 2009, pp. 205-227.
[40] J. S. Erickson, et al, TWC Intl. Open Government Dataset Catalog in
Intl. Conf. on Semantic Systems, NY, USA, 2011, pp. 227–229.
[41] Mulwad, V., Finin, T., & Joshi, A. “A domain independent framework
for extracting linked semantic data from tables”, In Search Computing,
2012, pp. 16-33, Springer Berlin Heidelberg.
[42] Wikitology: Wikipedia as an ontology http://ebiquity.umbc.edu/resource
/html/id/245/Wikitology-Wikipedia-as-an-ontology.
[43] V. V. Mulwad, “TABEL - A Domain Independent and Extensible
Framework for Inferring the Semantics of Tables,” Ph.D., University of
Maryland, Baltimore County, United States -- Maryland, 2015.
[44] S. Polfliet and R. Ichise, “Automated mapping generation for converting
databases into linked data,” in International Conference on Posters &
Demonstrations Track-Volume 658, 2010, pp. 173–176.
[45] Schultz, A., et al. “LDIF-a framework for large-scale Linked Data
integration”, In Intl. World Wide Web Conference (WWW 2012),
Developers Track, Lyon, France.
[46] F. Scharffe, S. Eurecom, “Enabling Linked Data Publication with the
Datalift Platform,” In Semantics Scholar. [Online].
[47] R. P. Deb Nath, et al, “Towards a Programmable Semantic ETL
Framework for Semantic Data Warehouses,” in ACM Intl. Workshop on
Data Warehousing and OLAP, New York, NY, USA, 2015, pp. 15–24.
[48] Sindice: Semantic Web Standards. www.w3.org/2001/sw/wiki/Sindice.
[49] Bergamaschi, et al. “A semantic approach to etl technologies”, In Data
& Knowledge Engineering, 2011, 70(8), 717-731.

REFERENCES

Vassiliadis, P., Simitsis, A., & Baikousi, E. “A Taxonomy of ETL
Activities” in Proceedings of the ACM Twelfth International Workshop
on Data Warehousing and OLAP, 2009, pp. 25–32, NY, USA
Vassiliadis, P., Simitsis, A., Georgantas, P., Terrovitis, M., &
Skiadopoulos, S. “A generic and customizable framework for the design
of ETL scenarios” in Information Systems, 2005, 30(7), 492–525.
“The Linking Open Data cloud diagram.” lod-cloud.net/.
D. Werner, “ETL yesterday, today and tomorrow: Something borrowed,
something green,” LinkedIn Pulse, 19-Jun-2015. [Online]
https://www.linkedin.com/pulse/etl-yesterday-today-tomorrowsomething-borrowed-green-dennis-werner.
“CloverETL Rapid Data Integration.” www.cloveretl.com
“Talend Real-Time Open Source Data Integration Software.” [Online].
http://www.talend.com/. [Accessed: Jun-2016].
Pentaho Data Integration www.pentaho.com/product/data-integration
“Oracle Warehouse Builder 11gR2” [Online].
“Apache Hadoop” [Online] http://hadoop.apache.org.
A. Calì, Reasoning in data integration systems: why lav and gav are
siblings, in Fndns. of Intelligent Systems, Springer ‘03, pp. 562–571.
M. S. Kettouch, C. Luca, M. Hobbs, and A. Fatima, “Data integration
approach for semi-structured and structured data (Linked Data),” in
IEEE Intl. Conf. on Industrial Informatics (INDIN), 2015, pp. 820–825.
C. A. Knoblock, et al, “Semi-automatically mapping structured sources
into the semantic web,” in The Semantic Web: Research and
Applications, 2012, pp. 375–390.
B. Alexe, M. Roth, and W.-C. Tan, “Preference-aware integration of
temporal data,” Proc. VLDB Endow., vol. 8, no. 4, pp. 365–376, 2014.
M. Poess, T. Rabl, H.-A. Jacobsen, and B. Caufield, “TPC-DI: the first
industry benchmark for data integration,” Proc. VLDB Endow., vol. 7,
no. 13, pp. 1367–1378, 2014.
Y. Yang, N. Meneghetti, R. Fehling, Z. H. Liu, and O. Kennedy,
“Lenses: an on-demand approach to ETL,” Proc. VLDB Endow., vol. 8,
no. 12, pp. 1578–1589, 2015.
IBM InfoSphere Platform: big data, info integration, data warehousing.
“Integration Services - MS SQL Server 2012.” [Online]. Available:
http://www.microsoft.com/en-us/sqlserver/solutions-technologies/
enterprise-information-management/integration-services.aspx.
“Enterprise Data Integration – Informatica.” [Online]. Available:
http://www.informatica.com/us/products/data-integration/enterprise/.
A. Doan, A. Halevy, Z. Ives, Principles of data integration. Elsevier.
M. Lenzerini, “Data integration: A theoretical perspective,” in ACM
SIGMOD symposium on Principles of DB systems, 2002, pp. 233–246.

418

DR@FT: Efficient Remote Attestation Framework
for Dynamic Systems
Wenjuan Xu1 , Gail-Joon Ahn2 , Hongxin Hu2 ,
Xinwen Zhang3 , and Jean-Pierre Seifert4
1

4

Frostburg State University
wxu@frostburg.edu
2
Arizona State University
gahn@asu.edu ,hxhu@asu.edu
3
Samsung Information Systems America
xinwen.z@samsung.com
Deutsche Telekom Lab and Technical University of Berlin
jean−pierre.seifert@telekom.de

Abstract. Remote attestation is an important mechanism to provide the trustworthiness proof of a computing system by verifying its integrity. In this paper,
we propose an innovative remote attestation framework called DR@FT for efficiently measuring a target system based on an information flow-based integrity
model. With this model, the high integrity processes of a system are first verified through measurements and these processes are then protected from accesses
initiated by low integrity processes. Also, our framework verifies the latest state
changes in a dynamic system instead of considering the entire system information. In addition, we adopt a graph-based method to represent integrity violations
with a ranked violation graph, which supports intuitive reasoning of attestation
results. Our experiments and performance evaluation demonstrate the feasibility
and practicality of DR@FT.

1 Introduction
In distributed computing environments, it is crucial to measure whether remote parties
run buggy, malicious application codes or are improperly configured by rogue software.
Remote attestation techniques have been proposed for this purpose through analyzing
the integrity of remote systems to determine their trustworthiness. Typical attestation
mechanisms are designed based on the following steps. First, an attestation requester
(attester) sends a challenge to a target system (attestee), which responds with the evidence of integrity of its hardware and software components. Second, the attester derives
runtime properties of the attestee and determines the trustworthiness of the attestee. Finally and optionally, the attester returns the attestation result, such as integrity measurement status, to the attestee. Remote attestation can help reduce potential risks that are
caused by a tampered system.


The work of Gail-J.Ahn and Hongxin Hu was partially supported by National Science Foundation (NSF-IIS-0900970 and NSF-CNS-0831360).

D. Gritzalis, B. Preneel, and M. Theoharidou (Eds.): ESORICS 2010, LNCS 6345, pp. 182–198, 2010.
c Springer-Verlag Berlin Heidelberg 2010


DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

183

Various attestation approaches and techniques have been proposed. Trusted Computing Group (TCG) [2] specifies trusted platform module (TPM) which can securely store
and provide integrity measurements of systems to a remote party. Integrity measurement
mechanisms have been proposed to facilitate the capabilities of TPM at application
level. For instance, Integrity Measurement Architecture (IMA) [12] is an implementation of TCG approach to provide verifiable evidence with respect to the current run-time
state of a measured system. Several attestation methods have been proposed to accommodate privacy properties [7], system behaviors [8], and information flow model [9].
However, these existing approaches still need to cope with the efficiency when attesting
a platform where its system state frequently changes due to system-centric events such
as security policy updates and software package installations. Last but not least, existing
attestation mechanisms do not have an effective and intuitive way for presenting attestation results and reflecting such results while resolving identified security violations.
Towards a systematic attestation solution, we propose an efficient remote attestation
framework, called Dynamic Remote Attestation Framework and Tactics (DR@FT) to
address aforementioned issues. Our framework is based on system integrity property
with a domain-based isolation model. With this property, the high integrity processes
of a system are first verified through measurements and these processes are then protected from accesses initiated by low integrity processes. In other words, the high integrity process protection is verified by analyzing the system’s security policy, which
specifies system configurations with system behaviors including application behaviors.
Having this principle in place, DR@FT enables us verify whether certain applications
in the attestee satisfy integrity requirements as part of attestation. Towards attesting a
dynamic nature of the systems, DR@FT verifies the latest changes in a system state, instead of considering the entire system information for each attestation inquiry. Through
these two tactics, our framework attempts to efficiently attest the target system. Also,
DR@FT adopts a graph-based analysis methodology for analyzing security policy violations, which helps cognitively identify suspicious information flows in the attestee.
To further improve the efficiency of security violation resolution, we propose a ranking
scheme for prioritizing the policy violations, which provides a method for describing
the trustworthiness of different system states with risk levels.
This paper is organized as follows. Section 2 overviews existing attestation work
and system integrity models. Section 3 describes a domain-based isolation model which
gives the theoretical foundation of DR@FT. Section 4 presents the design requirements
and attestation procedures of DR@FT, followed by policy analysis methods and their
usages in Section 5. We elaborate the implementation details and evaluation results in
Section 6. Section 7 concludes this paper and examines some limitations of our work.

2 Background
2.1 Attestation
The TCG specification [2] defines mechanisms for a TPM-enabled platform to report
its current hardware and software configuration status to a remote challenger. A TCG
attestation process is composed of two steps: (i) an attestee platform measures hardware and software components starting from BIOS block and generates a hash value.

184

W. Xu et al.

The hash value is then stored into a TPM Platform Configuration Register (PCR). Recursively, it measures BIOS loader and operating system (OS) in the same way and
stores them into TPM PCRs; (ii) an attester obtains the attestee’s digital certificate with
an attestation identity key (AIK), AIK-signed PCR values, and a measurement log file
from the attestee which is used to reconstruct the attestee platform configuration, and
verifies whether this configuration is acceptable. From these steps we notice that TCG
measurement process is composed of a set of sequential steps up to the bootstrap loader.
Thus, TCG does not provide effective mechanisms for measuring a system’s integrity
beyond the system boot, especially considering the randomness of executable contents
loaded by an running OS.
IBM IMA [12] extends TCG’s measurement scope to application level. A measurement list M is stored in OS kernel and composed of m0 ... mi corresponding to loaded
executable application codes. For each loaded mi , an aggregated hash Hi is generated
and loaded into TPM PCR, where H0 =H(m0 ), and Hi =H(Hi−1 || H(mi )). Upon receiving the measurements and TPM-signed hash value, the attester proves the authentication
of measurements by verifying the hash value, which helps determine the integrity level
of the platform. However, IMA requires to verify the entire components of the attestee
platform while the attestee may only demand the verification of certain applications.
Also, the integrity status of a system is validated by testing each measurement entry
independently, focusing on the high integrity processes. However, it is impractical to
disregard newly installed untrusted applications or data from the untrusted network.
PRIMA [9] is an attestation work based on IMA and CW-Lite integrity model [14].
PRIMA attempts to improve the efficiency of attestation by only verifying codes, data,
and information flows related to trusted subjects. On one hand, PRIMA needs to be
extended to capture the dynamic nature of system states such as software and policy
updates, which could be an obstacle for maintaining its efficiency. On the other hand,
PRIMA represents an attestation result with binary decision (trust or distrust) and does
not give semantic information about how much the attestee platform can be trusted.
Property-based attestation [7] is an effort to protect the privacy of a platform by collectively mapping related system configurations to attestation properties. For example,
“SELinux-enabled” is a property which can be mapped to a system configuration indicating that the system is protected with an SELinux policy. That is, this approach
prevents the configurations of a platform from being disclosed to a challenger. However, due to the immense configurations of the hardware and software of the platform,
mapping all system configurations to properties is infeasible and impractical.
2.2 Integrity Models
To describe the integrity status for a system, there exist various information flow-based
integrity models such as Biba [5], LOMAC [16], Clark-Wilson [13], and CW-Lite [14].
Biba integrity property is fulfilled if a high integrity process cannot read/execute a lower
integrity object, nor obtain lower integrity data in any other manner. LOMAC allows
high integrity processes to read lower integrity data, while downgrading the process’s
integrity level to the lowest integrity level that has ever been activated. Clark-Wilson
provides a different view of dependencies, which states information can flow from low
integrity objects to high integrity objects through a specific program called transaction

DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

185

procedures (TP). Later, the concept of TP is evolved as a filter in the CW-Lite model.
The filter can be a firewall, an authentication process, or a program interface for downgrading or upgrading the privileges of a process.
With existing integrity models, there is a gap between concrete the measurements
of a system’s components and the verification of its integrity status. We believe an
application-oriented and domain-centric approach accommodates the requirements of
attestation evaluation better than advocating an abstract model. For example, in a Linux
system, a subject in one of traditional integrity models can correspond to a set of
processes, belonging to a single application domain. For instance, an Apache domain
may include various process types such as httpd t, http sysadm devpts t, and
httpd prewikka script t. All of these types can have information flows among
them, which should be regarded as a single integrity level. Also, sensitive objects in a
domain should share the same integrity protection of its subjects. To comprehensively
describe the system integrity requirements, in this paper, we propose a domain-based
isolation approach as discussed in the subsequent section.

3 Domain-Based Isolation
According to TCG and IMA, the trustworthiness of a system is described with the
measured integrity values (hash values) of loaded software components. However, the
measurement efficiency and attestation effectiveness are major problems of these approaches since (i) too many components have to be measured and tracked, and (ii) too
many known-good hash values are required from different software vendors or authorities. Fundamentally, this requires that in order to trust a single application of a system,
every software component in the system has to be trusted; otherwise the attestation result should be negative. In our work, we believe that the trustworthiness of a system
is tightly related to the integrity status, which is, in turn, described by a set of integrity
rules that are enforced by the system. If any of the rules is violated, it should be detected.
Hence, in order to trust a single application domain, we just need to ensure the system
TCB–including reference monitor and integrity rules protecting the target application
domain–are not altered.
Based on this anatomy, we introduce domain-based isolation principles for integrity
protection, which are the criteria to describe the integrity status of a system and thus
its trustworthiness. We first propose general methodologies to identify high integrity
processes, which include system TCB and domain TCB, and then we specify security
rules for protecting these high integrity processes. System TCB (T CBs ) is similar to the
concept of traditional TCB [3], which can be identified along with the subjects functioning as the reference monitor of a system [4]. Applying this concept to SELinux [15],
for example, subjects functioning as reference monitor such as checkpolicy and
loading policy belong to system TCB. Also, subjects used to support reference
monitor such as kernel and initial should also be included into system TCB.
With this approach, an initial T CBs can be identified, and other subjects such as lvm
and restorecon can be added into T CBs based on their relationships with the initial
T CBs . Other optional methods for identifying T CBs are proposed in [10]. Considering the similarity of operating systems and configurations, we expect that the results

186

W. Xu et al.

would be similar. Furthermore, for attestation purpose, T CBs also includes integrity
measurement and reporting components, such as kernel level integrity measurement
agent [1] and attestation request handling agent.
In practice, other than T CBs , an application or user-space service also can affect
the integrity thus the behavior of a system. An existing argument [3] clearly states
the situation: “A network server process under a UNIX-like operating system might
fall victim to a security breach and compromise an important part of the system’s security, yet is not part of the operating system’s TCB.” Accordingly, a comprehensive
mechanism of policy analysis for TCB identification and integrity violation detection is
desired. Hence, we introduce a concept called information domain TCB (or simply domain TCB, T CBd ). Let d be an information domain functioning as a certain application
or service through a set of related subjects and objects, domain d’s TCB or T CBd is
composed of a set of subjects and objects in information domain d which have the same
level of security sensitivity. By the same level of security sensitivity, we mean that, if
information can flow to some subjects or objects of the domain, it can flow to all others
in the domain. That is, they need the same level of integrity protection. Prior to the
identification of T CBd , we first identify the information domain d based on its main
functions and relevant information flow associated with these functions. For example, a
running web server domain consists of many subjects–such as httpd process, plugins,
tools, and other objects–such as data files, configuration files, and logs.
The integrity of an object is determined by the integrity of subjects that have operations on this object. All objects dependent on T CBd subjects are classified as TCB
protected objects or resources. Thus we need to identify all T CBd subjects from an
information domain and verify the assurance of their integrity. To ease this task, a minimum T CBd is first discovered. In the situation that the minimum T CBd subjects have
dependency relationships with other subjects, these subjects should be added to domain
TCB, or the dependencies should be removed. Based on these principles, we first identify initial T CBd subjects which are predominant subjects for the information domain
d. We further discover other T CBd subjects considering subject dependency relationships with the initial T CBd through information flow transitions, which means that the
subjects that can only flow to and from the initial T CBd subjects should be included
into T CBd . For instance, for a web server domain, httpd is the subject that initiates
other web server related processes. Hence, httpd is the predominant subject and belongs to T CBd . Then, based on all possible information flows to httpd, we identify
other subjects such as httpd-suexec in T CBd .
To protect the identified T CBs and T CBd , we develop principles similar to those in
Clark-Wilson [13]. Clark-Wilson leverages transaction procedures (TP) to allow information flow from low integrity to high integrity processes. Hence, we also develop the
concept of filters. Filters can be processes or interfaces [11] that normally are distinct
input information channels and are created by a particular operation such as open(),
accept(), or other calls that enable data input. For example, su process allows a low
integrity process (e.g., staff) being changed to be a high integrity process (e.g., root) by
executing passwd process, thus passwd can be regarded as a filter for processes run
by root privilege. Also, high integrity process (e.g., httpd administration) can accept
low integrity information (e.g, network data) through the secure channel such as sshd.

DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

187

Consequently, sshd can be treated as another example of filter for higher privilege processes. With the identifications of T CBs , T CBd and filters, for information domain d,
all the other subjects in a system are categorized as NON-TCB.
Definition 1. Domain-based isolation is satisfied for an information domain d if information flows are from T CBd ; or information flows are from T CBs to either T CBd or
T CBd protected resources; or information flows are from NON-TCB to either T CBd
or T CBd protected resources via filter(s).

4 Design of DR@FT
DR@FT consists of three main parties: an attestee (the target platform), an attester
(attestation challenger), and a trusted authority, as shown in Figure 1. The attestee is
required to provide its system state information to the attester to be verified. Here,
we assume that an attestee initially is in a trusted system state. After certain system
behaviors, the system state is changed to a new state.
1
Initial Trusted System
State

3

Attestee Measurements
m(tsl)

m(cd)

m(policy)

m(filter)

m(rprocess)

System State
Changes
New System State

Subject 1

Code 1

Rule 1

Filter 1

Rprocess 1

Subject 2
Subject 3

Code 2
Code 3

Rule 2
Rule 3

Filter 2
Filter 3

Rprocess 2
Rprocess 3

Code and Data

TSL

Policy

Filter

Codes and Data
Verification
Known
Fingerprints

4
Reporting Process
Authentication

Rprocess

Reporting Daemon
2

AIKpub /
AIkpvt

IMA

Policy Updates

TPM

Rule 1'
Rule 2'

Initial Trusted
System State

5
Policy Analysis

6
Attester

Attestee

PKs /
Sks

Trusted Authority

Fig. 1. Overview of DR@FT

An attestation reporting daemon on the attestee gets the measured new state information (step 1) with IMA, and generates the policy updates (step 2). This daemon then
gets AIK-signed PCR value(s) and sends to the attester. After the attester receives and
authenticates the information, with the attestee’s AIK public key certificate from the
trusted authority, it verifies the attestee integrity through codes and data verification
(step 3), reporting process authentication (step 4) and policy analysis (step 5).
4.1 System State and Trust Requirement
For the attestation purpose, the system state is a snapshot of an attestee system at a
particular moment, where the factors characterizing the state can influence the system
integrity in any future moment of the system. Based on the domain-based isolation, the
attestee system integrity can be represented via information flows, which are characterized by the trusted subject list, filters, policies, and the trustworthiness of T CBs . Based
on these, we define the system state of the attestee as follows.

188

W. Xu et al.

Definition 2. A system state at the time period i is a tuple Ti ={ T SLi, CDi , P olicyi ,
F ilteri , RP rocessi }, where
– T SLi ={s0 , s1 ....sn } represents a set of high integrity processes which corresponds
to the set of subjects s0 , s1 ....sn in T CBs and T CBd .
– CDi ={cd (s0 ), cd (s1 )....cd (sn )} is a set of codes and data for loading a subject
sj ∈ T SLi .
– P olicyi is the security policy currently configured on the attestee.
– F ilteri is a set of processes defined to allow information flow from low integrity
processes to high integrity processes.
– RP rocessi represents a list of processes that measure, monitor, and report the current T SLi , CDi , F ilteri and P olicyi information. IMA agent and the attestation
reporting daemon are the examples of the RP rocessi .
According to this definition, a system state does not include a particular application’s
running state such as its memory page and CPU context (stacks and registers). It only
represents the security configuration or policy of an attestee system. A system state transition indicates one or more changes in T SLi , CDi , P olicyi , F ilteri , or RP rocessi .
A system state Ti is trusted if T SLi belongs to T CBs and T CBd ; CDi does not contain
untrusted codes and data; P olicyi satisfies domain-based isolation; F ilteri belongs to
the defined filter in domain-based isolation; and RP rocessi codes and data do not contain malicious codes and data and these RP rocessi processes are integrity protected
from the untrusted processes via P olicyi .
As mentioned, we assume there exists an initial trusted system state T0 which satisfies. Through changing the variables in T0 , the system transits to states T1 , T2 ... Ti .
The attestation purpose is to verify if any of these states is trusted.
4.2 Attestation Procedures
Attestee Measurements. The measurement at the attestee side has two different forms,
depending on how much the attestee system changes. In case any subject in T CBs
is updated, the attestee must be fully remeasured from the system reboot and the attester needs to attest it completely, as this subject may affect the integrity of subjects in
RP rocess of the system such as the measurement agent and reporting daemon. After
the reboot and all T CBs subjects are remeasured, a trusted initial system state T0 is
built. To perform this re-measurement, the attestee measures a state Ti and generates
the measurement list Mi which is added by Trusted subject list (T SLi) measurement,
Codes and data (CDi ) measurement,Policy (P olicyi ) measurement, Filter (F ilteri )
measurement and Attestation Process (RP rocessi ) measurement. Also, H(Mi ) is extended to a particular PCR of the TPM, where H is a hash function such as SHA1.
In another case, where there is no T CBs subject updated and the T SLi or F ilteri
subjects belonging to T CBd are updated, the attestee only needs to measure the updated
codes and data loading the changed TSL or filter subject, and generates a measurement
list Mi . The generation of this measurement list is realized through the run-time measurement supported by the underlying measurement agent.
To support both types of measurements, we develop an attestation reporting daemon which monitors the run-time measurements of the attestee. In case the run-time

DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

189

measurements for the T CBs are changed, the attestee is required to be rebooted and
fully measured with IMA. The measurements values are then sent to the attester by the
daemon. On the other side, the changed measurement value is measured by IMA and
captured with the reporting daemon only if the measurement for T CBd is changed.
Obviously, this daemon should be trusted and is included as part of T CBs . That is,
its codes and data are required to be protected with integrity policy and corresponding
hash values are required to be stored at the attester side.
Policy Updates. To analyze if the current state of the attestee satisfies domain-based
integrity property, the attester requires information about the current security policy
loaded at the attestee side. Due to the large volume of policy rules in a security policy,
sending all policy rules in each attestation and verifying all of them by the attester
may cause the performance overhead. Hence, in DR@FT, the attestee only generates
policy updates from the latest attested trusted state and sends them to the attester for the
attestation of such updates.
To support this mechanism, we have the attestation reporting daemon monitor any
policy update on attestee system and generate a list of updated policy rules. Note that
the policy update comparison is performed between the current updated policy and the
stored trusted security policy P olicy0 or previously attested and trusted P olicyi−1 . The
complexity of this policy update algorithm is O(nr), where nr represents the number of
the policy rules in the new policy file P olicyi .
Codes and Data Verification. With received measurement list Mi and AIK-signed
PCRs, the attester first verifies the measurement integrity by re-constructing the hash
values and compares with PCR values. After this is passed, the attester performs the
analyses. Specifically, it obtains the hash values of CDi and checks if they corresponds
to known-good fingerprints. Also, the attester needs to assure that the T SLi belongs
to T CBs and T CBd . In addition, the attester also gets the hash value of F ilteri and
ensures that they belong to the filter list defined on the attester side. In case this step
successes, the attester has the assurance that target processes on attestee side are proved
without containing any untrusted code or data, and the attester can proceed to next steps.
Otherwise, the attester sends a proper attestation result denoting this situation.
Authenticating Reporting Process. To prove that the received measurements and updated policy rules are from the attestee, the attester authenticates them by verifying that
all the measurements, updates and integrity measurement agent processes in the attestee
are integrity protected. That is, the RP rocessi does not contain any untrusted codes or
data and its measurements correspond to PCRs in the attester. Also, there is no integrity
violated information flow to these processes from subjects of T SLi , according to the
domain isolation rules. Note that these components can also be updated, but after any
update of these components, the system should be fully re-measured and attested from
boot time as aforementioned, i.e., to re-build a trusted initial system state T0 .
Policy Analysis by Attester. DR@FT analyzes policy using a graph-based analysis
method. In this method, a policy file is first visualized into a graph, then this policy
graph is analyzed against pre-defined security model such as our domain-based isolation, and a policy violation graph is generated. The main goal of this approach is to give

190

W. Xu et al.

semantic information of attestation result to the attestee, such that its system or security
administrator can quickly and intuitively obtain any violated integrity configuration.
Note that verifying all the security policy rules in each attestation request decrease
the efficiency, as loading policy graph and checking all the policy rules one by one
cost a lot of time. Thus, we need to develop an efficient way for analyzing the attestee
policy. In our method, the attester stores the policy of initial trusted system state T0 or
the latest trusted system state Ti , and its corresponding policy graph is loaded which
does not have any policy violation. Upon receiving the updated information from the
attestee, the attester just needs to analyze these updates to see if there is new information
flow violating integrity.
Through this algorithm, rather than analyzing all the policy rules and all information
flows for each attestation, we verify the new policy through only checking the updated
policy rules and the newly identified information flow. The complexity of this policy
analysis algorithm is O(nn +nl +nt), where nn represents the number of changed subjects and objects, nl is the number of changed subjects and objects relationship in the
policy update file; and nt represents the number of changed TCB in the TSL file.
Attestation Result Sending to Attester. In case the attestation is successful, a new
trusted system state is developed and the corresponding information is stored at the attester side for subsequent attestations. On the other hand, if the attestation fails, there are
several possible attestation results including CDi Integrity Fail, CDi Integrity Success,
RP rocessi Unauthenticated, and P olicyi Fail/Success, and CDi Integrity Success,
RP rocessi Authenticated, and P olicyi Fail /Success. To assist the attestee reconfiguration, the attester also sends a representation of the policy violation graph to the attestee.
Moreover, with this policy violation graph, the attester specifies the violation ranking
and the trustworthiness of the attestee, which is explained in next section.

5 Integrity Violation Analysis
As we discussed in Section 1, existing attestation solutions such as TCG and IMA lack
the expressiveness of the attestation result. In addition to their boolean-based response
for attestation result, DR@FT adopts a graph-based policy analysis mechanism, where
a policy violation graph can be constructed for identifying all policy violations on the

Fig. 2. Example policy violation graph and rank. The SubjectRank and PathRank indicate the
severity of violating paths.

DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

191

attestee side. We further introduce a risk model built on a ranking scheme, which gives
the implication of how severe the discovered policy violations are, and how to efficiently
resolve them.
5.1 Policy Violation Graph
Based on domain-based isolation model, we can find out two kinds of violation paths,
direct violation paths and indirect violation paths. A direct violation path is a one-hop
path through which an information flow can go from a low integrity subject to a high
integrity subject. We observe that information flows are transitive in general. Therefore,
there may exist information flows from a low integrity subject to a high integrity subject
via several other subjects. This multi-hop path is called indirect violation path. All direct
and indirect violation paths belonging to a domain can construct a policy violation graph
for this domain.
Definition 3. A policy violation graph for a domain d is a directed graph Gv= (V v , E v ):
– V v ⊆ VNv T CB ∪ VTvCBd ∪ VTvCB where VNv T CB , VTvCBd and VTvCB are subject
vertices containing in direct or indirect violation paths of domain d and belong to
NON-TCB, T CBd , and T CBs , respectively.
v
v
v
v
v
v
v
v
– E v ⊆ EN
d ∪ EdT ∪ EN T ∪ EN T CB ∪ ET CBd ∪ ET CB where EN d ⊆ (VN T CB ×
v
v
v
v
v
v
v
v
VT CBd ), EdT ⊆ (VT CBd × VT CB ), EN T ⊆ (VN T CB × VT CB ), EN T CB ⊆
(VNv T CB ×VNv T CB ), ETv CBd ⊆ (VTvCBd ×VTvCBd ), and ETv CB ⊆ (VTvCB ×VTvCB ),
and all edges in E v are contained in direct or indirect violation paths of domain d.
Figure 2 (a) shows an example of policy violation graph which examines information
flows between NON-TCB and T CBd 1 . Five direct violation paths are identified in this
graph: ¡S1 , S1 ¿, ¡S2 , S2 ¿, ¡S3 , S2 ¿, ¡S4 , S4 ¿, and ¡S5 , S4 ¿, crossing all the boundaries
between NON-TCB and T CBd . Also, eight indirect violation paths exist. For example,
¡S2 , S5 ¿ is a four-hop violation path passing through other three T CBd subjects S2 ,
S3 , and S4 .
5.2 Ranking Policy Violation Graph
In order to explore more features of policy violation graphs and facilitate efficient policy
violation detection and resolution, we introduce a scheme for ranking policy violation
graphs. There are two steps to rank a policy violation graph. First, T CBd subjects in
the policy violation graph are ranked based on dependency relationships among them.
The rank of a T CBd subject shows reachable probability of low integrity information
flows from NON-TCB subjects to the T CBd subject. In addition, direct violation paths
in the policy violation graph are evaluated based on the ranks of T CBd subjects to
indicate severity of these paths which allow low integrity information to reach T CBd
subjects. The ranked policy violation graphs are valuable for a system administrator
1

Similarly, the information flows between NON-TCB and T CBs , and between T CBd and
T CBs can be examined accordingly.

192

W. Xu et al.

as they need to estimate the risk level of a system and provide a guide for choosing
appropriate strategies for resolving policy violations efficiently.
Ranking Subjects in T CBd . Our notation of SubjectRank (SR) in policy violation
graphs is a criterion that indicates the likelihood of low integrity information flows
coming to a T CBd subject from NON-TCB subjects through direct or indirect violation
paths. The ranking scheme we introduce in this section adopts a similar process of
rank analysis applied in hyper-text link analysis system, such as Google’s PageRank [6]
that utilizes a link structure provided by hyperlinks between web pages to gauge their
importance. Comparing with PageRank which focuses on analyzing a web graph where
the entries are any web pages contained in the web graph, the entries of low integrity
information flows to T CBd subjects in a policy violation graph are only identified
NON-TCB subjects.
Consider a policy violation graph with N NON-TCB subjects, and si is a T CBd subject. Let N (si ) be the number of NON-TCB subjects from which low integrity informa
tion flows could come to si , N (si ) the number of NON-TCB subjects from which low
integrity information flows could directly reach to si , In(si ) a set of T CBd subjects
pointing to si , and Out(sj ) a set of T CBd subjects pointed from sj . The probability of
low integrity information flows reaching a subject si is given by:
SR(si ) =




N (si ) N (si )
N (si )
(
+ (1 −
)
N
N (si )
N (si ) s ∈In(s
j

i)

SR(sj )
)
|Out(sj )|

(1)

SubjectRank can be interpreted as a Markov Process, where the states are T CBd subjects, and the transitions are the links between T CBd subjects which are all evenly
probable. While a low integrity information flow attempts to reach a high integrity subject, it should select an entrance (a NON-TCB subject) which has the path(s) to this
(si )
.
subject. Thus, the possibility of selecting correct entries to a target subject is NN
After selecting correct entries, there still exist two ways, through direct violation paths
or indirect violation paths, to reach a target subject. Therefore, the probability of flow


N (si )
N (si ) for

N (si )
N (si ) mass

transition from a subject is divided into two parts:
1−



N (si )
N (si )

for indirect violation paths. The 1 −
SR(sj )
|Out(sj )|

direct violation paths and
is divided equally among

is the rank value derived from sj .
the subject’s successors sj , and
Figure 2 (b) displays a result of applying Equation (1) to the policy violation graph
showing in Figure 2 (a). Note that even though subject s4 has two direct paths from
NON-TCB subjects like subject s2 , the rank value of s4 is higher than the rank value of
s2 , because there is another indirect flow path to s4 (via s3 ).
Ranking Direct Violation Path. We further define PathRank (PR) as the rank of a
direct violation path2 , which is a criterion reflecting the severity of the violation path
through which low integrity information flows may come to T CBd subjects. Direct violation paths are regarded as the entries of low integrity data to T CBd in policy violation
2

It is possible that a system administrator may also want to evaluate indirect violation paths
for violation resolution. In that case, our ranking scheme could be adopted to rank indirect
violation paths as well.

DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

193

graph. Therefore, the ranks of direct violation paths give a guide for system administrator to adopt suitable defense countermeasures for solving identified violations.
To calculate PathRank accurately, three conditions are needed to be taken into account: (1) the number of T CBd that low integrity flows can reach through this direct
violation path; (2) SubjectRank of reached T CBd subjects; and (3) the number of hops
to reach a T CBd subject via this direct violation path.


Suppose < si , sj > is a direct violation path from a NON-TCB subject si to a T CBd

subject sj in a policy violation graph. Let Reach(< si , sj >) be a function returning
a set of T CBd subjects to which low integrity information flows may go through a


direct violation path < si , sj >, SR(sl ) the rank of a T CBd subject sl , and Hs (si , sl )

a function returning the hops of the shortest path from a NON-TCB subject si to a
T CBd subject sl . The following equation is utilized to compute a rank value of the

direct violation path < si , sj >.




P R(< si , sj >) =

SR(sl )


sl ∈Reach(<s ,sj >)
i



Hs (si , sl )

(2)

Figure 2 (c) shows the result using the above-defined equation to calculate the PathRank

of the example policy violation graph. For example, < s2 , s2 > has a higher rank than


< s1 , s1 >, because < s2 , s2 > may result in low integrity information flows to reach

more or important T CBd subjects than < s1 , s1 >.
5.3 Evaluating Trustworthiness
Let Pd be a set of all direct violation paths in a policy violation graph. The entire rank,
which can be considered as a risk level of the system, can be computed as follows:
RiskLevel =



<s ,sj >∈Pd
i



P R(< si , sj >)

(3)

The calculated risk level could reflect the trustworthiness of an attestee. Generally, the
lower risk level indicates the higher trustworthiness of a system. When an attestation is
successful and there is no violation path being identified, the risk level of the attested
system is zero, which means an attestee has the highest trustworthiness. On the other
hand, when an attestation is failed, corresponding risk level of a target system is computed. A selective service could be achieved based on this fine-grained attestation result.
That is, the number of services provided by a service provider to the target system may
be decided with respect to the trust level of the target system. On the other hand, a system administrator could refer to this attestation result as the evaluation of her system as
well as guidelines since this quantitive response would give her a proper justification to
adopt countermeasures for improving the system’s trustworthiness.

6 Implementation and Evaluation
We have implemented DR@FT to evaluate its effectiveness and performance. Our attestee platform is a Lenovo ThinkPad X61 with Intel Core 2Duo Processor L7500

194

W. Xu et al.

1.6GHz, 2 GB RAM, and Atmel TPM. We enable SELinux with the default policy
based on the current distribution of SELinux [15]. To measure the attestee system with
TPM, we update the Linux kernel to 2.6.26.rc8 with the latest IMA implementation [1],
where SELinux hooks and IMA functions are enabled. Having IMA enabled, we configure the measurement of the attestee information. After the attestee system kernel is
booted, we mount the sysfs file system and inspect the measurement list values in
ascii runtime measurements and ascii bios measurements.
6.1 Attestation Implementation
We start from a legitimate attestee and make measurements of the attestee system for the
later verification. To invoke a new attestation request from the attester, the attestation
reporting daemon runs in the attestee and monitors the attestee system. This daemon
is composed of two main threads: One monitors and gets the new system state measurements, and the other monitors and obtains the policy updates of the attestee. The
daemon is also measured and the result can be obtained through the legitimate attestee.
Thus the integrity of the daemon can be verified later by the attester. In case the attestee
system state is updated due to new software installation, changing policy, and so on, an
appropriate thread of the daemon automatically obtains the new measurement values
as discussed in 4. The daemon then securely transfers the attestation information to the
attester based on the security mechanisms supported by the trusted authority.
After receiving the updated system information from the attestee, the measurement
module of the attester checks the received measurements against the stored PCR to
prove its integrity. To analyze the possible revised attestee policy, the policy analysis
module is developed as a daemon, which is ported from a policy analysis engine. We
extend the engine to identify violated information flows from the updated policy rules
based on domain-based isolation rules. We also accommodate the algorithm presented
in Section 4.2, as well as our rank scheme to evaluate the trustworthiness of the attestee.
6.2 Evaluation
To assess the proposed attestation framework, we attest our testbed platform with Apache
web server installed. To configure the trusted subject list of the Apache domain, we first
identify the T CBs based on the reference monitor-based TCB identification, including
the integrity measurement, monitoring agents, and daemon. For T CBd of the Apache,
we identify the Apache information domain, Apache T CBd , including httpd t and
httpd suexec t, and the initial filters sshd t, passwd t, su t, through the
domain-based isolation principles. Both T CBs and T CBd are identified with a graphical policy analysis tool [17]. We then install the unverified codes and data to evaluate
the effectiveness of our attestation framework.
Installing Malicious Code. We first install a Linux rootkit, which gains administrative
control without being detected. Here, we assign the rootkit with the domain
unconfined t that enables information flows to domain initrc t labeling initrc
process, which belongs to T CBs of the attestee. Following the framework proposed in
Section 4, the attestee system is measured from the bootstrap with configured IMA. After

DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

195

getting the new measurement values, the reporting daemon sends these measurements to
the attester. Note that there is no policy update in this experiment. Different from IMA,
we only measure the T CBs and T CBd subjects. After getting the measurements from
the attestee, attester verifies them by trying to match the measured hash values. Partial
of our measurement shows the initial measurements of the initrc (in a trusted initial
system state) and the changed value because of the installed rootkit. The difference between these two measurements indicates the original initrc is altered, and the attester
confirms that the attestee is not in a trusted state.
Installing Vulnerable Software. In this experimentation, we install a vulnerable
software called Mplayer on the attestee side. Mplayer is a media player and encoder
software which is susceptible to several integer overflows in the real video stream dumuxing code. These flaws allow an attacker to cause a denial of service or potentially
execution of the arbitrary code by supplying a deliberately crafted video file. After a
Mplayer is installed, a Mplayer policy module is also loaded into the attestee policy. In
this policy module, there are several different subjects such as staff mplayer t,
sysadm mplayer t. Also, some objects are defined in security policies such as
user mplayer home t and staff mplayer home t.

httpd_awstats_script_t

httpd_prewikka_script_t

sysadm_mplayer_t
sysadm_mencoder_t

0.3426

0.1713

httpd_suexec_t

staff_mplayer_t

0.0556

httpd_rotatelogs_t

httpd_t

staff_mencoder_t

0.3889

0.3333

0.3426

0.3889

cifs_t

user_mplayer_t
user_mencoder_t
0.1713
0.1713 0.12963
0.12963

0.5269

0.12963

ncsd_var_run_t

0.26345
0.26345

sysadm_devpts_t

Fig. 3. Information flow verification of Mplayer. The links show the information flow from
Mplayer (filled circle nodes) to Apache (unfilled nodes). The rank values on the paths indicate
the severity of the corresponding violation paths.

After the Mplayer is installed, the attestation daemon finds that the new measurement of Mplayer is generated and the security policy of the system is changed. As the
Mplayer does not belong to T CBs and Apache T CBd , the attestation daemon does not
need to send the measurements to the attester. Consequently, the daemon only computes
the security policy updates and sends the information to the attester.
Upon receiving the updated policies, we analyze these updates and obtain a policy violation graph as shown in Figure 3. Through objects such as cifs t, sysadm dev
tps t, ncsd var run t, information flows from Mplayer can reach Apache domain. In addition, rank values are calculated and shown in the policy violation graph,
which guides effective violation resolutions. For example, there are three higher ranked

196

W. Xu et al.

paths including path from sysadm devpts t to httpd t, from ncsd var run t
to httpd rotatelogs t, and from cifs t to httpd prewikka script t.
Meanwhile, a risk level value (1.2584) reflecting the trustworthiness of the attestee system is computed based on the ranked policy violation graph.
Once receiving the attestation result shown in Figure 3, the attestee administrator
solves the violation that has the higher rank than others. Thus, the administrator can first
resolve the violation related to httpd t through introducing httpd sysadm devpt
s t.
allow httpd t httpd sysadm devtps t:chr file {ioctl read write
getattr lock append};

After the policy violation resolution, the risk level of the attestee system is lowered to
0.7315. Continuously, after the attestee resolves all the identified policy violations and
the risk level is decreased to be zero, the attestation daemon gets a new policy update
file and sends it to the attester. Upon receiving this file, the attester verifies whether
these information flows violate domain-based isolation integrity rules since these flows
are within the NON-TCB–even though there are new information flow compared to the
stored P olicy0 . Thus, an attestation result is generated which specifies the risk level (in
this case, zero) of the current attestee system. Consequently, a new trusted system state
is built for the attestee. In addition, the information of this new trusted system state is
stored in the attester side for the later attestation.
6.3 Performance
To examine the scalability and efficiency of DR@FT, we investigate how well the attestee measurement agent, attestation daemon, and the attester policy analysis module
scale along with the increased complexity, and how efficiently DR@FT performs by
comparing it with the traditional approaches.
In DR@FT, the important situations influencing the attestation performance include
system updates and policy changes. Hence, we evaluate the performance of DR@FT
by changing codes and data to be measured and modifying the security policies. Based
on our study, we observe that normal policy increased or decreased no more than 40KB
when installing or uninstalling software. Also, a system administrator does not make
the enormous changes over the policy. Therefore the performance is measured with the
range from zero to around 40KB in terms of policy size.
Performance on the attestee side. Based on DR@FT, the attestee has three main factors influencing the attestation performance. (1) Time spent for the measurement: Based
on our experimentation, the measurement time increases roughly linearly with the size
of the target files. For example, measuring policy files with 17.2MB and 20.3MB requires 14.63 seconds and 18.26 seconds, respectively. Measuring codes around 27MB
requires 25.3sec. (2) Time spent for identifying policy updates TP update : Based on
the specification in Section 4, policy updates are required to be identified and sent to
the attester. As shown in Table 1, for a system policy which is the size of 17.2MB at
its precedent state, the increase of the policy size requires more time for updating the

DR@FT: Efficient Remote Attestation Framework for Dynamic Systems

197

Table 1. Attestation Performance Analysis (in seconds)
Policy Change
Size
0
-0.002MB (Reduction)
-0.019MB (Reduction)
-0.024MB (Reduction)
0.012MB (Reduction)
0.026MB (Addition)
0.038MB (Addition)

TP update
0.23
0.12
0.08
0.04
0.37
0.58
0.67

Dynamic
Tsend TP analysis
0
0
0.002 0.02
0.01
0.03
0.02
0.03
0.01
0.03
0.02
0.03
0.03
0.04

Overhead
0.23
0.14
0.12
0.09
0.41
0.63
0.74

TP send
14.76
14.76
14.74
14.74
14.77
14.78
14.79

Static
TP analysis
90.13
90.11
89.97
89.89
90.19
90.33
90.46

Overhead
104.89
104.87
104.34
104.23
104.96
105.11
105.25

policy and vice versa. (3) Time spent for sending policy updates TP send : Basically, the
more policy updates, the higher overhead was observed.
Performance on the attester side. In DR@FT, the measurement verification is relatively straightforward. At the attester side the time spent for policy analysis TP analysis
mainly influences its performance. As shown in Table 1, the analysis time roughly increases when the policy change rate increases.
Comparison of dynamic and static attestation. To further specify the efficiency of
DR@FT, we compare the overhead of DR@FT with a static attestation. In the static
approach, the attestee sends all system state information to an attester, and the attester
verifies the entire information step by step. As shown in Table 1, the time spent for
static attestation is composed of TP send and TP analysis , which represent the time for
sending policy module and analyzing them, respectively. Obviously, the dynamic approach can dramatically reduce the overhead compared to the static approach. It shows
that DR@FT is an efficient way when policies on an attestee are updated frequently.

7 Conclusion
We have presented a dynamic remote attestation framework called DR@FT for efficiently verifying if a system satisfies integrity protection property and indicates integrity
violations which determine its trustworthiness level. The integrity property of our work
is based on an information flow-based domain isolation model, which is utilized to
describe the integrity requirements and identify integrity violations of a system. To
achieve the efficiency and effectiveness of remote attestation, DR@FT focuses on system changes on the attestee side. We have extended a powerful policy analysis engine
to represent integrity violations with the rank scheme. In addition, our results showed
that our dynamic approach can dramatically reduce the overhead compared to static
approach. We believe such an intuitive evaluation method would help system administrators reconfigure the system with more efficient and strategic manner.
There are several limitations of our attestation framework. First, DR@FT can attest
dynamic system configurations, but it does not attest the trustworthiness of dynamic
contents such as application state and CPU context. Second, our risk evaluation does
not explain under what kind of condition, what range of the risk value is acceptable
for the attestee or attester. In addition, in our work, all verification work is done at the

198

W. Xu et al.

attester side. There is a possibility that the attester can delegates some attestation tasks
to trusted components at the attestee side. In the future, we would further investigate
these issues.

References
1. LIM Patch, http://lkml.org/lkml/2008/6/27
2. Trusted computing group, https://www.trustedcomputinggroup.org/home
3. Trusted Computer System Evaluation Criteria. United States Government Department of Defense (DOD), Profile Books (1985)
4. Anderson, A.P.: Computer security technology planning study. ESD-TR-73-51 II (1972)
5. Biba, K.J.: Integrity consideration for secure compuer system. Technical report, Mitre Corp.
Report TR-3153, Bedford, Mass. (1977)
6. Brin, S., Page, L.: The anatomy of a large-scale hypertextual Web search engine. Computer
networks and ISDN systems 30(1-7), 107–117 (1998)
7. Chen, L., Landfermann, R., Löhr, H., Rohe, M., Sadeghi, A.-R., Stüble, C.: A protocol for
property-based attestation. In: ACM STC (2006)
8. Haldar, V., Chandra, D., Franz, M.: Semantic remote attestation: a virtual machine directed
approach to trusted computing. In: USENIX Conference on Virtual Machine Research And
Technology Symposium (2004)
9. Jaeger, T., Sailer, R., Shankar, U.: Prima: policy-reduced integrity measurement architecture.
In: ACM SACMAT (2006)
10. Jaeger, T., Sailer, R., Zhang, X.: Analyzing integrity protection in the selinux example policy.
In: USENIX Security (2003)
11. Provos, N., Friedl, M., Honeyman, P.: Preventing privilege escalation. In: 12th USENIX
Security Symposium, p. 11 (August 2003)
12. Sailer, R., Zhang, X., Jaeger, T., van Doorn, L.: Design and implementation of a tcg-based
integrity measurement architecture. In: USENIX Security (2004)
13. Sandhu, R.S.: Lattice-based access control models. IEEE Computer 26(11), 9–19 (1993)
14. Shankar, U., Jaeger, T., Sailer, R.: Toward automated information-flow integrity verification
for security-critical applications. In: NDSS (2006)
15. Smalley, S.: Configuring the selinux policy (2003),
http://www.nsa.gov/SELinux/docs.html
16. Fraser, T.: Lomac: Low water-mark integrity protection for cots environment. In: Proceedings
of the IEEE Symposium on Security and Privacy (May 2000)
17. Xu, W., Zhang, X., Ahn, G.-J.: Towards system integrity protection with graph-based policy analysis. In: Gudes, E., Vaidya, J. (eds.) Data and Applications Security XXIII. LNCS,
vol. 5645, pp. 65–80. Springer, Heidelberg (2009)

Speciﬁcation and Validation of Authorisation
Constraints Using UML and OCL
Karsten Sohr1 , Gail-Joon Ahn2, , Martin Gogolla1 , and Lars Migge1
1

Department of Mathematics and Computer Science,
Universität Bremen, Bibliothekstr. 1,
28359 Bremen, Germany
2
Department of Software and Information Systems,
University of North Carolina at Charlotte
Charlotte, NC 28223, USA

Abstract. Authorisation constraints can help the policy architect design and express higher-level security policies for organisations such as
ﬁnancial institutes or governmental agencies. Although the importance
of constraints has been addressed in the literature, there does not exist a systematic way to validate and test authorisation constraints. In
this paper, we attempt to specify non-temporal constraints and historybased constraints in Object Constraint Language (OCL) which is a constraint speciﬁcation language of Uniﬁed Modeling Language (UML) and
describe how we can facilitate the USE tool to validate and test such policies. We also discuss the issues of identiﬁcation of conﬂicting constraints
and missing constraints.

1

Introduction

Today information technology pervades more and more our daily life. This applies to very diﬀerent domains such as healthcare, e-government, banking. On
the other hand, new technologies go along with new risks, which must be systematically dealt with, such as preventing unauthorised access. Hence it is mandatory to establish adequate mechanisms that enforce the security and protection
requirements demanded by the rules and laws relevant to the organisation in
question. For example, in Europe there do exist strong data protection requirements as those formulated in the Directive 95/46/EC [7]. This directive among
other areas applies to clinical information systems where in particular the principle of patient consent must be enforced [4]. In contrast, in the banking domain
other security requirements such as data integrity are more important such that
often separation of duty policies (SoD) [17,5] must be enforced.
Implementing such higher-level organisational security policies in computer
systems can be cumbersome and ineﬃcient. However, it has turned out that


This work of Gail-J. Ahn was partially supported at the Laboratory of Information
of Integration, Security and Privacy at the University of North Carolina at Charlotte
by the grants from National Science Foundation (NSF-IIS-0242393) and Department
of Energy Early Career Principal Investigator Award (DE-FG02-03ER25565).

S. De Capitani di Vimercati et al. (Eds.): ESORICS 2005, LNCS 3679, pp. 64–79, 2005.
c Springer-Verlag Berlin Heidelberg 2005


Speciﬁcation and Validation of Authorisation Constraints

65

one of the great advantages of role-based access control (RBAC) is that SoD
rules can be implemented in a natural way [9]. Generally speaking, role-based
authorisation constraints are an important means for laying out higher-level
security policies [1,13]. Although there are several works on the speciﬁcation of
role-based authorisation constraints, e.g., [1,13], there is a lack of appropriate
tool support for the validation, enforcement, and testing of role-based access
control policies. Speciﬁcally, tools are needed which can be applied quite easily
by a policy designer without too much deeper training.
As demonstrated in [2,18], the Uniﬁed Modeling Language (UML) and the
Object Constraint Language (OCL) can be conveniently used to specify several
classes of role-based authorisation constraints. Moreover, owing to the fact that
OCL has proved its applicability in several industrial applications1 , OCL is a
good means for such a practically relevant process like the design of security
policies.
However, as mentioned above, tool support is needed in order to have a
broader practical use. Hence, we demonstrate in this paper how to employ the
USE system (UML Speciﬁcation Environment) [19,20] to validate and test access
control policies formulated in UML and OCL. In particular, USE is a validation
tool for UML models and OCL constraints, which has been reportedly applied
in industry and research [19]. With the help of this tool, a policy designer can
detect conﬂicting and missing authorisation constraints.
The paper is now organised as follows: Section 2 gives a short overview of
RBAC, UML/OCL, and introduces the USE system. In Section 3 typical and
partly more complex authorisation constraints are speciﬁed in OCL and in a
temporal OCL extension. Section 4 then demonstrates how USE can be employed
to validate and enforce RBAC security policies and test RBAC conﬁgurations
while Section 5 sketches related work. Section 6 summarises and gives an outlook
on future work.

2

Related Technologies

We ﬁrst give a short overview of RBAC, then we brieﬂy describe UML and
OCL, and ﬁnally introduce the USE tool, which can be employed to validate
OCL constraints.
2.1

RBAC and Authorisation Constraints

RBAC has received considerable attention as an alternative to traditional discretionary and mandatory access control. One reason for this increasing interest is that in practice permissions are assigned to users according to their
roles/functions in the organisation (governmental or commercial) [8]. In addition, the explicit representation of roles greatly simpliﬁes the security management and allows one to use well-known security principles like separation of duty
and least privilege.
1

OCL is UML’s constraint speciﬁcation language and UML has been widely adopted
in software engineering discipline.

66

K. Sohr et al.

In the sequel, we brieﬂy describe RBAC96, a family of RBAC models introduced by Sandhu et al. [22]. RBAC96 has the following components:
–
–
–
–

Users, Roles, P, S (sets of users, roles, permissions, activated sessions)
U A ⊆ U sers × Roles (user assignment)
P A ⊆ Roles × P (permission assignment)
RH ⊆ Roles × Roles is a partial order also called the role hierarchy or role
dominance relation written as ≤.

Users may activate a subset of the roles they are assigned to in a session. P
is the set of ordered pairs of operations and objects. In the context of security
and access control all resources accessible in an IT-system (e.g., ﬁles, database
tables) are referred to by the notion object. An operation is an active process
applicable to objects (e.g., read, write, append). The relation P A assigns to
each role a subset of P . So P A determines for each role the operation(s) it may
execute and the object(s) to which the operation in question is applicable for the
given role. Thus any user having assumed this role can apply an operation to an
object if the corresponding ordered pair is an element of the subset assigned to
the role by P A.
An important advanced aspect of RBAC are authorisation constraints. Authorisation constraints are sometimes argued to be the principal motivation behind the introduction of RBAC [22]. They allow a policy designer to express
higher-level organisational security policies. Depending on the organisation, different kinds of authorisation constraints are required such as SoD in the banking
ﬁeld [5] or constraints on delegation and context constraints in the healthcare
domain [24]. Later in this paper, diﬀerent kinds of authorisation constraints are
speciﬁed and discussed.
2.2

Overview of UML and OCL

Uniﬁed Modeling Language. The Uniﬁed Modeling Language (UML) [21] is
a general-purpose visual modeling language in which we can specify, visualize,
and document the components of software systems. It captures decisions and
understanding about systems that must be constructed. UML has become a
standard modeling language in the ﬁeld of software engineering.
UML permits the description of static, functional, and dynamic models. In
this paper, we concentrate on the static aspects of UML. A static model provides
a structural view of information in a system. Classes are deﬁned in terms of their
attributes and relationships. The relationships include speciﬁcally associations
between classes. In Figure 1, the conceptual static model for RBAC is depicted.
Object Constraint Language. The Object Constraint Language (OCL) [25]
is a declarative language that describes constraints on object-oriented models.
A constraint is a restriction on one or more values of an object-oriented model.
OCL is an industrial standard for object-oriented analysis and design.
Each OCL expression is written in the context of a speciﬁc class. In an OCL
expression, the reserved word self is used to refer to a contextual instance.

Speciﬁcation and Validation of Authorisation Constraints
Session

*

67

Activates
(roles)

name: String
Inherits

*
Establishes
(user)
1
User

*

*

*

Role

UA

*

name: String

name: String

*
PA

*

*

Permission
name: String

Fig. 1. Conceptual Class Model for RBAC-Entity Classes

The type of the context instance of an OCL expression is written with the
context keyword, followed by the name of the type. The label inv: declares the
constraint to be an invariant. Consider the RBAC model from Figure 1: If the
context is Role, then self refers to an instance of Role. The following line shows
an example of an OCL constraint expression describing a role with at most two
users:
context Role inv: self.user->size()<2

self.user is a set of User objects that is selected by navigating from objects
of class Role to User objects through an association. The ‘‘.’’ stands for a
navigation. A property of a set is accessed by an arrow ‘‘->’’ followed by the
name of the property. A property of the set of users is expressed using the size
operation in this example.
The following shows another example describing that a user can be assigned
to a role r2 only if she is already member of r1:
context User inv:
self.role_->includes(’r2’) implies self.role_->includes(’r1’)

The expression self.role ->includes(’r2’) means that r2 is a member
of the set of roles the user is assigned to. The implies connector is similar to
logical implication.
Furthermore, OCL has several built-in operations that can iterate over the
members of a collection (set, bag, ...) such as forAll, exists, iterate, any and
select (cf. [25]).
2.3

The USE Tool

This section explains the functionality of the UML Speciﬁcation Environment
(USE) which allows the validation of UML and OCL descriptions. USE is the
only OCL tool allowing interactive monitoring of OCL invariants and pre- and
postconditions, and the automatic generation of non-trivial system states. These
system states or snapshots consist of the current objects and links between those
objects adhering to the UML model in question.

68

K. Sohr et al.

The central idea of the USE tool is to check for software quality criteria
like correct functionality of UML descriptions already in the design level in an
implementation-independent manner. This approach takes advantage of descriptive design level speciﬁcations by expressing properties more concisely and in a
more abstract way. Such properties are given by invariants and pre- and postconditions, and these are checked by the USE system against the test scenarios,
i.e., object diagrams and operation calls given by sequence diagrams, which the
developer provides. These abstract design level tests are expected to be also used
later in the implementation phase.
The USE tool expects as an input a textual description of a model and its
OCL constraints (for an example of such a description refer to Figure 3). Then
syntax checks of this description are carried out, which verify a speciﬁcation
against the grammar of the speciﬁcation language, basically a superset of OCL
extended with language constructs for deﬁning the structure of the model. Having passed all these checks, the model can be displayed by the GUI provided
by the USE system. In particular, USE makes available a project browser which
displays all the classes, associations, invariants, and pre- and post-conditions of
the current model.
Figure 2 shows a USE screenshot with an example. On the left we see the
project browser displaying the classes, associations, invariants, and operation
pre- and post-conditions. In a detail window below, the selected class is pictured
with all details. On the right, we identify a sequence diagram presenting the
operations which lead to the current system state given in the object diagram
window below. The evaluation of the invariants in this system state is pictured
in the class invariant window to the right of the object diagram window. The

Fig. 2. USE screenshot

Speciﬁcation and Validation of Authorisation Constraints

69

developer gets feedback from USE about the validity of the invariants in the
invariant window and the validity of the pre- and post-conditions in the sequence
diagram window. Further information about the validity of invariants can be
requested by a dialog window for evaluating arbitrary OCL expressions. This
dialog allows ad-hoc queries useful for navigating and exploring a system state
at any time. Hence, USE helps the developer in analysing situations when an
invariant or a pre- or post-condition fails. This query window will be used several
times in Section 4.

3

Constraints Speciﬁcation

In this section, diﬀerent types of authorisation constraints are speciﬁed in OCL.
In the ﬁrst subsection, non-temporal authorisation constraints are formulated in
OCL, whereas in the second subsection history-based authorisation constraints
are formalised in a temporal extension of OCL.
3.1

Non-temporal Authorisation Constraints

Subsequently we give three examples that demonstrate how to use OCL to specify authorisation constraints.
Example 1: Simple Static Separation of Duty (SSOD)
The ﬁrst example concerns a separation of duty constraint. Consider two (or
more) conﬂicting roles such as accounts payable manager and purchasing manager. Mutual exclusion in terms of the user assignment UA speciﬁes that one
individual cannot have both roles. This constraint on UA can be speciﬁed using
the OCL expression as follows 2 :
context User inv SSOD:
let
CR:Set={{AccountsPayableManager, PurchasingManager}, ...}
in
CR->forAll(cr|cr->intersection(self.role_)->size()<=1)

This formulation of SSOD is based upon the SSOD speciﬁcation given in [1].
Speciﬁcally, CR denotes a set which consists of conﬂicting role sets.
Example 2: Prerequisite Roles
The second example is based upon the concept of prerequisite constraints as
introduced in [22]. In this example, we consider a prerequisite constraint stating
that a user can be assigned to the engineer role only if the user is already assigned to the employee role.
2

For the sake of simplicity, we have left out here the part for the creation of the
instances AccountsPayableManager and PurchasingManager. Similar remarks hold
for the subsequent OCL speciﬁcations.

70

K. Sohr et al.
context User inv Prerequisite Role:
self.role ->includes(engineer) implies self.role ->includes(employee)

Example 3: Static Separation of Duty - Conﬂict Users (SSOD-CU)
By means of OCL even more complex authorisation constraints can be formulated. One example of such a constraint is SSOD-CU identiﬁed by Ahn in [1].
SSOD-CU means that two or more colluding users cannot be assigned to conﬂicting roles. For example, it might be the company policy that members of
the same family cannot be assigned to the roles accounts payable manager and
purchasing manager. SSOD-CU can now be expressed in OCL in the following
way:
context User inv SSOD-CU:
let
CU:Set(Set(User))=Set{Set{Michael,Frank,Susan},Set{Lars,Maria}},
CR:Set(Set(Role))=Set{Set{AccountsPayableManager, BillingClerk},
Set{Cashier, CashierSupervisor}, ...}
in
CR->forAll(cr|cr->intersection(self.role_)->size()<=1)
and
CU->forAll(cu|
CR->forAll(cr|cr->iterate(r:Roles; result:Set(User)=Set{}|
result->union(r.user))->intersection(cu)->size()<=1))

SSOD-CU is a composite constraint consisting of two parts, an SSOD part and an
additional part concerning the conﬂicting users. The SSOD part is required because otherwise obviously the whole constraint would not be useful. The iterate
operation iterates over all roles r belonging to a set of conﬂicting roles and collects all users of these roles. CR has the same meaning as in example 1 whereas
CU is a set consisting of all conﬂicting user sets.
3.2

History-Based Constraints

OCL is quite similar to ﬁrst-order predicate logic. As expressions of the predicate
calculus, OCL expressions used in invariants are evaluated in a system state.
However, due to the fact that we consider here only one snapshot of the system,
we have no notion of time. Hence, authorisation constraints that consider the
execution history such as history-based or object-based dynamic SoD [10] cannot
be adequately expressed.
In the following, we sketch how history-based authorisation constraints can
be speciﬁed in TOCL (Temporal OCL) [26], an extension of OCL with temporal elements. In particular, temporal operators like always (in the future),
sometime (in the future), and next are available. To demonstrate how historybased authorisation constraints can be formulated in TOCL, we take dynamic
object-based SoD as an example, which has been introduced informally by Nash
and Poland [17]. Dynamic object-based SoD roughly speaking means that a user
must not act upon an object that the same user has previously acted upon.
Other dynamic SoD constraints enumerated in [10] can clearly be expressed in
TOCL, too.

Speciﬁcation and Validation of Authorisation Constraints
model RBAC
-- classes

71

association PA between
Permission[*] role permission
Role[*] role role
end
association establishes between
Users[1] role user
Session[*] role session
end

class Role
attributes
name:String
end
class User
attributes
name:String
end

association activates between
Session[*] role session
Role[*] role role
end

class Permission
attributes
name:String
op:Operation
o:Object
end

association inherits between
Role[*] role senior
Role[*] role junior
end
constraints
context Users inv PrerequisiteRole:
self.role ->includes(r2)
implies self.role ->includes(r1)

class Object
attributes
name:String
end
class Operation
attributes
name:String
end
class Session
attributes
name:String
end
-- associations
association UA between
User[*] role user
Role[*] role role
end

--constraint: user part of SSOD-CU
context Role inv SSOD-CU:
let
CU:Set(Set(User))=Set{{u1,u2,u3},{u4,u5}}
in
let
CR:Set(Set(Role))=Set{Set{r1,r2},...}
in
CU->forAll(cu|
CR->forAll(cr|cr->iterate(r:Role;
result:Set(User)=oclEmpty(Set(User))|
result->union(r.user))->intersection(cu)->size()<=1))

Fig. 3. USE speciﬁcation of an RBAC security policy

In order to specify dynamic object-based SoD in TOCL, we use two predicates introduced in [16], namely auth(u, op, obj) and exec(u, op, obj). The former
predicate means that a user u is authorised to execute operation op on object
obj while the latter means that user u executes operation op on object obj in the
present state. For the sake of simplicity, the full details of those predicates are
left out here. The interested reader is referred to [16] to obtain more information
on that topic.
Due to the fact that exec and auth are ternary predicates and OCL supports
only binary associations we extend OCL with additional predicates Exec and
Auth to express ternary associations, as proposed in [12].
With this extension, we obtain the following TOCL speciﬁcation for objectbased dynamic SoD (using the always operator):
context Object inv ObjDSoD:
Operation.allInstances->forAll(op,op1|
User.allInstances->forAll(u|
(Exec(u,op,self) and op1<>op) implies always not Auth(u,op1,self))))

This corresponds to the speciﬁcation of dynamic object-based SoD in ﬁrstorder linear temporal logic as given in [16]:
∀u : U sers; op, op1 : OpSet; obj : Object.op = op1 ∧ exec(u, op, obj)
⇒ 2¬auth(u, op1, obj).

72

4

K. Sohr et al.

Validation and Testing of RBAC Security Policies

With OCL we have a light-weight formalism at hand, which can help specifying RBAC security policies. What is however missing is a tool which helps a
policy designer in validating her RBAC policy. Hence, in the sequel it will be
demonstrated how the USE tool, which is a general-purpose validation tool for
OCL constraints, can be employed for this purpose (cf. Section 4.1). Speciﬁcally, authorisation constraints such as those categorised in [1] can be handled.
Additionally, USE can also be applied to test concrete RBAC conﬁgurations
against certain conditions (cf. Section 4.2). The last section sketches how the
USE functionality can be used to build an RBAC authorisation editor.
4.1

Validation of RBAC Security Policies

As mentioned in section 2.3, the main application of the USE tool is the validation of UML/OCL models. The same can be carried out with an RBAC security
policy. The USE speciﬁcation of a security policy is given in Figure 3 with the
authorisation constraints expressed by OCL constraints. This policy will serve
as an example within this section.
Through the validation of RBAC policies conﬂicting constraints can be detected and missing constraints identiﬁed. The validation can be done before the
deployment of the RBAC policy, i.e., during the design phase. As indicated above,
the USE approach for validation is to generate system states (snapshots) and
check these states against the speciﬁed constraints. In our case, the system states
are certain RBAC conﬁgurations (consisting of users, roles, the relations between
these entities). The RBAC conﬁgurations could be created automatically by running a script with the state manipulation commands, which are supported by
the USE tool, or as an alternative with a GUI provided by the USE system. This
animation-based approach for the validation of security policies can be regarded
as a complement to a rigorous formal veriﬁcation, which often requires deeper
training in formal methods.
The result of the validation can lead to diﬀerent consequences. Firstly, we
may have reasonable system states that do not satisfy one or more authorisation
constraints of the policy. This may indicate that the constraints are too strong
or the model is not adequate. Secondly, the security policy may allow undesired
system states, i.e., the constraints are too weak. In the following both situations
are discussed more thoroughly.
Conﬂicting Constraints. USE may help the policy designer ﬁnd conﬂicting
constraints. This will be subsequently demonstrated by an example, considering
the RBAC policy presented in Figure 3. Clearly, this example policy is rather
simple, but in reality we often have to deal with considerably more complex
policies. Now, let us further assume that the policy designer has forgotten that
he had once deﬁned a prerequisite role constraint between r1 and r2. Later, the
policy designer decided to deﬁne r1 and r2 mutually exclusive due to a change
of organisational rules/policies. Obviously, both constraints could not be satisﬁed at the same time and hence the composite constraint is too strong. The

Speciﬁcation and Validation of Authorisation Constraints

73

Fig. 4. USE screenshot: two conﬂicting constraints

USE screenshot in Figure 4 displays the situation after user u has been assigned
to r2. Clearly, the policy designer cannot have assigned u to role r1; otherwise the new SSOD constraint would be violated. However, now the constraint
User::PrerequisiteRole is evaluated to false (cf. “Class invariants” view in
Figure 4), and hence the current RBAC conﬁguration is not a correct system
state according to the given policy speciﬁcation.
Admittedly, the mere information that a constraint is false might often not
help to ﬁnd the real reason for the problem and to resolve the conﬂict. Additional
information is required which objects and links of the current state violate the
constraint. For such a purpose, the policy designer can debug the constraints that
are not satisﬁed by the current system state with the “Evaluate OCL expression”
dialog. For example, in Figure 4 the result of the query “all users who are assigned
to r2 but not to r1” applied to the given RBAC conﬁguration is shown. Here,
one can learn that u is not assigned to r1, although this is required by the
prerequisite role constraint. If the policy designer now conversely tries to assign
u to r1, the SSOD constraint fails, and one can conclude that both constraints
are contradictory. A policy designer could employ USE in a similar way for
other constraint types such as cardinality constraints or other SoD properties.
In particular, this approach is helpful if a new constraint is added to the policy,
in order to check if it is in conﬂict with the composition of the already deﬁned
constraints.
Nevertheless, USE may ﬁnd conﬂicts only in certain cases, and there is no
guarantee that all conﬂicts can be detected. Had u not been assigned to r2,
the conﬂict would have remained undetected. In order to eliminate contradictory constraints in general, a more formal approach such as model checking is
required. On the other hand, the USE approach is only meant to improve the
design of a security policy, and does not aim at a formally proven design. Given
the condition that there is often a lack of tools for policy analysis, the USE ap-

74

K. Sohr et al.

proach can be considered as a ﬁrst pratical step towards more reliable security
mechanisms.
However, various heuristics can be applied which may streamline the conﬂict
detection process with USE. For example, system states (snapshots) could be
created which are specially tailored towards certain constraint types. In particular, we could consider snapshots which satisfy the constraint in question and
which contain all the parameters (objects and links) occurring in this constraint
(cf. the system state in Figure 4 for the SSOD constraint). Such a snapshot can
then be taken as a starting point for the conﬂict detection process. Speciﬁcally,
we can check if this system state also adheres to the composition of the other
already deﬁned constraints. As a further improvement, we could store snapshot
templates for each constraint type (e.g., SSOD, prerequisite roles) and instantiate these templates for a certain constraint if needed. This way, a library with
snapshot templates is available, which can be reused and appropriately combined
with other snapshots to obtain test cases for conﬂict detection.
Detection of Missing Constraints. The second consequence of constraint
validation may be that the policy permits undesirable system states, i.e., the
authorisation constraints are too weak. Once again suppose that the policy designer has deﬁned a complex security policy. Let us further assume that she has
forgotten to deﬁne the SSOD part of the SSOD-CU constraint mentioned above
(cf. Figure 3) and that an undesirable system state has been created by USE in
which u is assigned to both the roles r1 and r2. Now, USE can help in detecting
the missing constraint in this scenario: all constraints (in our case speciﬁcally
the conﬂict user part of the SSOD-CU constraint) deﬁned so far are evaluated
to true and hence the policy seems supposedly to be correct. On the other hand,
the policy permits a user to be assigned to the mutually exclusive roles r1 and
r2. Therefore, a further SSOD constraint must be added to the policy in order
to exclude the undesirable state.
But how can we create a system state which reveals the missing constraint?
One possible solution is to create an RBAC conﬁguration tailored towards the
missing constraint as described in the previous section, but with the diﬀerence
that now snapshots must be considered that violate the missing constraint. Another possibility is to use the test generator provided by USE [11]. By means
of this generator we can create system states at random and then check if the
created system state violates certain conditions with the help of the “Evaluate
OCL expression” dialog.
4.2

Testing a Given RBAC Conﬁguration with USE

Beyond the validation of constraints, USE can be employed for testing an RBAC
conﬁguration after the constraints have been deployed. However, observe that
we consider here a predeﬁned RBAC conﬁguration of users, roles, etc. which
corresponds to a real-world RBAC conﬁguration of an organisation.
Testing an RBAC conﬁguration may be mandatory in several situations. For
example, in some domains (e.g., healthcare) strict data protection laws must
be fulﬁlled such as the European Directive 95/46/EC [7]. In order to assess

Speciﬁcation and Validation of Authorisation Constraints

75

the current RBAC conﬁguration deﬁned for security-relevant applications, often
some external review is required, e.g., from an government agency responsible for
data protection as established in Germany. What is often missing is a tool that
supports an external reviewer in checking a concrete RBAC conﬁguration of an
organisation against certain properties such as data protection rules. In addition,
the ability to test RBAC conﬁgurations may also be helpful for administrators
in order to check if a security policy has been implemented correctly.
USE can now be employed as an ad hoc query tool to check certain properties
of the current RBAC conﬁguration such as:
– there is no common user of mutually exclusive roles
– only clinicians of a patient’s current ward may have access to the patient’s
electronic patient record3
For this purpose, the “Evaluate OCL expression” dialog is helpful again. For
example, a reviewer can check the current RBAC conﬁguration if and which
users are assigned to the roles r1 and r2, which ought to be mutually exclusive.
Due to the fact that an administrator or external reviewer usually is not an
expert in speciﬁcation formalisms like OCL an authorisation editor should be
made available which hides the formalism behind a GUI. This is discussed in the
following.
4.3

Authorisation Editor

We have implemented an RBAC authorisation editor built upon the Java API
made available by the USE system. This way, the USE system is hidden from
the administrator and hence she need not be familar with UML/OCL and USE.
The authorisation editor can enforce several types of authorisation constraints
like those listed in [1]. More explicitly speaking, the authorisation editor can be
used in principle to specify and enforce all authorisation constraints expressible
in OCL. As a consequence, types of authorisation constraints beyond those enumerated in [1] can also be formulated and enforced such as context constraints.
In the following, the functionality of the authorisation editor will be presented
in more detail. First, the prototype of the authorisation editor currently supports
most of the functionality demanded by the ANSI standard for RBAC [3]. This
means that we have implemented administrative functions, system functions, and
review functions. According to [3] administrative functions allow the creation and
maintenance of the element sets (e.g., User, Role, Permission) and the RBAC
relations (e.g., UA, PA). For example, AddUser, DeleteUser, and AssignUser
belong to this class of functions. System functions are required by the authorisation editor for session management and making access control decisions. Thus,
examples are CreateSession, and AddActiveRole. Review functions allow for
reviewing the results of the actions created by administrative functions. Typical examples of review functions are AssignedUsers, and UserPermissions.
3

We assume here that there is a further attribute “ward” for certain roles and for
users.

76

K. Sohr et al.

Fig. 5. The authorisation editor

Administrative and system functions can be implemented by the state manipulation commands provided by the USE system. Due to the aforementioned query
facilities of USE, RBAC review functions can also be easily implemented.
Beyond this basic functionality, the RBAC authorisation editor provides
mechanisms for deﬁning and enforcing authorisation constraints (e.g., simple
static SoD, object-based static SoD, cardinality constraints). The basic idea of
the constraint checking mechanism is now as follows: The authorisation editor,
or to put it in another way, the USE system checks if the relevant authorisation
constraints are still satisﬁed after an administrative function has been carried
out. If any constraint is violated, the last function is automatically revoked. As
a consequence, the tool can only produce states that are consistent with the
speciﬁed constraints.
The authorisation editor can also deal with role hierarchies, which are not
restricted to inheritance trees, but can also in general form directed acyclic
graphs. Moreover, the tool can detect and then prevent inconsistencies such
as a senior role which inherits two mutually exclusive junior roles. For example,
assume we have a role Chair and two junior roles Reviewer and Author. Further
assume both junior roles are mutually exclusive. Then the role Chair is strictly
speaking useless because no user can ever be assigned to this role. To give a
better overview, a screenshot of the current prototype of the authorisation editor
is shown in Figure 5. In the upper part of the window, there are several buttons,
each button stands for a special administrative function. The large window in
the middle of the tool visualises the current system state (RBAC conﬁguration).
The visualisation of the system state will be immediately renewed when the
system state has been changed by an administrative function. At the bottom of

Speciﬁcation and Validation of Authorisation Constraints

77

the window there is a log window, which displays the result of the last applied
administrative function. There are currently two windows open: On the righthand side there is a small window to create a set of roles for a simple static SoD
constraint; on the left-hand side there is a window to create a session for a user
with active roles.

5

Related Work

There are several works concerning the speciﬁcation of RBAC authorisation constraints, e.g., a graphical language [13] and the RCL 2000 language based upon
restricted ﬁrst-order logic [1]. As demonstrated in [1], various classes of authorisation constraints can be expressed with RCL 2000. Although the classiﬁcation
and the case studies are insightful, no tool support for constraint validation, enforcement, and testing has been implemented so far. In [18] and [2], constraints
are formulated in UML/OCL, but once again no tool support for the validation
is available. In this respect, the USE approach ﬁlls this gap.
In [14,15], another approach for the veriﬁcation of RBAC policies is presented,
based upon graph transformations. However, this approach does not tackle the
problem of conﬂicting constraints, but the problem of graph rules conﬂicting
with constraints. Due to the fact that some constraints can only be expressed
clumsily (e.g., SSOD-CU, operational SoD) a formulation of those constraints in
OCL is often more intuitive.
In [6], an authorisation editor is presented which is similar to the one described in Section 4.3. However, with the approach from [6], for example, the
SSOD-CU constraint cannot be speciﬁed and enforced. On the other hand, with
USE no history-based SoD constraints can be enforced because TOCL is currently not supported.

6

Conclusion and Future Work

In this paper we demonstrated that with the help of OCL several classes of authorisation constraints and even complex composite constraints can be speciﬁed.
Due to the fact that the UML/OCL is quite familiar in industrial environments
there is hope that OCL can be used by policy designers in many organisations.
In addition, we demonstrated how the USE tool, a validation tool for OCL constraints, can be employed to fulﬁll several practical needs such as constraint
validation, testing of RBAC conﬁgurations and building an authorisation editor.
Owing to the fact that USE can only check the current snapshot of an RBAC
conﬁguration, history-based authorisation constraints [23] cannot be dealt with.
For this purpose a temporal extension of OCL like that sketched in this paper
is needed. Hence, it remains future work to extend USE in order to deal with
temporal constraints. Another goal is to integrate the authorisation editor into
middleware. Speciﬁcally, Web services could be an interesting target to enforce
authorisation constraints due to the high access control requirements of this
technology.

78

K. Sohr et al.

References
1. G.-J. Ahn, The RCL 2000 language for specifying role-based authorization constraints, Ph.D. thesis, George Mason University, Fairfax, Virginia, 1999.
2. G.-J. Ahn and M.E. Shin, Role-Based Authorization Constraints Speciﬁcation Using Object Constraint Language, Proc. of the 10th IEEE International Workshops
on Enabling Technologies: Infrastructure for Collaborative Enterprise, IEEE, 2001,
pp. 157–162.
3. American National Standards Institute Inc., Role Based Access Control, 2004,
ANSI-INCITS 359-2004.
4. R. Anderson, A security policy model for clinical information systems, Proceedings
of the IEEE Symposium on Research in Security and Privacy (Oakland, CA), IEEE
Computer Society Press, May 1996, pp. 30–43.
5. D. D. Clark and D. R. Wilson, A comparison of commercial and military computer
security policies, Proceedings of the 1987 IEEE Symposium on Security and Privacy
(1987), 184–194.
6. J. Crampton, Specifying and enforcing constraints in role-based access control,
Proc. of the 8th ACM Symposium on Access Control Models and Technologies
(New York), ACM Press, June 2–3 2003, pp. 43–50.
7. EU, Directive on the protection of individuals with regard to the processing of
personal data and on the free movement of such data. Directive 95/46/EC.
http://www.privacy.org/pi/intl orgs/ec/eudp.html, 1995.
8. D. Ferraiolo, D. Gilbert, and N. Lynch, An examination of federal and commercial
access control policy needs, Proc. of the NIST-NCSC Nat. (U.S.) Comp. Security
Conference, 1993, pp. 107–116.
9. D.F. Ferraiolo, D.R. Kuhn, and R. Chandramouli, Role-based access control, Artec
House, Boston, 2003.
10. V. D. Gligor, S. I. Gavrila, and D. Ferraiolo, On the formal deﬁnition of separationof-duty policies and their composition, 1998 IEEE Symposium on Security and
Privacy (SSP ’98), IEEE, May 1998, pp. 172–185.
11. M. Gogolla, J. Bohling, and M. Richters, Validation of UML and OCL Models
by Automatic Snapshot Generation, Proc. 6th Int. Conf. Uniﬁed Modeling Language (UML’2003), Springer, Berlin, LNCS 2863, 2003, pp. 265–279.
12. Martin Gogolla and Mark Richters, Transformation Rules for UML Class Diagrams, Proc. 1st Int. Workshop Uniﬁed Modeling Language (UML’98), Springer,
Berlin, LNCS 1618, 1999, pp. 92–106.
13. T. Jaeger and J.E. Tidswell, Practical safety in ﬂexible access control models, ACM
TISSEC 4 (2001), no. 2, 158–190.
14. M. Koch, L. V. Mancini, and F. Parisi-Presicce, A Graph Based Formalism for
RBAC, ACM Transactions on Information and System Security (TISSEC) 5
(2002), no. 3, 332–365.
15. M. Koch and F. Parisi-Presicce, Visual Speciﬁcation of Policies and their Veriﬁcation, Proc. of Fundamental Approaches to Software Engineering (FASE) 2003,
LNCS, no. 2621, Springer, 2003, pp. 278–293.
16. T. Mossakowski, M. Drouineaud, and K. Sohr, A temporal-logic extension of rolebased access control covering dynamic separation of duties, Proc. of TIME-ICTL
2003, Cairns, Queensland, Australia, July 8–10 2003.
17. M. J. Nash and K. R. Poland, Some conundrums concerning separation of duty,
Proc. IEEE Symposium on Research in Security and Privacy, 1990, pp. 201–207.

Speciﬁcation and Validation of Authorisation Constraints

79

18. I. Ray, N. Li, R. France, and D.-K. Kim, Using UML to visualize role-based access
control constraints, Proc. of the 9th ACM symposium on Access control models
and technologies, ACM Press New York, USA, 2004, pp. 115–124.
19. M. Richters, A Precise Approach to Validating UML Models and OCL Constraints,
Ph.D. thesis, Universität Bremen, Fachbereich Mathematik und Informatik, Logos
Verlag, Berlin, BISS Monographs, No. 14, 2002.
20. M. Richters and M. Gogolla, Validating UML Models and OCL Constraints,
Proc. 3rd Int. Conf. Uniﬁed Modeling Language (UML’2000), Springer, Berlin,
LNCS 1939, 2000, pp. 265–277.
21. J. Rumbaugh, I. Jacobson, and G. Booch, The Uniﬁed Modeling Language Reference Manual, Second Edition, Object Technology Series, Addison Wesley Longman,
Reading, Mass., 2004.
22. R.S. Sandhu, E.J. Coyne, H.L. Feinstein, and C.E. Youman, Role-based access
control models, Computer 29 (1996), no. 2, 38–47.
23. R. Simon and M. Zurko, Separation of duty in role-based environments, 10th IEEE
Computer Security Foundations Workshop (CSFW ’97), June 1997, pp. 183–194.
24. K. Sohr, M. Drouineaud, and G.-J. Ahn, Formal Speciﬁcation of Role-based Security
Policies for Clinical Information Systems, Santa Fe, New Mexico, Proc. of the 20th
ACM Symposium on Applied Computing, 2005, To appear.
25. J. Warmer and A. Kleppe, The Object Constraint Language: Getting your models
ready for MDA, Addison-Wesley, Reading/MA, 2003.
26. P. Ziemann and M. Gogolla, An OCL Extension for Formulating Temporal Constraints, Research Report 1/03, Universität Bremen, 2003.

A Universal Service-Semantics Description Language
Ajay Bansal, Srividya Kona, Luke Simon,
Ajay Mallya, and Gopal Gupt.a
Department of Computer Science
University of Texas at Dallas
Richardson, T X 75083

Abstract
For web-services to become pructdcal, an infrustructure needs to be supported that allo~ususers and applications to discover, deploy, compose, and synthesize
services automatically. This automation can take place
only if a formal description of the web-services is available. In this paper we present an infrastructure using
USDL (Universal Service-Semantics Description Language), a language for formally describing the semantics of web-services. USDL is based on the Web Ontology Language ( O W L ) and employs WordNet as a common basis for understanding the meaning of services.
USDL can be regarded as formal service documentation that ?udl allow sophisticated conceptual modeling
and searching of available web-services, automated service composition, and other forms of automated service integration. A theory of safe service substitution
for USDL is presented and proved sound and complete.
The rationale behind the design of USDL along with
its formal specification in O W L is presented with examples. We also compare USDL with other approaches
like 0 WL-S and WSML and show that USDL is complementary to these approaches.

1. Introduction
A web-service is a program available on a website
that "effects some action or change" in the world (i.e.,
causes a side-effect). Examples of such side-effects include a web-base being updated because of a plane
reservation made over the Internet, a device being controlled, etc. The next milestone in the Web's evohition is making services nbiquitousiy available. As automation increases, these web-services will be accessed
directly by the applications themselves rather than by
humans. In this context, a wcb-service can be regarded
as a "programmatic interface" that makes application

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Thomas D. Hite
Met,allect Corp
2400 Dallas Parkway
Plano, TX 75093

to application comml~nicationpossible. An infrastrncture that allows users to discover, deploy, synthesize
and compose services automatically needs t o be s u p
ported in order t o make web-services more practical.
T o make services ubiquitously available we need a
semantics-based approach such that applications can
reason about a service's capability t o a level of detail that permits their discovery, deployment, composition and synthesis. Several efforts are underway to
build such an infrastructure. These efforts include a p
proaches based on the semantic web (such as OWL-S
[ 5 ] ) as well as those based on XML, such as Web Setvices Description Language (WSDL [7]). Approaches
such as WSDL are purely syntactic in nature, that is,
they merely specify the format of the service. In this
paper we present an approach that is based on semantics. Our approach can be regarded as providing semantics t o WSDL statements. We present the design
of a language called Universal Service-Semantics Description Language (USDL) which service developers
can use t o specify formal semantics of web-services [14].
Thus, if WSDL can be regarded as a language for formally specifying the syntax of web services, USDL can
be regarded as a language for formally specifying their
semantics. USDL can be thought of as formal servzce
documentatzon that will allow sophisticated conceptual
modeling and searching of available web-services, automated composition, and other forms of automated
service integration. For example, the WSDL syntax
and USDL semantics of web services can be published
in a directory which applications can access to automatically discover services. T h a t is, given a formal
description of the context in which a service is needed,
the service(s) that will precisely fulfill that need can
be determined. The directory can then be searched for
the exact service, or two or more services that can be
composed to synthesize the required service, etc.
To provide formal semantics, a common denominator must be agreed upon that everybody can use as a

basis of understanding the meaning of services. This
common conceptual ground must also bc somewhat
coarse-grained so as to be tractable for use by both engineers and compl~tcrs.That is, semantics of services
should not be given in terms of low-level concepts such
as Tnring machines, first-order logic and their variants, since service description, discovery, and synthesis
then become tasks that are practically intractable and
theoretically undecidable. Additionally, the semantics
should be given at a concept~iallevel that captures
common real world concepts. Furthermore, it is too impractical to expect disparate companies t o standardize
on application (or domain) specific ontologies to formally define semantics of web-services, and instead a
common nniversal ontology must be agreed npon with
additional constrlictors. Also, application specific ontologies will be an impediment t o automatic discovery
of services since the application developer will have to
be aware of the specific ontology that has been used to
describe the semantics of the service in order to frame
the query that will search for the service. The danger
is that the service may not be defined wing the particnlar domain specific ontology that the application
developer uses to frame the clnery, however, it may be
defined using some other domain specific ontology, and
so the application developer will be prevented from discovering the service even though it exists. These reasons make an ontology based on OWL WordNet [2, 81
a suitable candidate for a universal ontology of atomic'
concepts upon which arbitrary meets and joins can be
added in order to gain tractable flexibility.
We describe the meaning of conceptual modeling
and how it could be obtained via a common universal
ontology based on WordNet in the next section. Section 3, gives a brief overview of how USDL attempts
to semantically describe web-services. In section 4, we
discuss precisely how a WSDL document can be prescribed meaning in terms of WordNet ontology. Section 5 gives a complete USDL annotation for a BookBuying service. Alltomatic discovery and composition
of web-services using USDL is disclissed in section 6.
In section 7 we explore some of the theoretical aspects
of service description in USDL. Comparison of USDL
with other approaches like OWL-S and WSML is discllssed in section 8. Finally, conclusions and fiiture
work are addressed in the last section.

2. A Universal Ontology
To describe service semantics, we should agree on a
common ground to model our concepts. We can describe what any given web-service does from first principles using approaches based on logic. This is the

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

approach taken by frameworks such as dependent type
systcms and programming logics prevalent in the field
of software verification where a "formal understanding" of the service/software is needed in order to verify
it. However, such sollitions are both low-level, tedious,
and nndecidable to be of practical use. Instead, we are
interested in modeling higher-level concepts. That is,
we are more interested in answering questions such as,
what does a service do from the end user's or service
integrator's perspective, as opposed to the far more
difficult questions, such as, what does the service do
from a computational view? We care more about real
world concepts such as "customer" , "bank account",
and "flight itinerary" as opposed to the data strlictiires and algorithms used by a service t o model these
concepts. The distinction is subtle, but is a distinct>ion
of granularity as well as a distinction of scope.
In order to allow interoperability and machinereadability of our documents, a common conceptual
ground must be agreed upon. The first step towards
this common ground are standard languages such as
WSDL and OWL. However, these do not go far enough,
as for any given type of service there are numerous distinct representations in WSDL and for high-level concepts (e.g., a ternary predicate), there are nnmerous
disparate representations in terms of OWL, representations that are distinct in terms of OWL'S formal semantics, yet equal in the actual concepts they model. This
is known as the semantic aliasing problem: distinct
syntactic representations with distinct formal semantics yet equal conceptual semantics. For the semantics t o equate things that are conceptually equal, we
need to standardize a sufficiently comprehensive set of
atomic concepts, i.e., a universal ontology, along with
a restricted set of connectives.
Industry specific ontologies along with OWL can
also be used to formally describe web-services. This
is the approach taken by the OWL-S language 151. The
problem with this approach is that it requires standardization and undne foresight. Standardization is a slow,
bitter process, and industry specific ontologies would
require this process to be iterated for each specific industry. Furthermore, reaching a industry specific standard ontology that is comprehensive and free of semantic aliasing is even more difficult. Undue foresight
is required because many useful web services will address innovative applications and industries that don't
currently exist. Standardizing an ontology for travel
and finances is easy, as these industries are well established, but new innovative services in new upcoming
industries also need be ascribed formal meaning. A
universal ontology will have no difficlilty in describing
such new services.

\Ye need an ontology that is somewhat coarsegrained yet, ~~niversal,
and at a similar conceptual
level to common rcal world concepts. Currently there
is only one sufficiently comprehensive ontology that
meets these criteria: WordNet [8]. As stated, part of
the common ground involves standardized languages
snch as OWL. For this reason, WordNet cannot be used
directly, and instead we make use of an encoding of
IVordNct as an OWI, base ontology [2]. Using an OWL
WordNet ontology allows for our solution to use a universal, complct,e, and tractable framework, which lacks
the semantic aliasing problem, to which we map web
service mcssages and operations. As long as this m a p
ping is precise and sn ficiently expressive, reasoning can
be done within the realm of OWL by using automated
inference systems (s~lcllas, one based on description
logic), and we antomatically reap the wealth of semantic information embodied in the OWL WordNet ontology that describes the relationships between ontological concepts, cspeciall y subsumption (hyponym) and
ecluivalencc (synonym) relationships.

3. USDL: An Overview
As mentioned earlier, USDL can be regarded as
a langnagc to formally specify the semantics of webservices. It is perhaps the first attempt, to capture the
semantics of web-services in a universal, yet decidable
manner. It, is quite distinct from previous approaches
such as WSDI, and OW1,-S [5]. As mentioned earlier,
WSDT, only defines syntax of the service; USDT, provides the missing semantic component. USDL can be
thought of as a formal langnage for service documentation. Thus, instead of docnmenting the function of
a service as comments in English, one can write USDT,
statements that describe the function of that service.
USDL is quite distinct from OWL-S, which is designed
for a similar purpose, and as we shall see the two are
in fact complement,ary. OWL-S primarily describes the
states that exist before and after the service and how
a service is composed of other smaller s~ihservices(if
any). Description of atomic services is left undcrspecified in OWJ,-S. They have to be specified using domain
specific ontologies; in contrast, atomic services are complet,ely specified in XiSDT,, and USDT, relics on a nniversa1 ontology (OWL FVordNet Ontology) to specify
the semantics of atomic services. USDT, and OW1,-S
are complementary in that OWL-S's strength lies in describing the structure of composite services, i.e., how
various atomic services are algorithmically combined
to prodnce a new service, while USDL is good for fully
describing atomic services. Thus, OWL-S can be used
for describing the s t r ~ ~ c t of
~ ~composite
re
services that,

combine atomic services described using USDI,.
USDL describes a service in terms of portType and
messages, similar to WSDT,. The semantics of a service
is given wing the OWL WordiXet ontology: portType
(operations provided by the service) and messages ( o p
eration parameters) are mapped to disjnnctions of conjunctions of (possibly negated) concepts in thc OWL
IVordNet ontology. The semantics is given in terms of
how a service affects the external world. The present
design of USDL asslimes that each side-effect is one
of following four operations: create, update, delete, or
find. A generic affects side-effect is nsed when none of
the four apply. An application that wishes to make use
l d able to reason with
of a service antomatically s h o ~ ~be
WordNet atoms using the OWL IVordNet ontology.
We also define the formal semantics of USDT,. As
stated earlier, the syntactic terms describing portType
and messages are mapped to disjunctions of conjunctions of (possibly negated) OWL WordNet ontological
terms which can be represented by points in the lattice
obtained from the WordNet ontology with regards to
the OWT, subsumption relation. A service is then formally defined as a function, labeled with zero or more
side-effects, between points in this lattice. The main
contribution of our work is the design of a llnivcrsal
service-semantics description language (USDL), along
with its formal semantics, and soundness and completeness proofs for a theory of safe service substitution.

4. Design of USDL
The design of USDI, rests on two formal languages:
Web Services Description Language (WSDL) [7] and
Web Ontology Language (OWL) [6]. The Web Services Description Language (WSDL) [7], is used to give
a syntactic description of the name and parameters of
a service. The description is syntactic in the sense that
it describes the formatting of services on a syntactic
level of method signatures, but is incapable of describing what concepts are involved in a service and what
a service actlially does, i t . , the conceptl~alsemantics
of the service. T,ikewise, the Web Ontology T,anguage
(OWL) [6], was developed as an extension to the Resource Description Framework (RDF) [3], both standards are designed to allow formal conceptlial modeling
via logical ontologies, and these languages also allow for
the markup of existing web resources with semantic
information from the conceptual models. USDL employs WSDI, and OWL in order to describe the syntax
and semantics of web-services. WSDL is used to describe message formats, types, and method prototypes,
while a specialized ~lniversalOWT, ontology is used to
formally describe what these messages and methods

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

,

mean, on a conccpt,~lallcvcl .
USDI, can be regarded as the semantic counterpart,
t,o the syntactic WSDI, description. WSDT, documents
contain two main c o n s t n ~ c t to
, ~ which we want to ascribe conccpt,l~al meaning: messages and portType.
Thcsc constructs arc aggregates of scrvlcc components
whlch w ~ lbe
l directly ascrlhed meanlng Messages cons ~ s of
t typed parts and portType consists of operations
parameterized on messages USDI, defines OWL surrogates or proxles of thtsc constrl~ctsIn the form of
clnsscs, wh~chhave properties wlth values In thc OIVT,
IVordNct ontology

4.1

4.1.1

Atomic Concept

AtomicConcept is the act,llal contact point bctu~ecn
USDT, and IVordNet. This class acts as proxy for
WordKet lexical entities.

Concept

USDL defines a generic class called Conxept which
is used to define thc semantics of parts of messages. Semantically, instances of Con,cept form a complete lattice, which will be covered in section 7.
<owl:Class rdf:ID="Concept">
<rdfs:comment>
Generic class o f USDL Concept
</rdfs:comment>
<oul:unionOf rdf:parseType="Collection">
< o w l ~ C l a s srdf:about="#AtomicConcept"/>
<owl:Class rdf:about="#InvertedConcept"/)
<oal:Class rdf:about="#ConjunctiveConcept"/)
<owl:Class rdf:about="1DisjunctiveConcept"/>
</owl:unionOf>

<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="#ofKind"/>
<owl:mincardinality
rdf :datatype="&xsd;nonBegativeInteger">
0
</owl:mincardinality>
</oul:Restriction>
</rdfs:subClassOf>

The property cardinality restrictions require all
USDL AtomicCoricepts t o have exactly one defining
value for the is,4 property, and zero or more vallies for
the ofKind property. An instance of iltomicConcept
is considered t o be equated with a WordNet lexical
concept given by the i s 4 property and classified by a
lexical concept given by the optional ofliind property.

4.1.2

The USDL Concept class denotes t,hc top element, of
the lattice of concept~ialobjects constn~ctedfrom the
OWT, IVordNct ontology. For most pnrpo.ws, message
parts and other WSDT, constrlicts will he mapped t o a
sl~bclassof USDT, Concept so that useft11 concepts can
be modeled as set thcorct,ic formlllas of llnion, intersection, and negation of atomic concepts. These snbclasses of Concept are iltomicConccpt, InuertcdConcept, C,'or~junctivcConccpt,and DisjunctducConcept.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Inverted Concept

In case of InvertedConcept the corresponding semantics
arc the complement of WordNct lexical entities.

4.1.3

C o n j u n c t i v e and D i s j u n c t i v e C o n c e p t

The Con.junctiveConcept and Disjunctit~cConcept respectively denote the intersection and union of cSDI,
Concepts.
Note that each of these specializations inherits the
domain and range of the affects property. Most services can be described as a conjl~nctionof these types of
effects. For those services that cannot be described in
terms of a combination of these specializations, the parent affects property can be used instead, or the p r o p
erty can be omitted entirely when the meaning of the
operation parameter messages are enough for concep
tnal reasoning. The purpose of limiting the types of
services as opposed t o allowing the creation of new arbitrary side-effect types, for cxample via OWL-DL, is
to: (i) make USDI, morc structured and therefore rasier t o create docliments in, (ii) make USDI, compiltationally more tractable for programs that process large
volumes of USDI, documents, and (iii) help prcvcnt the
semantic aliasing problem mentioned in section 2.

4.3 Conditions and Constraints

The property cardinality restrictions on Conjunctit:cConccpt and DisjtinctiveConcept allow for n,-ary
intersections and iinions (where n
2) of USDI, concepts. For generality, these concepts are either AtomicConcepts, Conj~~nctiveConccpts,
DisjurictiveConcepts,
or InvcrtcdConccpts .

>

Services may have some extmnal conditions (preconditions and post-conditions) specified on the input or outpnt parameters. Condition class is used
to describe all such constraints. Conditions are r e p
resented as conjunction or disjlinction of binary predicates. Predicate is a trait or aspect of the resource
being described.

4.2 Affects
Thc r~ffectsproperty is specialized into fonr types
of actions common to enterprise services: creates, updates, delctes, and finds.
<owl:ObjectProperty rdf:ID="affectsW>
<rdfs:comment>
Generic class of USDL Affects
</rdfs:comment>
<rdfs:domain rdf:resource="#Operation"/>

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

<owl:Class rdf:ID="Condition">
<rdfs:comment>
Generic class of USDL Condition
</rdfs:comment>
<owl:unionOf rdf:parseType="Collection">
<awl:Class
rdf:about="#AtomicCondition"/)
<owl :Class
rdf:about="#ConjunctiveCondition"/)
<owl:Class
rdf:about="#DisjunctiveCondition"/>
</owl:unionOf>

<onl:ObjectProperty rdf:ID="hasCondition">
<rdfs :domain rdf :resource="#Concept"/>
<rdfs:range rdf:resource="#Condition"/)
</owl:ObjectProperty>

The property cardinality restrictions on ConjunctiveConditioiz and DisjfinctiveCondition allow for n,ary conjllnctions and disjllnctions (where n
2) of
USDT, conditions. In general any n,-ary condition can
be written as a combination of conjl~nctionsand disjunctions of binary conditions.

>

4.4

A condition has exactly one value for the onPart
property and at most one valllc for the ha3 Valur property, each of which is of type USDL Concept
<owl:ObjectProperty rdf:ID="onPart">
<rdfs:domain rdf:resource="#AtornicCondition"/>
<rdf s :range rdf :resource="#Concept"/>
</owl:ObjectProperty>
<owl : ObjectProperty rdf :ID="hasValue0'>
<rdfs:domain rdf:resource="#AtomicCondition"/)
<rdfs:range rdf:resource="#Concept"/>
</owl:ObjectProperty>

4.3.1

Conjlinctive a n d Disjunctive Conditions

The Con,.i~~n,ctiireCondition
and Disjt~nctiveCondition
respectively denote the conjilnct,ion and disjunction of
USDL Conditions.

Messages

Services commnnicat,c by exchanging messages. As
mentioned, messages are simple tuples of actual data,
called parts. Take for example, a flight reservation service similar to thc SAP ABAP Workbench Interface
Repository for flight reservations [4], which makes use
of the following message.
(message name="&flight;ReserveFlight-Request?
<part name="&flight;CustomerUame" type="xsd:stringU>
<part name="&flight;FlightBumber" type="xsd:string">
<part nme="&flight;DepartureDate" type="xsd:date">

T h e USDL surrogate for a WSDL message is the
Message class, which is a composite entity with zero
or more parts. Note that for generality, messages are
allowed t o contain zero parts.
<owl:Class rdf:abaut="#lessageU>
<rdfs:subClassOf>
<onl:Restriction>
<owl :onproperty rdf :resource="PhasPart"/>
<owl:minCardinality
rdf :datatype="&xsd;nonlegativeZnteger">
0

</owl:minCardinality>
</owl:Restriction>
</rdfs:subClassOf>
</oal:Class>

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Each part of a nicssagc 1s simply a VSDT, Conccpf,
as defined by t,hc hasPrart propcrty. Scmantically mcssages are trcatcd as t,nplcs of concepts.

Continning onr example flight, reservation service,
the Itincrarv message is given scniantics using USDT, as
follows, where &wn ;customer and &wn;name are valid
XMT, rcfercnccs to IVordNet lexical concepts.

<owl:ObjectProperty rdf:ID="hasOperation">
<rdfs :domain rdf :resource="#portType"/>
<rdfs:range rdf:resource="#Operation"/>
</owl:ObjectProperty>

As with the case of messages, portTypes are not directly assigned meaning via t'he OWL IliordIL'et, ontology. Instead tho individual Operations are described by
their side-effects via an r~flectsproperty. Note that the
parameters of an operation are already given meaning
by ascribing meaning t o the messages that constitute
the parameters.

A scrvicc consists of portTypc, which is a collecor operations that arc parametric
tion of proced~~rcs
on messages. Our example flight rcscrvation scrvice
might contain a p o r t T w c definition for a flight reservation scrvicc that takes as input an itinerary and outputs a rcscrvation receipt.

An operation can have mnltiplc or no values for the
a f i c t s property, all of which are of type USDL Concept,
which is the target, of the effect.

Finishing our flight reservation scrvice cxamplc, we
can describe the main side-effect of invoking the ReserucFlzght operation in USDL as follows.
The USDL s ~ ~ r r o g ais
t cdcfincd as the class portType
which contains zero or more O p c m t l o n s as valllcsof the
hasOpcratzon property.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

Again, it, is not necessary t o annotate the operation
with regards to its input and out,put parameters, a..
t,hcse arc already annot,atcd by ~brcssc~gc
surrogates.

5. Semantic Description of a Service
This scction shows an cxample syntactic description
of a wcb-scrvicc nsing IVSDT, and its corresponding
scmantic dpscription Iislng ITSDT,.
A simple Book-Buying Service: T h e service dcscribed here is a simplifietl book-buying service published in a wehscrvic-c registry. This service can be
treated as atomic: i.c., no interactlions between buying
and selling agents are rcqiiired, apart from invocation
of t,hc service and rcceipt of it,s olitp~ltsby the buyer.
Given certhin inp~ltsand pre-conditions, the servicc
provides certain o ~ i t p u t sand has spccific effects.
This service takcs in a BooklSR,V, U~serldentifier,
and Poss,tuord as input parameters. It has an input,
prc-condition that a Ltscrldentificr for the buyer must
cxist before invoking the service. It also has a global
e r the buyer
constraint that, a valid credit card n ~ ~ m bfor
mlist exist. This service o11tp11tsan OrderNumber if the
In case the book is not
order was placed s~lcccssfi~lly.
available, the o i ~ t p ~isi t an "Out of Stock" mcssagc.

5.1 WSDL definition
The following is WSDT, definition of t,hc service.
This service provides a single operation called RookBuying. The inpnt and o u t p ~ i tmessagcs arc defincd
below. The conditions on the service cannot be described using WSDL.

<portType name="BookBuying-Serviceu>
(operation name="BookBuying">
<input message="BuyBook-Request"/>
<output message="BuyBook-Response"/>

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

5.2 USDL annotation
The following is the completc USDL annotation corresponding t o the above mentioned WSDL description.
The input pre-condition and the global constraint on
the service are also described semantically.
<definitions>

...

<portType rdf:about="#BookBuying_Service">
<hasoperation>
<operation name="BuyBook">
<input message="BuyBook-Request"/)
<output message="BuyBook-Response"/>
<creates>
<Atomicconcept rdf:about="#BookOrder">
<isA rdf:resource="&wn;order"/>
<ofKind>
<Atomicconcept>
<isA rdf:resource="&an;book"/>
</AtomicConcept>
</of Kind>
<hascondition>
(Condition rdf:about="#exists">
<Atomicconcept>
<isA rdf:resource="&wn;exists"/>
</AtomicConcept>
<onpart rdf :about="#CreditCard">
<Atomicconcept>
<isA rdf:resource="&nn;card"/>
<ofKind>
<Atomicconcept>
<isA rdf:resource="&wn;credit"/>
</AtornicConcept>
</ofKind>
</Atomicconcept>
</onpart>
</Condition>
</hascondition>
</Atomicconcept>
</creates>
</operation>
</hasoperation>
</portType>

6. Service Discovery and Composition
<isA rdf:resource="&an;identifier"/>
<of Kind>
<Atomicconcept>

<haspart>
<Atomicconcept rdf:about="#UserIdentifierU>
<isA rdf:resource="&wn;identifier"/>
<ofKind>
<AtomicConcept>

</haspart>

<DisjunctiveConcept

rdf:about="#OrderEurnber/Availability">
<hasconcept>
<Atomicconcept rdf:about="#Orderkunber">
<isA rdf:resource="&an;number"/>
<of Kind>

Note that, given a directory of services, a USDT, description conld bc inclllded for each service, making
the servicc directly "semantically" searchable. However, we still need a query language t o scarch this directory, i . ~ . .wc
, need a langnage t o frame the recll~iremcnts on the service t h a t an application developer is
seeking. Note that USDT, itself collld be such a query
language. A USDI, description of the dcsired service
can be writken, a clllcry processor can then search the
service directory to look for a "matching" service. For
matching we could treat USDT, descriptions as well
the USDI, query as terms, and perhaps nse some kind
of cxtendcd nnification t o chtck for a match. This work
is current.ly in progress [lo].
With tjhe USDT, descriptions and query language
in place, nllmerous applications become possible ranging from querying a database of services t o rapid a p
plication dcvclopmcnt via automated integration tools
and even real-time servicc c ~ m p o s i t ~ i o[In21. Take our
flight reservation service example. Assume that somebody wants to find a travcl reservation service and
that they qllcry a USDT, database containing gcneral
purpose fl~ghtrescrvation servlccs, bns reservation services, etc One could then form a USDL query conslstIng of a descrtpt~onof a travel reservat~onservice and
ld
with a sct of travel reserthe databasc c o ~ ~ respond
vation services whether it be via flight, bus, or some
other means of travcl. This flexibility of generalization
and specialization is gained from USDL's sllbsnmption
relation covered in section 7.
Furthermore, with a pool of USDI, services a t one's
disposal, rapid application development (RAD) tools
could be used to aid a systems integrator with the task
of creating composite services, i.c., services consisting
of the composition of already existing services. T h e
servicc designer collld 11sesuch a RAD tool by descrih
ing the desired service via a USDI, document,, and then
the tool would cluery the pool of services for composable sets of services that, can be 11scd t o accomplish the
task as well as automatically generate boilerplate code
for managing the composite service, as well as menial
inter-service d a t a format conversions and other glue.
Of course these additional RAD steps wo111d require
[13].
other technologies already being iii~estigat~ed
At present, we prcsnme only four specific opcrations (find, create, delete, update); this set of basic operations may be extended as more experience is
gained with VSDI,. In practice, most services, however, deal with manipnlating databases and for such
services these four operations s~~ffice.
As stated, one of
the reasons for limiting the side-effect types is t o safe-

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

gl~ardagainst, t,hc semantic aliasing problem described
in scct,ion 2. This is also one of t,hc main reasons for rest,ricting the combining forms in 'CTSDT, t,o conjunction,
disjnnction, and negated atoms. As disc~lsscd nest,,
this allows UST)T, descriptions to be put into a type of
disjnnctivc normal form (DNF), from which a sollnrl
and complet,~notion of snbsnmption is created.

Now that the formal definitions of concept and scrvice de~cript~ions
are given, we wonld likc t o define a
snbsumption relat,ion SZ:over C so that we can r c a s o ~ ~
,
this will, in turn,
about s~~hstit~lt,ability
of S C ~ V ~ C C Sbut
recluirc a sl~bsnmptionrelation
over O. The proof
of correctness of <c is covered by the principle of safc
sl~bst~itll
tion bclow.

7. Theory of Substitution of Services

Definition 5. As.su,rning ,tuithout loss of generality that
011 concepts are e.rpressed i n DNF, let
be the ordcring relation defined on concepts such that:

Next, we will in~cstigat~c
the theorct,ical aspects of
USDT,. This involves conccpts from set theory, lat,t,ice
theory, and typc thcory. From a systems integration
perspective, an engineer is intcrestcd in finding (discovering) a service that accomplishes some necessary
task. Of conrsc, snch ;I service may not be present
in any service directory. Tn such a casc the discovery soft,warc should return a sct of services that can
be used in a contcxt cspcct,ing a service that, meets
that description (of c o ~ ~ r sthis
e , set may be empty). To
find scrviccs that can be snbstit,~~ted
for a given service
that is not present in the directory, we need to develop
a theory of scrvice .substitutability. \Vc develop stlch a
theory in this section, and show its sonndness and completeness. Onr thcory relates service substitntabilit,y to
\?VordNct,'s snbsumption relation.
Tn order t,o develop this thcory, we must first formally define const,n~ctssuch as USDL-described concepts and scrviccs, which we will also call concepts and
servicc.~for short. While it is possiblc t o work dircctly
wit,h the SMT, USDI, syntax, doing so is cumbcrsomc
and so we will instead opt for sct theoret,ic notation.

Definition 1. Let 0 be the set of VVordNet entities
and <a
- be the OM'I, .stihsnrnption relation o n 0.
Definition 2. Let O be the least set of concepts s.t.:
,
:E implies E , 7.1: E O
2. z , y E O implies n: U y, z n y E O
Hence 0 is simply the set of USDT, concepts.
1.

2

Definition 3. Lrf r = { ( L , E ) I L E Q, F: E
O} be thc .set of USDL sidc-cfJccts, ~uhere Q =
{ c r c n t c s ,,trpdntrs, delctcs, finds), L is the affect type
and E is thc affected object.
Definition 4. For any sct S , lct

1. x i e y f o r x , y E R and x < n y
2. l x So ~y if and only if y Ln I for

Uvi u ~ i

2,

Uvj x"jf

and only if for all wj =
zl such that for
every q there exists some yk Ina.
Item 3 essentially extends the subsl~mptionrclation to
conccpts expressed in DNF.

3.

nVkyk there exists some xj = nvl
<@
-

Definition 6. I,et ( A , I , 0)5 ~(A',
: I', 0')if and only
if V ( L , E ) E A . 3 ( L , E') E A ' . E
E' and
I' So* I and 0 l o * 0',where <o.
is
the
elementwise e.tten.sion of So t o lists of concepts.

sO)

Note that (0,
and ( 3 ,Lc) respectively form
a complete lattice. Given a description of a service
cr E C, we can now define the set of s ~ ~ b s t i t l ~ t a b l e s
C'(rz),which in practice corresponds t o a query against
a database of services C, for services that satisfy dcscription a. Notice that t,he definition is contravariant
~ t s contravari.
for inputs and covariant, for o ~ ~ t p ~This
ance is also seen in the field of typc theory with regards
g , will be covered in the
to polymorphic s ~ ~ b t y p i n and
proof of thc principle of safe substitution below.

Definition 7. Let C be a function parametric over
services, 7ohich maps X t o a sribset of services such
that C ( a ) = {a'] a' Sc a).
s comTn order t o be able to prove the s o ~ ~ n d n e sand
~ l c t ~ e n e sofs C', we need a lemma known as the "Principle of Safe Snbstitt~t,ion."

Lemma 1. For a n y services a,a' E X, if a <_r, a' then
a can sc~felybe used i n a context expecting scrvicc a'.
Theorem 1. (Soundness of Sub.stztutab1e.s) For any
services (T and a', z f servzce a' can not be safelv 11sed
in o context expecting service a then a' # C(a), that is
the set of .silbstitt~tahlesfor a service does not contain
a n y incompatible scraiccs,
"

S* = { c l c $ S ) ~ S ~ { ( x , Iyx), : S S , y ~ S * }
he the set of 1i.st.s over S . Iiet C = ((4,I)O)I A E
2r, I E 0 " , 0 E O* } be the set of USDI, serr~iccdcscriptions, ~i:hcreA is the set of side-effects, I is the

E0

"

Proof. Assume the existence of services a and a' SIICII
list of input pnramctcrs, ond 0 is the list of o t ~ t p t ~ t that a' can not, be safely used in a contcxt expecting
service a. By the principle of safe s n b s t i t ~ ~ t i oitn is not,
por(~m.ctersof (I particular .seruice.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

true that a' S s a , a n t i hcncc by t,hc principle of ext,ensionality, a' # C((a). Th(v-cforc the set of snbstitutablcs
only cont,ains correct,, i.c., compatible scrvices.
Theorem 2. (C0~m~1ctcnc.s.s
of Substitutables) For any
service a , if thcrc c.rists a scrziicr (TI
that can safely be
ilsed in a coiite.rt c.rpectiny scrvicc a then cr' E C(cr),
that is thc set of suh.stitutab1e.s contains all ser1:iccs
compotiblc ~r:itlto yi11cn dc.scription.
Proof. Assnmc thcrc cxist,s snch a service n', then by
(T' S c a . So by the
the principle of safe s~lbst,it,~~t,ion
, therefore the set of
definition of Gq(a),n' E C ( ( T )and
0
s~~bstitntablcs
is complct,e for arbitrary (T E 3 .

The sollndncss and complet,cness theorems essentially state that, the WorclNet snbs~lmption relation
provides a strong enollgh fo~lndationfor safe servicc
snbstitt~tability.Note that this theory is quite general,
and is cven applicable to scrvicas described wing domain spccific ontologies (as in OW1,-S and WSML).

8. Comparison with OWL-S and WSML
In this section we present, a comparison of USDL
with othcr popnlar approaches such as OWL-S [5] and
WSMT, [I]. Our goal is t,o identify the similarities and
differences of LTSDI, with t,hese approaches. OWT,-S is
a scrvicc dcscription l a n g ~ ~ a gwhich
c
attempts to address the problcm of semantic description via a highly
detailed scrvicc ontology. Rnt OW1,-S also allows for
complicated combining forms, which seem to defeat, the
tractability and pract,icalit,y of O\VT,-S. The focns in
the design of O\VT,-S is to describe t,he structure of a
scrvicc in terms of how it combincs othcr slihservices
(if any nscd). The dcscription of atomic services in
OWTI-S is left nnderspccificd [9]. OW1,-S inclndes the
tags presents to dcscribc the scrvice profile, and the tag
describedRy t,o dcscribc the scrcicc m.odcl. The profile
describes the (possibly conditional) states that exist
before and after the scrvicc is esccnted. The service
model describes how the scrvicc is (algorithmically)
constructed from ot,her simpler scrvices. What the serly
has to be inferred from these
vice a c t ~ ~ a laccomplishes
two descriptions in OWT,-S. Given that OWL-S uses
complicated combining forms, inferring thc task that
a servicc actnally performs is, in general, nndecidable.
In contrast, in USDT,, what, the scrvice actually docs is
directly described (via the verb affects and its refinements create, update, dcletc, and find).
OW[,-S recommends that atomic services be defined
using domain specific ont,ologics. Thus, OWL-S needs
~lscrsdescribing the scrvices and ~lscrsusing the services to know, ~~ndcrst,and
and agree on domain specific

ontologies in which the scrvices arc described. Hence,
annot2ating scrvices with OW1,-S is a very timc consnming, cumbersome, and invasive process. The complicated natnrc of OWL-S's combining forms, cspccially conditions and control constructs, seems to alsemantic aliasing problem
low for the aforem~nt~ioned
[9]. Other recent approxhes such as WSMO, WSMT,,
etc., snffer from the same limitation [I]. In contrast,
USDL nscs the nnivcrsal IVordNet ontology t,o solve
this problem.
Note that USDT, and OW1,-S can be used together.
A USDT, description can be placed under the dcscribcdBy tag for atomic processes, while OWL-S can
be nsed to compose atomic USDI, services. Thus,
USDL along with JVordNet can be treated as the universal ontology that can make an OWL-S dcscrip
tion complete. USDT, documents can be nsed to describe the semantics of atomic services that Owl,-S
assumes will be described by domain specific ontologies and pointed to by the OWL-S describedRy tag. In
this respect, USDL and OWL-S are complementary:
USDL can be treated as an extension to OWL-S which
makes OW1,-S description easy to write and scmantically more complete.
OWL-S can also be regarded as the composition langnage for USDL. If a new service can be built, by composing a few already existing services, then this new
servicc can be described in OWL-S nsing the USDI,
descriptions of the existing services. Next, this new
scrvice can be al~tomaticallygenerated from its Owl,S dcscription. The control constructs like Scyucncc and
If-Then-Else of OWL-S allows us t o achieve this. Note
once a composite service has been defined using OWLS that uses atomic services described in USDT,, a new
USDL dcscription mnst be written for this composite
servicc (automatic generation of this description is currently being investigated [lo]). This USDL description
is the formal documentation of the new composite service and will make it antomatically searchable oncc the
new service is placed in the directory service. It also
allows this composite service to be treated as an atomic
service by some other application.
For example, the aforementioned ReserueFlight scrvice which creates a flight reservation can be viewed as
a composite process of first getting the flight details,
then checking the flight availability and then booking
the flight (creating the reservation). If we have these
three atomic services namely GetFlightDetails, CheckFlight.4 uailability and RookFlight then we can create
our Reserl~cFlightservice by composing these three services in sequence 11singthe OWL-S Scgr~cnceconstruct.
The following is the OWL-S description of t,hc composed Reser~teFltghtservicc.

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

<rdf :RDF
xmlns : rdf="http: //www.w3. org/1999
/02/22-rdf-syntax-ns#"
xmlns:process="http://nww.daml.org/services
/owl-s/l.O/Process.owl#">
<process:CompositeProcess rdf:ID="ReserveFlightU>
<process:composedOf>
<process:Sequence>
<process:components
rdf:parseType="Collection">
<process:AtomicProcess
rdf:about="#GetFlightDetails"/>
<process:AtomicProcess
rdf:about="#CheckFlightAvailability"/>
<process:AtomicProcess
rdf:about="#BookFlight"/>
</process:components>
</process:Sequence>

\Vc can generate this composed Rescr7icFlight service ant,omat,ically by irsing the USDT, descriptions of
the component scrviccs for discovering them from the
misting scrviccs. Once we have the component services, t11c OWTI-S description can be used to generate
the new composcd service.
Another rclatcrl area of research involves message
conversation constraints, also known as behavioral signatures [I:3]. Behavior signature models d o not stray
far from the explicit description of the lexical form
of mcssagcs, they cxpect, the mcssagcs t o be Iexically and semant,ically correct, prior to verification
via model checking. Hcncc behavior signatures deal
wit>h low-lcvcl fi~nctionalimplementation constraints,
while USDL deals with higher-level real world concepts. However, USDT, and behavioral signatnres can
be regarded as complemcnt,ary concepts when takcn in
the context, of real world service composition and both
technologies arc currently bcing nscd i n the devclopmcnt, of a commercial scrviccs intcgrat,ion tool [12].

9. Coriclusions and Future Work
To reliably catalognc, scardi and compose scrviccs
in a scrni-a~~tornatic
to fi~lly-al~toniatic
manner we need
standards to publish and docllmcnt services. This rcqnires langllagc standards for specifying not just the
syntax, i.e., protot,ypcs of service procedur~sand messages, but, it also ncccssitates a standard formal, yet
higll-level means for specifying the semantics of servicc
proccdrlrcs and nicssages. We have addressed thesc issnes by dcfining a nnivcrsal service-scmant,ics descript,ion language, its semantics, and we have proved sonic
nscfnl properties abont t,his langnage. The current version of USDI, incorporates crlrrcnt standards in a way

Proceedings of the Third European Conference on Web Services (ECOWS’05)
0-7695-2484-2/05 $20.00 © 2005

IEEE

t,o fnrther aid marknp of I T scrviccs by allowing constnlcts t,o be given meaning in terms of an OWT, based
WordNet ontology. This approach is more practical
and tractable than other approaches becansc dcscrip
tion docl~mentsare more casily created by humans and
more casily processed by complltcrs. USDT, is currently
being nscd t,o formally describe web-services related to
emergency response fi~nctions[Ill. Flltllrc work involves tho application of USDI, t o formally describing
commercial service repositories (for example SAP Interface Repository and services listed in UDDI), as well
as to scrvice discovery and rapid application develop
ment (RAD) in commercial environments [12]. Flitnre
work also inclndes developing tools that will allow alltomatic generation of new services based on combining
USDL descriptions of existing atomic services. The
interesting problem t o be addressed is: can USDT, description of such automatically generated services be
also alltomatically generated?

References
[I] A c o n c e p t ~ ~ acomparison
l
bet,ween wsmo and owl-s.
http://www.wsmo.org/TR/d4/d4.l/v0.1.
[2] Ont,ology-based information management syst,em,
wordnet owl-ontology.
http: //taurus.unine .ch/
knowler/wordnet.html.
[.?I Resoiirce description framework. http: / / w w .w3.
org/RDF.
[-11 Sap interface repository.
http: //ifr .sap.com/
catalog/query.asp.
[j] Semantic markup for web services. http: //wvw .daml.
org/services/ovl-s/l.O/owl-s.htm1.
[6] Web ontology language referenre. http :/ / w w .v3.
org/TR/owl-ref.
[7] Web services description I a ~ ~ g i a g ehttp:
.
/ / w w .w3.
org/TR/wsdl.
[8] Wordnet: a lexical database for t,he english langllage.
http://wuw.cogsci.princeton.edu/-vn.
[9] S. Balzer, T. T,iebig, and M. Wagner. Pitfalls of owl-s
- a practical semant,ic web use case. In 1(7SOC, 2004.
[I01 A. Bansal, S. Kona, I,. Simon, A. Mallya, G. Glrpt,a,
and T. Hite. Automatic querying and composite services generation with usdl. Working paper, 2005.
[I I] A. Ransal, K . Patel, G. Gupta, R. Raghavachari, E. D.
Harris, and J . C. Staves. Towartls intelligent services:
A case study in chemical emergency response. In
ICkV.7, pages 751-758, 2005.
[I?] T. Hite. Service composil.ion and ranking: A strategic
overview. Internal Report, Met,allect Inc., 2005.
[13] R. H~illand .l. Su. Tools for design of composit,~web
services. In SJGMOD,2004.
[II] I,. Simon, A. Ransal, A. Tvlallya, G. Cup(.a, and
T. Hite. Towards a universal service descript.ion Ianp a g e . I11 hrW'eSP, 2005.

COVER FEATURE BIG DATA MANAGEMENT

Integrating Big Data:
A Semantic ExtractTransform-Load
Framework
Srividya K. Bansal, Arizona State University
Sebastian Kagemann, Indiana University Bloomington

Current tools that facilitate the extract-transform-load
(ETL) process focus on ETL workflow, not on generating
meaningful semantic relationships to integrate data
from multiple, heterogeneous sources. A proposed
semantic ETL framework applies semantics to various
data fields and so allows richer data integration.

B

ig data, broadly defined, comprises information available from billions—even trillions—
of records generated by (and about) millions of
people and stored in myriad sources throughout the cyber universe. The insights revealed by big data
are transforming science, industry, and ultimately society itself by fostering more effective business practices
and government policies, and by enabling a broad array
of applications in engineering, biomedicine, energy,
environmental monitoring, genomics, and transportation, to name only a few.
Yet because this data is typically loosely structured and often incomplete, much of it is essentially

42	

CO M PUTE R P U B LISH ED BY TH E I EEE COMP UTER SOCI E T Y

inaccessible to users. We need technology and tools to
find, transform, analyze, and visualize data in order to
make it consumable for effective decision making.1 The
research community also agrees that engineering big
data meaningfully is crucial.2 Still, data integration
within a complex, schema-less world of heterogeneous
databases remains a significant challenge.

THE BIG DATA ENVIRONMENT

Much big data research centers on the three V’s:

›› volume—finding ways to store the huge amounts

of data constantly streaming in from social media,
0018-9162/15/$31.00 © 2015 IEEE

Weather

sensors, machine-to-machine
interfaces, and the like;
›› velocity—reacting quickly
enough to process all this data in
near real time; and
›› variety—dealing with data
in multiple formats, both
structured and unstructured:
numeric, text, video, audio, and
other media and sources.
The challenges big data researchers face lie not only in how to store and
manage all this data, but also in how to
extract and analyze information consistently. To this end, work in progress
focuses on creating a common conceptual model for integrated data.3
Linked data describes methods for
publishing structured data on the Web
so that the data is machine-readable,
its meaning is explicitly defined, it is
linked to other external datasets, and
it can be linked from other datasets
as well. The Linked Open Data (LOD)
community effort has led to a huge
data space, with 31 billion Resource
Description Framework (RDF) triples (www.w3.org/TR/2004/REC-rdf
-concepts-20040210) and a W3C specification for data interchange on the
Web.4 LOD can be used in a number of
interesting Web and mobile applications. For example, the Linking Open
Government Data (LOGD) project
investigates translating government-­
related data using semantic Web technologies.5 LOD has gained significant
adoption and momentum even though
the interconnecting relationships are
still of questionable quality.6
IBM’s Smarter Cities Challenge
(http://smartercitieschallenge.org)
aims to create cities that are vital and
safe for citizens and businesses, with a
focus on building infrastructures for
fundamental services—roadways, mass
	

Crime reports

Social
media

Transportation

d open urban data
Linke

Energy

Real e

state

ps

Ma

Pol

icie

s

FIGURE 1. Model for a linked open urban data portal that integrates data from various
sources and domains. Such integration is essential to create smart, sustainable environments.

transit, utilities, and the like—that make
a city desirable and livable. IEEE’s Smart
Cities initiative (http://smartcities
.ieee.org) brings together technological, governmental, and societal players
to encourage smart planning and policy-making for transportation, environmental sustainability, quality of life,
and other urban concerns.
Both initiatives require integrating
and using information from various
data sources in addition to creating an
appropriate infrastructure. Government
agencies are also increasingly making
data accessible through initiatives such
as Data.gov, an official US government
site that provides open data to promote
transparency and economic growth.7

CHALLENGES FOR BIG DATA

We need ways to organize data so that
concepts having similar meaning are
related through links, while concepts
distinct from one another are clearly
represented as well, via semantic metadata. This will allow query engines
and analytic tools to process big data
effectively and creatively, which can
be essential, for example, in creating
smart, sustainable environments.
In recent years, growing use of
mobile applications coupled with

increased demand to improve quality
of life in cities has heightened the need
to integrate big data in this context. For
example, a traffic jam that results from
an unplanned protest may be captured
through a Twitter stream but missed
when examining data concerning
weather conditions, scheduled events,
and reported roadwork. Moreover, a
city’s weather sensors tend to miss
localized events such as flooding.
When combined, however, all this
varied data about current conditions
can provide a richer, more complete
view of the city overall: merging traditional data sources with messy and
less reliable social media streams can
contribute to smarter living environments and improved economy, mobility, and governance. Such applications
must rely on big data available publicly
via the cloud.
Figure 1 envisions a future Web portal with linked open urban data integrated from what is published in various sources and domains.
In the private sector, Mc­K insey’s
latest Global Institute report des­
cribes a global economy beginning
to operate in real time, with total
value generated by new data technologies measured in the trillions of
MARCH 2015

43

BIG DATA MANAGEMENT

OPTIMIZING INTEGRATED DATA:
A SCENARIO

C

onsider a typical driver, John, who gets into his car in the
morning and turns on the ignition. A built-in application greets
him and, based on the time of day, asks if he is going to work. John
responds “yes,” and the app acknowledges that vehicle performance has been optimized for the trip.
The system uses global information software, road grade data,
and speed limit data to create an optimal velocity profile. As John
approaches Recker Road to turn left, the app informs him of road
repairs taking place for a mile-long stretch on Recker until 3 p.m.
and suggests that he instead take the next left onto Power Road.
John follows the suggestion.
He then receives and answers a text message from a colleague.
As he does so, his car drifts into the neighboring lane. The app
immediately notifies John of the drift, and he quickly adjusts the
wheel accordingly. As John approaches his workplace, he drives
toward Lot 1 where he usually parks. The app informs him that
only two open parking spots remain in Lot 1. Because he is already
running late for a meeting, John decides to drive directly to the
next closest lot, Lot 2, to avoid spending time searching for one
of Lot 1’s empty spots. After entering Lot 2, he gets too close to a
car parked there, and the app immediately warns him of a possible
collision. John quickly steers his car away from the other car and
parks in an empty spot.
All through his morning commute, the app has logged tracking
data to the server about John’s driving style for future use.
To perform successfully, such an app requires access to numerous datasets from various sources, including continuously updated real-time data related to traffic, road repairs, emergencies,
accidents, driving schedule, maps, parking, fuel economy, household habits, driving diagnoses, and the like. Automotive apps
could also be built that focus on reducing energy consumption and
emissions; increasing fuel economy based on actual vehicle data,
road conditions, and traffic; and, most importantly, monitoring
personal driving habits and ways to improve them.
Crucial to such innovative applications is an effective way to
integrate data tied to a rich and meaningful data model that the
apps can query.

dollars.8 Even as early as 1998, the
National Research Council’s Visionary
Grand Challenges of Manufacturing for
2020 (www.nap.edu/openbook.php
?record_id=6314) identified as one
such challenge the ability to instantaneously transform information
44	

COMPUTER 

gathered from a vast array of sources
into useful knowledge for making
effective decisions.

MEETING THE CHALLENGES

To achieve these goals, research
has focused on abstraction and

visualization tools. Big data analyses
using algorithms such as influence-­
based module mining (IBMM), as well
as online association rule mining,
graph analytics, and a provenance
analysis support framework, have all
been found applicable after data integration.9 Semantic technology has also
been shown to improve user interaction with a system, simplify data integration and extensibility, and improve
knowledge search and discovery.10
Traditionally, the problem for data
integration has been how to combine
data from different sources and provide users with a comprehensive, unified view.11,12 Any successful data integration methodology must expose to
users a schema for posing queries, generally referred to as a mediated or global
schema. To answer queries using various
information sources, the system needs
mappings that describe the semantic
relationships between the mediated
schema and the source schemas.
Two basic approaches have been
proposed for this purpose:

›› global-as-view, which requires

that the global schema be
expressed in terms of the data
sources; and
›› local-as-view, which requires that
the global schema be specified
independently from the sources
and that the relationships
between the global schema and
the sources be established by
defining every source as a view
over the global schema.12,13
These approaches are applicable
to relational data models in which
the integrated global schema is determined manually. The framework we
propose integrates linked data from
sources available across the Web to
W W W.CO M P U T E R .O R G /CO M P U T E R

Data sources

Extract
Transform

achieve the potential results described
in the sidebar “Optimizing Integrated
Data: A Scenario.”

Data preparation
Normalize
data

Remove
duplicates

Check integrity
violations

Filter
appropriate data

Sort and
group data

THE EXTRACT-TRANSFORMLOAD FRAMEWORK

In general computing, the extracttransform-load (ETL) framework is
used for integrating data from multiple sources or applications, possibly
even from different domains.14 ETL is a
data warehousing process that selects
and fits data to operational needs in
three stages:

Semantic data model
Ontology creation

Several commercial tools to facilitate the ETL process are available—
IBM Infosphere, Oracle Warehouse
Builder, Microsoft SQL Server Integration Services, and Informatica Powercenter—as are several open source ETL
products such as Talend Open Studio,
Pentaho Kettle, and CloverETL.
	

Alignment of similar data
fields from multiple sources

Semantic data instances

›› Extract involves culling data

from appropriate sources, with
data usually available in flat
file formats such as comma-­
separated values (CSV), Excel
(XLS), or .txt or through a RESTful client.
›› Transform involves cleansing
data, sometimes invoking quality checks, to comply with the
target schema. Typical transformation activities involve normalizing data, removing duplicates, checking for integrity
constraint violations, filtering
data based on defined regular
expressions, sorting and grouping data, and applying built-in
functions where necessary.
›› Load involves propagating the
data into a target operational
database, data mart, or data
warehouse for client use.

Mapping of data fields

Generation of RDF triples
Triple data storage

Web portal

Load
Back end

Front end

FIGURE 2. Semantic extract-transform-load (ETL) framework for data integration. While
traditional ETL extract and load operations remain the same, during the transform stage a
semantic data model is created to generate Resource Description Framework (RDF) triples
that allow operations on linked open data (LOD).

Existing approaches like these
focus on designing the ETL workf low,
not on generating the kind of meaningful, semantic data that is important for integrating data from multiple sources. Semantic approaches to
ETL have been proposed, but they use
semantic technologies to refine the
activities involved in the ETL process
rather than to manipulate the actual
data. One approach, for example,
allows semiautomatic definition of
inter-­
a ttribute semantic mappings
by identifying parts of data source
schemas, which are related to the
data warehouse schema; but this
supports the extraction phase of
ETL.15

SEMANTIC ETL OVERVIEW

Our proposed semantic ETL framework generates a semantic model of
the datasets to be integrated, and then
generates semantic linked data that
complies with the data model. This
generated semantic data is made available on the Web as linked data that can
be queried and used in analytics, and
also serve as a resource for innovative
data-driven apps.
We introduce semantic technologies in the ETL transform stage to
create a semantic data model and generate semantic linked data (as RDF triples) for ultimate storage in the data
mart or warehouse. Regular transform
activities such as data normalizing and
MARCH 2015

45

BIG DATA MANAGEMENT

cleansing continue to be performed,
and the extract and load stages of ETL
remain the same. Figure 2 outlines the
activities involved in semantic ETL.
The transform stage involves
manual analysis of the datasets, the
schema, and their purpose. Based on
these findings, the schema is mapped

›› RDF, the general vehicle for

data interchange on the Web,
which allows structured and
semi-structured data to be
shared and mixed across various
applications. A language for
describing Web resources, RDF
is used for representing infor-

SEMANTIC WEB TECHNOLOGIES CAN
AID IN CONSTRUCTING ANY ADVANCED
KNOWLEDGE MANAGEMENT SYSTEM.

to an existing domain-specific ontology or, alternatively, a functional
ontology must be created from scratch.
Data sources from disparate domains
may require multiple ontologies, with
alignment rules specified for any common or related data fields.

TECHNOLOGY STACK
FOR SEMANTIC ETL

Available semantic Web technologies
facilitate activities such as organizing knowledge into conceptual spaces
based on their meanings, extracting
new knowledge via querying, and maintaining knowledge integrity by checking for inconsistencies—and so can aid
in constructing any advanced knowledge management system. Our semantic ETL framework uses the following
semantic technologies and tools:

›› URI, the string of characters

used to identify a name or a
Web resource. This identification enables interaction with
resource representations over
a network (most often the Web)
using specific protocols.

46	

COMPUTER 

mation, especially metadata. As
noted earlier, RDF is designed to
be machine-readable so that it
can be used in software applications for intelligent processing
of information.
›› OWL, or Web Ontology Language, a markup language used
for publishing and sharing
ontologies. OWL builds on RDF;
an ontology created in OWL is
actually an RDF graph. Individuals with common characteristics
can be grouped as a class, and
OWL provides different class
descriptions. It also provides two
property types: object properties, which link individuals to
other individuals, and data properties, which link individuals
to data values. With OWL, users
can define concepts so they can
be mixed and matched with
other concepts for various uses
and applications.
›› SPARQL, an RDF query language
designed to query, retrieve,
and manipulate data stored in
RDF format. A SPARQL query

can consist of triple patterns,
conjunctions, disjunctions, and
optional patterns. It also allows
users to write queries against
data that can loosely be called
“key-value” data, as it follows
the W3C’s RDF specification. The
entire database is thus a set of
“subject-­predicate-object” triples.
›› Protégé (http://protege.stanford
.edu), an open source ontology
editor and framework for building intelligent systems that lets
users create ontologies in OWL.
Our framework uses Protégé
to provide semantic metadata
to the schema of datasets from
various sources.

TWO CASE STUDIES

We conducted two case studies to
assess how well our proposed semantic ETL framework performs. The first
is based on US government data from
the Federal Highway Administration’s National Household Travel Survey (NHTS; http://nhts.ornl.gov) and
fuel economy data from the Environmental Protection Agency (EPA; www
.fueleconomy.gov); the second uses
public datasets from several massive
open online course (MOOC) providers.

Integrated household travel
and fuel economy data

NHTS data, published by the US Department of Transportation, is collected
to assist planners and policymakers
needing information about transportation patterns around the country. The
set consists of daily travel data about
trips people take over a 24-hour period,
including the trip’s purpose (work, grocery shopping, school drop-off, and so
forth), the means of transportation (such
as bus, car, or walking), the trip’s length,
and the day it occurred, along with
W W W.CO M P U T E R .O R G /CO M P U T E R

has

Make

Vehicle ID

Ma

ke

hasFuelType

Fuel type

Hybrid
Vehicle

Type

hicle

hasVe

Driver count

Vehicle type

additional information in cases where
a private vehicle was used. Researchers
use this dataset to study relationships
between demographics and travel; correlations among modes of transportation, travel purpose and duration, and
time of day and day of the week; and
other transportation concerns.
EPA data on vehicle fuel economy
provides detailed information about
models from all manufacturers: vehicle
description, mileage statistics for both
city and highway driving, mileage for
different fuel types, emissions information, and fuel prices. Data is based
on vehicle testing conducted by the
National Vehicle and Fuel Emissions
Lab in consultation with automakers.
We designed and developed a common semantic data model to integrate
NHTS data with EPA fuel economy
data using Protégé for ontology engineering. Our model has three primary classes—Person, Household, and
­Vehicle—at the topmost level. All fields
in the datasets are modeled as classes,
subclasses, or properties and connected
to the primary classes via suitable relationships. The ontology includes both
object properties and data properties
depending on the data field being represented. Figure 3 illustrates the highlevel semantic data model, showing
the primary classes and properties and
also their relationships.

Integrated MOOC provider data

We also used our proposed semantic framework to integrate data from
three major MOOC providers:

›› Coursera, the world’s largest

MOOC provider, has 7.1 million
users enrolled in 641 courses
from 108 institutions as of April
2014. Courses span 25 categories including four computer

	

Home type

hasHomeType

Household
state
Education
level

hasVehicle

Vehicle year
Odometer reading
Household race

Household

House
size

hasPerson

Household ID
Number of adults
Person ID

hasEducatio

n

Object property
Data property
Subsidiary property

Person

Age
Distance to work

FIGURE 3. Semantic data model integrating household travel and fuel economy data
from the US government. The model’s three primary classes (Vehicle, Household,
Person) are connected to subclasses (green) and properties (blue) via pertinent object
properties, data properties, and subsidiary properties.

science subcategories. Course
data is retrievable via Coursera’s
RESTful course catalog API and
returned in JSON (JavaScript
Object Notation).
›› edX is a premier MOOC provider
with more than 2.5 million users
and more than 200 courses as
of June 2014. edX courses are
distributed among 29 categories, many of which overlap with
Coursera’s. edX does not provide
an API for accessing its course
catalog; however, edX’s entire
platform is currently open source.
›› Udacity is a vocational course–
centric MOOC provider with 1.6
million users, 12 full courses,
and free courseware for 26 more,
mostly in computer science, as
of April 2014. Udacity does not
provide an API for accessing its
course catalog data.
We created the semantic model integrating MOOC data by extending one
available at Schema.org (described as
“a collection of schemas that webmasters can use to markup HTML pages
in ways recognized by major search
providers, and that can also be used

for structured data interoperability”).
Schema.org offers a generic “creative
work” model described and organized
as a hierarchy of types, each associated
with a set of properties. We used a specification from the Learning Resource
Metadata Initiative (LRMI; www.lrmi
.net) to add properties to Schema.org’s
CreativeWork object: the time it takes
to work through a learning resource
(timeRequired), the typical age range
of the content’s intended audience
(typicalAgeRange), and so on. We used
the existing vocabulary for CreativeWork as a base type, extended to add
Course, Session, Category, and associated properties drawn from the MOOC
datasets. Figure 4 shows the high-level
semantic data model for this set.
The complete ontology is available
at the project website: https://parasol
.t a m u .e d u/d r e u 2 0 14/ K a g e m a n n
/research.html.

APPLYING THE MODELS

Once the two high-level models were
in place, we applied each model for
semantic instance data generation and
for querying.
We obtained household travel and
fuel economy datasets in CSV format
MARCH 2015

47

BIG DATA MANAGEMENT
Thing
schema.org/Thing
+ Object Property 1 : name
+ Object Property 2: URL

Intangible
schema.org/
Intangible

+ Data Property 1: Organization ID

Creative Work
schema.org/CreativeWork

Person
schema.org/Person

Organization
schema.org/Organization

+ Object Property 1: affiliation(Person, Organization)
+ Object Property 2: email
+ Object Property 3: teaches(Person, Course)

Course Track
sebk.me/MOOC.owl#Course_Track

+
+
+
+
+
+
+
+
+
+

Object
Object
Object
Object
Object
Object
Object
Object
Object
Object

Property
Property
Property
Property
Property
Property
Property
Property
Property
Property

1 : about
2 : author
3: video
4: inLanguage
5: isBasedOnUrl (syllabus)
6: timeRequired
7: keywords
8: discussionURL
9: aggregateRating
10: publisher

Rating
schema.org/
Rating

+ Data Property 1: Track ID
+ Object Property 1: hasCourse(Track, Course)

Language
schema.org/
Language

Course
sebk.me/MOOC.owl#Course
+
+
+
+
+
+
+
+
+

Category
sebk.me/MOOC.owl#Category
+ Data Property 1: Category ID
+ Data Property 2: Short Name
+ Object Property 1: includesCourse(Category, Course)

Data Property 1: Difficulty
Data Property 2: Course ID
Data Property 3: Recommended Background
Data Property 4: Course Format
Data Property 5: Course Syllabus
Data Property 6: Subtitle Languages
Object Property 1: isTaughtBy(Person, Course)
Object Property 2: hasCategory(Course, Category)
Object Property 3: hasSession(Course, Session)

Session
sebk.me/MOOC.owl#Session
+
+
+
+
+
+
+
+

Data Property 1: Session ID
Data Property 2: Session Status
Data Property 3: Active
Data Property 4: Start Date
Data Property 5: Eligible for Signature Track
Data Property 6: Eligible for Certificates
Data Property 7: Duration
Object Property 1: belongsToCourse(Session, Course)

FIGURE 4. Semantic data model integrating datasets from three massive open online course (MOOC) providers (Coursera, edX, and
Udacity) to create a MOOC aggregator Web application for potential students.

that we converted into RDF triples
using Oxygen XML editor. A Web
crawler was written using the Scrapy
Crawler framework (http://scrapy
.org) to obtain MOOC datasets in
JSON format. A total of 30,000 triples were generated; these can be
accessed from the project website.
The integrated datasets were queried
using SPARQL.
Apache Jena (http://jena.apache
.org), the Java open source framework
for building semantic Web and
linked data applications, was used for
48	

COMPUTER 

executing SPARQL queries on travel
and fuel economy. An application we
created loads the ontologies and data,
checks that data conforms with the
vocabulary, and then runs SPARQL queries against the data. Figure 5 illustrates
sample queries for the integrated household travel and fuel economy data.
Linked data generated from the
MOOC providers was stored into a
standalone Fuseki server (a Jena subproject) that provides an HTTP interface to RDF data. Following is a sample
SPARQL query:

PREFIX mooc: <http://sebk.me/MOOC.
owl#>
PREFIX schema: http://schema.org/
SELECT * WHERE {
?course rdf:type mooc:Course.
?course schema:name ?iname.
FILTER (regex(?iname,
“calculus”, “i”)).
}
We developed a MOOC aggregator
Web application called MOOCLink
that queries this integrated dataset
to provide consolidated information
W W W.CO M P U T E R .O R G /CO M P U T E R

No.
1
2
3
4
5
6

about courses currently offered and
upcoming courses and also allow
users to search for courses and compare courses. This Web application
is currently under testing. A screenshot of a course detail page of the Web
application is shown in Figure 6. Further details of this ongoing project are
updated at the project website.

Query
Most popular car in US
Most popular car in Arizona
Number of diesel cars in a particular census division
Estimated annual cost of regular gasoline per census division
Estimated annual cost of gasoline per household
Gas guzzler tax per household

FIGURE 5. Sample queries for the semantic integrated household travel and fuel economy data.

H

ow best to integrate data
from various heterogeneous
sources into a meaningful data
model that allows intelligent querying
and facilitates creation of useful applications is an important open issue in
big data research and development.
The case studies we describe show our
proposed semantic ETL framework has
significant potential to produce linked
data that supports innovative datadriven apps for smart living.
However, the ontology engineering stage requires a human expert
with deep understanding of data from
multiple sources and relies heavily
on manual data analysis, which is
quite time-consuming. Thus, algorithms need to be developed that
automatically—or, at least, with minimal human intervention—­generate
semantic data models. Based on this
requirement, we suggest several
future directions for research:

›› establishing a process to identify
existing ontologies for datasets
under integration;
›› extending an existing ontology
with relevant properties, classes,
and relationships or creating a
new ontology;
›› generating alignment rules
between multiple ontologies
that might exist for individual
datasets; and
	

FIGURE 6. Screenshot for a course detail page from the MOOC aggregator Web application. MOOCLink queries the integrated data to provide consolidated information about
course offerings on Coursera, edX, and Udacity.

›› generating alignment rules

between the semantic data
model and existing ontologies—­
such as FOAF (friend-of-afriend), DBPedia, DublinCore,
and Wordnet—to produce
semantically rich linked data.

Additionally, a semantic ETL process could be introduced into existing open source ETL software tools
such as CloverETL, a Java engine
library that does not have user interface components.
Our future work for this specific project will focus on creating

interesting and innovative applications—Web, mobile, or both—in
domains such as automotive and aerospace engineering, healthcare, and
education.

REFERENCES
1.	 E. Kandogan et al., “Data for All: A
Systems Approach to Accelerate the
Path from Data to Insight,” Proc. IEEE
2nd Int’l Congress Big Data (BigData
13), 2013, pp. 427–428.
2.	 C. Bizer et al., “The Meaningful Use
of Big Data: Four Perspectives–Four
MARCH 2015

49

BIG DATA MANAGEMENT

ABOUT THE AUTHORS
SRIVIDYA K. BANSAL is an assistant professor in the Ira A. Fulton Schools
of Engineering at Arizona State University. Her research interests include
semantics- based approaches for big data integration; Web service description,
discovery, and composition; and tools for outcome-based instruction design in
STEM education. Bansal received a PhD in computer science from the University of Texas at Dallas. She is a member of the IEEE Computer Society. Contact
her at srividya.bansal@asu.edu.

11.

SEBASTIAN KAGEMANN is an undergraduate in the Department of Computer
Science at Indiana University Bloomington. His academic interests include
mobile development, distributed computing, and cognitive science. Contact
him at sakagema@umail.iu.edu.

12.

13.

3.

4.

5.

6.

7.

Challenges,” SIGMOD Record, vol. 40,
no. 4, 2012, pp. 56–60.
A. Azzini and P. Ceravolo, “Consistent Process Mining over Big Data
Triple Stores,” Proc. IEEE 2nd Int’l
Congress Big Data (BigData 13), 2013,
pp. 54–61.
C. Bizer, T. Heath, and T. Berners-Lee, “Linked Data—The Story So
Far,” Int’l J. Semantic Web and Information Systems, vol. 5, no. 3, 2009,
pp. 1–22.
L. Ding et al., “TWC LOGD: A Portal
for Linked Open Government Data
Ecosystems,” J. Web Semantics, vol. 9,
no. 3, 2011, pp. 325–333.
S. Dastgheib, A. Mesbah, and K.
Kochut, “mOntage: Building Domain
Ontologies from Linked Open Data,”
Proc. 7th IEEE Int’l Conf. Semantic
Computing (ICSC 13), 2013, pp. 70–77.
F. Lecue, S. Kotoulas, and P. Mac

Aonghusa, “Capturing the Pulse of
Cities: Opportunity and Research
Challenges for Robust Stream Data
Reasoning,” Workshops at the 26th
AAAI Conf. Artificial Intelligence,
2012; www.aaai.org/ocs/index.php
/WS/AAAIW12/paper/view/5216.
8. J. Manyika et al., Big Data: The Next
Frontier for Innovation, Competition,
and Productivity, McKinsey Global
Inst., 2014; www.mckinsey.com
/insights/business_technology/big
_data_the_next_frontier_for
_innovation.
9. S.K. Bista, S. Nepal, and C. Paris,
“Data Abstraction and Visualisation
in Next Step: Experiences from a
Government Services Delivery Trial,”
Proc. IEEE 2nd Int’l Congress Big Data
(BigData 13), 2013, pp. 263–270.
10. D. Hecht, P.C. Sheu, and J.P. Tsai,
“SCDL Applications to Drug

14.

15.

Discovery,” Proc. 9th IEEE Int’l Conf.
Bioinformatics and BioEngineering
(BIBE 09), 2009, pp. 449–454.
A. Halevy, A. Rajaraman, and J.
Ordille, “Data Integration: The Teenage Years,” Proc. 32nd Int’l Conf. Very
Large Data Bases (VLDB 06), 2006,
pp. 9–16.
M. Lenzerini, “Data Integration: A
Theoretical Perspective,” Proc. 21st
ACM−SIGMOD Symp. Principles Database Systems, 2002, pp. 233–246.
A. Calì et al., “Data Integration under
Integrity Constraints,” Information Systems, vol. 29, no. 2, 2004, pp. 147−163.
P. Vassiliadis, A. Simitsis, and E.
Baikousi, “A Taxonomy of ETL Activities,” Proc. ACM 12th Int’l Workshop
Data Warehousing and OLAP (DOLAP
09), 2009, pp. 25–32.
S. Bergamaschi et al., “A Semantic
Approach to ETL Technologies,” Data
& Knowledge Eng., vol. 70, no. 8, 2011,
pp. 171–731.

Selected CS articles and
columns are also available for
free at http://ComputingNow
.computer.org.

Subscribe today for the latest in computational science and engineering research, news and analysis,
CSE in education, and emerging technologies in the hard sciences.

www.computer.org/cise
50

COMPUTER

W W W.CO M P U T E R .O R G /CO M P U T E R

Towards System Integrity Protection
with Graph-Based Policy Analysis
Wenjuan Xu1 , Xinwen Zhang2, and Gail-Joon Ahn3
1
2

University of North Carolina at Charlotte
wxu2@uncc.edu
Samsung Information Systems America
xinwen.z@samsung.com
3
Arizona State University
gahn@asu.edu

Abstract. Identifying and protecting the trusted computing base (TCB) of a system is an important task, which is typically performed by designing and enforcing
a system security policy and verifying whether an existing policy satisfies security
objectives. To efficiently support these, an intuitive and cognitive policy analysis
mechanism is desired for policy designers or security administrators due to the
high complexity of policy configurations in contemporary systems. In this paper,
we present a graph-based policy analysis methodology to identify TCBs with the
consideration of different system applications and services. Through identifying
information flows violating the integrity protection of TCBs, we also propose
resolving principles to using our developed graph-based policy analysis tool.

1 Introduction
In an operating system, an information flow occurs when one process can write to a resource (e.g., device or file) which can be read by another process. Inter-process communication between these processes may also cause possible information flows. Integrity
goal is violated if there exists information flow from an unprivileged process to a privileged process or between two different applications or services where information flow
should be controlled. In a typical system, information flows are controlled through security policies. Our objective in this paper is to provide an effective framework for
analyzing system security policies and finding policy rules causing information flows
with integrity violations.
There are several related approaches and tools to analyze policies based on certain
information flow models [1,9,11,16]. Most of these work focus on the identification of a
common and minimum system TCB which includes trusted processes for an entire system. These approaches can analyze whether a policy meets security requirements such
as no information flow from low integrity processes (e.g., unprivileged user processes)
to system TCB (e.g., kernel and init). However, they cannot identify integrity violations
for high level services and applications that do not belong to system TCB. In practical,
other than system TCB protection, a high level application or system service is required
to achieve integrity assurance through controlling any information flowing from other
E. Gudes, J. Vaidya (Eds.): Data and Applications Security 2009, LNCS 5645, pp. 65–80, 2009.
c IFIP International Federation for Information Processing 2009


66

W. Xu, X. Zhang, and G.-J. Ahn

applications. An existing argument in [20] clearly states the situation as follows: “a network server process under a UNIX-like operating system might fall victim to a security
breach and compromise an important part of the system’s security, yet is not part of the
operating system’s TCB.” Accordingly, a more comprehensive policy analysis for TCB
identification and integrity violations is desired.
Another issue in policy analysis is the large size and high complexity of typical policies in contemporary systems. For example, an SELinux [12] policy has over 30,000
policy statements in a desktop environment. Under this situation, several challenges
exist for system designers or administrators in the context of policy analysis. We enumerate such challenges to derive the motivation of this paper: (1) Understanding and
querying a policy All previous policy query tools lack an effective mechanism for a
user to understand a particular policy. Furthermore, without understanding a policy, the
user even cannot figure out what to query; (2) Recognizing information flow paths In
the work of information flow based policy analysis [9,16], information flow paths are
expressed with text-based expressions. However, primitive text-based explanations cannot provide appropriate degree of clarity and visibility for describing flow paths; and
(3) Identifying integrity violation patterns, inducements, and aftermaths In the work
of Jaeger et al. [11,17], they elaborate violation patterns using graphs. However, their
approach does not leverage the features and properties of graphs for analyzing policies
and identifying the causes and effects of integrity violations.
In this paper, we consider an information domain as a collection of subjects (e.g.,
processes) and objects (e.g., files, ports, devs) which jointly function for an application or service. We further define the domain TCB as subjects which should have the
same integrity level in the domain. The integrity of the domain cannot be judged unless
information flows between this domain TCB and the rest of the system are appropriately controlled. Based on these notions, we propose a domain-based integrity model.
Based on this model, we build a graph-based policy analysis methodology for identifying policy rules that cause integrity violations (or simply policy violations). Our graphbased approach can provide several benefits since information visualization helps a user
heuristically explore, query, analyze, reason, and explain obtained information. Also,
our graph-assisted approach simplifies analysis and verification tasks while rigorously
producing distinctive representation of analysis results. In addition, we describe a set
of principles for resolving identified policy violations in policy graphs. A graph-based
policy analysis tool (GPA) is also developed based on our approach.
The rest of this paper is organized as follows. Section 2 describes background and
some work related to our graph-based policy analysis. The principles and methodology
of our work are illustrated in Section 3. Section 4 presents how to develop and use
GPA for analyzing policies. In this section, we adopt SELinux policies as an example.
Section 5 concludes this paper presents our future work.

2 Background and Related Work
2.1 Trusted Computing Base
The concept of TCB partitions a system into two parts: the part inside TCB which is
referred as to be trusted (TCB) and the part outside TCB which is referred as to be

Towards System Integrity Protection with Graph-Based Policy Analysis

67

untrusted (NON-TCB). Therefore, the identification of TCB is always a basic problem in security policy design and management. The famous Orange Book [2] proposes
TCB as part of a system that is responsible for enforcing information security policies.
Reference monitor-based approach is proposed in [4], where a system’s TCB not only
includes reference monitor components, but also encompasses all other functionalities
that directly or indirectly affect the correct operation of the reference monitor such as
object managers and policy database. Considering an operating system, its TCB includes kernel, system utilities and daemons as well as all kinds of object management
and access control functions. Typical object management functions are responsible for
creating objects and processing requests while typical access control functions consist
of both rules and security attributes that support decision-making for access control.
2.2 Integrity Model
To describe information flow-based integrity protection, various models are proposed
and developed in past years, such as Biba [7], Clark-Wilson [15], LOMAC [19] and
CW-lite [17]. Biba integrity property is fulfilled if a high integrity process cannot read
lower integrity data, execute lower integrity programs, or obtain lower integrity data
in any other manner. LOMAC supports high integrity process’s reading low integrity
data, while downgrading the process’s integrity level to the lowest level that it has ever
read. Clark-Wilson provides a different view of dependencies, which states that through
certain programs so-called transaction procedures (TP), information can flow from low
integrity objects to high integrity objects. Later the concept of TP is evolved into filter
in CW-Lite model. A filter can be a firewall, an authentication process, or a program
interface for downgrading or upgrading the privileges of a process. In CW-lite model,
information can flow from low integrity processes (NON-TCB) to high integrity processes (TCB) through filters.
2.3 Policy Analysis
The closest existing work to ours include Jaeger et al. [11] and Shankar et al. [17]. In
these work, they use a tool called Gokyo for checking the integrity of a proposed TCB
for SELinux [18]. Also, they propose to implement their idea in an automatic way.
Gokyo mainly identifies a common TCB in SELinux but a typical system may have
multiple applications and services with variant trust relationships. Still, achieving the
integrity assurance for these applications and services is not addressed in Gokyo.
Several query-based policy analysis tools have been developed. APOL [1] is a tool
developed by Tresys Technology to analyze SELinux policies. SLAT (Security Enhanced Linux Analysis Tool) [9] defines an information flow model and policies are
analyzed based on this model. PAL (Policy Analysis using Logic Programming) [16]
uses SLAT information flow model to implement a framework for analyzing SELinux
policies. All these tools try to provide a way for querying policies. However, they all
display policies and policy query results in text-based expressions, which are difficult
to understand for policy developers or security administrators. Other policy analysis
methods are also proposed. For example, in [23], they propose to analyze the policies
with information-flow based method. For another example, in [22], they try to analyze

68

W. Xu, X. Zhang, and G.-J. Ahn

the policies with graph-based model. However, non these approaches are applicable in
our scenarios since they are specific to certain access control model, and none of them
are realized with certain tools.
To overcome these issues, we have developed a graph-based methodology for identifying and expressing interested information flows in SELinux policies [21]. We also
have proposed a policy analysis mechanism using Petri Nets is proposed in [3]. However, this work does not have the capability of policy query, thus is limited in identifying
system TCB and other domain TCBs.

3 Graph-Based Policy Analysis
To help a policy administrator better understand security policies and perform policy
analysis tasks, we first propose a graph-based policy analysis methodology in this section. Graphs leverage highly-developed human visual systems to achieve rapid uptake
of abstract information [10]. In our methodology, we have two parallel building blocks:
basic policy analysis and graph-based policy analysis. The basic policy analysis is composed of security policy definitions, integrity model for identifying policy violations,
and methods for resolving policy violations. Graph-based analysis is built according to
basic policy analysis and expresses corresponding components with graphical mechanisms and algorithms. We elaborate the details of our methodology in the remainder of
this section.
3.1 Basic Policy Analysis
Security Policies. A security policy is composed of a set of subjects, a set of objects,
and a set of policy statements or rules which states that a subject can perform what kind
of actions on an object. For information flow purpose, all operations between subjects
and objects can be classified as write like or read like [9] and operations between
subjects can be expressed as calls. Depending on the types of operations, information
flow relationships can be identified. If subject x can write to object y, then there is information flow from x to y, which is denoted as write(x, y). On the other hand, if subject
x can read object y, then there is information flow from y to x denoted as read(y, x).
Another situation is that if subject x can call another subject y, then there is information flow from y to x, which is denoted as call(y, x). Moreover, the information flow
relationships between subjects and objects can be further described through flow transitions. In a policy, if a subject s1 can write to an object o which can be read by another
subject s2 , then it implies that there is an information flow transition from subject s1
to s2 , denoted as f lowtrans(s1 , s2 ). Also, if subject s2 can call a subject s1 , there is
a flow transition from s1 to s2 . A sequence of flow transitions between two subjects
represents an information flow path.
Integrity Model. Retrospecting the integrity models introduced in Section 2, one-way
information flow with Biba would not be sufficient for many cases as communication
and collaboration between application or service domains are frequently required in
most systems. Although filters between high and low integrity data are sufficient enough
for TCB and NON-TCB isolations, it is not suitable for application or service domain

Towards System Integrity Protection with Graph-Based Policy Analysis

69

isolations. For example, processes of user applications and staff applications are required to be isolated since both are beyond the minimum and common TCB boundary.
With that reason, we develop a domain-based integrity model, in which a concept called
domain TCB is defined to describe subjects and objects required to be isolated for an
information domain. To be clear, the minimum and common TCB of a system is called
system TCB in our paper. Also, for a subject in a system, if it neither belongs to the
system TCB, nor belongs to the domain TCB of a particular application or service, then
it is in the NON-TCB of the system.
Information Domain. As mentioned in Section 1, an application or service information
domain consists of a set of subjects and objects. Here, we propose two steps to identify
an information domain.
– Step1: Keyword-based domain identification. Generally, subjects and objects in a
security policy are described based on their functions, e.g., http is always the prefix
for describing web server subjects and objects in SELinux. Hence, to identify the
web server domain, we use keyword http to identify the initial set of subjects and
objects in this domain.
– Step2: Flow-based domain identification. In a security policy, some subjects or
objects cannot be identified through keyword prefix. However, they can flow to
initially identified domain subjects and objects, influencing the integrity of this domain. Therefore, we also need to include these subjects and objects into the domain.
For instance, in a Linux system, var files can be read by web server subjects such
as httpd. Hence they should be included in the web server domain.
Domain TCB. To protect the integrity of an information domain, a domain TCB is
defined. TCB(d) (domain d’s TCB) is composed of a set of subjects and objects in
domain d which have the same level of security sensitivity. In other words, a web server
domain running in a system consists of many subjects–such as processes, plugins, and
tools, and other objects including data files, configuration files, and logs. We consider
all of these subjects and objects as TCB of this domain, while its network object such
as tcp : 80 (http port t) is not considered as TCB since it may accept low integrity
data from low integrity subjects. In a system, the integrity of an object is determined by
the integrity of subjects that have operations on this object. Hence, we need to identify
TCB(d) subjects of each information domain and verify the assurance of their integrity.
To ease this task, a minimum TCB(d) is preferred. However, in the situation that the
minimum TCB(d) subjects have dependency relationships with other subjects, these
other subjects should be added to domain TCB or dependencies should be removed.
Based on these principles, we first identify an initial TCB(d) subjects which are predominant subjects for domain d. We further discover TCB(d) considering subject dependency relationships with the initial TCB(d) through flow transition-based identification
and violation-based adjustment.
– Step1: Initial TCB(d) identification. In an information domain, there always exist
one or several predominant subjects, which launch all or most of other subjects
functioning in this domain. Here, we identify the initial TCB(d) subjects based on

70

W. Xu, X. Zhang, and G.-J. Ahn

these subject launching relationships and the number of subjects that a subject can
launch. For example, for web server domain, httpd launches all other processes like
httpd script, hence it belongs to the initial TCB(d) of this domain.
– Step2: Flow transition-based TCB(d) identification. The subjects that can flow
only to and from the initial identified TCB(d) are included into domain TCB. For instance, if subject httpd php can flow only to and from httpd, then httpd php should
be included into TCB(d).
– Step3: TCB(d) adjustment by resolving policy violations. After identifying policy
violations (or integrity violations described shortly in this subsection), we adjust the
identified TCB(d) with wrongly included or excluded subjects. For example, initially subject awstats script (web server statistics script) is excluded from TCB(d).
After identifying policy violations caused from awstats script to web server
TCB(d), we found that these violations can be ignored. Hence, the TCB(d) should
be adjusted to include awstats script.
Domain-based Integrity Model. Based on the concept of system TCB and TCB(d), a
domain-based integrity model is defined as follows.
Definition 1. Domain-based integrity model is satisfied for an information domain d if
for any information flow to TCB(d), the information flow path is within TCB(d); or the
information flow path is from the system TCB to TCB(d); or the information flow path
is from another domain TCB and it is filtered.
Through this definition, domain-based integrity model achieves the integrity of an information domain by isolating information flow to TCB(d). This model requires that
any information flow happening in a domain d adheres within the TCB(d), from system
TCB to the TCB(d), or from another domain TCB via filter(s). In this paper we do not
discuss the integrity of filters, which can be ensured with other mechanisms such as formal verification or integrity measurement and attestation [14]. Filters can be processes
or interfaces that normally is a distinct input information channel and is created by, e.g.,
a particular open(), accept() or other call that enables data input. For example, linux su
process allows a low integrity process (e.g., staff) changes to be high integrity process
(e.g., root) through calling passwd process. For another example, high integrity process
(e.g., httpd administration) can accept low integrity information (e.g, network data)
through the secure channel such as sshd. Normally, it is the developer’s tasks to build
filtering interfaces and prove effectiveness to the community [13]. Generally, without
viewing system application codes, an easier way for policy administrator to identify
filters is to declare filters with clear annotations during policy development [17]. Here,
we assume that filters can be identified through annotations. Also, in our work, initially
we do not have a set of predefined filters. After detecting a possible policy violation, we
identify or introduce a filter subject to resolve policy violations.
Policy Violation Detection. Based on domain-based integrity model, we treat a TCB(d)
as an isolated information domain. We propose the following rules for articulating possible policy violations for system TCB and TCB(d) protections.
Rule 1. If there is information flow to a system TCB from its subjects without passing
any filter, there is a policy violation for protecting the system TCB.

Towards System Integrity Protection with Graph-Based Policy Analysis

71

Rule 2. If there is information flow from TCB(dx ) to TCB(dy ) without passing any filter,
there is a policy violation in protecting TCB(dy ).
Policy Violation Resolution. After possible policy violations are identified with violation detection rules, we take systematic strategies to resolve them. Basically, for a
violation, we first evaluate if it can be resolved by adding or removing related subjects
to/from system or domain TCBs. This causes no change to the policy. Secondly, we try
to identify if there is a filter along with the information flow path that causes the violation. If a filter can be identified, then the violation is a false alarm and there is no change
to the policy graph. Thirdly, we attempt to modify policy, either by excluding subjects
or objects from the violated information flow path, or by replacing subjects or objects
with more restricted privileges. In addition, we can also introduce a filter subject that
acts as a gateway between unauthorized subjects and protected subjects.
3.2 Graph-Based Analysis
Semantic substrates [5] is a visualization methodology for laying out a graph, in which
graph nodes are displayed in non-overlapping regions based on node attributes. Through
this way, the location of a node conveys its information. Also, the visibility of graphic
links used to describe node relationships is available depending on user control. In our
work, we use semantic substrates to display policy graph and policy violation graph
which are defined based on the previously stated security policies and policy violations.
A graphical query-based violation identification method is introduced based on domainbased integrity model. Also, we illustrate how we can apply policy violation resolution
methods to policy violation graphs.
Policy Graph. A security policy consists of a set of subjects, objects, and operations
including write, read and call. We define a policy graph as follows:
Definition 2. A Policy Graph of a system is a directed graph G=(V, E), where the set
of vertices V represents all subjects and objects in the system, and the set of edges E=V
× V represents all information flow relations between subjects and objects. That is,

Vs , where Vo and Vs are the sets of nodes that represent objects and
– V=Vo
subjects,
respectively;


– E=Er Ew Ec . Given the vertices vs1 ,vs2 ∈ Vs separately representing subject
s1 and s2, and vertices vo ∈ Vo representing object o, (vs1 , vo ) ∈ Ew if and only if
write(s1, o), (vo , vs2 ) ∈ Er if and only if read(o, s2), and (vs1 , vs2 ) ∈ Ec if and
only if call(s1, s2).
As concluded in [8], humans perceive data coded in spatial dimensions far more easily
than those coded in non-spatial ones. Based on this concept, we use semantic substrates
to display policies. We divide a canvas into different areas based on the classification of
entities (subjects and objects) and then layout nodes expressing the entities into corresponding areas. We also use non-spacial cues (e.g., color or shape) to emphasize certain
nodes or a group of nodes. Figure 1 (a) shows the semantic substrates-based graph design. The Y-axis is divided into regions, where each region contains nodes representing

72

W. Xu, X. Zhang, and G.-J. Ahn

entities such as subjects and objects. Furthermore, in each region, nodes representing
entities of different classifications are placed in different spaces along with the X-axis.
For subjects and objects in a policy, Sc1 ...Scn and Oc1 ...Ocm separately represent certain classifications. Different colors and shapes are used to distinguish the identification
of different nodes. Circles and rectangles are used to represent subjects and objects,
respectively. Relationships between subjects and objects are expressed with lines in different colors or shapes. For instance, the write operation between subject s2 and object
o2 is expressed with a red link.
S

Sc2

Sc1

Sc3

s3

s5

O

Oc1

o3

s2

s1

s4

Oc2
o1

Scn

Ocm
o2

(a) Design 1- Policy Graph: Links between S2 and
O2 represents write operation; between S3 and O2
expresses read operation; between S4 and S3
denotes call operation

Algorithm 1: [Building Policy Graph]
Input: The Policy file Policy, the Policy Explanation File Fe , the Permission
Mapping File Fp , the Subject Classification File Fs, the Object Classification File Fo.
Output: A Semantic-based Policy Graph G
Method:
(1) Policy_t: = policyParsing( Policy,Fe, Fp, Fs, Fo);/* parsing the policies files into
subjects, objects and relationships , and mapping the classification into the parsed
policies.
(2) G = drawCanvas (Policy_t); /* constructing the canvas for drawing the graphs
and also dividing the graphs into different areas.
(3) G = drawNodes (G , Policy_t); /* reading the entities for the policies and drawing
them in nodes into the classified areas based on the Policy_t structure.
(4) G = drawLines (Policy_t, G , n); /* drawing the link from the node to the other
nodes and setting the attribute for the link.
(b) Algorithm for building policy graph

Fig. 1. Designation and algorithm for expressing policies in graph

Different security policies have different formats and components. To give a uniform
way for policy analysis, we need to preprocess a primitive policy. Figure 1 (b) summarizes the procedures of policy graph representation. First, a policy file Policy is parsed
and mapped through a policy explanation file Fe and a permission mapping file Fp . Fe
includes meta information such as the format of the policy and subject/object attributes
in the policy. The policy format can be binary or text and is organized in certain order. The subjects are users or processes and objects are system resources such as files,
data, port or labels specifying these resources. Fp states operations between subjects
and objects that are mapped to write(), read(), or call(). For instance, if a subject has an
operation to get the attribute of an object, the operation is mapped to read(). In addition,
Fs and Fo files separately define subject and object classifications in the system. After
parsing the policy, a canvas is drawn and divided into different areas, on which nodes
representing policy entities are drawn and relationships between them are expressed
with arrows. Also, during the execution of this algorithm, policy rules are stored as
attributes for corresponding graph nodes and arrows.
Violation Identification with Graphical Queries. According to our domain-based
integrity model, we need to investigate information domain, domain TCB, and then
identify policy violations. To complete these tasks, we design a graphical user interface to create and run queries against a policy graph, and then get required information
such as which subject should be included in the information domain. A query formulation is built with four basic components–Subject, Object, Direct flow and Indirect flow.
Figure 2 summarizes these visual components and details are elaborated as follows.
– Subject is shaped as a labelled circle node to indicate policy subject(s) in a query.
A user can define the Subject node as a single subject or a set of subjects based

Towards System Integrity Protection with Graph-Based Policy Analysis

subject

object

direct flow

73

indirect flow

Fig. 2. Policy query building blocks

on different query requirements. The label under the Subject node specifies user
defined subject names. For instance, a user can query to find a policy subject httpd,
a set of policy subjects httpd, php, and ssh, or any sequence of policy subjects “*”
from a policy graph. Color filled in the Subject node represents user defined policy
subject attribute for a query. An example of the attribute can be the name of an
information domain which is expressed in green color. Therefore, a user can create
an example query with green color in the Subject node which is labelled with httpd.
The meaning of this query is to find a subject which belongs to the information
domain and has name httpd. After the subject is found, it is changed to an expected
color (e.g., red), which is defined by the user through specifying the Subject node
line color. Also, if the query purpose is to identify policy subject(s), then wildcard
“?” should be marked on the Subject node.
– Object is represented as a labelled rectangle node to denote policy object(s) in a
query. Similarly, the label under the Object node illustrates a single policy object
(e.g., etc) or a set of policy object (e.g.,etc or bin). Also, a user can specify policy
object attribute (e.g., domain name) for a query and the color of identified policy
object in the result graph through coloring the Object node rectangle and rectangle
line, respectively. In the situation that the query purpose is to identify objects, “?”
is specified on the Object node.
– Direct flow is drawn with a link to connect Subject node and Object node and provides a way for direct information flow-based query. The label of a link represents
the intended information flow. For example, a user can query to find if a write
operation exists between a policy subject and a policy object. To specify that the
intended query is to identify a direct flow, a user can denote the Direct flow link
with “?”.
– Indirect flow is expressed in a curved link to connect Subject node and Object node.
The main purpose of indirect flow is to specify the intended information flow paths
between subjects and objects. A user can find the shortest path, all paths, or any
path. The wildcard “*” denotes all paths that can be found between subjects and
objects. If the intended query is to identify indirect flow paths, “?” should be drawn
on the Indirect flow link.
Information Domain Queries. Corresponding to two steps of information domain identification presented in Section 3.1, we propose two principles as follows.
– Name-based policy subjects or objects query To identify domain subjects and objects based on keyword, we construct subject and object queries by using their
names. Figure 3 (a) shows an example query: Identifying the subjects or objects
whose names have prefix “prefix”, and painting result nodes with green color. The
query result is shown in Figure 3 (a’), where subjects S1 , S3 and object O1 are
identified to be included in the information domain.

74

W. Xu, X. Zhang, and G.-J. Ahn

Fig. 3. Policy query examples

– Direct flow-based subjects or objects query To investigate domain subjects and
objects based on direct flows, we construct an example query shown in Figure 3
(b). The meaning of this query is: Finding the subjects or objects that can directly
flow to the initial identified domain subjects and objects (green colored), and painting result nodes with green color. The query result in Figure 3 (b’) indicates that
subjects S2 , S4 and object O2 should be added into the information domain.
Domain TCB Queries. For domain TCB identification steps described in Section 3.1,
we query TCB(d) for an information domain with following principles. After TCB(d)
queries are completed, TCB(d) subject information is saved into a file Ftcb .
– Transition-based subjects query To query a TCB(d), we first identify TCB(d)
based on subject launching relationships. The example query in Figure 3 (c) states:
Identifying and displaying the direct call flow links between domain subjects. Example result is shown in Figure 3 (c’), which displays the call transition relationships between subjects S1 , S2 and S3 . Hence, subject S3 belongs to TCB(d).
– Indirect flow-based subjects query To identify the subjects that can flow only to
initial TCB(d) as shown in Figure 3 (d), a query is constructed as follows: Identifying the subjects that belong to the information domain (green colored) can flow
only to the initial TCB(d) (red colored) with red color in result nodes. Figure 3 (d’)
indicates the example result that S4 should be added into TCB(d).
Policy Violation Queries. Before introducing our methodology for policy violation
queries, we first define a violation policy graph based on our proposed integrity model.

Towards System Integrity Protection with Graph-Based Policy Analysis

75

Fig. 4. Policy violation and modification example graphs

Definition 3. Given a policy graph G = (V, E), the subject vertices belonging to NONTCB, system TCB, and TCB(d) are represented by VN T CB , VT CB , and VT CBd , respectively. A violation policy graph Gv = (V v , E v ) for domain d is a subgraph of G where
– V v = {v : v ∈ VN T CB , ∃u : u ∈ VT CB ∪ VT CBd ∧ (v, u) ∈ E}
– E v = {(u, v) : u, v ∈ V v ∧ (u, v) ∈ E}
Figure 3 (e’) shows a violation path with a flow transition from a low integrity subject
(black) to a high integrity subject (red). To generate the policy violation graph, a query
is constructed as shown in Figure 3 (e), where we draw in the Subject node with black
color (NON-TCB) and red color (TCB), trying to identify the policy violations from
NON-TCB to TCB caused by indirect information flow. Figure 3 (e”) shows the details
of policy violation graph generation based on query. Through the query operations performed earlier, NON-TCB, TCB and TCB(d) are elaborated in a file Ftcb . Also, NONTCB, TCB and TCB(d) subject nodes are separately colored. Then we discover all flow
transitions from NON-TCB subjects to system TCB subjects or TCB(d) subjects. Note
that it is optional for a policy administrator to specify queries from NON-TCB to TCBs
through specifying the exact subject names rather than using “*”.
Policy Violation Resolutions in Graph. With a generated policy violation graph, we
introduce different approaches to modify the policy graph and remove policy violations and illustrate the expected graph result after the modification. Based on the policy
violation resolution strategies discussed in Section 3.1, other than ignoring a policy violation through adding related subjects to system or domain TCBs, we can remove the
violation by importing a filter subject. Comparing Figure 4 (a) with violation resolved
graph in Figure 4 (b), write and read operations by the NON-TCB and TCB are removed, transition relationships between subjects and the filter are added, and the policy
violations caused by NON-TCB subjects S1 and S2 are resolved. Another optional way
for resolving policy violations is to import new subjects or objects to restrict original
subjects or objects privileges. As shown in Figure 4 (c), new object O2 is introduced so
the information flows between NON-TCB and TCB are removed. Also, we can resolve
the policy violations through deleting related policy statements. Example result of the
modification is shown in Figure 4 (d), where the read operation between object O1 and
TCB subject S3 is removed to resolve policy violations between NON-TCB and TCB.

76

W. Xu, X. Zhang, and G.-J. Ahn

4 Graph-Based Policy Analysis
Our previous policy visualization tool [21] has shown the feasibility and benefits to visualize security policies with semantic substrates [5]. Extended from this, a more comprehensive policy analysis tool (GPA) is developed in our work, which implements the
query functionality for achieving high system assurance with information flow control.
In this section we use an example SELinux reference policy to show the flexibility and
efficiency of policy analysis with our tool. We use JDK1.6 and other necessary Java libraries to develop the main analysis components. We implemented graph drawing with
graph package Piccolo [6].
Applying the reference monitor-based TCB identification method to SELinux, subjects functioning as reference monitor such as checking policy and loading policy belong to system TCB. Also, processes used to support reference monitor such as kernel
and init, are included into system TCB. After reference monitor-based system TCB
identification is completed, other subject types such as restorecon are required to be
included into system TCB based on their flow transition relationship with the initial
system TCB. Table 1 shows the TCB domains that we have recognized.
As an example, we use Apache web server as a target service domain to achieve high
integrity. We first identify subjects and objects belonging to Apache domain. We then
specify Apache TCB(d), list the policy violations identified against our integrity model,
and resolve them with different principles.
Apache Domain Identification. To identify subjects and objects for Apache domain,
we first apply keyword-based identification principle. As discussed earlier, we use http
as a keyword prefix. As a result, subjects and objects such as httpd t, httpd php t
are included into Apache domain. With the flow-based identification principle, we discover all subjects and objects that have a direct flow to the initially identified Apache
subjects and objects and include them into Apache domain. Table 1 shows a selected
list of subjects and objects that we detected. Also, we use graph-based query functions
implemented in GPA to automatically identify Apache information domain. Figures 5.I
(a) and 5.I (b) show how we use graph queries to identify subjects and objects corresponding to Apache domain identification steps.
Apache TCB(d) Identification. Based on the initial TCB(d) identification principle
in Section 3, we get initial TCB(d) subjects from Apache information domain. Specifically, our analysis shows that all subject domains in Apache related policy rules include
a set of domain relationships since a domain httpd t can transit to other httpd domains such as httpd php t and so on. Thus, a subject labelled by httpd t is a predominant subject which launches other subjects in Apache server. Similarly, a subject
labelled as httpd suexec t is also a predominant subject since this domain can transit to most of other httpd domains. Naturally, httpd t and httpd suexec t are
included into Apache TCB(d). Secondly, we construct a query to find all subjects that
can transit only to the initially identified TCB(d) (shown in Figure 5.I (d)). Based on the
generated query results, httpd sysadm script t, httpd rotatelogs t and
httpd php t can transit only to httpd t and httpd suexec t other than system
TCB subjects.

Towards System Integrity Protection with Graph-Based Policy Analysis

77

Violations and Resolutions for Apache. Based on the example query drawn in
Figure 5.I (e), we automatically identify possible policy violations from NON-TCB
subjects to TCB(d) subjects in Apache service. Figure 5(II) shows the identified policy
violations, which implies that TCB(d) integrity is violated because NON-TCB subjects
have write like operations on objects that can be read by TCB(d) subjects. Due to possible high number of violations, our GPA tool can selectively show certain violations,
allowing a policy administrator to solve them based on the priority.
Adjust TCB(d). After policy violations are identified, Apache TCB(d) is required to
be adjusted and policy violations should be removed. As shown in Table 2, httpd
awstat s script t can flow to TCB(d) subjects through httpd awstats
script rw t. At the same time, it is flown in by many NON-TCB subjects through
some common types such as devtty t. Hence, we ignore the violations caused by
this awstats script and include it into TCB(d). Similar situation occurs for
httpd apcupsd cgi script t and httpd prewikka script t. However,
Table 1. Apache information domain
kernel t
mount t
hwclock t
kudzu t
syslogd t
dpkg t
lvm t

httpd staff script t
httpd unconﬁned script t
httpd prewikka script t
httpd staff script ra t
httpd user script exec t
httpd user htaccess t
httpd sys script rw t
httpd helper exec t
httpd awstats htaccess t
httpd awstats script rw t
httpd sys script ra t
httpd user script ro t
httpd prewikka content t
httpd rotatelogs exec t
http server packet t
httpd staff script rw t
httpd sysadm htaccess t
httpd sys script exec t
httpd sysadm content t
httpd sysadm script ra t
httpd awstats content t

applications
* node t (10 types)
httpd suexec t
httpd sysadm script t

System TCB
load policy t
initrc t
bootloader t
quota t
ipsec mgmt t
useradd t
automount t
passwd t
admin passwd exec t
cardmgr t
checkpolicy t
fsadm t
sshd login t
restorecon t
newrole t
klogd t
sysadm t
getty t
apt t
sshd t
logrotate t
snmpd t
ldconﬁg t
init t
local login t
setﬁles t
Identiﬁed Key word-based Apache Subjects and Objects
Apache Subjects
httpd awstats script t
httpd t
httpd rotatelogs t
httpd helper t
httpd php t
httpd sysadm script t httpd sys script t
httpd suexec t
httpd apcupsd cgi script t
httpd user script t
Apache Objects
httpd unconﬁned script ro t
httpd cache t
httpd user script rw t
httpd prewikka script ro t
httpd exec t
httpd apcupsd cgi script ra t
httpd apcupsd cgi htaccess t httpd lock t
http port t
httpd apcupsd cgi script rw t httpd tmpfs t
httpd awstats script ra t
http cache client packet t
httpd log t
httpd awstats script ro t
httpd awstats script exec t
httpd user content t http cache port t
httpd apcupsd cgi script exec t httpd staff htaccess t httpd sysadm script rw t
httpd prewikka script exec t
httpd suexec exec t httpd sysadm script ro t
httpd unconﬁned script ra t
httpd php tmp t
httpd php exec t
httpd prewikka htaccess t
httpd staff content t httpd staff script ro t
httpd prewikka script ra t
httpd squirrelmail t httpd unconﬁned script rw t
httpd prewikka script rw t
httpd sys htaccess t httpd modules t
httpd sysadm script exec t
httpd tmp t
httpd sys script ro t
httpd staff script exec t
httpd sys content t httpd apcupsd cgi script ro t
httpd unconﬁned script exec t httpd conﬁg t
httpd suexec tmp t
httpd unconﬁned content t
http client packet t httpd unconﬁned htaccess t
httpd apcupsd cgi content t
httpd var lib t
httpd user script exec t
http cache server packet t
httpd var run t
Identiﬁed Flow-based Apache Subjects and Objects
Apache Subjects
staff application
sysadm application services
user application
Apache Objects
* port t (116 types)
* fs t (38types)
* home dir t (4 types)
others
Identiﬁed Apache TCB(d)
httpd awstats script t
httpd t
httpd helper t
httpd php t
httpd prewikka script t
httpd rotatelogs t
httpd apcupsd cgi script t

78

W. Xu, X. Zhang, and G.-J. Ahn

Fig. 5. Policy violations and solving
Table 2. Policy violations of Apache domain
NON-TCB
270
270
270
6 subjects
6 subjects
25
104
16
270
270
270
3
5
270
134
5
72
72
httpd staff script t
httpd user script t
httpd sys script t
httpd unconfined script t
webalizer t
httpd apcupsd cgi script t
httpd awstats script t
httpd prewikka script t
NON-TCB
270
270
270

Policy Violations
Type:Class
TCB(d) Subject
* node t: node
TCB(d) subjects
* port t: tcp socket
TCB(d) subjects
netif t: netif
TCB(d) subjects
dns client packet t :packet
TCB(d) subjects
dns port t:packet
TCB(d) subjects
sysadm devpts t:chr file
httpd t
initrc devpts t: chr file
httpd t,httpd rotatelogs t
console device t: chr file
httpd t,httpd suexec t
devlog t :sock file
httpd t,httpd suexec t
device t:chr file
TCB(d) subjects
devtty t:chr file
TCB(d) subjects
sysadm tty device t:chr file
httpd t
urandom device t:chr file
httpd t
zero device t:chr file
TCB(d) subjects
initrc t:fifo file
TCB(d) subjects
var run t:dir
httpd t
var log t: dir
httpd t
tmpfs t:dir
httpd t
httpd staff script * t:file
httpd t
httpd user script * t:file
httpd t
httpd sys script * t:file
httpd t
httpd unconfined script * t:file httpd t
httpd sys content t:file
httpd t
httpd apcupsd cgi script * t:file httpd t
httpd awstats script * t:file
httpd t
httpd prewikka script * t:file
httpd t
Further Policy Violations Example
Type:Class
Adjusting Subject
devtty t:chr file
httpd prewikka script t
devtty t:chr file
httpd awstats script t
devtty t:chr file
httpd apcupsd cgi script t

Solve
Filter
Filter
Filter
Filter
Filter
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Modify
Ignore
Ignore
Ignore
Solve
Modify
Modify
Modify

httpd staff script t cannot be included into TCB(d) since it would lead unlimited file access for the staff services such as staff t, staff mozilla t, and
staff mplayer t.
Remove Policy Rules. Another way for resolving policy violations is to remove the related policy statements. For example, webalizer t is to label a tool for analyzing
the log files of web server and is not necessary to modify web server information. To

Towards System Integrity Protection with Graph-Based Policy Analysis

79

resolve the policy violations caused due to the write access to httpd sys content t
by webalizer t, we remove the policy rule stating write like operation between
webalizer t and httpd sys content t.
Modify Policy Rules. Many policy violations are caused because related subjects or
objects are given too much privileges. Hence, rather than just removing related policy statements, we also need to replace these subjects or objects with more restricted
rights. For example, for policy violations caused by read and write accesses to initrc
devpts t, our solution is to redefine initrc devpts t by introducing initrc
devpts t, system initrc devpts t, and * daemon initrc devpts t
(* representing the corresponding service name). Corresponding policy rules are also
modified as follows:
allow httpd t initrc devpts t:chr file {ioctl read getattr lock
write append}; is changed to
allow httpd t httpd daemon initrc devpts t:chr file {ioctl read
getattr lock write append};

Add Filter. Based on the domain-based integrity model, a filter can be introduced into
policies to remove policy violations. For example, to remove the violations caused by
http port t, we introduce a network filter subject as follows:
allow user xserver t networkfilter t:process transition;
allow networkfilter t http port t:tcp socket {recv msg send msg};

After the modification is applied, the original policy violations are eliminated. In general, to validate the result of a policy modification, we recheck the relationships between
the policy violation related domains and types. Comparing Figure 5 (c) with Figure 5
(b), we can observe that all read operations between TCB(d) and type http port t
are removed. Also, the write operations between NON-TCB and http port t are
also removed. Instead, a new domain networkfilter t is added, which has write
and read operations on http port t. Also, all TCB(d) and NON-TCB subjects can
transit to this new domain type.

5 Conclusion
In this paper, we have proposed a graph-based policy analysis framework to effectively
verify complex security policies for integrity protection, based on a proposed domainbased integrity model. We develop an intuitive visualization tool to demonstrate the
feasibility of our framework. Additionally, we discuss how we can use our framework
to analyze SELinux policies and the results demonstrate the effectiveness and efficiency
of our methodology. We believe that this is the first effort to formulate a general policy
analysis framework with graph-based approach. We are going to develop a violation
graph based ranking schema, which can be used to help resolving the policy violations.
Also, the userability of the graph-based policy analysis tool will be studied. In addition, we plan to enhance the flexibility of our approach and investigate how our policy
analysis framework and tool can be utilized with other integrity models.

80

W. Xu, X. Zhang, and G.-J. Ahn

References
1. Tresys Technology Apol., http://www.tresys.com/selinux/
2. Trusted Computer System Evaluation Criteria. United States Government Department of Defense (DOD), Profile Books (1985)
3. Ahn, G., Xu, W., Zhang, X.: Systematic policy analysis for high-assurance services in
selinux. In: Proc. of IEEE Workshop on Policies for Distributed Systems and Networks
(2008)
4. Anderson, A.P.: Computer security technology planning study. Technical Report ESD-TR73-51, II (1972)
5. Aris, A.: Network visualization by semantic substrates. IEEE Transactions on Visualization
and Computer Graphics 12(5), 733–740 (2006); Senior Member-Ben Shneiderman
6. H. C. I. L. at University of Maryland. Piccolo, http://www.cs.umd.edu/hcil/
jazz/download/index.shtml
7. Biba, K.J.: Integrity consideration for secure compuer system. Technical report, Mitre Corp.
Report TR-3153, Bedford, Mass (1977)
8. Green, M.: Toward a perceptual science of multidimensional data visualization: Bertin and
beyond (1998), http://www.ergogero.com/dataviz/dviz2.html
9. Guttman, J., Herzog, A., Ramsdell, J.: Information flow in operating systems: Eager formal
methods. In: Workshop on Issues in the Theory of Security (WITS) (2003)
10. Herman, I., Melancon, G., Marshall, M.: Graph visualization and navigation in information
visualization: A survey. IEEE Transactions on Visualization and Computer Graphics 6(1),
24–43 (2000)
11. Jaeger, T., Sailer, R., Zhang, X.: Analyzing integrity protection in the selinux example policy.
In: Proc. of USENIX Security Symposium (2003)
12. Loscocco, P., Smalley, S.: Integrating flexible support for security policies into the linux
operating system. In: USENIX Annual Technical Conference, FREENIX Track (2001)
13. Provos, N., Friedl, M., Honeyman, P.: Preventing privilege escalation. In: 12th USENIX
Security Symposium, August 2003, p. 11 (2003)
14. Sailer, R., Zhang, X., Jaeger, T., van Doorn, L.: Design and implementation of a TCG-based
integrity measurement architecture. In: USENIX Security Symposium (2004)
15. Sandhu, R.S.: Lattice-based access control models. IEEE Computer 26(11), 9–19 (1993)
16. Sarna-Starosta, B., Stoller, S.D.: Policy analysis for security-enhanced linux. In: Proceedings
of the 2004 Workshop on Issues in the Theory of Security (2004)
17. Shankar, U., Jaeger, T., Sailer, R.: Toward automated information-flow integrity verification
for security-critical applications. In: NDSS. The Internet Society (2006)
18. Smalley, S.: Configuring the selinux policy (2003), http://www.nsa.gov/SELinux/
docs.html
19. Fraser, T.: Lomac: Low water-mark integrity protection for cots environment. In: Proceedings
of the IEEE Symposium on Security and Privacy (May 2000)
20. WIKIPEDIA. Trusted computing base, http://en.wikipedia.org/wiki/
Tusted_Computing_Base
21. Xu, W., Shehab, M., Ahn, G.: Visualization based policy analysis: case study in selinux. In:
Proc. of ACM Symposium of Access Control Models and Technologies (2008)
22. Wang, H., Osborn, S.: Discretionary access control with the administrative role graph model.
In: Proc. of ACM Symposium of Access Control Models and Technologies (2007)
23. Osborn, S.: Information flow analysis of an RBAC system. In: Proc. of ACM Symposium of
Access Control Models and Technologies (2002)

Decision Support Systems 53 (2012) 234–244

Contents lists available at SciVerse ScienceDirect

Decision Support Systems
journal homepage: www.elsevier.com/locate/dss

Workﬂow composition of service level agreements for web services☆
M. Brian Blake a,⁎, David J. Cummings b, Ajay Bansal c, Srividya Kona Bansal c
a
b
c

Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, United States
Department of Computer Science, Stanford University, United States
Department of Engineering, Arizona State University, United States

a r t i c l e

i n f o

Article history:
Received 19 July 2010
Received in revised form 22 December 2011
Accepted 30 January 2012
Available online 8 February 2012
Keywords:
Service level agreements
Quality of service
Web services
Service-oriented computing

a b s t r a c t
Service-oriented architecture enables an environment where businesses can expose services for use by their
collaborators and their peer organizations. In this dynamic environment, organizations require the use of service level agreements (SLAs) to assure the quality of service (QoS) standards of services provided by their collaborators. In an ad-hoc workﬂow scenario, a business may need to perform real-time composition of existing
services in response to consumer requests. In this work, we suggest that, in parallel to traditional web service
composition, the business must also compose the existing SLAs in order to ensure the service levels that must
be guaranteed to new consumers. Ultimately, this approach to SLA composition must align with the overarching principles of the provider and the priorities of the consumer. In this paper, we introduce a model
and representations of service level agreement attributes appropriate for managing a service provider's expectations when adding new partners. Our evaluations suggest that the SLA composition can efﬁciently
run concurrently with traditional service composition.
© 2012 Elsevier B.V. All rights reserved.

1. Introduction
A service level agreement, SLA, is a technical contract between two
types of businesses, producers and consumers. A SLA captures the
agreed-upon terms between organizations with respect to quality of
service (QoS) and other related concerns. In simple cases, one consumer forms a SLA with a producer. In more complex cases, a consumer may form a SLA that deﬁnes a set of producer businesses.
Considering a service-oriented computing environment, capabilities
are shared via the implementation of web services exposed by a producer organization. The ultimate goal of service-oriented computing
is for consumers to access these shared capabilities on-demand. As
such, in cases where businesses have longstanding relationships,
such as workﬂow and supply chain environments, peer companies
that share services must be able to assure a level of service to their
underlying customers [4,8].
New speciﬁcations, such as the Web Service Level Agreement (WSLA)
and Web Service Agreement (WS-Agreement) [2] enable SLAs to be associated with an individual web service or even groups of web services.
These speciﬁcations deﬁne an eXtensible Markup Language (XML)based data model that can be used along with the Web Service Description Language (WSDL) documents that traditionally describe the
web services. These speciﬁcations provide a signiﬁcant opportunity.
☆ This paper is a substantial extension of earlier work presented in [7] and [5].
⁎ Corresponding author. Tel.: + 1 574 631 1625; fax: + 1 574 631 8007.
E-mail addresses: m.brian.blake@nd.edu (M.B. Blake),
david.cummings@stanford.edu (D.J. Cummings), srividya.bansal@asu.edu,
ajay.bansal@asu.edu (A. Bansal).
0167-9236/$ – see front matter © 2012 Elsevier B.V. All rights reserved.
doi:10.1016/j.dss.2012.01.017

Organizations can specify QoS-related concerns in concert with the functionality concerns already captured in the WSDL ﬁles. As a result, when a
new organization searches for a pertinent web service, the SLA-enhanced
WSDL ﬁle can be used to determine the appropriateness of the service to
meet the required business need. Furthermore organizations can use the
SLA-enhanced WSDL ﬁle to negotiate the QoS terms.
Although these SLA technologies and speciﬁcations present new
opportunities for service-oriented business processes, there are a
number of signiﬁcant barriers. When a consumer organization must
create a new business capability that requires the workﬂow composition of multiple web services, then that organization will also need to
understand the composite impact of the underlying SLAs. Consequently, in addition to composing web services that are functionally
compatible, the organization will need to ensure that the web services are compatible with regard to their service levels. Also, the
product of all the SLAs for a composition of web services must be
within the required threshold of feasibility as deﬁned by the end
users. As the service-oriented computing paradigm increases in popularity, the consumer will have the option of many similar services
that may meet a particular requirement. As such, the composition of
web services that is most efﬁcient for a particular business purpose
will rest on the organization's ability to understand and optimize
the corresponding composition of SLAs.
To deal with the aforementioned issues, we introduce the phrase,
workﬂow composition of SLAs. Our approach suggests the multidimensional evaluation of existing agreed-upon QoS standards in
order to predict the standards possible for the introduction of new
agreements. While the notions of multi-dimensional analysis, optimization, dynamic programming are not new [9,10,12,17,35] in this

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

work, we identify the speciﬁc SLA-based attributes that allows for the
introduction of new partners. Furthermore we develop a set of principles and the associated process that utilize the SLA information to estimate service levels. This approach favors services with clean
request/response (RPC-type) communication, generally known as
WSDL-based web services. Further investigation would be required
to assess this approach as it relates to REST-based services [37].
In this work, we investigate several research issues relevant to the
integration of web services-based workﬂow:
1. What SLA measures and principles are appropriate to support QoSbased assessment of existing service level guarantees?
2. Given a group of SLAs and knowledge about current consumer service
level needs, can an on-demand request be analyzed against existing
SLAs to guarantee a certain service level for a new consumer?
The paper proceeds in the following section with a discussion of
related work. In Section 3, we discuss how the SLA-based QoS assessment values are derived from higher-level organizational principles.
The formal details of the attributes are deﬁned in Section 4, and
how the attributes are physically captured in markup languages in
shown in Section 5. Finally, in Section 6 we evaluate the performance
SLA composition as it runs parallel with traditional service composition routines.
2. Related work
There are many related projects that investigate the general use of
SLAs for web services [18]. Some projects characterize SLA approaches to speciﬁc domains, such as military, database management,
or information systems [13,20,26,29]. There is also a large body of
work that attempts to automate the management and negotiation of
SLAs [11,16,23,27,33]. Other work attempts to use semantics to automate the negotiation of SLAs [15,25].
Our work leverages markup language (i.e. WS-Agreements) for
providing SLA measures as in other studies [1,28,30]. All related
work describes the importance of composing SLAs. In [28], their emphasis is on compatibility between user requirements and provider
constraints. Their approach suggests a promising model-based approach to assuring the compatibility.
Our work is closely related to the comprehensive work performed
by [9,10,35]. Each of these approaches investigates the QoS-based and
constrained composition of web services. Although Canfora et al. [9]
has an elegant approach that allows for the insertion and aggregation
of any user-deﬁned QoS attribute, our approach identiﬁes the speciﬁc
SLA measures that support the user-driven assessment of an environment where their SLAs dictate current system state. Table 1 shows a
survey of SLA attributes and how they are exploited in related projects speciﬁcally in the service-oriented computing domain.
Our work can be loosely classiﬁed in the body of work that looks
to automate the aggregation of QoS attributes [14,21,24,32]. The

235

uniqueness of our approach is that we consider the impacts when
new web service workﬂows must be added as they affect the existing
operational SLAs. More speciﬁcally, if a composite capability overlaps
multiple SLAs, then the characteristics of an early SLA can impact a
later SLA in the composition routine. Canfora et al., Cardoso et al.,
Zeng et al., [9,10,35] concentrate on deriving a speciﬁc composition
routine as constrained by QoS values. Canfora et al., Zhang et al.,
and Yu et al. focus on iterative multiattribute utility approaches [12]
where to focus is on the overall optimization function and less on
the details of each of the attribute. Our work attempts to consider
both consumer and producer concerns when assessing the entry of
a new workﬂow. As such, our work contains low-level details for
each service level objective such that subsequent optimization approaches use them as a model for optimization that targets each attribute at a low-level. Although [22] has a similar approach where SLAs
are aggregated formally, they do not consider consumer and producer
services independently as in our work.
In summary, we deﬁne speciﬁc SLA measures and formally integrate measures across multiple SLAs. We also deﬁne a principled process for the composition of SLAs. This work extends related work [6]
by concentrating on SLA measures in markup language ﬁles as opposed to Uniﬁed Modeling Language (UML) models. Unlike other
work in QoS-based web service composition, we attempt to classify
QoS attributes by those associated with the provider and those associated with the consumer. Another variation here is the introduction
of several high-level criteria that can be used to characterize organizations. We believe that by aggregating all lower-level attributes
into a smaller set of higher-level criteria then organizations can be
quantitatively evaluated or scored. Further evaluation in this paper
justiﬁes that such on-demand assessment of SLAs performs feasibly
in an operational environment where very large numbers of web service workﬂows exist.
3. Assessing an enterprise based on its SLAs
The typical SLA has a large number of measures and criteria. However, in this work, we attempt to choose the measures that are most
closely aligned to aggregation of a group of SLAs and ultimately
their assessment. In the operational notion of web service composition, a basic web services workﬂow system must ensure that the
input information supplied by the consumer ultimately leads to the
required actions and outputs required by that consumer. In parallel,
the workﬂow management system must ensure that the predicates
and requisites match (either by syntactical or semantic techniques)
in each step of the workﬂow. It is the operational composition routines that motivate the set of SLA attributes relevant to our work.
In order to designate which attributes that are most relevant to
our proposed innovation, we developed a set of principles important
to managing the quality of an enterprise with many business processes. The three relevant principles are Compliance (Suitability),

Table 1
Survey of research projects that consider SLA attributes for web services.
Author names

Run
time

(Blake et al.) [7] and this paper
✓
(Canfora et al.) [9]
✓
(Yu et al.) [34] Zhang et al.) [36] **
(Zeng et al.) [35]
(Cardoso et al.) [10]
(Jin et al.)[18] *
✓
(Mohabey et al.) [22]

Reputation Uptime
(Avail)

Resp time
***

Negotiation
(rebinding)

Cost
(price)

Success rate/
reliability

Problem
resolution

Maintenance

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓
✓

✓

✓
✓
✓
✓

✓
✓
✓
✓

✓
✓
✓
✓

✓
✓
✓
✓

✓
✓

* [18] list attributes, but do not develop formal approaches for composition.
** Although Canfora et al., Yu et al., and Zhang et al. do not formally deﬁne each attribute, their work focuses on an approach that allows any attribute to be aggregated within the
composition routine.
*** Response time and run time (and in other places Service Rate) have the same or different deﬁnitions. This table places those attributes into separate columns to show the
different meanings across the survey of related literature.

236

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

in this paper. As such, we assume the validity of this model as the applicability of the work to other domains, in some sense, relies on
these underlying elements. The paper proceeds with a formal deﬁnition of all attributes relevant to aggregating and assessing a group of
SLAs, and subsequently a description of how the information can be
stored physically. The ﬁnal sections discuss the approach and performance for aggregating SLAs of many business processes.

Sustainability, and Resiliency. By correlating the QoS attributes to
these principles, we have developed a list that is suitable for assessing
an organization based on existing SLAs. The three principles (illustrated in Fig. 1 as a Uniﬁed Modeling Language class diagram) are deﬁned
below in addition to their underlying SLA measures.
3.1. Compliance (suitability)

4. Aggregating and assessing service level agreements

Compliance is the principle that ensures that the consumer receives the requested composite capability at the service level that is
required. The functional notion of web service composition (as illustrated in Fig. 1) ﬁts within this principle, since the consumer speciﬁes
their required outputs of the composition. Considering SLA terms, the
composition process must assure that the aggregate cost, uptime, and
run time are compliant with the user requirements. Cost is the sum
total price of all services participating in the solution process. Uptime
is a guarantee by the service providers that their services will be
available a speciﬁed percentage of the time per day or month. Finally,
run time is the time it takes to complete the process by adding the response times of each service in the composition.

When assessing a group of web service workﬂows, the resulting
assessment must be a result of the aggregated SLA terms of the underlying services. In some cases, aggregating the SLA terms is as
straightforward as adding the measures of each of the dependent services, but in other cases the aggregate measure must be created based
on consumer requirements. Since there may be subjectivity with respect to how the aforementioned SLA measures can be aggregated
and calculated, we introduce the following formal concepts for composing the SLA measures in the context of web service composition.
4.1. Deﬁning the physical entities for assessing SLAs

3.2. Sustainability
Sustainability is the ability to maintain the underlying services in a
timely fashion. Success rate, Negotiation/renegotiation, and problem resolution are related to ensuring the process continues to execute effectively. Success rate is deﬁned by the historical rate at which a provider
successfully completes a request. A consumer will require assurance
that a particular business is capable of agreeing on contract terms (i.e.
negotiation/renegotiation) in a timely manner. Moreover, the result of
a negotiation scheme might be the development of new agreement. In
addition, the service providers must be capable of resolving highimpact problems (perhaps identiﬁed by the consumer) in a timely manner. Success rate, negotiation, and problem resolution times ensure that
a consumer can meet the demands of their end users.

Deﬁnition 1. Server
Composition in a service-oriented architecture involves a consumer collaborating with a producer. The mapping from service to resources (servers and computational units) must consider system
boundaries (i.e. which servers are dedicated, which are not), the
size of the messages and transactions, and initialization/set up costs.
Here, we illustrate an over-simpliﬁed correlation of the services to
the server. The capabilities of the producers and consumers are
hosted on servers. Each server can be characterized by its performance, uptime percentage, throughput, and the pre-notiﬁcation
time, i.e., the server informing its constituents prior to any downtime
associated with server enhancements and repairs. A server, v, can be
formally deﬁned as a tuple of these SLA measures as shown below:

3.3. Resiliency



v ¼ Perf v ; Upv ; Tiv; Tov ; PreNT v

Resiliency is the principle of a service to perform at high service
levels over an extended period of time. This principle was derived
from our work with data-centric government transactional systems.
The resiliency principle specializes many of the principles that
underly the general notion of mean time between failures (MTBF)
[3,31]. Low resiliency is represented by a service that is frequently
taken off-line for maintenance. Additional, the frequency of updates
may impede the predictability of its operation. Consumers will need
adequate notice prior to maintenance downtimes. In addition, resiliency dictates a low frequency of maintenance downtime. Peer consumers may also add comments about a particular provider and
thus create a quantiﬁed reputation. The reputation rating also inﬂuences the resiliency of a service.
The model elements of our work were derived from our literature
review and from work with collaborating stakeholders acknowledge

*

ð1Þ

where Perfv is the performance of the server measured loosely in
computations/second, Upv is the uptime percentage of the server,
Tiv is the throughput of input messages measured in bytes/second,
Tov is the throughput of output messages measured in bytes/second,
and PreNTv is the pre-notiﬁcation time measured in seconds.
Deﬁnition 2. Set of servers
Let Ω be the set of servers of all producer and consumers involved
in the collaboration.
Ω ¼ fv1 ; v2 ; v3 ;…; vn g:

1 1 1

<<has criteria>>

*

<<is a>>
<<is a>>

*

<<is a>>

Fig. 1. A taxonomy of SLA measures for web services workﬂow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

where v1 , v2 , v3 , …, vn are the producer and consumer servers. Each
vi Ω (for all i = 1 to n), is a tuple of SLA measures as described above.
Deﬁnition 3. Service
A service can be deﬁned by some function (in some cases the average) of computations (i.e. processor cycles or relevant measure for
the speciﬁc domain) it requires based on the nature of the processing
domain. This computation must incorporate length of the message it
receives when realizing its capabilities. A service, s, can be deﬁned
as the following pair:
s ¼ ðComps ; MLs Þ

ð2Þ

where Comps is the number of required computations of the service
and MLs is the message length measured in bytes. The intention of
this formalization is to illuminate conceptually the connection between the services and the servers that host them.
Deﬁnition 4. Set of services

237

consumer concerns. The authors work closely with a federal government organization that leverages data-centric web services to develop the list of attributes that are most closely related to services
concerned with sharing data from distributed information stores.
Hence, the SLAs here consists of uptime of the composite service,
the allowable run time, the service response time, the subscription
cost or price, the maintenance pre-notiﬁcation time, and the renegotiation time of the agreement.
Let AWf represent the composite SLA that is calculated by composing the set of all agreements of all relevant services, i.e., the services
on the producer servers and any other services on the consumer's
server that might impact the overall system-wide assessment.
Let APS represent the set of agreements of the services on the producer's servers that are involved in the composition.
APS ¼ fAPS1 ; APS2 ; APS3 ;…; APSm g
Let ACS represent the set of agreements of the services on the consumer's servers that may impact the composite agreement AWf.
ACS ¼ fACS1 ; ACS2 ; ACS3 ;…; ACSn g

Let Γ be the set of all services involved in a composition.
Γ ¼ fs1 ; s2 ; s3 ;…; sm g
where s1 , s2 , s3 , …, sm are the services involved in the composition.
Each si Γ (for all i = 1 to m), is a pair of the number of required computations and the message length (in bytes) as described above.
Deﬁnition 5. SLA of a service
SLA of a service consists of speciﬁc web service-oriented measures, as deﬁned in the lower elements in the taxonomy in Fig. 2.
Let Asi be the SLA of a service si. Asi can be represented as a tuple
shown below:


Asi ¼ Upsi ; RTimesi ; SRespsi ; Cost si ; PreNT si ; RenegotT si; Repsi; Relsi
ð3Þ
where Upsi is the uptime of the service speciﬁed by the agreement,
RTimesi is the allowable run time, SRespsi is the allowable service response time, Costsi is the subscription cost or price of the service,
PreNTsi is the maintenance pre-notiﬁcation time, RenegotTsi is the renegotiation (expiration) time of the agreement, Repsi is the reputation
of the service and Relsi is the reliability rating of the service.
4.2. Generating aggregate SLAs across workﬂows
In this context, we deﬁne a web service workﬂow as a preestablished process (as deﬁned by an SLA) between a consumer and
provider (consisting of a group of web services). At composition
time, this web service workﬂow is not yet an active process or instance, but the speciﬁcation of all pre-established partnerships or
agreements of organizations to share web services. The SLA for a
body of many web service workﬂows is obtained by composing the
set of SLAs of all the services participating in each composition. This
process is similar to the traditional QoS-based service composition
process although QoS measures must be separated by provider and

4.2.1. Composite service uptime (UpWf)
The uptime for the composite SLA must be less than the minimum of:
• The uptime Upvi for consumer's server vi Ω and
• The agreed uptimes of the producer's services in APS = {APS1, A
…, A PSm}
The relationship can be shown as:
UpWf bminðUpvi ; mini¼1tom ðUpPSi ÞÞ

4.2.2. Composite run time (RTimeWf)
The time required for an individual service to complete its execution
can be considered the task time. The repeatability of the task time while
the service is in commission translates to the run time. For a web service,
the run time is a function of the internet connection, the internal network connection, the hosting hardware, and the software service. Run
time is deﬁned by the provider or consumer which is captured within
the SLA. The combined run time is illustrated in Fig. 2.
The run time for the composite SLA must be less than the minimum of:
• The producer server throughput divided by the message lengths of
service input and output
• The consumer server throughput divided by the message lengths of
service input and output
Let RTimeC represent the run time of the consumer. Including the
sum of run times already guaranteed to others is important to avoid
the impact of a voluminous load from external operations. The run
time of the consumer server is obtained by dividing the server
throughput TC by the message length of the consumer services MLCS.
RTimeC ¼ T C =MLCS

RTimeP ¼ ∑i¼1tom RTimePSi −∑i¼1ton RTimeCSi
Network

System

ð5Þ

ð6Þ

Let RTimeP represent the run time of all the stakeholders, i.e., the
producers, p. It is the difference of the run times of the sum, m, of all services on the provider-side and the sum, n, of all client-side services.

SLA Task Time (RunTime)

Internet

PS2,

ð7Þ

Service

The run time for the composite SLA is represented as:
Fig. 2. Task time and run time.

RTimeWf bmin ðRTimeP ; RTimeC Þ

ð8Þ

238

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

4.2.3. Composite service cost (CostWf)
The cost of using a web service can be aggregated to understand the
price of an entire process. The cost of an individual service is the sum of:
• The bandwidth used (i.e. the product of the usage frequency and
the message length) multiplied by the cost per byte/sec of bandwidth, which is a predeﬁned constant BC. BC could be zero if bandwidth costs are negligible to the service provider.
• The computations used multiplied by the cost of computation/sec,
which is some predeﬁned constant CC. As above, CC could be zero
if computation costs are negligible to the provider.
• The sum of the costs of all dependent services.
This results in the following equation:

 

Cost Wf ≥ RTimeWf  MLCS  BC þ RTimeWf  C S  CC
ð9Þ

þ ∑i¼1tom Cost PSi

• The reputation for the consumer server vi
• The average reputation for all the dependent services
This relationship can be illustrated as:
RepWf ¼ averageðRepvi þ ð∑i¼1tom RepPsi Þ=mÞ

ð12Þ

4.2.7. Composite service reliability (RelWf)
Reliability in traditional QoS-based web service composition has
been calculated using several approaches in related work [19]. Unlike
other approaches, this approach deals with SLA speciﬁcations which
tend to be the worst case agreement between consumer and provider. As such, we believe closest estimate is to aggregate the values by
calculating the minimum of:
• The reliability for the consumer server vi
• The minimum of reliabilities for all the dependent services
This relationship can be illustrated as:

4.2.4. Composite service pre-notiﬁcation time (PreNTWf)
When a service must be disabled for maintenance, organizations
must inform their collaborators. The minimum time for notiﬁcation
before maintenance begins is calculated as the minimum of:
• The time of notiﬁcation for the consumer server vi
• The minimum of all notiﬁcation times agreed upon for all dependent services

Deﬁnition 6. Composite SLA
The composite SLA, AWf is a tuple of all the aggregated SLA measures as shown:

ð10Þ

4.2.5. Composite service renegotiation time (RenegotTWf)
Negotiation/renegotional time is the guarantee giving by the provider and consumer that deﬁnes how quickly a request for new QoS
attributes will be acknowledged the other party. The renegotiation
date for this agreement must be after the predeﬁned constant waiting
time, WT, with respect to each other's agreement to which this server
is a party. This notion of renegotiation time is illustrated in Fig. 3. The
equation below must hold for each agreement ACSi ACS to which the
service provider is a party. This test can be shown as:
f or all i ¼ 1 to n; Renegot T Wf ¼ maxðRenegotT CSi þ WT Þ

ð13Þ



AWf ¼ UpWf ; RTimeWf ; SRespWf ; Cost Wf ; PreNT Wf ; Renegot T Wf ; RepWf ; RelWf

This relationship can be illustrated as:
PreNT Wf ≤minðPreNT vi ; mini¼1tom ðPreNT PSi ÞÞ

RelWf ≤minðRelvi ; mini¼1tom ðRelPSi ÞÞ

ð11Þ

Problem resolution is not formally deﬁned because the calculation
is a similar formula as renegotiation. As a deﬁnition (shown in Fig. 4),
problem resolution can be deﬁned as the sum of the time for recognizing the error, the time that it takes for a provider to acknowledge
the error, and the actual time for resolving the problem.
4.2.6. Composite service reputation (RepWf)
In a service composition scenario, reputation is deﬁned as a numeric score on a relative scale for a web service as captured by consumers and providers in a web service repository. The Reputation
for the composite service is calculated as the average of:

where the aggregated SLA measures UpWf, RTimeWf, SRespWf, CostWf,
PreNTWf, RenegTWf , RepWf , RelWf are obtained from the relations
shown in Eqs. (5), (8), (9), (10), (11), (12), and (13) respectively.
5. Representing SLA attributes as WS-Agreements (WSAG)
In order to store and manage SLAs, the attributes must be represented in a format conducive for distributed data management. XML
is a language that allows complex information to be represented
with embedded metadata. An XML-based approach to representing
SLA information facilitates quick interpretation of data and using
translation techniques, such as the eXtensible Stylesheet Language,
XSL, allows the comparison and aggregation of the underlying information. WSAG is an XML-based language that is deﬁned with SLA attributes. A brief background is discussed here, but more information
can be found at [21]. In a WSAG document, the data are represented
hierarchically underneath the notion of an agreement. An agreement
can be further speciﬁed with name or identifying string. An agreement can also be described by its context. Context information includes the name of the consumer and producer, the timeframe by
which the agreement is valid, and other related template information.
Each agreement encapsulates a list of terms. Terms describe the information of the services that are included.
Of most importance to this work are the guarantee terms. Guarantee terms consist of a service scope which contains the service names
of the speciﬁc service relevant to the guarantee. The service level objective contains a predicate for the metrics that quantitatively deﬁne

SLA Renegotiation Time
SLA Problem Resolution Time

Request

Negotiation
(Wait Time)

Fig. 3. Aspects of renegotiation time.

Error
Acknowledge
Recognition
ment

Resolution Time by
Service

Fig. 4. Aspects of problem resolution.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

the guarantee. The service level objective contains the parameter
name, value, and unit of measure. As an example, the SLA attributes
of uptime and maintenance are shown in Table 2. Uptime is a common attribute for SLAs. As described earlier, an uptime SLA guarantees the availability of a service by percentage over a designated
period of time. The other SLA metric, maintenance notiﬁcation time,
is not as universally used as uptime. The example in Table 2 shows
that maintenance notiﬁcation time is also straightforward with regard to representation in the WS-Agreement notations.
Capturing service level agreements in XML-based notations allows
the SLA attributes to be represented in a format similar to the WSDL
ﬁles that represent the operational speciﬁcations of the service.
WSAG can be transported and negotiated along with WSDL ﬁles.
This represents a beneﬁt if a service stakeholder wants to evaluate
multiple services, side-by-side. Another beneﬁt of capturing SLA attributes in XML-based ﬁles is the ability of enhancing attributes with semantics. It is possible that organizations will name attributes with
their own specialized naming schemes. Semantics would allow disparate organizations to mediate SLA attributes that are the same but
may be named differently. This approach leverages the numerous
projects that use semantics to highlight web services for mediation.
However, a detriment of capturing these attributes in XML-based notations occurs when organizations have pre-established agreements. It is
likely that, in a service-oriented computing environment, coalitions of
businesses will form similar to partnerships that occur in traditional businesses. In such cases, the overhead of descriptive tags may be unnecessary. In addition, SLA constraints may become more comprehensive as
the partnerships enhance their coordination and negotiation. These ﬁles
may become too cumbersome for semi-automated manipulation where
human inspection may be a required step in overall process.
6. Creating aggregated SLAs on-demand
The formalization in previous sections shows the necessary measures for creating SLAs for new business workﬂows that reﬂect the effects of other workﬂows existing at the same organization. The
authors collaborated with The MITRE Corporation for the development
of a framework for defense information systems organizations shown in
Table 2
Sample SLA attributes shown in WS-Agreement representations.
WS-Agreement for uptime
b wsag:GuaranteeTerm wsag:Name = "uptimePref" wsag:Obligated = "ACME">
bwsag:ServiceScope >
b wsag:ServiceName > AcmeService1b/wsag:ServiceName >
b/wsag:ServiceScope >
bwsag:ServiceLevelObjective >
b wsag:predicate type = "greater">
bwsag:parameter>job:uptimePercentageb/wsag:parameter >
bwsag:value>5b/wsag:value >
bwsag:unit>time:secondsb/wsag:unit >
b/wsag:predicate >
b/wsag:ServiceLevelObjective >
b/wsag:GuaranteeTerm >
WS-Agreement for Maintenance
b wsag:GuaranteeTerm
wsag:Name="maintenanceNotiﬁcationPref"wsag:
Obligated="ACME">
bwsag:ServiceScope >
b wsag:ServiceName>AcmeService1b/wsag:ServiceName >
b/wsag:ServiceScope >
bwsag:ServiceLevelObjective >
b wsag:predicate type="greater">
bwsag:parameter>job:maintPreNotiﬁcationb/wsag:parameter >
bwsag:value>7b/wsag:value >
bwsag:unit>time:daysb/wsag:unit >
b/wsag:predicate >
b/wsag:ServiceLevelObjective >
b/wsag:GuaranteeTerm >

239

Fig. 5. These organizations host large numbers of web services that provide battleﬁeld information such as weather information, force location
and tracking information, and satellite telemetry. Each of the various
defense forces (Army, Navy, Air Force, etc.) provides access their web
services via this shared portal. The sources vary in their guaranteed service level objectives. The defense information systems organizations are
devising portals that manage SLAs in governance database while recording historical service level information. The approaches devise in
this paper are incorporated within the portal logic. As such, when producers provide new services and when clients gain access to existing
services, SLAs are veriﬁed for validity.
In such an ad-hoc environment, it is important to understand if
the SLA composition process can be performed efﬁciently enough to
support the real-time insertion of new partners looking to exploit
existing services. In this work, we deﬁne an integrated process for
workﬂow-based SLA composition when suggesting new SLAs. This
process can be decomposed into two steps, SLA composition and evaluation. These two steps are illustrated in Fig. 6.
The SLA composition step is similar to traditional QoS-based web service composition approaches. In the evaluation step, user preferences are
used to prioritize the list of candidate service chains. Numerous dynamic
programming techniques can be used to achieve this step. We present a
unique approach where instead of acquiring speciﬁc QoS value expectation from the user, instead user's priorities are collected. A quality score
is generated based on the user's preferences. Our evaluation shows that,
in real-time operations, the composition and evaluation steps are feasible
considering a large number of existing business processes.
6.1. Composition: building candidate workﬂows
In order for SLA measures to be useful for both real-time operations and decision support, the web services workﬂow generation
must integrate traditional web service composition with the SLA
composition procedures. The SLA composition procedure must be integrated at each step and also applied to the workﬂow as a whole
once the full process is generated. We deﬁned the information provided by the user to be the user.predicate and the desired outcome
to be the user.reqresults. The step.predicate is the set of information
used to select subsequent services in the composition. At the initiation of a composition routine, the user.predicate is equivalent to the
step.predicate. These relationships are illustrated in Fig. 7.
We introduce an integrated procedure that combines standard web
service composition with SLA composition. Table 3 shows the pseudocode of the integrated process. The composition process has a main integrated process, IntegratedComp(). The StepCompose() process occurs
at each step and WorkﬂowChk() process occurs once the required information and actions are realized with the execution of the sequence of
web services. The ComposeSLA() process is the computation mechanisms that implement the SLA aggregation procedures.
6.2. Evaluation: prioritizing SLA compositions
In the prior section, candidate service chains are generated that
meet the functional and SLA requirements of the user. Nevertheless,
at this point, there are still multiple chains that can fulﬁll a capability.
As such, there remains an open requirement to sort the chains based
on quality and choose the best chain in the group.
In order to prioritize service chains, users are asked to provide a priority, Pr, for each attribute with respect to their environment, where
Rr = {Pr1, Pr2, Pr3,…, Prn}. The priorities represent the rank ordered
SLA measures from greatest to least importance. The priorities relate
to the corresponding set of SLA measures, where SLA = {SLA1, SLA2,
SLA3,…, SLAn}. A SLA attribute has the best possible value, B. Our approach will strongly consider chains that perform favorably with respect to the user's preferences. After evaluation, GS and GW represent
the quality score for the service and workﬂow, respectively.

240

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

Certification
Module

WS
Interface

Governance
Governance
DB

Concerns ....
Module
WS interface

Web
Service
DB

WS
Interface

WS interface

QoS-Repos/Gov
Manager

WS-Repos
Manager

Historical
QoS
DB
WS interface

QoS
Manager

Portal User Interface

Introduce new web
service that
require new SLA
generation in
Governance DB

Service Provider Service Consumer

Many different
providers/clients
on the Portal

Require regular
access to existing
service that affects
existing SLAs

Fig. 5. A practical operational scenario.

As such, the quality score for an individual service can be deﬁned
by weighing the user's priority with respect to the ratio of the SLA
measure to the best possible measure for that attribute. The calculation is deﬁned as:

n 
X
SLAn −BðSLAn Þ
GS ¼
ð14Þ
Prn •
BðSLAn Þ
n¼0
The corresponding quality score for the service chain is:
GW ¼

X

ðGS Þm



ð15Þ

m¼0

This approach relies on user-supplied priority weights and perhaps lacks the precision of standard multiple criteria decision analysis

[12] which focuses on decision optimization. However, in the next
section, we demonstrate that this approach performs favorably as a
real-time assessment during the dynamic composition process.
7. Performance evaluation of the integrated composition process
In real-time operations, SLA composition will be required to occur
within a reasonable response time. In this work, we evaluate
expected response time for the composition and prioritization of
SLA measures. Our experiments are performed on a Mobile Intel Pentium 4, 2.4 GHz, 1 GB RAM, running Windows XP and the Java Runtime Environment (JRE) 6 Update 1. An initial experiment was
performed that evaluates the response when prioritizing SLA-

SLA COMPOSITION

Assess existing and new workflow chains

EVALUATION

2. Prioritize predicted SLAs based on
existing services with respect to user
preferences
Chain1.effectiveness=
WSa WSb + WSc + WSd

Chain 1

Chain2.effectiveness=
WSa WSb + WSc + WSd

WSc

WSa

WSb

WSd

ChainN.effectiveness=
WSa WSb + WSc + WSd

WSc

Chain 2
WSa

WSb

WSd
WSc

Chain n
WSa

WSb

WSd
WSc

Fig. 6. Two-step process for integrated SLA workﬂow composition.

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

step.predicate
user.predicate

241

WS

WS

WS

WS

user.ReqResult

WS

Fig. 7. Data ﬂow in a composition routine.

annotated workﬂows of web services. Based on a set of random
WSAG ﬁles (with uniformly distributed randomly generated SLA
terms and their values), we created software that generates web service objects. In this context, using a uniformly distributed, randomly
generated set of attributes for values is appropriate. This experimentation is a proof of concept that the SLA composition can execute fast
enough to appropriately be included into the real-time composition
scenarios. This is mostly an information management procedure.
The authors understand that a non-uniform list of attributes may
incur additional overhead. Here, the argument is made that datacentric organizations must keep their list of guarantees (as represented by attributes) uniform, such that their operations are consistent across each of their consumers. It is also not implausible that
the provider use such policy to ensure a predictable processing time.
Each service objects has a unique identiﬁcation codes that associates
it with the corresponding WSAG ﬁles. We duplicated identiﬁcation
codes (i.e. ids) such that increasing numbers of services have the
same ids. The process of aggregating ids was done in an attempt to

Table 3
Integrated composition pseudo-code.
IntegratedComp: Main integrated composition function
StepCompose: Function that occurs at each step
WorkﬂowChk : Process-Level Functional/SLA check
composeSLA: Function that aggregates SLA measures
PR,RenegotT,Cost : Problem Resolution, Renegotiation, Cost
RTime,Up,PreNT: Run time, uptime, maintenance
Rel,Rep: Reliability, reputation
step.predicate: Message information required to execute service
ws WSDL Object with SLA measures
user.reqresults: Message information required as output of service
service_chain: Candidate workﬂow of web services
IntegratedComp
{
step.predicate = user.predicate
WHILE (step.predicate ! = user.reqresults)
{ StepCompose() }
WorkﬂowChk()
}
StepCompose
{
FOR EACH candidate ws where
ws. message step. predicate
THEN ADD (ws) to List b ws >
FOR EACH candidate ws IN List b ws>
IF ((ws.PR b = step.PR) || (ws.RenegotT b = step.RenegotT) ||
(ws.Cost > step.Cost) || (ws.RTime b = step.RTime) ||
(ws.Up b = step.Up) || (ws.PreNT b = step.PreNT) ||
(ws.Rel b = step.Rel) || (ws.Rep b = step.Rep))
THEN REMOVE (ws) from List b ws>
ELSE {
ADD ws to List b service_chain >
ADD ws.outputs to List b step.predicate >
}
}
WorkﬂowChk
{
FOR EACH service_chain
IF (chain. outputs user.reqresults ) &&
(ComposeSLA (ws.PR, ws.RenegotT ,ws.Cost, ws.Up,
ws.PreNT, ws.RTime, ws.Rep, ws.Rel ) b user.SLA))
THEN ADD (service_chain) to Candidate List
}

simulate composition as a prerequisite step to the actual SLA composition. Each web service object was populated with six SLA measures. Although there are more attributes, we decided to experiment with 6
with the expectation that more attributes would scale consistently.
Our experimentation searches the repository of web service objects,
composes services of the same id, and then prioritizes the resulting
chain. This experiment was executed on a repository with varying
sizes from 100 to 100,000 services. The workﬂow size (number of services in a chain) was also varied. The number of services per chain is
varied from 10 to 100,000 services within a repository of 1,000,000 services. Fig. 8 shows the results of this ﬁrst experiment. Response time
represents the average time required for the proposed algorithm to correlate the SLA attributes to generate a composite measure for a particular web service composition routine. Considering a reasonable
workﬂow of web services of 100 interconnected services or less, a response time of 1.6 ms per service chain is promising. As a variation of
the response time experimentation, we also investigated how the size
of the repository affects the performance of our algorithm. Fig. 9
shows the response time of our algorithm as the repository increases.
The workﬂow chain of the composition request is held constant at 10
services but the repository increases from 100 to 100,000. In the results
of this experiment, search time is also considered. Given the largest repository of 100,000 services, the SLA composition time (including the
processing time for discovering the relevant services (i.e. 10 services)
is 30.2 s. Although the results are favorable in a simulated environment,
the reader should understand that many other conditions with regard
to performance variations and real-time factors on open systems reduce
the conﬁdence of these results.
In a second experiment, we evaluated the performance by varying
the number of SLA attributes that are used for calculation. In this
paper, we experiment with up to six attributes, but we expect that
other attributes will be required to extend this approach in the future.
As such, it is important to understand the overhead associated with
adding a new SLA attribute for real-time operations. Although it is understood that the performance increases as the computing hardware
is improved, we are generally interested in how the approach scales
as the number of attributes increases in a ﬁxed-size repository.
Fig. 10 shows the search and calculation time when varying the
numbers of attributes (i.e. the number of attributes tested for each).
Although this graph has a high concentration of search time (~ 90%),
it is clear that the performance degrades favorably (linearly) at less
than 12 ms per attribute (i.e. the calculation time increases less than
1 millisecond for the addition of each new attribute). Another variation of this experiment, also shown in Fig. 10, considers the impact
of SLAs that process fewer attributes. In some cases, service agreements may only consider a subset of the total possible guarantee conditions. As such, less attributes may be stored per service (i.e.
Attributes Stored). The Attributes Stored measure in Fig. 10 shows
that there is only a slight advantage for service-oriented providers
to be less stringent. Another variation considers increasing performance of SLA composition by limiting the number of attributes that
a service composition considers. The major question is “Can runtime performance increase if the business management system only
considers a subset of possible SLA attributes?”. Experimentation
shows that even at the most extreme case, if a service provider only
allows services to be constrained by one attribute, the performance
is only improved by approximately 30%. Predictably, as shown in

242

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

Performance Evaluation by
Number of Services per ID
(constant 100000 services with search time subtracted)

SLA Composition Time
(Milliseconds)

1000

100

10

1
10

100

1000

10000

0.1

Services per ID
Fig. 8. The performance of the SLA prioritization function as the number of services per workﬂow increase (service discovery time excluded).

Fig. 10, the improvement in performance decreases as more attributes
are considered.
8. Conclusion
When provider organizations expose their services for consumption by their peers, it is important for them to understand what
they are guaranteeing. Moreover, consumers that receive such commitments must abide by their own commitments such that the providers can meet their guarantees to all consumers. In this work, we
introduce a method of assessing an organization based on both pending and existing SLAs. Since the estimation of QoS measures for web
services has been investigated in great detail, in this paper, we
make a varied contribution. Our work builds on the existing studies
by considering the QoS guarantees, as captured in SLAs, such that provider and consumer concerns can be modeled independently. This
work also considers that SLA assessment occurs in a separate process
than standard QoS estimation at service composition time. Here, we
introduce high-level criteria that can be created from the aggregation
of a comprehensive list of lower-level QoS-based attributes. This variation to related work facilitates automated cross-enterprise SLA negotiation. In this paper, we deﬁne a two-step process for composing
SLAs and evaluating their efﬁciency. Consistent with results in related

work that estimate QoS on fully operational web services, we have
found through simulated experimentation that the management of
third-party SLA information can be composed efﬁciently in parallel
with the actual operational service composition. In future work, we
plan to implement our approach within a real operational setting as
opposed to the simulation setting that is represented in this paper.
In the real operational setting, data will be produce using a variety
of distributions. As such, WS-Agreement ﬁles will be appended to
WSDL ﬁles and evaluated in a network environment for performance
and feasibility. In addition, we plan to extend the SLA composition
paradigm to protocols that mandate the discovery process within
web service registries. In this way, it may be possible for adaptive
software (or intelligent agents) to negotiate SLAs, in real time, while
they perform on-demand, service discovery.
Acknowledgment
This work was beneﬁted by the participation of Dr. M. Brian Blake
in the Service Level Agreement Technical Exchange Meeting held at
The MITRE Corporation on July 2006 in McLean, Virginia. In addition,
the service discovery approach/software used in this work was partially funded by the National Science Foundation under award number 0548514.

Performance Evaluation by Repository Size
(constant 10 services per workflow chain)

SLA Composition Time
(Seconds)

100

10

1
100

1000

10000

100000

0.1

Number of Services
Fig. 9. The performance of the SLA prioritization function as the repository increases (service discovery time included).

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

243

Performance Evaluation by
Attributes Stored and Attributes Tested
(100,000 services, 10 services per chain)
SLA Composition Time
(Milliseconds)

80
70
60
50
40
30
20
10
0

1

2

3

4

5

6

Attributes
Attributes Stored

Attributes Tested

Fig. 10. The performance overhead associated with the addition of new attributes.

References
[1] P. Alipio, S. Lima, P. Carvalho, XML service level speciﬁcation and validation, Proc.
of the 10th IEEE Symposium on Computers and Communications (ISCC), June
2005, pp. 975–980.
[2] A. Andrieux, K. Czajkowski, A. Dan, K. Keahe, H. Ludwig, T. Nakata, J. Pruyne, J. Rofrano,
S. Tuecke, M. Xu, “Web Service Agreements Speciﬁcation (WS-Agreement),” Proposed
Recommendation, Open Grid Forum (OGF) Document Number GFD-R-P.107, Mar 2007
OGF Grid Resource Allocation Agreement Protocol Working Group (GRAAP-WG)Accessible at, http://www.gridforum.org/Public_Comment_Docs/Documents/Oct-2005/WSAgreementSpeciﬁcationDraft050920.pdf.
[3] J.E. Angus, On computing MTBF for a k-out-of-n: G repairable system, IEEE Transactions on Reliability 37 (3) (1988) 312–313.
[4] M.B. Blake, B2B electronic commerce: where do agents ﬁt in? Proceedings at the
AAAI-2002 Workshop on Agent Technologies for B2B E-Commerce/AAAI Press,
Edmonton, Alberta, Canada, July 2002.
[5] M.B. Blake, Decomposing composition: service-oriented software engineers, IEEE
Software 24 (6) (Nov/Dec 2007) 68–77.
[6] M.B. Blake, A lightweight software design process for web services workﬂows,
Proc. of the 4th IEEE International Conference on Web Services (ICWS), Sept.
2006, pp. 411–418.
[7] M.B. Blake, D.J. Cummings, Workﬂow composition of service level agreements,
Proc. of the IEEE International Conference on Services Computing (SCC 2007),
July 2007, pp. 138–145.
[8] M.B. Blake, M. Gini, Guest editorial: agent-based approaches to B2B electronic
commerce, International Journal of Electronic Commerce 7 (1) (2002) 113–114.
[9] G. Canfora, G. M Di Penta, R. Esposito, F. Perfetto, M.L. Villani, Service composition
(re)binding driven by application-speciﬁc QoS, Proc of International Conference
on Service-Oriented Computing (ICSOC), 2006, pp. 141–152.
[10] A.J. Cardoso, “Quality of Service and Semantic Composition of Workﬂows,” Ph.D.
Dissertation, University of Georgia, 2002.
[11] G. Di Modica, V. Regalbuto, O. Tomarchio, L. Vita, Dynamic re-negotiations of SLA
in service composition scenarios, Proc. of the 33rd EUROMICRO Conference on
Software Engineeringand Advanced Applications, August 2007, pp. 359–366.
[12] J. Dyer, Maut — multiattribute utility theory in multiple criteria decision analysis:
state of the art surveys, International Series in Operations Research, Management
Science, vol. 78, Springer, 2005, pp. 265–292.
[13] T. Falkowski, S. Vob, Application service providing as part of intelligent decision
support for supply-chain management, Proc. of the 36th Annual Hawaii International Conference on System Sciences (HICSS), vol. 3, 2003, p. 80.
[14] M. Gillman, G. Weikum, W. Wonner, Workﬂow management with service quality
guarantees, Proc. of ACM SIGMOD International Conference on Management of
Data, 2002, pp. 223–239.
[15] L. Green, Service level agreements: an ontological approach, Proc. of the 8th ACM
International Conference on Electronic Commerce (ICEC), August 2006,
pp. 185–194, Fredericton, Canada.
[16] D. Greenwood, G. Vitaglione, L. Keller, M. Calisti, Service level agreement management with adaptive coordination, Proc. of the International Conference on Networking and Services (ICNS), July 2006, p. 45–50, Silicon Valley, USA.
[17] C.-W. Hang, M.P. Singh, Trustworthy service selection and composition, ACM
Transactions on Autonomous and Adaptive Systems 6 (1) (Feb 2011).
[18] L. Jin, V. Machiraju, A. Sahai, Analysis on service level agreement of web services,
HP Technical Report, HPL-2002-180, June 2002, accessible at (2008), http://www.
hpl.hp.com/techreports/2002/HPL-2002-180.pdf.
[19] J. Ko, C.O. Kim, I. Kwon, Quality-of-service oriented web service composition algorithm and planning architecture, Journal of Systems and Software 81 (11) (November 2008) 2079–2090.
[20] H. Ludwig, A. Keller, A. Dan, R.P. King, R. Franck, Web Service Level Agreement (WSLA)
Language SpeciﬁcationAccessible at, http://www.research.ibm.com/wsla2007.

[21] M. Mecella, M. Scannapieco, A. Virgillito, R. Baldoni, T. Catarci, C. Batini, Managing
data quality in cooperative information systems, Lecture Notes in Comptuer Science 2512 (2002) 486–502.
[22] M. Mohabey, Y. Narahari, S. Mallick, P. Suresh, S.V. Subrahmanya, An intelligent
procurement marketplace for web services composition, Proc. of the IEEE/WIC/ACM International Conference on Web Intelligence, Nov. 2007, pp. 551–554.
[23] N.J. Muller, Managing service level agreements, International Journal of Network
Management 9 (3) (May 1999).
[24] F. Naurmann, U. Leser, J.C. Freytag, Quality-driven integration of heterogeneous
information systems, Proc. of the 25th International Conference on Very Large
Databases, September 1999, pp. 447–458, Edinburgh, Scotland, UK.
[25] N. Oldham, K. Verma, A.P. Sheth, F. Hakimpour, Semantic WS-Agreement partner
selection, Proc. of the 15th International World Wide Web Conference (WWW),
2006.
[26] F.R. Reiss, T. Kanungo, Satisfying database service level agreements while minimizing cost through storage QoS, Proc. of the IEEE International Conference on
Services Computing (SCC), July 2005, pp. 13–21.
[27] A. Sahai, V. Machiraju, M. Sayal, A. Moorsel, F. Casati, Automated SLA monitoring
for web services, Proc. of the IEEE/IFIP International Workshop on Distributed
Systems: Operation and Management (DSOM), October 2002, pp. 28–41, Montreal,
Canada.
[28] D. Skene, Lamanna, W. Emmerich, Precise service level agreements, Proc. of the 26th
International Conference on Software Engineering (ICSE), 2004, pp. 179–188,
Edinburgh, UK.
[29] I. Sorteberg, O. Kure, The use of service level agreements in tactical military coalition
force networks, IEEE Communications Magazine 43 (11) (November 2005) 107–114.
[30] W. Sun, Y. Xu, F. Liu, The role of XML in service level agreements management,
Proc. of the International Conference on Services Systems and Services Management, June 2005, pp. 1118–1120.
[31] K.M. van Hee, L.J. Somers, M. Voorhoeve, A modeling environment for decision
support systems, Decision Support Systems 7 (1) (1991) 241–251.
[32] G. Xiaohui, K. Nahrstedt, A scalable QoS-aware service aggregation model for
peer-to-peer computing grids, Proc. of the 11th IEEE International Symposium
on Higher Performance Distributed Computing (HPDC), 2002, pp. 73–82.
[33] J. Yan, R. Kowalczyk, J. Lin, M.B. Chhetri, S.K. Goh, J. Zhang, Autonomous service
level agreement negotiation for service composition provision, Future Generation
Computer Systems 23 (6) (July 2007) 748–759.
[34] T. Yu, K.J. Lin, Service selection algorithms for composing complex services with
end-to-end QoS constraints. Proc. 3rd International Conference on Service Oriented
Computing (ICSOC2005), The Netherlands Amsterdam, 2005.
[35] L. Zeng, B. Benatallah, A.H.H. Ngu, M. Dumas, J. Kalagnanam, H. Chang, QoS-aware
middleware for web services composition, IEEE Transactions on Software Engineering 30 (5) (May 2004) 311–327.
[36] Y. Zhang, K.J. Lin, J.Y.J. Hsu, Accountability monitoring and reasoning in service oriented architectures, Service-Oriented Computing and Applications 1 (2007) 35–50.
[37] M. zur Muehlen, J. Nickerson, K.D. Swenson, Developing web services choreography
standards — the case of REST vs. SOAP, Decision Support Systems 40 (1) (2005).

M. Brian Blake received the BS degree in electrical engineering from the Georgia Institute of Technology, Atlanta and the PhD degree in information technology with a concentration in information and software engineering from George Mason University,
Fairfax, Virginia. He is currently Professor of Computer Science and Associate Dean of
Engineering at the University of Notre Dame, Indiana. As a professor, he has published
more than 120 journal and refereed conference papers in the domains of workﬂow and
agent-based systems, service-oriented computing, distributed data management, and
software engineering education. His investigations cover the spectrum of software engineering: design, speciﬁcation, proof of correctness, implementation/experimentation,
performance evaluation, and application.

244

M.B. Blake et al. / Decision Support Systems 53 (2012) 234–244

Ajay Bansal received his B. Tech. degree in Computer Science from NIT (previously
known as REC), Warangal, India, M.S. in Computer Science from Texas Tech Univ.,
Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas. He is
currently a lecturer in the College of Technology and Innovation at Arizona State
University. He has over 3 years of industry experience. His research interests include
Programming Languages, Logic Programming, Declarative Programming, Automated
Reasoning, Service-Oriented Computing, Semantic Web Services, and Language-based
Security.

Srividya Kona Bansal received her B. Tech. degree in Computer Science from NIT (previously known as REC), Warangal, India, M.S. in Computer Science from Texas Tech
Univ., Lubbock and Ph.D. in Computer Science from the University of Texas at Dallas.
She is currently an assistant professor in the College of Technology and Innovation at
Arizona State University. She has over 5 years of industry experience. Her research interests include Service-Oriented Computing, Semantic Web, Software Engineering, Engineering Education, and Bioinformatics.

2014 IEEE International Congress on Big Data

Towards a Semantic Extract-Transform-Load (ETL) framework for Big Data
Integration
Srividya K Bansal
Dept. of Engineering & Computing Systems
Arizona State University
Mesa, AZ, USA
srividya.bansal@asu.edu

schema-less, and complex Big Data world of databases is a
big open challenge.
Big Data research is usually discussed in the areas of the
3V’s – Volume, Velocity, and Variety. Volume being the
storage of massive amount of data streaming in from social
media, sensors, and machine-to-machine data being
collected; determination of relevance within large data
volumes; and use of analytics to create value from relevant
data. Velocity is streaming in at unprecedented speed at
which the data is streaming in and reacting quickly enough to
deal with it in near-real time. Variety deals with the various
types of formats in which data comes in (structured, numeric,
unstructured text data, email, video, audio, stock ticker, etc.).
Big Data challenges are not only in storing and managing
this variety of data but also extracting and analyzing
consistent information from it. Researchers are working on
creating a common conceptual model for the integrated data
[3]. Managing, merging, and governing heterogeneous data
is an open research challenge that this paper focuses on.
There has been a tremendous increase of published data
on the Web. Linked Open Data community effort has led to
a huge data space with 31 billion RDF triples as shown in
[4]. This data can be used in a number of interesting Web
applications, mobile applications, and for analytics. An
interesting example application would be a Smart City
project that would integrate and use information from
various sources such as transportation, weather, social media
streams, maps, energy, real estate, policies, crime reports,
etc. Government agencies are increasingly making their data
accessible through initiatives such as data.gov to promote
transparency and economic growth [5]. For example, a
traffic jam that emerges due to an unplanned protest may be
captured through a twitter stream, but missed when
examining weather conditions, event databases, reported
roadwork, etc. Additionally, weather sensors in the city tend
to miss localized events such as flooding. These views of the
city combined however, can provide a richer and more
complete view of the state of the city, by merging traditional
data sources with messy and unreliable social media streams
thereby contributing to smart living, people, environment,
economy, mobility, and governance.
We need ways to organize a variety of data such that
common things are represented together, while the things
that are distinct can be represented as well. This will allow
effective and creative use of search/query engines and
analytic tools for Big Data, which is absolutely essential to

Abstract—Big Data has become the new ubiquitous term used
to describe massive collection of datasets that are difficult to
process using traditional database and software techniques.
Most of this data is inaccessible to users, as we need technology
and tools to find, transform, analyze, and visualize data in
order to make it consumable for decision-making. One aspect
of Big Data research is dealing with the Variety of data that
includes various formats such as structured, numeric,
unstructured text data, email, video, audio, stock ticker, etc.
Managing, merging, and governing a variety of data is the
focus of this paper. This paper proposes a semantic ExtractTransform-Load (ETL) framework that uses semantic
technologies to integrate and publish data from multiple
sources as open linked data. This includes - creation of a
semantic data model to provide a basis for integration and
understanding of knowledge from multiple sources; creation of
a distributed Web of data using Resource Description
Framework (RDF) as the graph data model; extraction of
useful knowledge and information from the combined data
using SPARQL as the semantic query language.
Keywords – Big data; Data integration; Ontology; Semantic
technolgies;

I.

INTRODUCTION

There has been an exponential growth and availability of
data, both structured and unstructured. Big Data has become
the new ubiquitous term used to describe massive collection
of datasets that is so large that it's difficult to process using
traditional database and software techniques. Big Data may
comprise of petabytes (1,024 terabytes) or exabytes (1,024
petabytes) of data consisting of billions to trillions of records
of millions of people - all from different sources (e.g. Web,
sales, customer contact center, social media, mobile data,
etc). The data is typically loosely structured data that is often
incomplete and inaccessible. Big Data is transforming
science, engineering, medicine, healthcare, finance, business,
and ultimately society itself. Massive amounts of data are
available to be harvested for competitive business advantage,
sound government policies, and new insights in a broad array
of applications (including healthcare, biomedicine, energy,
smart cities, genomics, transportation, etc.). Yet, most of this
data is inaccessible for users, as we need technology and
tools to find, transform, analyze, and visualize data in order
to make it consumable for decision-making [1]. Research
community also agrees that it is important to engineer Big
Data meaningfully [2]. Meaningful data integration in a
978-1-4799-5057-7/14 $31.00 © 2014 IEEE
DOI 10.1109/BigData.Congress.2014.82

535
522

create smart and sustainable environments. This project
focuses on dealing with the Variety – V of Big Data more
specifically creating a semantic extract-transform-load
framework for data integration. This paper proposes the use
of semantic technologies to connect, link, and load data into
a data warehouse. This includes: (i) creation of a semantic
data model via ontologies to provide a basis for integration
and understanding knowledge from multiple sources; (ii)
creation of integrated semantic data using Resource
Description Framework (RDF) as the graph data model [6];
(iii) extracting useful knowledge and information from the
combined web of data using SPARQL as the semantic query
language [7]. This paper presents the preliminary
implementation and results using a few sample public
datasets that provide household travel data, vehicle data, and
fuel economy data.
The rest of the paper is organized as follows: Section 2
presents related work in this area. Section 3 presents a
motivating scenario showing an application of meaningful
data integration. Section 4 presents the conceptual model for
a semantic ETL framework. Section 5 presents the system
architecture and prototype implementation followed by
conclusions and future work.
II.

Data [16] that is applicable after the data integration phase.
Numerous studies are being done on the analysis of Big Data
using various algorithms such as influence-based module
mining (IBMM) algorithm [17], online association rule
mining [18], graph analytics [19], and provenance analysis
support framework [20].
The method of publishing and linking structured data on
the web is called Linked Data. This data is machinereadable, its meaning is explicitly defined, it is linked to
other external data sets, and it can be linked to from other
data sets as well [4]. Reference [21] uses these interlinked
RDF data stores from the Linked Open Data (LOD) cloud
and queries them using SPARQL to perform analysis. They
use statistical Learning predictive models to learn classifiers
from the data.
III.

MOTIVATING SCENARIO

There has been a growing demand and interest in mobile
applications for automotive industry that enhance the
driver’s experience through personalization, provide
feedback on optimizing vehicle performance, diagnosis and
part failure detection, and driver assistance. Most of these
applications rely on Big Data that is available to the public
through the cloud or as linked open data.
Consider the following scenario where a typical driver,
John gets into his car in the morning and turns on the
ignition. A built-in innovative application in the car greets
him and asks him if he is going to work based on the time of
the day. John responds by saying “yes” and the app replies
that the vehicle performance has been optimized for the trip.
The built-in system uses the GIS system, road grade data,
and speed limits data to create an optimal velocity profile. As
John starts driving and is approaching Recker road to turn
left, the app informs John about a road repair on Recker road
for a 1-mile stretch on his route up to 3pm that day. The app
suggests that John should continue driving and take the next
left on Power road. John follows the suggestion and answers
a text message that he receives from his collaborator. As he
is answering to the text message, his car drifts into the
neighboring lane. The app immediately notifies John that he
is departing from his lane and John quickly adjusts his
driving. As John approaches his workplace he drives towards
Green Lot 1 where he usually parks. The app informs John
that there are only 2 parking spots open in Green Lot 1. As
John is already running late for a meeting by 5 minutes, he
decides to directly drive to the next parking lot – Green Lot 2
and avoid spending the time looking for the 2 empty spots in
Lot 1. As John enters Lot 2 and is driving towards one of the
empty spots, he gets too close to one of the parked cars. The
app immediately warns John of a collision. John quickly
adjusts his car away from the parked cars and parks in one of
the empty spots. The app logs tracking data about John’s
style of driving on the server for future use.
In order to build such apps for automobiles, access to a
number of data sets from various sources is required. Some
of this data is real-time data that is continuously being
updated. Data related to traffic, road repairs, emergencies,
accidents, driving habits, maps, parking, fuel economy data,
household data, diagnosis data, etc. would be required.

RELATED WORK

One of the popular approaches to data integration has
been Extract-Transform-Load (ETL) as shown in [8], [9].
Authors of this work have described a taxonomy of activities
in ETL and a framework that uses a workflow approach to
design ETL activities. They used a declarative database
programming language called LDL to define the semantics
of ETL activities. Similarly there are other groups that have
used various other approaches such as UML and data
mapping diagrams for representing ETL activities, quality
metrics driven design for ETL, and scheduling of ETL
activities [10]–[13]. The focus in all of these papers has been
on the design of ETL workflow and not about generating
meaningful/semantic data.
A semantic approach to ETL technologies was proposed
in [14], [15]. In this approach semantic technologies are used
to further enhance definitions of the ETL activities involved
in the process rather than the data itself. A tool that allowed
semi-automatic definition of inter-attribute semantic
mappings, by identifying parts of data source schemas,
which are related to the data warehouse schema was
proposed in [15]. This supported the extraction phase of
ETL. The use of semantics here was to facilitate the
extraction process and workflow generation with semantic
mappings. In contrast, our approach uses ontologies to
provide a common vocabulary for the integrated data and
generates semantic data as part of the transformation phase
of ETL. This semantic data is then loaded in the data store or
warehouse for querying and analytics.
There is related work on creating a common conceptual
model for data integration to handle the heterogeneous data
coming from multiple sources. Reference [3] uses processmining techniques to handle the heterogeneity by computing
the mismatch among data sources being integrated. Work has
been done on data abstraction and visualization tools for Big

523
536

Various other apps could be built that focus on reducing
energy consumption, reducing emissions, and using existing
related data to analyze and provide useful feedback. Another
useful app is for fuel economy guidance that is based on
actual vehicle data, road conditions, traffic, and most
importantly personal driving habits. This app would show
the car’s fuel efficiency, and provide feedback on how good
or bad it is based on data from others in the community. It
also provides guidance on improving one’s personal driving
habits and thereby saving on fuel.
It is important to effectively integrate data such that the
data is tied to a meaningful and rich data model that can be
queried by these innovative applications.
IV.

CONCEPTUAL MODEL

In this section we present our conceptual model by
describing existing ETL process, our proposed idea of a
Semantic ETL process and describe the semantic technology
stack that will be used in our model.

Figure 1: Example ETL workflow

B. Semantic ETL
In this paper we introduce the use of semantic technologies
in the Transform phase of an ETL process to create a
semantic data model and generate semantic linked data (RDF
triples) to be stored in a data mart or warehouse. The
transform phase will still continue to perform other activities
such as normalizing and cleansing of data. Extract and Load
phases of the ETL process would remain the same. Figure 2
shows the overview of activities in semantic ETL. Transform
phase will involve a manual process of analyzing the
datasets, the schema and their purpose. Based on the
findings, the schema will have to be mapped to an existing
domain-specific ontology or an ontology will have to be
created from scratch. If the data sources belong to disparate
domains, multiple ontologies will be required and alignment
rules will have to be specified for any common or related
data fields.

A. Extract-Transform-Load (ETL) Process
Extract-Transform-Load (ETL) process in computing has
been in use for integration of data from multiple sources or
applications, possibly from different domains. It refers to a
process in data warehousing that extracts data from outside
sources, transforms it to fit operational needs, which can
include quality checks, and loads it into the end target
database, more specifically, operational data store, data mart,
or data warehouse. A number of tools facilitate the ETL
process, namely IBM Infosphere [22], Oracle Warehouse
Builder [23], Microsoft SQL Server Integration Services
[24], and Informatica Powercenter for Enterprise Data
Integration [25]. Talend Open Studio [26] and Pentaho
Kettle [27] are two open source ETL products. The three
phases of this process are described as follows:
• Extract: this is the first phase of the process that involves
data extraction from appropriate data sources. Data is
usually available in flat file formats such as csv, xls, and
txt or is available through a RESTful client.
• Transform: this phase involves the cleansing of data to
comply with the target schema. Some of the typical
transformation activities involve normalizing data,
removing duplicates, checking for integrity constraint
violations, filtering data based on some regular
expressions, sorting and grouping data, applying built-in
functions where necessary, etc. [8]
• Load: this phase involves the propagation of the data into a
data mart or a data warehouse that serves Big Data.

C. Technology Stack
Semantic web technologies facilitate: the organization of
knowledge into conceptual spaces, based on their meanings;
extraction of new knowledge via querying; and maintenance
of knowledge by checking for inconsistencies. These
technologies can therefore support the construction of an
advanced knowledge management system [28], [29]. The
following Semantic technologies and tools are used as part of
our Semantic ETL framework:
• Uniform Resource Identifier (URI), a string of
characters used to identify a name or a web resource.
Such
identification
enables
interaction
with
representations of the web resource over a network
(typically the Web) using specific protocols.
• Resource Description Framework (RDF) [6], a general
method for data interchange on the Web, which allows
the sharing and mixing of structured and semi-structured
data across various applications. As the name suggests,
RDF is a language for describing web resources. It is

Most ETL tools provide a graphical interface to create a
workflow of ETL activities and automate their execution.
Figure 1 shows an example workflow created by Talend to
transform and load NASA Aviation Safety Reporting System
data into a database.

524
537

Figure 2: Overview of Semantic ETL Process

•

•

•

used for representing information, especially metadata,
about web resources. RDF is designed to be machinereadable so that it can be used in software applications
for intelligent processing of information.
Web Ontology Language (OWL) [30] is a markup
language that is used for publishing and sharing
ontologies. OWL is built upon RDF and an ontology
created in OWL is actually a RDF graph. Individuals
with common characteristics can be grouped together to
form a class. OWL provides different types of class
descriptions that can be used to describe an OWL class.
OWL also provides two types of properties: object
properties and data properties. Object properties are
used to link individuals to other individuals while data
properties are used to link individuals to data values.
OWL enables users to define concepts in a way that
allows them to be mixed and matched with other
concepts for various uses and applications.
SPARQL – a RDF Query Language [7], which is
designed to query, retrieve, and manipulate data stored
in RDF format. SPARQL allows for a query to consist
of triple patterns, conjunctions, disjunctions, and
optional patterns. SPARQL allows users to write queries
against data that can loosely be called "key-value" data,
as it follows the RDF specification of the W3C. The
entire database is thus a set of "subject-predicate-object"
triples.
Protégé Ontology Editor - Protégé is an open-source
ontology editor and framework for building intelligent
systems [31]. It allows users to create ontologies in
W3C’s Web Ontology Language. It is used in our
framework to provide semantic metadata to the schema
of datasets from various sources.

V.

IMPLEMENTATION

In this section we present our prototype implementation
of the semantic ETL process using public datasets on
National Household travel survey data and EPA’s Fuel
Economy data. These data sets were chosen to build a
prototype for a proof-of-concept of the semantic ETL
framework.
Table 1: Sample data fields from National Household data set

525
538

A. Data sets
Our first data source is the National Household Travel
survey (NHTS) data for the year 2009 published by the U.S.
Department of Transportation. This data was collected to
assist transportation planners and policy makers who needed
transportation patterns in the United States. The dataset
consists of daily travel data of trip taken in a 24-hour period
with information on the purpose of the trip (work, grocery
shopping, school dropoff, etc.), means of transportation used
(bus, car, walk, etc.), how long the trip took, day of week
when it took place, and additional information in case a
private vehicle was used. For private vehicle driver
characteristics and vehicle attributes were all also collected.
This data was collected for all areas of the country, urban
and rural. This dataset has been used by research community
to study relationships between demographics and travel,
correlation of the modes of transportation, amount, or
purpose of travel with the time of the day and day of the
week. We chose this dataset to integrate the private vehicles
used by members of the household and fuel economy of
those corresponding vehicle models by getting that
information from another dataset on Fuel Economy.
The second data source is the U.S. Environmental
Protection Agency’s (EPA) data on Fuel Economy of
vehicles. This dataset provided detailed information about
vehicles produced by all manufacturers (make) and models.
It provided detailed vehicle description, mileage data for
both city drive and highway drive, mileage for different fuel
types, emissions information, and fuel prices that included
regular, midgrade, and premium gasoline, diesel, electric,
lpg, e85, and cng. This data was obtained as a result of
vehicle testing conducted by EPA’s National Vehicle and
Fuel Emissions Lab and the manufacturers with an oversight
by EPA. Table 1 & 2 show sample data fields from the
National Household Travel dataset. Table 3 shows sample
data fields from the Fuel Economy dataset.

Table 2: Sample fields from National Household Vehicle data

Table 3: Sample data fields for EPA Fuel Economy data

B. Semantic Data Model generation
We used Protégé OWL [31] editor for ontology
engineering and created a data model that comprised of
primary classes Person, Household, Vehicle at the top most
level. All the fields in the datasets were modeled as classes,
subclasses, or properties and connected to the primary
classes via suitable relationships. The ontology had both
object properties and data properties depending on the type
of data field being represented. Figure 3 shows a partial
semantic data model with the primary classes and properties
and their relationships.

D. Semantic Querying
We tested the integrated data by querying it using the
RDF Query language SPARQL [7]. The Java open source
framework for building Semantic Web and Linked data
applications, Apache Jena [33], was used for executing
SPARQL queries. An application was built that loads the
ontologies and data, checks for conformity of data with the
vocabulary, and then run SPARQL queries against the data.
A sample list of queries is provided in Table 6. This is an
ongoing project and this initial version was built to test the
proposed concept of a semantic ETL framework. We will
continue building in this data set by pulling information from
a variety of sources and analyzing the data for interesting
results.

C. Semantic Instance Data generation
The datasets were obtained from the sources in CSV
format. An XML tool called Oxygen XML editor [32] was
used to convert the CSV data into OWL instance data
conforming to the vocabulary defined by the ontology. Table
4 shows a couple of sample instances from the Household
dataset and table 5 shows a couple of sample instances from
the Fuel Economy dataset.

526
539

Figure 3: Semantic data model

Table 5: Sample OWL Fuel Economy Instance data
Table 4: Sample OWL Household Instance data

Table 6: Sample Queries

527
540

VI.

CONCLUSIONS AND FUTURE WORK

[7] “SPARQL Query Language for RDF.” [Online].
Available: http://www.w3.org/TR/rdf-sparql-query/.
[Accessed: 28-Feb-2014].
[8] P. Vassiliadis, A. Simitsis, and E. Baikousi, “A
Taxonomy of ETL Activities,” in Proceedings of the
ACM Twelfth International Workshop on Data
Warehousing and OLAP, New York, NY, USA, 2009,
pp. 25–32.
[9] P. Vassiliadis, A. Simitsis, P. Georgantas, M.
Terrovitis, and S. Skiadopoulos, “A generic and
customizable framework for the design of ETL
scenarios,” Information Systems, vol. 30, no. 7, pp.
492–525, Nov. 2005.
[10] S. Luján-Mora, P. Vassiliadis, and J. Trujillo, “Data
mapping diagrams for data warehouse design with
UML,” in Conceptual Modeling–ER 2004, Springer,
2004, pp. 191–204.
[11] J. Trujillo and S. Luján-Mora, “A UML based approach
for modeling ETL processes in data warehouses,” in
Conceptual Modeling-ER 2003, Springer, 2003, pp.
307–320.
[12] A. Simitsis, K. Wilkinson, M. Castellanos, and U.
Dayal, “QoX-driven ETL design: reducing the cost of
ETL consulting engagements,” in Proceedings of the
2009 ACM SIGMOD International Conference on
Management of data, 2009, pp. 953–960.
[13] A. Karagiannis, P. Vassiliadis, and A. Simitsis,
“Macro-level Scheduling of ETL Workflows,”
Submitted for publication, 2009.
[14] D. Skoutas and A. Simitsis, “Ontology-Based
Conceptual Design of ETL Processes for Both
Structured and Semi-Structured Data:,” International
Journal on Semantic Web and Information Systems,
vol. 3, no. 4, pp. 1–24, 34 2007.
[15] S. Bergamaschi, F. Guerra, M. Orsini, C. Sartori, and
M. Vincini, “A semantic approach to ETL
technologies,” Data & Knowledge Engineering, vol.
70, no. 8, pp. 717–731, Aug. 2011.
[16] S. K. Bista, S. Nepal, and C. Paris, “Data Abstraction
and Visualisation in Next Step: Experiences from a
Government Services Delivery Trial,” in 2013 IEEE
International Congress on Big Data (BigData
Congress), 2013, pp. 263–270.
[17] Y. Guo, X. Shang, J. Li, and Z. Li, “Revealing the
Causes of Dynamic Change in Protein-Protein
Interaction Network,” in 2013 IEEE International
Congress on Big Data (BigData Congress), 2013, pp.
189–194.
[18] E. Olmezogullari and I. Ari, “Online Association Rule
Mining over Fast Data,” in 2013 IEEE International
Congress on Big Data (BigData Congress), 2013, pp.
110–117.
[19] M. U. Nisar, A. Fard, and J. A. Miller, “Techniques for
Graph Analytics on Big Data,” in Big Data (BigData
Congress), 2013 IEEE International Congress on,
2013, pp. 255–262.

Integration of data from various heterogeneous sources
into a meaningful data model that allows intelligent querying
is an important open issue in the area of Big Data.
Traditionally, Extract-Transform-Load (ETL) process has
been used for data integration in the industry. This paper
proposes a semantic ETL framework that uses semantic
technologies to produce rich and meaningful knowledge
around data integration and produces semantic data that can
possible be published on the web and contribute to the Web
of data. Successful creation of such a framework will be of
tremendous use to various innovative Big Data applications
as well as analytics. In order to test the proposed technique,
we used the semantic ETL process to integrate a few public
data sets with information on vehicles, household
transportation, and fuel economy. We created a simple java
application that ran SPARQL queries against the combined
semantic data. One of the challenges of this project is
ontology engineering that needs a fairly good understanding
of the data from different sources. A human expert has to
perform this step and it is often a time-consuming process.
This is an ongoing project with scope for numerous
interesting web and mobile applications that will use a
variety of data to enhance user experience. Future work also
includes looking into real-time data and how semantic ETL
can help with its integration. Apps can be built for various
domains such as automotive, aerospace, healthcare,
education to list a few.
REFERENCES
[1] E. Kandogan, M. Roth, C. Kieliszewski, F. Ozcan, B.
Schloss, and M.-T. Schmidt, “Data for All: A Systems
Approach to Accelerate the Path from Data to Insight,”
in 2013 IEEE International Congress on Big Data
(BigData Congress), 2013, pp. 427–428.
[2] C. Bizer, P. Boncz, M. L. Brodie, and O. Erling, “The
Meaningful Use of Big Data: Four Perspectives – Four
Challenges,” SIGMOD Rec., vol. 40, no. 4, pp. 56–60,
Jan. 2012.
[3] A. Azzini and P. Ceravolo, “Consistent Process Mining
over Big Data Triple Stores,” in 2013 IEEE
International Congress on Big Data (BigData
Congress), 2013, pp. 54–61.
[4] C. Bizer, T. Heath, and T. Berners-Lee, “Linked datathe story so far,” International journal on semantic web
and information systems, vol. 5, no. 3, pp. 1–22, 2009.
[5] F. Lecue, S. Kotoulas, and P. Mac Aonghusa,
“Capturing the Pulse of Cities: Opportunity and
Research Challenges for Robust Stream Data
Reasoning,” in Workshops at the Twenty-Sixth AAAI
Conference on Artificial Intelligence, 2012.
[6] P. Hayes and B. McBride, “Resource description
framework (RDF),” 2004. [Online]. Available:
http://www.w3.org/TR/rdf-mt/. [Accessed: 28-Feb2014].

528
541

[20] Y.-W. Cheah, R. Canon, B. Plale, and L.
Ramakrishnan, “Milieu: Lightweight and Configurable
Big Data Provenance for Science,” in Big Data
(BigData Congress), 2013 IEEE International
Congress on, 2013, pp. 46–53.
[21] H. T. Lin and V. Honavar, “Learning Classifiers from
Chains of Multiple Interlinked RDF Data Stores,” in
Big Data (BigData Congress), 2013 IEEE
International Congress on, 2013, pp. 94–101.
[22] “IBM InfoSphere Platform – big data, information
integration, data warehousing, master data
management, lifecycle management and data security.”
[Online]. Available: http://www01.ibm.com/software/data/infosphere/. [Accessed: 28Feb-2014].
[23] “Warehouse Builder 11gR2: Home Page on OTN.”
[Online]. Available:
http://www.oracle.com/technetwork/developertools/warehouse/overview/introduction/index.html.
[Accessed: 28-Feb-2014].
[24] “Integration Services | Microsoft SQL Server 2012.”
[Online]. Available: http://www.microsoft.com/enus/sqlserver/solutions-technologies/enterpriseinformation-management/integration-services.aspx.
[Accessed: 28-Feb-2014].

[25] “Enterprise Data Integration – Informatica.” [Online].
Available:
http://www.informatica.com/us/products/dataintegration/enterprise/. [Accessed: 28-Feb-2014].
[26] “Talend Open Studio | Talend.” [Online]. Available:
http://www.talend.com/products/talend-open-studio.
[Accessed: 28-Feb-2014].
[27] “Data Integration | Pentaho Business Analytics
Platform.” [Online]. Available:
http://www.pentaho.com/product/data-integration.
[Accessed: 28-Feb-2014].
[28] N. Shadbolt, W. Hall, and T. Berners-Lee, “The
semantic web revisited,” Intelligent Systems, IEEE, vol.
21, no. 3, pp. 96–101, 2006.
[29] G. Antoniou and F. Van Harmelen, A semantic web
primer. the MIT Press, 2004.
[30] “OWL - Semantic Web Standards.” [Online].
Available: http://www.w3.org/2001/sw/wiki/OWL.
[Accessed: 23-Jan-2014].
[31] “Protégé Ontology Editor.” [Online]. Available:
http://protege.stanford.edu/. [Accessed: 28-Feb-2014].
[32] “Oxygen XML Editor.” [Online]. Available:
http://www.oxygenxml.com/. [Accessed: 28-Feb2014].
[33] “Apache Jena - Home.” [Online]. Available:
https://jena.apache.org/. [Accessed: 28-Feb-2014].

529
542

LPM: Layered Policy Management
for Software-Defined Networks 
Wonkyu Han1 , Hongxin Hu2 and Gail-Joon Ahn1
1

Arizona State University, Tempe, AZ 85287, USA
{whan7,gahn}@asu.edu
2
Clemson University, Clemson, SC 29634, USA
hhu@desu.edu

Abstract. Software-Defined Networking (SDN) as an emerging paradigm in networking divides the network architecture into three distinct layers such as application, control, and data layers. The multi-layered network architecture in SDN
tremendously helps manage and control network traffic flows but each layer heavily relies on complex network policies. Managing and enforcing these network
policies require dedicated cautions since combining multiple network modules in
an SDN application not only becomes a non-trivial job, but also requires considerable efforts to identify dependencies within a module and between modules. In
addition, multi-tenant SDN applications make network management tasks more
difficult since there may exist unexpected interferences between traffic flows. In
order to accommodate such complex network dynamics in SDN, we propose a
novel policy management framework for SDN, called layered policy management (LPM). We also articulate challenges for each layer in terms of policy management and describe appropriate resolution strategies. In addition, we present a
proof-of-concept implementation and demonstrate the feasibility of our approach
with an SDN-based simulated network.
Keywords: Policy Management, Software-Defined Networking, Security.

1 Introduction
Traditional network environments are ill-suited to meet the requirements of today’s enterprises, carriers, and end users. Software-Defined Networking (SDN) was recently
introduced as a new network paradigm which is able to provide unprecedented programmability, automation, and network control by decoupling the control and data layers, and logically centralizing network intelligence and state [6]. A typical architecture
of SDN consists of three distinct layers such as application, control, and data layers.
Network applications in the application layer can communicate with an SDN controller
via an open interface and define network-wide policies based on a global view of the
network provided by the controller. The SDN controller, which resides in the control
layer, manages network services, and provides an abstract view of the network to the
application layer. At the same time, the controller translates policies defined by applications into actual rules for processing packets, which are identifiable by the data layer.


This work was partially supported by the grant from Department of Energy (DE-SC0004308).

V. Atluri and G. Pernul (Eds.): DBSec 2014, LNCS 8566, pp. 356–363, 2014.
c IFIP International Federation for Information Processing 2014


LPM: Layered Policy Management for Software-Defined Networks

357

The multi-layered SDN architecture significantly helps manage and process network
traffic flows. However, each layer of SDN architecture heavily relies on complicated
network policies and managing those policies in SDN requires not only dedicated cautions but also considerable efforts. Our study reveals that such a multi-layered architecture brings great challenges in policy management for SDN as follows:
– Policy management in SDN application layer: An SDN application could employ
multiple modules, such as Firewall (FW), Load-Balance (LB), Route, and Monitor,
to process the same flow by composing rules produced by those modules [9]. However, such a task is not trivial since rules may overlap each other within a module
(intra-module dependency) or between modules (inter-module dependency).
– Policy management in SDN control layer: In SDN control layer, there may exist
multiple SDN applications running on top of a controller and they might jointly
process the same traffic flow. In such a situation, flow rules from different applications that process the same flow may also overlap each other (inter-application
dependency) and even lead to policy conflicts [10].
– Policy management in SDN data layer: In SDN data layer, different flows may go
through the same switches and rules defining different flows in the same flow table
may also overlap each other (intra-table dependency). In such a case, an unintended
modification of a flow path could happen.
To address the above-mentioned challenges, we propose a novel framework for managing policies with respect to three layers in SDN architecture. In SDN application
layer, we adopt a policy segmentation mechanism to compute and eliminate intramodule and inter-module dependencies, and enable a secure and efficient policy composition. In SDN control layer, our framework identifies inter-application dependencies
and provides two kinds of resolution strategies. In addition, we propose a flow isolation mechanism to resolve intra-table dependencies in SDN data layer. We also provide
a prototype implementation of our framework in an open SDN controller and evaluate our approach using a real-world network configuration and an emulated OpenFlow
network.
This paper is organized as follows. Section 2 overviews our framework and presents
policy management challenges and corresponding resolution strategies based on three
layers of SDN architecture. In Section 3, we describe our implementation details and
evaluation results followed by the related work discussed in Section 4. Section 5 concludes this paper.

2 Layered Policy Management (LPM) Framework
2.1 Overview
Our LPM framework enables a layered policy management with respect to three layers
of SDN architecture as illustrated in Figure 1.
In SDN application layer, a main challenge comes from policy composition in an
SDN application, where intra-module and inter-module dependencies should be addressed. Partially or entirely overlapped rules in a module make nontrivial intra-module

358

W. Han, H. Hu, and G.-J. Ahn

Application
layer

FW
Security Module

App 2

App 1

Route Monitor
Non-Security Module

LB

FW
Security Module

Traffic Flow 1

App 3

Control
layer

Route Monitor

LB

Non-Security Module

Traffic Flow 2
Data layer

Fig. 1. Multi-layered SDN policy management: (i) application layer; (ii) control layer; and (iii)
data layer

dependencies and make the process of policy composition more difficult. In addition,
inter-module dependencies between security and non-security modules may cause security challenges due to inappropriate composition sequence and dynamic packet modification. Our framework addresses insecure and inefficient policy composition issues
in an SDN application and adopts a policy segmentation mechanism to address those
issues.
In SDN control layer, multiple applications in an SDN controller processing the same
flow may cause inter-application dependencies. As shown in Figure 1, App 2 and App
3 intend to process the same flow Flow 2, thereby the policies produced by two applications may conflict with each other. Our framework also leverages the policy segmentation mechanism to eliminate the conflicts and applies two resolution strategies
by allowing them to jointly process the same flow or assigning dependent applications
with different priorities to break inter-application dependencies.
In SDN data layer, each physical switch stores a number of flow rules with corresponding priorities into its flow table. A rule defining one flow, such as Flow 1 in
Figure 1, with a lower priority might be affected by another rule for another flow, such
as Flow 2 in Figure 1, with a higher priority, causing intra-table dependency. Since
intra-table dependency might change the behaviors of associated flows, our framework
provides a flow isolation mechanism to address such an issue.
2.2 Policy Management in SDN Application Layer
Considerations and Challenges. While an SDN application with multiple modules
processes a network traffic flow, a fundamental consideration is to address intra-module
and inter-module dependencies in policy composition. To illustrate these issues, we
adopt two kinds of policy composition operators introduced in [9]. “Parallel” operator
(|) means the union of two modules and generates a set of packet processing rules which
should be applied to the same flow simultaneously. “Sequential” operator () stands
for the serialization of modules so that the matching rules would be performed one by
one on the same flow. We next investigate several policy management challenges in
SDN application layer.

LPM: Layered Policy Management for Software-Defined Networks
Firewall Policy
r1: src = 10.0.x.x, dst = 1.2.3.x → deny
r2: dst = 1.2.3.4 → allow
r3: src = 10.0.0.x, dst = 1.2.3.x → deny

Load-balance Policy
r4: src = 10.0.1.1, dst = 1.2.x.x → src = 10.2.2.2

359

Route Policy
r5: src = 10.0.0.x, dst = 1.2.3.4 → fwd(1)
r6: src = 10.2.2.2, dst = 1.2.x.x → fwd(2)
r7: src = 10.2.2.2, dst = 1.2.3.x → fwd(3)

Monitor Policy
r8: src = 10.0.1.1, dst = 1.2.10.11 → count
r9: src = 10.1.x.x, dst = 1.2.3.4 → count

Fig. 2. Sample policies defined by four different network modules

(1) Intra-module and inter-module dependency: Assume that there exist four different
modules in an SDN application, such as Firewall (FW), Load-Balance (LB), Route,
and Monitor, and all rules in each module have been sorted by their priorities as
depicted in Figure 2. In FW policy, r1 , r2 , and r3 are mutually dependent, and r2
and r3 are partially overlapped by r1 , representing intra-module dependencies. In
addition, the rule r1 in FW policy is dependent with both r4 in LB policy and r5
in Route policy, representing inter-module dependencies. Furthermore, since r2 in
FW policy is dependent with r9 in Monitor policy, FW module is dependent with all
other modules which implies that determining inter-module dependencies requires
considerable efforts.
(2) Insecure and inefficient policy composition: Suppose that two modules in an SDN
application are sequentially composed, represented as LB  F W . In this case,
r4 in LB policy modifies packets’ source IP address to 10.2.2.2 and could enable
malicious packets to bypass the firewall since r1 in FW policy cannot block these
packets. Hence, we could notice that an inaccurate sequence during the composition may cause security breaches in SDN applications. In addition, a programmer
may want to compose two modules in parallel, such as F W | Route. We could
observe that all rules in FW policy are dependent with r5 in Route policy. Since
r1 has the highest priority, r1 and r5 can be jointly combined and the following
rule can be obtained: src = 10.0.0.x, dst = 1.2.3.4 −→ deny, f wd(1). Indeed, r5
is not necessary to be composed with FW rules, since the FW rule r1 ultimately
blocks packets matching the rule pattern. Therefore, we could also observe that it
is obviously inefficient to always compose the multiple policies as Pyretic [9] does.
As discussed above, there exist a few challenges in the application layer. First, since
the security policies are generally considered more important than the policies produced
by non-security modules, distinguishing security modules from non-security modules
is vital in composing secure policies. In addition, commodity SDN switches typically
support only a few thousands of rules [12], hence we should also strive to provide
mechanisms with respect to an efficient policy composition.
Resolution Strategy. Our resolution approach globally examines all modules along
with their rules to identify overlapping rules and generate disjointed matching space
for removing intra-module and inter-module dependencies. To eliminate these depen-

360

W. Han, H. Hu, and G.-J. Ahn

dencies, we first sort the rules in each module by their priorities and insert all modules
into a global segmentation table. Derived from the approach discussed in [8], our policy
segmentation mechanism generates a set of disjointed matching space, called segment.
For example, r1 and r2 in FW policy are partially dependent with each other. Thus, we
obtain three disjointed segments: sa = r1 − r2 , sb = r2 − r1 , and sc = r1 ∩ r2 . Each
segment maintains overlapping rules, which indicate the existence of intra-module or
inter-module dependencies.
Regarding intra-module dependencies, not all overlapping rules from the same module in a segment are effective, since only one of those rules with the highest priority will
be applicable to process matching packets. Therefore, to remove intra-module dependencies, we only need to consider the effective rule for policy composition. However,
inter-module dependencies between security and non-security modules may cause insecure and inefficient policy composition as discussed above. To address such an issue,
we distinguish security modules from non-security modules using a separator (:), which
indicates that its left-hand side refers security modules with higher priorities while nonsecurity modules are located on the right-hand side with lower priorities. At the same
time, to achieve an efficient policy composition, our resolution only enables to composing allow rules from security modules with other rules from non-security modules
since it is unnecessary to perform policy composition once a rule from security modules denies the matching space. For instance, the composition sequence, LB  F W , is
not valid in our scheme since the separator (:) would distinguish security modules and
non-security modules, i.e., F W : LB. In addition, our mechanism does not compose
r1 in FW policy with r4 in LB policy. Because r1 is a deny rule, our mechanism simply
generates a deny flow entry without considering overlapping rules from non-security
modules.
2.3 Policy Management in SDN Control Layer
Inter-application Dependency. The root cause for inter-application dependencies is
that multiple SDN applications may attempt to enforce their policies over the same network flow. Suppose that APP 2 in Figure 1 composes LB, Route and Monitor modules
sequentially, LB  Route  M onitor. However, APP 3 composes the same modules in the opposite order, M onitor  Route  LB. Incoming packets matching the
source IP address 10.0.1.1 and the destination IP address 1.2.10.11 will be managed
by two different applications, since both r4 in LB policy and r8 in Monitor policy can
handle these packets. The APP 2 first applies r4 in LB policy to modify the source IP
address of matched packets to 10.2.2.2 and then applies r6 in Route policy to forward
them to port 2. Note that there is no matching rule in Monitor policy. On the other hand,
the APP 3 first enforces r8 in Monitor policy to count the packets of the same flow and
then drops the matched packets because there is no matching rule in Route policy.
Resolution Strategy. In our resolution approach, we consider two situations: (i) different applications are allowed to jointly manage the same flow and (ii) applications
are mutually exclusive. For the former case, we may allow inter-application dependencies and apply composition operators to combine multiple policies from different

LPM: Layered Policy Management for Software-Defined Networks

361

applications. With respect to the latter case, we eliminate inter-application dependencies by assigning different priorities to conflicting applications. Then, the application
with the highest priority overrides the applications with the lower priorities when the
flows are processed. For example, an application that employs security modules may
have a higher priority to take the precedence over other normal applications. Different conflict resolution strategies proposed by our previous work [8] are also applied to
resolve inter-application dependencies caused by conflicting applications.
2.4 Policy Management in SDN Data Layer
Intra-table Dependency. The flow paths of distinct flows managed by different SDN
applications may overlap each other in the flow tables, introducing intra-table dependencies. For example, suppose that there exist two traffic flows processed by different
applications as shown in Figure 1. One application, App 1, generates a policy for a
flow Flow 1, which matches packets whose source and destination IP addresses are
10.2.2.2 and 1.2.3.4 respectively and forwards the packets to the port 2. On the other
hand, another application App 2 manages a different flow Flow 2, but the generated
policy modifies the source IP address of matched packets to 10.2.2.2 and forwards the
packets to the port 2. Even though incoming packets of two flows might be different,
outgoing packets for those flows may overlap with each other. Thus, this situation may
cause a potential loss of flow control for an application if there exists an intra-table
dependency between the flow paths.
Resolution Strategy. Our resolution approach for this layer is to remove intra-table
dependencies through flow isolation. Inspired by the approach discussed in [7], which
leverages tags to differentiate packets belonging to different versions of policies for
enabling consistent network updates, we also utilize tags to eliminate the dependencies
in a flow table. Using this strategy, a new flow policy is preprocessed by adding a tag
to distinguish the matching pattern with other policies. The rule of the flow policy in
the ingress switch will take additional action on the packets to label them with the same
tag. When the packets leave the network, the corresponding rule of the flow policy in
the egress switch will remove the tag from the packets.

3 Implementation and Evaluation
We have implemented our framework on top of an open SDN controller, Floodlight [1].
Our proof-of-concept implementation captures every flow rule created by applications
and produces a set of segments, which are able to remove intra-module and inter-module
dependencies. Also, our resolution strategy component obtains a global view of network
from the Floodlight controller and utilizes various resolution strategies for SDN control
layer and data layer.
Our experiments were performed with Floodlight v0.90 and Mininet v2.1.0 [3].
We obtained a real-world network configuration from Stanford backbone network [2],
which has 26 switches with corresponding ACL rules. We removed redundant ACL
rules, converted them to a FW policy, and in turn obtained 1, 206 FW rules in total.

362

W. Han, H. Hu, and G.-J. Ahn
Empirical CDF
25

1

Elapsed Time (milli seconds)

0.9
0.8

P(x < t)

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

Segment computing time (t, milli seconds)

(a) Segment generation time (CDF)

0.5

20

15

10

5

Assign priorities (layer 2)
Update VLAN field (layer 3)
0

1

2

3

4

5

6

7

8

9

10

Number of rules per flow

(b) Elapsed time for resolution strategies

Fig. 3. Experimental results of our approach

At the same time, we generated 8, 908 Floodlight-recognizable flow rules by parsing
original network rules in Stanford network configuration. Because these real network
rules perform both routing and load-balancing tasks, we assume that these rules are
generated by two modules, Route and LB modules.
To measure overheads caused by our policy segmentation mechanism, we installed
all network rules into the simulated network and measured the update operation of policy segmentation. Our experiments show that 456 segments out of 688 FW rules were
produced by the policy segmentation mechanism and 8, 273 segments out of 8, 908 network rules were generated. As shown in Figure 3(a), 75% of updates were completed
within 0.2 milliseconds and most of cases (98%) were computed in less than 0.5 milliseconds.
We also evaluated the performance of two resolution strategies: (i) assigning priorities to eliminate inter-application dependencies in the SDN control layer and (ii) updating VLAN fields (flow-tagging) to eliminate intra-table dependencies in the SDN data
layer. Both resolution strategies update a set of flow rules which define corresponding
flows. First, we measured elapsed time for assigning priorities of rules. As shown in
Figure 3(b), the elapsed time grows in accordance with the increased number of rules
per each flow. Similarly, we checked the elapsed time for updating VLAN fields for isolating conflicting flows. The elapsed time increases with the increased number of rules
per each flow, as expected. However, it generally took more time, since our mechanism
needs to figure out ingress and egress switches in examining flows, adding and striping
VLAN tags from the packets.

4 Related Work
Modular network programming has recently received considerable attention in SDN
community. For instance, Pyretic [9] enables a program to combine different policies
generated by different modules together using policy composition operators. However,
due to the lack of policy dependency detection mechanism in Pyretic, it is obviously
inefficient to always compose the multiple policies and install them into the network
switches. FRESCO [11] deals with security application development framework using modular programming for SDN, but it cannot directly handle dependencies between modules in SDN applications either. Several policy composition mechanisms

LPM: Layered Policy Management for Software-Defined Networks

363

such as [4,5] support pair-wise composition for access control policies and could be
potentially utilized to deal with intra-module dependencies in SDN. In contrast, our
framework addresses various dependencies including intra-module, inter-module, interapplication, and intra-table dependencies in SDN.

5 Conclusion
We have articulated numerous problematic issues and security challenges in SDN policy
management and proposed a novel framework to facilitate a layered policy management
approach with respect to three layers in the SDN architecture. Our experimental results
with the proof-of-concept prototype showed that our resolution is efficient and only
introduces manageable performance overheads to the networks. For the future work, we
will extend our framework to support dynamic policy updates. In addition, we would
expand our solution to support comprehensive SDN policy management considering
heterogeneous and distributed controllers.

References
1.
2.
3.
4.

5.

6.
7.

8.
9.

10.

11.

12.

Floodlight: Open SDN Controller, http://www.projectfloodlight.org
Header Space Library, https://bitbucket.org/peymank/hassel-public
Mininet: An Instant Virtual Network on Your Laptop, http://mininet.org
Bandara, A.K., Lupu, E.C., Russo, A.: Using event calculus to formalise policy specification and analysis. In: Proceedings of the 4th IEEE International Workshop on Policies for
Distributed Systems and Networks, pp. 26–39. IEEE (2003)
Bonatti, P., De Capitani di Vimercati, S., Samarati, P.: An algebra for composing access
control policies. ACM Transactions on Information and System Security (TISSEC) 5(1),
1–35 (2002)
ONF Market Education Committee, et al.: Software-defined networking: The new norm for
networks. ONF White Paper. Open Networking Foundation, Palo Alto (2012)
Fayazbakhsh, S.K., Chiang, L., Sekar, V., Yu, M., Mogul, J.C.: Enforcing network-wide
policies in the presence of dynamic middlebox actions using flowtags. In: 11th USENIX
Symposium on Networked Systems Design and Implementation (NSDI 2014), pp. 543–546.
USENIX Association (2014)
Hu, H., Ahn, G.-J., Kulkarni, K.: Detecting and resolving firewall policy anomalies. IEEE
Transactions on Dependable and Secure Computing 9(3), 318–331 (2012)
Monsanto, C., Reich, J., Foster, N., Rexford, J., Walker, D.: Composing software-defined
networks. In: Proceedings of the 10th USENIX Conference on Networked Systems Design
and Implementation, pp. 1–14. USENIX Association (2013)
Porras, P., Shin, S., Yegneswaran, V., Fong, M., Tyson, M., Gu, G.: A security enforcement
kernel for openflow networks. In: Proceedings of the First Workshop on Hot Topics in Software Defined Networks, pp. 121–126. ACM (2012)
Shin, S., Porras, P., Yegneswaran, V., Fong, M., Gu, G., Tyson, M.: Fresco: Modular composable security services for software-defined networks. In: Proceedings of Network and
Distributed Security Symposium (2013)
Stephens, B., Cox, A., Felter, W., Dixon, C., Carter, J.: Past: Scalable ethernet for data centers. In: Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies (CoNEXT 2012), pp. 49–60. ACM (2012)

Unimate: A Student Information System
Prabhu T Kannan, Srividya K Bansal
Department of Engineering
Arizona State University – Poly Campus
Mesa, AZ 85212
{prabhu.kannan, srividya.bansal}@asu.edu
Abstract— Learning management systems and student
information systems have been in use for years in a number of
universities across America. However, their use is not as
widespread in universities in India. The main reasons have
been the lack of high-speed Internet availability, reluctance
from universities to try new software, and the high cost of
commercial products (which are not differently priced for the
Indian market). In recent years, Internet penetration has
significantly improved and a number of universities even have
direct connections to the internet backbone. Social media use
has become very widespread, and students and teachers are
familiar and comfortable with a number of web-based
applications. The goal of this project is to develop a prototype
for a low-cost web-based application that provides features of
both learning management systems and student information
systems, and is customized to the needs of universities in India.
Keywords - document manangement, learning management,
learning technology systems, student information systems,
distributed systems, model-view-controller architecture

S

I. INTRODUCTION

TUDENT Information System (SIS) is a software
application to manage the administrative processes of an
educational institution like admissions, attendance and
housing. Student information is made available to
administrators across the institution, which facilitates
planning and co-ordination. Learning Management System
(LMS) is a software application to share course content and
track the progress of individual students. It can be used as a
tool to enhance classroom teaching by engaging students in
discussions and learning activities. Learning management
systems can augment course material with rich multimedia
content. Learning Management Systems help personalize
the learning process. Teachers get a richer picture of how
students are faring in quizzes and tests and can find out
which concepts students are struggling with. They can also
be used as a system to deliver and administer distant
learning courses.

Siddaganga Institute of Technology (SIT) is an
educational institute in Tumkur, India, having around 4000
students [1]. SIT has 17 departments and offers 12 undergraduate and 7 post-graduate programs. At present, SIT
does not use an LMS or SIS. Courses are delivered in a

c
978-1-4673-6217-7/13/$31.00 2013
IEEE

traditional classroom model. Quizzes, assignments and
exams are paper-based. Courses that require the use of
computers use them only for lab-work and exams. Some
assignments require students to collaborate, and personal
emails and social media websites are used for this purpose.
There is no university-wide email system.
A. Problem Statement
Industry sponsors in India have a tough time processing
thousands of job-applications from Engineering students.
The current method for on-campus placements is to shortlist candidates based on a minimum required grade-pointaverage (GPA) followed by an in-person interview. This
method provides recruiters with limited and dissimilar
information about the students. This project addresses this
problem by building a prototype that integrates the features
of SIS and LMS. The LMS stores performance statistics
from assignments, quizzes, exams and projects, along with
academic achievements outside the course curriculum, like
seminars, presentations, projects and computer skills. This
data is used to build a portfolio for each student. The SIS
improves communication between students and the
university and makes the education process more
transparent, accessible and manageable.
India is a cost sensitive market and needs a significant
lower cost model for learning management system products.
The system should be low-cost, easy-to-use and easy-tomaintain. For this reason, open-source technologies were
preferred over commercial alternatives. The system was
opted to be web-based so that it
• can be accessed from anywhere over an Internet
connection.
• can be accessed from any platform using a web browser.
• have minimal requirements for the end-user (no setup or
updates).
This project integrates an open source Learning Management
System with a custom Student Information System, which
has the capability to manage hostel1 selection, and identifies
other areas for future work.
1

‘Hostel’ in this context refers to student dormitories that are part of
university campus

1251

II. BACKGROUND
The application consists of 3 distinct components:
• Learning Management System (LMS)
• Student Information System (SIS)
• Public website
These 3 components and the technologies they are based-on
are discussed in detail below:
A. Learning Management System (LMS)
There are a number of open-source learning management
systems in the LMS market. The 2010 annual survey by
Campus Computing Project shows a significant rise in the
adoption of open-source LMS by universities across
America [2]. This section will briefly describe some popular
open source learning management systems:
1) Moodle
Moodle (Modular Object-Oriented Dynamic Learning
Environment) ‘is a software package for producing Internetbased courses and web sites’ [3]. It is written in PHP and is
provided under GNU General Public License (GPL). It has
an active community that provides online support and a
number of community contributed modules, which can
extend the core functionality. Moodle is the most popular
open source LMS, according to the 2010 annual survey by
Campus Computing Project [2].
2) Sakai CLE
Sakai CLE (Collaboration and Learning Environment) is
a ‘full-featured system supporting technology-enabled
teaching, learning, research and collaboration for education’
[4]. Sakai CLE is written in JAVA and is supported by the
Sakai Foundation. It is provided under Educational
Community License (ECL). It also has a number of add-on
tools contributed by the community.
3) Canvas
Canvas is an LMS built using Ruby on Rails by
Instructure. A commercial version is offered in Software as
a Service (SaaS) model hosted on Amazon cloud. A
community version is provided under Affero General Public
License (AGPL). At present, there are no Canvas LMS
hosting providers catering to the Indian market.
Canvas appears to be a good candidate for the LMS
component. It has a modern look and is intuitive to use. It
integrates with social media sites like Facebook, Twitter and
LinkedIn, collaboration tools like Google Docs, Scribd and
TinyChat, and services like Kaltura (an open source video
platform) and BigBlueButton (an open-source web
conferencing tool) [5]. It was designed to be hosted on the
cloud, which will make a future transition to the cloud
model an easy process.

1252

B. Student Information System (SIS)
PHP 5.3.2 and Zend Framework 1.11.11 were used for
developing the SIS component. Zend framework is an open
source object-oriented Model-View-Controller (MVC)
framework. ‘Zend Framework is focused on building more
secure, reliable, and modern Web 2.0 applications & web
services’ [6]. It is provided by Zend Technologies under the
new Berkeley Software Distribution (BSD) license. Mysql
Community Edition 5.1.61, which is provided under the
General Public License (GPL) license, was used as the
database for this application. For client-side scripting, these
3 open source Javascript frameworks were considered:
1) jQuery
jQuery is provided under MIT (Massachusetts Institute of
Technology) and General Public License (GPL) licenses. It
is ‘a fast and concise JavaScript Library that simplifies
HTML document traversing, event handling, animating, and
Ajax interactions for rapid web development’ [7].
2) Ext JS
Sencha provides Ext JS under a license compatible with
the GNU GPL license v3. It is a Model-View-Controller
(MVC) framework and includes a number of rich userinterface widgets and charting tools.
3) Dojo toolkit
Dojo is provided under the modified Berkeley Software
Distribution (BSD) license and the Academic Free License
version 2.1. Dojo is a comprehensive modular framework
with features similar to Ext JS.
jQuery is relatively the easiest to use. However, it is not a
complete framework like Dojo or Ext JS. A large number of
its features come from community-contributed plugins, and
hence, are not standardized. The Ext JS license forces any
project that uses it to also be distributed freely. ‘Although
the GPLv3 has many terms, the most important is that you
must provide the source code of your application to your
users so they can be free to modify your application for their
own needs [8]. Both of Dojo’s licenses ‘grant you broad
rights to use and build on and with Dojo in both Open
Source and commercial settings’ [9]. Dojo Toolkit 1.7 was
used for client side scripting.
C. Public website
A web content management system is software that provides
website authoring, collaboration and administration tools,
requires very little technical skill or knowledge to manage
and keeps track of content on the website. This section will
briefly describe some popular open source content
management systems:

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1) Wordpress
WordPress is a simple CMS, which started as a blogging
system. It has evolved to be a full content management
system and has a number of plugins, widgets, and themes. It
is provided under ‘GNU GPL license v2 or later’ [10].

are used as application servers, and MySQL Community
Edition 5.1.61 serves as the back-end database server. The
following sections discuss the high level design of the
system in more detail.

2) Drupal
Drupal is an open source Content Management System
(CMS) provided under ‘GNU GPL license v2 or later’ [11].
It provides a number of tools to organize and structure
content. Drupal's flexibility handles a number of content
types like video, text, blog, podcasts, and polls. It comes
‘with robust user management, menu handling, real-time
statistics and optional revision control’ [12]. Drupal also has
a number of community-contributed modules which provide
a wide range of capabilities.

Three Arizona State University servers running Ubuntu
10.04.4 (Lucid Lynx) [14] are used to host the different
components of the system. Figure 1 shows the 3 servers and
the components that they host.
The LDAP server hosts a OpenLDAP directory. This
directory will be accessed by all 3 components during user
authentication. A single Apache 2.2 instance serves the 3
components using port-based virtual host configuration
[15]. The virtual host configuration for the SIS component
is shown in Table 1 below:

3) Joomla
Joomla is also a full-fledged content management system
(CMS), with features similar to Drupal. It is provided under
the GPL license [13].

Table 1: Apache virtual-host configuration

Wordpress is not as flexible as Joomla or Drupal, and also
does not have as many features. Drupal and Joomla have
almost similar features. However, the Drupal community is
very active and Drupal has more user contributed modules
and better online community support. Drupal 7.12 and
MySQL 5.1.61 were used for developing the public website.

L
D
A
P

S
e
r
v
e
r

W
e
b
/
A
p
p

S
e
r
v
e
r

D
a
t
a
b
a
s
e

S
e
r
v
e
r

OpenLDAP

Drupal

Canvas
LMS

Zend
Framework
Fra

Mysql

Mysql

Mysql
M

database

d
database

da
database

Figure 1: Server
erver architecture
arcchitecture

III. ARCHITECTURE
The system employs a 3-tier-architecture. Apache 2.2 serves
as the front-end web server, PHP 5.3.2 and Ruby on Rails

A. Server architecture

<VirtualHost *:445>
ServerName unimate.sit.ac.in
DocumentRoot /var/unimate/public
SetEnv APPLICATION_ENV "development"
SSLEngine on
BrowserMatch "MSIE [2-6]" nokeepalive ssl-uncleanshutdown downgrade-1.0 force-response-1.0
BrowserMatch "MSIE [17-9]" ssl-unclean-shutdown
SSLCertificateFile /etc/ssl/certs/ssl-cert-snakeoil.pem
SSLCertificateKeyFile /etc/ssl/private/ssl-certsnakeoil.key
<Directory /var/drupal/sit>
DirectoryIndex index.php
AllowOverride All
Order allow,deny
Allow from all
</Directory>
</VirtualHost>
B. OpenLDAP
‘A directory is a specialized database specifically
designed for searching and browsing, in addition to
supporting basic lookup and update functions’ [16].
Lightweight Directory Access Protocol (LDAP) is a
protocol used for accessing directory services. OpenLDAP,
an open source implementation of LDAP has been used to
store end-user attributes, so that it can be centrally managed
and accessible via standards based methods. LDAP Data
Interchange Format (LDIF) is used to represent LDAP
entries in a simple text format [17].
The directory information tree (DIT) binding user data is
shown below in LDAP Data Interchange Format. The LDIF
shown in Table 2, below, will create the DIT structure and
add a single ‘people’ entry.

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1253

Table 2: Directory Information Tree for OpenLDAP

# FIRST level object - organization
dn: dc=sit,dc=ac,dc=in
objectClass: top
objectClass: dcObject
objectClass: organization
dc: sit
o: Siddaganga Institute of Technology
description: LDAP for Siddaganga Institute of Technology
# SECOND level object - people
dn: ou=people,dc=sit,dc=ac,dc=in
objectClass: organizationalUnit
ou: people
description: All people in organisation
# THIRD level - instance of people
dn: uid=cnvsdmn,ou=people,dc=sit,dc=ac,dc=in
objectClass: person
objectClass: posixAccount
uid: cnvsdmn
cn: Site Administrator
sn: Site Administrator
uidnumber: 1000
gidnumber: 10000
homedirectory: /var/mail/sit.ac.in/cnvs
userPassword:
{SSHA}4a3Ob9mmwkxGiAd9KcmIC36VnhMswcmZ
Lines beginning with‘#’ are comments. The top level object
of the tree is the organization (objectClass: organization).
We use a Domain Name Service (DNS) style distinguishedname (dn: dc=sit,dc=ac,dc=in) to identify this object.
Under this, we create a group (objectClass:
organizationalUnit) for all people within this organization.
And finally, we create an entry for an end-user (having uid:
cnvsdmn) and identified by the LDAP as dn:
uid=cnvsdmn,ou=people,dc=sit,dc=ac,dc=in.

Figure 1: Activity Diagram for Authentication and Authorization

The system should ensure that end-users can perform only
those actions which their role is authorized to perform. Endusers having ‘administrator’ role shall perform CRUD
(create-remove-update-delete) operations on hostel entities.
Hostel entities can be hostel, block or room. Figure 3 shows
the use cases for end-users having ‘student’ role. End-users
having ‘student’ role shall view hostel entities which are
open and available. The system should display the capacity
and availability for each entity. Once a room is picked, the
system shall allocate this to the user.

IV. IMPLEMENTATION
The SIS component was developed using PHP 5.3.2 along
with Zend Framework 1.11.11, and MySQL Community
Edition 5.1.61. Implementation details are discussed in the
following sections.
A. System requirements
This section discusses the system requirements for the
hostel allocation feature under the SIS component. The SIS
component will also have other requirements that are
outside the scope of this project. The hostel module should
ensure that end-users are authenticated before they perform
any action as shown in the activity diagram of Figure 2.

1254

Figure 3: Use case Diagram for 'student' role

B. Database design
InnoDb has been selected as the storage engine for the
following reasons [18]:
• It is transaction-safe and ACID
(atomicity, consistency, isolation, durability)
compliant, and has commit, rollback, and crashrecovery capabilities.

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

•

•
•
•

It supports row-level locking and non-locking reads
that increase multi-user concurrency and
performance.
It stores user data in clustered indexes to reduce I/O
for common queries based on primary keys.
It supports foreign-key referential-integrity
constraints.
It is published under GNU General Public License
(GPL) Version 2.

Figure 4 explains the relationship between hostels, blocks,
rooms and users. A hostel can have multiple blocks and
each block must have a hostel. This is enforced by a one-tomany relationship from hos_hostel table to hos_block table
using foreign-key constraint. Similarly, there is a one-tomany relationship between blocks and rooms, and rooms
and users. The IsOpen field determines whether that entity
and its children will be displayed to end-users having
‘student’ role. All fields are marked as Not Null except for
AddressLine2 of Hos_Hostel which is an optional field and
Hos_Room_RoomId field under Usr_User table because
there can be users without a assigned room. Unique indices
(uk_hostel_code, uk_block_code and uk_room_number)
prevent duplicate entities and ensure data integrity.

C. Coding details
The SIS component was developed using PHP 5.3.2
along with Zend Framework 1.11.11 as a modular
application following Model-View-Controller (MVC)
architecture. 3 modules: admin, student and user are created.
The admin module contains functionality for ‘administrator’
role’s CRUD operations. The student module contains
functionality for ‘student’ role’s hostel selection operation.
The user module has just the model for user table, and is
used during authentication. The complete project directory
is shown in the Project Structure section under Appendix.
1) Model
This is the part of the application that defines data access
routines and some business logic. An LDAP adapter is used
to connect to the OpenLDAP directory for authentication.
For the Mysql database, a Php Data Object (PDO) adapter is
used. The configuration for both these adapters is present in
application.ini file. The PDO is used with parameterized
queries to ensure protection against SQL injection.
A Table Data Gateway is used to connect to the
database by extending Zend_Db_Table_Abstract (e.g.
Admin_Model_DbTable_Block). This class identifies the
table it belongs to and all foreign key constraints. A model
class (e.g. Admin_Model_Hostel) identifies the fields in this
table and provides accessor and mutator methods. A data
mapper (e.g. Admin_Model_HostelMapper) maps the
controller to the model.
2) View
Views define exactly what is presented to the user. View
scripts are placed in the respective module’s scripts folder,
where they are further categorized using the controller and
action names. The view script for actions return content
only related to the action itself, not the application as a
whole. This content is composed into a full HTML page
using a site wide layout. The layout script is present in
layout.phtml and provides the application with consistent
look and feel.
Zend_Form has plugins to filter and validate user input,
and is used to create forms. All forms are Dojo-enabled,
which gives them a consistent look across different
platforms. Ajax is used for form submission and to refresh
grids, which reduces the number of page-loads. Dojo’s
TabContainer is used for the layout and Toaster widget is
used to provide unobtrusive color-coded notifications.

Figure 4: Database design

3) Controller
Controllers bind the view and model together. They
manipulate models, decide which view to display based on
the user's request and other factors, pass along the data that
each view will need, or hand off control to another
controller entirely. Each controller can have multiple
actions. The location of a page is determined by the module

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1255

name, controller name and action name. Authentication is
implemented using Zend_Auth, which uses the LDAP
adapter. Access control is implemented as a controllerplugin using Zend_Acl. The ‘administrator’ role is restricted
to access only admin module, and the ‘student’ role is
restricted to access only student module.
D. Security
All forms contain a cross-site request forgery
(CSRF) protection token which is verified when the form is
submitted. This ensures that data is only submitted by the
same user session that generated the form, thus providing
protection from CSRF attacks. The Mysql database has been
configured to accept requests only from the web-server’s IP
address. Each component has a separate Mysql account,
which has permissions only to access its own database. This
is to protect the data owned by each component from
accidental overwriting by another component. The SIS
component can be accessed only over secure socket layer
(SSL). Canvas LMS automatically redirects the browser to
the SSL port. The Drupal website has been configured to
force the user to use secure socket layer (SSL) for user
authentication so passwords are encrypted before they are
sent over the Internet.
V. TESTING
The application has been confirmed to work in multiple
browsers like Mozilla Firefox 11.0, Internet Explorer 9 and
Google Chrome 18.0. The functionality of the system is
tested by creating a new hostel with rooms and blocks with
‘administrator’ role, and selecting the room with ‘student’
role. Some of the tests performed are listed below:
• The system should check if users are authenticated.
• The system should check if users are authorized to
view a page.
• The system should check if hostel codes and
descriptions are unique.
• The system should check if block codes are unique
• Before removing a hostel or block or room, the
system should validate that it does not have any
occupants.
• The system should check if a hostel or room or block
is open before displaying it to students.
• The system should ensure that an entity does not
have any children before deletion.
• The system should ensure referential integrity by
checking for child-entities before deletion.

allocation. A Linux-based distributed-system-architecture
was designed and configured. A number of modules can be
added to extend the capabilities of the application. It was to
support these future capabilities, and ensure extensibility
and maintainability, that Zend MVC and Dojo toolkit were
selected. Some of the areas identified for future work are
listed below:
• Implement a single-sign-on mechanism
• Host the application on a cloud platform
• Student portfolios using Canvas data
• Extend and customize the Drupal website
• Accounting module for fee processing
• User management for administrators
• Integrate with Google Apps for Education [19]
• Ghosting capability for administrators
• Display university exam results
• Display university announcements and news.
Some technical enhancements that can be applied to the
current system are listed below:
• Secure OpenLDAP connection
• Modify LMS and SIS to fetch roles from LDAP
• Implement cache to reduce load on database
• Implement business-logic using Mysql stored
procedures, to reduce the number of calls to the
database.
REFERENCES
[1]
[2]

[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]

VI. SUMMARY AND FUTURE WORK
This project provides a proof-of-concept for a SIS
customized to the needs of universities in India. It covered
aspects like system architecture design, database design and
creation of a simple user interface for hostel room

1256

[18]
[19]

SIT - About. SIT. [Online] http://sit.ac.in/about.html.
Campus Computing 2010 Survey. Campus Computing. [Online]
http://www.campuscomputing.net/sites/www.campuscomputing.net/fi
les/Green-CampusComputing2010.pdf.
Moodle - About. Moodle. [Online]
http://docs.moodle.org/22/en/About_Moodle.
Sakai - About. Sakai. [Online] http://sakaiproject.org/about-sakai.
Canvas - About. Canvas. [Online]
http://www.instructure.com/canvas.
Zend Framework - About. Zend Framework. [Online]
http://framework.zend.com/about/overview.
jQuery [Online] http://jquery.com/.
Ext JS. [Online] http://www.sencha.com/products/extjs/license/.
Dojo. [Online] http://dojotoolkit.org/license.
Wordpress [Online] http://wordpress.org/about/.
Drupal License. [Online] http://drupal.org/licensing/faq/.
Drupal Features. [Online] http://drupal.org/features.
Joomla. [Online] http://www.joomla.org/about-joomla.html.
Lucid Lynx - Ubuntu. [Online] http://releases.ubuntu.com/lucid/.
Virtual Hosts - Apache. [Online]
http://httpd.apache.org/docs/2.0/vhosts/.
OpenLDAP Admin. [Online] http://www.openldap.org/doc/admin24/.
LDIF - OpenLDAP. [Online]
http://www.openldap.org/doc/admin24/dbtools.html#The LDIF text
entry format.
InnoDb Storage Engine. Mysql. [Online]
http://dev.mysql.com/doc/refman/5.0/en/innodb-storage-engine.html.
Google Apps for Education - Google Apps. [Online]
http://www.google.com/apps/intl/en/edu/index.html

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

2016 IEEE Tenth International Conference on Semantic Computing

IMOD-Ont: Towards an ontology for Instructional
Module Design
Srividya K. Bansal

Odesma Dalrymple

Arizona State University
School of Computing, Informatics, and Decision Systems
Engineering, Mesa AZ 85212
srividya.bansal@asu.edu

University of San Diego
Shiley-Marcos School of Engineering
San Diego, CA 92110
odesma@sandiego.edu
learning objectives, select content to be covered, develop an
instruction and assessment plan, and define the learning
environment and context for their course(s).
• contains a repository of current best pedagogical and
assessment practices, and based on selections the user makes
when defining the learning objectives of the course, the
system will present options for assessment and instruction
that align with the type/level of student learning desired.
We have developed the underlying framework for
Instructional Module Development System (IMOD) that
supports the design of various components of a course such as
the learning context, learning objectives, course content,
assessments, and instructional techniqes of an instructional
module. In this paper we present the translation of the IMOD
theorectical model called PC3 model [1] into an ontological
model (IMOD-Ont) that provides intelligent interactions with
the user, provides just-in-time help and explanations to the user
on how to perform course design tasks efficiently and
accurately, dictates a course design process that conforms with
the underlying framework, checks for omissions and
inconsistencies in the design, provides feedback to the user on
their course design, and recommends relevant assessment and
pedagogical approaches along with help on how they are
implemented. The paper is organized as follows. In section 2,
we discuss related work. In section 3, we present the IMOD
framework and the ontology called IMOD-Ont that is based on
a framework for outcome-based course design. We present
how it describes and formalizes entities in the domain of
instructional design. Section 4 presents a discussion followed
by conclusions and future work.

Abstract— An information technology (IT) tool that can guide
STEM educators through the complex task of course design
development, ensure tight alignment between various
components of an instructional module, and provide relevant
information about research-based pedagogical and assessment
strategies will be of great value. A Semantic Web-based software
tool called Instructional Module Development (IMOD) System,
currently under development, is built upon a framework for
outcome-based course design. It uses Semantic Web technologies
to provide intelligent interactions with the users, dictate a course
design process in conformance with the underlying framework,
checks for omissions and inconsistencies in the design, provides
feedback to the user on their course design, and recommends
relevant assessment and pedagogical approaches along with help
on how they are implemented. In this paper, we present an
ontology called IMOD-Ont that aims at describing and
formalizing entities in the domain of instructional design and is
used by the IMOD system. IMOD-Ont ontology complements
existing metadata schemas and linked data approaches in the
field of technology-enhanced learning that focus on describing
learning resources and educational content that is available on
the Web. The goal is to make the semantic data generated from
course designs created through the IMODS tool available as
linked education data on the web.
Keywords— ontology design; instructional design; semantic
web; STEM education; technology-enhanced learning.

I. INTRODUCTION
Research studies show that for 95% of new faculty
members, it takes four to five years, through trial and error (the
most common method of gaining expertise in teaching), to
deliver effective instruction. While there are a number of
options available to faculty for receiving instructional
development training (i.e., training focused on improving
teaching and learning), most share similar format, features, and
shortcomings. For example: workshops, courses and seminar
series, the most common program structures, are often offered
at a cost to the institution, department or individual attendee;
delivered face-to-face at specified times; and accessible to a
restricted number of persons. Even when interest is high, these
factors can become obstacles to participation. Our research
goal is to develop a framework for outcome-based course
design process and translate it into a Semantic Web-based
software tool that:
• guides individual or collaborating users, step-by-step,
through an outcome-based education process as they define
978-1-5090-0662-5/16 $31.00 © 2016 IEEE
DOI 10.1109/ICSC.2016.98

II. RELATED WORK
To further justify the need for the development of the IMOD
system and ontology IMOD-Ont we conducted a competitive
analysis to determine the strengths and weaknesses of tools
and approaches currently used to support course design and
related training. The tools and approaches that were evaluated
were categorized into Knowledge/Learning Management
Systems (examples: Blackboard, Moodle, Sakai, Canvas);
Educational Digital Libraries (examples: National Engineering
Education Delivery System (NEEDS), Connexions);
Personalized Learning Services (examples: Content
Automated Design and Development Integrated Editor
(CADDIE), Intelligent Web Teacher (IWT), LOMster [2], and
LOCO-Analyst [3]). The main focus of these efforts is

354

– a description of the level of competence that must be
reached or surpassed. We included an additional characteristic
in the IMOD framework, i.e., the Content – description of the
disciplinary knowledge, skill, or behavior to be attained.

towards integrating educational Web resources, delivery of
courses and learning, or building repositories of learning
resources [4], [5]. There are existing ontologies and metadata
schemas to handle educational resources. Learning Object
Metadata (LOM) standard [6] developed by IEEE LTSC
(Learning Technology Standards Committee) specifies format
of the metadata that is used to describe a learning object.
Tools like LOMster developed to “share and reuse”
educational resources are based on peer-to-peer technology [2]
and use this standard. They allow users to add learning
objects/resources to a system and generate metadata for the
added content and also provides the ability to share these
resources with peers connected in the network. Other similar
and competing metadata schemas are Dublin Core [7] and
ADL SCORM [8]. Dublin Core provides description of Web
resources for the purpose of discovery. SCORM is a collection
of specifications for web-based e-learning systems that is
widely used and supported by most Learning Management
Systems. It describes course structure, content structure, and
communication between client-side content and run-time
environment. The main focus of SCORM is the course content
and its management. IMOD-Ont explicitly focuses on the
outcome-based course design, generation of design
documentation and feedback to the user on the fidelity of the
design. It will use the existing Dublin Core standard for web
resource description and possibly SCORM standard for course
content description. In addition to these entities, IMOD-Ont
will provide formal description of a number of other essential
entities for instruction design and the relationships between
these entities.

B. IMOD-Ont Ontology
Semantic web technologies facilitate: the organization of
knowledge into conceptual spaces, based on their meanings;
extraction of new knowledge via querying; and maintenance of
knowledge by checking for inconsistencies. These technologies
can therefore support the construction of an advanced
knowledge management system [10], [11]. We translated the
IMOD framework into a rich meaningful knowledge structure
in the form of ontology, i.e., an explicit and formal
specification of a conceptualization.
1) Primary classes and relationships in IMOD-Ont
IMOD-Ont consists of classes IMOD, Learning Objective,
Content, Assessment, and InstructionalTechnique. The IMOD
class refers to an instructional module or course. The
remaining classes represent the important high-level
components of a course design. IMOD class has relationships
hasInstructor,
hasLearningObjective,
hasContent,
hasAssessment, hasInstructionalTechnique, hasContent, and
hasSchedule modeled as Object properties. In addition to these
primary classes every IMOD class also has course overview
information modeled using Data properties (such as
hasCourseID,
hasCourseTitle,
hasDescription,
hasCreditHours, etc.) Figure 2 shows the hierarchy of these
primary classes and their relationships. Figure 3 depicts the
schema model as well as instance data for an example course
with ID CST 100. Instance data on course design is obtained
through the web application during the course design
elicitation process, logical inference algorithms will test the
course design for consistency and adherence with this
ontological model.
2) Learning Objective class hierarchy and relationships in
IMOD-Ont
Learning Objectives form the linchpin on which an
instructional module rests. Learning objective is built upon
Bloom’s revised taxonomy by Anderson and Krathwohl [23]
and consists of three parts - Condition, Performance and
Criteria. The learning domain and domain category of the
Performance part of the objective is based on Bloom’s revised
taxonomy. The learning domain class comprises of subclasses
cognitive, affective, and psychomotor, while the domain
categories are the learning activities that form the learning
domains. The model of the Learning Objective entity is shown
as part of figure 2. Learning objective assessment criteria are
categorized as quality, quantity, speed, and accuracy. Criteria
for learning objectives are described in terms of one or more
of these categories with a criteria value defined or determined
later when the assessment is defined. The Content entity of the
ontological model is based on content prioritization model by
Wiggins and McTighe [12]. Topics are categorized as either:
“good to know”, “important”, and “critical”. Content topics
are organized into a hierarchy of topics and sub-topics. The
ontological model supports the description of this hierarchy.

Figure 1: Overview of Course Design Components

III. IMOD-ONT: ONTOLOGY FOR INSTRUCTION DESIGN
In this section we describe the IMOD framework followed
by its translation into ontology called IMOD-Ont that is built
using Web Ontology Language (OWL) and Protege.
A. IMOD Framework
The IMOD framework called PC3 model adheres strongly to
the OBE approach and treats the course objective as the spine
of the structure as shown in Figure 1 (presented by the authors
at ASEE-PSW conference) [1]. The work of Robert Mager [9]
informs the IMOD definition of the objective. Mager
identifies three defining characteristics of a learning objective:
Performance – description of what the learner is expected to
be able to do; Conditions – description of the conditions under
which the performance is expected to occur; and the Criterion

355

	










Figure 2: Primary Classes and relationships in IMOD-Ont

action ‘apply’ indicates that the learning objective belongs to
the cognitive learning domain whose domain category is
applying. The system will also allow the user to enter the
content topics that the performance belongs to; for example,
the content topic in this instance could be sequence, selection,
and iteration. The percentage of accuracy with which the
student is supposed to carry out the performance forms the
Criteria part of the objective and is of the accuracy type.

Similarly Assessment and Pedagogy entities are defined that
describe the assessment and pedagogical techniques used for
instruction. The IMOD system provides information on latest
assessment and pedagogical techniques to its users. These
techniques are added to the IMOD repository in conformance
with the representation of the ontological entities for
assessment and pedagogy. IMOD class captures information
on the learning environment (e.g., type of course, meeting
days and times, anticipated number of hours that will be spent
on in-class and out-of-class activities, instructor(s)
information, course policies, etc.)

4) Semantic Querying
Relationships described between classes formalize the
alignment between various components of instruction design
though definition of object properties, data properties, class
hierarchy, etc. Semantic rules are setup for validated against
the user entered course design data and feedback is provided
to the instructional designer. Some of the rules are as follows:
• Every learning objective must have associated content,
assessment(s), and instructional/pedagogical strategy(s);
• Content prioritization is mapped to learning levels where by
topics that were categorized as critical are linked to higher
order levels of learning, and topics categorized under the
other two levels are linked to progressively to lower levels;

3) Instance data for IMOD-Ont
Consider the following example of a sample course on
Introduction to Programming with course number CST100
with the the following learning objective:
LO1: Given a program specification, apply the concepts of
sequence, selection, and iteration by constructing algorithms
and formal code for problem solving with 85% accuracy
In this example, the Condition is that a program specification
has been provided to the student, who is the target audience.
Performance is the act of applying the content concepts such
as sequence, selection, and iteration. The use of the learning

356

• Alignment of learning objective performance actions with
corresponding domain category and learning domain is
validated using the hierarchy;
• Instructional techniques suitable for the content topics and
performance level chosen in the learning objective are
suggested to the user. User is alerted whenever an incorrect
choice is made.
• Instructional techniques at a lower learning level than that
defined in the learning objective can also be allowed. For
example, LO1 shown in table 1 has learning level of apply.
In order to apply concepts, students should first remember
and understand those concepts, which are the lower order
learning levels. So using an instructional technique to teach
a lower learning level may be appropriate and the tool
would suggest them as well through extended search
feature.
• Similar to instructional techniques, assessment techniques
are also suggested based on the performance level of the
learning objective. Criteria component of learning objective
is used in determining appropriate assessments.
These rules support the implementation of the course design
fidelity testing in the IMOD software system.

V. SUMMARY AND FUTURE WORK
The IMODS prototype is currently under testing. Future work
on the ontology will involve providing alignment with existing
popular metadata schemas such as Dublin Core LOM, and
SCORM for the high-level and common entities (e.g., web
resources, course structure, etc.) such that IMOD-Ont can
complement these existing schemas in providing
interoperability solutions to the wealth of educational
resources and data that is available on the Web. Sharing and
reviewing of instructional modules designed using the IMOD
system and the ability to allow collaboration during the design
phase of an instructional module are also part of future work..
ACKNOWLEDGMENTS
The authors gratefully acknowledge the support for this project
under the National Science Foundation's Transforming
Undergraduate Education in Science, Technology, Engineering
and Mathematics (TUES) program Award No. DUE-1246139.
REFERENCES
[1]

[2]

[3]

[4]

[5]

Figure 3: IMOD-Ont schema layer and instance layer

IV. DISCUSSION

[6]

IMOD-Ont ontology is built in the Web Ontology Language
(OWL) that is used for publishing and sharing ontologies.
OWL is built upon the ontology language Resource
Description Framework (RDF) and the resulting ontology is a
RDF graph. Individuals with common characteristics can be
grouped together to form a class. OWL provides different
types of class descriptions that can be used to describe an
OWL class. OWL also provides two types of properties:
object properties and data properties. Object properties are
used to link individuals to other individuals while data
properties are used to link individuals to data values. We used
Protégé - an open-source ontology editor to create IMOD-Ont.
In the IMOD system, instruction design data entered by the
user is compared against the ontology for compliance and
feedback is offered to user on the design, cohesiveness of the
alignment of the course entities, and suggestions on
incomplete entities.

[7]

[8]

[9]

[10]
[11]

[12]

357

K. Andhare, O. Dalrymple, and S. Bansal, “Learning
Objectives Feature for Instructional Module Development
System,” presented at the PSW American Society for
Engineering Education Conference, San Luis Obispo,
California, 2012.
S. Ternier, E. Duval, and P. Vandepitte, “LOMster: peer-topeer learning object metadata,” in Proceedings of EdMedia,
2002, pp. 1942–1943.
“LOCO-Analyst.” [Online]. Available:
http://jelenajovanovic.net/LOCO-Analyst/index.html.
[Accessed: 28-May-2012].
S. Dietze, H. Q. Yu, D. Giordano, E. Kaldoudi, N. Dovrolis,
and D. Taibi, “Linked Education: interlinking educational
Resources and the Web of Data,” in Proc. of the 27th Annual
ACM Symposium on Applied Computing, 2012, pp. 366–371.
T. Berners-Lee, Y. Chen, L. Chilton, D. Connolly, R.
Dhanaraj, J. Hollenbach, A. Lerer, and D. Sheets, “Tabulator:
Exploring and analyzing linked data on the semantic web,” in
Proceedings of the 3rd International Semantic Web User
Interaction Workshop, 2006, vol. 2006.
M. Nilsson, M. Palmér, and J. Brase, “The LOM RDF
binding: principles and implementation,” in Proc. of the Third
Annual ARIADNE conference, Leuven Belgium, 2003.
“Dublin Core Metadata Initiative.” [Online]. Available:
http://dublincore.org/%20%20%20DBPedia%20http://wiki.db
pedia.org/About. [Accessed: 12-Jul-2014].
O. Bohl, J. Scheuhase, R. Sengler, and U. Winand, “The
sharable content object reference model (SCORM)-a critical
review,” in Computers in education, 2002. proceedings.
international conference on, 2002, pp. 950–951.
R. F. Mager, “Preparing Instructional Objectives: A critical
tool in the development of effective instruction 3rd edition,”
The Center for Effective Performance, Inc, 1997.
G. Antoniou and F. Van Harmelen, A semantic web primer.
the MIT Press, 2004.
N. Shadbolt, W. Hall, and T. Berners-Lee, “The semantic web
revisited,” Intelligent Systems, IEEE, vol. 21, no. 3, pp. 96–
101, 2006.
G. P. Wiggins and J. McTighe, Understanding by design.
Association for Supervision & Curriculum Development,
2005.

2008 IEEE International Conference on Web Services

Generalized Semantics-based Service Composition
Srividya Kona, Ajay Bansal, M. Brian Blake
Department of Computer Science,
Georgetown University
Washington, DC 20057

Abstract

cation possible. Informally, a service is characterized by its
input parameters, the outputs it produces, and the actions
that it initiates. The input parameter may be further subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order
to make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize, and
compose services automatically. To make services ubiquitously available we need a semantics-based approach such
that applications can reason about a service’s capability to a
level of detail that permits their discovery, composition, deployment, and synthesis [6]. Several efforts are underway
to build such an infrastructure [15, 17, 20].

Service-oriented computing (SOC) has emerged as
the eminent market environment for sharing and reusing
service-centric capabilities. The underpinning for an organization’s use of SOC techniques is the ability to discover
and compose Web services. Although industry approaches
to composition have a strong notion of business processes,
these approaches largely use syntactic descriptions. As
such composition is limited since the true functionality of
ambiguous service operations cannot be inferred. Alternatively, academia uses semantic approaches to disambiguate
services, but, at the same time, most of these approaches neglect the process rigor needed for complex compositions. In
this paper we present a generalized semantics-based technique for automatic service composition that combines the
rigor of process-oriented composition with the descriptiveness of semantics. Our generalized approach extends the
common practice of linearly linked services by introducing the use of a conditional directed acyclic graph (DAG)
where complex interactions, containing control flow, information flow and pre/post conditions, are effectively represented. Furthermore, the composition can be represented
semantically as OWL-S documents. Our contributions are
applied for automatic workflow generation in context of the
currently important bioinformatics domain.

With regards to service composition, a composite service
is a collection of services combined together in some way
to achieve a desired effect. Traditionally, the task of automatic service composition has been split into four phases:
(i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [26]. Most efforts reported in the literature focus on
one or more of these four phases. The first phase involves
generating a plan, i.e., all the services and the order in which
they are to be composed in order to obtain the composition.
The plan may be generated manually, semi-automatically,
or automatically. The second phase involves discovering
services as per the plan. Depending on the approach, often
planning and discovery are combined into one step. After all the appropriate services are discovered, the selection
phase involves selecting the optimal solution from the available potential solutions based on non-functional properties
like QoS properties. The last phase involves executing the
services as per the plan and in case any of them are not
available, an alternate solution has to be used.

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered, and consumed [1]. A Web service is an autonomous,
platform-independent program accessible over the web that
may affect some action or change in the world. Sample of
Web services include common plane, hotel, rental car reservation services or device controls like sensors or satellites.
As automation increases, these services will be accessed directly by applications rather than by humans [11]. In this
context, a Web service can be regarded as a “programmatic
interface” that makes application to application communi-

978-0-7695-3310-0/08 $25.00 © 2008 IEEE
DOI 10.1109/ICWS.2008.118

Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

In this paper we present a general approach for automatic
service composition. Our composition algorithm performs
planning, discovery, and selection automatically, all at once,
in one single process. This is in contrast to most methods
in the literature where one of the phases (most frequently
planning) is performed manually. Additionally, our method
generates most general compositions based on (conditional)
directed acyclic graphs (DAG). Note that service discovery

219

is a special case of composition of n services, i.e., when
n=1. Thus, we mainly study the general problem of automatically composing n services to satisfy the demand for
a particular service, posed as a query by the user. In our
framework, the DAG representation of the composite service is reified as an OWL-S description. This description
document can be registered in a repository and is thus available for future searches. The composite service can now
be discovered as a direct match instead of having to look
through the entire repository and build the composition solution again. We show how service composition can be applied to a Bioinformatics analysis application, for automatic
workflow generation in the field of Phylogenetics [4].
Our research makes the following novel contributions:
(i) We formalize the generalized composition problem
based on our conditional directed acyclic graph representation; (ii) we present an efficient and scalable algorithm
for solving the composition problem that takes semantics of
services into account; our algorithm automatically discovers and selects the individual services involved in composition for a given query, without the need for manual intervention; (iii) we automatically generate OWL-S descriptions of
the new composite service obtained; and, (iv) we apply our
generalized composition engine to automatically generate
workflows in the field of Bioinformatics.
The rest of the paper is organized as follows. In Section 2
we present the related work in the area of service composition and discuss their limitations. In Section 3, we formalize the generalized service composition problem followed
by a discussion of our technique for automatic Web service
composition and automatic generation of OWL-S service
descriptions in Section 4. Section 5 presents an application of our generalized composition engine to automatically
generate workflows for Bioinformatics analysis tasks. The
last section presents the conclusions and future work.

knowledge, most of these approaches that use planning are
restricted to sequential compositions, rather than a directed
acyclic graph. In this paper we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential but also nonsequential that can be represented in the form of a directed
acyclic graph. The authors in [18] present a composition
technique by applying logical inferencing on pre-defined
plan templates. Given a goal description, they use the logic
programming language Golog to instantiate the appropriate
plan for composing Web services. This approach also relies
on a user-defined plan template which is created manually.
One of the main objective of our work is to come up with
a technique that can automatically produce the composition
without the need for any manual intervention.
There are industry solutions based on WSDL and
BPEL4WS where the composition flow is obtained manually. BPEL4WS can be used to define a new Web service
by composing a set of existing ones. It does not assemble
complex flows of atomic services based on a search process. They select appropriate services using a planner when
an explicit flow is provided. In contrast, our technique automatically determines these complex flows using semantic
descriptions of atomic services.
A process-level composition solution based on OWL-S
is proposed in [19]. In this work the authors assume that
they already have the appropriate individual services involved in the composition, i.e., they are not automatically
discovered. They use the descriptions of these individual
services to produce a process-level description of the composite service. They do not automatically discover/select
the services involved in the composition, but instead assume that they already have the list of atomic services. In
contrast, we present a technique that automatically finds the
services that are suitable for composition based on the query
requirements for the new composed service.
There are solutions such as [23] that solve the selection
phase of composition. This work uses pre-defined plans
and discovered services provided in a matrix representation.
Then the best composition plans are selected and ranked
based on QoS parameters like cost, time, and reputation.
These criterion are measured using fuzzy numbers.
There has been a lot of work on composition languages
such as WS-BPEL, FuseJ, AO4BPEL, etc. which are useful
only during the execution phase. FuseJ is a description language for unifying aspects and components [22]. Though
this language was not designed for Web services, the authors contend that it can be used for service composition as
well. It uses connectors to interconnect services. We believe
that there is no centralized process description, but instead
information about services is spread across the connectors.
With FuseJ, the planning phase has to be performed manually, that is the connectors have to be written by the de-

2. Related Work
Composition of Web services has been active area of
research recently [25, 24, 26]. Most of these approaches
present techniques to solve one or more phases listed in
Section 1. There are many approaches [18, 19, 20] that
solve the first two phases of composition namely planning
and discovery. These are based on capturing the formal semantics of the service using action description languages
or some kind of logic (e.g., description logic). The service composition problem is reduced to a planning problem where the sub-services constitute atomic actions and
the overall service desired is represented by the goal to be
achieved using some combination of atomic actions. A
planner is then used to determine the combination of actions needed to reach the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available. To the best of our

220

I is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the list of
post-conditions. The pre- and post-conditions are ground
logical predicates.

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Definition (Query): The query service is defined as Q
= (CI  , I  , A , AO , O , CO ) where CI  is the list of preconditions, I  is the input list, A is the service affect, AO
is the affected object, O is the output list, and CO is the
list of post-conditions. These are all the parameters of the
requested service.

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph

veloper. Similarly, OWL-S also describes a composite service but does not automatically find the services involved
in the composition. So these languages are only useful for
execution which happens after the planning, discovery, and
selection of services is done. Service grounding of OWL-S
maps the described abstract service to the concrete WSDL
specification which helps in executing the service. In contrast, our approach automatically generates the composite
service. This new composite service generated can then be
described using one of these languages.
In this paper we present a technique for automatically
planning, discovering, and selecting services that are suitable for obtaining a composite service, based on the user
query requirements. As far as we know, all the related approaches to this problem assume that they either already
have information about the services involved or use human
input on what services would be suitable for composition.

Definition (Generalized Composition): The generalized
Composition problem can be defined as automatically finding a directed acyclic graph G = (V, E) of services from
repository R, given query Q = (CI  , I  , A , AO , O , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph either represents a service
involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can
be determined only after the execution of the service. Each
outgoing edge of a node (service) represents the outputs and
post-conditions produced by the service. Each incoming
edge of a node represents the inputs and pre-conditions of
the service. The following conditions should hold on the
nodes of the graph:
1. ∀i Si ∈ V where Si has exactly one incoming edge
that 
represents the query inputs and pre-conditions,
I   i I i , CI  ⇒∧i CI i .
2. ∀i Si ∈ V where Si has exactly one outgoing edge
that represents
the query outputs and post-conditions,

O  i Oi , CO ⇐∧i COi .
3. ∀i Si ∈ V where Si represents a service and has at
least one incoming edge, let Si1 , Si2 , ..., Sim be the
nodes such that there is a directed
 edge from each
of these nodes to Si . Then Ii  k Oik ∪ I  , CI i ⇐
(COi1 ∧COi2 ... ∧ COim ∧ CI  ).
4. ∀i Si ∈ V where Si represents a condition that is
evaluated at run-time and has exactly one incoming
edge, let Sj be its immediate predecessor node such
that there is a directed edge from Sj to Si . Then the
inputs and pre-conditions at node Si are Ii = Oj ∪ I  ,
CI i = COj . The outgoing edges from Si represent
the outputs that are same as the inputs Ii and the postconditions that are the result of the condition evaluation at run-time.
The meaning of the  is the subsumption (subsumes) relation and ⇒ is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage

3. Web service Composition
Informally, the Web service Composition problem can
be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested
service, in case a matching service is not found, the composition problem involves automatically finding a directed
acyclic graph of services that can be composed to obtain
the desired service. Figure 1 shows an example composite
service made up of five services S1 to S5 . In the figure, I 
and CI  are the query input parameters and pre-conditions
respectively. O and CO are the query output parameters
and post-conditions respectively. Informally, the directed
arc between nodes Si and Sj indicates that outputs of Si
constitute (some of) the inputs of Sj . We next formalize
the generalized composition problem. In this generalization, we extend our previous notion of composition [10] to
handle non-sequential conditional composition (which we
believe is the most general case of composition).
Definition (Repository of Services): Repository (R) is a
set of Web services.
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI, I, A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions,

221

in composition should imply the pre-conditions of services
in the next stage. When it cannot be determined at compile
time whether the post-conditions imply the pre-conditions
or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible
conditions which will be evaluated at run-time. Depending
on the condition that holds, the corresponding services are
executed. That is, if a subservice S1 is composed with subservice S2 , then the postconditions CO1 of S1 must imply
the preconditions CI 2 of S2 . The following conditions are
evaluated at run-time:
if (CO1 ⇒ CI 2 ) then execute S1 ;
else if (CO1 ⇒ ¬ CI 2 ) then no-op;
else if (CI 2 ) then execute S1 ;
With the above formalism in place, we next present our
generalized composition algorithm.

can see that every composite service generated by our algorithm is indeed a service that meets all the query requirements and hence is sound. Also our algorithm generates
every composite service possible for the given query and
hence is complete. Intuitively, we can see from the algorithm that these theorems hold and can be shown via proofs
by contradiction. These soundness and completeness proofs
are not included here due to lack of space.

Algorithm: GenerateServiceDescription
(Input: G - Solution Graph)
1. Generate generic header constructs
2. Start Composite Service element
3. Start SequenceConstruct
4.
If Number(SourceVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each starting/source Vertex V
GenerateAtomicService
End For
EndSplitJoinConstruct
End If
5.
If Number(SinkVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each ending/sink Vertex V
GenerateAtomicService
End For
EndSplitJoinConstruct
End If
6.
For Each remaining vertex V in G
If V is non-conditional vertex
with one outgoing edge
GenerateAtomicService
If V is non-conditional vertex
with more than one outgoing edge
GenerateSplitJoinConstruct
If V is Conditional vertex
with one outgoing edge
GenerateAtomicService
If V is Conditional vertex with
more than one outgoing edge
GenerateConditionalConstruct
End For
7. End SequenceConstruct
8. End Composite Service element
9. Generate generic footer constructs

4. Automatic Generation of Composite Services
In this section we present our algorithm for Automatic
Composition that produces a general directed acyclic graph.
The composition solution produced is for the generalized
composition problem presented in Section 3. We also
present our algorithm for automatic generation of OWL-S
descriptions for the new composite service produced.

4.1. Generalized Composition Algorithm
In order to produce the composite service which is the
graph, as shown in the example Figure 1, we filter out services that are not useful for the composition at multiple
stages. Figure 2 shows the filtering technique for the particular instance shown in Figure 1. The composition routine starts with the query input parameters. It finds all those
services from the repository which require a subset of the
query input parameters. In Figure 2, CI, I are the preconditions and the input parameters provided by the query.
S1 and S2 are the services found after step 1. O1 is the
union of all outputs produced by the services at the first
stage. For the next stage, the inputs available are the query
input parameters and all the outputs produced by the previous stage, i.e., I2 = O1 ∪ I. I2 is used to find services at
the next stage, i.e., all those services that require a subset
of I2 . In order to make sure we do not end up in cycles,
we get only those services which require at least one parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.
The Web service Composition algorithm presented here
has been proven to be sound and complete. Intuitively, we

Table 1. Automatic Generation of OWL-S description of Composite Service

4.2. Automatic Generation of OWL-S Description
After we have obtained a composition solution (sequential, non-sequential, or conditional), the next step is to produce a semantic description document for this new composite service. Then this document can be used for execution of the service and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of performing the composition process
all over again. We used the existing language OWL-S [7]

222

I=I
CI, I

1

S1
S2
.
.

O1

I=IUO
2

1

1

O

S

2

I=IUO
3

3

2

2

O

3

S

I=IUO
4

4

.
.

.
.

3

3

O

S

4

O

5

.
.

Figure 2. Generalized Composition Technique
to describe composite services. OWL-S models services as
processes and when used to describe composite services, it
maintains the state throughout the process. It provides control constructs such as Sequence, Split-Join, If-Then-Else
and many more to describe composite services. These control constructs can be used to describe the kind of composition. OWL-S also provides a property called composedBy
using which the services involved in the composition can be
specified. Table 1 presents the algorithm for automatic generation of the OWL-S description of the composite service.
A sequential composition can be described using the Sequence construct which indicates that all the services inside
this construct have to invoked one after the other in the same
order. The non-sequential composition can be described in
OWL-S using the Split-Join construct which indicates that
all the services inside this construct can be invoked concurrently. The process completes execution only when all
the services in this construct have completed their execution. The non-sequential conditional composition can be
described in OWL-S using the If-Then-Else construct which
specifies the condition and the services that should be executed if the condition holds and also specifies what happens
when the condition does not hold. Conditions in OWL-S
are described using SWRL.
There are other constructs such as looping constructs in
OWL-S which can be used to describe composite services
with complex looping process flows. We are currently investigating other kinds of compositions with iterations and
repeat-until loops and their OWL-S document generation.
We are exploring the possibility of unfolding a loop into a
linear chain of services that are repeatedly executed.

composition engine consists of modules: (i) Tuple Generator; (ii) Query Reader; (iii) SemanticRelations Generator;
(iv) Composition Query Processor; (v) OWL-S Description
Generator;
TupleGenerator converts each service in the repository
into the tuple format. The SemanticRelationsGenerator
module extracts all the semantic relations and creates a list
of Prolog facts. The CompositionQueryProcessor module
uses the repository of facts, which contains all the services,
their input and output parameters and the semantic relations
between the parameters. The output of the query processor
is the composition solution which is directed acyclic graph
of all the services involved in the composition. Our algorithm selects the optimal solution with least composition
length (i.e., the number of stages involved in the composition). The next step is to produce a description of the
new composite service solution found. OWL-S DescriptionGenerator automatically generates the OWL-S description
of the composite service using constructs depending on the
type of composition.

5. Application to Bioinformatics
We illustrate the practicality of our general framework
for automatically composing services by applying it to phylogenetics, a subfield of bioinformatics, for automatic generation of workflows. In this section, we present a brief
description of the field of Phylogenetics [4] followed by
an example of a workflow generation problem that can be
mapped to a non-sequential conditional composition problem (the most general case of the composition problem) and
can be solved using our generalized composition engine.

4.3. Implementation

Phylogenetics

We implemented a prototype composition engine using
Prolog with Constraint Logic Programming over finite domain [16], referred to as CLP(FD) hereafter. In our current
implementation, we used semantic descriptions of web services written in the language called USDL [9]. The repository of services contains one description document for each
service. USDL itself is used to specify the requirements of
the service that an application developer is seeking. The

Phylogenetic inferencing involves an attempt to estimate
the evolutionary history of a collection of organisms (taxa)
or a family of genes [5]. The two major components of this
task are the estimation of the evolutionary tree (branching
order), then using the estimated trees (phylogenies) as analytical framework for further evolutionary study and finally
performing the traditional role of systematics and classification. Using this study a number of interesting facts can

223

MIPSBlastBetterE13
Query
Inputs

CreateMobyData

MOBYSHoundGetGen
BankWhateverSequence

ExtractBestHit
Query
Outputs
ExtractAccession

Figure 3. Example of Non-Sequential Composition as a Directed Acyclic Graph
Service
Query
Create
MobyData
MOBYSHound
GetGenBank
WhateverSeq
MIPSBlast
BetterE13
Extract
Accession
Extract
BestHit

Pre-Conditions

Format(MobyData)
= NCBI

Input Parameters
GeneInput
GeneInput

Output Parameters
AccessionNumbers,
AGI, GeneIdentity
MobyData

MobyData

GeneSequence

GeneSequence

WUGeneSequence
AccessionNumbers
AGI,
GeneIdentity

GeneSequence
WUGeneSequence

Post-Conditions

Format(MobyData)
= NCBI

Table 2. Example Scenario for Non-Sequential Composition
be discovered, for example, who are the closest living relatives of humans, who are whales related to, etc. Different
studies can be conducted, for example, studying dynamics
of microbial communities, predicting evolution of influenza
viruses and other applications such as Drug Discovery, and
vaccine development, etc.

format translators and put them in the correct order to produce a workflow. We show how Web service composition
can be directly applied to automate this task of producing
a workflow. MyGrid [8] has a wealth of bioinformatics resources and data that provides opportunities for research. It
has hundreds of biological Web services and their WSDL
descriptions, provided by various research groups around
the world. We illustrate our generalized framework for Web
service composition by applying it to these services to generate workflows automatically that are practically useful for
this field.

In order to perform these tasks scientists use a number
of software tools and programs already available. They use
them by putting them together in a particular order (i.e., a
workflow) to get their desired results. That is, it involves
execution of a sequence of steps using various programs or
software tools. The software tools and programs created
for specific phylogenetic tasks use different data formats for
their input and output parameters. The data description language Nexus [12, 13] is used as a universal language for
representation of these bioinformatics related data. There
are translator programs that convert different formats into
Nexus and vice versa. For example, one could use the
BLAST program to get a sequence set of genes. Once the
sequence set is obtained, the sequences can be aligned using the CLUSTAL program. But the output from BLAST
cannot be directly fed to CLUSTAL as their data formats are
different. The translator can be used to convert the BLAST
format to Nexus and then the Nexus format to CLUSTAL.

Example 1: Workflow Generation (Non-sequential
Composition)
Suppose we are looking for a service that takes a GeneInput
and produces its corresponding AccessionNumbers, AGI,
and GeneIdentifier as output. The directory of services contains CreateMobyData, MOBYSHoundGetGenBankWhateverSequence, MIPSBlastBetterE13, ExtractAccession, and
ExtractBestHit services. In this scenario, the GeneInput
first needs to be converted to NCBI data format and then
its corresponding GeneSequence is further passed to ExtractAccession and ExtractBestHit to obtain the AccessionNumbers, AGI, and GeneIdentity respectively. Figure 3
shows this non-sequential composition example as a directed acyclic graph. In this example:

In order to perform an inferencing task, one has to manually pick all the appropriate programs and corresponding

• Service CreateMobyData has a post-condition on its

224

MEGA

Paup

PAUP
BLAST

Format(AlignedSequenceSet)
?

CLUSTAL
Query
Inputs

Query
Outputs

BlastNexus
Phylip
NexusClustal
Phylip

Figure 4. Example of Non-Sequential Conditional Composition as a Directed Acyclic Graph
output parameter MobyData that the format is NCBI
and the service MOBYSHoundGetGenBankWhateverSequence has a pre-condition that the its input parameter MobyData has to be in NCBI format for service execution. The post-condition of CreateMobyData should imply the pre-condition of the service
MOBYSHoundGetGenBankWhateverSequence.
• Both services ExtractAccession and ExtractBestHit
have to be executed to obtain the query outputs.
• The semantic descriptions of the service input/output
parameters should be the same as the query parameters
or have the subsumption relation. This can be inferred
using semantics from the ontology provided.

vice should imply the pre-conditions of the following service. The post-condition of the service CLUSTAL is that
the output parameter AlignedSequenceSet has either Paup or
Phylip format. Depending on which one of these two conditions hold, the next service for the composition is chosen.
In this case, one cannot determine if the post-conditions of
the service CLUSTAL imply the pre-conditions of PAUP or
PHYLIP until the services are actually executed. In such a
case, a condition can be generated which will be evaluated
at runtime and depending on the outcome of the condition,
corresponding services will be executed. The vertex for service CLUSTAL in the Figure 4 has an outgoing edge to a
conditional node. The outgoing edge represents the outputs
and post-conditions of the service. The conditional node
has multiple outgoing edges which represent the generated
conditions that are evaluated at run-time. In this case the
following conditions are generated:
• (Format(AlignedSequenceSet) = Paup ∨ Format(AlignedSequenceSet) = Phylip) ⇒ (Format(AlignedSequenceSet) = Paup
• (Format(AlignedSequenceSet) = Paup ∨ Format(AlignedSequenceSet) = Phylip) ⇒ (Format(AlignedSequenceSet) = Phylip
Depending on the condition that holds, the corresponding
services PAUP and MEGA or PHYLIP are executed respectively. The outputs EvolutionTree and EvolutionDistance
are produced in both the cases along with the post-condition
that the format of the evolution tree is Newick. Figure 4
shows this non-sequential conditional composition example
as a conditional directed acyclic graph.

Example 2: Workflow Generation (Non-sequential Conditional Composition)
Suppose we are looking for a service that takes a GeneSequence and produces an EvolutionTree and EvolutionDistance after performing a phylogenetic analysis. Also the
service should satisfy the post-condition that the EvolutionTree produced is in the Newick format. This involves producing a sequence set first, followed by aligning the sequence set and then producing the evolution tree and evolution distance. Also any necessary intermediate data format
translations have to be performed. Table 3 shows the services in the repository and their corresponding input/output
parameters and user-query. For the sake of simplicity, the
query and services have fewer input/output parameters than
the real-world services.
In this example, service BLAST has to be executed first
so that its output BLASTSequenceSet can be used as input
by CLUSTAL after the data format has been translated using BLASTNexus and NexusCLUSTAL. The service BLASTNexus has a post-condition that the format of the output
parameter NexusSequenceSet is Nexus which is the precondition of the next service NexusCLUSTAL. Similarly the
service NexusCLUSTAL has a post-condition that the format of the output parameter ClustalSequenceSet is Clustal
which is the pre-condition of the next service CLUSTAL.
At every step of composition, the post-conditions of a ser-

6. Performance
We tested our composition engine using repositories
from WS-Challenge website [14], slightly modified to fit
into our framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The queries and solutions are provided in an XML format. The semantic
relations between various parameters are provided in an
XML Schema file. We evaluated our approach on three

225

Service
Query

Pre-Conditions

Input Parameters
Sequence,OrganismType,
WordSize,DatabaseName
Sequence,OrganismType,
DatabaseName
ClustalSequenceSet

Output Parameters
EvolutionTree,
EvolutionDistance
BlastSequenceSet

Post-Conditions
Format(EvolutionTree)=Newick

AlignedSequenceSet

BLASTNexus

BlastSequenceSet

NexusSequenceSet

NexusCLUSTAL Format(NexusSequenceSet)=Nexus
PAUP
Format(AlignedSequenceSet)=Paup
PHYLIP
Format(AlignedSequenceSet)=Phylip
MEGA
Format(AlignedSequenceSet)=Paup

NexusSequenceSet

ClustalSequenceSet

AlignedSequenceSet,
WordSize
AlignedSequenceSet

EvolutionTree

Format(AlignedSequenceSet)=Paup ∨
Format(AlignedSequenceSet)=Phylip
Format(NexusSequenceSet)=Nexus
Format(ClustalSequenceSet)=Clustal
Format(EvolutionTree)=Newick
Format(EvolutionTree)=Newick

AlignedSequenceSet

EvolutionDistance

BLAST
CLUSTAL

Format(ClustalSequenceSet)=Clustal

EvolutionTree

Table 3. Example Scenario for Non-Sequential Conditional Composition
different kind of queries that produce sequential compositions, non-sequential compositions, and non-sequential
conditional compositions. We tested the engine on different
size repositories and tabulated Pre-processing and Query
Execution time. Table 4 shows the performance results for
these three kind of compositions. We noticed that there was
a significant difference in pre-processing time and query execution time. Also the pre-processing performed on the
repositories remains the same irrespective of the kind of
queries. Also the query execution time is slightly higher for
the case of non-sequential and non-sequential conditional
composition. The results are consistent with our expectations: for a fixed repository size, the preprocessing time
increases with the increase in number of input/output parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, the efficiency of service query processing is
negligible (just 1 to 3 msecs) even for complex queries with
large repositories.

tion, etc.). The composition flow is determined automatically without the need for any manual intervention. Our engine finds any sequential, non-sequential or non-sequential
conditional composition that is possible for a given query
and also automatically generates OWL-S description of the
composite service. This OWL-S description can be used
during the execution phase and subsequent searches for this
composite service will yield a direct match. We are able to
apply many optimization techniques to our system so that
it works efficiently even on large repositories. Use of Constraint Logic Programming helped greatly in obtaining an
efficient implementation of this system.
Our future work includes investigating other kinds of
compositions with loops such as repeat-until and iterations
and their OWL-S description generation. Analyzing the
choice of the composition language (e.g., BioPerl [3] for
phylogenetic workflows) and exploring other language possibilities is also part of our future work. We are also exploring combining technologies of automated service composition and domain specific languages to develop a framework
for problem solving and software engineering [2].

7. Conclusions and Future Work
To make Web services more practical we need an infrastructure that allows users to discover, deploy, synthesize and compose services automatically. Our semanticsbased approach uses semantic description of Web services
to find substitutable and composite services that best match
the desired service. Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services, number of services in a composi-

References
[1] G. Castagna, N. Gesbert, L. Padovani, et al. A Theory of Contracts for Web Services. In Symposium on
Principles of Programming Languages, January 2008
[2] G. Gupta, S. Kona, B. Devries, et al. Problem
Solving in Evolutionary Analysis with Web Ser-

226

Repository
Size
(num of
services)
2000
2000
2000
2500
2500
2500
3000
3000
3000

Number
of I/O
param-eters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.5
45.8
57.8
47.7
58.7
71.6
56.8
77.1
88.2

QueryExec Time (msecs)
Sequential
Composition
1
1
2
1
1
2
1
1
3

NonSequential
Composition
1
1
2
1
2
2
1
2
3

Conditional
Composition
1
2
2
1
2
3
1
3
4

Table 4. Performance of Generalized Composition Engine
vices and DSLs. https://www.nescent.org/
wg_evoinfo/Announcements.

[14] WS Challenge 2006. http://insel.flp.cs.
tu-berlin.de/wsc06.
[15] D. Mandell, S. McIlraith Adapting BPEL4WS for the
Semantic Web: The Bottom-Up Approach to Web Service Interoperation. In ISWC, 2003.
[16] K. Marriott and P. Stuckey. Prog. with Constraints:
An Introduction. MIT Press, 1998.
[17] M. Paolucci, T. Kawamura, T. Payne, and K. Sycara
Semantic Matching of Web Service Capabilities. In
ISWC, pages 333-347, 2002.
[18] S. McIlraith, T. Son Adapting golog for composition
of semantic Web services. In KRR, pp.482–493, 2002.
[19] M. Pistore, P. Roberti, and P. Traverso Process-Level
Composition of Executable Web Services In European
Semantic Web Conference, pages 62-77, 2005.
[20] J. Rao, D. Dimitrov, P. Hofmann, and N. Sadeh A
Mixed-Initiative Approach to Semantic Web Service
Discovery and Composition In International Conference on Web Services, 2006.
[21] P. Hofmann. SAP AG. Personal Communication.
[22] D. Suvee, B. Fraine, and M. Cibran Evaluating FuseJ
as a Web Service Composition Language In European
Conference on Web Services, 2005.
[23] D. Claro, P. Albers, and J. Hao Selecting Web services
for Optimal Compositions In Workshop on Semantic
Web Services and Web Service Composition, 2004.
[24] B. Srivastava, J. Koehler. Web Services Composition Current Solutions and Open Problems In ICAPS, 2003.
[25] J. Rao, X. Su. A Survey of Automated Web Service
Composition Methods In Workshop on Semantic Web
Services and Web Process Composition, 2004.
[26] J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

[3] BioPerl. http://www.bioperl.org.
[4] A.W.F. Edwards, L.L. Cavalli-Sforza (1964). Systematics Assoc. Publ. No. 6: Phenetic and Phylogenetic Classification: Reconstruction of evolutionary trees, 67-76.
[5] J. Felsenstein. Inferring Phylogenies Sinauer; 2 ed.,
2003
[6] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp.
46-53, March 2001.
[7] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[8] MyGrid. http://www.mygrid.org.uk/.
[9] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In ECOWS, pp. 214-225, 2005.
[10] S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007.
[11] A. Bansal, K. Patel, G. Gupta, B. Raghavachari,
E. Harris, and J. Staves. Towards Intelligent Services:
A case study in chemical emergency response. In
ICWS, pp.751-758, 2005.
[12] J. Iglesias, G. Gupta, E. Pontelli, D. Ranjan, and
B. Milligan. Interoperability between Bioinformatics
Tools: A Logic Programming Approach. In Practical Aspects of Declarative Languages (PADL), Lecture
Notes in Computer Science. Springer Heidelberg, 2001.
[13] G. Gupta, et al. Semantics-Based Filtering: Logic Programming’s Killer App In International Symposium on
Practical Aspects of Declarative Languages (PADL),
pages 82–100. Springer-Verlag, 2002.

227

SOCA (2016) 10:111–133
DOI 10.1007/s11761-014-0167-5

ORIGINAL RESEARCH PAPER

Generalized semantic Web service composition
Srividya Bansal · Ajay Bansal · Gopal Gupta ·
M. Brian Blake

Received: 29 January 2014 / Revised: 31 August 2014 / Accepted: 23 October 2014 / Published online: 8 November 2014
© Springer-Verlag London 2014

Abstract With the increasing popularity of Web Services
and Service-Oriented Architecture, we need infrastructure
to discover and compose Web services. In this paper, we
present a generalized semantics-based technique for automatic service composition that combines the rigor of processoriented composition with the descriptiveness of semantics.
Our generalized approach presented in this paper introduces
the use of a conditional directed acyclic graph where complex
interactions, containing control flow, information flow, and
pre-/post-conditions are effectively represented. Composition solution obtained is represented semantically as OWL-S
documents. Web service composition will gain wider acceptance only when users know that the solutions obtained are
comprised of trustworthy services. We present a framework
that not only uses functional and non-functional attributes
provided by the Web service description document but also
filters and ranks solutions based on their trust rating that is
computed using Centrality Measure of Social Networks. Our
contributions are applied for automatic workflow generation
in context of the currently important bioinformatics domain.
We evaluate our engine for automatic workflow generation
of a phylogenetic inference task. We also evaluate our engine
S. Bansal (B) · A. Bansal
Arizona State University, Mesa, AZ, USA
e-mail: srividya.bansal@asu.edu
A. Bansal
e-mail: ajay.bansal@asu.edu
G. Gupta
The University of Texas at Dallas, Richardson, TX, USA
e-mail: gupta@utdallas.edu
M. B. Blake
University of Miami, Miami, FL, USA
e-mail: m.brian.blake@miami.edu

for automated discovery and composition on repositories of
different sizes and present the results.
Keywords Service composition · Service discovery ·
Semantic Web · Ontology · Workflow generation

1 Introduction
The next milestone in the evolution of the World Wide Web
is making services ubiquitously available. As automation
increases, Web services will be accessed directly by the applications themselves rather than by humans [1,2]. In this context, a Web service can be regarded as a “programmatic interface” that makes application-to-application communication
possible. To make services ubiquitously available, we need
infrastructure that applications can use to automatically discover, deploy, compose, and synthesize services. A Web service is an autonomous, platform-independent program accessible over the web that may affect some action or change in
the world. Sample of Web services include common plane,
hotel, rental car reservation services or device controls like
sensors or satellites. A Web service can be regarded as a “programmatic interface” that makes application-to-application
communication possible. Informally, a service is characterized by its input parameters, the outputs it produces, and the
actions that it initiates. The input parameter may be further
subject to some pre-conditions, and likewise, the outputs produced may have to satisfy certain post-conditions. In order
to make Web services more practical, we need an infrastructure that allows users to discover, deploy, synthesize, and
compose services automatically. To make services ubiquitously available, we need a semantics-based approach such
that applications can reason about a service’s capability to
a level of detail that permits their discovery, composition,

123

112

deployment, and synthesis [3]. Several efforts are underway
to build such an infrastructure [4–6].
With regard to service composition, a composite service
is a collection of services combined together in some way to
achieve a desired effect. Traditionally, the task of automatic
service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [7].
Most efforts reported in the literature focus on one or more
of these four phases. The first phase involves generating a
plan, i.e., all the services and the order in which they are to
be composed in order to obtain the composition. The plan
may be generated manually, semi-automatically, or automatically. The second phase involves discovering services as per
the plan. Depending on the approach, often planning and discovery are combined into one step. After all the appropriate
services are discovered, the selection phase involves selecting the optimal solution from the available potential solutions
based on non-functional properties like QoS properties. The
last phase involves executing the services as per the plan and
in case any of them are not available, an alternate solution
has to be used.
In this paper, we present a general approach for automatic
service composition. Our composition algorithm performs
planning, discovery, and selection automatically, all at once,
in one single process. This is in contrast to most methods
in the literature where one of the phases (most frequently
planning) is performed manually. Additionally, our method
generates most general compositions based on (conditional)
directed acyclic graphs (DAG). Note that service discovery is
a special case of composition of n services, i.e., when n = 1.
Thus, we mainly study the general problem of automatically
composing n services to satisfy the demand for a particular
service, posed as a query by the user. In our framework,
the DAG representation of the composite service is reified
as an OWL-S description. This description document can
be registered in a repository and is thus available for future
searches. The composite service can now be discovered as
a direct match instead of having to look through the entire
repository and build the composition solution again. We show
how service composition can be applied to a Bioinformatics
analysis application, for automatic workflow generation in
the field of Phylogenetics [8].
One of the current challenges in automatic composition
of Web services also includes finding a composite Web service that can be trusted by consumers before using it. Our
approach uses analysis of Social Networks to calculate a
trust rating for each Web service involved in the composition
and further prune results based on this rating. Web-based
Social Networks have become increasingly popular these
days. Social Network Analysis is the process of mapping
and measuring the relationships between connected nodes.
These nodes could represent people, groups, organizations,
computers, or any knowledge entity. We propose to measure

123

SOCA (2016) 10:111–133

the trust factor of a service by measuring the centrality of
a service provider and/or a service provider organization in
a well-known Social Network. The three level indices that
can be applied to measure centrality are degree, betweenness, and closeness [9]. We adopt our idea of computing
trust using centrality measure based on the notion of centrality and prestige being key in the study of social networks
[9,10]. The role of central people (nodes with high centrality) in a network seems to be fundamental as they adopt the
innovation and help in transportation and diffusion of information throughout the rest of the network. So our rationale
is that these central figures who play a fundamental role in
the network are trusted by others in the network who are
connected (directly or indirectly) to them.
A simple use case scenario Jane is a researcher in the field
of Evolutionary Genetics. One evening she is examining the
evolution of crab species and needs to build a Phylogenetic
tree for various crab species using protein sequence data. In
order to complete this task, she will have to go to her lab and
access the computer with necessary software and perform
multiple computations using various algorithms. She uses
the well-known Molecular Evolutionary Genetics Analysis
software program (MEGA5) [11] that is an integrated tool
for conducting automatic and manual sequence alignment,
inferring phylogenetic trees, mining web-based databases,
estimating rates of molecular evolution, inferring ancestral
sequences, and testing evolutionary hypotheses. Jane has to
use this software to first align the sequence data using one
of several algorithms (such as Clustal W [12], MUSCLE
[13], etc.) provided by MEGA5 for this purpose. Next, she
wants to compute and compare the evolutionary distances
for sequences from crab species using various algorithms. In
order to do this she must first compute relevant models for
crab species. The next step is to compute evolutionary distances using Jukes–Cantor model followed by Tamura–Nei
model and compare them. She is also interested in the evolutionary distance computed based on proportion of amino acid
differences. Finally, she is interested in building a Phylogenetic tree from the aligned sequence data. She will have to
pick one of the algorithms/methods provided by MEGA5 that
include Maximum Likelihood, Minimum Evolution, Maximum Parsimony, Neighbor-Joining, etc. Currently, there is
no easy way to perform this analysis and she will have to use
the software tools manually and go through this step-by-step
process and wait while computation for each of the steps is
being performed. Jane will have to go through this laborintensive process in order to find an answer to her research
question.
Imagine a Software-as-a-Service (SaaS) platform [14]
available on the cloud and Jane has access to it from any
computer or mobile device. With a few simple clicks, Jane
provides a query request that includes input parameters to this
workflow process and expected final outputs. The software

SOCA (2016) 10:111–133

platform built upon our composition engine produces multiple possible workflows using different combinations of algorithms/methods (available as Web services) for each of the
tasks such as sequence alignment, model computation, distance computation, and generation of phylogeny. Jane picks a
workflow that is most suited for her research analysis and possibly even edits the workflow by adding a service to compute
the diversity in the subpopulation of crabs. She saves off this
workflow to her profile for future use. She initiates the workflow execution and on the following day, analyses the output
results that were produced and saved off in her account. This
software platform is able to save Jane a significant amount of
time—not only in performing the computations for analysis,
but also with configuring workflows and using interesting
workflows already created by her colleagues. This is just one
simple example of the potential of Web service discovery
and composition in various disciplines. This paper presents
the underlying composition engine that is needed in order to
build such a software platform.
This paper extends our previous work in the area of Web
service composition [15] by conducting a case study on automatic workflow generation for Phylogenetic Inference tasks
in Bioinformatics using our composition engine and introducing the computation of a trust rating of each Web service,
based on Centrality measure in Social Network analysis, and
using this trust rating in filtering and ranking services. This
work would support the development of a SaaS platform that
supports domain-specific workflow generation. Our research
makes the following novel contributions:
(i) Formalization of the generalized composition problem based on our conditional directed acyclic graph representation; (ii) Computation of trust rating of composition
solutions based on individual ratings of service providers
obtained using the Centrality measure of Social Networks;
(iii) Efficient and scalable algorithm for solving the composition problem that takes semantics of services into account;
our algorithm automatically discovers and selects the individual services involved in composition for a given query,
without the need for manual intervention; (iv) Automatic
generation of OWL-S descriptions of the new composite service obtained; (v) Case study of our generalized composition
engine to automatically generate workflows in the field of
Bioinformatics for Phylogenetic Inference tasks.
The rest of the paper is organized as follows. In Sect. 2, we
present the related work in the area of Web service discovery
and composition and discuss their limitations. In Sect. 3, we
formalize the generalized Web service composition problem.
We present our multi-step narrowing technique for automatic
Web service composition and automatic generation of OWLS service description in Sect. 4. We present the implementation and experimental results in Sect. 5. Section 6 presents
an application of our generalized composition engine to
automatically generate workflows for Bioinformatics analy-

113

sis tasks. The last section presents conclusions and future
work.

2 Related work
Composition of Web services has been active area of research
[7,16,17]. Most of these approaches present techniques to
solve one or more phases of composition as listed in Sect. 1.
There are many approaches [6,18,19] that solve the first two
phases of composition namely planning and discovery. These
are based on capturing the formal semantics of the service
using action description languages or some kind of logic
(e.g., description logic). The service composition problem
is reduced to a planning problem where the sub-services
constitute atomic actions and the overall service desired is
represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine the combination of actions needed to reach the goal.
With this approach an explicit goal definition has to be provided, whereas such explicit goals are usually not available.
To the best of our knowledge, most of these approaches that
use planning are restricted to sequential compositions, rather
than a directed acyclic graph. In this paper, we present a technique to automatically select atomic services from a repository and produce compositions that are not only sequential
but also non-sequential that can be represented in the form of
a directed acyclic graph. The authors in [18] present a composition technique by applying logical inferencing on predefined plan templates. Given a goal description, they use the
logic programming language Golog to instantiate the appropriate plan for composing Web services. This approach also
relies on a user-defined plan template, which is created manually. One of the main objectives of our work is to come up
with a technique that can automatically produce composition
without the need for any manual intervention. Boustil et al.
[20] present an approach that uses an intermediate ontology
built using OWL-DL and SWRL rules to define the affected
object and their relationships. Their selection strategy considers relationships between services by looking at object
values of affected objects. They use a custom intermediate
ontology that is built within their framework using OWL-DL.
Our approach focuses on the semantics of the parameters as
well as constraints represented as pre- and post-conditions.
Also our approach is generic and can be used with any domain
ontology to provide semantics.
There are industry solutions based on WSDL and BPEL4
WS where the composition flow is obtained manually.
BPEL4WS can be used to define a new Web service by
composing a set of existing ones. It does not assemble complex flows of atomic services based on a search process.
They select appropriate services using a planner when an
explicit flow is provided. In contrast, our technique auto-

123

114

matically determines these complex flows using semantic
descriptions of atomic services. A process-level composition
solution based on OWL-S is proposed in [19]. In this work,
the authors assume that they already have the appropriate
individual services involved in the composition, i.e., they are
not automatically discovered. They use the descriptions of
these individual services to produce a process-level description of the composite service. They do not automatically discover/select the services involved in the composition, but
instead assume that they already have the list of atomic services. In contrast, we present a technique that automatically
finds the services that are suitable for composition based on
the query requirements for the new composed service. There
are solutions such as [21] that solve the selection phase of
composition. This work uses pre-defined plans and discovered services provided in a matrix representation. Then, the
best composition plans are selected and ranked based on QoS
parameters like cost, time, and reputation. These criterions
are measured using fuzzy numbers.
There has been a lot of work on composition languages
such as WS-BPEL, FuseJ, AO4BPEL, etc. which are useful only during the execution phase. FuseJ is a description
language for unifying aspects and components [22]. Though
this language was not designed for Web services, the authors
contend that it can be used for service composition as well.
It uses connectors to interconnect services. We believe that
there is no centralized process description, but instead information about services is spread across the connectors. With
FuseJ, the planning phase has to be performed manually that
is the connectors have to be written by the developer. Similarly, OWL-S also describes a composite service but does not
automatically find the services involved in the composition.
So these languages are only useful for execution which happens after the planning, discovery, and selection of services
is done. Service grounding of OWL-S maps that describe
abstract services to the concrete WSDL specification helps
in executing the service. In contrast, our approach automatically generates the composite service. This new composite
service generated can then be described using one of these
composition languages.
QoS-aware composition has also been active area of
research [6,21]. Research on a QoS-aware composition [23–
25] consider applying SLA’s to workflow compositions or
Web service compositions, although they do not perform
dynamic composition. They use one of the existing composition languages to create the composite service manually or create a template that is later used to select appropriate services for each stage of composition. After obtaining composition solutions manually or semi-automatically,
these approaches present a QoS model and apply the nonfunctional attributes on the potential solutions to confirm
that they comply with the pre-defined agreements. Thus,
the solutions are pruned based on SLA compliance. Work

123

SOCA (2016) 10:111–133

on workflow Composition of service-level agreements [26]
presents a set of SLA measures and principles that best support QoS-based Composition. A model and representation of
SLA attributes were introduced and an approach to compose
SLA’s associated with a workflow of Web services was presented. The research on creating a QoS-Aware middleware
for Web service Composition in [27] is similar to our work
as they identify services that can fit into a useful composition
based on QoS measures. They use two approaches for selection: one based on local (task-level) selection of services and
the second is based on a global allocation of tasks to services.
They also use a template for composition; in this case, a state
chart that has the generic service tasks defined. Finding a
composite service involves finding concrete services that fit
into the template. In contrast, we do not use any template
but instead find the composition solution automatically. The
work presented in [28] combine semantic annotations and
SLA’s thereby providing better approach to specification of
SLA’s.
Researchers have looked into a fuzzy linguistic preference model to provide preference relations on various QoS
dimensions [29]. They use a specific weighting procedure to
provide numeric weights to preference relations, and then
use a hybrid evolutionary algorithm to find skyline solutions efficiently. Their algorithm is designed on the basis
of Pareto-dominance and weighted Tchebycheff distance. In
this approach, the authors assume that they have candidate
services for composition. Their algorithm helps identify best
solution based on their SLA’s.
Feng’s research group proposed an approach to composition that associated QoS attributes to service dependencies
and showed their approach could model real-life services and
perform effective QoS constraint satisfaction and optimization [30]. The attributes taken into consideration by this study
are Response time, Cost, Reliability, Availability, and Reputation. They consider that QoS values of a service may be
dependent not only on the service itself but also some other
services in the workflow. They propose 3 types of QoS for
each attribute namely: default QoS, partially dependent QoS,
and totally dependent QoS. Default QoS applies no matter
what the preceding service is in a workflow, just like the conventional QoS. Partially dependent QoS applies if and only if
some of the inputs of a service are provided by the outputs of
another service. Totally dependent QoS applies if and only if
all inputs of a service are provided by the outputs of another
service. Formal modeling of QoS attributes is provided in
OWL-S [27]. Work by Wen et al. [32] presents an approach
to obtaining probabilistic top-K dominating services with
uncertain QoS. QoS values tend to fluctuate at run-time and
hence this approach uses probabilistic characteristics of service instances to identify dominating service abilities for better selection. A detailed survey of approaches for a reliable
dynamic Web service composition is presented by Immonen

SOCA (2016) 10:111–133

115

and Pakkala [33]. They discuss various approaches that use
Reliability ontology to manage and achieve reliable composition. They address the lack of formalization to handle reliability of composition, whereas the focus of our approach is the
formalization of a generalized composition that uses functional attributes to compose a solution and non-functional
attributes help in further filtering and ranking solutions.
A number approaches focus on trust and reputation QoS
criteria for service selection. Mehdi et al.’s [34] approach
assigns trust scores to Web services and only services
with highest scores are selected for composition. They use
Bayesian networks to learn the structure of composition. The
approach presented by Kutler et al. [35] considers social trust
in Web service composition. They compute trust based on
similarity measures over ratings of users added into a system.
They use the correlation between trust and overall similarity
measures in online communities. On the contrary, we use the
centrality measure in a Web-based Social Network.
In this paper, we present a technique for automatically
planning, discovering, and selecting services that are suitable for obtaining a composite service based on user-query
requirements. As far as we know, all the related approaches
to this problem assume that they either already have information about services involved or use human input on what
services would be suitable for composition. This work is an
extension of our earlier work [15] that introduced a generalized Web service composition engine. In this paper, we
use the trust rating of a Web service in addition to the functional and non-functional attributes of a service in filtering
and ranking solutions. In this paper, we evaluate our composition the engine using a case study from the bioinformatics
domain for Phylogenetic inference tasks to show that this
engine can be used for automatic workflow generation. The
case study uses example workflows that would be generated
as a sequential composition, non-sequential composition as
well as non-sequential conditional composition.

3 Automated Web service discovery and composition
Discovery and composition are two important tasks related
to Web services. In this section, we formally describe these

tasks and develop the requirements of an ideal discovery/composition engine.
3.1 The discovery problem
Given a repository of Web services, and a query requesting
a service (hereafter query service), automatically finding a
service from the repository that matches these requirements
is the Web service discovery problem. Only those services
that produce at least the requested output parameters that
satisfy the post-conditions and use only from the provided
input parameters that satisfy the pre-conditions and produce
the same side effects can be valid solutions to the query. Some
of the solutions may be over-qualified, but they are still considered valid as long as they fulfill input and output parameters, pre-/post-conditions, and side effect requirements. This
activity is best illustrated using an example:
Example (Discovery) A buyer is looking for a service to buy
a book and the directory of services contains services S1 and
S2 . Table 1 shows the input/output parameters of the query
and services S1 and S2 . In this example service S2 satisfies
the query; however, S1 does not as it requires BookISBN
as an input and it is not provided by the query. Our query
requires ConfirmationNumber as the output and S2 produces
ConfirmationNumber and TrackingNumber. The extra output
produced can be ignored. Also the semantic descriptions of
service input/output parameters should be same as the query
parameters or satisfy the subsumption relation. The discovery engine should be able to infer that the query parameter
BookTitle and input parameter BookName of service S2 are
semantically the same concepts. This can be inferred using
semantics from the annotation of the service and the ontology (e.g., OWL WordNet ontology) provided. The query also
has a pre-condition that the CreditCardNumber is numeric,
which should logically imply pre-conditions of the discovered service.
Definition (Service) A service is a 6-tuple of its preconditions, inputs, side effect, affected object, outputs and
post-conditions.
S = (CI, I, A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions, I is the input list,

Table 1 Discovery—example
Service

Input parameters

Pre-conditions

Output parameters

Query

BookTitle, CreditCardNumber,
AuthorName, CreditCardType

lsNumeric(CreditCard Number)

ConfirmationNumber

S1

BookName, AuthorName,
BooklSBN, CreditCardNumber

S2

BookName, CreditCardNumber

Post-conditions

ConfirmationNumber
lsNumeric(CreditCard Number)

ConfirmationNumber,
TrackingNumber

123

116

SOCA (2016) 10:111–133

A is the service’s side effect, AO is the affected object, O is
the output list, and CO is the list of post-conditions. The preand post-conditions are ground logical predicates.
Definition (Repository of Services) Repository (R) is a set
of Web services.
Definition (Query) The query service is defined as Q =
(CI , I  , A , AO , O  , CO ) where CI is the list of preconditions, I  is the input list, A is the service affect, AO is
the affected object, O  is the output list, and CO is the list of
post-conditions. These are all the parameters of the requested
service.
Definition (Discovery) Given a repository R and a query
Q, the discovery problem can be defined as automatically
finding a set S of services from R such that S = {s | s =
(CI, I, A, AO, O, CO), s  R, CI ⇒ CI, I  I  , A =
A , AO = AO  , CO ⇒ CO , O ⊇ O  }. The meaning
of  is the subsumption (subsumes) relation and ⇒ is the
implication relation. For example, say x and y are input and
output parameters, respectively, of a service. If a query has
(x > 5) as a pre-condition and (y > −x) as post-condition,
then a service with pre-condition (x > 0) and post-condition
(y > x) can satisfy the query as (x > 5) ⇒ (x > 0) and

Fig. 1 Substitutable service

Fig. 2 Composite service represented as a directed acyclic graph

(y > x) ⇒ (y > −x) since (x > 0). Figure 1 shows the
substitution rules for the discovery problem.
3.2 The composition problem
Given a repository of service descriptions, and a query with
the requirements of the requested service, in case a matching
service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can
be composed to obtain the desired service. Figure 2 shows
an example composite service made up of five services S1 to
S5 . In figure, I  and CI are the query input parameters and
pre-conditions, respectively. O  and CO are the query output
parameters and post-conditions respectively. Informally, the
directed arc between nodes Si and S j indicates that outputs
of Si constitute (some of) the inputs of S j .
Example (Sequential Composition) Suppose we are looking for a service to make travel arrangements, i.e., flight,
hotel, and rental car reservations. The directory of services
contains ReserveFlight, ReserveHotel, and ReserveCar services. Table 2 shows the input/output parameters of the user
query and the three services ReserveFlight, ReserveHotel,
and ReserveCar. For the sake of simplicity, the query and
services have fewer input/output parameters than the realworld services. In this example, service ReserveFlight has
to be executed first so that its output ArrivalFlightNum can
be used as input by ReserveHotel followed by the service
ReserveCar which uses the output HotelAddress of ReserveHotel as its input. The semantic descriptions of the service
input/output parameters should be the same as the query parameters or have the subsumption relation. This can be inferred
using semantics from the ontology provided. Figure 3 shows
this example sequential composition as a directed acyclic
graph.
Definition (Sequential Composition) The sequential Composition problem can be defined as automatically finding a
directed acyclic graph G = (V, E) of services from repository R, given query Q = (CI , I  , A , AO , O  , CO ),
where V is the set of vertices and E is the set of edges

Table 2 Sequential composition example
Service

Input parameters

Query

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

HotelConfirmationNum,
CarConfirmationNum

ReserveFlight

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

FlightConfirmationNum,
ArrivalFlightNum

ReserveHotel

PassengerName, ArrivalFlightNum,
StartDate, ReturnDate

HotelConfirmationNum,
HotelAddress

ReserveCar

PassengerName, ArrivalDate,
ArrivalFlightNum, HotelAddress

CarConfirmationNum

123

Pre-conditions

Output parameters

Post-conditions

SOCA (2016) 10:111–133

117

Fig. 3 Sequential composition example

Fig. 5 Non-sequential composition example

Fig. 4 Sequential composition

of the graph. Each vertex in the graph represents a service
in the composition. Each outgoing edge of a node (service)
represents the outputs and post-conditions produced by the
service. Each incoming edge of a node represents the inputs
and pre-conditions of the service. The following conditions
should hold on the nodes of the graph: ∀i Si V, Si R, Si =
(CIi , Ii , Ai , AOi , Oi , COi )
1. I   I1 , O1  I2 , . . . , On  O 
2. CI ⇒ CI1 , CO1 ⇒ CI2 , . . . , COn ⇒ CO
The meaning of the  is the subsumption (subsumes) relation, and ⇒ is the implication relation. In other words, we
are deriving a possible sequence of services where only the
provided input parameters are used for the services and at
least the required output parameters are provided as an output by the chain of services. The goal is to derive a solution
with minimal number of services. Also, the post-conditions
of a service in the chain should imply the pre-conditions of
the next service in the chain. Figure 4 depicts an instance of
sequential composition.
Example (Non-sequential composition) Suppose we are
looking for a service to buy a book and the directory of
services contains services GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. Table 3 shows the
input/output parameters of the query and the four services
in the repository. Suppose a single matching service is not

found in the repository, a solution is synthesized from among
the set of services available in the repository. Figure 5 shows
this composite service. The post-conditions of the service
GetAvailability should logically imply the pre-conditions of
service PurchaseBook.
Definition (Non-sequential composition) More generally,
the Composition problem can be defined as automatically
finding a directed acyclic graph G = (V, E) of services from
repository R, given query Q = (CI , I  , A , AO , O  , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph represents a service in the
composition. Each outgoing edge of a node (service) represents the outputs and post-conditions produced by the service. Each incoming edge of a node represents the inputs
and pre-conditions of the service. The following conditions
should hold on the nodes of the graph:
1. ∀i Si  V where Si has exactly one incoming edge that
represents the query inputs and pre-conditions, I  
∪i Ii , CI ⇒ ∧i CIi .
2. ∀i Si  V where Si has exactly one outgoing edge that
represents the query outputs and post-conditions, O  
∪i Oi , CO ⇐ ∧i COi .
3. ∀i Si  V where Si has at least one incoming edge,
let Si1 , Si2 , . . . , Sim be the nodes such that there is a
directed edge from each of these nodes to Si . Then, Ii 
∪k Oik ∪ I  , CIi ⇐ (COi1 ∧ COi2 . . . ∧ COim ∧ CI ).
Figure 6 depicts an instance of non sequential composition.

Table 3 Non-sequential composition example
Service

Input parameters

Query

BookTitle, CreditCardNum,
AuthorName, CardType

Pre-conditions

Output parameters

Post-conditions

ConfNumber

GetlSBN

BookName, AuthorName

ConfNumber

GetAvailability

BooklSBN

NumAvailable

NumAvailable > 0

Authorize CreditCard

CreditCardNum

AuthCode

AuthCode > 99 ∧
AuthCode < 1000

PurchaseBook

BooklSBN,
NumAvailable, AuthCode

NumAvailable > 0

ConfNumber

123

118

SOCA (2016) 10:111–133

Fig. 6 Non-sequential composition

Example (Non-sequential conditional composition) Nonsequential conditional composition consists of if-then-else
conditions, i.e., the composition flow varies depending on
the result of the post-conditions of a service. Suppose we are
looking for a service to make international travel arrangements. We first need to make a tentative flight and hotel
reservation and then apply for a visa. If the visa is approved,
we can buy the flight ticket and confirm the hotel reservation, else we will have to cancel both the reservations. Also,
if the visa is approved, we need to make a car reservation.
The repository contains services ReserveFlight, ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table 4 shows the
input/output parameters of the user query and services. In this
example, service ProcessVisa produces the post-condition
VisaApproved ∨ VisaDenied. The services ConfirmFlight and
ConfirmHotel have the pre-condition VisaApproved. In this
case, one cannot determine whether the post-conditions of

service ProcessVisa implies the pre-conditions of services
ConfirmFlight and ConfirmHotel until the services are actually executed. In such a case, a condition can be generated
which will be evaluated at runtime and depending on the
outcome of the condition, the corresponding services will be
executed. The vertex for service ProcessVisa in the graph
is followed by a condition node which represents the postcondition of service ProcessVisa. This node has two outgoing
edges one representing the case if the condition is satisfied at
run-time and other edge for the case where the condition is not
satisfied. In other words, these edges represent the generated
conditions which in this case are, (VisaApproved ∨ VisaDenied) ⇒ VisaApproved and VisaApproved ∨ VisaDenied) ⇒
VisaDenied. Depending on which condition holds, the corresponding services ConfirmFlight or CancelFlight are executed. Figure 7 shows this conditional composition example
as a directed acyclic graph.

Definition (Generalized Composition) The generalized
Composition problem can be defined as automatically finding
a directed acyclic graph G = (V, E) of services from repository R, given query Q = (CI , I  , A , AO , O  , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph either represents a service
involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can
be determined only after the execution of the service. Each
outgoing edge of a node (service) represents the outputs and
post-conditions produced by the service. Each incoming edge
of a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:

Table 4 Non sequential conditional composition example
Service

Pre-conditions

Input parameters

Output parameter

Query

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

FlightConfirmationNum,
HotelConfirmationNum,
CarConfirmationNum

ReserveFlight

PassengerName, OriginAirport, StartDate,
DestinationAirport, ReturnDate

FlightConfirmationNum,
ArrivalFlightNum

ReserveHotel

PassengerName, ArrivalFlightNum,
StartDate, ReturnDate

HotelConfirmationNum,
HotelAddress

ProcessVisa

PassengerName, VisaType,
FlightConfirmationNum,
HotelConfirmationNum

ConfirmationNum

ConfirmFlight

VisaApproved

FlightConfirmationNum, CreditCardNum

FlightConfirmationNum

ConfirmHotel

VisaApproved

HotelConfirmationNum, CreditCardNum

HotelConfirmationNum

CancelFlight

VisaDenied

FlightConfirmationNum, PassengerName

CancelCode

Cancel Hotel

VisaDenied

HotelConfirmationNum, PassengerName

CancelCode

PassengerName, ArrivalDate,
HotelAddress, ArrivalFlightNum

CarConfirmationNum

ReserveCar

123

Post-conditions

VisaApproved
V VisaDenied

SOCA (2016) 10:111–133

119

Fig. 7 Non-sequential conditional composition

1. ∀i Si  V where Si has exactly one incoming edge that
represents the query inputs and pre-conditions, I  
∪i Ii, CI ⇒ ∧i CIi .
2. ∀i Si  V where Si has exactly one outgoing edge that
represents the query outputs and post-conditions, O  
∪i Oi, CO ⇐ ∧i COi .
3. ∀i Si  V where Si represents a service and has at least
one incoming edge, let Si1 , Si2 , ..., Sim be the nodes such
that there is a directed edge from each of these nodes to
Si . Then, Ii  ∪k Oik ∪ I  , C Ii ⇐ (COi1 ∧ COi2 . . . ∧
COim ∧ CI ).
4. ∀i Si  V where Si represents a condition that is evaluated
at run-time and has exactly one incoming edge, let S j
be its immediate predecessor node such that there is a
directed edge from S j to Si . Then, the inputs and preconditions at node Si are Ii = O j ∪ I  ; CIi = CO j .
The outgoing edges from Si represent the outputs that
are same as the inputs Ii and the post-conditions that are
the result of the condition evaluation at run-time.

The meaning of the  is the subsumption (subsumes) relation
and ⇒ is the implication relation. In other words, a service
at any stage in the composition can potentially have as its
inputs all the outputs from its predecessors as well as the
query inputs. The services in the first stage of composition
can only use the query inputs. The union of the outputs produced by the services in the last stage of composition should
contain all the outputs that the query requires to be produced.
Also the post-conditions of services at any stage in composition should imply the pre-conditions of services in the next
stage. When it cannot be determined at compile time whether
the post-conditions imply the pre-conditions or not, a conditional node is created in the graph. The outgoing edges of
the conditional node represent the possible conditions that
will be evaluated at run-time. Depending on the condition
that holds, the corresponding services are executed. That is,
if a subservice S1 is composed with subservice S2 , then the

post-conditions CO1 of S1 must imply the preconditions CI2
of S2 . The following conditions are evaluated at run-time:
if (CO1 ⇒ CI2 ) then execute S1;
else if (CO1 ⇒ ¬ CI2 then no-op};
else if (CI2 then execute S1 };
When the number of nodes in the graph is equal to one,
the composition problem reduces to the discovery problem.
When all nodes in the graph have not more than one incoming edge and not more than one outgoing edge, the problem
reduces to a sequential composition problem. Further details
and examples are available in our prior work [15].
3.3 Requirements of an ideal engine
The features of an ideal discovery/composition engine are as
follows:
Correctness One of the most important requirements for
an ideal engine is to produce correct results, i.e., the services
discovered and composed by it should satisfy all the requirements of the query. Also, the engine should be able to find
all services that satisfy the query requirements.
Minimal query execution time Querying a repository of
services for a requested service should take a reasonable
amount of (minimal) time, i.e., a few milliseconds. Here, we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental updates Adding or updating a service to an
existing repository of services should take minimal time.
An ideal discovery and composition engine should not preprocess the entire repository again; rather incrementally
update pre-processed data (indexes, etc.) with data for the
new service.

123

120

Cost function If there are costs associated with every service in the repository, then an ideal discovery and composition engine should be able to provide results based on requirements (minimize, maximize, etc.) over the costs. We can
extend this to services having an associated attribute vector, and the engine should be able to provide results based on
maximizing or minimizing functions over the attribute vector.
These requirements have driven the design of our semanticsbased discovery and composition engine described in this
paper.
3.4 Centrality measure in social networks
Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds.
It involves measuring the formal and informal relationships to
understand information/knowledge flow that binds the interacting units that could be a person, group, organization, or any
knowledge entity. Social Network Analysis has an increasing application in social sciences that has been applied to
diverse areas such as psychology, health, electronic communications, and business organization. In order to understand
social networks and their participants, the location of an actor
in a network is evaluated. The network location is measured
in terms of centrality of a node that gives an insight into the
various roles and groupings in a network. Centrality gives
a rough indication of the social power of a node based on
how well they “connect” the network. There has been extensive discussion in the Social Network community regarding the meaning of the term centrality when it is applied to
Social Networks. One view stems directly from graph theory [9]. The graph-theoretic conception of compactness has
been extended to the study of Social Networks and simply
renamed “graph centrality”. Their measures are all based
upon distances between points, and all define graphs as centralized to the degree that their points are all close together.
The alternative view emerged from substantive research on
communication in Social Networks. From this perspective,
the centrality of an entire network should index the tendency
of a single point to be more central than all other points in
the network. Measures of a graph centrality of this type are
based on differences between the centrality of the most central point and that of all others. Thus, they are indexes of the
centralization of the network [36]. The three most popular
individual centrality measures are Degree, Betweenness, and
Closeness Centrality.
• Degree centrality The network activity of a node can be
measured using the concept of degrees, i.e., the number
of direct connections a node has. In the example, network
shown in Fig. 10 and Table 5, Provider D has the most
direct connections in the network, making it the most

123

SOCA (2016) 10:111–133
Table 5 Degree centrality of nodes in Fig. 10
Service provider

Degree

Provider A

2

Provider B

3

Provider C

1

Provider D

8

Provider E

3

Provider F

4

Provider G

5

Provider H

3

Provider I

1

Provider J

2

Provider K

4

Provider L

1

Provider M

active node in the network. In personal Social Networks,
the common thought is that “the more connections, the
better”.
• Betweenness centrality Though Provider D has many
direct ties, Provider H has fewer direct connections (close
to the average in the network). Yet, in many ways,
Provider H has one of the best locations in the network
by playing the role of a “broker” between two important
components. A node with high betweenness has greater
influence over what flows and does not in the network.
• Closeness centrality Provider F and G have fewer connections than Provider D, yet the pattern of their direct
and indirect ties allow them to access all the nodes in the
network more quickly than anyone else. They have the
shortest paths to all others, i.e., they are close to everyone
else. They are in an excellent position and have the best
visibility into what is happening in the network.

Individual network centralities provide insight into the individual’s location in the network. The relationship between
the centralities of all nodes can reveal much about the overall network structure.
3.5 Trust rating of a service and trust threshold
The trust rating of each service in the repository is computed
as a measure of the degree centrality (CD) of the social network to which the service provider belongs. It is calculated
as the degree or count of the number of adjacencies for a
node, sk :
C D (sk ) =

n
i−0

a (si , sk )

SOCA (2016) 10:111–133

where
a(si sk ) = 1 iff si and sk are connected by a line
0 otherwise
As such it is a straightforward index of the extent to which
sk is a focus of activity [9]. C D (sk ) is large if service provider
sk is adjacent to, or in direct contact with, a large number of
other service providers, and small if sk tends to be cut off
from such direct contact. C D (sk ) = 0 for a service provider
that is totally isolated from any other point. Our algorithm
filters out any services whose provider has a zero degree
centrality in a social network, i.e., such services will not
be used in building composition solutions. Trust rating of
the entire composite service is computed as an average of
the individual trust ratings of the services involved in the
composition. We also need to set a Trust Threshold and any
service with a Trust rating that is below this threshold is not
used while generating composition solutions. In our initial
prototype implementation, we set the Trust threshold to zero,
i.e., degree centrality of the service provider in the network
is zero. A service provider or service provider organization
that is not connected to any other nodes in the Social network
is not known to anyone else and is an immediate reason to be
pruned out from composition solutions, as the service cannot
be trusted. Composition solutions can be ranked such that
solutions with highest trust rating appear on top of the list.
4 Dynamic Web service composition: methodology
In this section, we describe our methodology for automatic
Web service composition that produces a general directed
acyclic graph. The composition solution produced is for the
generalized composition problem presented in Sect. 3. We
also present our algorithm for automatic generation of OWLS descriptions for the new composite service produced.
4.1 Algorithms for Web service discovery and composition
Our approach is based on a multi-step narrowing of the list of
candidate services using various constraints at each step. As
mentioned earlier, discovery is a simple case of Composition.
When the number of services involved in the composition
is exactly equal to one, the problem reduces to a discovery
problem. Hence, we use the same engine for both discovery and composition. We assume that a directory of services
has already been compiled, and that this directory includes
semantic descriptions for each service. In our implementation, we use semantic descriptions written in USDL [37],
although the algorithms are general enough that they will
work with any semantic annotation language. The repository of services contains one USDL description document
for each service. However, we still need a query language to

121

search this directory, i.e., we need a language to frame the
requirements of the service that an application developer is
seeking. USDL itself can be used as such as a query language. A USDL description of the desired service can be
written (with tool assistance), a query processor can then
search the service directory for a “matching” service. For
service composition, the first step is finding the set of composable services. USDL itself is used to specify the requirements of the composed service that an application developer
is seeking. Using the discovery engine, individual services
that make up the composed service can be selected. Part substitution techniques [38] can be used to find the different parts
of a whole task and the selected services can be composed
into one by applying the correct sequence of their execution. The correct sequence of execution can be determined
by the pre-conditions and post-conditions of the individual
services. That is, if a subservice S1 is composed with subservice S2 , then the post-conditions of S1 must imply the
pre-conditions of S2 . The goal is to derive a single solution,
which is a directed acyclic graph of services that can be composed together to produce the requested service in the query.
Figure 8 shows a pictorial representation of our composition
engine.
4.2 Multi-step narrowing solution
To produce a composite service, as shown in the example
Fig. 2, our algorithm filters services that are not useful for
the composition at multiple stages. Figure 9 shows the filtering technique for the particular instance graph represented in
Fig. 2. The composition routine begins with the query input
parameters and finds all those services from the repository
that require a subset of the query input parameters. In Fig. 9,
CI, I are the pre-conditions and the input parameters provided
by the query. S1 and S2 are the services found after step 1. O1
is the union of all outputs produced by the services at the first
stage. For the next stage, the inputs available are the query
input parameters and all the outputs produced by the previous
stage, i.e., I2 = O1 ∪ I. I2 is used to find services at the next
stage, i.e., all those services that require a subset of I2 . To
make sure we do not end up in cycles, we get only those services that require at least one parameter from the outputs produced in the previous stage. This filtering continues until all
the query output parameters are produced. At this point, we
make another pass in the reverse direction to remove redundant services that do not directly or indirectly contribute to
the query output parameters. This is done starting with the
output parameters and working our way backwards. Next,
another level of filtering is performed using the trust ratings
of services. Table 6 shows the algorithm and we have prototype implementation of this algorithm implemented using
Prolog [39] with Constraint Logic Programming over finite
domain (CLP(FD)) [40]. The rationale behind the choice of

123

122

SOCA (2016) 10:111–133

Fig. 8 Composition
engine—design

Fig. 9 Multi-step narrowing solution
Table 6 Algorithm for multi-step narrowing
Multi-step Narrowing Algorithm
Algorithm: Composition
(Input QI - QueryInputs, QO - QueryOutputs, QCI - Pre-Cond, QCO - Post-Cond, T - TrustThreshold)
(Output: Result - ListOfServices)
1. L ⇐ NarrowServiceList(QI, QCI);
2. O ⇐ GetAllOutputParameters(L);
3. CO ⇐ GetAllPostConditions(L);
4. While Not (O  QO)
5. I = QI ∪ O; CI ⇐ QCI ∧ CO;
6. L ⇐ NarrowServiceList(I, CI);
7. End While;
8. IntResult ⇐ RemoveRedundantServices(QO, QCO);
9. Result ⇐ RemoveRedundantServices(T, IntResult);
10. Return Result;

CLP(FD), implementation details, and experimental results
are available [15].
4.3 Automatic generation of OWL-S descriptions
After obtaining a composition solution (sequential, nonsequential, or conditional), the next step is to produce a
semantic description document for this new composite service. This document can be used for execution of the service

123

and to register the service in the repository, thereby allowing subsequent queries to result in a direct match instead of
performing the composition process all over again. We used
the existing language OWL-S [31] to describe composite services. OWL-S models services as processes and when used to
describe composite services, it maintains the state throughout
the process. It provides control constructs such as Sequence,
Split-Join, If-Then-Else and many more to describe composite services. These control constructs can be used to describe

SOCA (2016) 10:111–133
Table 7 Generation of composite service description
Generation of Composite Service Description
Algorithm: GenerateCompositeServiceDescription
(Input: G - CompositionSolutionGraph)
(Output: D - CompositeServiceDescription)
1. Generate generic header constructs
2. Start Composite Service element
3. Start SequenceConstruct
4. If Number(SourceVertices) = 1
GenerateAtomicService
Else StartSplitJoinConstruct
For Each starting/source Vertex V
GenerateAtomicService
End For
EndSplitJoinConstruct

123

rently. The process completes execution only when all the
services in this construct have completed their execution.
The non-sequential conditional composition can be described
in OWL-S using the If-Then-Else construct which specifies
the condition and the services that should be executed if the
condition holds and also specifies what happens when the
condition does not hold. Conditions in OWL-S are described
using SWRL. There are other constructs such as looping constructs in OWL-S that can be used to describe composite services with complex looping process flows. We are currently
investigating other kinds of compositions with iterations and
repeat-until loops and their OWL-S document generation.
We are exploring the possibility of unfolding a loop into a
linear chain of services that are repeatedly executed. We are
also analyzing our choice of the composition language and
looking at other possibilities as part of our future work.

End If
5. If Number(SinkVertices) = 1

5 Implementation and experimental results

GenerateAtomicService
Else StartSplitJoinConstruct
For Each ending/sink Vertex V
GenerateAtomicService

This section presents implementation details of the composition engine. We also analyze the performance and present
experimental results.

End For
EndSplitJoinConstruct
End If
6. For Each remaining vertex V in G
If V is AND vertex with one outgoing edge
GenerateAtomicService
If V is AND vertex with > 1 outgoing edge
GenerateSplitJoinConstruct
If V is OR vertex with one outgoing edge
GenerateAtomicService
qquad If V is OR vertex with > 1 outgoing edge
GenerateConditionalConstruct
End For
7. End SequenceConstruct
8. End Composite Service element
9. Generate generic footer constructs

the kind of composition. OWL-S also provides a property
called composedBy using which the services involved in the
composition can be specified. Table 7 shows the algorithm for
generation of the OWL-S document when the composition
solution in the form of a graph is provided as the input.
A sequential composition can be described using the
Sequence construct that indicates that all the services inside
this construct have to be invoked one after the other in the
same order. The non-sequential composition can be described
in OWL-S using the Split-Join construct which indicates that
all the services inside this construct can be invoked concur-

5.1 Implementation
Our discovery and composition engine is implemented using
Prolog [39] with Constraint Logic Programming over finite
domain [40], referred to as CLP(FD) hereafter. In our current
implementation, we used semantic descriptions written in
the language called Universal Semantics-Service Description
Language (USDL) [37]. The repository of services contains
one USDL description document for each service. USDL
itself is used to specify the requirements of the service that an
application developer is seeking. USDL is a language that service developers can use to specify formal semantics of Web
services. In order to provide semantic descriptions of services, we need an ontology that is somewhat coarse-grained
yet universal, and at a similar conceptual level to common
real-world concepts. USDL uses WordNet [41] which is a
sufficiently comprehensive ontology that meets these criteria. Thus, the “meaning” of input parameters, outputs, and
the side effect induced by the service is given by mapping
these syntactic terms to concepts in WordNet [38] for details
of the representation. Inclusion of USDL descriptions thus
makes services directly “semantically” searchable. However,
we still need a query language to search this directory, i.e., we
need a language to frame the requirements on the service that
an application developer is seeking. USDL itself can be used
as such a query language. A USDL description of the desired
service can be written, a query processor can then search the
service directory for a “matching” service. These algorithms
can be used with any other Semantic Web service descrip-

123

124

SOCA (2016) 10:111–133

A USDL description of the desired service can be written,
which is read by the query reader and converted to a triple.
This module can be easily extended to read descriptions written in other languages.
(iii) Semantic relations generator

Fig. 10 A social network of Web service providers

tion language as well. It will involve extending our implementation to work for other description formats, and we are
looking into that as part of our future work. The parsing of
all the USDL description documents and the universal ontology is written in Java. The parsing is done via SAXReader
library of Java and after the parsing, prolog engine is instantiated to run the Composition query processor. The complete
discovery and composition engine is implemented as a Web
service in Java using Apache Tomcat. The Web service in
turn invokes Prolog to do all the processing [42–44]. The
high-level design of the Discovery and Composition engines
is shown in Fig. 10. The software system is made up of the
following components:
(i) Triple generator
The triple generator module converts each service description
into a triple. In this case, USDL descriptions are converted
to triples like:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions)
The function symbol affect-type is the side effect of the
service and affected object} is the object that changed due
to the side effect. I is the list of inputs, and O is the list
of outputs. Pre-Conditions are the conditions on the input
parameters, and Post-Conditions are the conditions on the
output parameters. Services are converted to triples so that
they can be treated as terms in first-order logic and specialized
unification algorithms can be applied to obtain exact, generic,
specific, part and whole substitutions [38]. In case conditions
on a service are not provided, the Pre-Conditions and PostConditions in the triple will be null. Similarly, if the affecttype is not available, this module assigns a generic affect to
the service.

We obtain the semantic relations from the OWL WordNet
ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms,
hypernyms, meronyms, holonyms and many more. USDL
descriptions point to OWL WordNet for the meanings of concepts. A theory of service substitution is described in detail
in [38] which uses the semantic relations between basic concepts of WordNet to derive the semantic relations between
services. This module extracts all the semantic relations and
creates a list of Prolog facts. We can also use any other
domain-specific ontology to obtain semantic relations of concepts. We are currently looking into making the parser in this
module more generic to handle any other ontology written in
OWL.
(iv) Discovery query processor
This module compares the discovery query with all the services in the repository. The processor works as follows:
1. On the output parameters of a service, the processor
first looks for an exact substitutable. If it does not find
one, then it looks for a parameter with hyponym relation
[38], i.e., a specific substitutable.
2. On the input parameters of a service, the processor first
looks for an exact substitutable. If it does not find one,
then it looks for a parameter with hypernym relation
[38], i.e., a generic substitutable.
The discovery engine, written using Prolog with CLP(FD)
library, uses a repository of facts, which contains a list of
all services, their input and output parameters and semantic
relations between parameters. The code snippet of our engine
is shown in Table 8. The query is parsed and converted into
a Prolog query that looks as follows:
discovery(sol(queryService, ListOfSolutionServices).
The engine will try to find a list of SolutionServices that
match the queryService.
(v) Composition engine

(ii) Query reader
This module reads the query file and passes it on to the
Triple Generator. We use USDL itself as the query language.

123

The composition engine is written using Prolog with CLP(FD)
library. It uses a repository of facts, which contains all the
services, their input and output parameters and the semantic

SOCA (2016) 10:111–133

125

Discovery Algorithm

step narrowing-based approach to solve these problems and
implemented it using constraint logic programming.

discovery(sol(Qname,A)) :-

(i) Correctness

Table 8 Discovery Algorithm—Code Snippet

dQuery(Qname,I,O), encodeParam(O,OL),
/* Narrow candidate services(S) using output list(OL) */
narrowO(OL,S), fdset(S,FDs), fdsettolist(FDs,SL),
/* Expand InputList(I) using semantic relations */
getExtInpList(I, ExtInpList), encodeParam(ExtInpList,IL),
/* Narrow candidate services(SL) using input list (IL) */
narrowI(IL,SL,SA), decodeS(SA,A).

Our system takes into account all the services that can be
satisfied by the provided input parameters and pre-conditions
at every step of our narrowing algorithm. So our search space
has all the possible solutions. Our backward narrowing step,
which removes the redundant services, does so taking into
account the output parameters and post-conditions. So our
algorithm will always find a correct solution (if one exists)
in the minimum possible number of steps.

Table 9 Composition Algorithm—Code Snippet
Composition Algorithm
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs),
encodeParam(QueryOutputs, QO),
getExtInpList(QueryInputs, InpList),
encodeParam(InpList, QI),
performForwardTask(QI, QO, LF),
performBackwardTask(LF, QO, LR),
getMinSolution(LR, QI, QO, A), reverse(A, RevA),
confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

relations between the parameters. A code snippet of our composition engine is shown in Table 9. The query is converted
into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate “bagof” to return all possible ListOfServices that can be composed to get the requested
queryService.
(vi) Output generator
After the Composition engine finds a matching service, or
the list of atomic services for a composed service, the results
are sent to the output generator in the form of triples. This
module generates the output files in any desired XML format.
5.2 Efficiency and scalability issues
In this section, we discuss the salient features of our system
with respect to the efficiency and scalability issues related
to Web service discovery and composition problem. It is
because of these features that we decided on the multi-

(ii) Pre-processing
Our system initially pre-processes the repository and converts all service descriptions into Prolog terms. The semantic relations are also processed and loaded as Prolog terms
in memory. Once the pre-processing is done, then discovery
or composition queries are run against all these Prolog terms
and, hence, we obtain results quickly and efficiently. The
built-in indexing scheme and constraints in CLP (FD) facilitate the fast execution of queries. During the pre-processing
phase, we use the term representations of services to set up
constraints on services and the individual input and output
parameters. This further helped us in getting optimal results.
(iii) Execution efficiency
The use of CLP (FD) helped significantly in rapidly obtaining
answers to the discovery and composition queries. We tabulated processing times for different size repositories, and
the results are shown in the next section. As one can see,
after pre-processing the repository, our system is quite efficient in processing the query. The query execution time is
insignificant.
(iv) Programming efficiency
The use of Constraint Logic Programming helped us in coming up with a simple and elegant code. We used a number of
built-in features such as indexing, set operations, and constraints and, hence, did not have to spend time coding these
ourselves. This made our approach efficient in terms of programming time as well. Not only the whole system is about
200 lines of code, but we also managed to develop it in less
than 2 weeks.
(v) Scalability
Our system allows for incremental updates on the repository,
i.e., once the pre-processing of a repository is done, adding

123

126

a new service or updating an existing one will not need reexecution of the entire pre-processing phase. Instead, we can
easily update the existing list of CLP (FD) terms loaded in
the memory and run discovery and composition queries. Our
estimate is that this update time will be negligible, perhaps
a few milliseconds. With real-world services, it is likely that
new services will get added often or updates might be made
on existing services. In such a case, avoiding repeated preprocessing of the entire repository will definitely be needed
and incremental updates will be of great practical use. The
efficiency of the incremental update operation makes our system highly scalable.

SOCA (2016) 10:111–133
Table 10 Sample service interface description
Sample WSDL description
<message name =“InputName”>
<part name = “part0” type = “Name”/>
</message >
<message name =“OutputAddress”>
<part name = “part0” type = “US-Address” />
</message>
<portType name = “AdressConverter”>
<operation name =“Convert” >
<input message =“InputName” />
<output message = “OutputAddress”/>

(vi) Use of external database
In case the repository grows extremely large in size, then saving off results from the pre-processing phase into some external database might be useful. This is part of our future work.
With extremely large repositories, holding all the results of
pre-processing in the main memory may not be feasible. In
such a case, we can query a database where all the information is stored. Applying incremental updates to the database is easily possible, thus avoiding recomputation of preprocessed data.
(vii) Searching for optimal solution
If there are any properties with respect to which the solutions
can be ranked, then setting up global constraints to get the
optimal solution is relatively easy with the constraint-based
approach. For example, if each service has an associated cost,
then the discovery and the composition problem can be redefined to find the solutions with the minimal cost. Our system
can be easily extended to take these global constraints into
account.
5.3 Performance and experimental results
To conduct our experiments, we looked at various benchmarks and Web services challenge (WSC) datasets [42,43]
best suited our needs and fit well into the overall architecture with minimal changes. They provided semantics through
XML schema, provided queries and corresponding solutions.
We used repositories from WSC website [42,43], slightly
modified to fit into USDL framework. They provide repositories of various sizes (thousands of services). These repositories contain WSDL descriptions of services. The input
and output messages of the services may contain multiple
parameters. Each parameter is annotated with a semantic
concept stored in the attribute type. Table 10 shows a service AddressConverter with one operation named Convert.
It can be invoked with an input message (InputName) and
produces a response message (OutputAddress). The value of

123

</operation >
</ portType >

the attribute message represents a reference to a message element. Each message has a set of part elements as children,
which represent the service parameters, annotated with concepts referenced by the type-attribute. The Convert operation
in this example requires a parameter of the type Name and
returns an instance of US-Address.
The queries and solutions are provided in an XML format. The semantic relations between various parameters are
provided in an XML Schema format. Concepts were treated
as data types and taxonomies encoded as hierarchies of such
data types in XSD schemas. The subsumes relation between
two semantic concepts can be compared to the subclass relationship in Object-oriented programming. Table 11 shows
a sample XSD schema defining the data types Address and
US-Address inheriting from Address. In the context of the
WSC, this schema would be interpreted as a taxonomy introducing the concepts Address and US-Address with subsumes(Address, US-Address).
We evaluated our approach on different size repositories
and tabulated Pre-processing, Query Execution, and Incremental update time. We noticed that there was a significant
difference in pre-processing time between the first and subsequent runs (after deleting all the previous pre-processed data)
on the same repository. What we found is that the repository
was cached after the first run and that explained the difference in the pre-processing time for subsequent runs. Table 12
shows performance results of our Composition algorithm on
discovery queries, and Table 13 shows the results of our algorithm on composition queries. The times shown in tables are
the wall clock times. The actual CPU time to pre-process
the repository and execute the query should be less than or
equal to the wall clock time. The experiments were conducted on different repository sizes as well as varying number of input and output parameters for each service in the
repository. The results are plotted in Figs. 11 and 12 where

SOCA (2016) 10:111–133

127

Table 11 Sample XSD Schema providing semantics
Sample XSD Schema
<complexType name =“ Address ” >
<sequence >
<element name = “name” type = “string” minOccurs = “0”/>
<element name = “street” type = “string” />
<element name = “city” type = “string” />
</ sequence>
</complexType>
<complexType name = “US - Address”>
<complexContent >
<extension base = “Address” >
<sequence>
<element name = “state” type = “US - State”/>
<element name = “zip” type = “positiveInteger”/>
</sequence>
</extension>
</complexContent>

Fig. 11 High-level design on composition engine

</complexType>

Table 12 Performance on discovery queries
Pre-processing
time (ms)

Incremental
Query
execution update (ms)
time (ms)

Repository
size

Number
of I/O
parameters

2,000

4–8

36.5

1

18

2,000

16–20

45.8

1

23

2,000

32–36

57.8

2

28

2,500

4–8

47.7

1

19

2,500

16–20

58.7

1

23

2,500

32–36

71.6

2

29

3,000

4–8

56.8

1

19

3,000

16–20

77.1

1

26

3,000

32–36

88.2

3

29

Table 13 Performance on composition queries
Pre-processing
I/O time (ms)

Incremental
Query
execution update (ms)
time (ms)

Repository
size

Number
of I/O
parameters

2,000

4–8

36.1

1

18

2,000

16–20

47.1

1

23

2,000

32–36

60.2

1

30

3,000

4–8

58.4

1

19

3,000

16–20

60.1

1

20

3,000

32–36

102.1

1

34

4,000

4–8

71.2

1

18

4,000

16–20

87.9

1

22

4,000

32–36

129.2

1

32

the numbers of I/O parameters were 4–8, 16–20, and 32–36,
respectively. The graphs exhibit behavior consistent with our
expectations: for a fixed repository size, the pre-processing
time increases with the increase in number of input/output
parameters. Similarly, for fixed input/output sizes, the preprocessing time is directly proportional to the size of the
repository. However, what is surprising is the efficiency of
service query processing, which is negligible (just 1–3 ms)
even for complex queries with large repositories.
Discussion
We evaluated our approach for correctness, efficiency, and
scalability of the software. The correctness of the algorithm
has been described in Sect. 4. In the experiments conducted,
discovery and composition solutions obtained were checked
against a pre-defined set of solutions produced for the WSChallenge datasets [43]. This was the first and most important
criterion for evaluation of the engine. Our second criterion
was query efficiency, i.e., rapidly obtaining answers to discovery and composition queries. The results show that irrespective of the repository size the query execution time was
always 1–2 ms. This was because the repositories were preprocessed, and the queries were run against pre-processed
data. Different repository sizes were used along with varying the number of input and output parameters of the Web
services. The software also scaled well with varying the size
of the repositories. These three criterion are important for
the successful adoption of the composition engine to produce a Software-as-a-Service (SaaS) platform for automatic
workflow generation in a specific domain as described in the
sample scenario in Sect. 1. An important part of scalability
is the ability to sustained performance even if pre-processed

123

128

SOCA (2016) 10:111–133

Fig. 12 Performance on discovery queries

repositories change in size, new services are added, existing
services are removed, etc. To evaluate this aspect, we studied the time taken for incremental updates to the repository.
The results obtained were all less than one second for different repository sizes and well as varying number of input
and output parameters. A network of service providers was
introduced synthetically into the experimental datasets. Trust
rating of services was computed based on this network. Trust
ratings generator helps with filtering services based on the
trust threshold and helps in ranking. The performance of the
engine on discovery and composition queries still remains
the same.

6 Application to bioinformatics
We illustrate the practicality of our general framework for
automatically composing services by applying it to phylogenetics, a subfield of bioinformatics, for automatic generation
of workflows. In this section, we present a brief description
of the field of Phylogenetics [8] followed by an example
of a workflow generation problem that can be mapped to a
non-sequential conditional composition problem (the most
general case of the composition problem) and can be solved
using our generalized composition engine.
6.1 Phylogenetics
Phylogenetic inferencing involves an attempt to estimate the
evolutionary history of a collection of organisms (taxa) or
a family of genes [45]. The two major components of this
task are the estimation of the evolutionary tree (branching
order), then using the estimated trees (phylogenies) as analytical framework for further evolutionary study and finally
performing the traditional role of systematics and classification. Using this study, a number of interesting facts can be
discovered, for example, who are the closest living relatives
of humans, who are whales related to, etc. Different studies
can be conducted, for example, studying dynamics of microbial communities, predicting evolution of influenza viruses
and other applications such as Drug Discovery, and vaccine

123

development, etc. In order to perform these tasks, scientists
use a number of software tools and programs already available. They use them by putting them together in a particular
order (i.e., a workflow) to get their desired results. That is, it
involves execution of a sequence of steps using various programs or software tools. These tools use different data formats, and hence translating from one data format to another
becomes necessary.
6.2 Automatic workflow generation
The software tools and programs created for specific phylogenetic tasks use different data formats for their input and output parameters. The data description language Nexus [46,47]
is used as a universal language for representation of these
bioinformatics related data. There are translator programs
that convert different formats into Nexus and vice versa. For
example, one could use the BLAST program to get a sequence
set of genes. Once the sequence set is obtained, the sequences
can be aligned using the CLUSTAL program. But the output
from BLAST cannot be directly fed to CLUSTAL, as their
data formats are different. The translator can be used to convert the BLAST format to Nexus and then the Nexus format
to CLUSTAL. In order to perform an inferencing task, one
has to manually pick all the appropriate programs and corresponding format translators and put them in the correct order
to produce a workflow. We show how Web service composition can be directly applied to automate this task of producing a workflow. MyGrid [48] has a wealth of bioinformatics
resources and data that provides opportunities for research.
It has hundreds of biological Web services and their WSDL
descriptions, provided by various research groups around the
world. We illustrate our generalized framework for Web service composition by applying it to these services to generate
workflows automatically that are practically useful for this
field.
Example Workflow Generation (Non-sequential Composition) Suppose we are looking for a service that takes a
GeneInput and produces its corresponding AccessionNumbers, AGI, and GeneIdentifier as output. The directory of
services contains CreateMobyData, MOBYSHoundGetGen-

SOCA (2016) 10:111–133

129

Table 14 Bioinformatics application—non-sequential composition example
Service

Input parameters

Output parameters

Query

Genelnput

AccessionN umbers,
AGIj Geneldentity

CreateMoby Data

Genelnput

MobyData

MobyData

GeneSequence

MOBYSHoundGet
GenBankWbatev
erSequence

Pre-conditions

Format (MobyData) =
NCBI

MlPSBlastBetterE13

GeneSequence

WUGene Sequence

Extract Accession

GeneSequence

AccessionNumbers

ExtractBestHit

WUGeneSequence

AGI, Geneldentity

Post-conditions

Format (MobyData) = NCBI

Fig. 13 Performance on composition queries

Bank WhateverSequence, ExtractAccession, ExtractBestHit,
MIPSBlastBetterE13 services. In this scenario, the GeneInput first needs to be converted to NCBI data format and then
its corresponding GeneSequence is further passed to ExtractAccession and ExtractBestHit to obtain the AccessionNumbers, AGI, and GeneIdentity respectively. Table 14 shows the
input/output parameters of the user query and the services.
Figure 13 shows this non-sequential composition example as
a directed acyclic graph. In this example:
• Service CreateMobyData has a post-condition on its output parameter MobyData that the format is NCBI and service MOBYSHoundGetGenBankWhateverSequence has
a pre-condition that its input parameter MobyData has
to be in NCBI format for service execution. The postcondition of CreateMobyData must imply pre-condition
of MOBYSHoundGetGenBankWhateverSequence service.
• Both services ExtractAccession and ExtractBestHit have
to be executed to obtain the query outputs.
• The semantic descriptions of the service input/output
parameters should be the same as the query parameters
or have the subsumption relation. This can be inferred
using semantics from the ontology provided.
Example Workflow Generation (Non-sequential Conditional
Composition) Suppose we are looking for a service that takes

a GeneSequence and produces an EvolutionTree and EvolutionDistance after performing a phylogenetic analysis. Also
the service should satisfy the post-condition that the EvolutionTree produced is in the Newick format. This involves producing a sequence set first, followed by aligning the sequence
set and then producing the evolution tree and evolution distance. Also any necessary intermediate data format translations have to be performed. Table 15 lists the services in
the repository and their corresponding input/output parameters and user query. For the sake of simplicity, the query and
services have fewer input/output parameters than the realworld services. In this example, service BLAST has to be
executed first so that its output BLASTSequenceSet can be
used as input by CLUSTAL after the data format has been
translated using BLASTNexus and NexusCLUSTAL. The service BLASTNexus has a post-condition that the format of
the output parameter NexusSequenceSet is Nexus which is
the pre-condition of the next service NexusCLUSTAL. Similarly the service NexusCLUSTAL has a post-condition that the
format of the output parameter ClustalSequenceSet is Clustal
that is the pre-condition of the next service CLUSTAL. At
every step of composition, the post-conditions of a service
should imply the pre-conditions of the following service. The
post-condition of the service CLUSTAL is that the output
parameter AlignedSequenceSet has either Paup or Phylip format. Depending on which one of these two conditions hold,

123

130

SOCA (2016) 10:111–133

Table 15 Bioinformatics application—non-sequential conditional composition example
Service

Input parameter

Outpur parameter

Post-conditions

Query

Sequence,
OrganismType, Word
Size, DatabaseName

EvolutionTree,
EvolutionDistance

Format (EvolutionTree)
= Newick

BLAST

Sequence, Organism
Type, DatabaseName

BlastSequenceSet

Clustal SequenceSet

AlignedSequenceSet

Format (AlignedSequence
Set) = Paup ∨
Forrmat(AlignedSequence
Set) = Phylip

BlastSequenceSet

NexusSequenceSet

Format (NexusSequenceSet)
= Nexus

CLUSTAL

Pre-conditions

Format (Clustal
SequenceSet) = Clustal

BLASTNexus
NexusCLUSTAL

Format (Nexus
SequenceSet) = Nexus

NexusSeqtienceSet

ClustalSequenceSet

Forma(ClustalSequenceSet)
= Clustal

PAUP

Format (Aligned
SequenceSet) = Paup

AlignedSequenceSet,
Word Size

EvolutionTree

Format (EvolutionTree)
= Newick

PHYLIP

Format (Aligned
SequenceSet) = Phylip

AlignetBeciuenceSet

EvokiSonTree

Format (EvolutionTree)
= Newick

MEGA

Format (Aligned
Sequenced) = Paup

AlignedSequenceSet

EvaluationDistance

Fig. 14 Bioinformatics
application—non sequential
composition as a directed
acyclic graph

the next service for composition is chosen. In this case, one
cannot determine whether the post-conditions of the service
CLUSTAL imply pre-conditions of PAUP or PHYLIP until
the services are actually executed. In such a case, a condition can be generated which will be evaluated at runtime and
depending on the outcome of the condition, corresponding
services will be executed.
The vertex for service CLUSTAL in Fig. 14 has an outgoing
edge to a conditional node. The outgoing edge represents the
outputs and post-conditions of the service. The conditional
node has multiple outgoing edges that represent the generated
conditions that are evaluated at run-time. In this case, the
following conditions are generated:
• (Format(AlignedSequenceSet) = Paup ∨
Format(AlignedSequenceSet) = Phylip) ⇒
(Format(AlignedSequenceSet) = Paup)
• (Format(AlignedSequenceSet) = Paup ∨
Format(AlignedSequenceSet) = Phylip) ⇒
(Format(AlignedSequenceSet) = Phylip)

123

Depending on the condition that holds, the corresponding
services PAUP and MEGA or PHYLIP are executed respectively. The outputs EvolutionTree and EvolutionDistance are
produced in both the cases along with the post-condition
that the format of the evolution tree is Newick. Figure 14
shows this non-sequential conditional composition example
as a conditional directed acyclic graph.
6.3 Implementation
In order to apply service composition to obtain the sequence
of tasks automatically, these programs have to be made available as Web services and their descriptions should be provided using one of the Web services description languages
like WSDL (Web Services Description Language) or USDL
(Universal Service-Semantics Description Language). The
translator programs also have to be available as Web services. Then, we can write a query specifying the input parameters provided and the output parameters that have to be
obtained. The Composition engine then looks up the repository of available services and finds the solution, i.e., a set

SOCA (2016) 10:111–133

131

Fig. 15 Bioinformatics
application—non sequential
conditional composition as a
directed acyclic graph

Fig. 16 Composition Solution
1 for query in Table 15

of services that can be executed to obtain the requested output parameters. These set of services obtained will need only
those input parameters that are provided by the query and
their execution produces the output parameters specified by
the query. We tested our Composition engine on a repository
of services that had descriptions of Web services corresponding to the following programs:

1. BLAST: This service compares protein sequences to
sequence databases and calculates the statistical significance of matches. It query’s a public database of generic
information like GenBank and GSDB and produces a
molecular sequence. It takes in Sequence, DatabaseName, OrganismType, SelectionOptions, MaxTargetSequences, ExpectedThreshold, WordSize as input parameters and produces BlastSequenceSet as the
output.
2. CLUSTAL: This service produces multiple sequence
alignment for DNA or proteins. It takes in ClustalSequenceSet as input parameter and produces ClustalAlignedSequenceSet as the output.
3. PHYLIP: This service is used for inferring phylogenies. It analyzes molecular sequences and infers phylogenetic information. It takes in PhylipAlignedSequenceSet, UseThresholdParsimony, UseTransversionParsimony as input parameters and produces NewickEvolutionTree as the output.
4. PAUP: This service is used for inferring phylogentic
trees. It analyzes molecular sequences and infers phylogenetic information. It takes in PaupAlignedSequenceSet as input parameter and produces NewickEvolutionTree as the output.
5. BLASTNexus: This service takes input in BLAST format and converts it into Nexus format. It takes in BLAST-

Table 16 Bioinformatics application—workflow query
Service

Input parameters

Output parameters

Query1

Sequence, DatabaseName,
OrganismType,
SelectionOptions,
MaxTargetSequences,
ExpectedThreshold,
UserTransversion Parsimony,
UseThresholdParsimony,
WordSize

NewickEvoIutionTree

SequenceSet as input and produces NexusSequence
Set.
6. NexusCLUSTAL: This service takes input in Nexus
format and converts it into CLUSTAL format. It takes
in NexusSequenceSet as input parameter and produces
CLUSTALSequenceSet as the output.
7. CLUSTALNexus: This service takes input in CLUSTAL
format and converts it into Nexus format. It takes in
CLUSTALAlignedSequenceSet as input parameter and
produces NexusAlignedSequenceSet as the output.
8. NexusPAUP: This service takes input in Nexus format and converts it into PAUP format. It takes in
NexusAlignedSequenceSet as input parameter and produces PaupAlignedSequenceSet as the output.
9. NexusPHYLIP: This service takes input in Nexus format and converts it into PHYLIP format. It takes in
NexusAlignedSequenceSet as input parameter and produces PhylipAlignedSequenceSet} as the output.
10. MEGA: This service is used for Molecular Evolutionary Genetics Analysis. It takes in MEGAInp as
input parameter and produces MEGASequence as the
output.

123

132

SOCA (2016) 10:111–133

Fig. 17 Composition Solution
2 for query in Table 15

11. KEPLER: This service provides scientific workflows. It
takes in KEPLERData as input parameter and produces
KEPLERSequence as the output.
The composition engine can automatically discover a complex workflow based on the query requirements, from a large
repository without having to analyze all the programs manually. Figure 15 shows the non-sequential conditional composition for query specified in Table 16 as a directed cyclic
graph. Figures 16 and 17 show the solutions obtained for the
query specified in Table 16.
A task that had to be performed manually by biologists
whenever they had to make phylogenetic inferences can now
be done automatically with Web services Composition. The
programs have to be made available as Web services and
their descriptions provided. Once we have a repository of
such services, the composition engine can be used as shown
above to automatically generate workflows.
7 Conclusions and future work
Due to the growing number of services on the Web, we need
automatic and dynamic Web service composition in order to
utilize and reuse existing services effectively. It is also important that the composition solutions obtained can be trusted.
Our semantics-based approach uses semantic description of
Web services to find substitutable and composite services that
best match the desired service. Given semantic description
of Web services, our engine produces optimal results (based
on number of services in the composition). The composition
flow is determined automatically without the need for any
manual intervention. Our engine finds any sequential, nonsequential or non-sequential conditional composition that is
possible for a given query and also automatically generates
OWL-S description of the composite service. This OWLS description can be used during the execution phase and
subsequent searches for this composite service will yield a
direct match. A trust rating is computed for every service in
the repository based on the degree centrality of the service
provider in a known social network. Currently, we are in
the process of testing the trust-based dynamic Web service
composition engine in a complete operational setting and
running experiments to measure the quality of composition
results obtained. We will also explore the other measures of
centrality such as betweenness centrality and closeness centrality and analyze the possibility of using a combination of

123

all three measures of centrality to compute trust rating of
a service provider. We are able to apply many optimization
techniques to our system so that it works efficiently even on
large repositories. The strengths of this engine is the minimal
query execution time that is achieved through pre-processing
of repositories and incremental updates to the pre-processed
data whenever a service is added, removed, or modified. Use
of Constraint Logic Programming helped greatly in obtaining
an efficient implementation of this system and made it easy to
incorporate non-functional parameters for ranking of results.
The limitations of the engine include trust aspect of the Web
services involved in a composition solution. A model that
provides a trust rating to services or service providers would
improve confidence in the generated solutions. Also, when
working with domains such as Bioinformatics where software systems involved in a workflow need to be converted
into services, generation of semantics of the inputs and outputs of the Web services is a challenge. They have to be
manually assigned semantics by a domain expert.
Our future work includes investigating other kinds of compositions with loops such as repeat-until and iterations and
their OWL-S description generation. Analyzing the choice
of the composition language (e.g., BioPerl [49] for phylogenetic workflows) and exploring other language possibilities
is also part of our future work. We are also exploring combining technologies of automated service composition and
domain-specific languages to develop a framework for problem solving and software engineering.

References
1. Castagna G, Gesbert N, Padovani L (2008) A theory of contracts
for web services. ACM SIGPLAN Not 43:261–272
2. Bansal A, Patel K, Gupta G, Raghavachari B, Harris ED, Staves
JC (2005) Towards intelligent services: a case study in chemical
emergency response. In: IEEE International conference on web
services (ICWS)
3. McIlraith SA, Son TC, Zeng H (2001) Semantic web services.
IEEE Intell Syst 16(2):46–53
4. Mandell DJ, McIlraith SA (2003) Adapting BPEL4WS for the
semantic web: the bottom-up approach to web service interoperation. In: The semantic web-ISWC. Springer 2003, pp 227–241
5. Paolucci M, Kawamura T, Payne TR, Sycara K (2002) Semantic matching of web services capabilities. In: The semantic Web–
ISWC. Springer 2002, pp 333–347
6. Rao J, Dimitrov D, Hofmann P, Sadeh N (2006) A mixed initiative approach to semantic web service discovery and composition:
SAP’s guided procedures framework. In: International conference
on web services. ICWS’06, 2006, pp 401–410

SOCA (2016) 10:111–133
7. Cardoso J, Sheth AP (2006) Semantic web services, processes and
applications. Springer, Berlin
8. Edwards AWF, Cavalli-Sforza LL (1964) Reconstruction of evolutionary trees, systematics association publication number 6, No.
Phenetic and Phylogenetic Classification, pp 67–76
9. Freeman LC (1979) Centrality in social networks conceptual clarification. Social Netw 1(3):215–239
10. Wasserman SF (1994) Social network analysis: methods and applications. Cambridge University Press, Cambridge
11. Tamura K, Peterson D, Peterson N, Stecher G, Nei M, Kumar S
(2011) MEGA5: molecular evolutionary genetics analysis using
maximum likelihood, evolutionary distance, and maximum parsimony methods. Mol Biol Evol 28(10):2731–2739
12. Thompson JD, Higgins DG, Gibson TJ (1994) CLUSTAL W:
improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties
and weight matrix choice. Nucleic Acids Res 22(22):4673–4680
13. Edgar RC (2004) MUSCLE: multiple sequence alignment with
high accuracy and high throughput. Nucleic Acids Res 32(5):1792–
1797
14. Badr Y, Caplat G (2010) Software-as-a-service and versionology:
towards innovative service differentiation. In: 24th IEEE international conference on advanced information networking and applications (AINA), 2010, pp 237–243
15. Kona S, Bansal A, Blake MB, Gupta G (2008) Generalized
semantics-based service composition. In: IEEE international conference on web services (ICWS), pp 219–227
16. Rao J, Su X (2005) A survey of automated web service composition
methods. In: Cardoso J, Sheth A (eds) Semantic web services and
web process composition. Springer, Berlin, pp 43–54
17. Srivastava B, Koehler J (2003) Web service composition-current
solutions and open problems. In: ICAPS 2003 workshop on planning for web services, vol 35, pp 28–35
18. McIlraith S, Son TC (2002) Adapting golog for composition of
semantic web services. KR 2:482–493
19. Pistore M, Roberti P, Traverso P (2005) Process-level composition
of executable web services: on-the-fly versus once-for-all composition. In: Gomez-Perez A, Euzenat J (eds) The semantic web:
research and applications. Springer, Berlin, pp 62–77
20. Boustil A, Maamri R, Sahnoun Z (2013) A semantic selection
approach for composite web services using OWL-DL and rules.
Serv Oriented Comput Appl 8:1–18
21. Claro DB, Albers P, Hao JK (2005) Selecting web services for
optimal composition. In ICWS international workshop on semantic
and dynamic web processes, Orlando-USA
22. Suvee D, De Fraine B, Cibrán MA, Verheecke B, Joncheere N,
Vanderperren W (2005) Evaluating FuseJ as a web service composition language. In: Third IEEE European conference on web
services (ECOWS)
23. Dong W, Jiao L (2008) QoS-aware Web service composition based
on SLA. In: Fourth international conference on natural computation
(ICNC) vol 5, pp 247–251
24. Yan J, Kowalczyk R, Lin J, Chhetri MB, Goh SK, Zhang J (2007)
Autonomous service level agreement negotiation for service composition provision. Future Gener Comput Syst 23(6):748–759
25. Wada H, Champrasert P, Suzuki J, Oba K (2008) Multiobjective
optimization of SLA-aware service composition. In: IEEE congress
on services-Part I, pp 368–375
26. Blake MB (2007) Decomposing composition: service-oriented
software engineers. IEEE Softw 24(6):68–77
27. Zeng L, Benatallah B, Ngu AH, Dumas M, Kalagnanam J, Chang
H (2004) QoS-aware middleware for web services composition.
IEEE Trans Softw Eng 30(5):311–327

133
28. Cardoso J, Sheth A, Miller J, Arnold J, Kochut K (2004) Quality
of service for workflows and web service processes. Web Semant
1(3):281–308
29. Zhao X, Shen LW, Peng X, Zhao W (2013) Finding preferred
skyline solutions for SLA-constrained service composition. In:
2013 IEEE 20th international conference on web services (ICWS),
pp 195–202
30. Feng Y, Ngan LD, Kanagasabai R (2013) Dynamic service composition with service-dependent QoS attributes. In: 2013 IEEE 20th
international conference on web services (ICWS), pp 10–17
31. “OWL-S”. [Online]. http://www.w3.org/Submission/OWL-S/.
(Accessed: 22-Jan-2014)
32. Wen S, Tang C, Li Q, Chiu DK, Liu A, Han X (2014) Probabilistic
top-K dominating services composition with uncertain QoS. Serv
Oriented Comput Appl 8(1):91–103
33. Immonen A, Pakkala D (2014) A survey of methods and approaches
for reliable dynamic service compositions. Serv Oriented Comput
Appl 8(2):129–158
34. Mehdi M, Bouguila N, Bentahar J (2013) A QoS-based trust
approach for service selection and composition via Bayesian networks. In: 2013 IEEE 20th international conference on web services (ICWS), pp 211–218
35. Kuter U, Golbeck J (2009) Semantic web service composition in
social environments. Springer, Berlin
36. Leavitt HJ (1951) Some effects of certain communication patterns
on group performance. J Abnorm Soc Psychol 46(1):38
37. Bansal A, Kona S, Simon L, Mallya A, Gupta G, Hite TD (2005) A
universal service-semantics description language. In: Third IEEE
European conference on web services (ECOWS), pp 214–225
38. Kona S, Bansal A, Simon L, Mallya A, Gupta G (2009) USDL: a
service-semantics description language for automatic service discovery and composition. Int J Web Serv Res 6(1):20–48
39. Sterling L, Shapiro EY, Warren DH (1986) The art of Prolog:
advanced programming techniques. MIT Press, Cambridge
40. Marriott K, Stuckey PJ (1998) Programming with constraints: an
introduction. MIT Press, Cambridge
41. “RDF/OWL Representation of WordNet”. [Online]. http://www.
w3.org/TR/wordnet-rdf/. (Accessed: 23-Jan-2014)
42. Blake MB, Cheung W, Jaeger MC, Wombacher A (2006) WSC-06:
the web service challenge. In: The 3rd IEEE international conference on E-commerce technology, 2006. The 8th IEEE international
conference on enterprise computing, E-commerce, and E-services,
pp 62–62
43. Blake MB, Cheung WKW, Jaeger MC, Wombacher A (2007)
WSC-07: Evolving the web services challenge. In: The 9th
IEEE international conference on E-commerce technology and
the 4th IEEE international conference on enterprise computing,
E-commerce, and E-services, 2007. CEC/EEE 2007, pp 505–508
44. Kona S, Bansal A, Gupta G, Hite D (2007) Automatic composition of semantic web services. In: International conference on web
services (ICWS), vol 7, pp 150–158
45. Felsenstein J (2004) Inferring phylogenies, vol 2. Sinauer Associates, Sunderland
46. Iglesias JR, Gupta G, Pontelli E, Ranjan D, Milligan B (2001) Interoperability between bioinformatics tools: a logic programming
approach. In: Practical aspects of declarative languages. Springer,
Berlin, pp 153–168
47. Maddison DR, Swofford DL, Maddison WP (1997) NEXUS:
an extensible file format for systematic information. Syst Biol
46(4):590–621
48. “myGrid”. [Online]. http://www.mygrid.org.uk/
49. “BioPerl”. [Online]. http://www.bioperl.org/wiki/Main_Page

123

2008 IEEE International Conference on Services Computing

Towards a General Framework for Web Service Composition
Srividya Kona, Ajay Bansal, M. Brian Blake
Department of Computer Science,
Georgetown University
Washington, DC 20057

Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

Abstract

S2
S5
CI’,I’

Service-oriented computing (SOC) has emerged as
the eminent market environment for sharing and reusing
service-centric capabilities. The underpinning for an organization’s use of SOC techniques is the ability to discover
and compose Web services. In this paper we present a generalized semantics-based technique for automatic service
composition that combines the rigor of process-oriented
composition with the descriptiveness of semantics. Our
generalized approach extends the common practice of linearly linked services by introducing the use of a conditional
directed acyclic graph (DAG) where complex interactions,
containing control flow, information flow and pre/post conditions, are effectively represented.

S1

S4

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph

representation. We present our approach that generates
most general compositions based on (conditional) directed
acyclic graphs (DAG). In our framework, the DAG representation of the composite service is reified as an OWL-S
description. This description document can be registered in
a repository and is thus available for future searches. The
composite service can now be discovered as a direct match
instead of having to look through the entire repository and
build the composition solution again.

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered,
and consumed. A composite service is a collection of services combined together in some way to achieve a desired
effect. Traditionally, the task of automatic service composition has been split into four phases: (i) Planning, (ii) Discovery, (iii) Selection, and (iv) Execution [2]. Most efforts
reported in the literature focus on one or more of these four
phases. The first phase involves generating a plan, i.e., all
the services and the order in which they are to be composed
in order to obtain the composition. The plan may be generated manually, semi-automatically, or automatically. The
second phase involves discovering services as per the plan.
Depending on the approach, often planning and discovery
are combined into one step. After all the appropriate services are discovered, the selection phase involves selecting
the optimal solution from the available potential solutions
based on non-functional properties like QoS properties. The
last phase involves executing the services as per the plan and
in case any of them are not available, an alternate solution
has to be used.
In this paper we formalize the generalized composition
problem based on our conditional directed acyclic graph

978-0-7695-3283-7/08 $25.00 © 2008 IEEE
DOI 10.1109/SCC.2008.134

CO’,O’

S3

2. Web service Composition
In this section we formalize the generalized composition
problem. In this generalization, we extend our previous
notion of composition [1] to handle non-sequential conditional composition (which we believe is the most general
case of composition). Informally, the Web service Composition problem can be defined as follows: given a repository of service descriptions, and a query with the requirements of the requested service, in case a matching service
is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be
composed to obtain the desired service. Figure 1 shows an
example composite service made up of five services S1 to
S5 . In the figure, I  and CI  are the query input parameters
and pre-conditions respectively. O and CO are the query
output parameters and post-conditions respectively. Informally, the directed arc between nodes Si and Sj indicates
that outputs of Si constitute (some of) the inputs of Sj .
Definition (Repository of Services): Repository (R) is a
set of Web services.

497

Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI, I, A, AO, O, CO) is the representation of a service where CI is the list of pre-conditions,
I is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the list of
post-conditions. The pre- and post-conditions are ground
logical predicates.

tion can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage
in composition should imply the pre-conditions of services
in the next stage. When it cannot be determined at compile
time whether the post-conditions imply the pre-conditions
or not, a conditional node is created in the graph. The outgoing edges of the conditional node represent the possible
conditions which will be evaluated at run-time. Depending
on the condition that holds, the corresponding services are
executed. That is, if a subservice S1 is composed with subservice S2 , then the postconditions CO1 of S1 must imply
the preconditions CI 2 of S2 . The following conditions are
evaluated at run-time:
if (CO1 ⇒ CI 2 ) then execute S1 ;
else if (CO1 ⇒ ¬ CI 2 ) then no-op;
else if (CI 2 ) then execute S1 ;

Definition (Query): The query service is defined as Q
= (CI  , I  , A , AO , O , CO ) where CI  is the list of preconditions, I  is the input list, A is the service affect, AO
is the affected object, O is the output list, and CO is the
list of post-conditions. These are all the parameters of the
requested service.
Definition (Generalized Composition): The generalized
Composition problem can be defined as automatically finding a directed acyclic graph G = (V, E) of services from
repository R, given query Q = (CI  , I  , A , AO , O , CO ),
where V is the set of vertices and E is the set of edges of the
graph. Each vertex in the graph either represents a service
involved in the composition or post-condition of the immediate predecessor service in the graph, whose outcome can
be determined only after the execution of the service. Each
outgoing edge of a node (service) represents the outputs and
post-conditions produced by the service. Each incoming
edge of a node represents the inputs and pre-conditions of
the service. The following conditions should hold on the
nodes of the graph:
1. ∀i Si ∈ V where Si has exactly one incoming edge
that 
represents the query inputs and pre-conditions,
I   i I i , CI  ⇒∧i CI i .
2. ∀i Si ∈ V where Si has exactly one outgoing edge
that represents
the query outputs and post-conditions,

O  i Oi , CO ⇐∧i COi .
3. ∀i Si ∈ V where Si represents a service and has at
least one incoming edge, let Si1 , Si2 , ..., Sim be the
nodes such that there is a directed
 edge from each
of these nodes to Si . Then Ii  k Oik ∪ I  , CI i ⇐
(COi1 ∧COi2 ... ∧ COim ∧ CI  ).
4. ∀i Si ∈ V where Si represents a condition that is
evaluated at run-time and has exactly one incoming
edge, let Sj be its immediate predecessor node such
that there is a directed edge from Sj to Si . Then the
inputs and pre-conditions at node Si are Ii = Oj ∪ I  ,
CI i = COj . The outgoing edges from Si represent
the outputs that are same as the inputs Ii and the postconditions that are the result of the condition evaluation at run-time.
The meaning of the  is the subsumption (subsumes) relation and ⇒ is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composi-

3. Automatic Generation of Composite Services
In order to produce the composite service which is the
graph, we filter out services that are not useful for the composition at multiple stages. The composition routine starts
with the query input parameters. It finds all those services
from the repository which require a subset of the query input parameters. For the next stage, the inputs available are
the query input parameters and all the outputs produced by
the previous stage, i.e., I2 = O1 ∪ I. I2 is used to find services at the next stage, i.e., all those services that require
a subset of I2 . In order to make sure we do not end up in
cycles, we get only those services which require at least one
parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.

4. Conclusions and Future Work
To make Web services more practical we need a general framework for composition of Web services. The generalized approach presented in this paper can handle nonsequential conditional composition that can be used in automatic workflow generation in a number of applications.

References
[1] S. Kona, A. Bansal, and G. Gupta. Automatic Composition of Semantic Web Services. In ICWS, 2007.
[2] J. Cardoso, A. Sheth. Semantic Web Services, Processes and Applications. Springer, 2006.

498

Interactive Exploration of Large Filesystems
Joshua Fostera
a Computer

Science

Kalpathi Subramaniana
b Software

Gail-Joon Ahnb

and Information Systems

The University of North Carolina at Charlotte, Charlotte, NC 28223, USA
ABSTRACT
Secure management of ﬁle systems of large organizations can present signiﬁcant challenges to system administrators, in terms of the number of users, shared access to parts of the ﬁle system for supporting large software
projects, and securing and monitoring critical parts of the ﬁle system from intruders. We present interactive
visualization tools for monitoring and viewing the complex access control relationships within large ﬁle systems.
This tool is targeted as an aid to system administrators to manage users, software applications and shared access.
We tested our tool on UNC Charlotte’s Andrew File System (AFS), which contains 7043 users, 560 user groups,
and about 2.1 million directories. Our system displays summary information about the ﬁle system, and two types
of visualizations to explore access control relationships among classes of users. In addition, drill-down features
are provided to explore the user ﬁle system structure and manage access control information of any directory
within the system. All of the views are linked to permit easy navigation and features are provided that make
the system scalable to larger ﬁlesystems.
Keywords: AFS, ﬁlesystem, monitoring, drill-down, visualization

1. INTRODUCTION
Current trends in electronic information storage, communication and exchange of business documents, facilitated
in large part by the Internet, inexpensive disk storage, etc., has resulted in massive ﬁle systems in large academic
and industrial organizations. Administration and secure management of such ﬁle systems is a challenging task.
System administrators have to typically manage large numbers of users and software applications, while at the
same time ensuring the security of critical parts of the ﬁle system, privacy of sensitive information, and ensure
appropriate access to users. While some of these issues are also the domain of ﬁle systems, the sheer size and the
increasing risk of intruders demands more robust and scalable solutions. Information visualization techniques
can be employed to address many of these challenges to assist the system administrator.
The majority of the work on ﬁle system visualization has focused on utilizing their size attribute. Treemaps1
portray a ﬁle system using a compact space-ﬁlling(pixelized) representation, by recursively subdividing a 2D
rectangular region into partitions as a function of the subtree ﬁle/directory size. To overcome some of the
diﬃculties (such as poor aspect ratio) of this representation, several reﬁnements have been proposed. Ordered
treemap layouts2 provide more stability for dynamically changing data as well as better aspect ratios, cushion
treemaps3 use shading to bring out the hierarchical structure, and beam trees4 use nested cylindrical beams to
display hierarchical information.
Work on utilizing visualization to monitor network intrusion detection5 has also been explored by researchers.
Erbacher6, 7 analyzed log ﬁles of campus network activity of their university’s principal server and a dozen other
workstations. The visualization consists of circular glyphs, with the main server at the center and nodes connected
to the server placed radially depending on their IP addresses, as well as whether they are a local or a remote
client. Links between clients and servers may be colored, directed, or dashed, representing various attributes of
the connection. Similarly, Teoh et al.8 built visualization tools to examine Internet routing data, which can also
become very extensive. The goal here was to detect and comprehend anomalous events. They used a pixelized
visualization based on a quadtree subdivision of IP preﬁxes. Links representing routing paths of interest were
visualized over time, in order to determine their stability.
Correspondence: Kalpathi Subramanian, krs@uncc.edu
Author contact information: Email: {krs,jafoster,gahn}@uncc.edu

Visualization and Data Analysis 2005, edited by Robert F. Erbacher,
Jonathan C. Roberts, Matti T. Gröhn, Katy Börner, Proc. of SPIE-IS&T
Electronic Imaging, SPIE Vol. 5669 © 2005 SPIE and IS&T · 0277-786X/05/$15

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

307

While the work presented here is not directly related to network security, a long term goal of this research is
to monitor and detect in real-time ﬁle system intrusions; in particular, the existence of trustworthy log ﬁles is
of importance in computer forensic analysis.9 Log ﬁles contain valuable information that may be destroyed by
sophisticated attackers to remove any record of their activities after an intrusion. Such critical parts of large ﬁle
systems may be specially ﬂagged for proactive monitoring and detection.
In this article, we present visualization tools to explore user and access control relationships within large
ﬁlesystems. The current system uses well understood visualization techniques to display relationships: hierarchical displays, pixelized visualizations, zooming and linked views. It displays relationships across diﬀerent classes of
users, displays ﬁle/user access vulnerabilities from a security standpoint, displays a selected user’s ﬁle structure,
and access privileges of any particular directory within. Access privileges of a chosen directory may be modiﬁed, either interactively or via ﬁle system speciﬁc commands. A graphical interface provides the basis for easy
navigation, with drill down features to points of interest. The diﬀerent views in our system are linked, so as to
maintain context. We demonstrate our visualization system using an Andrew File System (AFS)10 dataset that
was acquired from our campus network. A number of metrics have been designed to deﬁne “interesting” users
and incorporated within the visualizations. These metrics support exploration of the ﬁle system and facilitate
speciﬁc visual queries, and is expected to be of assistance to system administrators.

2. METHODS
2.1. Data Acquisition
We demonstrate our ﬁle system visualization system using an Andrew File System (AFS)10, 11 dataset. AFS is a
highly scalable ﬁlesystem that has found wide acceptance in large academic and industrial organizations. AFS
imposes access control mechanisms (which is of interest here) in all of its directories, in a manner that promotes
ﬂexibility. Similar to Unix, it supports user groups, however, users themselves may create groups (in contrast to
just system administrators) and assign privileges. Each AFS directory contains seven access control privileges:
read, lookup, insert, delete, write, lock and administer. Our initial goal was to extract all these relationships
from a large ﬁlesystem and support interactive visual queries.
The ﬁle system structure was extracted (using scripts) from our campus network and saved to disk, after
masking user names and other personal information. Filesystem data for the system came from two sources: a
listing of all groups and their members, and a recursive listing of the access controls for every directory in the
system. This data can be retrieved using simple AFS commands. The data totals about 450 MB in text form,
representing, 7043 users, 560 user groups and about 2.1 million directory names and their access privileges. We
used a converter to read in the ﬁles, build in memory the data structures needed by our system, and save these
structures in binary form. The resulting binary data ﬁle was about 80MB.
Users were classiﬁed into faculty, students, administrators and dormant (inactive users). In addition, a
set of critical directories were identiﬁed for specialized processing. These directories typically contain software
applications that are of general use by larger numbers of users. In our system, we group these together as a
distinct class of critical users.

2.2. Data Structures
There are three types of entities in the visualization system; therefore three main data structures: User, Group,
and Directory. A Group simply contains a list of references to the Users who belong to it. Users consist of a
reference to their home directory, and a Statistics structure which is a catch-all for various statistics computed
about the user and his/her directories. Each directory is a node in a tree structure, containing a list of references
to its children. Access data is also carried at the directory level. Each directory contains two lists of AccessRights
structures: one for user access rights and one for group access rights. The AccessRights structure simply contains
a reference to the corresponding user or group and an access byte, each bit of which represents one type of access:
read, look, insert, delete, write, lock, and administer. At the top level, there is a master list of groups and a
master list of users, with each user having a reference to the top node of a tree of Directories representing the
path to the home directory. The organization of the ﬁlesystem data in memory parallels the drill-down method
of data exploration in the visualizations; that is, selection of a user or group ﬁrst, then selection of one of the
user’s directories, and ﬁnally selection of an individual group or user with access to that directory.
308

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

2.3. Visualization Design
The ﬁle visualization system was designed with the following considerations:
• Support both high-level overviews as well as detailed structure of the ﬁlesystem.
• Smooth interactive navigation across diﬀerent levels of detail, providing the means to drill-down into
features of interest.
• Highly scalable, in terms of the number of users, and ﬁles/directories.
• Support metrics for constructing queries to identify features of interest, as well as encourage interactive
exploration of the dataset.
• Robust and secure management of users and ﬁle system access.

Figure 1. Relationships between classes of users

Fig. 1 shows our overall design. The upper left panel shows aggregate information of the ﬁlesystem, the lower
left panel displays user-centric relationships (two diﬀerent visualization types are currently supported), the lower
right panel illustrates user ﬁle system structure, and the upper right panel shows access control privileges and
associated information for a particular user or directory. We describe each of these views next.

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

309

Aggregate View
The high-level visualizations show a summarized view of the ﬁle system. There are four classes of users:
students, faculty members, administrators, and dormant accounts. A pie chart illustrates the distribution. The
total number of users, groups and directories are also displayed. The histogram view reﬂects the distribution of
the users, in terms of three important metrics: size of the user’s home directory (deﬁned as the total number of
directories under the user’s home directory), number of groups the user belongs to, and percentage of directories
the user is sharing (we deﬁne a shared directory as one for which the owner has given access privileges to another
user or group). For instance, it can be seen in Fig. 1 that a majority of users own small sized home directories
(which is generally to be expected in very large academic ﬁlesystems). This could be useful in thresholding such
users, in order to identify “interesting” users; for instance, by thresholding users owning a very small number of
directories.
User Relationships View
This view permits exploring relationships between classes of users as well as displaying user’s relationships to
underlying attributes of their ﬁle system. Two visualization types are currently supported, (1) scrollable lists of
users with explicit links to display relationships, and (2) a pixel style visualization that can compactly represent
all users, suitably colored to reﬂect a linear combination of various attributes of interest.
The link display (lower left panel, Fig. 1 reveals the relationships between students, faculty, and groups. This
visualization consists of two vertical scrollable lists, either of groups and students or faculty and groups. When
an item in the scrollable list is selected, its associated entities in the second list are linked to and highlighted. For
example, one can switch to the groups/students display and select a user, and all groups that the user belongs to
will be highlighted. One of these groups may then be selected and the visualization switched to faculty/groups
mode to see all the faculty members that belong to that group. Linked items are highlighted, clustered together,
and automatically scrolled to the center of the window, and a linking polygon is drawn to connect them. The
lists may be manually scrolled using the mouse wheel, Page Up/Down keys, or by moving the mouse towards
the top or bottom of the list. In Fig. 1, user2464 is seen to belong to 11 diﬀerent groups. The panel on the right
automatically also displays his home directory structure (as described later).
A pixel-style visualization is also provided to display additional relationships relating to the underlying user
ﬁle system. Users are represented within a square grid of cells; currently the users are ordered by their id, which
is determined by the input dataset. Any of the four user classes (student, faculty, dormant, admin) may be
shown individually. The cell for each user is colored according to a weighted combination of four parameters,
• Size of the users’ home directory,
• Number of groups the user belongs to,
• Percentage of directories the user shares with other users/groups.
• Type of access in shared directories.
The ﬁrst 3 parameters have been mentioned earlier; the fourth parameter distinguishes users based on the access
privileges provided by a user to other users or groups. For example it may be important to classify the users on
the basis of how much read/look, write, or administrative access they have given other users or groups to their
directories. Each of these four parameters has an importance, or a weight, which may be adjusted with sliders.
The ﬁnal color for each user is a normalized sum of the values for each of these parameters multiplied by the
parameter’s weight. Colors are mapped from blue to red, and cells with a ﬁnal value of zero (no importance with
the selected parameters and weights) are colored grey.
Fig. 2 illustrates pixel visualization of the students class. In this example, the visualization displays the size
of the student directories. The cross-hairs show the current selection (the red square), which is user2631 with
4476 directories. While this case could have been better illustrated with a treemap visualization, other attributes
can be mapped (as we will see later) to the user glyphs.
310

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Figure 2. File system relationships using pixel style visualization

While all of the users (in this example dataset) are easily accommodated in the visualization, larger datasets
can cause diﬃculties in interactive user selection. Thus, we provide a magnifying glass or local zoom capability
as part of the interface. The local zoom is centered around the cursor position and its level and size of the
magniﬁed area are both adjustable. As can be seen in Fig. 2, all the users within the white box surrounding
the selected(red) user are magniﬁed for ease of picking. Currently, areas outside of the zoom are of the original
size; in the future, we could consider adjusting their size based on their distance from the focus area, similar to
normal focus+context methods.12
User File Structure View
When a user is selected in either the link display or the pixel visualization, the user’s home directory tree
is shown in the directory display (lower right, Figs. 1, 2). The directory structure is shown with a Windows
Explorer style vertical layout. Each directory is represented with a colored square, either red, green, or yellow.
Non-shared directories are green, and shared directories are yellow or red depending on the type of sharing. A
yellow directory represents one in which the owner has given access to either another user or another group, but
not both. Directories in which the owner has given access to other users and other groups are red. A horizontal bar
at the top of this panel displays the distribution of the shared directories for a quick summary. This visualization
supports trackball-style panning and zooming and is scrollable with the mouse wheel. One diﬃculty with this
node-link layout is its ineﬃcient use of screen space and thus even moderately sized user directories will easily
exceed the boundaries of the view. An alternate is to use a treemap style display, for instance, by using a count
of subdirectories as a metric for the partitioning (if directory/ﬁle sizes are not available).

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

311

Access Control View
At the ﬁnest level of detail, information about a selected directory may be shown. Selecting any directory within
a user’s ﬁle structure results in the the directory’s access privileges being displayed (upper right panel, Figs. 1, 2).
The full pathname to the directory is displayed at the top, along with a list of users and groups with access to
the directory. Any of these users and groups may be selected to reveal the individual access privileges given to
that user. In Fig. 2, user2464 is sharing the displayed directory with a number of users and groups; user968’s
access privileges are displayed. The individual access privileges of a directory may be modiﬁed at this step, as
this is a tool that will be geared toward ﬁlesystem administrators in the future.

3. RESULTS
3.1. Implementation
We are developing this system in C++ under Linux∗ . All of the drawing is done in OpenGL, using the FLTK
toolkit13 for the user interface.

3.2. Examples
We describe three examples to illustrate the features of this tool as well as its possible application for ﬁlesystem
monitoring. As this system is currently a prototype, no extensive testing or user study has been performed yet.

Figure 3. Example illustrating student users sharing write privileges
∗

312

As we use public domain tools, this application can be easily ported to Windows based PCs or other flavors of Unix.

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Fig. 3 illustrates student users in the pixel visualization. Here the color mapping illustrates the Shared Access
attribute speciﬁc to Write Access; in other words, we are interested in looking at users who have given write
access to other groups or users. As all other attributes have been turned oﬀ, the colormap is determined by
the percentage of user directories that are writable by other users. The selected user in Fig. 3, user848 has 193
directories under his home directory. The yellow horizontal bar in the lower right panel clearly indicates the
100% sharing for write privileges. It is possible that a user might have mistakenly provided such privileges; this
might alert an administrator (with a tool such as this) to warn the user. More important, this visualization
indicates that there are a number of users (red squares indicate almost the entire directory tree is writable by
at least one other user or group) who have provided write access privileges to other users/groups.

Figure 4. Example illustrating faculty users sharing write privileges in combination with home directory size

Fig. 4 indicates the pixel visualization of the faculty users. Here again, we explore the Shared Access
attribute with Write access; however, we now make this sensitive to the size of the directory. Thus the directory
size attribute (slider) is turned on about halfway. To reach a high value (red square), a user must have a large
directory, as well as provide Write access to a signiﬁcant part of his/her home directory. The picked user (yellow
square in Fig. 4), faculty251 has 5400 directories and shares 89% of the ﬁlesystem with other users.
Fig. 5 illustrates an example of modifying user access privileges. Here faculty245’s home directory has full
administrative access to user657 except for directories. The top panels show the selection of faculty245 (orange
square in the top left panel), and the largely yellow bar and yellow squares in the directory view(top right panel).
We have implemented the means to apply ﬁle system commands to our data in memory (at this time); the
bottom panels indicates the result after user657’s access privileges have been removed. Now faculty245 is the
blue square, indicating that this user’s directory is largely unshared. This is conﬁrmed by the mostly green

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

313

Figure 5. Example illustrating modification of access control privileges, before(top panels) and after (bottom panels).

horizontal bar and the green squares in the directory view. This simple example illustrates a more robust and
less error-prone means to managing access privileges of users who might have very large home directories.

4. CONCLUSIONS
As ﬁlesystems continue to increase in complexity and size in the information age, secure management and
monitoring of such systems will be an important issue in large-scale networks. In this article, we have presented
scalable information visualization tools that can provide a front-end to such large systems. By looking at diﬀerent
attributes of user directories, we have begun to deﬁne useful metrics that can capture the underlying relationships.
Here we have demonstrated our system with the AFS ﬁlesystem.
There are a number of issues that need to be dealt with before this system can be utilized by system
administrators on a routine basis, as follows:
• Currently, our system works with a snapshot of the ﬁle system. Changes to the ﬁle system and access
privileges do not update the visualization system. Thus, we will need the means to record such changes
to both ﬁle system structure (creation and deletion of users, directory changes, etc.), and access privileges
need to be recorded in log ﬁles that can be monitored by the visualization system, so as to be uptodate.
Given that such changes come from a large body of users and groups of system administrators, this brings
up issues of synchronization, that must be addressed.
• Changes to the access privileges (Fig. 3) are only made to the data structures in memory, to illustrate the
capabilities of the system. These changes should then be propagated to the underlying ﬁle system. The

314

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

ability to change access privileges by the administrator via the visualization interface is a more attractive
means to deal with large ﬁlesystems.
• The current system is somewhat user-centric, meaning only user directories are stored. However, there are
some non-user directories that must still be monitored, such as applications and data directories. In the
example dataset, there are twelve such critical directory paths that can be monitored. In the future, we
would need to further classify these into individual application directories. For instance, during upgrades,
these directories may undergo signiﬁcant changes in structure and user access and the visualization system
would be useful to ensure that the access privileges are appropriate prior to public release.
Our long-term goal is to use information visualization as the means to view and comprehend complex security
policies that are currently in use, as well as understand their vulnerabilities and shortcomings. This can then
lead to the development of new infrastructure and strategies that call for shared access to resources in networked
environments. Mechanisms must be provided to protect sensitive and conﬁdential information from adversaries.
We believe our tool can address the issue of how to advocate selective information sharing while minimizing the
risks of unauthorized access through the eﬀective visual analysis of a) unauthorized sharing and b) violation of
access control policy.

5. ACKNOWLEDGEMENTS
Our thanks to Dr. Charles Price who generated several versions of the AFS ﬁlesystem dataset. The work of
Gail-Joon Ahn was partially supported at the Laboratory of Information Integration, Security and Privacy at
the University of North Carolina at Charlotte by a grant from National Science Foundation (NSF-IIS-0242393) .

REFERENCES
1. B. Shneiderman, “Tree visualization with tree-maps:2-d space ﬁlling approach,” ACM Transactions on
Graphics 11(1), pp. 92–99, 1992.
2. B. Shneiderman, M. Wattenberg, and D. Jones, “Ordered treemap layouts,” in Proceedings of IEEE Information Visualization 2001, Oct. 22-23, San Diego, CA., pp. 73–78, IEEE Computer Society, 2001.
3. J. van Wijk and H. van de Wetering, “Cushion treemaps: Visualization of hierarchical information,” in
Proceedings of IEEE Information Visualization 99, Oct. 24-29, San Francisco, CA., IEEE Computer Society,
1999.
4. F. van Ham and J. van Wijk, “Beam trees: Compact visualization of large hierarchies,” in Proceedings of
IEEE Information Visualization 2002, Oct. 19-24, Boston, MA., pp. 93–100, IEEE Computer Society, 2003.
5. P. Proctor, Practical Intrusion Detection Handbook, Prentice Hall Inc., 2000.
6. R. Erbacher, K. Walker, and D. Frinckle, “Intrusion and misuse detection in large-scale systems,” IEEE
Computer Graphics and Applications 22, Jan/Feb 2002.
7. R. Erbacher, “Visual traﬃc monitoring and evaluation,” in Proceedings of the Conference on Internet Performance and Control of Network Systems II, pp. 153–160, Denver, Co., August 2001.
8. S. Teoh, K. Ma, and X. Zhao, “Case study: Interactive visualization for internet security,” in Proceedings
of the IEEE Visualization 2002, pp. 505–508, nov 2002. 0ct. 27-Nov. 1, Boston, MA.
9. Y. Wang and Y. Zheng, “Fast and secure worm storage systems,” in Proceedings of the IEEE Security in
Storage Workshop (SISW), pp. 11–19, Oct. 31, Washington DC 2003.
10. R. Campbell, Managing AFS, The Andrew File System, Prentice Hall Inc., 1998.
11. “Open afs.” http://www.openafs.org.
12. Y. Leung and M. Apperley, “A review and taxonomy of distortion-oriented presentation techniques,” ACM
Transactions on Computer Human Interaction 1(2), 1994.
13. B. Spitzak, “The fast light toolkit.” http://www.ﬂtk.org.

SPIE-IS&T/ Vol. 5669

Downloaded From: http://proceedings.spiedigitallibrary.org/ on 05/24/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

315

SocialImpact: Systematic Analysis
of Underground Social Dynamics
Ziming Zhao, Gail-Joon Ahn, Hongxin Hu, and Deepinder Mahi
Laboratory of Security Engineering for Future Computing (SEFCOM)
Arizona State University, Tempe, AZ 85281, USA
{zmzhao,gahn,hxhu,dmahi}@asu.edu

Abstract. Existing research on net-centric attacks has focused on the
detection of attack events on network side and the removal of rogue programs from client side. However, such approaches largely overlook the
way on how attack tools and unwanted programs are developed and
distributed. Recent studies in underground economy reveal that suspicious attackers heavily utilize online social networks to form special
interest groups and distribute malicious code. Consequently, examining social dynamics, as a novel way to complement existing research
eﬀorts, is imperative to systematically identify attackers and tactically
cope with net-centric threats. In this paper, we seek a way to understand
and analyze social dynamics relevant to net-centric attacks and propose
a suite of measures called SocialImpact for systematically discovering
and mining adversarial evidence. We also demonstrate the feasibility and
applicability of our approach by implementing a proof-of-concept prototype Cassandra with a case study on real-world data archived from the
Internet.

1

Introduction

Today’s malware-infected computers are deliberately grouped as large scale destructive botnets to steal sensitive information and attack critical net-centric
production systems [1]. The situation keeps getting worse when botnets make
use of legitimate social media, such as Facebook and Twitter, to launch botnet
attacks [2]. Previous research eﬀorts on countering botnet attacks could be classiﬁed into four categories: (i) capturing malware samples [3], (ii) collecting and
correlating network and host behaviors of malware [27], (iii) understanding the
logic of malware [4], and (iv) inﬁltrating and taking over botnets [5].
Notably, most studies in the area of countering malware and botnets have
been focused on detecting bot deployment, capturing and controlling bot behaviors. However, there is little research on examining how these malicious programs
are created, rented and sold by adversaries. Even though preventive solutions


This work was partially supported by the grants from National Science Foundation
(NSF-IIS-0900970 and NSF-CNS-0831360). All correspondence should be addressed
to Dr. Gail-Joon Ahn, gahn@asu.edu.

S. Foresti, M. Yung, and F. Martinelli (Eds.): ESORICS 2012, LNCS 7459, pp. 877–894, 2012.
c Springer-Verlag Berlin Heidelberg 2012


878

Z. Zhao et al.

against thousands of known bots have been deployed on networked systems,
and some botnets were even taken down by law enforcement agencies [6], the
majority of adversaries are still at large and keep threatening the Internet by
developing more bots and launching more net-centric attacks. The major reason
for this phenomenon is that previous malware-related activities–such as developing, renting and selling bots–occurred mostly oﬄine, which were way beyond
the scope of security analysts.
In recent years, the pursuit of more proﬁt in underground communities leads to
the requirement for global collaboration among adversaries, which tremendously
changed the division of labor and means of communication among them [8].
(Un)fortunately, adversaries started to communicate with each other, distribute
and improve attack tools with the help of the Internet, which leaves security
analysts new clues for evidence acquisition and investigation on unwanted program development and trade. Before the widespread use of online social networks
(OSNs), adversaries would communicate via electronic bulletin board systems
(BBS), forums, and Email systems [10].
Content-rich Web 2.0, ubiquitous computing equipments, and newly emerging
online social networks provide an even bigger arena for adversaries. In particular,
the value of OSNs for adversaries is the capability to cooperate with destructive
botnets. The role of OSNs in botnet attacks is twofold: ﬁrst, OSNs are the platforms to form online black markets, release bots, and coordinate attacks [3,9];
second, OSN user accounts act as bots to perform malicious actions [7] or C&C
server nodes coordinates other networked bots [2]. Although our eﬀorts in this
paper are mainly concerned about the former case, our proposed model for online underground social dynamics and corresponding social metrics can be also
utilized to identify compromised and suspicious OSN proﬁles.
Given the great amount of valuable information in online social dynamics, the
investigation of the relationships between online underground social communities
and network attack events are imperative to tactically cope with net-centric
threats. In this paper, we propose a novel solution using social dynamics analysis
to counter malware and botnet attacks as a complement to existing research
investments.
The major contributions of this paper are summarized as follows:
– We formulate an online underground social dynamics considering both social
relationships and user-generated contents.
– We propose a suite of measures named SocialImpact to systematically
quantify social impacts of individuals and groups along with their online conversations which facilitate adversarial evidence acquisition and
investigation.
– We implement a proof-of-concept system based on our proposed model and
measures, and evaluate our solution with real-world data archived from the
Internet. Our results clearly demonstrate the eﬀectiveness of our approach
for understanding, discovering, and mining adversarial behaviors.
The rest of this paper is organized as follows. Section 2 presents our online
underground social dynamics model and addresses SocialImpact, which is a

SocialImpact: Systematic Analysis of Underground Social Dynamics

879

systematic ranking analysis suite for mining adversarial evidence based on the
model. In Section 3, we discuss the design and implementation of our proof-ofconcept system Cassandra. Section 4 presents the evaluation of our approach
followed by the related work in Section 5. Section 6 concludes this paper.

2

SocialImpact: Bring Order to Online Underground
Social Dynamics

In this section, we ﬁrst address the modeling approach we utilized to represent
online underground social dynamics (OUSDs). Unlike existing OSN models [11]
which emphasize on user proﬁle, friendship link, and user group, our model gives
attention to user-generated contents due to the fact that a wealth of information resides in online conversations. We also elaborate the design principles of
social metrics to identify adversarial behaviors in OUSDs. Then, we present
SocialImpact, which consists of nine indices, to bring order to underground
social dynamics based on our OUSD model.
2.1

Online Underground Social Dynamics Model

As shown in Figure 1, an OUSD can be represented by six fundamental entities
and ﬁve basic types of unidirectional relationships between them.

followerOf

User

Post
authorOf

Article

memberOf
hostOf

Group

String

containerOf

Comment

Fig. 1. OUSD Model: Entities and Relationships

Users are those who have proﬁles in the network and have the rights to join
groups, post articles, and give comments to others. Groups are those to which
users can belong. In an OUSD, groups are mainly formed based on common
interests. Articles are posted by users who want to share them with the society. In an OUSD, articles might introduce the latest technologies, analyze recent
vulnerabilities, call for participation of network attacks, and trade newly developed and deployed botnets. In terms of the form of articles, they do not have
to be literary. They could also contain multimedia contents, such as photos and
melodies. Comments are the subsequent posts to articles. Posts are the union
of articles and comments. Strings are the elementary components of articles and
comments. Strings are not necessarily meaningful words. They could be names,

880

Z. Zhao et al.

URLs, and underground slangs. A user has a relationship authorOf with each
post s/he creates. A user has a relationship followerOf with each user s/he
follows. A user has a relationship memberOf with each group s/he joins. An
article has a relationship hostOf with each comment it receives. A post has a
relationship containerOf with each string it consists of.
The following formal description summarizes the above-mentioned entities
and relationships.
Definition 2.1 (Online Underground Social Dynamics). An OUSD is
modeled with the following components:
–
–
–
–
–
–
–
–
–
–

–

U is a set of users;
G is a set of user groups;
A is a set of articles;
C is a set of comments;
P is a set of posts. P = A ∪ C;
S is a set of strings;
U P = {(u, p)| u ∈ U, p ∈ P and u has an authorOf relationship with p} is a
one-to-many user-to-post relation denoting a user and her posts;
F L = {(u, y)| u ∈ U, y ∈ U and u has a followerOf relationship with y} is a
many-to-many user-to-user follow relation;
M B = {(u, g)| u ∈ U, g ∈ G and u has a memberOf relationship with g} is
a many-to-many user-to-group membership relation;
AC = {(a, c)| a ∈ A, c ∈ C and a has a hostOf relationship with c} is a oneto-many article-to-comment relation denoting an article and its following
comments; and
P S = {(p, s)| p ∈ P, s ∈ S and p has a containerOf relationship with s} is a
many-to-many post-to-string relation.

We focus on the main structure and activities in online underground society and
overlook some sophisticated features & functionalities, such as online chatting,
provided by speciﬁc OSNs and BBS. Hence, our OUSD model is generic and can
be a reference model for most real-world OSNs and BBS. As a result, security
analysts could easily map real-world social dynamics data archived from any
OSNs and BBS to our model for further analysis and investigation.
2.2

Principles of Metric Design and Definitions

We also address the following critical issues related to evidence mining in underground society: How can we identify adversaries among a crowd of social users?
Given the additional evidence acquired from other sources, how can we correlate
them with underground social dynamics? How can we measure the evolution
in underground community? To answer these questions, we articulate several
principles that the measures for underground social dynamics analysis should
follow: 1) The measures should support identiﬁcations of interesting adversaries
and groups based on both their social relationships and online conversations; 2)

SocialImpact: Systematic Analysis of Underground Social Dynamics

881

The measures should be able to take external evidence into account and support interactions with security analysts; and 3) The measures should support
temporal analysis for the better understanding of the evolution in adversarial
groups.
To this end, we introduce several feature vectors to achieve aforementioned
goals. For the mathematical notations, we use lower case bold roman letters such
as x to denote vectors, and uppercase bold roman letters such as V to denote
matrices. We assume all vectors to be column vectors and a superscript T to denote the transposition of a matrix or vector. We also deﬁne max() as a function
to return the maximum value of a set.
Definition 2.2 (Article Influence Vector). Given an article a ∈ A, the article influence vector of a is defined as vaT = (v1 , v2 , v3 ), where v1 is the length
of the article, v2 = |{c | c ∈ C and (a, c) ∈ AC}| is the number of comments
received by a, and v3 is the number of outlinks it has.
When stacking all articles’ inﬂuence vector together, we get the article influence matrix V. We assess an article’s inﬂuence by its activity generation,
novelty and eloquence [12].
Definition 2.3 (Article Relevance Factor). Given a set of strings s =
{s1 , s2 , ..., sn } ⊆ S and an article a ∈ A, article relevance factor, denoted as
r(a, s), is defined as the number of occurrence of strings s in the article a.
The strings s could represent an external evidence that security analysts acquired
from other sources and query keywords in which security analysts are interested.
Definition 2.4 (User Activeness Vector). The user activeness vector of u
is defined as zTu = (z1 , z2 , z3 ), where z1 = |{p | p ∈ P and (u, p) ∈ U P }| is the
number of articles and comments u posted, z2 = |{y | y ∈ U and (u, y) ∈ F L}|
is the number of users u follows, and z3 = |{g | g ∈ G and (u, g) ∈ M B}| is the
number of groups u joins.
We measure a user’s activeness by the number of posts s/he sends, users s/he
follows, and groups s/he joins. By aggregating all users’ zu , we get user activeness matrix Z.
Definition 2.5 (Social Matrix). Social matrix, denoted as Q, is defined as a
|U | × |U | square matrix with rows and columns corresponding to users. Let v be
a user and Nv be the number of users v follows. Qu,v = 1/Nv , if (v, u) ∈ F L
and Qu,v = 0, otherwise.
Social matrix is similar to transition matrix for hyperlinked webpages in PageRank. The sum of each column in social matrix is either 1 or 0, which depends on
whether the vth column user follows any other user.
Definition 2.6 (δ-n Selection Vector). A δ-n selection vector, denoted as
yδn , is defined as a boolean vector with n components and yδn 1 = δ.

882

Z. Zhao et al.

A δ-n selection vector is used to select a portion of elements for one set. For
example, the top 10 inﬂuential articles of a user a could be represented by a
|A|
selection vector y10 over the article set A. By stacking all users’ δ-n selection
vectors over the same set together, we get the δ-n selection matrix Yδn .
2.3

Ranking Metrics

As shown in Figure 2, SocialImpact consists of nine indices, which are classiﬁed
into three categories: string & post indices, user indices, and group indices. Each
index in upper categories is computed by the indices from lower categories.
To fulﬁll Principle 1, user and group indices are devised to identify inﬂuential,
active, and relevant users and groups. We devise personalized PageRank models [13] to calculate UserInfluence and UserRelevance, since it could capture the
characteristics of both user-to-user relationships and user-generated contents in
social dynamics. To accommodate Principle 2, ArticleRelevance, UserRelevance
and GroupRelevance are designed to take external strings as inputs, combine
them with existing data in social dynamics, and generate more comprehensive
results. To fulﬁll Principle 3, all feature vectors and indices could be calculated
for a given time window and StringPrevalence could indicate the topic evolution in the society. Moreover, we believe the combination of UserActiveness and
UserInfluence could also be used to identify suspicious spam proﬁles in online
social networks.
We consider a weighted additive model [14] when there exist several independent factors to determine one index. To reduce the bias introduced by diﬀerent
size of sets, we use δ-n selection vector to choose a portion of data in calculation.
The followings are the detailed descriptions of indices.
ArticleInfluence, denoted as x1 (a), represents the influence of article a. x1 (a)
is computed as vaT w1 , where w1 denotes the weight vector.
By normalizing x1 (a) to [0, 1] and stacking x1 (a) from all articles together,
we get a vector x1 .
x1 =

VT w1
maxb∈A (x1 (b))

(1)

ArticleRelevance, denoted as x2 (a, s), represents the relevance of the article a to
given strings s. x2 (a, s) is proportional to the occurrence of the given strings in
the article and the influence of the article.

Group Indices

User Indices
String & Post
Indices

GroupInfluence

GroupRelevance

GroupActiveness

UserInfluence

UserRelevance

UserActiveness

ArticleInfluence

ArticleRelevance

StringPrevalence

Fig. 2. SocialImpact: Systematic Ranking Indices

SocialImpact: Systematic Analysis of Underground Social Dynamics

x2 (a, s) =

r(a,s)x1 (a)
maxb∈A (r(b,s)x1 (b))

883

(2)

By stacking x2 (a, s) from all users together, we get a vector x2 (s) denoting all
articles’ relevance to s.
UserInfluence, denoted as x3 , represents the influence of a user. x3 can be
measured by two parts. One is the impact of the user’s opinions, which is modeled
by ArticleInfluence. The other is the user’s social relationships, which is modeled
by Q. x3 is devised as a personalized PageRank function to capture both parts.
By stacking x3 from all users together, we get a vector x3 .
|A|

x3 = d3 Qx3 + (1 − d3 )Yα x1

(3)

Where d3 ∈ (0, 1) is the decay factor which makes the linear system stable and
|A|
convergent. Yα is the δ − n selection matrix corresponding to all users’s top α
influential articles.
UserRelevance, denoted as x4 (s), represents the relevance of a user to
strings s.
By stacking x4 (s) from all users together, we get a vector x4 .
|A|

x4 (s) = d4 Qx4 (s) + (1 − d4 )(Yα x2 (s))

(4)

|A|

Where d4 ∈ (0, 1) is the decay factor. Yα is a δ − n selection matrix corresponding to all users’s top α relevant articles to s.
UserActiveness, denoted as x5 , represents the activeness of a user.
x5 = ZT w5

(5)

We use the addition of a group’s top α members’ inﬂuence, relevance, and activeness to model its inﬂuence, relevance, and activeness, respectively. As mentioned
before, this model can reduce the bias caused by the number of members.
GroupInfluence, denoted as x6 , represents the influence of a group.
By stacking all x6 together, we get x6 .
|U|

x6 = Yα x3

(6)

|U|

Where Yα is the δ-n selection matrix corresponding to all groups’ top α influential users.
GroupRelevance, denoted as x7 , represents the relevance of a group to
strings s.
By stacking all x7 together, we get x7 .
|U|

x7 = Yα x4
|U|

(7)

Where Yα is the δ-n selection matrix corresponding to all groups’ top α relevant
users.
GroupActiveness, denoted as x8 , represents the activeness of a group.

884

Z. Zhao et al.

By stacking all x8 together, we get x8 .
x8 = Yα|U| x5

(8)

|U|

Where Yα is the δ-n selection matrix corresponding to all groups’ top α active
users.
StringPrevalence, denoted as x9 (s), represents the popularity of a string s.

tis,pj
(9)
x9 (s) =
pj ∈P

where tis,pj is the term frequency-inverse document frequency [15] of a string s
in post pj .
The computations for UserInfluence and UserRelevance are proven to be convergent [16]. And the corresponding time complexity is O(|H|log(1/)), where
|H| is the number of f ollowerOf relationships in the social dynamics and  is a
given degree of precision [16]. The time complexity for calculating StringPrevalence is O(|P ||S|), where |P | is the number of posts and |S| is the size of string
set. The complexities for all other indices are linear if the underlying indices are
calculated.

Cassandra: System Design and Implementation

3

In this section, we describe the challenges in analyzing real-world underground
social dynamics data. We address our eﬀorts to cope with these challenges and
present the design and implementation of our proof-of-concept system
Cassandra.
3.1

Challenges from Real-World Data

The ﬁrst challenge of real-world data is its multilingual contents. The most effective way of coping with this challenge is to take advantage of machine translation systems. Cassandra utilizes Google Translate1 to detect the language of the
contents and translate them into English. However, machine translation systems
may fail to generate meaningful English interpretations for the following cases: i)
adversaries may use cryptolanguages that no machine translation system could
understand. For instance, Fenya, a Russian cant language that is usually used in
prisons, is identiﬁed in online underground society [17]; and ii) both intentional
and accidental misspellings are common in online underground society [18]. In
order to cope with this challenge, Cassandra maintains a dictionary of known
jargons, such as c4n as can and sUm1 as someone.
Another challenge is that the social dynamics data may not be in a consistent
format. Diﬀerent OSNs use diﬀerent styles in web page design. Even in one
OSN, in order to make the web page more personalized, the OSN allows users to
1

http://code.google.com/apis/language/translate/overview.html

SocialImpact: Systematic Analysis of Underground Social Dynamics

885

customize the format of their posts. Since HTML is not designed to be machineunderstandable in the ﬁrst place, extracting structural information from HTML
is a tedious and heavy-labor work. To address this problem, we ﬁrst cluster data,
and then devise an HTML parser for each cluster. We also design a light-weight
semi-structure language to store the information extracted from HTML.
Since one major component in social dynamics is the relationships between
entities, storing and manipulating social dynamics data in a relational database
become relatively time-consuming. We choose a graph database [19] which employs the concepts from graph theory, such as node, property, and edge, to realize
faster operations for associative data sets.
3.2

System Architecture and Implementation

Figure 3 shows a high level architecture of Cassandra. The upper level of
Cassandra includes several visualization modules and provides query control
for security analysts to provide the additional evidence. In reality, these evidences could be in the format of text, picture, video, audio or any other forms.
Yet, representing multimedia contents like pictures and videos in a machineunderstandable way is still a diﬃcult challenge. Cassandra acts like a modern
web search engine in response to keyword queries. Social graph viewer is designed
to show social relationships among users and groups. Ranking analysis viewer
is used to list the ranking results based on security analysts’ queries. Content
viewer can show both original and translated English web resources.
The lower level of the architecture realizes underlying functionalities addressed
in our framework. After underground community data is crawled from the Internet, the HTML parser module extracts meaningful information from it. If
the content is not in English, our translator takes over and generates English
translation. All extracted information is stored in a graph database for the efﬁcient retrieval. Analysis modules have two working modes: oﬄine and online.
The oﬄine mode generates demographical information with demographical analysis engine (DAE) and intelligence, such as user inﬂuence and activeness, with
SocialImpact engine (SIE). When security analysts provide the additional evidence, SocialImpact engine switches to online mode and generates analysis
Social
Dynamics

Extra
Evidence

Visualization Modules
Social Graph Viewer
Query Control

Ranking Analysis Viewer
Content Viewer

Underlying Functionality Modules
Pre-process Modules

Analysis Modules

Web Crawler
HTML Parser
Translator

Graph
Database

SocialImpact
Engine (SIE)
Demographical
Analysis Engine
(DAE)

Fig. 3. System Architecture of Cassandra

886

Z. Zhao et al.

(a) Social Graph

(b) User Ranking

(c) Article Ranking

Fig. 4. Screenshots of Cassandra

results, such as user relevance, based on data in the graph database and additional evidence provided by security analysts.
Cassandra was implemented in Java programming language. We took advantage of Java swing and JUNG to realize graphical user interfaces and graph
visualization. As we mentioned before, Cassandra uses Google Translate API to
translate texts. In most cases, Google Translate could output acceptable translations from original texts. Cassandra stores user proﬁles, user-generated contents,
and social relationships among users in a Neo4j2 graph database. For each group,
user, article, and comment, Cassandra creates a node in the database, stores associated data–such as the birthday of user and the content of article–in each
node’s properties, and assigns the relationships among nodes.

3.3

Visualization Interfaces of Cassandra

Figure 4 depicts interfaces of Cassandra. As illustrated in Figure 4(a), all users
in a social group are displayed by a circle. And their followerOf relationships
are displayed with curved arrows. It is clear to view that some users have lots
of followers while others do not. By clicking any user in the group, Cassandra
has the ability to highlight this user in red and all his followers in green. In
this way, Cassandra helps analysts understand the social impact of any speciﬁc
user. Another window as shown in Figure 4(b) displays the ranking results.
Analysts can specify the ranking metric, such as UserInfluence and UserActiveness,
to reorder the displayed rank. Clicking a user’s name which is the second column
in Figure 4(b) would bring the analysts to the list of all articles posted by the
user in descending order of ArticleInfluence. Clicking the user’s proﬁle link which
is the third column in Figure 4(b) would bring the analysts to the webpage of
the user’s proﬁle archived from the Internet. Analysts could also specify some
keywords in query control and Cassandra would display the results in descending
order of ArticleRelevance. As shown in Figure 4(c), Cassandra displays both the
original and translated texts and highlights the input keywords in red.
2

http://neo4j.org/

SocialImpact: Systematic Analysis of Underground Social Dynamics

4

887

A Case Study on Real-World Online Underground
Social Dynamics

In this section, we present our evaluation on real-world social dynamics. We
evaluated Cassandra on 4GB of data crawled from Livejournal.com which is a
popular online social network especially in the Russian-speaking countries. We
anonymized the group names and user names in this OSN for preserving privacy.
All webpages in this OSN could be roughly divided into two categories in terms
of content: i) proﬁle and ii) article. A proﬁle webpage contains basic information
of a user or a group, which includes name, biography, location, birthday, friends,
and members. Every article has title, author, posted time, content, and several
comments by other users. The webpages are mainly .html ﬁles, along with some
.jpeg, .gif, .css, and .js ﬁles. Our solution only considers text data from .html
ﬁles.
We started to crawl group proﬁles from six famous underground groups in this
OSN 3 . Then we crawled all members’ proﬁles and articles of these six groups.
We also collected one-hop friends’ articles of these members. Therefore, we ended
up with 29,614 articles posted by 6,364 users which are from 4,220 groups. Based
on the information in user proﬁles, we noticed that about 32.7% and 52.7% users
were born in early and mid-late 80’s. This clearly illustrates the age distribution
of active users in this community.
4.1

Post, User and Group Analysis

Cassandra calculated all articles’ ArticleInfluence and identiﬁed top 50 articles
over a time window of 48 months. Since not all of these articles are related
to computer security, we checked these articles in descending order of their inﬂuences and picked ﬁve articles that are highly related to malware. We could
observe some popular words related to malware, such as PE (the target and vehicle for Windows software attacks), exploits (a piece of code to trigger system
vulnerabilities), hook (a technique to hijack legitimate control ﬂow) and so on.
Table 1. Top Five Inﬂuential/Active Users/Groups
Top Five Influential Users
User
UserInfluence
z xx ur
49.5020
43.7800
andxx ur
34.8074
arkxx ur
moxx ur
26.7700
20.6292
kyp ur

Top Five Active Users
User
UserActiveness
xsbxx ur
4024
enkxx ur
3942
kalxx ur
3936
exixx ur
3170
kolxx ur
3092

Top Five Influential Groups
Group
GroupInfluence
b gp
344.4807
c gp
79.7781
d gp
45.5222
murxx gp
26.2094
chrxx gp
18.6487

Top Five Active Groups
Group
GroupActivenss
b gp
57798
d gp
28644
demxx gp
20846
beaxx gp
20290
hoxx gp
19486

Cassandra also generated each user’s UserInfluence and UserActiveness and
group’s GroupInfluence and GroupActiveness over a time window of 48 months.
3

These targeted groups are indicated by law enforcement agency who sponsored this
project.

888

Z. Zhao et al.
3

2
1.8

GroupInfluence

UserInfluence

2.5

2

1.5

1

0.5

1.6
1.4
1.2
1
0.8
0.6
0.4
0.2

0

0

50

100

150

200

250

300

350

0

400

0

50

UserActiveness

100

150

200

250

300

350

400

GroupActiveness

(a) corrcoef = 0.5204

(b) corrcoef = 0.9094

5
4
3
2
1
0
40

1200

GroupInfluence

UserActiveness

UserInfluence

6

1000
800
600
400
200
0
40

40

30
30

20
10

Month

20

(a) UserInfluence
over 48 months

20

15

10

5

0
40

40

30
30

20
10

10

User

GroupActiveness

Fig. 5. Correlation Coeﬃcient of UserActiveness & UserInfluence and GroupActiveness
& GroupInfluence

Month

20

(b) UserActiveness
over 48 months

2000
1500
1000
500
0
40

40

30
30

20
10

10

User

2500

Month

20

40

30

Group

(c) GroupInfluence
over 48 months

30

20
10

10

Month

20
10

Group

(d) GroupActiveness
over 48 months

Fig. 6. Temporal Pattern Analysis

And, Table 1 shows the top ﬁve inﬂuential/active users/groups for the entire
period of our observation. We can notice that there is no overlap between the
top ﬁve influential users and the top ﬁve active users, while there exists similarity
for the top ﬁve influential groups and the top ﬁve active groups.
We calculated the correlation coeﬃcient (corrcoef ) for the pairs of
UserInfluence and UserActivenss, GroupInfluence and GroupActivenss based on the
results generated from Cassandra. Similar to the phenomenon we identiﬁed in
Table 1, in Figure 5(a) we observed that the correlation coeﬃcient between
UserInfluence and UserActivenss is around 0.52 (the maximum value for correlation coeﬃcient is 1 indicating a perfect positive correlation between two variables), which means one user’s inﬂuence is not highly correlated to her/his activeness. This phenomenon indicates that talking more does not make a user more
inﬂuential in a community. On the other hand, as shown in Figure 5(b) we observed that the correlation coeﬃcient between GroupInfluence and GroupActivenss
is around 0.90, which indicates a very strong positive correlation between the
inﬂuence and the activeness of a group. The application of inﬂuence and activeness indices is not limited to identify such a social phenomenon. We could also
leverage the high UserActivenss and the low UserInfluence as indicators for the
analysis of social spammers in any OSN.
The temporal patterns of the inﬂuential/active users/groups could be observed in Figure 6, where x-axis denotes the users/groups who were identiﬁed

SocialImpact: Systematic Analysis of Underground Social Dynamics

889

Table 2. Results from Cassandra for Queries

(a) Results for Botnet

(c) Results for Vulnerabil(b) Results for Identity Theft ity Discovery and Malicious
Code Development
and Credit Card Fraud

Keywords Relevant Articles #

Keywords

spam
botnet
zeus
rustock
mega-d

pin
credit card
carding
credit card sale
ssn

490
44
9
1
0

Relevant Articles #
129
93
1
0
0

Keywords

Relevant Articles #

vulnerability
shellcode
polymorphic
zero-day
cve

418
169
12
11
2

as the most inﬂuential/active ones for each month. For example, x = 1 denotes
the most inﬂuential/active user/group of the ﬁrst month in our time window
and x = 48 denotes the most inﬂuential/active user/group of the last month in
our time window; y-axis denotes the entire 48 months in the time window; and
z-axis denotes user/group’s inﬂuence/activeness value. As shown in Figure 6(a),
some users maintain their inﬂuence status for several months. The large plain
area in the right part of this ﬁgure indicates most users come as the most inﬂuential ones suddenly. This observation implies that a user does not need to be
a veteran to be an inﬂuential one in the community. On the other side, we can
see from Figure 6(b) that most active users remain active before they became
the most active ones. The plain area in the left portion of Figure 6(b) implies
that most users do not always keep active. Normally they keep active for 15 - 30
months, then get relatively silent. While the smaller plain area in the left part
of Figure 6(a) shows once a user becomes inﬂuential, s/he keeps the status for a
long period of time. Figure 6(c) shows that there are 2 or 3 groups who maintain
the status of inﬂuence during the whole 48 months and get even more inﬂuential
as time goes on. While, other groups only keep inﬂuential for a relatively short
period of time and just fade out. Figure 6(d) shows the similar phenomenon.
4.2

Evidence Mining by Correlating Social Dynamics
with Adversarial Events

We present our ﬁnding with keyword queries on the same dataset in Cassandra.
For each query, Cassandra returns the lists of articles, users, and groups in
descending order of ArticleRelevance, UserRelevance and GroupRelevance, respectively. The results we present in this section are with regard to three major
adversarial activities: i) botnet; ii) identity theft and credit card fraud; and iii)
vulnerability analysis and malicious code development.
Botnet. As we mentioned before, botnet is a serious threat to all networked
computers. In order to identify adversaries and their conversations in our dataset
related to botnet, we queried the keywords shown in Table 2(a) in Cassandra.
Cassandra was able to identify 490 articles related to ‘spam’, 44 articles related
to ‘botnet’, 9 articles related to ‘zeus’ and 1 article about ‘rustock’.

890

Z. Zhao et al.

Then, we checked the results returned by Cassandra carefully and Table 3
shows several interesting articles and their information including the number of
comments they received, ArticleRelevance of each article, and authors of these
articles. We ﬁrst noticed one article titled ‘Rustock.C’ with very high ArticleRevelance and ArticleInfluence. This article presented an original analysis of the C
variant of Rustock that once accounted for 40% of the spam emails in the world.
Table 3. Selected Top Relevant Articles
Translated Article Title
# Comments Received
x2 1Author
Rustock.C
13 135.3 swx ur
On startup failure to sign the drivers in Vista x64
5 59.8 crx ur
video
3 35.6 zlx ur
sleepy
3 32.3 crx ur
FireEye Joins Internet2
2 27.8 eax ur
1

ArticleRelevance.

Another article titled ‘On startup failure to sign the drivers in Vista x64’ returned by Cassandra as a top relevant article to ‘botnet’ attracting our attention
as well. In this article, the author crx ur discussed about how to load unsigned
driver to Windows Vista x64 by modifying PE ﬁle header. The corresponding
author claimed that malware vendors would use this technique to build bots
and infect thousands of computers. A further investigation on this user shown
in Table 4 reveals that s/he authored several security-related articles. Her/his
proﬁle indicated that s/he was very active in malicious code development and
interested in several cybercrime topics, such as rootkit, exploits, and shellcode.
Table 4. Selected Articles by crx ur and Her/His Information
Translated Article Title
# Comments Received x1 1 Translated Interests
The old tale about security
7 79.6
malware, ring0, rootkit,
Malcode statistics
6 68.9
botnets, asm, exploits,
Cold boot attacks on encryption keys
2 37.6
cyber terrorism,
Wanted Cisco security agent
2 28.1
shellcode, viruses,
Antirootkits bypass
1 18.7
underground,
Syser debugger
0 8.9
Kaspersky, paintball
Termorektalny cryptanalysis
0 7.8
1

ArticleInfluence.

Identity Theft and Credit Card Fraud. Identity theft and credit card
fraud are both serious issues in Internet transactions. Online identity theft includes stealing usernames, passwords, social security numbers (SSNs), personal
identiﬁcation numbers (PINs), account numbers, and other credentials. Credit
card fraud also consists of phishing (a process to steal credit card information),
carding (a process to verify whether a stolen credit card is still valid), and selling
veriﬁed credit card information.
Table 2(b) shows results that Cassandra returned when these keywords are
queried. Cassandra identiﬁed one article that was authored by a user dx ur
related to ‘carding’ in the dataset. A further investigation on this user revealed

SocialImpact: Systematic Analysis of Underground Social Dynamics

891

Table 5. Information about dx ur
Translated Interests

carding, banking, shells, hacking, freebie, web hack, credit card
fraud, security policy, system administrators, live in computer
bugs
# Articles Posted
1295
# Comments Posted 7294
# Comments Received 2693

that s/he was a member of a carding interest group, which had more than 20
members around the world. Table 5 shows some basic information of dx ur.
Compared to crx ur, it is obvious that dx ur has more interests in ﬁnancial
security issues, such as credit card fraud, web hack, and banking. We could also
notice that dx ur was very active in posting articles and replying others’ posts.
Vulnerability Analysis and Malicious Code Development. We analyzed
several keywords related to vulnerability analysis and malicious code development, such as polymorphism (a technique widely used in malware to change
the appearance of code, but keep the semantics), CVE (a reference-method for
publicly-known computer vulnerabilities), shellcode (small piece of code used
as the payload in the exploitation of software vulnerabilities), and zero-day
(previously-unknown computer vulnerabilities, viruses and other malware).
As shown in Table 2(c), the community is very active in these topics. More
than 400 articles related to vulnerabilities were found. However, we noticed most
of these articles have low-ArticleInfluence. We checked these low-ArticleInfluence
articles and discovered that most of them were articles copied from other research blogs and kept the links to original webpages. Our ArticleInfluence index
successfully identiﬁed these articles were not very novel, thus calculated low
ArticleInfluence for them.
At the same time, as shown in Table 6, Cassandra also identiﬁed several
high-ArticleInfluence vulnerability analysis articles. For example, the article entitled ‘Blind spot’ authored by arx ur which analyzed a new Windows Internet
Explorer vulnerability even attracted 79 replies.
Table 6. Selected Top Relevant Articles
Translated Article Title
# Comments Received
x2 1Author
Blind spot
79 793.2 arx ur
Seven thirty-four pm PCR
14 146.4 tix ur
HeapLib and Shellcode generator under windows
1 15.6 eax ur
Who fixes vulnerabilities faster, Microsoft or Apple?
0
5.6 bux ur
FreeBSD OpenSSH Bugfix
0
4.2 sux ur
1

4.3

ArticleRelevance

Comparison with HITS Algorithm

In order to evaluate the eﬀectiveness of our approach, we implemented the hubs
and authorities algorithm (HITS) [20] in Cassandra and compared the results

892

Z. Zhao et al.

with our SocialImpact metrics. HITS algorithm is able to calculate the authorities and hubs in a community by examining the topological structure where
authority means the nodes that are linked by many others and hub means the
nodes that point to many others. Note that the fundamental diﬀerence between
SocialImpact and HITS is that SocialImpact takes more parameters, such
as user-generated content and activity, into account, therefore ranking results
are based on a more comprehensive set of social features.
Table 7. Top Five Authorities and Hubs by HITS
Top Five Authorities
User
auth
zhengxx ur
0.506
0.214
crx xx ur
0.163
yuz ur
t1mxx ur
0.148
0.143
rst ur

Top Five Hubs
User
hub
zlo xx ur 0.265
zhengxx ur 0.237
crx xx ur 0.234
yuz ur
0.205
t1mxx ur 0.183

Comparing the results for authorities and hubs shown in Table 7 with
UserInfluence and UserActiveness (SocialImpact) in Table 1, we can observe
that the authorities and hubs have much overlap with HITS algorithm when
online conversations are ignored and the results generated by SocialImpact
are diﬀerent from HITS counterparts.

5

Related Work

Computer-aided crime analysis (CACA) utilizes the computation and visualization of modern computer to understand the structure and organization of
traditional adversarial networks [21]. Although CACA is not designed for the
analysis of cybercrime, its methods of relation analysis, and visualization of social network are adopted in our work. Zhou et al. [22] studied the organization
of United State domestic extremist groups on web by analyzing their hyperlinks.
Chau et al. [23] mined communities and their relationships in blogs for understanding hate group. Lu et al. [24] used four actor centrality measures (degree,
betweenness, closeness, and eigenvector) to identify leaders in hacker community.
Motoyama et al. [29] analyzed six underground forums. In contrast, our proposed
solution in this paper considers both social relationships and user-generated contents in identifying interesting posts and users for cybercrime analysis.
Systematically bringing order to a dataset has plenty of applications in both
social and computer science. With the development of web, ranking analysis in
hyperlinked environment received much attention. Kleinberg [20] proposed HITS
by calculating the eigenvectors of certain matrices associated with the link graph.
Also, Page and Brin [25] developed PageRank that uses a page’s backlinks’ sum
as its importance index. However, both HITS and PageRank only consider the
topological structure of given dataset but ignore its contents [16]. Therefore, we

SocialImpact: Systematic Analysis of Underground Social Dynamics

893

devised a ranking system based on personalized PageRank, which is proposed
to eﬃciently deal with ranking issues in diﬀerent situations [13].
In order to provide a safer platform for net-centric business and secure the
internet experience for end users, huge research eﬀorts have been invested in
defeating malware and botnets. Cho et al. [26] proposed to infer protocol state
machines in botnet C&C protocols. Gu et al. analyzed botnet C&C channels
for identifying malware infection and botnet organization [27]. Stone-Gross et
al. [5] took over Torpig for a period of ten days and gathered rich and diverse
set of data from this infamous botnet. Besides research eﬀorts, legal actions are
taken to shutdown certain botnets. Srizbi and Mega-D botnets were taken down
in late 2008 and 2009 [6]. Recently, Microsoft took down Rustock by blocking
the controller and clearing out the malware infected [28]. Our work focusing on
the analysis of malware circulation is complementary to those existing eﬀorts on
countering net-centric attacks.

6

Conclusions

In this paper, we have presented a novel approach to help identify adversaries by
analyzing social dynamics. We formally modeled online underground social dynamics and proposed SocialImpact as a suite of measures to highlight interesting adversaries, as well as their conversations and groups. The evaluation of our
proof-of-concept system on real-world social data has shown the eﬀectiveness of
our approach. As part of future work, we would continuosly test the eﬀectiveness
and the usability of our system with subject matter experts and broader datasets.

References
1. Anselmi, D., Kuo, J., Santhanam, N., Boscovich, R.: Microsoft Security Intelligence
Report, vol. 9
2. Thomas, K.: The Koobface botnet and the rise of social malware. In: Proc. of the
5th IEEE International Conference on Malicious and Unwanted Software (MALWARE), pp. 1–8 (2010)
3. Bächer, P., Holz, T., Kötter, M., Wicherski, G.: Know your Enemy: Tracking
Botnets–Using honeynets to learn more about Bots (2005)
4. Chiang, K., Lloyd, L.: A case study of the rustock rootkit and spam bot. In: Proc.
of Usenix Workshop on Hot Topics in Understanding Botnets (2007)
5. Stone-Gross, B., Cova, M., Cavallaro, L., Gilbert, B., Szydlowski, M., Kemmerer,
R., Kruegel, C., Vigna, G.: Your botnet is my botnet: Analysis of a botnet takeover.
In: Proc. of Computer and Communications Security (CCS). ACM (2009)
6. Mushtaq, A.: Smashing the Mega-d/Ozdok botnet in 24 hours,
http://blog.fireeye.com/research/2009/11/smashing-the-ozdok.html
7. Athanasopoulos, E., Makridakis, A., Antonatos, S., Antoniades, D., Ioannidis, S.,
Anagnostakis, K.G., Markatos, E.P.: Antisocial Networks: Turning a Social Network into a Botnet. In: Wu, T.-C., Lei, C.-L., Rijmen, V., Lee, D.-T. (eds.) ISC
2008. LNCS, vol. 5222, pp. 146–160. Springer, Heidelberg (2008)
8. Dunham, K., Melnick, J.: Malicious bots: an inside look into the cyber-criminal
underground of the internet. Auerbach Pub. (2008)

894

Z. Zhao et al.

9. Holt, G.W.B., Thomas, J., Bossler, A.M.: Social Learning and Cyber Deviance:
Examining the Importance of a Full Social Learning Model in the Virtual World.
Journal of Crime and Justice, 33 (2010)
10. Goodin, D.: Online crime gangs embrace open source ethos,
http://www.theregister.co.uk/2008/01/17/globalization-of-crimeware
11. Zheleva, E., Getoor, L.: To join or not to join: the illusion of privacy in social
networks with mixed public and private user proﬁles. In: Proc. of the 18th International Conference on World Wide Web (WWW), pp. 531–540. ACM (2009)
12. Agarwal, N., Liu, H., Tang, L., Yu, P.: Identifying the inﬂuential bloggers in a
community. In: Proc. of the 1st International Conference on Web Search and Web
Data Mining (WSDM). ACM (2008)
13. Chakrabarti, S.: Dynamic personalized pagerank in entity-relation graphs. In: Proc.
of World Wide Web, WWW (2007)
14. Keeney, R., Raiﬀa, H.: Decisions with multiple objectives. Cambridge Books (1993)
15. Salton, G., Buckley, C.: Term-weighting approaches in automatic text retrieval.
Information Processing & Management 24(5), 513–523 (1988)
16. Bianchini, M., Gori, M., Scarselli, F.: Inside pagerank. ACM Transactions on Internet Technology (TOIT) 5(1), 92–128 (2005)
17. Yarochki, F.V.: From Russia with love.exe, http://www.seacure.it/archive/
2009/stuff/Seacure2009FyodorYarochkin-FromRussiaWithLove.pdf
18. Raymond, E.: The new hacker’s dictionary. The MIT Press (1996)
19. Angles, R., Gutierrez, C.: Survey of graph database models. ACM Computing
Surveys (CSUR) 40(1), 1–39 (2008)
20. Kleinberg, J.: Authoritative sources in a hyperlinked environment. Journal of the
ACM (JACM) 46(5), 604–632 (1999)
21. Xu, J., Chen, H.: CrimeNet explorer: a framework for criminal network knowledge discovery. ACM Transactions on Information Systems (TOIS) 23(2), 201–226
(2005)
22. Zhou, Y., Reid, E., Qin, J., Chen, H., Lai, G.: US domestic extremist groups on
the Web: link and content analysis. In: IEEE Intelligent Systems, pp. 44–51 (2005)
23. Chau, M., Xu, J.: Mining communities and their relationships in blogs: A study
of online hate groups. International Journal of Human-Computer Studies 65(1),
57–70 (2007)
24. Lu, Y., Polgar, M., Luo, X., Cao, Y.: Social Network Analysis of a Criminal Hacker
Community. Journal of Computer Information Systems, 31–42 (2010)
25. Page, L., Brin, S., Motwani, R., Winograd, T.: The PageRank Citation Ranking:
Bringing Order to the Web (1999)
26. Cho, C., et al.: Inference and analysis of formal models of botnet command and
control protocols. In: Proc. of the 17th ACM Conference on Computer and Communications Security (CCS), pp. 426–439. ACM (2010)
27. Gu, G., Porras, P., Yegneswaran, V., Fong, M., Lee, W.: Bothunter: Detecting malware infection through ids-driven dialog correlation. In: Proc. of USENIX Security
Symposium. USENIX Association (2007)
28. Prince, B.: Microsoft takes down a botnet responsible for 39 percentage of global
spam, http://www.pcmag.com/article2/0,2817,2368935,00.asp
29. Motoyama, M., McCoy, D., Levchenko, K., Savage, S., Voelker, G.M.: An analysis
of underground forums. In: Proceedings of the 2011 ACM SIGCOMM Conference
on Internet Measurement Conference. ACM (2011)

A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian
Arizona State University
Mesa, AZ 85212
USA
{kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu
Abstract
Software engineering education is a technologically challenging, rapidly evolving
discipline. Like all STEM educators, software engineering educators are bombarded with
a constant stream of new tools and techniques (MOOCs! Active learning! Inverted
classrooms!) while under national pressure to produce outstanding STEM graduates.
Software engineering educators are also pressured on the discipline side; a constant
evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the
technology, guidance on the adoption of project-centric curricula is needed. This paper
focuses on vertical integration of project experiences in undergraduate software
engineering degree programs or course sequences. The Software Enterprise, now in its
9 th year, has grown from an upper-division course sequence to a vertical integration
program feature. The Software Enterprise is presented as an implementation of a project
spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those
in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software
engineering and computer science education focus on content taxonomies and bodies of
knowledge. This is not a bad thing, but taken in isolation may lead educators to believe
content coverage is more important than applied learning experiences. There is literature
on project-based learning within computing as a means to learn soft skills and complex
technical competencies. However, project experiences tend to be disjoint [5]; there may
be a freshman project or a capstone project or a semester project assigned by an
individual instructor. Yearlong capstone projects are offered at most institutions as a
synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do
it all the time?
Project experiences, while pervasive in computing programs, are not a central
integrating feature. Sheppard et al. [6] suggests that engineering curricular design should
move away from a linear, deductive model and move instead toward a networked model:
“The ideal learning trajectory is a spiral, with all components revisited at increasing
levels of sophistication and interconnection” ([6] p. 191). The general engineering degree
program at Arizona State University (ASU) was designed from its inception in 2005 [7]
to be a flexible, project-centric curriculum that embodied such integration (even before
[6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division
course sequence to integrate contextualized project experiences with software engineering
fundamental concepts. The computing and engineering programs at ASU’s Polytechnic
campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

299
c 2013 IEEE
978-1-4673-5140-9/13/$31.00 

CSEE&T 2013, San Francisco, CA, USA

Board of Regents (ABOR) approved a new Bachelor’s degree in software engineering
(BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo
accreditation review shortly thereafter.
At the course level the Software Enterprise defines a delivery structure integrating
established learning techniques around a project-based contextualized learning
experience. At the degree program level, the Enterprise weaves project experiences
throughout the BS SE degree program, integrating program outcomes at each year of the
major. There are several publications on the manner in which the Software Enterprise is
conducted within a project course (for example, [8][9]]), and we summarize this in-course
integration pedagogy in section 2. The intent of this work-in-progress paper is to describe
extending the Enterprise as a spiral curricular design feature we refer to as the project
spine, and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a
student’s competencies from understanding to comprehension to applied knowledge by
co-locating preparation, discussion, practice, reflection, and contextualized learning
activities in time. In this model, learners prepare for a module by doing readings,
tutorials, or research before a class meeting time. The class discusses the module’s
concepts, in a lecture or seminar-style setting. The students then practice with a tool or
technique that reinforces the concepts in the next class meeting. At this point students
reflect to internalize the concepts and elicit student expectations, or hypotheses, for the
utility of the concept. Then, students apply the concept in the context of a team-oriented,
scalable project, and finally reflect again to (in)validate their earlier hypotheses. These
activities take place in a single three-week sprint, resulting is a highly iterative
methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right)
The Software Enterprise represents an innovation derived from existing scholarship in
that it assembles best practices such as preparation, reflection, practice (labs), and
project-centered learning in a rapid integration model that accelerates applied learning.
Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle
[10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on
maturing the delivery process, creating new or packaging existing learning materials to fit
the delivery model, and to explore ways to assess project-centered learning.

300

3. The Software Enterprise Project Spine
An innovation in the new BS in Software
Engineering at ASU has been the vertical adoption of
the Software Enterprise. Enterprise courses are now
required from the sophomore to senior years. This
innovation represents what [6] calls a professional
spine, as the Enterprise serves as an integrator of
learning outcomes for a given year in the major. We
refer to our project-centered realization as a project
spine, where foundational concepts are tied to project
work throughout the undergraduate program. There is
significant
computing
literature
on
projects
(embedded, mobile, gaming, etc.) to achieve learning
or retention outcomes. However, computing lacks a
framework for integrating concepts in a project spine.
The Enterprise is an implementation that moves
students from basic comprehension to applied
Figure 2. ASU Project Spine
knowledge to critical analysis outcomes. In the BS SE
at ASU, program outcomes are described at 4 levels: describe, apply, select, and
internalize. Students must achieve level 3 (select between alternatives) in at least 1
outcome and achieve level 2 (apply) in all others. The program outcomes for the BS SE
include Design, Computing Practice, Critical Thinking, Professionalism, Perspective,
Problem Solving, Communication, and Technical Competence. An example leveled
outcome description for Perspective is given in Table 1. The Enterprise accelerates level
3 outcomes by providing contextualized integrated experiences fostering decision-making
in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes.
Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in
business, global, economic, environmental, and societal contexts.
Level 1. Understands technological change and development have both positive & negative effects.
Level 2. Identifies and evaluates the assumptions made by others in their description of the role and
impact of engineering and computing on the world.
Level 3. Selects from different scenarios for the future and appropriately adapts them to match current
technical, social, economic and political concerns.
Level 4. Has formed a constructive model for the future of our society, and makes life and
career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical
competencies by assigning projects inclusive of the technical material covered in the
regular computing courses. So for example, junior projects (Software Enterprise III and
IV) emphasize technical complexities in Networks, Distributed Computing, and
Databases, while senior projects emphasize technical complexities in Web and Mobile
computing. The technical “focus area” courses are chosen more based on faculty expertise
and recruitment goals than software engineering outcomes; one can envision many
different areas represented by upper division courses here. These do help address the
concern that an accredited software engineering degree has an application area. A risk we
have not yet addressed is if the technical area impacts the software engineering process,
such as with a soon-to-be-introduced embedded systems focus area.

301

There are 2 additional aspects of integration to the project spine. As summarized in
section 2, the Enterprise integrates software engineering concepts throughout the project
experiences. Students in the sophomore year learn the Personal Software Process [11] as a
means to build individual understanding of time management, defect management, and
estimation skills. They then focus on Quality, including but not limited to testing. In the
junior year Enterprise students focus on Design (human-centered and system design
principles) followed by best practices in software construction, taken primarily from
eXtreme Programming. In the senior year students focus on Requirements Engineering
then Process and Project Management. The final aspect of integration is with soft-skill
outcomes such as Communication, Teamwork, and Professionalism (see Table 1).
Throughout the spine the project experiences are crafted to ensure variations on pedagogy
to address these outcomes. For example, in the freshman year students receive explicit
instruction in teamwork. In the senior year the emphasis is on formal documentation as a
means of communication. In the junior year, students work on service learning projects of
high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of
program adoption. There are examples of program design and lessons learned [5][12][13],
or reflections and recommendations on the software engineering education landscape
[14][15][16][17][18]. These are worthwhile guides but do not offer examples on
evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on
“Program Implementation and Assessment” which discusses a number of key factors in
program adoption, but is geared toward accreditation and not evaluation instruments. A
survey instrument is presented in [19] but is designed for comparison of a large number
of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate
programs in software engineering but more as an aggregate counting exercise in
knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software
Engineering project conducted a survey of graduate degree programs [20] and then
produced a comparison report [21] of graduate programs to the GSwE2009 reference
model, which includes data on program characteristics and in-depth profiles from 3
institutions. A recent study is Conry’s [23] survey of accredited software engineering
degree programs. Conry summarizes institutional, administrative, and curricular
(knowledge area) aspects in describing the 19 accredited programs as of October 2009.
Certainly program adoption measures from other engineering programs are also relevant,
though software engineering programs are unique due to the forces discussed in section 1.
Our next steps for the Enterprise-as-project-spine involve defining measures for
adoption impact, and determining how this concept fits with established patterns for
curricular maps in software engineering programs. We plan to use quantitative and
qualitative instruments to evaluate adoption. Quantitative data, such as program size,
institution type, faculty and student backgrounds, can be collected via available resources
(departmental archives or online) and direct surveys. Qualitative data can be collected
through survey instruments and interviews of all stakeholders (faculty participants,
administrators, and advisors). Different instruments may be used at different times to
evaluate “in-stream” attitudes versus post-adoption reflections. Defining and validating
these instruments is a significant area of work going forward.
The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in
software engineering. Taxonomies are useful and the sign of an emerging discipline. We

302

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas,
and plan to elaborate on these mappings. Specifically, we intend to produce CS2013
course exemplars. Further, the SE2004 report includes a section on program curricular
patterns, and we will propose new patterns based on the project spine concept, which we
hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the
New Century. The National Academies Press, Washington D.C., 2005.
[2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of
Knowledge (SWEBOK). Los Alamitos, CA, 2004.
[3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society
Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at
http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013.
[4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society.
Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software
Engineering. Joint Task Force on Computing Curricula, 2004.
[5] Shepard, T. “An Efficient Set of Software Degree Programs for One Domain.” In Proceedings of the
International Conference on Software Engineering (ICSE) 2001.
[6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the
Future of the Field, Jossey-Bass, San Francisco, 2008.
[7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. “A Flexible
Curriculum for a Multi-disciplinary Undergraduate Engineering Degree.” Proceedings of the Frontiers in
Education Conference 2005.
[8] Gary, K. “The Software Enterprise: Practicing Best Practices in Software Engineering Education”, The
International Journal of Engineering Education Special Issue on Trends in Software Engineering Education,
Volume 24, Number 4, July 2008, pp. 705-716.
[9] Gary, K., “The Software Enterprise: Preparing Industry-ready Software Engineers” Software Engineering:
Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group
Publishing. October 2008.
[10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984.
[11] Humphrey, W.S. Introduction to the Personal Software Process, Addison-Wesley, Boston, 1997.
[12] Lutz, M. and Naveda, J.F. “The Road Less Traveled: A Baccalaureate Degree in Software Engineering.”
Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997.
[13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor’s Program.
IEEE Software November/December 2006.
[14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. “Guidance for the
development of software engineering education programs.” The Journal of Systems and Software,
49(1999):163-169. 1999.
[15] Ghezzi, C. and Mandrioli. “The Challenges of Software Engineering Education.” In Proceedings of the
International Conference on Software Engineering (ICSE) 2006.
[16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. “Improving software practice through
education: Challenges and future trends.” Proceedings of the Future of Software Engineering Conference, 2007.
[17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the
Future of Software Engineering, Limerick Ireland, 2000.
[18] Mead, N. (2009). Software Engineering Education: How far We’ve Come and How far We Have to Go.
Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009.
[19] Modesitt, K., Bagert, D.J., and Werth, L. “Academic Software Engineering: What is it and What Could it be?
Results of the First International Survey for SE Programs.” Proceedings of the International Conference on
Software Engineering (ICSE) 2001.
[20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering
Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008.
[21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master’s Programs in
Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013.
[22] Bagert, D.J. & Chenoweth, S.V. “Future Growth of Software Engineering Baccalaureate Programs in the United
States”, Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005.
[23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of
the American Society for Engineering Education, Louisville, KY, 2010.
[24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). “Revision of the SE2004
Curriculum Model.” Panel at the ACM Conference of the Special Interest Group on Computer Science
Education (SIGCSE), Denver, CO, 2013.

303

Portable User-Centric Identity Management
Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

Abstract User-centric identity management has recently received significant attention for handling private and critical identity attributes. The notable idea of usercentric identity management allows users to control their own digital identities. Current user-centric identity management approaches are mainly focused on interoperable architectures between existing identity management systems. Normally, users
can access the Internet from various places such as home, office, school or public
Internet café. We observe that the importance of portability of the a user’s digital
identity should be addressed in the user-centric identity management practices. In
other words, users should be able to export their digital identities and transfer them
to various computers in a secure manner. In this paper, we focus on the portability
issue of the Identity Metasystem and describe three possible types of portabilityenhanced Identity Metasystem model including our implementation experience.

1 Introduction
The Internet has dramatically changed the way people communicate and do business. Businesses heavily depend on the Internet to draw in commerce and make
information available on demand. Managing bank accounts, paying bills and purchasing goods via Internet are commonly exercised. The diverse Internet services
Gail-Joon Ahn
Arizona State University, Ira A. Fulton School of Engineering, Department of Computer Science
and Engineering, 699 S. Mill Avenue, Tempe, AZ 85281, e-mail: gahn@asu.edu
Moo Nam Ko
The University of North Carolina at Charlotte, 9201 University City Blvd., Charlotte, NC 28223,
e-mail: mnko@uncc.edu
Mohamed Shehab
The University of North Carolina at Charlotte, 9201 University City Blvd., Charlotte, NC 28223,
e-mail: mshehab@uncc.edu
Please use the following format when citing this chapter:
Ahn, G.-J., Ko, M.N. and Shehab, M., 2008, in IFIP International Federation for Information Processing, Volume 278; Proceedings of the
IFIP TC 11 23rd International Information Security Conference; Sushil Jajodia, Pierangela Samarati, Stelvio Cimato; (Boston: Springer),
pp. 573–587.

574

Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

and the tremendous amounts of personal data collected over the Internet have raised
various problems such as identity theft, fraud, and privacy breaches [22]. Numerous
identity management systems have been introduced to solve the identity management problems of business domains 1 . Different identity management systems have
their strengths and weaknesses and have been deployed in different contexts. Most
identity management systems were designed mainly from the business’s perspective. Users were not considered carefully in the design stage which led to serious
identity related problems. In addition, most identity management systems have focused on identity management issues in an isolated domain and federation issues
between identity management systems in the circle of trust.
The digital identity industry recognizes that identity management systems are
designed without the consideration of user experience and the non-interoperability
between current identity management systems which restricts the growth of ecommerce activities. As a result, user-centric identity management has recently received significant attention for handling private and critical identity attributes. The
main objective of user-centric identity management is to put the users in control
of their identity information. Users are allowed to select their credentials that are
used to respond to an authentication or attribute requester. Through the user-centric
identity management, the users have more rights and responsibilities for their identity information than before. In this paper, we articulate the portability issues of the
user-centric identity management system, attempting to enhance an existing Identity Metasystem. The paper is organized as follows. Section 2 overviews the digital
identity management and discusses the related technologies. Section 3 describes
our portability enhanced Identity Metasystem approaches. Section 4 describes implementation details followed by the related works in Section 5. Section 6 includes
the concluding remarks.

2 Digital Identity Management
In this section, we first start with the discussion of digital identity and digital identity management. We then discuss the user-centric identity management approach,
portability issues and the related technologies.

2.1 Digital Identity
There are various definitions of digital identity. Depending on organizations, systems and contexts, the diverse definitions of digital identity have been created and
used. From our perspective, we define a user’s digital identity as the global set of
attributes that make up an online representation of who and what an entity is. It can
1

Such identity management systems include IBM Tivoli [11], Liberty Alliance [18], LID [19],
OpenID [24], Sxip [31], Microsoft CardSpace [40] and Live ID [41]

Portable User-Centric Identity Management

575

Fig. 1 Digital Identity: Global Set of Attributes of a User

include access credentials, personal attributes and personal references. Over the Internet, a user has numerous access credentials that are issued from different sites and
different or duplicated personal attributes and references on each site. We believe
all of these attributes should be considered as the user’s digital identity as shown
in Figure 1. In each site, a user can be represented by subsets of these attributes.
Depending on the situation and the context, different subsets of attributes are used
to represent the same user in the Internet. For example, in an auction site, a subset
of a user’s attributes such as username, password, shopping history, and reputation
record represent the user’s identity in this site, while a subset of the user’s attributes
such as a student ID number, class record, and GPA may represent the user’s identity
in an university site.

2.2 Digital Identity Management
Digital identity management consists of several tasks such as maintaining user attributes and using subsets of attributes to enable secure online interactions between
users or between users and systems. Digital identity management enables the addition, utilization, and deletion of identity attributes. In [2], the identity management
systems are categorized into three models: isolated, centralized, and distributed
identity management. In the isolated identity management model, each site has
its own identity management domain and its own way of maintaining the identities of users including employees, customers, and partners. The centralized identity
management model has a single identity provider that brokers trust to other participating members or service providers in a circle of trust. The distributed identity
management model provides a frictionless identity management solution by forming a federation and making authentication a distributed task. Every member agrees
to trust user identities vouched for by other members of the federation. These identity management models were mostly focused on the domain centric approach. Our
analysis and observation indicate that most identity management systems neglect

576

Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

user-friendliness and usability issues. Therefore, it leads users to be the weakest
link in digital identity management systems.

2.3 User-Centric Identity Management and Portability
Under domain centric identity management systems, a user’s information is collected and managed by service providers so it is difficult for the user to manage
their identity information located at service providers and to monitor the usage of
the user’s private information. Putting the owner of the identity information into the
transaction gives the user-centric identity management approach the ability to solve
identity related problems. To achieve the goal, several requirements from the user’s
perspective need to be accommodated in the design of user-centric identity management systems. As the users have more rights on their own identity information, they
can decide what information they want to share, how much information to be disclosed with other trusted service providers, and under what circumstances. Thereby
better protection of the user’s private information is enabled by user.
Domain centric identity management systems focus on the user authentication
to protect their properties from malicious users. However, the authentication of service providers is equally important for a user to figure out the trustworthiness of
the service providers. Current browsers provide the padlock icon to give notice
to the users for the SSL communication between the users and service providers
but it is not enough for the users to figure out the trustworthiness of the service
providers [44]. By providing the identity information of service providers clearly to
the users in web-based interactions enables the users to distinguish trusted service
providers from malicious service providers. The users can then decide to disclose
their information to only trusted service providers. Hence, the users can protect their
information from phishing attacks and possible frauds.
In the current Internet environments, a user has to create a separate account for
each web site the user wishes to access. The user also has to maintain multiple separate accounts, which would be a tedious job. In addition, the users often choose insecure passwords, rarely change their passwords, and use the same password across
different accounts [1]. These trends make the password-based authentication systems insecure. New strong authentication methods are required to overcome the
security problems of the password-based authentication method. The new methods
should be easy for the users to manage their digital identities. Existing identity management systems provide different user experiences and interfaces that could lead
the users to improperly interact with different entities in Internet environments. Under the user-centric identity management systems, the users manage their identity
information directly through a proper interface which provides a consistent experience to control their identity information legitimately.
People carry identity cards such as a driver license card, a student ID card, and an
employee ID card in their wallet and they use each identity card in its appropriate
context. Similar to the identity cards in the real world, the digital identity should

Portable User-Centric Identity Management

577

be carried by the users and it should be used without the limitation of locations
and devices. Actually, people access the Internet from different sites such as home,
office, school, public Internet café, and so on. Therefore, the digital identity should
be both interoperable and portable.

2.4 Related Technologies
The Identity Metasystem is an interoperable architecture for digital identity management [6]. It is defined based on the “Laws of Identity” which are intended to codify a
set of fundamental principles to which any universally adopted, sustainable identity
architecture must conform [5]. The Identity Metasystem provides interoperability
between existing and future identity systems using Web Services (WS-*) protocols which is a set of specifications built on the web service platform. Specifically,
WS-Trust [38], an encapsulation protocol, is used for the claim transformation. WSMetadataExchange [35] and WS-SecurityPolicy [37] are used to conduct the format
and claim negotiations between participants. Finally, WS-Security [36] is used to
secure transmitted messages. The Identity Metasystem can transform the claims of
one type into the claims of another type and WS-* protocols negotiate the acceptable claim type between two parties to provide interoperability between them. The
Identity Metasystem also provides a consistent and straightforward user interface to
all the users. There are three roles within the identity metasystem: Identity Providers
who issue digital identities, Relying Parties who require identities, and Subjects who
are individuals and other entities about whom claims are made. To build an identity
metasystem, the system is required to follow five key components [22]:
1.
2.
3.
4.

A way to represent identities using claims.
A means for identity providers, relying parties, and subjects to negotiate.
An encapsulating protocols to obtain claims and requirements.
A means to bridge technology and organizational boundaries using claims transformation.
5. A consistent user experience across multiple contexts, technologies, and operators.
CardSpace [40], is an implementation of the Identity Metasystem, provides the
consistent user experience required by the Identity Metasystem. When a user needs
to authenticate to a relying party, CardSpace interprets the security policy of the
relying party and displays an Identity Selector containing a set of information cards
which satisfy the requested claims in the relying party’s security policy. Once the
user selects a card, CardSpace contacts the relevant identity provider and requests
a security token. The identity provider generates a signed and encrypted security
token which includes the required information and returns it to the Identity Selector.
The user then decides whether to release this information to the relying party. If the
user approves then the token is sent to the relying party where the token is processed
and the user is authenticated.

578

Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

Java Card is a Smart Card running a small Java based operating system. It is
useful in the areas of personal security and can be used to add authentication and
secure access to information systems that require a high level of security. The user
can carry around valuable and sensitive personal information such as medical history, credit card numbers and private key in the Java card. The Java Card technology enables Smart Cards and other devices with very limited memory to run small
applications (applets) and provides Smart Card manufactures with a secure and interoperable execution platform that can store and update multiple applications on a
single device [14]. Java-powered iButton is based on Java Card technology and provides the processing features which include a high-speed 1024 bit RSA encryption,
Non-Volatile RAM(NVRAM) capacity, and unalterable realtime clock [8]. It utilizes
NVRAM for program and data storage. Unlike electrically erasable programmable
read-only memory, the NVRAM iButton memory can be erased and rewritten as
often as necessary without wearing out. Therefore multiple applets can co-exist in
NVRAM and control the sensitive data in a secure way. It can be attached to accessories such as a key fob, watch, and finger ring so the users can easiliy carry the
iButton. We adopt this technology to demonstrate the feasibility of our approach.

3 PORTABILITY IN IDENTITY METASYSTEM
In this section we discuss the principles behind the Identity Metasystem and seek
methods to extend the Identity Metasystem for addressing portability aspects. We
focus on the information card and security token service modules in the Identity
Metasystem.

3.1 Information Card
The users of Identity Metasystem can manage their digital identities using visual
information cards in the Identity Selector. The information card draws a line between the self-issued card and the managed card. Both types of information cards
do not contain personally identifiable information (PII). The information card generally contains the card name, card image, a list of claims, and issuer information.
However, there are differences between the two types of information cards. In case
of the self-issued card, after the user provides the general user’s information such
as last name, first name, and e-mail address, the Identity Selector grants the user a
self-issued card. The self-issued card is stored in the local machine. Although the
self-issued card includes general PII, it is not supposed to include the sensitive user
information such as social security number, bank account and credit card number.
On the other hand, the managed cards are obtained from identity providers such as
employers, financial institutions, or the government. Like the self-issued information card, the managed card can be stored in local machines but the PII associated

Portable User-Centric Identity Management

579

Fig. 2 STSs and Information Cards in Identity Metasystem

with the card is not stored in the local machine. The PII is stored and managed
by each identity provider. The managed card enables the identity providers to issue their own set of claims. For example, credit card companies can design a set of
claims such as card name, card number and expiration date in their managed card
and the DMV can design a set of claims such as driver license number, license class
and expiration date in their managed card.

3.2 Security Token Service
When the digital identities are transmitted on the network, every digital identity is
presented by some sort of security tokens such as X.509 certificate [42], Kerberos
ticket [16], and SAML assertion [28]. The Identity Metasystem generates a security token by contacting the Security Token Service (STS) in the identity provider.
When the Identity Selector sends a “RequestSecurityToken” message to the identity
provider, the STS in identity provider responds back with a “RequestSecurityTokenResponse” message that contains a new security token. The current implementation
of Identity Metasystem has two STSs as illustrated in Figure 2. The STS located
at the third party identity provider generates security tokens for the managed cards,
whereas the STS in Identity Selector at the user’s local machine generates the security tokens for the self-issued cards.

3.3 Portability of Information Cards and STS
The CardSpace stores the information cards in a local machine and provides basic
import and export functions for information cards. Using these functions, the users
can export their information cards to portable storage devices such as portable USB
flash drive, mobile phone, and PDAs and import the information cards into other
machines. When the information cards are exported, the information cards are encrypted using a key derived from a user-selected pass-phrase [7]. Hence, if a user

580

Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

loses a portable storage device with the exported information cards, other people
cannot decrypt the exported information cards unless they know the pass-phrase of
the information cards. However, these export and import functions are not sufficient
to support the various practical scenarios. For example, a user carries the exported
information cards in a USB flash driver and imports the information cards in a kiosk
machine from the USB flash driver. After using the information cards in the kiosk
machine, if the user forgets to delete the imported information cards, then the next
user of the kiosk machine can access the previous users’ information cards without
any restrictions. The bottom line is to enable the users to carry the information cards
in a secure manner, considering the portability of STS as well. To achieve such an
intrinsic goal, we categorize the portability enhanced Identity Metasystem into three
models based on the location of the information cards and STS as follows:

(a) Simple Model

(b) Controlled Model

(c) Integrated Model
Fig. 3 Portability-enhanced User-centric Identity Management

• Simple Model: This model is similar to the general architecture of the Identity
Metasystem. Figure 3(a) shows the simple portability enhanced Identity Metasystem model. The STS is located in the identity provider and the users carry
their information cards using portable secure devices such as Java Card or Smart
Card. By storing the information cards in portable secure devices, only a user
who knows the PIN number of the secure device can access the information cards

Portable User-Centric Identity Management

581

and is able to export their information cards to multiple machines. When the
user removes the secure device from a machine, the imported information cards
should be removed from the machine automatically. This model can be applied
between different machines to synchronize the information cards.
• Controlled Model: This model shifts the role of identity provider to the portable
secure device. The user’s attributes and STS are located in a portable secure device and the information cards are located in a local machine. The user carries
the STS and attributes in portable secure devices so the Identity Selector does not
have to contact the identity provider to get a secure token. The Identity Selector
directly contacts the STS in the portable secure device and gets the security token.
The Figure 3(b) shows the controlled portability enhanced Identity Metasystem
model. This model can be applied to the one-time credit number system [4, 39],
where A credit card company issues a portable secure device with STS to the customers. The customers can treat the portable secure device with STS as a portable
identity provider. When a customer does an online purchase, the Identity Selector gets a secure token from the STS in the portable secure device directly. The
issued secure token includes the one-time credit card number so the user can protect the real credit card number. The drawback of this model is that information
cards are still in a local machine and a high expense is expected to distribute the
portable STS devices
• Integrated Model: This model is a combination of the Simple and Controlled
models. The users carry the information cards, STS and attributes in a portable
secure device, this enables them to directly manage their identity. When a user
plugs a portable secure device into a machine and provides the PIN number, the
identity selector imports and shows the information cards in a portable secure
device to the user. After the user selects a managed information card which requests a token from the STS in the portable secure device, the Identity Selector
directly gets the request token from the STS in the portable secure device. This
model combines the advantages of previous models so the user can carry their
information cards, portable STS and attributes in a secure device according to
the user’s purpose. This model gives the user more flexibility and extensibility
to manage his/her digital identities. Figure 3(c) shows the integrated portability
enhanced Identity Metasystem model.

4 Implementation Details
Based on our analysis of the Identity Metasystem and articulation of potabilityenhanced models, we developed a prototype of the Identity Selector, which is a
Java-based implementation of Identity Metasystem. Furthermore, we enhanced the
Identity Selector to support our portability enhanced Identity Metasystem models.
In this section we give an overview of our implementation experience and outcomes.

582

Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

4.1 Identity Selector
Identity Selector is an important component in Identity Metasystem. Using the visual information card, the users can select their identity cards with the same experience as the one in their real life. Figure 4 illustrates our Java-based prototype of
CardSpace-compatible Identity Selector. Each information card contains a subset of
the available user attributes that are used to represent the user’s identities in different contexts. Each card mainly includes meta information required to acquire the
real attributes from the identity provider. The meta information includes the necessary user attribute fields, identity provider contact information, and token related
information.
Our Identity Selector consists of seven components: Information Card Manager,
Graphical User Interface, Card Store, iButton/Smartcard Agent, Web Service Client,
Local STS/Token Issuer, and libraries as shown in Figure 4 (b). The Information
Card Manager handles all events generated by users and systems, and performs the
appropriate action. It also provides the card creation, editing, and deleting functions for the self-issued information card. The Graphical User Interface component
manages the user interface of Identity Selector. It consists of a set of screens such
as the creation of new card, the examination of cards and the selection of a card.
The Card Store contains information cards, which are usually stored in XML format. The Web-Service Client supports the communication between identity provider
and Identity Selector. The iButton/Smartcard agent manages the communication between the Identity Selector and the Java-powered iButton. It sends the PIN number
and token request message to iButton and receives the issued token from iButton.
The iButton/Smartcard agent and the Java-powered iButton exchange messages using the APDU (Application Protocol Data Unit). The Java-powered iButton includes
the Java Applet which provides STS module, user attribute storage, and information
card storage. The Java Applet is designed based on our integrated model. The Local
STS/Token Issuer generates CardSpace compatible security token for self-issued information card and also transforms the token issued from iButton to the CardSapce
compatible security token. Using openSAML 1.1 [25], Bouncy Castle API [32] and
our libraries, the local STS/Token Issuer encrypts and signs the XML token. The
libraries include the required standard and customized modules that are necessary
for supporting the functionalities of Identity Selector.

4.2 Portable Security Token Service
To generate a CardSpace-compatible security token in portable secure devices, the
Portable Security Token Service (PSTS) needs to support strong cryptographic algorithms. Moreover, portable secure devices should be able to generate SAML asser-

Portable User-Centric Identity Management

(a) Interface

583

(b) Components

Fig. 4 Java version of Identity Selector

tions. We identify three approaches to address how CardSpace-compatible security
tokens can be generated by Java Card technology 2 .
• Basic Mode: The PSTS in Java Card generates its own token and the local STS
in Identity Selector transforms the issued token into a CardSpace compatible
security token. The local STS signs and encrypts the token for the relying party.
This PSTS approach is only available for self-issued cards.
• Non-auditing Mode: The PSTS in Java Card generates a SAML assertion and
then the local STS in Identity Selector encrypts it for the relying party. This
is a “non-auditing” mode of Identity Metasystem [4], as the identity provider
has no knowledge of the relying party to protect the user’s privacy for Internet
activities. In other words, when Identity Selector receives a singed token from
Identity provider, PSTS can generates the SAML assertion by using a predefined
XML SAML assertion document and dynamically generated assertion data such
as digested value, signature values, and RSA public key value. Identity Selector
then encrypts the SAML assertion for the relying party. This approach can be
applied to both self-issued information cards and managed information cards.
• Auditing Mode: The PSTS in Java Card directly generates CardSpace compatible
security token for the relying party under the assumption that Java Card supports
the WS-Trust standard with strong cryptographic algorithms. When the PSTS
generates the security token, the PSTS knows the identity of relying party and
generates the security token for relying party directly. This is in “auditing” mode
of Identity Metasystem [4]. When PSTS receives “RequestSecurityToken” message from Identity Selector, the PSTS generates a security token for the relying
party and sends it to Identity Selector using “RequestSecurityTokenResponse”
message. This approach is similar to current .NET Smart Card approach and it is

2

.Net Smart Cards such as Gemalto Cryptoflex NET [9] and MXI security Stealth MXP [30] can
also provide cryptographic functions necessary to implement the PSTS.

584

Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

can be easily implemented when Java Card supports the WS-Trust standard with
strong cryptographic algorithms.

Fig. 5 System Flows and Corresponding Messages

Our prototype of the PSTS applet and iButton/ SmartCard agent is based on the
Basic Mode. Using a predefined protocol, iButton/ SmartCard agent requests a token
for self-issued card to PSTS applet. The PSTS applet is a PIN protected applet and
provides card storage, user attribute storage, and token generation service.
Figure 5 depicts the system flow diagram and corresponding messages in our
portability-enhanced user-centric identity management model. The process begins
when a user accesses a login page at a relying party’s web site. The site sends a
login form to the browser. The login form contains a specific OBJECT tag which
includes the site’s security policy and invokes the Identity Selector, which displays
the information cards that satisfy the relying party’s security policy. On the other
hand, when the user accesses a kiosk machine, the Identity Selector does not contain
any cards because the kiosk machine should not store the user’s information cards.
In that case, the user needs to select the iButton mode and insert a Java-Powered
iButton into the kiosk machine. The iButton agent in Identity Selector immediately
recognizes the iButton and asks for the PIN to reads the information cards from
iButton. Next, the user selects an information card and the Identity Selector sends
the token requests to iButton. The Identity Selector transforms the token issued by
iButton into a CardSpace compatible security token using the local STS module and
displays the attribute information. If the user consents to release the security token,
the Identity Selector presents the security token to the relying party. Finally, the
relying party verifies the security token as part of the authentication process. With

Portable User-Centric Identity Management

585

this scenario, we believe our prototype enable users to carry their digital identities
using portable secure devices.

5 Related Works and Discussion
There are several open source projects for user-centric identity management systems
or related technologies. To address the interoperability issue among those identity
management systems, the Open-Source Identity Systems (OSIS) working group was
formed [33]. The OSIS fosters several identity-related open-source projects such
as Bandit [3], Heraldry [12], Higgins [10], OpenSSO [26], OpenXRI [27], Shibboleth [29], and xmldap [43] and harmonizes the construction of an interoperable
identity layer for the Internet.
In [23], the authors pointed out the portability problem of client side storage of
user profile information. Once the user stores their information in a local machine,
it assures that the user has as much control over their information as possible. However, the personal information stored on a local machine is not portable. The authors
briefly suggested smart card or other portable devices to solve the portable problem
in client side storage of user information. Another approach is to use IDRepository [17], IDRepository approach is to separate user profile information from the
services, and store the identity in a central place where it can be maintained and accessed by appropriate entities. In [15], the authors allowed users to store identifiers
and credentials from different service providers in a personal authentication device
(PAD). The functionality of a PAD could be integrated into other approach.
In our work, the secure channel between smart card and smart card application
and the trust of client machine might be issues in using portable secure device on
various machines. Our approach assumes both secure channel and trustworthiness
are intact. If the communication channel between smart card and smart card application is not secure, the communication can be monitored by malicious software
on client machine. Markantonakis et al. [20] proposed a secure channel protocol between smart card and smart card application using the Diffie-Hellman protocol [34].
Using their approach we can further establish a secure channel between Identity
Selector and Java Card as needed. In case of CardSpace, it runs on Secure Desktop in .NET Framework 3.0 [21] for preventing any distrusted activities in a client
machine. To support this security feature, we would require trust computing technologies that can be either software or hardware-based solutions. These issues are
currently being explored as ongoing research tasks.

6 Conclusion and future work
In this paper, we have articulated three types of portable Identity Metasystem models and explored the applicable environments of each model. To demonstrate our

586

Gail-Joon Ahn, Moo Nam Ko and Mohamed Shehab

models, we have developed our own prototype of a CardSpace-compatible Identity Selector using the Java language and extended the portability using Java Card
technologies. We also proposed three possible approaches to generate CardSpace
compatible security tokens using the Java Card. We believe our implementation
demonstrated the feasibility of proposed portable user-centric identity management
models that effectively enable the users to carry information cards and user attributes
in a secure manner.
Our future work would include possible enhancements of our Identity Metasystem to support Web 2.0. Mashups and Social network service environments.In these
environments users can share their information attributes with other users more frequently and easily through creative and innovative Web 2.0 based applications. Also,
our work would include the development of metrics to characterize and measure
user-centricity in the digital identity management that eventually leads us to have the
common understanding of principles and practices. In addition, we strongly believe
that private and critical identity attributes exchanged in our portable user-centric
identity management models should be also protected based on the users’ preferences. Such privacy-preservation techniques will be studied as part of our future
works.

References
1. Adams, A. and Sasse, M. A. 1999. Users are not the enemy. Commun. ACM 42, 12 (Dec.
1999), 40-46. DOI= http://doi.acm.org/10.1145/322796.322806
2. Ahn, G. and Lam, J. 2005. Managing privacy preferences for federated identity management. In Proceedings of the 2005 Workshop on Digital Identity Management (Fairfax, VA, USA, November 11 - 11, 2005). DIM ’05. ACM, New York, NY, 28-36. DOI=
http://doi.acm.org/10.1145/1102486.1102492
3. Bandit-project.org Home. Available at http://www.bandit-project.org/
4. Cameron, K.: Kim Cameron’s Identity Weblog. Available at http://www.identityblog.com/
5. Cameron, K.: The Laws of Identity. Microsoft Corporation, White Paper, May 2005
6. Cameron, K. and Jones, M.: Design Rationale behind the Identity Metasystem Architecture.
Microsoft Corporation, White Paper, May 2005
7. Chappell, D.: Introducing InfoCard. Microsoft Corporation, Draft version for MIX, March
2006
8. Curry, S.:An introduction to the Java Ring, Java Wrold, April 1998
9. Gemalto Cryotoflex.NET. Available at http://www.cardsolutions.se/Cryptoflex.NET.pdf
10. Higgins Trust Framework Project Home. Available at http://www.eclipse.org/higgins/
11. Identity Management solutions from IBM Tivoli software. Available at http://www306.ibm.com/software/tivoli/solutions/identity-mgmt/
12. Incubation Status for Heraldry. Available at http://incubator.apache.org/projects/heraldry.html
13. Java Card Technology. Available at http://java.sun.com/products/javacard/index.jsp
14. Java Card Technology Overview. Available at http://java.sun.com/products/javacard/overview.html
15. Jsang, A. and Pope, S.: User Centric Identity Management. Proceedings of AusCERT, Gold
Coast, May 2005
16. Kerberos
Token
Profile
1.1.
Available
at
http://www.oasisopen.org/committees/download.php/16788/wss-v1.1-spec-os-KerberosTokenProfile.pdf
17. Koch, M.:Global Identity Management to Boost Personalization, 9th reserch sysmp. on
Emerging Electronic Markets, 137-148, 2002

Portable User-Centric Identity Management

587

18. Liberty Alliance Project. Available at http://www.projectliberty.org/
19. LID Wiki. Available at http://lid.netmesh.org
20. Markantonakis, K. and Mayes, K.: A Secure Channel Protocol for Multi-Application Smart
Card Based on Public Key Cryptography, IFIP CMS, 2004
21. Microsoft .NET Framework 3.0 Community (NetFx3). Available at http://www.netfx3.com/
22. Microsofts Vision for an Identity Metasystem. Microsoft Corporation, White Paper, May
2005
23. Mulligan, D. and Schwartz, A. 2000. Your place or mine?: privacy concerns and solutions for server and client-side storage of personal information. In Proceedings of the Tenth
Conference on Computers, Freedom and Privacy: Challenging the Assumptions (Toronto,
Ontario, Canada, April 04 - 07, 2000). CFP ’00. ACM, New York, NY, 81-84. DOI=
http://doi.acm.org/10.1145/332186.332255
24. OpenID: an actually distributed identity. Available at http://openid.net/
25. OpenSAML - an Open Source Security Assertion Language toolkit. Available at
http://www.opensaml.org/
26. OpenSSO Home, Available at https://opensso.dev.java.net/
27. OpenXRI.org Home. Available at http://openxri.org/
28. SAML
Token
Profile
1.1.
Available
at
http://www.oasisopen.org/committees/download.php/16768/wss-v1.1-spec-os-SAMLTokenProfile.pdf
29. Shibboleth Project- Internet2 Middleware. Available at http://shibboleth.internet2.edu/
30. Stealth MXP. Available at http://www.mxisecurity.com/docs/mxi stealth mxp.pdf
31. Sxip identity. Available at http://www.sxip.com/
32. The Legion of the Bouncy Castle. Available at http://www.bouncycastle.org/
33. OSIS: Open Source Identity Systems. Available at http://osis.idcommons.net/
34. Ueli M. Maurer, Stefan Wolf: The Diffie-Hellman Protocol. Des. Codes Cryptography
19(2/3): 147-171 (2000)
35. Web
Services
Metadata
Exchange(WS-MetadataExchange).
Available
at
http://specs.xmlsoap.org/ws/2004/09/mex/W S-MetadataExchange.pdf
36. Web Services Security: SOAP Message Security 1.0 (WS-Security 2004). Available at
http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0.pdf
37. Web Services
Security Policy Language(WS-SecurityPolicy).
Available at
http://specs.xmlsoap.org/ws/2005/07/securitypolicy/ws-securitypolicy.pdf
38. Web
Services
Trust
Language
(WS-Trust).
Available
at
http://specs.xmlsoap.org/ws/2005/02/trust/WS-Trust.pdf
39. What is ShopSafe. Available at http://www.bankofamerica.com/creditcards/index.cfm?template=faq
40. Windows CardSpace. Available at http://cardspace.netfx3.com/
41. Windows Live ID. Available at https://accountservices.passport.net/ppnetworkhome.srf?lc=1033
42. X.509
Token
Profile
1.1.
Available
at
http://www.oasisopen.org/committees/download.php/16785/wss-v1.1-spec-os-x509TokenProfile.pdf
43. xmldap.org - cardspace/infocard resources. Available at http://xmldap.org/
44. Ye, Z. and Smith, S. 2002. Trusted Paths for Browsers. In Proceedings of the 11th USENIX
Security Symposium (August 05 - 09, 2002). D. Boneh, Ed. USENIX Security Symposium.
USENIX Association, Berkeley, CA, 263-279

Int. J. Inf. Secur. (2013) 12:155–171
DOI 10.1007/s10207-012-0180-7

REGULAR CONTRIBUTION

Visualization-based policy analysis for SELinux:
framework and user study
Wenjuan Xu · Mohamed Shehab · Gail-Joon Ahn

Published online: 8 November 2012
© Springer-Verlag Berlin Heidelberg 2012

Abstract In this paper, we propose a visualization-based
policy analysis framework that enables system administrators
to query and visualize security policies and to easily identify the policy violations, especially focused on SELinux.
Furthermore, we propose a visual query language for
expressing policy queries in a visual form. Our framework
provides an intuitive cognitive sense about the policy, policy
queries and policy violations. We also describe our implementation of a visualization-based policy analysis tool that
supports the functionalities discussed in our framework. In
addition, we discuss our study on usability of our tool with
evaluation criteria and experimental results.
Keywords

Policy analysis · Visualization-based · SELinux

1 Introduction
In computing systems, security policies are specified to meet
security goals such as access to protected resources, information flow to and from protected resources, and resource
isolation and separation of duty. Policy administration is a
challenging task due to the complexity and interdependence
W. Xu
Frostburg State University, 101 Braddock Rd.,
Frostburg, MD 21532, USA
e-mail: wxu@frostburg.edu
M. Shehab
UNC Charlotte, 9201 University City Boulevard,
Charlotte, NC 22087, USA
e-mail: mshehab@uncc.edu
G.-J. Ahn (B)
Arizona State University, 660 South Mill Avenue,
Tempe, AZ 85287, USA
e-mail: gahn@asu.edu

of policy rules. This is further exacerbated by large policy
sizes, for example, the Security-Enhanced Linux (SELinux)
policy includes over 30,000 statements [18]. Access control
systems can become significantly ineffective if the implemented policies are not representative of targeted security goals. Simple policy misconfigurations might allow an
unprivileged process α to write to some resource that can
be read by a privileged process β, causing information flow
from α to β leading to an integrity violation. System administrators use policy analysis tools to locate and correct policy
violations. Several policy analysis frameworks have focused
on information flow models [7,11–14,27,28,36] to enable
policy verification and testing. Policy analysis frameworks
assume that the policy administrator is a security expert that
completely understands and interprets all the policy rules.
Furthermore, such policy analyses would help locate policy
violations.The output of policy analysis tools may list possible violations, which does not necessarily give the system
administrator a clear view on how the violation was originated and how it might propagate in the systems. Information visualization [8] enables users to explore, analyze,
reason, and explain abstract information by taking advantage
of their visual cognition. Several disciplines have adopted
information visualization mechanisms to better understand
and reason about the collected data. For example, visualization techniques have been adopted in bio-informatics,
networks, data mining, information retrieval, social networks, and several other areas. In the security arena, visualization has been used to understand and present data related
to network attacks [20,38–40], intrusion detection [5,10,22,
34], firewall policies [16,21,35], and trust negotiations [37].
In this paper, we propose a policy analysis framework that
is based on information visualization principles to simplify
policy analysis and to provide a better understanding to the
policy administrator.

123

156

A policy visualization framework should provide mechanisms to both display and query the policy base. Our framework models the security policy as a policy graph and adopts
both the semantic substrates [2,6] and adjacency matrix
[15,29] mechanisms to generate policy layouts for representing policy portions. In the semantic substrates mechanism,
the nodes and links expressing policy statements are arranged
based on semantic classifications, which provide a systematic approach to trace policy rules. The adjacency matrix
mechanism provides an intuitive approach to trace the read
and write relationships between subjects and objects. Providing simple and descriptive policy graph layouts enables
the policy administrator to easily examine and understand
the policy. Another novel module in our framework is the
visual query formulation that enables the administrator to
build queries against the policy base by simply dragging
and connecting provided query components. This mechanism follows an approach similar to the query by example
mechanism used for relational databases [24,25]. Using a
graphical query platform enables the average administrator to
easily probe a policy for identifying violations by specifying
graphical queries, without the need to write any script or learn
a new query language. Also, our policy visualization framework is realized as a policy visualization analysis (PVA)
tool and we attempt to visualize and query SELinux policies. In addition, we conduct a user study with two aspects:
comparison between PVA and an existing tool, and comparison between semantic substrates approach and adjacency
matrix approach. In this study, we examine the usability of
our framework with participants who browse and analyze
security policies with tools and two visualization methods
based on the designed instruction. The results are collected
and analyzed with paired samples t-tests [23] based on the
participants’ feedback.
The rest of the paper is organized as follows. Section 2
provides an overview of SELinux, trusted computing base
and information flow models. In Sect. 3, we introduce our
visualization-based policy analysis framework and policy
visualization approaches. The policy query classification and
query execution are presented in Sect. 4. Our policy visualization tool, PVA is presented in Sect. 5 followed by the
evaluation through user study in Sect. 6. Section 7 describes
the related work. Section 8 concludes the paper along with
the future work.

2 Preliminaries
2.1 SELinux overview
Security-Enhanced Linux [18] implements mandatory access
control (MAC)-based policies. The MAC mechanisms are
implemented through the Type Enforcement model, in which

123

W. Xu et al.

domains are used to label processes, and types are used to
label files and other resources. The policy rule set specifies
how domains can access different types. For example, a policy defines a domain passwd_t and assigns it to processes
running a specific set of executables used for a password. The
policy would also allow the passwd_t domain to operate
on resources with a type security_t. The operation is
identified by two pieces of information: a class (e.g., file,
directory, process, and socket) and a permission (e.g., read,
unlink, signal, and sendto). SELinux defines 28 classes and
120 permissions. For the sake of simplicity, SELinux uses
the notion of type to interchangeably describe both domain
and type. In addition to Type Enforcement, SELinux also
provides a role-based access control (RBAC) model [18]. A
user is assigned to a role which is an abstraction designed to
make policy rules more concise. Policy rules are introduced
to state the user to role assignments and the role to permission
assignments. The set of permissions associated with a role is
specified using types. Correspondingly, all the resources in
SELinux are labeled with a set composed of user, role and
domain or type. This kind of set is called security context. For
all object types, SELinux uses a role object_r and a user
system_u to specify their security contexts. A domain type
can be associated with different roles and users for different
security contexts. Figure 1 shows an example SELinux policy
showing the type, domain, and role declarations, a user jdoe
operating in the untrusted domain user_t, the domain type
allow rules, and the security context declarations.
2.1.1 SELinux type characteristics
The SELinux types are classified based on the functions performed by processes and the operations performed on the
different objects [30]. The domain and type classifications
are defined as follows:
– Domain Classification: According to the SELinux
policy configuration from NSA [30], domain types in
SELinux can be classified into system domains, user
program domains, and user login domains. System
domains are composed of domains labeled as system
processes (e.g., kernel_t, initrc_t, and init_t)
or daemons (e.g., sendmail_t and ftpd_t). User
program domains include unprivileged user program
domains (e.g., user_xserver_t), administrator program domains (e.g., sysadm_xserver_t), and some
other program domains (e.g., logrotate_t and
passwd_t). User login domains are the domains used
for user authorization such as user_t, sysadm_t, and
staff_t. Due to the large number of vulnerabilities that
have been found in daemons (e.g.,sendmail_t), we
divide system domains into daemons and general system
domains.

Visualization-based policy analysis for SELinux

(a)

157

(b)

Fig. 1 SELinux example policy and classifications

– Type Classification: Types in SELinux can be classified
into security types (e.g., security_t), device types
(e.g., fixed_disk_device_t and device_t),
file types (e.g., etc_t), procfs types (e.g.,
sysctl_kernel_t and proc_t), devpts types (e.g.,
ptmx_t), nfs types (e.g., nfs_t), and network types
(e.g., icmp_socket_t and port_t). The details
of domain and type classifications are summarized
in Fig. 1.
2.1.2 SELinux policy security goals
Loscocco et al. [19] outlined six critical security goals to
be achieved by SELinux security policies. These goals are
summarized as follows: (G1) Limiting raw access to data,
(G2) Protecting kernel integrity, (G3) Protecting system file
integrity, (G4) Confining privileged process, (G5) Separating processes, and (G6) Protecting the administrator domain.
Goals G2, G3, and G6 are focused on integrity protection of
resources that include the boot files, proc files, and security
policy-related objects. Goal G1 protects both the integrity
and confidentiality of the system device resources, for example, the write operation to the fixed disk devices is restricted
to the fsck labeled programs for checking file system consistency. Goals G4 and G5 target the implementation of the
principle of the least privilege by restricting access to certain domains [26]. For example, a mail server process should
only access certain resources such as the mail spool file.
These goals are implemented in SELinux policies by limiting
access with the allow/deny rules targeting specific domains
and types. Goal-related rules can be identified by checking

the allow/deny rules and the affected resources. For example,
the policies related to G1, G2, and G3 can be identified by
locating rules affecting raw data, kernel files, and systems
files, respectively. Later, we use the classification of goalrelated policies to analyze the security policies against these
security goals and locate security violations.

2.2 Trusted computing base (TCB)
The early understanding of trust perceived that hardware
and software, which need to be trusted, should be generally
equated to operating systems and the supporting hardware.
Then, the concept of the reference monitor was introduced
in system architectures to validate all access requests by programs against information security policies [1]. This consequently led to the introduction of the Trusted Computing
Base, which is defined as part of a system that is responsible
for enforcing security policies of the system [33]. The Trusted
Computing Base not only includes the reference validation
mechanism, but also encompasses all other functionalities
that directly or indirectly affect the correct operation of the
reference validation mechanism. Using a operating system
as an example, the Trusted Computing Base of the system
includes the object management and access control functions.
The object management function is responsible for creating
objects and processing requests. The access control function
contains both rules and security attributes that support the
access control decision-making process. The Trusted Computing Base partitions the hardware and software into two
parts: the part inside the Trusted Computing Base is referred

123

158

to as trusted (TCB) and the part outside the Trusting Computing Base is referred to as untrusted (N-TCB).
2.3 Information flow model
In an operating system, the operations between subjects and
objects can be classified as write_like or r ead_like [7]
and the operations between subjects can be expressed as
calls. If a subject s1 can write to an object o (write(s1 , o)),
which can be read by another subject s2 (r ead(o, s2 )), we
say there is a flow transition from a subject s1 to a subject s2
( f lowtrans(s1 , s2 )). The subject to subject calling relationship is considered as a flow transition from a subject s1 to a
subject s2 if s1 can call s2 .
Definition 1 The Flow Transition. f lowtrans(si , s j ) specifies that information flows from a subject si to a subject s j .
We say there is a flow transition from a subject si to a subject
s j if: (∃o ∈ O : write(si , o) ∧ r ead(s j , o)) ∨ call(s1 , s2 ).
The flow transition describes the direct information flow
between subjects. Suppose there is a sequence of flow transitions f lowtrans(si−1 , si ) for subjects i = 1, . . . , n, then
without loss of generality there is an information flow path
from a subject s0 to a subject sn .
Definition 2 The Information Flow Path. f lowpath(s0 , sn )
specifies a sequence of flow transitions from a subject si to a
subject s j . Assume there is a flow transition f lowtrans(si−1 ,
s
i ) for i = 1, . . . , n then f lowpath(s0 , sn ) is represented as:
n
i=1 f lowtrans(si−1 , si ).
Traditional models describing information flow related
to integrity and confidentiality include Lattice [4], BellLaPadula [32], and Biba [3] models. The Biba model
is related to integrity, the Bell-LaPadula model is concerned with confidentiality and the Lattice-based approach
is the combination of Biba and Bell-LaPadula models. The
integrity property in Biba model is fulfilled if a high integrity
process cannot read lower-integrity data, execute lowerintegrity programs, nor otherwise obtain lower-integrity data
in any other manner. In SELinux policy analysis, we later
adopt Biba or Bell-LaPadula models in checking information flow paths and finding possible policy violations against
security goals.

3 Framework overview: visualization-based policy
analysis
In this section, we present our framework for enabling policy
visualization with the emphasis on SELinux policies. Our
framework consists of the following major modules:

123

W. Xu et al.

– Policy Files: The policy files include the security
policy, role permission mappings, TCB and N-TCB
definitions and the goal-related rule labeling. These provide information related to policy statements, mappings
of the operations between the subjects and objects, the
initial TCB/N-TCB classification, and types targeted by
the different security goals (G1 to G6).
– Policy Parser: This module involves the parsing of policies and the mapping of policies into goals and TCB definitions. This information is used to compile the policy
graph, which is discussed in the subsequent section.
– User Input: This module is composed of the overview
module which provides a general view of the policy
graph, the content view module which is used for viewing
the policy statements, the detailed view module which is
used for exploring detailed portions of the policy graph,
and the policy analysis module which provides interactive interfaces used for analyzing and finding the policy
violations.
– Query: This module enables the user to specify, translate
and execute queries against the policy graph. The query
writer helps the user specify the query. The query is then
translated into path queries on the policy graph by the
query translator and finally the query executor applies
path finding algorithms on the policy graph to execute
the query.
– Policy Visualization: This module provides the visualization capabilities. It provides several graph visualization layouts for the query computed on policy graphs such
as the semantic substrates and the adjacency matrix. It
also enables the user to perform several operations on the
visual layouts such as zoom, pan, annotation, rearrangement and clockwise.

3.1 Policy visualization
In our framework we use information visualization techniques to visualize the policy so that the system administrator
is empowered to better understand the configured policy. In
this section, first we define the policy graph, then we present
our proposed semantic substrates and adjacency matrix
policy visualization techniques. A policy graph is defined
as:
Definition 3 A Policy Graph is a directed categorized graph
G = (V, E), where the set of vertices V and the set of
edges E represent the types of entities and the flow transitions
between them respectively.
– V = Vo ∪ Vs ∪ Vr ∪ Vu is the set of nodes representing
different entities. Vo , Vs , Vr , and Vu are the set of nodes
that represent objects, subjects, roles, and users, respec-

Visualization-based policy analysis for SELinux

(a)

159

(b)

Fig. 2 Semantic substrate template and example

tively. The objects are the assigned types and the subjects
are the assigned domains.
– E = Er ∪ E w ∪ E c is the set of edges describing information flow between the different vertices. Given subject
vertices vsi , vsk ∈ Vs and object vertex Vobj ∈ Vo :
– (Vsi , Vobj ) ∈ E w if there is a write(si , obj).
– (Vobj , Vsk ) ∈ Er if there is a r ead(sk , obj).
– (Vsi , Vsk ) ∈ E c if there is a call(si , sk ).
Two information visualization techniques are related to
visualization work: Semantic Substrates and Adjacency
Matrix. Semantic Substrates [2] is a visualization method
that generates graph layouts that are based on user-defined
semantic substrates, which are non-overlapping regions in
which node placement is based on node attributes. Also,
users interactively control link visibility to limit clutter and
thus ensure comprehensibility of source and destination. Of
course semantic substrates are effective only if there exist
some categorical attributes or if a numerical attribute can be
used to form categories. Although there are limitations in
the implementation, the utility of semantic substrates apparently copes with a large number of nodes and links. Also, as
the node-link-based diagram, the semantic substrates method
shows strong advantages in small graphs. However, in many
situations, the graph may be very big and dense. Adjacency
matrix [15] is widely used in graph visualization because it
can effectively display a big and dense graph by interpreting
the structural information embedded in a matrix view of a
graph. Although adjacency matrices can be used to visualize
both directed and undirected graphs, it is argued that finding the path from one node to another node in the directed
graph might not be an essential ability. In the following, we

will explain how to use these techniques to visualize security
policies.
3.1.1 Semantic substrates
Several visualization studies concluded [2,6] that humans
perceive data coded in spatial dimensions far more easily than those coded in non-spatial ones. Building on these
results, we propose the use of semantic substrates based on
node attributes to layout nodes in non-overlapping screen
regions. We also make use of non-spatial cues, such as
color or shape to emphasize certain nodes or a group of
nodes. An SELinux policy graph consists of mainly four
node categories, namely User, Role, Domain and Type. Furthermore, domains and types can be further classified, for
example administration domain and user program domain.
Based on this semantic classification of nodes, the policy
graph can be displayed spatially by distributing nodes into
non-overlapping regions. Figure 2 shows the semantic substrate template and examples. The Y -axis is divided into
regions, where each region contains nodes representing a
certain entity. Furthermore, in each region, nodes representing entities with different classifications are placed in different districts on the X -axis. Different colors and shapes
are used to aid the identification of nodes, for example,
black circles, red circles and black squares are used to represent trusted domains, untrusted domains and protected types,
respectively. Based on the policy graph definition, we distinguish the transitions between nodes by assigning different colors to the different transition classes. For example,
the user to role assignment is represented by a red arc,
and similarly the role to domain, domain to type and type

123

160

W. Xu et al.

(a)

(b)

Fig. 3 Adjacency matrix template and example

to domain are denoted with other colors. One advantage of
semantic substrates is that the administrator can easily visualize links that cross from one category (region) to another
region [2].
3.1.2 Adjacency matrix
The semantic substrates is a very good choice for finding a
path, given that the links are not heavily crossed or tangled.
For visualizing a path in a dense policy graph we propose
to use an adjacency matrix approach which is more compact and free of visual clutter [15,29]. We further enhance
the path visualization capabilities of the adjacency matrices approach by adding direction characteristics. We also
develop a direction-based approach that enables the administrator to intuitively trace the visualized paths.
Figure 3a shows our proposed adjacency matrix visualization template. The nodes are arranged on both the X -axis and
the Y -axis. To visualize a path P = {v0 , v1 , . . . , vn } in the
adjacency matrix, we highlight entries (vi , vi ) and (vi , vi+1 ),
for i = 0, . . . , n − 1. We draw an arc from entries (vi , vi )
and (vi , vi+1 ) for i = 0, . . . , n − 1, and we draw an arc from
entries (vi−1 , vi ) and (vi , vi+1 ) for i = 0, . . . , n − 1. Figure 3b shows the visualization of path, P = {d2 , t2 , d4 , t1 }.
The series of arcs carry all information of the original path.
In our template the types and domains are arranged on both
the X -axis and the Y -axis. Furthermore, the grid is divided
into four quadrants:
– Quadrant 1: This is the write quadrant, a slot (di , t j )
signifies that a domain di can write to a type t j .
– Quadrant 2: Slot (di , d j ) signifies that a domain di can
call a domain d j .
– Quadrant 3: This is the read quadrant. A slot (ti , d j )
signifies that a type ti can read by a domain d j .

123

– Quadrant 4: Slot (ti , t j ) is used to enable transition.
For example, a path P = {d2 , t2 , d4 , t1 } represents information flow write(d2 , t2 ), r ead(d4 , t2 ) and write(d4 , t1 ). In
our proposed adjacency matrix template this requires the path
to visit the write quadrant then the read quadrant. Therefore,
information flow paths will always follow a clockwise direction. Using this property, an administrator can easily find the
directed path information by scanning the adjacency matrix
template. Furthermore, we use different colors to represent
trusted, non-trusted and goal protected entities in the adjacency matrix.

4 Security policy querying
Users may have difficulties in writing or formulating a
query [31]. The idea of the visual query formulation is to
help system administrators specify precise queries on the
policy base using an interactive visual querying technique.
Using an approach similar to the Query by Example (QBE)
for querying relational data [24,25], our approach provides
a user interface and a policy graph that enables the administrator to create and run queries against the policy base. The
queries are generated by connecting our proposed query operators to formulate the intended information flows. The query
classification and operators are designed to provide functionalities adopted from the previous policy analysis mechanisms [36,27]. In general, there are two classes of queries:
Q1. Identify policy integrity violations based on information
flow against security goals.
Q2. Identify other policy violations like separation of duty
and incompleteness.

Visualization-based policy analysis for SELinux

(a)

(d)

161

(b)

(c)

(e)

(f)

Fig. 4 Example query results

4.1 Query classification
Integrity checking is based on performing reachability
analysis on the policy graph. For example, PAL [27] focused
on finding information flow paths from N-TCB to TCB.
In addition to the TCB and N-TCB classification, our
framework provides a goal-related policy classification,
which enables us to query information flow paths affecting
resources protected by certain goals. Also, we provide a set
of basic query classes that are supported by our framework.
A node represents a user, role, type or domain, and a group
represents a set of nodes. Groups include TCB, N-TCB, goalrelated nodes, and user-defined groups.

C1. Node to Node information flow paths. This enables the
querying for information flow from a specific domain
to a specific type. Figure 4a shows the query result in
the form of the information flow path from a domain
mount_t to a type fixed_disk_device_t.
C2. Group to Groups information flow paths. This enables
the querying for information flow from N-TCB to TCB,
or from an N-TCB to a set of goal-related domains or
types. Figure 4b shows the query result from N-TCB to
TCB.
C3. Node (Group) to Group (Node) information flow paths.
This enables the querying for information flow from one
domain to the goal protected types, or from N-TCB to
a certain domain. Figure 4c shows the result of finding
information flow paths from all N-TCB to the mount_t
domain.

C4. Node to Node information flow paths through another
Node. It helps finding information flow from one type to
another type through a certain type, where types can be
domains or types. Figure 4d shows the result of finding
information flow path from a domain user_games_t to
a type fixed_disk_device_t through a type devtty_t.
C5. Reachability. It enables to find all possible information flows from or to a certain type. For example, it
finds all information flows to fixed_disk_device_t, or
the information flows from user_t. Figure 4e shows
the result of finding the information flow paths to
fixed_disk_device_t.
C6. Separation of Duty (SoD). This helps check constraints
on authorizations to types. For example, in the context
of SELinux, separation of duty can be interpreted as
separation of domains allowed to modify (e.g., write
or create) executable files from the domains allowed to
execute those executables. In PAL [27], these queries
are restricted to direct access. In the SELinux example
policies, we introduce policies that enable the rootkit_t
domain to have write access on sendmail_exec_t type
and transition operation on sendmail_t. By querying the
policy graph we are able to locate this SoD violation as
depicted in Fig. 4f.

4.2 Basic query formulation
Our framework provides an interactive drag and drop query
platform that enables the administrators to issue information
flow queries by simply connecting the provided components

123

162

W. Xu et al.

Figure 5e shows composed queries that specify how to query
policy graph node relationships such as have and flow path.
4.2.1 Join query construction
The policy administrator can use a join query to construct
more complex queries such as finding the domain that can
both write and read the goal protected objects. Also, using our
join query the policy administrator can accumulate several
query results on a single graph. Based on the different ways
of sharing E-Nodes, we define three basic joins: Simple Join,
Merge Join and Tuple-sharing Join. More complex join can
also be constructed by combining the three basic joins.

Fig. 5 Query construction

compared to current policy analysis frameworks [36,27]
which are based on scripting. Figure 5 summarizes the basic
visual components.
– Element Nodes (E-Nodes) are shaped as labeled
circles; their label represents the attributes of the element.
For example, using SELinux policy as an example, the
element nodes include USER, ROLE, DOMAIN, TYPE,
TCB, NON-TCB and Goal. The character # is used to
help the attribute specification. For example, Goal# can
be customized to be G1, G2, etc.
– Operator Edge (O-Edge) is represented as the curve that
connects the element nodes to another element nodes.
The label of the operator edges represents the query classification of the query. Based on the query classification,
the operator edges include write, read, call, have, indirect
have, indirect flow to, SOD and indirect SOD.
– Element Nodes Annotation (EN-Annotation) is to specify
the element nodes value. It can be a single value or a
set. When the policy administrator draws the query, this
value can be partially specified as the wildcards “?” and
“*” denote any character and any sequence of characters,
respectively.
– Operator Edges Annotation (OE-Annotation) is to specify required path properties. For example, to query the
information flow path from one node to another node,
we can specify a query to find the shortest path, all the
paths, any path or the paths that can be found in the time
limitation. The value “*” denotes all paths.

123

– Simple Join specifies that a set of E-Nodes is sequentially
connected through the O-Edges; given E-Nodes { n i , n j ,
n k } and O-Edges {oi , o j }, if oi (n i , n j ) and o j (n j , n k ),
then we say there is a simple join. An example join query
is shown in Fig. 5f.
– Tuple-sharing Join specifies that two or more E-Nodes
are connected out from the same E-Node through the
O-Edges; given E-Nodes {n i , n j , n k } and O-Edges {oi ,
o j }, if o j (n i , n k ) and ok (n i , n k ), then we say there is a
tuple-sharing join.
– Merge Join specifies that two or more E-Nodes are sortmerge into one E-Node through the O-Edges; given
E-Nodes {n i , n j , n k } and O-Edges {oi , o j }, if oi (n i ,
n k ) and o j (n j , n k ), then we say there is a merge join.
4.3 Query execution
Based on the definitions of the join query construction, the
identification of the different join format can facilitate the
query execution. The paths computed during the query executions are based on the OE-Annotations associated with
operator edges which include the shortest path, any path or
the paths found given a execution time limit. Query execution makes use of the shared nodes between group nodes. For
example, in the tuple-sharing join (shown in Fig. 5), suppose
A is NTCB, B is TCB, C is fsadm_t and the O-Edges having
same annotation, since fsadm_t belongs to TCB, the query
only needs to be executed from A to B. Similarly, in the merge
join, if D is NTCB and E is a subset of NTCB (e.g., xdm_t) or
shares labels with the NTCB, the query will evaluate paths
from D to F then the paths from E − (E ∩ D) to F.
Referring to the algorithm in Fig. 6, the policy query
execution algorithm is mainly composed of two parts. In
the first part, the algorithm identifies all the E-Nodes from
the query graph using the function get Element N odes(G q ),
then for each E-Node n a , it finds all the outgoing O-Edges
from node n a using getConnect Edges(n a , G q ). In the second part, for each of the edges identified e in the previ-

Visualization-based policy analysis for SELinux

163

5.1 Policy visualization analysis tool (PVA)

Fig. 6 Query execution algorithm

ous step, the algorithm identifies the nodes connected to
the edge identified by n b which are retrieved by the function f indConnect N ode(e, n a , G q ). Since merge query
and tuple-sharing are built based on the shared E-nodes,
there will be duplicated nodes retrieved from the function
f indConnect N ode(e, n a , G q ). If n a and n b are part of the
merge query, the duplicated nodes are removed from n a by
using the expression n a − n b .get MergeN odes(n b ,e), the
merge join nodes information are stored in the n b attribute
and can be retrieved using n b .get MergeN odes(n b ,e). On
the other hand, if n a and n b are part of tuple-sharing,
the duplication of n b using n b − n a .get T upleN ode(n a ,e),
where the information of tuple-sharing nodes is maintained in the n a attribute and can be retrieved using
n a .get T upleN ode(n a ,e). After the information duplication
is removed, the query from n a to n b with an operator e is executed. Finally, the executed queries are leveraged by adding
the nodes and edge information into n a and n b respectively
using n a .addT upleN ode(n b ) and n b .add MergeN ode(n a ).

5 Case study with SELinux policies
In this section we discuss the implementation details of our
proposed framework, we give design snapshots of our PVA
tool and we discuss how the tool is used to identify policy
violations in SELinux policies.

The PVA tool is presented to the user via a self explanatory graphical user interface. To enhance the cognition and
understanding of the policy information, we provide implementations of both the semantic substrates- and adjacency
matrix-based visualization layouts. Another important aspect
of our design is to be expressive and directly mapped to the
real system policy analysis. By providing a visualizationbased policy query platform, our design enables the administrator to build a query by example.
Our implementation is based on the Java JDK1.6 and supporting libraries. The graph drawing modules were based on
our extensions to the open source package Piccollo [9]. Our
parsing tool is based on the policy structure adopted by the
APOL [36] tool. In this case study the SELinux policy binary
file policy.19 was used. Figure 7a shows a snapshot of
our tool. The policy administrator can import, analyze, query
and modify the policy through the menu. The left window
is composed of two parts: semantic substrates-based visualization and adjacency matrix-based visualization, and each
window includes the tabs for view, analysis, and violation.
The view tab provides the GUI for the policy graph overview,
content view, and detail view, e.g., viewing the whole
policy graph through zoom in, zoom out, etc. The analysis
tab supports the analysis of the policy by enabling the administrator to select the security goals of interest and ultimately
locate policy violations with the help of the query function.
The violation tab displays all the policy statements that are
involved in a security violations. Furthermore, in this tab the
policy administrator can directly modify the policy by using
the text editor or directly editing the policy graph. In the main
window, the policy graph, query results, goal-related policy
graphs and the policy violation graph are displayed.
5.2 Policy graph
The main window in Fig. 7a shows the visualized SELinux
policy based on semantic substrate design proposed in
Sect. 3.1. The policy is composed of 308 domains, 1092
types and 31604 links. The Y -axis is divided into four regions
including USERS, ROLES, DOMAINS and TYPES. The
X -axis is labeled using the domain and type classifications
discussed in Sect. 3.1. The domain regions are divided into
four different areas SD (System Domain), DAE (Daemons
Domain), PRO (Program Domain) and ULO (User Login
Domain). The type regions are divided into seven different areas ST (security types), DT (device types), FT (file
types), PT (procfs types), DE (devpts types), NF (nfs types),
and NE (network types). To help the policy administrator
to easily identify the different regions, the elements in nonneighboring regions are represented as different shapes, for
example users and domains are expressed with circle, and

123

164

W. Xu et al.

(a)

(b)

(c)

(d)

Fig. 7 PVA tool introduction

roles and types are expressed with rectangle. The edges
between different regions are represented by different colored lines, for instance the write operation between a domain
and a type is represented by red edges and the read operations
by green edges. Also, policy administrator can view node
attributes by clicking on the specific nodes. Figure 7b shows
the adjacency matrix-based policy visualization method,
which was compiled by selecting a subset of the nodes in
the semantic substrates overlay.
5.3 Policy query and violation detection
Figure 7c shows the graphical query interface and a query
designed to discover the paths from N-TCB to resources
related to the goal G1 (limiting raw access to data) such
as fixed_disk_device_t through a specific type
devtty_t and TCB resources. Starting from left to right
(Fig. 7c), the first node selects the N-TCB resources and

123

finds the paths to a type devtty_t, then finds the paths
from devtty_t to the TCB resources. Finally, the query
builds the paths from the TCB resources to the goal G1
fixed_disk_device_t device. Figure 7d shows the
identified policy violations by this query. Note that the
display divides the TCB and N-TCB to provide a better understanding to the system administrator. Running the
visualization tool on a 1.4 GHz Intel Pentium CPU with
512 MB of memory, query loading and parsing take 15 s
and query execution and display take 21 s. Another example query, which investigates information flow paths from
N-TCB to fsadm_t (TCB) without the constraint of passing through a specific intermediate node, was executed and
displayed in 88 s due to the large number of policy violations. However, if we only query the information flow
path from user_t (N-TCB) to fsadm_t (TCB), it takes
about 0.5 s. Hence, the time cost depends on the size of the
query set and query results. Table 1 shows identified policy

Visualization-based policy analysis for SELinux
Table 1 Policy violation
examples

165

Subjects

Type : Class

Example policy violations
200

Subject

Resolution

network

fsadm_t

Filter

rhgb_t

mnt_t:dir

fsadm_t

Modify

smpmount_t

mnt_t:dir

fsadm_t

Modify

hotplug_t

etc_runtime_t:file

fsadm_t

Ignore

33

unpriv_userdomain:fd use

fsadm_t

Modify

134

initrc_t:fifo_file

fsadm_t

Modify

16

removable_device_t:chr_file

fsadm_t

Modify

3

scsi_generic_device_t:chr_file

fsadm_t

Modify

200

devlog_t:sock_file

fsadm_t

Ignore

violations caused by information flow from N-TCB to TCB
fsadm_t.

6 User study
This section describes our user study that was conducted
to evaluate the usability of PVA. Participants in our study
browse and analyze SELinux security policies using PVA
and APOL [36] that is a GUI-based tool developed by
Tresys Technology to analyze SELinux policies. Currently,
the APOL analysis tool is issued together with other Linux
packages such as Fedora Core. APOL helps browse and
search policy components (e.g., types, attributes, object
classes, roles, users, and booleans), searching type enforcement and other rules, and viewing file contexts from a
filesystem. In addition, APOL allows policy administrators to perform automated, complex analysis of a policy, which include domain transition, file relabel, types
relationship, and information flow analysis. In our evaluation work, we mainly focus on comparison between
PVA and APOL. Also, participants are required to compare semantic substrates method and adjacency matrix
method in displaying of SELinux security policies and policy
violations.

students (12.1 %) with different specialities within computer
science discipline (69.7 % information security; 6.1 % computer networking; 6.1 % database systems; 6.1 % computer
graphics and visualization; and 12.1 % other) of various ages
[12.1 % were 18–22 years old (y. o.), 42.4 % were 22–26
y. o., 30.3 % were 26–30 y. o., and 15.2 % were 30–40 y.
o.]. Participants differed in terms of their most frequently
used OS (15.2 % Linux; 72.7 % Windows; and 12.1 % Mac
OS), the frequency with which they change OS configuration settings (27.3 % never; 54.5 % monthly; 12.1 % weekly;
and 6.1 % daily), the frequency with which they configure
or check their OS security policies (30.3 % never; 54.5 %
monthly; 6.1 % weekly; and 9.1 % daily), whether or not they
use special software tools to manage their OS security policies (21.2 % use special software tools; 63.6 % do not use
special software tools; and 15.2 % do not know whether or
not they use special software tools), the extent to which they
agree that configuring security policies is an important task
(3 % strongly disagree; 0 % disagree; 0 % neither agree not
disagree; 45.5 % agree; and 51.5 % strongly agree), and the
amount of time they would be willing to spend on configuring
a security policy (6.1 % no time; 33.3 % up to 15 min; 30.3 %
up to 30 min; 6.1 % up to 1 h; 3 % up to 2 h; and 21.2 % more
than 2 h).
6.2 User study preparation

6.1 Participant enrollment and general feedback
To enroll the participants, we first interviewed some student
participants after giving guest lectures. Other participants
were recruited from several research laboratories and the system administrator office of the college. In addition, we contacted some participants who were recommended as either
Linux expert or system administrators and had a total of 60
enrolled users.
The sample for our study consists of system administrators (15.2 %), graduate students (72.7 %), and undergraduate

The participants are asked to analyze the same security policies with APOL and PVA. The policy we chose is a real
SELinux policy from a major publisher. We chose this policy for the following reasons:
– This real policy set is representative of security policies
that the policy administrator might encounter in practice.
– The size of policy set is about 200 KB that covers core
policies and sufficiently helps measure usability of tools.
Although there might be a larger SELinux policy set

123

166

W. Xu et al.

Fig. 8 APOL tool introduction

available, analyzing such policies will bring difficulty for
participants in both evaluating APOL and PVA. On the
other hand, in case the security policy size is too small, the
benefit of visualization-based approach will be probably
downgraded.

First, in order to give ideas about how to use APOL and
PVA and how to compare the two visualization mechanisms
in PVA, we introduced SELinux policy overview to the participants and an instruction was provided with screenshots as
shown in Figs. 7 and 8.
The first part of the instruction explains how to use APOL
and PVA for browsing and analyzing security policies. The
participants are instructed to load the security policy set and
browse any contents they are interested in. Then, the participants are required to follow the instructions for completing the following tasks: (1) identifying the information types
containing gcon; (2) identifying all the direct information
flows from jvm_t ; (3) identifying direct information flows
from jvm_t to java_t; (4) identifying indirect information flows from test_t to user_install_t ; (5) identifying direct information flows from jvm_t, test_t to
root_t ; (6) identifying information flows from jvm_t
through root_t to sysxo_t ; (7) identifying direct
information flows from group 1 = {test_t, jvm_t}
to group 2 = {xo_t, root_t} and group 3 = {user
_install_var_t, user_install_exec_t}; and
(8) repeating steps 6 and 7 to examine the result of analysis composition. In addition, the participants are asked to
test the tool freely and provide their feedback on how the
interface is designed, how the security policy is composed,

123

and other information that would be necessary for our user
study.
The second part of the instruction mainly elaborates how
to use the semantic substrates approach and adjacency matrix
approach separately to visualize the security policy set, read
information from the visualized policy, generate and display
security policy violations against predefined security goals,
and identify the root causes of the existing policy violation such as why there exists an information flow from an
untrusted domain user_install_t to a trusted domain
sysxo_t.
6.3 User study procedures and data collection
To perform the user study, we installed APOL and PVA
tools on our lab machine in an independent room where
the participant cannot be interrupted during their session.
The participants first performed the five classified policy
queries with example scenarios using APOL and PVA based
on our instruction. Then, the participants were required to
visualize and identify security policy violations with the
Node-Link and Adjacency Matrix approach. For conducting these steps, the participants are required to use the same
example SELinux policy.
After completing the policy analysis, participants were
asked to complete a post-session questionnaire assessing
their attitudes toward two tools and two approaches, their
experiences with and attitudes toward security policy analysis, their general control inclinations, and their demographic
characteristics.
In the questionnaire, we design totally 20 items to measure participant attitude including the ease of constructing

Visualization-based policy analysis for SELinux

queries, the ease of understanding policy analysis results and
the overall ease of using the tool interface. Among these
measures, the first two items are to measure PVA and APOL
in the policy analysis aspect and the latter items focus on
policy browsing, understanding and the overall experience
of the tool interface. For each measure, we design separate
questionnaires for each tool but with semantically equivalent
items. The following summarizes what we intend to measure.
– Ease of constructing queries using APOL and PVA for
policy analysis was measured with 4 items. Participants
were asked to rate the easiness of the processes described
in the items using APOL and PVA functions on a 5-point
rating scale (1 = very complicated to 5 = very easy). A
sample item is “browsing the security policy using APOL
was.”
– Ease of understanding query results using APOL and
PVA for policy analysis was measured with 3 items rated
on a Likert scale (1 = strongly disagree to 5 = strongly
agree). A sample item is “PVA can give you some general
knowledge about the loaded SELinux policy.”
– Overall ease of using the interface using APOL and PVA
for policy analysis was measured with 3 items rated on a
Likert scale (1 = strongly disagree to 5 = strongly agree).
A sample item is “The APOL interface is easy to combine
and compose policy queries.”
– Ease of identifying policy violations using policy visualization functions of Node-Link and the Adjacency Matrix
methods was measured with 3 items rated on a Likert
scale (1 = strongly disagree to 5 = strongly agree). A sample item is “It is easy to identify trusted and untrusted
domains.”
– Ease of tracing policy violation elements using policy
visualization functions of Node-Link and the Adjacency
Matrix methods was measured with 3 items rated on a
Likert scale (1 = strongly disagree to 5 = strongly agree).
A sample item is “This visualization is clear, not crowded
and not cluttered.”
– Satisfaction with the visualization policy using policy
visualization functions of Node-Link and the Adjacency
Matrix methods was measured with 3 items. Participants were asked to rate the degree to which they liked
different aspects of the two policy visualization functions on a 5-point rating scale (1 = do not like at all
to 5 = like it very much). A sample item is “Viewing specific policy element relationships (domains and
types).”
– Scale scores were calculated by computing participants’ mean responses to the items included in each
scale.
– Participant characteristics were aggregated based on
their feedback such as their age range, specialization,
occupation, and their general attitudes and behaviors

167
Table 2 PVA versus APOL (number of participants = 32)
Measure items

APOL

PVA

Mean

SD

Mean

SD

Ease of constructing queries

2.78

.94

4.36

.41

Ease of understanding query results

3.13

.78

4.32

.40

Overall ease of using the interface

2.41

.86

4.42

.43

regarding the configuration of OS settings, such as the
security policies.
For the questionnaire design, we implemented the questionnaire using the lime survey tool [17] on our lab server.
For each participant, our lab sever recorded their answers to
the questions and helped determine whether or not they finished the questions. Their answers were stored and can be
exported through a database for subsequent analysis.
6.4 User study results
PVA versus APOL: Paired samples t-tests [23] were conducted to compare participants’ attitudes toward two
approaches in policy analysis. Corresponding to each measure, we were able to produce the following results:
– Ease of constructing queries: Through calculating the
answers for this measure, the satisfaction tendency for
APOL was (M = 2.78, SD = .94)1 compared to PVA
(M = 4.36, SD = .41). Therefore, we concluded that constructing queries with PVA is perceived to be significantly
easier than APOL, where t (32) = −9.67 and p < .001.
– Ease of understanding query results: Based on the the
answers for this measure, the satisfaction tendency was
calculated. APOL’s satisfaction tendency was (M = 3.13,
SD = .78) compared to PVA (M = 4.32, SD = .40). Hence,
understanding query results with PVA is also perceived
to be relatively easier than APOL, where t (32) = −9.07
and p < .001.
– Overall ease of using the interface: Through examining
the answers for this measure, the satisfaction tendency
for APOL was (M = 2.41, SD = .86) compared to PVA
(M = 4.42, SD = .43). Also, the overall interface of PVA is
perceived to be significantly easier to use than the APOL
interface, where t (32) = −11.82 and p < .001.
In summary, participants indicated that all measurements
of the PVA approach were superior to the corresponding
aspects of the APOL approach in performing policy analysis
as shown in Table 2.
1

M and SD denote mean and standard deviation, respectively.

123

168
Table 3 Semantic substrates
versus adjacency matrix
(number of participants = 32)

W. Xu et al.

Measure items

Semantic substrates

Adjacency matrix

Mean

SD

Mean

SD

Satisfaction with the visualization policy

4.36

.50

4.11

.65

Ease of identifying policy violations

4.40

.40

4.17

.64

Interpretability of visualization results

4.23

.64

3.99

.70

Semantic Substrates versus Adjacency Matrix: With a
method similar to that we used to compare PVA with APOL,
paired samples t-tests [23] were conducted to compare participants’ attitude toward the two visualization approaches
for policy analysis. For each measure, our results were as
follows:
– Ease of identifying policy violations: Through
calculating the answers for this measure, the satisfaction
tendency for semantic substrates was (M = 4.40, SD =
.40) compared to adjacency matrix (M = 4.17, SD = .64).
It implies that identifying policy violations with the
semantic substrates visualization is perceived to be easier
than adjacency matrix visualization, where t (32) = 2.21
and p < .05.
– Satisfaction with the visualization policy: Based on the
answers for this measure, although there was a tendency
for participants to be more satisfied with the Node-Link
visualization compared to the adjacency matrix visualization (M = 4.11, SD = .65), this tendency is not statistically significant, where t (32) = 1.98 and p = .056.
– Interpretability of visualization results: Based on the
answers for this measure, the satisfaction tendency for
semantic substrates was (M = 4.23, SD = .64)
compared to adjacency matrix (M = 3.99, SD = .70).
Therefore, we could conclude that interpreting the visualization results with the semantic substrates visualization
is perceived to be not significantly easier than adjacency
matrix visualization, where t (32) = 1.98 and p < .056.
In summary, although there is a tendency for participants’ perceptions of the semantic substrates visualization
to be more favorable, compared to their perceptions of the
adjacency matrix visualization, this tendency is only significant with respect to the ease of identifying policy violations
(Table 3).
6.5 Lessons learned from user study
This section describes some lessons that we learned through
the evaluation of PVA. Although the evaluation itself may
have diverse goals and claims for different types of security
policies, we share our lessons so that other researchers can

123

consider these lessons for their policy analysis work. The
lessons learned are summarized as follows:
Lesson 1: Ensure that participants understand the
assigned task. It is necessary to clearly communicate with
participants and confirm participants’ understanding before
initiating the survey. We found out that the best way to achieve
this objective was to ask the participants to explain back to
us what they understood and they needed to do. These participants had less problems in executing the assigned tasks than
the participants who simply nodded to passively indicated
that they understood.
Lesson 2: Test and validate materials before user study. It
is very important to run a pre-test before starting the survey.
In our experiments, we first tested all the materials including
instruction, tools and questionnaire. Then, we invited few
participants to evaluate our materials and setup, so that the
design of the overall experiment could be fully debugged,
modified and validated before conducting the user study.
Lesson 3: Use as little paper work as possible. Initially,
we designed a hard-copy questionnaire which was perceived
to be too exhaustive and boring to the participants. Also the
paper work caused additional problem such as bringing overhead to the data analysis, difficulty for data backup, and so
on. Hence, we prepared online-version of the questionnaire
which tremendously helped us leverage the benefits of webbased application such as interactive question & answer, data
collection, analysis, and backup.
Lesson 4: Minimize the chances that participants may
make mistake. It was a challenging task to minimize the
chances that participants could make mistakes unrelated to
the claims. In other words, we should pay attention not to
introduce unnecessary complications to the tasks. For example, in our instructions, we stated that PVA allows users to
perform the same kind of security policy analysis through
two different ways. Having both mechanisms to perform the
analysis, some participants felt that they could choose one
mechanism based on their first impression of GUIs. Consequently, it would introduce unnecessary confusion to the
participants. For another example, in our initially designed
questionnaire, it had a “no answer” option for some items,
which resulted in data collected to be invalid and required us
to re-invite participants.
There are several directions that we may consider to
improve our user study for the future work. First, the lack

Visualization-based policy analysis for SELinux

of policy analysis tool was one of critical obstacles in our
user study. Due to such limitations, we could select only
the most sophisticated tool APOL for our user study. Therefore, we could not have a chance to test PVA with more
generic evaluation criteria. Also, APOL does not consider
trusted and untrusted concepts in the tool. With such a distinction, we can formulate and detect information flows.
Hence, we were not able to examine corresponding functionalities of PVA compared to APOL. Second, some naive
participants have never used any policy analysis tool before
so they were not able to provide more constructive feedback
on each questionnaire–even though their unbiased feedback
was significantly helpful to evaluate our tool. The participants with different background are critical for user study
but it is also crucial to be balanced. Third, both APOL and
PVA worked a bit slow when analyzing a large security policy set to identify policy violations against a large volume of
information flow. However, some participants were intolerant of such delays. Especially, if the policy size reaches gigabyte, APOL and PVA consumed most of system resources.
Hence, we had to reduce the size of policy set as we discussed
before. The evaluation with diverse data sets would provide
some potential clues to improve our tools in terms of interface design and functional aspects. Fourth, although our current user sample can evaluate the user study well, enrolling
more users into the user study will make the study more
meaningful.

7 Related work
Previous typical methods and tools developed to analyze
SELinux policies include Gokyo [12,28,14,13], SLAT [7],
PAL [27] and APOL [36]. Gokyo was used to check integrity
of a proposed TCB for SELinux. Integrity of the TCB holds if
there is no type that can be written by a type outside the TCB
and read by a type inside the TCB, except for special cases
in which a designated trusted program sanitizes untrusted
data when it enters the TCB. Because Gokyo only identifies one common TCB in SELinux and SELinux has multiple security goals with obviously different kinds of trust
relationship, Gokyo cannot cover all the aspects of policy
violations. SLAT (Security-Enhanced Linux Analysis Tool)
defines an information flow model and the SELinux policies are analyzed based on this model. In the information
flow model, SLAT characterizes information flow caused by
allowed operations for a given policy. It defines the information flow relation (write operation transfers information
from process to resource; read operation transfers information from resource to process) as the flow transition. Then,
through this flow transition relationship, a path is defined
to reflect a sequence of events through which some causal
effects are transmitted from the first process to the last. SLAT

169

also contains an implementation by analyzing an information
flow model. Sarna-Sota et al. [7] used SLAT’s information
flow model to implement a framework for analyzing configuration policies in SELinux; it is called PAL (Policy Analysis using Logic Programming). PAL creates a logic program
based on an SELinux policy to run queries for analyzing the
policy. APOL [36] is a tool developed by Tresys Technology
to analyze SELinux configuration policies. Its main features
include forward and reverse domain transition analysis, direct
and transitive information flow analysis, relabel analysis, and
type relationship analysis based on user request.
SLAT, PAL and APOL tools require the administrator to
be well familiar with SELinux policies for generating queries
against the policy base which ultimately leads to extracting
meaningful information. Furthermore, APOL tool provides
graphical interface to aid policy analysis and supports policy
query. However, our PVA has the following differences and
advantages compared to APOL. First, PVA is built based on
a policy visualization framework. All the policy rules can be
expressed with graphs and their relationships are automatically displayed. APOL only provides text-based expressions
for the policy rules. A policy administrator has to read the policy rules one by one for identifying the relationship between
rules. Second, we support the visual query formulation that
enables a user to easily construct the query. Especially, the
administrator can construct complex queries to examine policies for their need. APOL only supports a simple query construction based on clicking buttons and choosing items from
the list provided by the graphical interface. Our user study
clearly indicates users found our policy analysis framework
more intuitive to perform tasks. In addition, PVA tool supports N-TCB and TCB concepts for policy violation detection
while APOL does not support such concepts and corresponding functions.

8 Conclusion
In this paper, we have proposed a visualization-based policy analysis framework to analyze the security policies.
We have provided both semantic substrates and adjacency
matrix approaches for policy visualization. We presented
our visualization-based query mechanism that enables the
administrator to query the policy base by simply connecting query components, which is similar to the query
by example approach. Our main methodology also facilitates visualization-based queries to identify the possible policy violations. We have developed a PVA tool to
implement our framework. Additionally, we discussed how
to use our framework to analyze SELinux policies and
the results confirmed the feasibility and applicability of
our methodology. In addition, we have conducted a user

123

170

study to evaluate the usability of our policy visualization
framework.
For the future work, we plan to investigate node and link
reordering mechanisms that minimize the link crossings and
entanglement for more appealing policy visualizations. Also,
the application of our framework for visualizing and analyzing the web-based access control policy such XACML
policies will be investigated.
Acknowledgments The work of Gail-Joon Ahn and Wenjuan Xu was
partially supported by the grants from National Science Foundation and
Department of Energy.

References
1. Anderson, A.P.: Computer Security Technology Planning Study.
Technical Report ESD-TR-73-51, II (1972)
2. Aris, A.: Network visualization by semantic substrates. IEEE
Trans. Vis. Comput. Graph. 12(5), 733–740 (2006). Senior
Member-Ben Shneiderman
3. Biba, K.J.: Integrity Consideration for Secure Compuer System.
Technical report, Mitre Corp. Report TR-3153, Bedford, Mass
(1977)
4. Denning, D.E.: A lattice model of secure information flow. Commun. ACM 19(5), 236–243 (1976)
5. Erbacher, R.: Intrusion behavior detection through visualization.
In: IEEE International Conference on Systems, Man and Cybernetics, pp. 2507–2513 (Oct 2003)
6. Green, M.: Toward a perceptual science of multidimensional
data visualization: Bertin and beyond. Available from http://www.
ergogero.com/dataviz/dviz2.html, 1998
7. Guttman, J., Herzog, A., Ramsdell, J.: Information flow in operating systems: Eager formal methods. In: Workshop on Issues in the
Theory of Security (WITS) (2003)
8. Herman, I., Melancon, G., Marshall, M.: Graph visualization and
navigation in information visualization: A survey. IEEE Trans. Vis.
Comput. Graph. 6(1), 24–43 (2000)
9. H.C. I. L. at University of Maryland. Piccolo. Available from http://
www.cs.umd.edu/hcil/jazz/download/index.shtml
10. Itoh, T., Takakura, H., Sawada, A., Koyamada, K.: Hierarchical
visualization of network intrusion detection data. IEEE Comput.
Graph. Appl. 26(2), 40–47 (2006)
11. Jaeger, R.S.T., Zhang, X.: Resolving Constraint Conflicts. In: Sacmat ’04: Proceedings of the Ninth Acm Symposium on Access
Control Models And Technologies, pp. 105–114 (2004)
12. Jaeger, X.Z.T., Edwards, A.: Policy management using access control spaces. ACM Trans. Inf. Syst. Secur. (TISSEC) 6, 327–364
(2003)
13. Jaeger, T., Sailer, R., Shankar, U.: Prima: policy-reduced integrity
measurement architecture. In: SACMAT ’06: Proceedings of the
Eleventh ACM Symposium on Access Control Models and Technologies, pp. 19–28. ACM, New York, NY, USA (2006)
14. Jaeger, T., Sailer, R., Zhang, X.: Analyzing integrity protection in
the selinux example policy. In: SSYM’03: Proceedings of the 12th
Conference on USENIX Security Symposium, pp. 59–74. USENIX
Association, Berkeley, CA, USA (2003)
15. Keller, R., Eckert, C.M., Clarkson, P.J.: Matrices or node-link diagrams: which visual representation is better for visualising connectivity models? Inf. Vis. 5(1), 62–76 (2006)
16. Lee, C., Trost, J., Raheem, N.G.B., Copeland, J.: Visual firewall:
Real-time network security monitor. In: IEEE Workshops Visualization for Computer, Security, pp. 129–136 (2005)

123

W. Xu et al.
17. Lime Survey Tool http://www.limesurvey.org/
18. Loscocco, P., Smalley, S.: Integrating flexible support for security policies into the linux operating system. In: USENIX Annual
Technical Conference, FREENIX Track, pp. 29–42 (2001)
19. Loscocco, P.A., Smalley, S.D.: Meeting critical security objectives
with security-enhanced linux. In: Proceedings of the Ottawa Linux
Symposium (2001)
20. Mathew, S., Giomundo, R., Upadhyaya, S., Sudit, M., Stotz, A.:
Understanding multistage attacks by attack-track based visualization of heterogeneous event streams. In: VizSEC ’06: Proceedings
of the 3rd International Workshop on Visualization for Computer
Security, pp. 1–6. ACM, New York, NY, USA (2006)
21. Nidhi, S.: Fireviz: A personal firewall visualizing tool. In: Thesis
(M. Eng.), Massachusetts Institute of Technology, Department of
Electrical Engineering and Computer Science (2005)
22. Noel, S., Jajodia, S.: Managing attack graph complexity through
visual hierarchical aggregation. In: VizSEC/DMSEC ’04: Proceedings of the 2004 ACM workshop on Visualization and data mining
for computer security, pp. 109–118. ACM, New York, NY, USA
(2004)
23. Paired Samples T-tests. http://www.statisticssolutions.com/
methods-chapter/statistical-tests/paired-sample-t-test/
24. Reiterer, H., Muler, G.: A visual information seeking system for
web search. In: Proceedings of the Oberquelle, H., Oppermann, R.,
Krause, J. (eds) Mensch & Computer Conference, pp. 297–306,
(March 2001)
25. Reiterer, H., Tullius, G., Mann, T.: Insyder: A content-based visualinformationseeking system for the web. Springer-Verlag GmbH,
International Journal on Digital Libraries (2005)
26. Saltzer, J., Schroeder, M., (1975) The protection of information in
computer systems. In: Proceedings of the IEEE, pp. 1278–1308.
27. Sarna-Starosta, B., Stoller, S.D.: Policy analysis for securityenhanced linux. In: Proceedings of the 2004 Workshop on Issues
in the Theory of Security (WITS), pp. 1–12 (April 2004)
28. Shankar, U., Jaeger, T., Sailer, R.: Toward automated informationflow integrity verification for security-critical applications. In:
NDSS, The Internet Society (2006)
29. Shen, Z., Ma, K.: Path visualization for adjacency matrices. In:
Proceedings of Eurographics/IEEE Symposium on Visualization
(EuroVis), May 2007
30. Smalley, S.: Configuring the SELinux policy. http://www.nsa.gov/
SELinux/docs.html, 2003
31. Sutcliffe, A.G., Ennis, M., Watkinson, S.J.: Empirical studies of
end-user information searching. J. Am. Soc. Inf. Sci. 51(13), 1211–
1231 (2000)
32. Secure computer systems: Unified exposition and multics interpretation. MITRE Corporation, 1976
33. System management concepts: Operating system and devices, 1
ed., (1999)
34. Thompson, R.S., Rantanen, E.M., Yurcik, W., Bailey, B.P.: Command line or pretty lines?: comparing textual and visual interfaces
for intrusion detection. In: CHI ’07: Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, pp. 1205.
ACM, New York, NY, USA (2007)
35. Tran, T., Al-Shaer, E.S., Boutaba, R.: Policyvis: Firewall security
policy visualization and inspection. In: Lisa, pp. 1–16 (2007)
36. Tresys Technology Apol. http://www.tresys.com/selinux/
37. Yao, D., Shin, M., Tamassia, R., Winsborough, W.H.: Visualization
of automated trust negotiation. In: VizSEC 05: IEEE Workshop on
Visualization for Computer, Security, Oct 2005
38. Yin, X., Yurcik, W., Treaster, M., Li, Y., Lakkaraju, K.: Visflowconnect: netflow visualizations of link relationships for security situational awareness. In: VizSEC/DMSEC ’04: Proceedings
of the 2004 ACM Workshop on Visualization and Data Mining
for Computer Security, pp. 26–34. ACM, New York, NY, USA
(2004)

Visualization-based policy analysis for SELinux
39. Yurcik, W.: Visualizing netflows for security at line speed: the
sift tool suite. In: LISA’05: Proceedings of the 19th Conference on Large Installation System Administration Conference,
pp. 169–176. USENIX Association, Berkeley, CA, USA (2005)

171
40. Yurcik, W.: Tool update: visflowconnect-ip with advanced filtering from usability testing. In: VizSEC ’06: Proceedings of the 3rd
International Workshop on Visualization for Computer Security,
pp. 63–64. ACM, New York, NY, USA (2006)

123

Instructional Module Development (IMODTM)
System: Building Faculty Expertise in Outcome-based
Course design
Odesma Dalrymple, Srividya Bansal, Kavitha Elamparithi, Husna Gafoor, Adam Lay, Sai Shetty
Department of Engineering
Arizona State University – Polytechnic Campus
Mesa, AZ 85212 USA
{odesma.dalrymple, srividya.bansal}@asu.edu
Abstract— A well-designed and constructed course plan or
curriculum is an integral part of the foundation of effective
STEM instruction. This paper presents a framework for
outcome-based course design process and its translation into a
semantic web-based tool; i.e., the IMODTM system. This system
guides STEM educators through the complex task of curriculum
design, ensures tight alignment between various components of a
course (i.e., learning objectives, content, assessments, and
pedagogy), and provides relevant information about researchbased pedagogical and assessment strategies. The theoretical
framework is presented, along with descriptions and screenshots
of the implementation of key features.
Keywords—course design, instructional module, learning
objectives, outcome-based education, semantic web application.

I.

INTRODUCTION

At many colleges and universities, engagement in scholarly
teaching is becoming a minimum expectation of faculty who
are held accountable for the quality of the learning experienced
by students enrolled in their course(s). These expectations are
even greater for Science, Technology, Engineering, and
Mathematics (STEM) faculty given the national demands for a
well-trained STEM workforce [1]. Since education training is
not typically included in the plan of study of most STEM
programs, faculty who graduate with STEM degrees gain their
teaching expertise post-appointment and "on-the-job". In the
absence of formal training, most faculty can take as much as
five years to truly become proficient teachers, and during that
period, it is the students who are most affected [2].
There is a growing demand and interest in faculty
professional development in areas such as outcome-based
education (OBE), curriculum design, and pedagogical and
assessment strategies. In response to this demand, a number of
universities have established teaching and learning centers to
provide institution-wide, and sometimes program specific
support. This paper describes the development of the
Instructional Module Development (IMOD) System, which
further supports these ventures and broadens the impact and
reach of professional development in the scholarship of
teaching and learning, particularly to STEM faculty. The

978-1-4673-5261-1/13/$31.00 ©2013 IEEE

IMOD system is an open-source web-based course design
software that:
• Guides individual or collaborating users, step-by-step,
through an outcome-based education process as they
define learning objectives, select content to be covered,
develop an instruction and assessment plan, and define
the learning environment and context for their course(s)
• Contains a repository of current best pedagogical and
assessment practices, and based on selections the user
makes when defining the learning objectives of the
course, the system will present options for assessment and
instruction that align with the type/level of student
learning desired
• Generates documentation of course designs. In the same
manner that an architect's blueprint articulates the plans
for a structure, the IMOD course design documentation
will present an unequivocal statement as to what to expect
when the course is delivered
• Provides just-in-time help to the user. The system will
provide explanations to the user on how to perform course
design tasks efficiently and accurately. When the user
explores a given functionality, related explanations will
be made available
• Provides feedback to the user on the fidelity of the course
design. This will be assessed in terms of the cohesiveness
of the alignment of the course design components (i.e.,
content, assessment, and pedagogy) around the defined
course objectives.
II.

THEORETICAL FRAMEWORK

Many of the leaders in faculty development programs have
identified facilitation by experts as a key ingredient in
increasing the effectiveness of instructional development
programs [3]. For the IMOD system, which will provide
professional development with the use of an online tool, expert
facilitation is embedded within its design, through the
application of a framework that is informed by research in the
area of instructional development for STEM disciplines. This
framework translates the scholarship into a software platform
that supports the development of a rich, meaningful knowledge

structure that can be queried to: (1) identify omissions in a
course design; (2) identify inconsistencies in the relationships
between the elements of the course being designed; (3) identify
relevant strategies for instruction and/or assessments; (4)
provide just-in-time guidance to the user on the design process.
The structure of the framework and its implementation in the
IMODTM system are discussed in the subsequent sections.
A. Previous Models of Outcome-Based Course Design
Outcome-based education (OBE) is an approach where the
product defines the process, i.e., the outcomes that specify
what students should be able to demonstrate upon leaving the
system are defined first, and drive decisions about the content
and how it is organized, the educational strategies, the teaching
methods, the assessment procedures and the educational
environment [4]–[6]. This is a contrast to the preceding “inputbased” model that placed emphasis on the means as opposed to
the end of instruction. OBE was used as the principal guide for
the development of the IMOD framework. It was chosen for
the following reasons: 1) Win-for-all solution – OBE is shown
to improve student success, provides a structure to educators
for designing instruction, and facilitates reporting to external
stakeholders in an accountability education climate; 2) It
supports the How People Learn framework for designing
learning environments [7]; 3) Growing adoption of outcomebased program accreditation – Accreditation boards such as
ABET, have moved to an outcome focused model (what
students learned) to assess the quality of programs in Applied
Science, Computing, Engineering, and Engineering
Technology; 4) Alignment with other models that are meant to
increase innovation in STEM education – OBE dictates the end
and not the means thereby allowing innovation in instruction. It
also provides an empirical structure to track impact and
identify shortcomings.

constructs that can be used to make inferences on the
inconsistencies in the relationships between the elements of the
course being designed. While the backward-looking process
dictates an ideal sequencing of tasks, it is limited in its ability
to support automated inferencing on course element coherence.
The IMOD framework, therefore, expands on the current
models with the inclusion of new constructs.
B. IMOD Framework
The IMOD framework adheres strongly to the OBE
approach and treats the course objective as the spine of the
structure. New constructs (not included in the models
previously discussed) are incorporated to add further definition
to the objective. The work of Robert Mager [12] informs the
IMOD definition of the objective. Mager identifies three
defining characteristics of a learning objective: Performance –
description of what the learner is expected to be able to do;
Conditions – description of the conditions under which the
performance is expected to occur; and the Criterion – a
description of the level of competence that must be reached or
surpassed. For use in the IMOD framework an additional
characteristic was included, i.e., the Content to be learned –
description of the factual, procedural, conceptual or metacognitive knowledge; skill; or behavior related to the
discipline. The resulting IMOD definition of the objective is
referred to as the PC3 model.

A number of models have been developed to represent the
application of OBE in the design of effective courses. Four key
models widely discussed in the engineering education literature
are: 1) the Effective Course Model by Felder & Brent [8]; 2)
Integrated Course Design by Fink [9]; 3) Understanding by
Design Model [10]; 4) Content Assessment Pedagogy Model
by Streveler, Smith, & Pilotte [11]. All of these models either
directly or indirectly identify four main elements that must be
tightly aligned when defining a course design, i.e., course
objectives, content, assessments, and pedagogy. Therefore, one
of the main challenges in adhering to an outcome-based
approach is maintaining the alignment between course
elements. Inconsistencies in the interrelation of these elements
can lead to the overall incoherence of the course.
One approach for achieving alignment among course
elements is through a “backward-looking” design process
where the desired results are identified first, and then
assessments are designed to verify that these results have been
achieved. The learning experiences and instruction are then
formulated around the desired results and the assessments. The
use of this approach forms the basis of the Understanding by
Design model, and it is also applied by the other models. One
of the key functions the IMOD system is expected to perform
is the evaluation of the fidelity of the course design. To achieve
this, the IMOD framework must include machine processable

Figure 1: PC3 Model
The other course design elements (i.e., Content, Pedagogy,
and Assessment) are incorporated into the IMOD framework
through interactions with two of the PC3 characteristics.
Course-Content is linked to the content and condition
components of the objective. The condition component is often
stated in terms of pre-cursor disciplinary knowledge, skills or
behaviors. This information, together with the content defined
in the objective, can be used to generate or validate the list of
course topics. Course-Pedagogy is linked to the performance

Figure 2: Screenshots of Learning Objective and Pedagogy tabs of the IMODTM system
and content components of the objective. The types of
instructional approaches or learning activities used in a course
should correspond to the level of learning expected and the
disciplinary knowledge, skills or behaviors to be learned. The
content and performance can be used to validate pedagogical
choices. Course-Assessment is linked to the performance and
criteria components of the objective. This affiliation can be
used to test the suitability of the assessment strategies since an
effective assessment, at the very least, must be able to
determine whether the learner’s performance constitutes
competency. Figure 1 shows a visual representation of the
IMOD framework.
III.

implementation has already underway. Once the first version is
completed, user testing will be conducted to test for
effectiveness, efficiency and usability.
REFERENCES
[1]
[2]
[3]
[4]

IMPLEMENTATION OF IMODTM SYSTEM

The implementation of the IMOD system shown in Figure
2, consists of five features described below. 1) Course
Overview – a feature used to capture information on the
learning environment (e.g., type of course, meeting days and
times, instructor(s) information, course policies, etc.). 2)
Learning Objectives – a feature used to guide the user through
the creation of learning objective statements that conform to
the PC3 model. Revised Bloom’s taxonomy of learning
objectives [13] was also used in this feature to help the user
describe performance characteristics. 3) Content – a feature
used to capture information on the course topics. The content
prioritizing model by Wiggins and McTighe [10] and the
Knowledge Dimension from Anderson and Krathwohl version
of Bloom’s taxonomy [13] are also used in this feature. 4)
Assessment - features used to suggest relevant assessment
techniques based on the type of learning and evaluation criteria
specified in the learning objectives. 5) Pedagogy - features
used to suggest relevant instructional techniques based on the
type of learning and knowledge specified in the learning
objectives.
IV.

FUTURE WORK

The design of the IMOD system is still ongoing, and will
be further described in future publications. Some of the

[5]
[6]
[7]
[8]
[9]
[10]
[11]

[12]
[13]

M. T. Huber and S. P. Morreale, Eds., Disciplinary Styles in the
Scholarship of Teaching and Learning: Exploring Common Ground.
AAHE Publications, 2002.
R. Boice, Advice for new faculty members. Allyn & Bacon, 2000.
R. M. Felder, R. Brent, and M. J. Prince, “Engineering Instructional
Development: Programs, Best Practices, and Recommendations.,”
Journal of Eng. Education, vol. 100, no. 1, pp. 89–122, Jan. 2011.
R. M. Harden, J. R. Crosby, M. H. Davis, and M. Friedman, “AMEE
Guide No. 14: Outcome-based Education: Part 5--From Competency
to Meta-Competency: A Model for the Specification of Learning
Outcomes.,” Medical Teacher, vol. 21, no. 6, pp. 546–552, 1999.
R. M. Harden, J. R. Crosby, and M. H. Davis, “AMEE Guide No. 14.
Outcome-Based Education: Part 1--An Introduction to OutcomeBased Education.,” Medical Teacher, vol. 21, no. 1, pp. 7–14, 1999.
W. G. Spady and K. J. Marshall, “Beyond Traditional Outcome-Based
Education.,” Educational Leadership, vol. 49, no. 2, pp. 67–72, 1991.
J. D. Bransford, A. L. Brown, and R. R. Cocking, How people learn.
National Academy Press Washington, DC, 2000.
R. M. Felder and R. Brent, “Designing and teaching courses to satisfy
the ABET engineering criteria.,” Journal of Engineering Education,
vol. 92, no. 1, pp. 7–25, 2003.
L. D. Fink, “Creating significant learning experiences: An integrated
approach to designing college courses.” Jossey-Bass, 2003.
G. P. Wiggins and J. McTighe, Understanding by design. Association
for Supervision & Curriculum Development, 2005.
R. A. Streveler, K. A. Smith, and M. Pilotte, “Aligning Course
Content, Assessment, and Delivery: Creating a Context for OutcomeBased Education,” K. Mohd Yusof, S. Mohammad, N. Ahmad Azli, M.
Noor Hassan, A. Kosnin and S. K, Syed Yusof (Eds.), Outcome-Based
Education and Engineering Curriculum: Evaluation, Assessment and
Accreditation. Hershey, Pennsylvania: IGI Global, 2012.
R. F. Mager, “Preparing Instructional Objectives: A critical tool in the
development of effective instruction 3rd edition,” The Center for
Effective Performance, Inc, 1997.
L. W. Anderson and D. R. Krathwol, “A taxonomy for learning,
teaching and assessing: A revision of Bloom’s Taxonomy of
educational objectives.” New York Longman, 2001.

Risk Evaluation for Personal Identity Management
Based on Privacy Attribute Ontology
Mizuho Iwaihara1 , Kohei Murakami1, Gail-Joon Ahn2 ,
and Masatoshi Yoshikawa1
1

Department of Social Informatics, Kyoto University, Japan
kmurakami@db.soc.i.kyoto-u.ac.jp, iwaihara@i.kyoto-u.ac.jp,
yoshikawa@i.kyoto-u.ac.jp
2
Department of Computer Science and Engineering, Arizona State University, USA
gahn@asu.edu

Abstract. Identity providers are becoming popular for distributed authentication and distributed identity management. Users’ privacy attributes are stored at
an identity provider and they are released to a service provider upon user’s consent. Since a broad range of privacy information of different sensitiveness can be
exchanged in advanced web services, it is necessary to assist users by presenting
potential risk on financial and personality damage, before releasing privacy attributes. In this paper, we present a model of privacy attribute ontology and risk
evaluation method on this ontology. Then we formalize several matching problems which optimize similarity scores of matching solutions under several different types of risk constraints. We show sophisticated polynomial-time algorithms
for solving these optimization problems.

1 Introduction
A wide variety of new services are created on the web, by connecting existing web
services. To carry out services and/or businesses with their customers, many of service
providers (SP) require basic personal information of customers, such as name, address,
phone number, as well as more critical information such as credit card number. Identity
providers (IdPs) offer identity management functionalities, including user authentication and management of basic personal information. Since basic information such as
name and email/postal addresses are frequently asked, provisioning of these information from IdP to SP through the user’s one-click action can save the user’s workload.
Liberty Alliance[10], OpenID[11] and CardSpace[1] are proposed identity management
standards which provide single sign-on and trust management. However, in these standards, users are still required to carefully examine requested attributes for sensitiveness
and criticality. Then users select appropriate identities to be used for the request, where
excessive exposure of identities and attributes should be avoided by users’ discretion.
Web services are rapidly evolving to cover every kind of social activities among
people, and categories of personal attributes are also growing beyond basic attributes.
Social network services are offering exchange of very personal attributes such as such
as age, ethnicity, religion, height and eye color. For example, orkut(www.orkut.com)
Q. Li et al. (Eds.): ER 2008, LNCS 5231, pp. 183–198, 2008.
c Springer-Verlag Berlin Heidelberg 2008


184

M. Iwaihara et al.

has an registration form having 30 attributes for “social” page, 16 attributes for “professional” page, and 15 attributes for “personal” page. User-centric control of sharing of
personal information is required for healthy support of social activities, and an identity
provider of the near future should assist the user through categorization and evaluation
of attributes from the point of criticality and sensitiveness.
In this paper, we propose the concept of privacy attribute ontology (PAO), built on
the OWL web ontology language[12]. One of primal objectives of PAO is to provide a
taxonomy of privacy attributes. Each class of PAO corresponds to a sensitive attribute
or an identity, and an individual of the class corresponds to a value of the attribute.
IdP manages a PAO as a shared ontology among users as well as a personal information database for each user. Also PAO provides risk evaluation functionality through
financial and personality risk values defined on PAO classes. When a service provider
presents a list of requested attributes, IdP matches the list with PAO classes, and then the
risk values of the requested attributes are evaluated from matched classes. Here we have
a number of issues to be solved. First, we need to design a matching algorithm that maximizes linguistic/structural similarities between PAO classes and requested attributes.
Secondly, the algorithm also needs to consider risk constraints such that matched classes
must not exceed given upper limits of risk values. The algorithm should select a lowrisk combination of identities and attributes associated to these identities, covering requested attributes. In this optimization, we need to consider combination risks which
arise if a certain combination of classes is selected for release.
The contribution of this paper is summarized as follows: (1) We present a model of
privacy attribute ontology and risk evaluation method on this ontology. (2) We formalize matching problems which optimize similarity scores of matching solutions under
three different types of risk constraints. (3) We show sophisticated polynomial-time
algorithms for solving the optimization problems of (2).
P3P (Platform for Privacy Preferences Project) [14] is a standard for describing and
exchanging privacy policies in XML format. While P3P is targeted at interpreting privacy
practices of service providers, our research is focused on identity providers and users for
managing linkages between privacy attributes and identities of different aspects.
Developing ontologies for privacy and trust management on the web has been discussed in the literature[4][5][7]. Our research is different in the way that we focus on
risk evaluation for attribute disclosure and selecting disclosing attribute values (individuals) that have minimum risk values. Utilizing semantic web technologies for security
and trust management on the web is discussed in [4], which covers authentication, delegation, and access control in a decentralized environment. But an ontology for assessing
privacy risk values is not considered.
Matching and aligning ontologies have been extensively studied for integrating
ontologies. As a linguistic approach, OntoGenie[13] uses WordNet[16] for extracting
ontologies from web pages. Structural similarity is considered in [9] for neural networkbased schema matching. Udrea et al.[15] combined data and structural matching as well
as logical inference to improve quality. Our algorithms utilize these linguistic and structural approaches. But we need to deal with the new problem of considering risk values
during matching. We have successfully solved ontology matching under various types
of risk constraints.

Risk Evaluation for Personal Identity Management

185

The rest of the paper is organized as follows. In Section 2, we introduce an existing
risk evaluation method for privacy information, and discuss automated risk evaluation
based on privacy attribute ontology. In Section 3, we formalize privacy attribute ontology. In Section 4, we discuss matching requested attributes with PAO classes, and
define optimization problems under certain risk constraints. In Section 5, we discuss
several issues that need to be solved, and present polynomial-time algorithms for the
optimization problems. Section 6 is a conclusion.

2 Risk Evaluation for Personal Identity Management
2.1 JNSA Privacy Risk Evaluation
Service providers holding customer’s privacy data are having risk of privacy leakage.
Several measures for evaluating risk of privacy leakage have been proposed. Japan
Network Security Association (JNSA) published surveys on information security incidents[6]. The report also presents a method for estimating amount of compensation
if a certain portion of privacy data are leaked. The JNSA model is based on classifying
reported cases from court decisions and settlements, and the model was validated on
these cases. Its evaluation proceeds as follows:
The value of leaked privacy data of an individuation is evaluated in terms of (a) economical loss and (b) emotional pain. The Simple-EP Diagram contains representative
privacy attributes according to the dimensions of (a) and (b). Given an attribute, an integer from 1 to 3 is chosen as the value for each dimension. Let x (resp. y) be the value for
(a) economical loss (resp. (b) emotional loss). Then the sensitiveness factor is defined
as EP = (10x−1 + 5y−1 ).
Given a collection of privacy attributes for an individual, we take maximum values
for x and y from the Simple-EP Diagram. Suppose a record of an individual consists
of the attributes: real name, address, birth date, sex, phone, medical diagnosis, bank
account and password. Then by the Simple-EP Diagram, the value (x, y) is equal to
(1, 1) for real name, address, birth date, sex, and phone. On the other hand (x, y) is
equal to (2, 1) for medical diagnosis, and (1, 3) for bank account and password. Since
the maximum value for x is 2 and the maximum value for y is 3, we obtain EP = 35.
Let the basic information value BIV be 500 points, and let the identifiability factor IF be defined as: IF = 6 if the individual can be easily identified (for example, real name and address are included), IF = 3 if the individual can be identified by a certain effort (for example, real name is included, or address and phone
are included), and IF = 1 otherwise (for the case identification is difficult). The
leaked privacy information value LP IV is computed by: LP IV = BIV ∗ EP ∗ IF.
LPIV is designed to approximate the amount of compensation in Japanese yen paid to
each leakage victim. The LPIV is further adjusted to reflect other factors such as the
social status of the information holder and evaluation on the response after the incident.
However, these factors are not directly related to our goal.
The JNSA risk evaluation model can be a basis of risk evaluation for risk-aware identity management, from the points that the model can capture the emotional and financial losses according to a classification of privacy attributes, and it enables quantitative

186

M. Iwaihara et al.

comparison of the risks between attributes. However, the method requires human reasoning in determining values from the diagram.
2.2 Risk Evaluation at Identity Provider
The basic scenario of personal information management by an identity provider (IdP)
utilizing PAO proceeds as follows:
1. IdP manages and holds personal information of the user.
2. The user requests execution of a service to the service provider (SP). SP sends to
IdP requested attributes RA necessary for the service. RA includes basic identity
information as well as privacy attributes of the user.
3. IdP matches attributes of RA with classes of PAO, to compute releasing classes
RC. In the matching process, IdP evaluates risks of releasing information held in
RC, and IdP tries to find RC which has maximum conceptual similarities with RA,
while RC satisfies a certain risk constraint imposed by the user.
4. RC is presented to the user. The user modifies and supplements RC if necessary.
Some requested attributes A may not be included in RC, because either A’s risk
is intolerable to the user or the user has declined release of of A. After SP and the
user agree on RC, the information on RC is sent from IdP to SP.
IdP manages a number of identities of the user, such as student ID, a number of
email addresses, citizenship, net identities used for blogs and social network services.
Some of these identities are anonymous, while others have solid identities. One identity
is associated with a number of attributes, as well as other identities. In selecting RC,
IdP needs to find low-risk combination of attributes and avoid linking of identities if it
is prohibited by the user.
2.3 Risk Evaluation Using Privacy Attribute Ontology
In the following, we summarize the basic notions of our risk evaluation method utilizing
PAO.
Risk value is a numerical scale of 1 to 5 representing severity of the risk, where 1 is least
severe and 5 is most severe. Risk values are categorized into financial and personality
risk values. PAO holds risk values in its classes. However, some classes may not have
risk values defined. If a risk value of C is undefined, then the risk value is inherited
from C’s super classes. If a class C is in the releasing class RC, then the risk values of
C become effective. The risk value of releasing classes RC is the maximum effective
risk value in the classes of RC.
Financial risk value (f-risk value for short) is a risk value for financial damage to the
information subject (user). Credit card number, bank account number, and social security number should have high financial risk values. We use rf (·) to denote the financial
risk value function on various constructs such as class C and releasing classes RC.
Personality risk value (p-risk value for short) is a risk value for personality damage
to the user, including emotional pain, damage to social reputation, and generic damage
caused by privacy breach. We use rp (·) to denote the personality risk value function.

Risk Evaluation for Personal Identity Management

187

Combined risk value rc (RC) combines f-risk and p-risk values by the function
rc (RC) = cr(rf (RC), rp (RC)) such that cr(x, y) = c1 log(F x + P y ) + c2 , where
the risk values x and y are converted into an exponential scale by the exponential functions of bases F and P, and the average of these values are converted back to risk values
by the logarithmic function. The bases F and P assign weights between the financial
and personality risk values, and we can choose F = 10 and P = 5 following the JNSA
model. Constants c1 and c2 shall be determined to let cr(x, y) have a range between 1
and 5.
Combination risk is a risk arising from combination of attributes. Some privacy attributes, such as age and income, may be disclosed under an anonymous username, but
combining these attributes with the real name raises the risk of privacy breach. Thus the
user should be notified of such high risk combination. Also, the user holding a number of identities at IdP can choose one identity or a combination of identities to cover
requested attributes. In this scenario, the user should be advised of the risk in linking
several identities. For modeling combination risks, we need to introduce combination
risk classes to PAO.
Risk limit is a given upper limit on f-risk, p-risk or combined risk values. If the user
gives his/her tolerable risk limit, then disclosing attributes should not exceed the limit.
Here exists an optimization problem for finding most-similar matching between the
PAO classes and requested attributes, while satisfying the risk limit. Trustability of service providers can be reflected to risk limits, in a way that when dealing with a questionable service provider, the user can define a lower, more cautious risk limit. Detailed
linkage between risk limits and existing trustability models is beyond the scope of the
paper.

3 Modeling Privacy Attribute Ontology
In this section, we formalize privacy attribute ontology. We follow the definitions of
OWL[12] as the underlying ontology model. A class represents a concept. A class is
associated with zero or more individuals belonging to that class. An ontology can be
represented as a directed graph, where nodes are labeled with a class name or an individual, and directed edges are labeled with link types. A link labeled type from an
individual to a class represents the membership relation between the individual and the
class. A link labeled subClassOf from class C1 to class C2 indicates that C1 is a
subclass of C2 meaning that C1 is a concept more specific than C2 and an individual
belonging to C2 also belongs to C1 . A link labeled partOf from class C1 to class C2
indicates that C2 is a composite class composed of a number of component classes, including C1 . Formally, if a class C1 is connected to a class C2 through a directed path of
partOf and subClassOf links, then C1 is a component class of C2 . partOf links
are not allowed to form a directed cycle. We define composite attributes for requested
attributes, similarly to composite classes.
PAO has two special link types named financialRisk and personalityRisk,
representing the financial risk value rf (C) and and personality risk value rf (C) of a class

188

M. Iwaihara et al.

Fig. 1. Privacy attribute ontology

C, leading to individuals of real numbers in the range [1.0, 5.0]. An example of privacy
attribute ontology is shown in Figure 1, where risk values are shown as numbers of the
form rf : rp . Also, composite classes are depicted as black circles.
In PAO, we assume that each individual belongs to a single class. For an individual i
belonging to multiple classes, we can insert a virtual class between i and these classes,
to satisfy the single-class restriction. Thus this is not a tight restriction. Also, if some
risk values need to be defined on particular individuals, we create a class for such an
individual, and let all the risk values be defined on classes.
As discussed in Section 2.3, we introduce a combination risk class, which is a composite class connected by partOf links from its component classes. In Figure 1, combination risk classes are depicted as double circles. The risk value of a combination
risk class is applied if all of its component classes are selected for release. For example, the class rn&bn represents that if real name and blog name are going to
be released, then its risk values 2:5 will be applied. These values are higher than that
of classes real name and blog name alone, indicating that combination of these
classes increase the risk values, or it can be interpreted that the user is not allowing
linking of these identities. Thus a combination risk class should have f-risk and p-risk
values no less than that of its component classes.
PAO can be shared by a group of users so that the users’ common knowledge on
risks can be reflected. However, each user may have different views on privacy, and
individuals in the ontology are also user-dependent. Thus personalization of PAO is
necessary. Personalization of PAO can be done by the following ways: (a) overriding
financial and/or personality risk values of a class, (b) adding individuals to a class, and
(c) adding a class as a subclass of an existing class. Sharing and personalization of PAO
is beyond the scope of this paper, so we do not elaborate on this direction any further.

4 Matching PAO and Requested Attributes
4.1 Matching Problems
Now we discuss evaluating risk of a set RA of requested attributes sent by a service
provider, utilizing PAO. Then using the risk evaluation method, we consider optimization problems to find an optimum combination of releasing individuals that achieves
given risk constraints.
For associating individuals of PAO and requested attributes RA, we consider the
following two-staged approach: First find a bipartite matching between classes of PAO

Risk Evaluation for Personal Identity Management

189

and RA, then choose an individual from each class selected by the matching. A bipartite
matching finds a one-to-one mapping between classes and RA. Since we assumed that
each individual belongs to a single class, this process is straightforward.
We introduce similarity score σ(C, A) ≥ 0 on a PAO class C and a requested attribute A ∈ RA. When σ(C, A) > β holds for a given lower threshold β, C and A are
regarded as distinct concepts. We discuss construction of σ by linguistic similarities in
Section 4.2. We construct a matching graph Gσ,β = (C, RA, E) which is a bipartite
graph such that C is the set of classes in PAO, RA is the set of requested attributes,
and E is the set of edges (Ci , Aj ) such that Cj ∈ C, Aj ∈ RA, and σ(C, A) > β
is true. We also use the similarity function σ for edge weights of the bipartite graph
Gσ,β (C, RA, E). The weighted bipartite matching problem can be solved in O(N 3 )
time by the Hungarian method [8], where N is the number of nodes in Gσ,β . A matching M on bipartite graph Gσ,β = (C, RA, E) is a bipartite subgraph (CM , RAM , EM )
such that CCM ⊆ C, RA ⊆ RAM , EM ⊆ E, and no edge in EM conflicts each
other, that is, any two edges in EM are not adjacent at either end. Let σ(M ) denote the
sum of the edge weights of EM . A matching M on Gσ,β is a maximum matching if
σ(M ) ≥ σ(M  ) holds for any matching M  on Gσ,β .
Figure 2 shows an example of matching graphs. The nodes on the left are classes of
PAO, and the nodes on the right are requested attributes. Here, ’+’ sign means a composite class or attribute, and ’-’ sign means a component class or attribute. Edge weights
are not displayed in the graph. A matching is shown as bold edges in Figure 2. Notice
that this matching includes edges (email, e-mail) and (address, address).
These associations may appear reasonable, but unacceptable because structural integrity
is ignored. The email class of PAO is a component of class blog account, while
address of PAO is a component of class shopping. These composite classes represent distinct identities, and component classes should not be intermixed. Intuitively,
a proper matching should preserve component-composite relationships. In Section 5.1,
we discuss this component integrity and present a solution. We note that combination
risk classes should be excluded from matching candidates, because they are just for
internally defining combinational risk values.
Recall that the combined risk value is determined by maximum f-risk value rf and prisk value rp found in releasing classes. In a matching M = (CM , RAM , EM ), the set
of matched classes CM is the releasing classes. Let rf (M ) and rp (M ) be the maximum
f-risk and p-risk values in M , respectively. Then the combined risk value rc (M ) is
computed by cr(rf (M ), rp (M )).
Requested attributes RA may not have any matchable class in C. Such a dangling
attribute can be reported by the matching algorithm. In this case, the system needs to
start a dialog with the user to create a new class in PAO for the attribute.
By using predefined parameters on risk limits and similarity score limits, a number
of optimization problems can be defined:
1. (similarity score maximization) Tolerable upper risk limits mf and mp are given
by the user, where mf > 0 and mp > 0 are maximum f-risk value and maximum
p-risk value, respectively. The optimization problem is to find a matching M such
that similarity score σ(M ) is maximum and rc (M ) ≤ cr(mf , mp ) holds. The user
may specify mf = ∞ and/or mp = ∞ if he/she does not restrict one or both of

190

M. Iwaihara et al.

Fig. 2. Matching graph

Fig. 3. Augmented matching graph

the risk values. Another version of the problem is similarity score maximization
under combined-risk limit mc , which is to find a maximum matching under the
constraint rc (M ) ≤ mc . In this case, we need to test varying combinations of mf
and mp that satisfy cr(mf , mp ) ≤ mc .
2. (risk minimization) This problem assumes that a lowerbound wmin for the total
similarity score is given. The problem is to find a matching M such that σ(M ) >
wmin and the combined risk value rc (M ) is minimum.
3. (combined score maximization) Let sc (M ) be combined total score defined by
sc (M ) = σ(M )/rc (M ). The problem is to find a matching M that maximizes
sc (M ). Unlike others, this problem does not have a predefined parameter.
Since specifying all the risk values is tedious, it is likely that some classes or individuals may not be given risk values in PAO. For this issue, we utilize subClassOf links of
PAO for inferring risk values, based on the principle that a subclass inherits a missing
property value from its parent. Here we also adopt the principle of taking the highest
possible risk value, for conservative risk estimation.
(subClassOf rule) Let C be a class in PAO such that rf (C) is undefined. Let C1 , . . . ,
Ck be classes in PAO such that there is a path Si of subClassOf links from C to
each Ci and Ci is the only class in Pi where rf (Ci ) is defined. Then let rf (C) :=
max(rf (C1 ), . . . , rf (Ck )). For the case rp (C) is undefined, simply replace rf with rp .
Applying the subClassOf rule to every class that has an undefined risk value gives us
a unique PAO that has no undefined risk value, and this procedure can be done in linear
time.
4.2 Linguistic Similarity
We use Jaro-Winkler score[2] for string similarity and WordNet similarity [16] for score
on synonymity. Jaro-Winkler is effective for matching pairs having common substrings,
such as between e-mail, email, Email and netmail. WordNet is a lexical dictionary,
where words are ground into synonyms (synsets), each synset expressing a distinct
concept. WordNet similarity is measured by conceptual-semantic and lexical relations
between synsets. We use the sum of Jaro-Winkler score and WordNet score as the similarity score σ between PAO classes and RA.

Risk Evaluation for Personal Identity Management

191

Table 1 shows an experimental result on matching names of PAO classes and attributes from web sign-up forms. We constructed a PAO containing 186 classes. The
class names of this PAO are matched with two attribute sets from web sign-up forms
of eBay and PayPal, using the above similarity score σ. Case 2 is matches detected
by string similarity, which included pair “Primary telephone number” and “Primary
telephone”. Case 3 is matches detected by synonymity, which included pair “Secret
Question” and “Security Question”. Overall, the similarity score σ is showing enough
accuracy for matching requested attributes with PAO.
Table 1. Linguistic matching result on PAO having 186 classes
eBay PayPal
case total attributes
17
23
1 string match with PAO classes
7
15
2 attributes matched by string similarity(Jaro- 3/3 6/6
Winkler)
3 attributes matched by synonymity score (Word- 3/5 1/2
Net) (Excluding case 2)
4 attributes having no matching class in PAO
2
0
(detected matches)/(correct matches)

5 Matching Algorithms
5.1 Component Integrity and Two-Level Matching
First we formalize component integrity, and present a matching algorithm that achieves
a certain type of component integrity while maximizing similarity score. At this moment, we assume that mf = ∞ and mp = ∞ hold, namely no constraint is given on
risk values. We also assume that the PAO has no combination risk class. We extend the
algorithm later in this section.
(component integrity) Let us consider a matching M between PAO classes C and requested attributes RA. A matching M is said to satisfy component integrity, if the
following holds: Let (C, A) and (D, B) be any pair of edges in M such that C, D ∈ C
and A, B ∈ RA. Then C is a component class of D if and only if A is a component
attribute of B.
Note that PAO can have a multi-level component class, i.e., a composite class can
be a component class of another class. PAO can also include a component class shared
by multiple composite classes. In such a DAG-structured PAO, imposing the above
component integrity becomes a hard problem:
Theorem 1. Given a bipartite graph Gσ,β = (C, RA, E) and a minimum weight w,
deciding whether Gσ,β has a matching M having weight σ(M ) > w and satisfying
component integrity is NP-complete.
Proof. (sketch) Transformation from SET PACKING[3].

192

M. Iwaihara et al.

Thus it is intractable to enforce the above composite integrity. Also this integrity does
not consider link connectivities. In object-oriented modeling, link connectivities are
often used to add different perspectives to a class. For example, consider the following subgraphs containing the class email in Figure 1: email → blog account, email →
shopping, and email. Note that the last subgraph is a singleton node. These subgraphs
represent e-mail of the blog account, email of the shopping identity, and emails of the
person, respectively. Thus each subgraph is representing a different concept.
Now let us consider the following multi-level nesting of composite classes for matching: A class C is a level-k component class of D if (1) for k = 1, C is a component
class of D, and (2) for k > 1, C is a component class of a level-(k − 1) component
class of D.
We can adopt the interpretation such that for each different k, each path from a
composite class to its level-k component class represents a distinct concept. To treat
these paths as distinct concepts in matching, new nodes shall be created for each path
for varying k. However, since PAO can have shared component classes, the number of
such paths can be exponential to k. Thus considering all the paths to level-k component classes as matching candidates is impractical. In the following, we resrict level
k to be 1, and augment the matching graph Gσ,β with new nodes representing pairs
of composite classes and their level-1 component classes. For each composite class D
and component class C, we create a new node labeled with the concatenation D.C.
Likewise, we create a new node labeled with the concatenation B.A for composite
attribute B and composite attribute A. Formally, let Gaσ,β = (C a , RAa , E a ) be the bipartite such that C a = C ∪ {D.C | D, C ∈ C, C is a component class of D} , RAa =
RA ∪ {B.A | B, A ∈ RA, A is a component attribute of B}. The edge set E a is obtained by adding edge (D.C, B.A) to E for each new class D.C and new attribute
B.A satisfying σ(D.C, B.A) > β, and removing edge (C, A) from E where C and
A are component class and attributes, respectively. Here we remove the edge (C, A)
because it will be represented by the new component-level edge (D.C, B.A). We call
Gaσ,β a composite-augmented graph. Also, we call a bipartite matching M a on Gaσ,β an
augmented matching.
For a class C shared by composite classes D1 , . . . , Dm in Gσ,β , Gaσ,β has duplicated
nodes C, D1 .C, . . . , Dm .C. Thus an augmented matching M a can include one or more
nodes from C, D1 .C, . . . , Dm .C in its edges. The following realizes integrity of level-1
component classes in augmented matching:
(augmented component integrity) An augmented matching M a is said to satisfy augmented component integrity, if the following holds: Let (D.C, B.A) and (D1 , B1 )
be any pair of edges in M a such that D1 , C1 ∈ C, D.C ∈ C a , A, B ∈ RA, and
B.A ∈ RAa . Then D1 = D holds if and only if B1 = B holds.
To satisfy augmented component integrity, we divide the matching of PAO and RA
into two phases: First, we take each composite class D and each composite attribute
B and solve matching between the component classes and attributes of D and C,
and then augment the (linguistic) similarity score σ(D, B) with the matching score
(component-level matching). Secondly, we solve matching between the component
classes and component attributes using the augmented scores (composite-level matching). The matching algorithm PAOMatch is shown in Figure 4.

Risk Evaluation for Personal Identity Management

193

1. For each class D in C and for each attribute B in RA, compute augmented score
σ a (D, B) as follows:
1.1 If either D or B is not a composite class/attribute, then let σ a (D, B) = σ(D, B)
and goto Step 1.
1.2 /* Now D is a composite class and B is a composite attribute. */
Let Ci (i = 1, . . . , k) be the component classes of D. Let Aj (j = 1, . . . , m) be
the component attributes of B.
1.3 Let GDB be the bipartite graph such that its two node sets are {D.Ci } and
{B.Aj }, respectively, and each edge (D.Ci , B.Aj ) has augmented weight
σ a (D.Ci , B.Aj ). If σ a (D.Ci , B.Aj ) is undefined for some i and j, then recursively apply Step 1.1-1.4 to obtain σ a (D.Ci , B.Aj ).
1.4 Solve weighted bipartite matching on GDB to obtain matching MDB and its total
maximum weight wDB . Let σ a (D, B) = σ(D, B) + λ · wDB . Here, 0 < λ < 1
is a pre-defined damping factor.
2. /* Now σ a (D, B) is defined for each D and B. Note that Gσ a ,β does not include
augmented nodes. */
Solve weighted bipartite matching on Gσ a ,β , where edge weight σ is replaced by
σ a , and obtain matching M .
3. Construct solution matching M a as follows: For each matching edge (D, B) in M ,
add the matching MDB obtained at Step 1.3 to M .
Fig. 4. PAOMatch: Two-phased structural matching

Step 1 of PAOMatch computes maximum matching for each component classattribute pair. Then the resulting weight wDB is added to the linguistic similarity score
σ(D, B), to reflect structural similarity of the components of D and B (Step 1.4). Here,
damping factor 0 < λ < 1 is introduced to reflect the nesting level of component hierarchy. A component class or attribute far from its composite root will have a reduced
influence to the score.
After solving maximum matching for each composite class and each composite attribute, the top-level matching is carried out (Step 2). Here, we use Gaσ,β to exclude
component classes and component attributes, since component-level matching is already done at Step 1.
Figure 3 shows application of PAOMatch. Gray nodes are augmented nodes created for
each component class/attribute at Step 1.3 of PAOMatch. At Step 1.4, Component-level
matching is done between the augmented nodes of composite classes {blog account,
shopping} and attribute {contact}. Using the scores of these matchings, compositelevel matching is carried out (Step 2). In Figure 3, edge (blog account, contact) is
chosen as one of the four composite-level edges. Thus edges (blog account.email,
contact.e-mail) and (blog account.country, contact.country) are
added at Step 3, as the result of component-level matching. On the hand, although
component-level edges (shopping.email, contact.e-mail), (shopping.
address, contact.address) are matched at component-level matching, they are
eventually discarded because their parents shopping and contact are not matched.
Notice that blog name is matched to name at the composite level, not as the composite
class blog account.blog name.

194

M. Iwaihara et al.

Theorem 2. For a matching graph Gσ,β = (C, RA, E), let N be the number of nodes
and E be the number of partOf links in Gσ,β . Then PAOMatch returns a maximum
matching satisfying augmented component integrity in O(N 3 + E 3 ) time.
Proof. For augmented component integrity, suppose that augmented matching M a includes edges (D.C, B.A) and (D1 , B1 ) such that D1 , C1 ∈ C, D.C ∈ C a , A, B ∈ RA,
and B.A ∈ RAa . Now, assume that D1 = D holds. Since D.C is an augmented node,
the edge (D.C, B.A) must be added at Step 3 of PAOMatch as one of the edge in MDB .
Since matching at Step 2 guarantees that (D1 , B1 ) is the only edge in M a that is adjacent to D1 = D, the composite attribute B of MDB must be B1 . The only-if part can
be shown by a symmetric argument.
For the time bound, first consider Step 1 of PAOMatch. Let D be the set of composite
classes in C, and let B be the set of composite attributes in RA. Weighted bipartite
matching is executed at Step 1.4 for each D ∈ D and for each B ∈ B. Let |D| (resp.
|B|) denote the number of component classes of D (resp. component attributes of B).
3
Then one execution
+ |B|) ) time.
 The total time of Step 1.4
 of Step 1.4 takes O((|D|
3
is bounded by D∈D,B∈B (|D| + |B|) ≤ ( D∈D |D| + B∈B |B|)3 = E 3 . For Step
2, bipartite matching is performed on Gσa ,β , which has N nodes. Thus Step 2 takes
O(N 3 ) time. Step 3 can be done in O(N + E) time.



5.2 Combination Risk Class and Inhibitor
Now consider combination risk classes. A combination risk class Dr is a composite
class having component classes C1 , . . . , Ck , where Ci is a class in C or componentcomposite classes, and the risk values rf (Dr ) and rp (Dr ) are given. These risk values
are applied when and only when all of Dr ’s component classes are selected in an augmented matching M a . Thus combination risk classes can express high-risk combination
of privacy attributes.
Let us consider similarity score maximization where tolerable maximum limits are
imposed on f- and/or p-risk values, as we discussed in Section 4.1. If Dr exceeds the
risk limit, selecting all the component classes of Dr should be prohibited in the matching. Now let Dr be the subset of combination risk classes such that Dr ∈ Dr exceeds
a given risk limit. We need to design an algorithm that finds a maximum matching that
avoids selecting all the component classes for each Dr ∈ Dr . To solve this problem,
we introduce a combination inhibitor Inh(Dr ), which is a supplementary graph constructed by the algorithm CombInhibitor, shown in Figure 5.
Let us reconsider the running example, and assume that p-risk limit mp = 4 is
given. Then rn&bn is the only combination risk class in Figure 1 that should be inhibited. CombInhibitor adds a component inhibitor for rn&bn to the augmented matching graph. In Figure 3, the inhibitor node is labeled as !rn&ad. The combination inhibitor works as follows: The dashed edges attached to the inhibitor node have the
highest weight in the graph. Therefore, if both real name and shopping.address
are selected in a matching M , we can always make another matching M  by replacing one of the matching edges, say, the one adjacent to real name, with (real
name, !rn&ad). Then M  should have a score higher than M . Therefore maximum
matching will give us a solution that avoids simultaneously selecting real name and

Risk Evaluation for Personal Identity Management

195

For each combination risk class Dr ∈ Dr , do:
1. Add the following bipartite subgraph Inh(Dr ) = (Vh , Uh , Eh ) to the augmented
matching graph Gaσ,β (C, RA, E). Let C1 , . . . , Ck be the component classes of
Dr .
1.1 The node set Vh equals the component classes {C1 , . . . , Ck }, and the other node
set Uh equals the singleton set {Ah } containing a newly introduced inhibitor node
Ah .
1.2 The edge set Eh consists of k edges (C1 , Ah ), . . . , (Ck , Ah ), where each edge
has an equal weight wh such that wh is any fixed value higher than the maximum
similarity score found in Gσ,β (C, RA, E).
Fig. 5. CombInhibitor(Dr , Gaσ,β ): Adding combination inhibitors

shopping.address. Thus we have succeeded in preventing p-risk value from exceeding 4. Formally, we have the following property:
Theorem 3. Suppose that a matching graph Gσ,β = (C, RA, E) is augmented with the
combination inhibitor Inh(Dr ) for each Dr ∈ Dr , where Dr is a subset of combination
risk classes of Gσ,β . Then a maximum matching M of Gσ,β always includes an edge of
Inh(Dr ) for any Dr ∈ Dr . Thus there is no maximum matching that includes all the
component classes of Dr .
Proof. (omitted due to space limitation)
By the above theorem, just adding combination inhibitor Inh(Dr ) to the matching
graph can prevent application of the exceeded risk values of Dr . The supplementary
subgraphs introduced by combination inhibitors have a maximum total size equal to
the size of combination risk classes. Thus adding combination inhibitors multiplies the
graph size only by a constant factor. We also note that the total maximum weight includes the weight of inhibitor edges given by winh = (the number of inhibitors)* wh .
Thus we need to subtract winh from the matching weight wM to obtain the actual total
similarity score.
5.3 Finding Optimum Matching
We use the following monotonicity in matching solutions for searching on risk values.
Lemma 1. Let M1a be a matching of graph Gσ,β such that M1a satisfies risk limits rf
and rp . Then there is a matching M2a such that M2a satisfies risk limits rf > rf and
rp > rp , and σ a (M1a ) ≤ σ a (M2a ).
Proof. It is obvious that M1a remains a matching under the weaker limits of rf and rp .
Thus at least M1a satisfies the condition of M2a of the lemma.



Let F (resp. P ) be the number of distinct f-risk (resp. p-risk) values appearing in Gaσ,β .
If we are using 5-digit risk values, then we have F ≤ 5 and P ≤ 5. Figure 6 shows
matching algorithms for the optimization problems defined in Section 4.1.

196

M. Iwaihara et al.
Algorithm MaxSim(Gaσ,β , mf , mp ) /* Similarity score maximization under risk limit
*/
Input Augmented bipartite graph Gaσ,β , maximum f-risk mf , and maximum p-risk
mp , where mf and/or mf may be ∞. /*
Output Maximum augmented matching M a of Gaσ,β such that rf (M a ) ≤ mf and
rp (M a ) ≤ mp .
1. Remove from Gaσ,β classes C such that rf (C) > mf or rp (C) > mp .
2. Let Dr be the set of combination risk classes Dr such that rf (Dr ) > mf or
rp (Dr ) > mp holds. Execute CombInhibitor(Dr , Gaσ,β ).
3. Apply PAOMatch to Gaσ,β to obtain M a .
Algorithm MaxSimCombinedRisk(Gaσ,β , mc ) /* Similarity score maximization under
combined-risk limit */
Input: Augmented bipartite graph Gaσ,β , and maximum combined-risk mc .
of
Gaσ,β
such
that
Output: Maximum
augmented
matching
Ma
a
a
cr(rf (M ), rp (M )) ≤ mc .
1. Assume that F < P . Otherwise in the following steps swap F with P , and swap
mf with mp .
2. For each value rf in F do:
2.1 Compute maximum rp such that cr(rf , rp ) ≤ mc holds.
2.2 Call MaxSim(Gaσ,β , rf , rp ).
3. Report matching M a that had maximum score at 2.2
Algorithm MinRisk(Gaσ,β , wmin ) /* Risk minimization under minimum similarity
score wmin . */
1. Assume that F < P . Otherwise in the following steps swap F with P , and swap
mf with mp .
2. For each value rf in F do:
2.1 Perform binary search on p-values to find minimum rp such that result M a of
MaxSim(Gaσ,β , rf , rp ) has score no smaller than wmin .
3. Report the matching found in Step 2.1 such that its combined risk r is minimum.
Algorithm MaxCombined(Gaσ,β ) /* Combined score maximization. */
1. For each f-risk value pf and for each p-risk value pf , do:
1.1 Call MaxSim(Gaσ,β , rf , rp ).
2. Report matching M a that had maximum combined score σ(M a )/rc (M a ) in Step
1.1.
Fig. 6. Algorithms for maximum matching under given risk constraints

Theorem 4. Let F (resp. P ) be the number of distinct f-risk (resp. p-risk) values appearing in augmented matching graph Gaσ,β . Let R be F + P , and let N be the number
of nodes and E be the number of partOf links in Gaσ,β . The following holds:
1. MaxSim(Gaσ,β , mf , mp ) solves similarity score maximization in O(N 3 +E 3 ) time.
2. MaxSimCombinedRisk(Gaσ,β , mc ) solves similarity score maximization under
combined-risk limit mc in O((N 3 + E 3 )R) time.
3. MinRisk(Gaσ,β , wmin ) solves risk minimization under minimum similarity score
wmin in O((N 3 + E 3 )R log R) time.

Risk Evaluation for Personal Identity Management

197

4. MaxCombined(Gaσ,β ) solves combined score maximization in O((N 3 + E 3 )R2 )
time.
Proof. 1. In Step 1 of MaxSim(Gaσ,β , mf , mp ), classes C that violate the maximum
limit mf or mp are removed. If these classes C are not removed, it is easy to construct
a graph that has a maximum matching violating one of these limits. In Step 2, CombInhibitor introduces combination inhibitors so that by Theorem 3, any matching of Gaσ,β
will not include a combination risk class that violates the limits. If CombInhibitor is not
applied, it is easy to construct a graph that has a maximum matching that includes all
the component classes of a combination risk class which violates the limits. Thus the
matching obtained at Step 3 gives maximum score under the limits mf and mp . For the
time bound, Step 1 and Step 2 can be done in linear time and increase the size of Gaσ,β
by a factor of a constant. Thus by Theorem 2, Step 3 can be done in O(N 3 + E 3 ) time.
2. For similarity score maximization under combined-risk limit mc , testing maximum
matching score among every combination of risk values rf and rp that satisfy the limit
mc guarantees that there will be no other matching that has a higher score while satisfying mc . We do not need to test on combinations rf and rp which have combined risk
values less than mc , since by Lemma 1, matching score satisfying rf and rp does not exceed the score satisfying rf ≥ rf and rp ≥ rp such that cr(rf , rp ) ≤ cr(rf , rp ) ≤ mc .
For the time bound, MaxSim(Gaσ,β , rf , rp ) is called F ≤ R times, which gives the
bound O((N 3 + E 3 )R).
3. For risk minimization under minimum similarity score wmin , it is sufficient to test all
the combinations of f-risk and p-risk values that have matching M a such that σ a (M a ) ≥
wmin holds. Again by Lemma 1, if a combination of rf and rp has a matching score
greater than the minimum limit wmin , then all the combinations such that rf ≥ rf and
rp ≥ rp also have matching score greater than wmin . This property allows us to perform
binary search on rf for each fixed rp . Thus MaxSim(Gaσ,β , rf , rp ) is called O(R log R)
at Step 2.2 and we have the time bound.
4. For combined score maximization, again it is sufficient to test all the combinations
of f-risk and p-risk values to find a matching M a having maximum combined score
sc (M a ) = σ(M a )/rc (M a ). Since the combined score sc = w/r does not have mono


tonicity, we try MaxSim(Gaσ,β , rf , rp ) for O(R2 ) times.

6 Conclusion
In this paper, we proposed the concept of privacy attribute ontology for identity management involving complex attributes and identities. Our ontology model realizes risk
evaluation of matching attributes, and the algorithms presented in this paper solve maximum similarity matching under various types of risk constraints.

Acknowledgment
This work is in part supported by the Grant-in-Aid for Scientific Research of JSPS
(Japan Society for the Promotion of Science) (#18300031), and Strategic International
Cooperative Program of JST (Japan Science and Technology Agency). The work of

198

M. Iwaihara et al.

Gail-J. Ahn was partially supported by the grants from US National Science Foundation (NSF-IIS-0242393) and the US Department of Energy Early Career Principal
Investigator Award (DE-FG02-03ER25565).

References
1. Microsoft Developer Network (MSDN) CardSpace page,
http://msdn.microsoft.com/CardSpace
2. Cohen, W., Ravikumar, P., Feinberg, S.: A Comparison of String Metrics for Matching
Names and Records. In: Proc. KDD Workshop on Data Cleaning and Object Consolidation
(2003)
3. Garey, M.R., Johnson, D.S.: Computers and Intractability - A Guide to the Theory of NPCompleteness. Freeman, New York (1979)
4. Kagal, L., Finin, T.W., Joshi, A.: A Policy Based Approach to Security for the Semantic
Web. In: Fensel, D., Sycara, K.P., Mylopoulos, J. (eds.) ISWC 2003. LNCS, vol. 2870, pp.
402–418. Springer, Heidelberg (2003)
5. Jutla, D.N., Bodorik, P.: Sociotechnical Architecture for Online Privacy. IEEE Security &
Privacy 3(2), 29–39 (2005)
6. Japan Network Security Association, Surveys on Information Security Incidents (in
Japanese) (2006),
http://www.jnsa.org/result/2006/pol/insident/070720/
7. Kolari, P., Li Ding, S., Ganjugunte, L., Kagal, A.J., Finin, T.: Enhancing Web Privacy Protection through Declarative Policies. In: Proc. IEEE Workshop on Policy for Distributed
Systems and Networks(POLICY 2005) (June 2005)
8. Kuhn, H.W.: The Hungarian Method for the Assignment Problem. Naval Research Logistics
Quarterly 2, 83–97 (1955)
9. Li, W.-S., Clifton, C.: SEMINT: a Tool for Identifying Attribute Correspondences in Heterogeneous Database Using Neural Networks. Data Knowledge Eng. 33(1), 49–84 (2000)
10. Liberty Alliance Project Homepage, http://www.projectliberty.org/
11. OpenID Foundation, http://openid.net/
12. OWL Web Ontology Language Overview, W3C Recommendation 10 (February 2004),
http://www.w3.org/TR/owl-features/
13. Patel, C., Supekar, K., Lee, Y.: OntoGenie: Extracting Ontlogy Instances from WWW. In:
Proc. Huaman Language Technology for the Semantic Web and Web Services, ISWC 2003
(2003)
14. The Platform for Privacy Preferences 1.1 (P3P1.1) Specification, W3C Working Group Note
(November 13, 2006)
15. Udrea, O., Getoor, L., Miller, R.J.: Leveraging Data and Structure in Ontology Integration.
In: Proc. ACM SIGMOD 2007, pp. 449–460 (2007)
16. WordNet — a Lexical Database for the English Language, Princeton University,
http://wordnet.princeton.edu/

Automatic Composition of Semantic Web Services
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
The University of Texas at Dallas
Richardson, TX 75083

Abstract
Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services
automatically. For this automation to be effective, formal
semantic descriptions of Web services should be available.
In this paper we formally define the Web service discovery
and composition problem and present an approach for automatic service discovery and composition based on semantic
description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This
engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are
its scalability, i.e., its ability to handle very large service
repositories, and its extremely efficient processing times for
discovery and composition queries. We evaluate our engine
for automated discovery and composition on repositories of
different sizes and present the results.

1

Introduction

A Web service is a program accessible over the web that
may effect some action or change in the world (i.e., causes
a side-effect). Examples of such side-effects include a webbase being updated because of a plane reservation made
over the Internet, a device being controlled, etc. An important future milestone in the Web’s evolution is making services ubiquitously available. As automation increases, these
Web services will be accessed directly by the applications
rather than by humans [8]. In this context, a Web service
can be regarded as a “programmatic interface” that makes
application to application communication possible. An infrastructure that allows users to discover, deploy, synthesize
and compose services automatically is needed in order to
make Web services more practical.
To make services ubiquitously available we need a

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their discovery, deployment, composition and synthesis [3]. Informally, a service is characterized by its input
parameters, the outputs it produces, and the side-effect(s)
it may cause. The input parameter may be further subject
to some pre-conditions, and likewise, the outputs produced
may have to satisfy certain post-conditions. For discovery,
composition, etc., one could take the syntactic approach in
which the services being sought in response to a query simply have their inputs syntactically match those of the query,
or, alternatively, one could take the semantic approach in
which the semantics of inputs and outputs, as well as a semantic description of the side-effect is considered in the
matching process. Several efforts are underway to build an
infrastructure [17, 23, 15] for service discovery, composition, etc. These efforts include approaches based on the
semantic web (such as USDL [1], OWL-S [4], WSML [5],
WSDL-S [6]) as well as those based on XML, such as Web
Services Description Language (WSDL [7]). Approaches
such as WSDL are purely syntactic in nature, that is, they
only address the syntactical aspects of a Web service [14].
Given a formal description of the context in which a service is needed, the service(s) that will precisely fulfill that
need can be automatically determined. This task is called
discovery. If the service is not found, the directory can be
searched for two or more services that can be composed to
synthesize the required service. This task is called composition. In this paper we present an approach for automatic
discovery and composition of Web services using their semantic descriptions.
Our research makes the following novel contributions:
(i) We formally define the discovery and composition problems; to the best of our knowledge, the formal description of
the generalized composition problem has been given for the
first time; (ii) We present efficient and scalable algorithms
for solving the discovery and composition problem that take
semantics of services into account; our algorithm automatically selects the individual services involved in composition
for a given query, without the need for manual intervention;

Q

and, (iii) we present a prototype implementation based on
constraint logic programming that works efficiently on large
repositories.
The rest of the paper is organized as follows. Section
2 describes the two major Web services tasks, namely, discovery and composition with their formal definitions. In
section 3 and 4, we present our multi-step narrowing solution and implementation for automatic service discovery
and composition. Finally we present our performance results, related work and conclusions.

2

Automated Web service Discovery and
Composition

Discovery and Composition are two important tasks related to Web services. In this section we formally describe
these tasks. We also develop the requirements of an ideal
Discovery/Composition engine.

2.1

The Discovery Problem

Given a repository of Web services, and a query requesting a service (we refer to it as the query service in
the rest of the text), automatically finding a service from
the repository that matches the query requirements is the
Web service Discovery problem. Valid solutions to the
query satisfy the following conditions: (i) they produce
at least the query output parameters and satisfy the query
post-conditions; (ii) they use only from the provided input
parameters and satisfy the query pre-conditions; (iii) they
produce the query side-effects. Some of the solutions may
be over-qualified, but they are still considered valid as
long as they fulfill input and output parameters, pre/post
conditions, and side-effects requirements.
Example 1: Say we are looking for a service to buy a book
and the directory of services contains services S1 and S2 .
The table 1 shows the input/output parameters of the query
and services S1 and S2.
In this example service S2 satisfies the query, but
S1 does not as it requires BookISBN as an input but
that is not provided by the query. Our query requires
ConfirmationNumber as the output and S2 produces ConfirmationNumber and TrackingNumber. The extra output
produced can be ignored. Also the semantic descriptions
of the service input/output parameters should be the same
as the query parameters or have the subsumption relation.
The discovery engine should be able to infer that the query
parameter BookTitle and input parameter BookName of
service S2 are semantically the same concepts. This can be
inferred using semantics from the ontology provided. The
query also has a pre-condition that the CreditCardNumber
is numeric which should logically imply the pre-condition
of the discovered service.

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

CI’,I’

CO,O

CI,I

S

where CI’ ==> CI,
I’
I,

CO’,O’

CO ==> CO’,
O
O’

Figure 1. Substitutable Service
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions. S = (CI ; I ; A; AO; O; CO) is the representation of a service where CI is the pre-conditions, I
is the input list, A is the service’s side-effect, AO is the
affected object, O is the output list, and CO is the postconditions.
Definition (Repository of Services): Repository is a set of
Web services.
Definition (Query): The query service is defined as
Q = (CI 0; I 0; A0; AO0 ; O0; CO0 ) where CI 0 is the preconditions, I 0 is the input list, A0 is the service affect,
AO0 is the affected object, O0 is the output list, and CO0
is the post-conditions. These are all the parameters of the
requested service.
Definition (Discovery): Given a repository R and a query
Q, the Discovery problem can be defined as automatically
finding a set S of services from R such that S = fs j s =
(CI ; I ; A; AO ; O ; CO ), s 2 R, CI 0 ) CI , I v I 0 , A =
A0 , AO = AO0 , CO ) CO0 , O w O0 g. The meaning
of v is the subsumption (subsumes) relation and ) is the
implication relation. For example, say x and y are input
and output parameters respectively of a service. If a query
has (x > 5) as a pre-condition and (y > x) as postcondition, then a service with pre-condition (x > 0) and
post-condition (y > x) can satisfy the query as (x > 5) )
(x > 0) and (y > x) ) (y > x) since (x > 0).
In other words, the discovery problem involves finding
suitable services from the repository that match the query
requirements. Valid solutions have to produce at least those
output parameters specified in the query, satisfy the query
pre and post-conditions, use at most those input parameters that are provided by the query, and produce the same
side-effect as the query requirement. Figure 1 explains the
discovery problem pictorially.

2.2

The Composition Problem

Given a repository of service descriptions, and a query
with the requirements of the requested service, in case a
matching service is not found, the composition problem involves automatically finding a directed acyclic graph of services that can be composed to obtain the desired service.
Figure 2 shows an example composite service made up of

Service
Query

S1
S2

Input Parameters
BookTitle,CreditCardNumber,
AuthorName,CreditCardType
BookName,AuthorName
BookISBN,CreditCardNumber
BookName,
CreditCardNumber

Pre-conditions
IsNumeric(Credit
CardNumber)

Output Parameters
Post-Cond
ConfirmationNumber
ConfirmationNumber

IsNumeric(Credit
CardNumber)

ConfirmationNumber,
TrackingNumber

Table 1. Example Scenario for Discovery problem

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Figure 2. Example of a Composite Service as
a Directed Acyclic Graph

BookName,
AuthorName
GetISBN

NumAvailable

BookISBN

ConfNumber
PurchaseBook

AuthCode
CreditCardNum

AuthorizeCreditCard

Figure 3. Example of a Composite Service
five services S1 to S5 . In the figure, I 0 and C I 0 are the query
input parameters and pre-conditions respectively. O0 and
0
C O are the query output parameters and post-conditions
respectively. Informally, the directed arc between nodes Si
and Sj indicates that outputs of Si constitute (some of) the
inputs of Sj .
Example 2: Suppose we are looking for a service to
buy a book and the directory of services contains services
GetISBN, GetAvailability, AuthorizeCreditCard, and PurchaseBook. The table 2 shows the input/output parameters
of the query and these services.
Suppose the repository does not find a single service that
matches these criteria, then it synthesizes a composite service from among the set of services available in the repository. Figure 3 shows this composite service. The postconditions of the service GetAvailability should logically
imply the pre-conditions of service PurchaseBook.
Definition (Composition): The Composition problem can
be defined as automatically finding a directed acyclic graph
G = (V ; E ) of services from repository R, given query Q =
(CI 0; I 0; A0; AO0 ; O0; CO0 ), where V is the set of vertices
and E is the set of edges of the graph. Each vertex in the

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

S

S

GetAvailability
BookISBN

graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of
a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:
1. 8i Si 2 V where Si has exactly one incoming edge
that represents the query inputs and pre-conditions,
I 0 w i I i , CI 0 )^i CI i .
2. 8i Si 2 V where Si has exactly one outgoing edge
that represents the query outputs and post-conditions,
O0 v i Oi , CO0 (^iCO i .
3. 8i Si 2 V where Si has at least one incoming edge,
let Si1, Si2 , ..., Sim be the nodes such that there is
a directed edge from each of these nodes to Si . Then
Ii v
O [ I 0 , C I i ( (COi1^CO i2::: ^COim ^ C I 0).
k ik
The meaning of the v is the subsumption (subsumes) relation and ) is the implication relation. In other words, a
service at any stage in the composition can potentially have
as its inputs all the outputs from its predecessors as well as
the query inputs. The services in the first stage of composition can only use the query inputs. The union of the outputs
produced by the services in the last stage of composition
should contain all the outputs that the query requires to be
produced. Also the post-conditions of services at any stage
in composition should imply the pre-conditions of services
in the next stage.
Figure 4 explains one instance of the composition problem pictorially. When the number of nodes in the graph
is equal to one, the composition problem reduces to the
discovery problem. When all nodes in the graph have not
more than one incoming edge and not more than one outgoing edge, the problem reduces to a sequential composition
problem (i.e., the graph is a linear chain of services).

S

2.3

Requirements of an ideal Engine

Discovery and composition can be viewed as a single problem. Discovery is a simple case of composition
where the number of services involved in composition is
exactly equal to one. The features of an ideal Discovery/Composition engine are:
Correctness: One of the most important requirements for

Service

Input Parameters

Pre-Conditions

Query

Output
Params
ConfNumber

PurchaseBook

BookISBN
NumAvailable NumAvailable > 0
AuthCode
AuthCode > 99^
AuthCode < 1000
ConfNumber

BookTitle,CreditCardNum,
AuthorName,CardType
GetISBN
BookName,AuthorName
GetAvailability
BookISBN
AuthorizeCreditCard CreditCardNum
BookISBN, NumAvailable, AuthCode

NumAvailable > 0

Post-Conditions

Table 2. Example Scenario for Composition problem
These requirements have driven the design of our
semantics-based Discovery and Composition engine described in the following sections.

2.4

Figure 4. Composite Service
an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the
requirements of the query. Also, the engine should be able
to find all services that satisfy the query requirements.
Small Query Execution Time: Querying a repository of
services for a requested service should take a reasonable
amount of (small) time, i.e., a few milliseconds. Here we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental Updates: Adding or updating a service to an
existing repository of services should take a small amount
of time. A good Discovery/Composition engine should not
pre-process the entire repository again, rather incrementally
update the pre-processed data (indexes, etc.) of the repository for this new service added.
Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition
engine should be able to give results based on requirements
(minimize, maximize, etc.) over the costs. We can extend
this to services having an attribute vector associated with
them and the engine should be able to give results based on
maximizing or minimizing functions over this vector.

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

Semantic Description of Web Services

A Web service is a software system designed to support
interoperable machine-to-machine interaction over a network. It has an interface that is described in a machineprocessible format so that other systems can interact with
the Web service through its interface using messages. The
automation of Web service tasks (discovery, composition,
etc.) can take place effectively only if formal semantic descriptions of Web services are available. Currently, there
are a number of approaches for describing the semantics of
Web services such as OWL-S [4], WSML [5], WSDL-S [6],
and USDL [1].

3

A Multi-step Narrowing Solution

With the formal definition of the Discovery and Composition problem, presented in the previous section, one can
see that there can be many approaches to solving the problem. Our approach is based on a multi-step narrowing of the
list of candidate services using various constraints at each
step. In this section we discuss our Composition algorithm
in detail. As mentioned earlier, discovery is a simple case of
Composition. When the number of services involved in the
composition is exactly equal to one, the problem reduces
to a discovery problem. Hence we use the same engine for
both discovery and composition. We assume that a directory of services has already been compiled, and that this
directory includes semantic descriptions for each service.

3.1

The Service Composition Algorithm

For service composition, the first step is finding the set
of composable services. Using the discovery engine, individual services that make up the composed service can be
selected. Part substitution techniques [2] can be used to find
the different parts of a whole task and the selected services
can be composed into one by applying the correct sequence
of their execution. The correct sequence of execution can

be determined by the inputs, outputs, pre-conditions and
post-conditions of the individual services. That is, if a subservice S1 is composed with subservice S2 , then the postconditions of S1 must imply the pre-conditions of S2 . The
goal is to derive a single solution, which is a directed acyclic
graph of services that can be composed together to produce
the requested service in the query. Figure 6 shows a pictorial representation of our composition engine.
In order to produce the composite service which is the
graph, as shown in the example figure 2, we filter out services that are not useful for the composition at multiple
stages. Figure 5 shows the filtering technique for the particular instance shown in figure 2. The composition routine starts with the query input parameters. It finds all those
services from the repository which require a subset of the
query input parameters. In figure 5, C I ; I are the preconditions and the input parameters provided by the query.
S1 and S2 are the services found after step 1. O1 is the
union of all outputs produced by the services at the first
stage. For the next stage, the inputs available are the query
input parameters and all the outputs produced by the previous stage, i.e., I2 = O1 [ I . I2 is used to find services at
the next stage, i.e., all those services that require a subset
of I2. In order to make sure we do not end up in cycles,
we get only those services which require at least one parameter from the outputs produced in the previous stage.
This filtering continues until all the query output parameters are produced. At this point we make another pass in
the reverse direction to remove redundant services which
do not directly or indirectly contribute to the query output
parameters. This is done starting with the output parameters
working our way backwards.

I=I
CI, I

1

S1
S2
.
.

O1

I=IUO
2

1

1

S
.
.

O

2

3

I=IUO
3

2

2

O
S

3

I=IUO
4

3

4

.
.

3

S

O

4

O

5

.
.

Figure 5. Composite Service
Algorithm: Composition
Input: QI - QueryInputs, QO - QueryOutputs, QCI - PreCond, QCO - Post-Cond
Output: Result - ListOfServices
1. L NarrowServiceList(QI, QCI);
2. O GetAllOutputParameters(L);
3. CO GetAllPostConditions(L);
4. While Not (O w QO)
5.
I = QI [ O; CI QCI ^ CO;
6.
L’ NarrowServiceList(I, CI);
7. End While;
RemoveRedundantServices(QO, QCO);
8. Result
9.Return Result;

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

S1
Query
described
using
USDL
(S)

Infer
Sub-queries

.
.
.
Sn

Discovery Module
(Discovery Engine + Service
Directory + Term Generator)
S1

Composed Service

Pre-Cond(S)
S1
Pre-Cond( S)1

Post-Cond( S)1
Pre-Cond( S)2

S2

..........................

Sn

Composition Engine
(implemented using
Constraint Logic
Programming

................................. S n

Post-Cond( S)n
Post-Cond(S)

Figure 6. Composition Engine

4

Implementation

Our discovery and composition engine is implemented
using Prolog [11] with Constraint Logic Programming over
finite domain [10], referred to as CLP(FD) hereafter. In
our current implementation, we used semantic descriptions
written in the language called USDL [1]. The repository of
services contains one USDL description document for each
service. USDL itself is used to specify the requirements of
the service that an application developer is seeking.
USDL is a language that service developers can use to
specify formal semantics of Web services. In order to provide semantic descriptions of services, we need an ontology
that is somewhat coarse-grained yet universal, and at a similar conceptual level to common real world concepts. USDL
uses WordNet [9] which is a sufficiently comprehensive ontology that meets these criteria. Thus, the “meaning” of
input parameters, outputs, and the side-effect induced by
the service is given by mapping these syntactic terms to
concepts in WordNet (see [1] for details of the representation). Inclusion of USDL descriptions, thus makes services
directly “semantically” searchable. However, we still need
a query language to search this directory, i.e., we need a language to frame the requirements on the service that an application developer is seeking. USDL itself can be used as
such a query language. A USDL description of the desired
service can be written, a query processor can then search
the service directory for a “matching” service. Due to lack
of space, we do not go into the details of the language in
this paper. They are available in our previous work [2].
These algorithms can be used with any other Semantic
Web service description language as well. It will involve
extending our implementation to work for other description
formats, and we are looking into that as part of our future
work.
The software system is made up of the following components.
Triple Generator: The triple generator module converts

each service description into a triple. In this case, USDL
descriptions are converted to triples like:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions).
The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to the
side-effect. I is the list of inputs and O is the list of outputs.
Pre-Conditions are the conditions on the input parameters
and Post-Conditions are the conditions on the output parameters. Services are converted to triples so that they can
be treated as terms in first-order logic and specialized unification algorithms can be applied to obtain exact, generic,
specific, part and whole substitutions [2]. In case conditions on a service are not provided, the Pre-Conditions and
Post-Conditions in the triple will be null. Similarly if the
affect-type is not available, this module assigns a generic
affect to the service.
Query Reader: This module reads the query file and passes
it on to the Triple Generator. We use USDL itself as the
query language. A USDL description of the desired service
can be written, which is read by the query reader and converted to a triple. This module can be easily extended to
read descriptions written in other languages.
Semantic Relations Generator: We obtain the semantic
relations from the OWL WordNet ontology. OWL WordNet ontology provides a number of useful semantic relations like synonyms, antonyms, hyponyms, hypernyms,
meronyms, holonyms and many more. USDL descriptions
point to OWL WordNet for the meanings of concepts. A
theory of service substitution is described in detail in [2]
which uses the semantic relations between basic concepts of
WordNet, to derive the semantic relations between services.
This module extracts all the semantic relations and creates
a list of Prolog facts. We can also use any other domainspecific ontology to obtain semantic relations of concepts.
We are currently looking into making the parser in this module more generic to handle any other ontology written in
OWL.
The query is parsed and converted into a Prolog query that
looks as follows:
discovery(sol(queryService, ListOfSolutionServices).
The engine will try to find a list of SolutionServices that
match the queryService.
Composition Engine: The composition engine is written
using Prolog with CLP(FD) library. It uses a repository of
facts, which contains all the services, their input and output
parameters and the semantic relations between the parameters. The following is the code snippet of our composition
engine:
composition(sol(Qname, Result)) :dQuery(Qname, QueryInputs, QueryOutputs),
encodeParam(QueryOutputs, QO),
getExtInpList(QueryInputs, InpList),
encodeParam(InpList, QI),
performForwardTask(QI, QO, LF),

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

performBackwardTask(LF, QO, LR),
getMinSolution(LR, QI, QO, A), reverse(A, RevA),
confirmSolution(RevA, QI, QO), decodeSL(RevA, Result).

The query is converted into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate ’bagof’ to return all possible
ListOfServices that can be composed to get the requested
queryService.
Output Generator: After the Composition engine finds a
matching service, or the list of atomic services for a composed service, the results are sent to the output generator in
the form of triples. This module generates the output files
in any desired XML format.

5

Efficiency and Scalability Issues

In this section we discuss the salient features of our system with respect to the efficiency and scalability issues related to Web service discovery and composition problem. It
is because of these features, we decided on the multi-step
narrowing based approach to solve these problems and implemented it using constraint logic programming.
Correctness: Our system takes into account all the services that can be satisfied by the provided input parameters and pre-conditions at every step of our narrowing algorithm. So our search space has all the possible solutions.
Our backward narrowing step, which removes the redundant services, does so taking into account the output parameters and post-conditions. So our algorithm will always
find a correct solution (if one exists) in the minimum possible steps. The formal proof of correctness and minimality
is beyond the scope of this paper.
Pre-processing: Our system initially pre-processes the
repository and converts all service descriptions into Prolog
terms. The semantic relations are also processed and loaded
as Prolog terms in memory. Once the pre-processing is
done, then discovery or composition queries are run against
all these Prolog terms and hence we obtain results quickly
and efficiently. The built-in indexing scheme and constraints in CLP(FD) facilitate the fast execution of queries.
During the pre-processing phase, we use the term representations of services to set up constraints on services and the
individual input and output parameters. This further helped
us in getting optimal results.
Execution Efficiency: The use of CLP(FD) helped significantly in rapidly obtaining answers to the discovery and
composition queries. We tabulated processing times for different size repositories and the results are shown in Section
6. As one can see, after pre-processing the repository, our
system is quite efficient in processing the query. The query
execution time is insignificant.

Programming Efficiency: The use of Constraint Logic
Programming helped us in coming up with a simple and
elegant code. We used a number of built-in features such as
indexing, set operations, and constraints and hence did not
have to spend time coding these ourselves. This made our
approach efficient in terms of programming time as well.
Not only the whole system is about 200 lines of code, but
we also managed to develop it in less than 2 weeks.
Scalability: Our system allows for incremental updates on
the repository, i.e., once the pre-processing of a repository is
done, adding a new service or updating an existing one will
not need re-execution of the entire pre-processing phase.
Instead we can easily update the existing list of CLP(FD)
terms loaded in the memory and run discovery and composition queries. Our estimate is that this update time will
be negligible, perhaps a few milliseconds. With real-world
services, it is likely that new services will get added often
or updates might be made on existing services. In such a
case, avoiding repeated pre-processing of the entire repository will definitely be needed and incremental update will
be of great practical use. The efficiency of the incremental
update operation makes our system highly scalable.
Use of external Database: In case the repository grows
extremely large in size, then saving off results from the preprocessing phase into some external database might be useful. This is part of our future work. With extremely large
repositories, holding all the results of pre-processing in the
main memory may not be feasible. In such a case we can
query a database where all the information is stored. Applying incremental updates to the database is easily possible
thus avoiding recomputation of pre-processed data .
Searching for Optimal Solution: If there are any properties with respect to which the solutions can be ranked,
then setting up global constraints to get the optimal solution is relatively easy with the constraint based approach.
For example, if each service has an associated cost, then the
discovery and the composition problem can be redefined to
find the solutions with the minimal cost. Our system can be
easily extended to take these global constraints into account.

6

Repository Size
(num of
services)
2000
2000
2000
2500
2500
2500
3000
3000
3000

Number
of I/O
parameters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.5
45.8
57.8
47.7
58.7
71.6
56.8
77.1
88.2

Query
Exec
Time
(msecs)
1
1
2
1
1
2
1
1
3

Incremental
update
(msecs)
18
23
28
19
23
29
19
26
29

Table 3. Performance on Discovery Queries

found is that the repository was cached after the first run
and that explained the difference in the pre-processing time
for subsequent runs. Table 3 shows performance results of
our Composition algorithm on discovery queries and table 4
shows results of our algorithm on composition queries. The
times shown in the tables are the wall clock times. The actual CPU time to pre-process the repository and execute the
query should be less than or equal to the wall clock time.
The results are plotted in figure 8. The graphs exhibit behavior consistent with our expectations: for a fixed repository size, the preprocessing time increases with the increase
in number of input/output parameters. Similarly, for fixed
input/output sizes, the preprocessing time is directly proportional to the size of the repository. However, what is surprising is the efficiency of service query processing, which
is negligible (just 1 to 3 msecs) even for complex queries
with large repositories.

Performance

We used repositories from WS-Challenge website[13],
slightly modified to fit into USDL framework. They provide repositories of various sizes (thousands of services).
These repositories contain WSDL descriptions of services.
The queries and solutions are provided in an XML format. The semantic relations between various parameters
are provided in an XML Schema file. We evaluated our
approach on different size repositories and tabulated Preprocessing and Query Execution time. We noticed that there
was a significant difference in pre-processing time between
the first and subsequent runs (after deleting all the previous pre-processed data) on the same repository. What we

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

Figure 7. Performance on Discovery Queries

Figure 8.
Queries
Repository Size
(num of
services)
2000
2000
2000
3000
3000
3000
4000
4000
4000
Table 4.
Queries

7

Performance

Number
of I/O
parameters
4-8
16-20
32-36
4-8
16-20
32-36
4-8
16-20
32-36

PreProcessing
Time
(secs)
36.1
47.1
60.2
58.4
60.1
102.1
71.2
87.9
129.2

Performance

on

Composition

Query
Exec
Time
(msecs)
1
1
1
1
1
1
1
1
1
on

Incremental
update
(msecs)
18
23
30
19
20
34
18
22
32

Composition

Related Work

Composition of Web services has been active area of research recently [14, 15, 23, 18]. Most of these approaches
are based on capturing the formal semantics of the service
using an action description languages or some kind of logic
(e.g., description logic). The service composition problem
is reduced to a planning problem where the sub-services
constitute atomic actions and the overall service desired is
represented by the goal to be achieved using some combination of atomic actions. A planner is then used to determine
the combination of actions needed to reach the goal. With
this approach an explicit goal definition has to be provided,
whereas such explicit goals are usually not available [17].
To the best of our knowledge, most of these approaches that
use planning are restricted to sequential compositions (i.e.,

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

a linear chain of services), rather than a directed acyclic
graph. Our approach automatically selects atomic services
from a repository and produces the composition flow in the
form of a directed acyclic graph.
The authors in [19, 20] present a composition technique
by applying logical inferencing on pre-defined plan templates. Given a goal description, they use the logic programming language Golog to instantiate the appropriate plan for
composing Web services. This approach also relies on a
user-defined plan template which is created manually.
There are industry solutions based on WSDL and
BPEL4WS where the composition of the flow is obtained
manually. A comparison of the approaches based on AI
planning techniques and approach based on BPEL4WS is
presented in [17]. This work shows that in both these approaches, the flow of the composition is determined manually. They do not assemble complex flows of atomic services based on a search process. They select appropriate
services using a planner when an explicit flow is provided.
In contrast, we have shown a technique to automatically determine these complex flows using semantic descriptions of
atomic services.
A process-level composition solution based on OWL-S
is proposed in [21]. In this work the authors assume that
they already have the appropriate individual services involved in the composition, i.e., they are not automatically
discovered. They use the descriptions of these individual
services to produce a process-level description of the composite service. They do not automatically discover/select
the services involved in the composition, but instead assume
that they already have the list of atomic services. In contrast, we automatically find the services that are suitable for
composition based on the query requirements for the new
composed service.
In [22], a semi-automatic composition technique is presented in which atomic services are selected for each stage
of composition. This selection process involves decision
making by a human controller at each stage, i.e., the selection process requires some manual intervention.
Another related area of research involves message conversation constraints, also known as behavioral signatures
[16]. Behavior signature models do not stray far from the
explicit description of the lexical form of messages, they
expect the messages to be lexically and semantically correct prior to verification via model checking. Hence behavior signatures deal with low-level functional implementation constraints, while our approach deals with higher-level
real world concepts. However, both these approaches can
be regarded as complementary concepts when taken in the
context of real world service composition, and both technologies are currently being used in the development of a
commercial services integration tool.
Our most important, novel contribution in this paper is

our technique for automatically selecting the services that
are suitable for obtaining a composite service, based on the
user query requirements. As far as we know, all the related approaches to this problem assume that they either already have information about the services involved or use
human input on what services would be suitable for composition. Our technique also handles non-sequential compositions (i.e., composition where there can be more than
one service involved at any stage, represented as a directed
acyclic graph of services) rather than sequential composition (i.e, a linear chain of services) which is the case with
most of the existing approaches.

8

Conclusions and Future Work

To catalogue, search and compose Web services in a
semi-automatic to fully-automatic manner we need infrastructure to publish Web services, document them, and query
them for matching services. Our semantics-based approach
uses semantic description of Web services (example USDL
descriptions). Our composition engines find substitutable
and composite services that best match the desired service.
Given semantic description of Web services, our engine produces optimal results (based on criteria like cost of services,
number of services in a composition, etc.). The composition flow is determined automatically without the need for
any manual intervention. Our engine finds any sequential
or non-sequential composition that is possible for a given
query. We are able to apply many optimization techniques
to our system so that it works efficiently even on large
repositories. Use of Constraint Logic Programming helped
greatly in obtaining an efficient implementation of this system.
Our future work includes extending our engine to work
with other web services description languages like OWLS, WSML, WSDL-S, etc. This should be possible as long
as semantic relations between concepts are provided. It
will involve extending the TripleGenerator, QueryReader,
and SemanticRelationsGenerator modules. We would also
like to extend our engine to support an external database to
save off pre-processed data. This will be particularly useful when service repositories grow extremely large in size
which can easily be the case in future. Future work also
includes developing an industrial-strength system based on
the research reported in this paper, in conjunction with a
system that allows (semi-) automatic generation of USDL
descriptions from code and documentation of a service [24].

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In ECOWS, pp. 214-225, 2005.
[2] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta, and
T. Hite. USDL: A Service-Semantics Description Lan-

2007 IEEE International Conference on Web Services (ICWS 2007)
0-7695-2924-0/07 $25.00 © 2007

guage for Automatic Service Discovery and Composition. Tech. Report UTDCS-18-06. www.utdallas.
edu/˜sxk038200/USDL.pdf.
[3] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems, pp. 46-53, Mar. ’01.
[4] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[5] WSML: Web Service Modeling Language. www.
wsmo.org/wsml/.
[6] WSDL-S: Web Service Semantics. http://www.
w3.org/Submission/WSDL-S.
[7] WSDL: Web Services Description Language. http:
//www.w3.org/TR/wsdl.
[8] A. Bansal, K. Patel, G. Gupta, B. Raghavachari, E. Harris, and J. Staves. Towards Intelligent Services. In
ICWS, pp 751-758, ’05.
[9] OWL WordNet http://taurus.unine.ch/
knowler/wordnet.html.
[10] K. Marriott, P. Stuckey. Prog. with Constraints: An
Introduction. MIT Press, ’98.
[11] L. Sterling, S. Shapiro. The Art of Prolog. MIT Press.
[12] OWL: Web Ontology Language Reference. http:
//www.w3.org/TR/owl-ref.
[13] WS Challenge 2006. http://insel.flp.cs.
tu-berlin.de/wsc06.
[14] U. Keller, R. Lara, H. Lausen, A. Polleres, D. Fensel.
Automatic Location of Services. In ESWC, ’05.
[15] S. Grimm, B. Motik, and C. Preist Variance in eBusiness Service Discovery. In Workshop at ISWC, ’04.
[16] R. Hull and J. Su. Tools for design of composite Web
services. In SIGMOD, ’04.
[17] B. Srivastava, J. Koehler. Web Services Composition
- Current Solutions and Open Problems. In ICAPS, ’03.
[18] B. Srivastava. Automatic Web Services Composition
using planning. In KBCS, pp 467-477.
[19] S. McIlraith, T.C. Son Adapting golog for composition of Web services. In KRR, pp 482-493, ’02.
[20] S. McIlraith, S. Narayanan Simulation, verification
and automated composition of services. In WWW, ’02.
[21] M. Pistore, P. Roberti, P. Traverso Process-Level
Composition of Executable Services In ESWC, pp 6277, ’05.
[22] E. Sirin, J. Hendler, and B. Parsia Semi-automatic
Composition of Web Services using Semantic Descriptions In Workshop at ICEIS, ’02 .
[23] M. Paolucci, T. Kawamura, T. Payne, and K. Sycara
Semantic Matching of Web Service Capabilities. In
ISWC, pp 333-347, ’02.
[24] Metallect IQ Server. http://metallect.com/
downloads/Metallect_Product_Brief_
IQServer.pdf

2017 IEEE 11th International Conference on Semantic Computing

Linking and maintaining quality of data about
MOOCs using Semantic Computing
Chinmay Dhekne

Srividya K. Bansal

Arizona State University
School of Computing, Informatics, and Decision Systems
Engineering, Mesa AZ 85212
cdhekne@asu.edu

Arizona State University
School of Computing, Informatics, and Decision Systems
Engineering, Mesa AZ 85212
srividya.bansal@asu.edu
this integrated solution in place, maintaining the highest level
of data quality is still a concern. Especially in the field of
online education, where course offerings are dynamic, it is all
the more important that relevant data is being displayed to the
end users at all times. The focus of this paper is to “devise
algorithms to maintain linked data quality (in the domain of
online education) and evaluate the approach through the
implementation of MOOCLink web application which serves
as an aggregator of all courses that are available”. This project
focuses primarily on integrating data from 5 different course
providers, publishing it as linked data and maintaining the
quality of the linked data generated. Specifically, this paper
makes the following novel contributions:
• Devised techniques to extract data from 5 different MOOC
providers that includes dynamic (with a specific start and end
date) as well as static (can be accessed anytime) courses.
• Updated the existing semantic data model (OWL ontology)
for the integrated datasets.
• Published the integrated data as linked open data on the web.
• Designed and implemented algorithms to maintain the
quality of the generated linked data by updating outdated and
irrelevant data and addition of new data.
• Extended the implementation of MOOCLink [2] web
application to include the generated linked data, search
algorithms (using SPARQL queries), and front-end User
interface of the application.
• Experiments to evaluate the algorithms and MOOCLink tool.
The rest of the paper is organized as follows: Related work is
presented in section 2. The semantic data model for
MOOCLink is presented in section 3. Section 4 presents the
algorithms for maintaining data quality followed by
experimental results and conclusions.

Abstract— Emergence of Linked Data has made it possible to
make sense of huge data that is scattered all over the web, and
link data from multiple heterogeneous sources. This leads to the
challenge of maintaining the quality and integrity of Linked
Data, i.e., ensuring outdated data is removed and latest data is
included. The focus of this paper is devising strategies to
effectively integrate data from multiple sources, publish it as
Linked Data, and maintain the quality of Linked Data. The
domain used in the study is online education. We present the
integration of data from various MOOC providers and
algorithms for incrementally updating linked data to maintain
their quality in order to constantly keep the users engaged with
up-to-date data. Experimental results of the evaluation of the
algorithms are presented.
Keywords— ontology design; data quality; semantic web;
semantic querying.

I.

INTRODUCTION

The development of semantic web technologies has resulted
into huge amounts of data being published on the Linked Open
Data (LOD) cloud, a community effort to make data available.
A survey in 2012 shows that around 50 billion facts were
represented as RDF triples and published on the cloud. Even
though gathering such massive amount of heterogeneous data
and publishing it on the LOD is a good step, the data is just as
good as its quality [1]. Online education is slowly gaining
widespread importance. According to the New York Times,
2012 became “the year of the MOOC”, as various providers
such as EdX, Udacity, Coursera, etc. emerged. Due to this,
courses with similar syllabi and content are offered by multiple
providers. The user is spoilt for choices in terms of which
online course to choose, as there is such a fine line between
courses by different providers. The user has to scan through
multiple providers each with their own webpage and browse
through all their course offerings. Also, in many cases, it so
happens that the users want to learn a specific topic within a
domain. In such a scenario, it becomes increasingly difficult
for the user to look through the syllabus of all courses offered
by different providers before arriving at a decision as to which
provider to go ahead with. There exists initial work in this area
in the form of a web application called MOOCLink [2]. With
MOOCLink, the user can make an informed decision by just
visiting the MOOCLink Website and using the enhanced
SPARQL search engine, he/she can quickly compare courses
offered by various MOOC providers, all in the one place. The
aim of this web app is to help users solve the problem of
looking through multiple course providers. However, even with

978-1-5090-4284-5/17 $31.00 © 2017 IEEE
DOI 10.1109/ICSC.2017.100

II.

RELATED WORK

There are two aspects of Linked Data quality that need to be
studied and strategies devised. The first aspect is to maintain
data quality in terms of adding or replacing missing
information, coupling and de-coupling of wrong relationships
amongst data entities. The second aspect of maintaining data
quality is by making sure that the data published on the Web,
is up to-date. Approaches are developed to ensure that there’s
no outdated copy of data. Lots of research and development
has been made for the first aspect of data quality. The paper
“Crowdsourcing Linked Data Quality Assessment” explains
the first aspect in detail [3]. This research focuses on using
crowdsourcing as a means to maintain data quality. Analysis
81

of the most commonly occurring problems was done and a
model was designed which aimed to eliminate these issues.
This model consisted of two different approaches. First, a
contest which targets an expert crowd of Linked Data
researchers; Second, by means of paid microtasks published
on Amazon Mechanical Turk. The dataset for this research
was DBpedia and these two novel approaches were evaluated
against the data published on DBpedia. The results show that
crowdsourcing is an effective mechanism to eliminate data
quality issues, and this approach could potentially be included
in the Linked Data curation process.
Another research named “Test-driven Evaluation of Linked
Data Quality” aims to solve data quality issue using testdriven software development [4]. This research proposes that
vocabularies, ontologies, datasets should be accompanied by
test-cases, which help in maintaining a basic level of data
quality. MOOCLink differentiates itself from other MOOC
aggregators by using semantic web technologies that allows
for publishing of the integrated MOOC datasets as Linked
Open Data on the cloud. This opens up the data to the
community as a whole and can be tweaked and modified as
required by the community for other research as well. Another
aspect which sets MOOCLink apart from the above listed
MOOC aggregators is the power of MOOCLink to be able to
search on a number of search parameters. MOOCLink
supports usage of parameters such as “Search by Name”,
“Search by Category”, “Search by Course Type (Free/Paid)”,
“Search by Course Provider”, “Search by Start/End Date”,
“Search by Course Description”.
III.

may belong to multiple categories. Similar relationships are
found in the other classes as well.

Figure 1: Semantic Data Model for MOOCLink

IV.

MAINTAINING QUALITY OF LINKED DATA

The web of today is extremely dynamic and information
presented and viewed by the user at a point of time may
become irrelevant within a day or even earlier than that. To be
able to cope up with the dynamic nature of data, we need
strategies so that the users are interacting with the latest copy
of data. MOOCLink application is a MOOC aggregator that
must provide users latest data on course offerings. Data was
collected from various providers such as Coursera, edX,
Udacity, Khan Academy, and OpenCourseWare in various
formats such as JSON, XML, and Excel. Linked data was
generated by converting the raw data captured from MOOC
providers into RDF data (in TTL format) compliant with the
ontology, creating and maintaining a SPARQL endpoint (using
Apache Jena Fuseki Server), and finally executing SPARQL
queries on the generated RDF data. A general architecture of
the MOOCLink application is shown in Figure 1.

SEMANTIC DATA MODEL GENERATION

Previous work on MOOCLink involved creation of an
ontology as an extension to Schema.org’s ontology. However
recently, schema.rdfs.org website was no longer maintained
and hence the MOOCLink ontology import from
schema.rdfs.org was no longer possible. Several changes had
to be made to the existing MOOCLink Ontology as described
below. Firstly, all imports from schema.rdfs.org had to be
removed. This was a considerable challenge as Creative Work,
which was Schema.org’s type for creative general creative
work in the field of education amongst others, was not present
at all. This meant that the entire structure had to be remodeled
from scratch. Also, apart from the remodeling, there were
some additions and deletions that had to be made to the new
ontology in order to accommodate two more data sources, i.e.
Khan Academy and Open Courseware (OCW). For example,
there was no provision for describing a particular course in
brief in the original ontology. This was added as Course_Desc
in the new ontology. Figure 2 shows the structure of the new
ontology. As shown in the figure, each class has certain data
properties as well as object properties. There is a one-to-many
relationship between all the classes. For example, if one
considers Category and Course class, a one-to-many
relationship exists between them and is depicted as
“includes_course” and “belongs_To_Category”. This means
that for each category, there are many courses and each course

Figure 2: MOOCLink - High-level Architecture

We propose two algorithms for maintaining Linked Data
Quality in this project: (i) Naïve Batch Update; (ii)
Incremental Update.
1) Naïve Batch Update Algoirthm
This approach is a simplistic and traditional way of
maintaining data quality. Once new data is extracted from a
MOOC provider, it is converted into an appropriate JSON
format. The next step is to locate the previous copy of JSON
data on the server. Once the file is located, it is replaced by the
new JSON file. The next step is to convert this “new” but raw

82

server and/or how much amount of data can be downloaded
using the services of your Internet Service Provider (ISP).
While the latter is not much of a concern in an industry
setting, the former is a real-world issue. Coming up with a
provider score for each MOOC provider will ensure that if
there arises a bandwidth constraint, then the system is in a
position to target only those MOOC providers that have a
greater provider score than the others. This will, in turn, make
sure that maximum numbers of data updates are done in each
run of the check for Data Quality. Provider score is calculated
based on three factors: (i) Size of data; (ii) Number of Courses
added or deleted; (iii) Number of Course details modified. The
next parameter is the “threshold”. This parameter will decide
when to use the Naïve batch update and when to use the
Incremental update. The threshold value is defined as the
“percentage change at which one approach becomes better
than the other in terms of efficiency”.
A master threshold value was evaluated after extensive
experimentation on the data sets. The process of evaluation of
the master threshold value is presented in the next section.
Once the master value is obtained, it is compared with the
change percentage of the current run. If the change percentage
is less than the master threshold value, then the “Incremental
Update approach” is used. On the other hand, if the change
percentage is greater than the master threshold value, then the
“Naïve approach” is used.

JSON data into RDF/TTL file format. This is a crucial step
because all SPARQL queries will be executed on the
RDF/TTL data, and not the raw JSON data. Once the file is
converted to RDF/TTL file format, it completely replaces the
old RDF/TTL file residing on the Apache Jena Fuseki Server.
Once this change is done, the same process is followed for
other MOOC providers.
2) Incremental Update Approach
The difference between Naïve Batch update and Incremental
update is that as opposed to the former, number of factors are
taken into consideration before obtaining updated Linked
Data. Figure 6 depicts the working of this approach. In the
first step of the incremental update approach, the original
JSON file residing on the server is compared with the new
JSON file. This is done using Hash Maps, as this data
structure fits in perfectly to satisfy the requirement and also
has a faster retrieval time (O(1)). Two levels of Hash Maps are
created, i.e. one on the class level, and the other on the
property level of a class. A “Course Change Counter” and
“Course Add/Delete Counter” is maintained to be incremented
whenever a course change or course addition/deletion is
encountered between the two sets of data respectively. These
counter variables are also used in the “provider score” module
that is described later in this section. The next step of this
process is to update the changes to the original JSON file. The
old JSON file is parsed and the changes, which are again
captured in the Hash Map data structure in the previous step,
are updated in the original JSON file. The last two steps of the
Incremental update approach is same as the Naïve batch
update, that is conversion to RDF/TTL and reflecting the
changes on the web application.
3) Decision Maker Module
The Decision maker module is responsible to identify which
approach to choose from the above mentioned approaches.
This decision is based on a “Threshold” value that is
calculated based on a number of factors. Figure 3 illustrates
the working of the “decision maker” module. This module is
responsible to determine whether data updates are to be done
using approach 1, i.e. the Naïve Batch update or approach 2,
i.e. the Incremental update. While comparing the new data and
the old data, the system is looking to obtain two parameters
from it: (i) Percentage change; (ii) Provider Score. The
“percentage change” parameter tells how much percentage of
data has been changed in the new data file as compared to the
older version of data. For example, if 100 courses are retrieved
in the first run, and in the next run, 100 courses are retrieved
but it is found that out of the new 100 courses, 36 courses
have changes in them. These changes may be changes to
“course name”, “course description”, “category”, etc. This
shows that the percentage change is 36%. It also signifies that
the remaining 64 courses are unchanged.
The “provider score” parameter helps to rank various
MOOC providers in descending order of their score. This
parameter is helpful in the real-world where unlimited amount
of bandwidth is not a reality. There are bandwidth limitations
in terms of how much amount of data can reside on the system

Figure 3: Provider score

4) Web Application Development
MOOCLink Web application is hosted on Apache Tomcat 8
Server. The web application GUI is developed in Javascript
and HTML with Bootstrap 3.0 library. The most important
feature of the MOOCLink application is the “Search” bar. The
user can search for any course or even keywords and the
underlying SPARQL query engine will search throughout the
RDF triples and return a list of courses with an exact or at
least a partial match with the keyword.
V.

EXPERIMENTAL RESULTS

In order to better evaluate the results obtained through
MOOCLink, “precision” and “recall” values of the results
were used. By definition, precision is defined as “the fraction
of documents retrieved that are relevant to the user’s
information need”[11]. On the other hand, Recall is defined as
“the fraction of documents that are relevant to the query and
are successfully retrieved” [11]. In reference to this project,
precision would give us the percentage of courses that are
retrieved by the MOOCLink system and are relevant to the

83

information queried by the user. Recall would give us the
percentage of courses that are relevant to the user query and
are retrieved using MOOCLink. The table 1 presents a
summary of the data loaded into MOOCLink.

authenticty of results retrieved by MOOCLink. In case of
Coursera and OCW, the recall is comparitively low. A major
reason for this is the non-availability of access to the entire
course catalog via the API’s provided by the respective
MOOC providers.

Table 1: Linked data in MOOCLink

VI.

CONCLUSIONS AND FUTURE WORK

This study explores the need for maintaining data quality,
and establishes an approach to handle this problem in the
domain of online education. The algorithms presented can be
applied to data in other domains as well. These techniques
thereby contribute to the community effort of Linked Open
data for data interchange on the web and facilitate
development of intelligent data-driven applications. This study
uses two approaches for improving linked data quality for the
MOOCLink application. Currently, the system supports
integration of data from five MOOC providers, i.e. Coursera,
edX, Udacity, Khan Academy and OCW supported by a
semantic data model implemented as ontology. Future work
includes incorporation of more MOOC providers. In addition
to this, strategies can be devised that will allow extracting and
integrating data from other providers as well. Also as part of
future work, a new feature can be introduced in the system for
“Course Comparison”. This would be visual comparison of
courses from different MOOC providers to better illustrate the
similarities and differences. For the data quality aspect of the
project, more work can be done to automate the maintenance
of data quality. This would mean writing scripts or Cron jobs
(time-based scheduler jobs) that would run at a set interval of
time and pull data from the listed MOOC providers and
possibly even more.

In order to find the “master threshold value” where one
approach becomes efficient than the other, the changes made
to the data were broken down into four parts and four
experiments were conducted. In the first part, only 10% of
data was changed as compared to the original data. The
second part consisted of 50% changes to the data, the third
part consisted of 60% changes to the data, and the fourth one
had 90% changes to the data when compared to the original
data from the MOOC providers. For each set of changes, the
Incremental update approach was applied and the time
required for the algorithm to parse and update the original data
file from each MOOC provider was noted. At the same time,
the Naïve Batch approach was applied, wherein replacing new
data from each MOOC provider is done. Again, the time
required for this algorithm to replace the old JSON file with a
fresh copy for each MOOC provider was noted. In an effort to
be certain that all files are being downloaded at the same
download speed, a third party application called “Net Limiter”
was used to keep the download speed uniform for each run of
the experiment for both approaches. Continuous
implementation of both algorithms was done for about a week
and the results of the experiments (time in milliseconds) for
the Naïve Batch update is shown in table 2. This time includes
the time to download the new data file using API’s of each
MOOC provider, conversion of raw data to human readable
JSON format, and replacing the old JSON file with the newly
obtained and processed JSON data file. Table 3 shows the
results of the experiments for the Incremental update
approach.

REFERENCES
[1] Zaveri, A., et al. (2012). Quality Assessment Methodologies for
Linked Open Data: A Systematic Literature Review and
Conceptual Framework. Journal of Semantic Web –
Interoperability, Usability, Applicability, pp. 1 - 33.
[2] Kagemann, S., & Bansal, S. (2015). MOOCLink: Building and
utilizing linked data from Massive Open Online Courses. Intl.
Conference on Semantic Computing (ICSC), pp. 373–380.
[3] Acosta, M., Zaveri, A., Simperl, E., Kontokostas, D., Auer, S., &
Lehmann, J. (2013). Crowdsourcing Linked Data Quality
Assessment. Intl. Semantic Web Conference, pp. 260–276.
[4] Kontokostas, D., & Westphal, P. (2014). Test-driven evaluation
of linked data quality. WWW '14, pp. 747–757.
[5] Coursera API:
https://api.coursera.org/api/catalog.v1/courses?fields=id,startDate
[6] EdX API: https://www.edx.org/api/v2/report/course-feed/rss
[7] Udacity’s API endpoint is (https://www.udacity.com/publicapi/v1/courses
[8] Academy’s API: http://www.khanacademy.org/api/v1/topictree
[9] OCW API: https://github.com/ocwc/ocwc-data
[10] Szekely, P., et al. (2013). Connecting the Smithsonian American
Art Museum to the Linked Data Cloud. The Semantic Web:
Semantics and Big Data: 10th International Conference, ESWC
2013, Montpellier, pp. 593–607.
[11] Zhu, M. (2004). Recall, Precision and Average Precision.
Retrieved from
http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingP
apers/2004-09.

Table 2: Results for Naive Batch update approach

Table 3: Results of Incremental update approach

The precision and recall results for search queries on most
MOOC providers are good. This, in turn, proves the

84

ScrumTutor: A Web-based Interactive Tutorial For
Scrum Software Development
Sindhura Potineni, Srividya K Bansal, Ashish Amresh
Department of Engineering
Arizona State University – Poly Campus
Mesa, AZ 85212 USA
{sindhura.potineni, srividya.bansal, amresh}@asu.edu
Abstract— In a traditional software engineering class,
students are typically engaged in theoretical lectures
followed by homework assignments or a project. Use of
hands-on training and laboratory activities using realworld projects is more likely to produce students with a
higher level of achievement and more confidence in the
course material. If every topic or technique introduced in
the course has a corresponding hands-on activity that
demonstrates an application or use of the concept in the
industry, students better understand the need for the
technique and the learning environment is more
interactive, engaging, and interesting to students. This
paper presents a project called ScrumTutor that aims at
providing an engaging learning experience of Scrum
Software development process through a web-based
Interactive tutorial. It is designed and developed for
undergraduate students in introductory Software
Engineering courses. This software tool introduces its
users to modern software engineering methodology used
these days in the software industry known as Agile
Software Development that includes the Scrum framework
for managing software projects or products.
Keywords— Software Engineering Education, Agile
Software Development, Scrum, Web-based interactive
tutorial.
I.

INTRODUCTION

Software Engineering courses are perceived as dry and boring.
Traditional software engineering classes mostly consist of
theoretical lectures and that do not engage students actively
thereby resulting in students not learning key concepts [2, 5].
Therefore application of those key concepts in real-world
scenarios is important for student learning. One can gain more
knowledge and retain it only when (s)he is able to implement it
or use it in a project that they might work on. An interactive
tutorial designed to engage a user in active learning of software
engineering concepts is of value. The requirements of this
tutorial would be: i) introduce user to a course topic through
the tutorial; ii) provide information on all the basic concepts
and key terms of the topic; iii) allow users to practice what they
have learnt in a user-friendly and engaging manner.
The goal of this project is the design and implementation of
a web-based interactive game to teach Scrum framework for

c
978-1-4673-6217-7/13/$31.00 2013
IEEE

managing software development. Interaction with the user is
divided in 3 significant phases where the user plays three
different roles in each phase. For example, in the Scrum
process there are multiple roles such as a product owner,
scrum master, developer, knowledge manager, etc. The first
phase is ‘Observation’; here the user will observe the Agile
process in action. The second phase is ‘Data collection’; here
the user will use the techniques observed in phase I and
contribute towards data collection part of the implementation
of the project. Finally the third phase is the ‘Development’
phase. Here the user performs a developer role on the team.
The user picks one of the available tasks for development and
implements it. The aim is to successfully implement all the
assigned tasks. After going through all three phases the student
would gain hands-on experience in executing a Scrum process.
The interactive tutorial covers all aspects of an Agile Scrum
process, describes the basic unit of Scrum called sprint,
describes a sprint cycle and its duration, and demonstrates how
a project is implemented in sprints.
This paper presents the prototype implementation of the
ScrumTutor as an interactive tutorial with 2 phases. In phase I
the user plays the role of an observer. The sprint is one weeklong and the tutorial depicts activities from day one to day five.
The tutorial involves four roles and uses a specific software
project that involves developing a website to manage music,
music albums, artists, and events using a Content Management
System such as Drupal [13]. The Scrum roles simulated in the
tutorial are a Product owner, a Scrum Master, and two team
members. In later phases of the tutorial the user gets involved
as a developer on the team. In the first phase, the team starts a
weekly sprint by discussing the product that they are going to
develop at a sprint Planning meeting led by the product owner.
Each day of the sprint, the team has a daily standup meeting to
discuss the tasks each member worked on yesterday, what they
are going to work on today and if they have any impediments
in completing their tasks. At midpoint during the weekly sprint,
the team also has a Scrum review meeting where they discuss
all the sprint tasks in detail, their status, and if any team
member needs help on any task. In phase II, the user plays the
role of a data collector and participates as one of the team
member. The user is involved in the process and daily Scrum
meetings where (s)he provides their status update and also
collects data required to handle certain testing tasks for the
team. The player gets involved in the project thereby gaining
better understanding of the Scrum process.

1884

 7KH UHPDLQGHU RI WKLV SDSHU LV RUJDQL]HG DV IROORZV
6HFWLRQ  SURYLGHV EDFNJURXQG PDWHULDO RQ $JLOH SURFHVVHV
6HFWLRQ  SUHVHQWV UHODWHG ZRUN LQ 6RIWZDUH (QJLQHHULQJ
(GXFDWLRQ 6HFWLRQ  SUHVHQWV WKH 6\VWHP GHVLJQ RI
6FUXP7XWRUIROORZHGE\LPSOHPHQWDWLRQLQVHFWLRQ6HFWLRQ
SUHVHQWVDTXDOLWDWLYHHYDOXDWLRQRIWKHVRIWZDUHWRRODQGLWV
UHVXOWV7KHODVWVHFWLRQSUHVHQWVVXPPDU\DQGIXWXUHZRUN
,,

%$&.*5281'

6RIWZDUH 'HYHORSPHQW /LIHF\FOH 6'/& LV D FRQFHSWXDO
PRGHO XVHG LQ SURMHFW PDQDJHPHQW WKDW GHVFULEHV WKH VWDJHV
LQYROYHGLQDQLQIRUPDWLRQV\VWHPGHYHORSPHQWSURMHFWIURP
DQ LQLWLDO IHDVLELOLW\ VWXG\ WKURXJK PDLQWHQDQFH RI WKH
FRPSOHWHG DSSOLFDWLRQ >@ 7KHUH DUH PDQ\ GLIIHUHQW W\SHV RI
6'/& PRGHOV DQG LQGXVWU\ XVHV RQH WKDW EHVW VXLWV WKHLU
QHHGV 7KH EDVLF DFWLYLWLHV LQ DQ\ 6'/& DUH WKH VDPH (DFK
VRIWZDUHOLIHF\FOHPRGHOVSHFLILHVSKDVHVRIWKHOLIHF\FOHDQG
WKH RUGHU LQ ZKLFK WKH\ ZLOO EH H[HFXWHG ,QLWLDOO\ JDWKHUHG
UHTXLUHPHQWVDUHFRQYHUWHGWREDVLFGHVLJQ&RGHLVSURGXFHG
LQ WKH SURFHVV RI LPSOHPHQWDWLRQ WKDW LV HYROYHG IURP WKH
GHVLJQ'HOLYHUDEOHVIURPWKHLPSOHPHQWDWLRQSKDVHDUHWHVWHG
DJDLQVW WKH UHTXLUHPHQWV 6RPH RI WKH WUDGLWLRQDO 6'/&
PRGHOV DUH  :DWHUIDOO ± D OLQHDUVHTXHQWLDO OLIHF\FOH PRGHO
WKDW LV YHU\ VLPSOH DQG HDV\ WR XQGHUVWDQG DQG XVH  9
6KDSHG  DQ H[WHQVLRQ RI WKH ZDWHUIDOO PRGHO ZKHUH WHVWLQJ
VWDUWV HDUO\RQ LQ WKH SURFHVV  6SLUDO  PRUH IDYRUDEOH IRU
ODUJH H[SHQVLYH DQG FRPSOLFDWHG SURMHFWV GXH WR LW H[WUD
HPSKDVLV RQ SODQQLQJ ULVN DVVHVVPHQW DQG GHVLJQ RI
SURWRW\SHV DQG VLPXODWLRQV  $JLOH ± DQ LWHUDWLYH DQG
LQFUHPHQWDO GHYHORSPHQW PHWKRGRORJ\ ZKHUH UHTXLUHPHQWV
DQG VROXWLRQV HYROYH WKURXJK FROODERUDWLRQ EHWZHHQ FURVV
IXQFWLRQDODQGVHOIRUJDQL]LQJWHDPV>@

WKDWSURYLGHVLQIRUPDWLRQRQZKDWLVWREHSURGXFHGZKHQLW
LVSURGXFHGDQGKRZPXFKLVSURGXFHG
• 6FUXPLVDSRSXODUIUDPHZRUNGHULYHGIURPDJLOHGHYHORSPHQW
WKDWSURYLGHVDIOH[LEOHDQGKROLVWLFVWUDWHJ\ZKHUHWKHHQWLUH
WHDPZRUNVDVDXQLWWRZDUGVRQHFRPPRQJRDO

% 6FUXP
6FUXP LV DQ LWHUDWLYH DQG LQFUHPHQWDO SURFHVV ZKHUH WKH
SURGXFWWKDWLVEHLQJGHYHORSHGNQRZQDVWKHSURGXFWEDFNORJ
LV GHVFULEHG DQG GLVFXVVHG DPRQJ WKH WHDP PHPEHUV 7KH
SURGXFWEDFNORJLVGLYLGHGLQWRVPDOOHUWDVNVWKDWDUHDVVLJQHG
WRWKHWHDPPHPEHUV7KHVHWRIWDVNVWKDWWKHWHDPZRUNVRQ
GXULQJDVSULQWLVFDOOHGWKHVSULQWEDFNORJ$SK\VLFDOERDUG
FDOOHGWKHVFUXPERDUGLVXVHGWRNHHSWUDFNRIDOOWKHVHWDVNV
DQG DVVLJQPHQWV 7KH UROHV LQ WKH 6FUXP IUDPHZRUN DUH DV
IROORZV
• 3URGXFW 2ZQHU LV UHVSRQVLEOH IRU WKH ZKROH SURGXFW LGHD
PDQDJHVWKHUHWXUQRQLQYHVWPHQW52,IRUWKHHIIRUWE\ WKH
WHDP NHHSV WUDFN RI SULRULWL]DWLRQ RI WKH SURGXFW EDFNORJ 	
UHOHDVHSODQVDQGFRXOGDOVRDFWDVDWHDPPHPEHU
• 6FUXP PDVWHU SURPRWHV 6FUXP SURFHVV VXSSRUWV WR UHVROYH
DQ\ LPSHGLPHQWV PDNHV D WHDP VHOIRUJDQL]HG NHHSV WKH
VSULQW EDFNORJ YLVLEOH SURWHFWV WKH WHDP IURP H[WHUQDO
LQWHUIHUHQFH DQG GLVWXUEDQFHV WR JHW DORQJ ZLWK WKH IORZ RI
ZRUNDQGKDVQRDXWKRULW\RQWKHWHDP
• 7HDP PHPEHU LV FURVVIXQFWLRQDO ZKR SRVVHVV VNLOOV RI D
WHVWHU EXVLQHVV DQDO\VW DQG QRW MXVW D GHYHORSHU DQG VWURQJO\
FROODERUDWHVZLWKRWKHUWHDPPHPEHUV




)LJXUH$JLOHSURFHVV


$ $JLOH3URFHVV
7KH $JLOH 0DQLIHVWR IRFXVHV RQ WKH IROORZLQJ LQGLYLGXDOV
DQG LQWHUDFWLRQV ZRUNLQJ VRIWZDUH FXVWRPHU FROODERUDWLRQ
DQGUHVSRQGLQJWRFKDQJH)LJXUHGHSLFWVDQ$JLOHSURFHVV
6RPHRIWKHYDULDWLRQVRIDQ$JLOHSURFHVVDUHDVIROORZV
• $JLOH 8QLILHG 3URFHVV $83 LV D VLPSOH DQG HDV\ WR
XQGHUVWDQGSURFHVVWRGHYHORSEXVLQHVVDSSOLFDWLRQVRIWZDUH
,WLQFOXGHVDJLOHWHFKQLTXHVVXFKDVWHVWGULYHQGHYHORSPHQW
DJLOH PRGHOLQJ DJLOH FKDQJH PDQDJHPHQW DQG GDWDEDVH
UHIDFWRULQJIRUKLJKHUSURGXFWLYLW\
• .DQEDQLVDMXVWLQWLPHGHOLYHU\SURFHVVWKDWGRHVQRW
RYHUORDGWKHGHYHORSHUV,WXVHVYLVXDOSURFHVVPDQDJHPHQW


)LJXUH6SULQW&\FOH


& 6SULQW
7KHVFUXPSURFHVVFRQVLVWVRIDOOPHPEHUVFRQWULEXWLQJWRWKH
GHYHORSPHQW DQG LPSOHPHQWDWLRQ RI WKH SURGXFW LQ D VSHFLILF
SHULRG RI WLPH FDOOHG D VSULQW 6SULQW LV WKH EDVLF XQLW RI
GHYHORSPHQW LQ 6FUXP ,W LV UHVWULFWHG WR D VSHFLILF GXUDWLRQ
WKDWFDQODVWDQ\ZKHUHIURPDZHHNWRDPRQWK$VSULQWVWDUWV
ZLWKDSODQQLQJPHHWLQJZKHUHSURGXFWXQGHUGHYHORSPHQWLV
GLVFXVVHG DQG WKH VSULQW WDVNV DUH GHULYHG 7KH\ DUH GLYLGHG
DQG DVVLJQHG WR HDFK PHPEHU RI WKH WHDP $ GDLO\ VFUXP
PHHWLQJ DOVR NQRZQ DV D VWDQGXS PHHWLQJ LV KHOG HYHU\ GD\
ZKHUHDOOPHPEHUVRIWKHWHDPSURYLGHDVWDWXVXSGDWHRIWKHLU
WDVNVWKDW ZHUHSHUIRUPHGRQWKHSUHYLRXVGD\WKHWDVNV WKDW
WKH\DUHFXUUHQWO\ZRUNLQJRQDQGDOVRLQIRUPWKHWHDPRIDQ\
LPSHGLPHQWV WKDW WKH\ DUH IDFLQJ LQ FRPSOHWLQJ WKHLU WDVNV
)LJXUHVKRZVDSLFWRULDOUHSUHVHQWDWLRQRIDVSULQWF\FOH

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1885

III.

RELATED WORK

The use of simulation games for education has become quite
widespread in the last few years [2-4]. Most of the related
work have established and published results that students
found simulation games for education useful in learning and
helped students in being more engaged in class. Although
games have been used to teach various concepts in a number
of fields, there are very few games that teach concepts in
Software engineering. In this section we present related work
on simulation games for teaching software engineering and
other related literature that provided inspiration for the design
and implementation of ScrumTutor.
Researchers have worked on simulation games to
teach software engineering processes through an experimental
card game that highlights process issues that were not
sufficiently addressed in lectures and projects [5]. This game
uses physical cards to reinforce software engineering concepts
unlike ScrumTutor that is a web-based interactive tutorial.
SimSE [6] is another game-based software
engineering simulation environment that provides a number of
games to teach various software development lifecycle models
such as waterfall model, incremental model, rapid prototyping
model, rational unified process, and extreme programming
model. SimSE does not teach Agile software development that
is the most recent and popular model. ScrumTutor addresses
Agile processes and draws inspiration from SimSE that creates
a game scenario with an office background and team members
working in an office thereby simulating a real-world
environment. ScrumTutor has used this aspect in its design
and addresses the area of Scrum that has not been addressed in
SimSE.
Students were introduced to Software Engineering
Interview process through a decision-based computer game in
a study at Rowan University [7]. Students learnt about
interview process that is needed in requirements analysis
phase of software engineering life cycle. By playing the game
multiple times students realized that gathering facts before
design and implementation is important for the success of the
project. Based on this work, ScrumTutor was designed to
have multiple phases so that concepts can be reinforced to
students to make sure they attain a high level of
understanding.
Researchers have also used the concept of games for
teaching various management concepts such as Risk
management and knowledge management [8, 9]. They
assessed the effectiveness in meeting learning objectives and
their findings clearly demonstrated the advantage of a
simulation game.
Other related work includes evaluation of objectoriented design patterns in game development [10] and seniors
rehabilitation using video game technology [11]. These
projects are close to education through interactive tools and
games but are in other domains. We believe ScrumTutor will
be a useful tool to the Software Engineering Education
community and can be customized to teach several other Agile
Process concepts.

1886

IV.

SCRUM TUTOR – DESIGN

The long-term goal of the project is to design Scrum Tutor as
a simulation game with game features such as engaging user
interaction, time factors, adjustability of player skill levels, replayability, scoring, multi-player, and providing score
statistics. The first version of ScrumTutor was designed with
this end goal in mind.
A. What is being taught and how?
The initial version of ScrumTutor provides hands-on practical
experience to students in a software engineering class.
Students learn Scrum process by engaging with this tool as an
observer and assimilating the flow of process and management
of software development. The tutorial takes its user through a
weeklong sprint with a Scrum team working on various
development activities distributed from day 1 to day 5. On day
1 the user observes the product owner describing the product
and the product backlog to the software team. The user is
introduced to various concepts such as the Scrum board that
displays the sprint backlog. User watches scrum task
allocation happening during a sprint planning meeting,
discussion of tasks and allocation among team members at the
scrum board, and discussion on status of various activities
through a daily standup meeting in front of the Scrum board.
The tutorial has a pre-defined simulated time assigned to each
day. When the pre-defined time for a day lapses, the user then
progresses to day 2 and observes a daily scrum meeting held
to discuss the activities of the team members performed on
that day and on previous day, to get the updates from the team,
and also discuss any impediment’s that a team member might
be facing. This process is repeated for the remainder of the
days. The user observes these daily standup meetings
happening at a fixed time everyday, example at 9:00am in the
morning. During the remainder of the day the user observes
the team working on their tasks in their cubicles and
interacting with each other. On day 4, a sprint review meeting
is held to discuss the progress made during the current sprint
and the team assesses if there is a need to make any changes to
their plan in order to successfully complete their tasks and
deliver the artifacts. In this simulation, the sprint review
meeting happens at around mid-point in the sprint at a fixed
time, example 11:00am on day 4.
In order to encourage active learning and engagement, as
the user is progressing from one day to the next, a number of
quiz-type questions are presented to the user to test their
understanding of the concepts. The user is allowed to process
to the next day only after answering these questions correctly.
If not, the user is redirected back to the previous step to
observe the process and understand it.
ScrumTutor is designed to have multiple phases so that
the user can more actively engage in the process and gain
hands-on experience as they are learning and getting better at
Scrum. This version of the tool is designed to have 2 phases.
Phase 1 is the observation phase that was presented so far.
Phase 2 is the data collection phase where the user is taken
through a new sprint with a new set of tasks in the sprint
backlog and the involvement of the user progressively

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

increases. The user is involved as one of the team members
participating in the meetings, providing inputs, and status
updates. Tasks are allocated to the user that has to be
completed for a successful completion of the sprint.
B. What product is being developed using the Scrum
framework in ScrumTutor?
This software tool is designed such that any project/product
can be used as an example and plugged in to demonstrate the
Scrum framework, sprints, and various activities. In this
version of the tool, the product being developed is a website to
manage music, music albums, artists, events, and venues. This
product is built using a Content Management System called
Drupal. An existing Software requirements specification and
product backlog of this product was used to demonstrate the
process in Scrum Tutor.
#
1
2

Table 1: Sample formative & summative assessment questions
Question
Answer Choices
(correct answers in bold)
Is the sprint backlog created
during the planning meeting?
How often do the teams
integrate their work and rerun
the regression tests?

3

What happens during a scrum
review meeting?

4

What is the duration of a daily
scrum meeting?

5

Who describes the product
backlog to the team?

6

When is a sprint retrospective
meeting be held?

7

During a sprint planning
meeting, how many sprints are
discussed and planned for?

8

Who attends the sprint
planning meeting?

9

How often is the sprint
retrospective meeting
conducted?

• Yes
• No
• Only at the end of
each sprint
• Once per day
• Continuously as things change;
potentially many times per day
• Discussion of sprint
progress
• Check if any member of the
team needs help to complete
their tasks
• Remove tasks from
sprint backlog
• Discuss a team outing
• As long as necessary
• 1 hour
• 15 minutes
• Product owner
• Scrum Master
• Knowledge Manager
• Quality Manager
• At the end of each
sprint
• It is not needed
• Few minutes before the sprint
planning meeting starts
• Review/Retrospective meeting is
the the same
• 4 sprints
• Current sprint only
• All sprints in the
project
• The scrum
development team
• Outside stakeholders
• Manager of the team
• Scrum Master
• Product owner
• Every day
• Every sprint
• Every project

C. What technology is used for implementation?
The long-terms goals of ScrumTutor is to provide education

through a fun and engaging environment, provide the thrill of
playing a game, and provide most features of a good game.
With these end goals in mind, HTML and Javascript was
chosen for UI development, C#.NET was chosen as the
server-side technology and Visual Studio as the Integrated
Development Environment (IDE).
D. Scoring/tests
ScrumTutor has a scoring mechanism through intermediate
quizzes to motivate the user to do better, replay and revisit
concepts that were not well understood, and gain further
understanding of the process by playing subsequent phases of
the game. A user is allowed to progress to the next phase only
after they have received a certain minimum score. The
quizzes are provided throughout the system in two ways:
formative assessment presented to the user during a sprint
while the scrum activities are happening and summative
assessment presented to the user at the end of a sprint.
Successful completion of the summative assessment allows
the user to move on to the next phase (and next sprint of the
project). Table 1 shows sample formative and summative
assessment questions.
V.

SCRUM TUTOR – IMPLEMENTATION

ScrumTutor was developed using C#.NET and the ModelView-Controller (MVC) architecture. Visual Studio 2010 was
used as the Integrated Development Environment (IDE), C#
.NET as the middleware and SQL Server 2008 R2 as the
database management system. User interface was designed
using HTML, JavaScript, and JQuery [12]. Technologies used,
database design and user interface design are further described
in the following sections.
A. Technologies
C#.NET is an object oriented programming language
developed by Microsoft; it is mostly implemented in the IDE
called Visual Studio, a product of Microsoft. Visual Studio
2010 was used as an IDE to write the functional code in C# to
access the database and the user interface.
SQL Server 2008 is a ‘Database Management System’
developed to allow definition, creation, querying, updating
and administrating an organized collection of data. SQL
Server Express 2008 R2 is the version used in this application
to maintain the assessment data, user scores, and business
logic data required for the simulation.
JavaScript is a client-side scripting language used to provide
interactions with the user. Client-side scripts can be written to
control, communicate and alter the content that needs to be
displayed.
JQuery, an open source software, is ‘a fast and concise
JavaScript library that simplifies HTML document traversals,
event handling, animations, and Ajax interactions for rapid
Web application development [12].
B. Database Design
The database of ScrumTutor consists of several tables to save

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1887

application data, user profile information, user scores, and
statistics. The detailed database design is shown in Figure 3.
Some of the important tables are listed and briefly described
below:
• Registration – This table is used to store user login
information. When a student or faculty registers for the first
time their data is saved in this table.
• Roledetails – This table tracks roles of the scrum team.
• FormativeQuestions – This table is used to store all the
formative assessment questions that are presented to the
user at the end of every day in a sprint with the answer
choices and the correct answer.
• SummativeQuestions – This table is used to store all the
summative assessment questions that are presented to the
user at the end of a sprint with the answer choices and
correct answer.
• Task – This table has detailed description of all the tasks
involved in a sprint.
• TeamMember – This table maintains the scrum team
member information and description.
• SprintStatus – This table stores data about a specific sprint,
status information and task assignments from all the
meetings.

VI.

SCRUM TUTOR – EVALUATION

ScrumTutor has been tested on multiple browsers like Mozilla
Firefox 19.0.2, Internet Explorer 10 and Google Chrome 26.0.
The functionality of the application was tested thoroughly to
ensure that an undergraduate student could go through the
complete tutorial without any issues. An evaluation study was
conducted at Arizona State University (ASU) to assess the
usefulness of the tool. Students from the Masters in Computer
Science Engineering program at ASU were surveyed prior to
their usage of ScrumTutor to assess their understanding of
Scrum. They were then allowed to use ScrumTutor and go
through phases 1 and 2 of the tutorial. A post-survey of these
same students was conducted after they used the tool and the
students were asked the same questions to assess improvement
in learning. Table 2 shows some of the questions on the preand post-survey. Unanimously, all of them felt that the tool
was useful, easy to follow and gave them a better
understanding of the Scrum process and the basic workflow
unit in Scrum called sprint. Many of them felt that going
through this tutorial provided them an industry perspective
and context of how this Software Engineering concept in used
in the industry on a project while working in teams. Some of
them said that they better understood the team dynamics and
various roles than earlier. Students also commented that they
felt this was an innovative and engaging pedagogical practice
for teaching software engineering processes.
#
1
2
3
4

Table 2: Pre- and Post-Survey Questions about ScrumTutor tool
Survey Question
What do you know about agile software development?
Describe Scrum framework?
Where do you think these concept can be used and applied?
Did ScrumTutor help in improving your knowledge of Scrum (Post)

VII. CONCLUSIONS AND FUTURE WORK

Figure 3: Database Design

C. User Interface Design
Figure 4 presents screenshots of the login, registration, and
home page of ScrumTutor tool. Figure 4 also shows
screenshots that introduce Scrum framework and values of
Scrum. It also shows screens that introduce the Scrum team,
Scrum board and terminology at the beginning of the tutorial.
Figure 5 shows screenshots of a sprint cycle going from day 15 in a week. Day 1 includes a discussion of the product, a
sprint planning, followed by assignment of tasks on the Scrum
board. Day 2 - 5 involve daily Scrum meetings that happen at
same time and place in the morning followed by the team
interacting and working on their tasks. In addition to the daily
Scrum meeting, on day 4 a scrum review meeting is held. Day
5 ends with a sprint retrospective meeting.

1888

ScrumTutor provides hands-on experience on Scrum
Framework to undergraduate students The interactive tutorial
has multiple learning phases, i.e., Observation and Data
Collection in this version and Development in the future
versions. An evaluation survey of students about this tool
provides preliminary results that using ScrumTutor enhanced
their knowledge of Scrum, provided hands-on training of
concepts learnt and provided a contextualized reinforcement
of Agile process via the music software product that was
managed and developed using ScrumTutor. Based on these
results we are confident that a full version of ScrumTutor that
is a game-based tool with multiple levels of learning through
phases will be a useful addition to the software engineering
classes. Future work on this project includes the
implementation of phase III where the user plays the role of a
‘team member’. The user gets to be part of the software team
and contributes as a developer. User picks a task and
implements it within the given timeframe of the sprint.
Furthermore, sprints can be customized based on the project
that is being implemented and managed. If the project is large,
a sprint can be anywhere from two to four weeks. A Mobile
app version of this game/tutorial for various platforms such as
Android, iOS, and Windows will be a great addition and is

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

Figure 4: User Interface Screenshots showing terminology and Scrum Process

Figure 5: User Interface screenshots showing sprint cycle in Phase 1

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

1889

likely to be adopted by a larger audience. A pilot study will be
conducted in introductory Software Engineering classes at
sophomore level at ASU. Data will be collected on the
usability of the tool, student learning outcomes, student
reflection of the tool and faculty reflection of the tool. Phase I
and II of the tool can be further improvised with game-like
features by adding more user interaction. HTML5 may be
used to provide a rich user interface.

[5]

[6]

[7]

[8]

REFERENCES
[1]

[2]

[3]

[4]

I. Sommerville, “Software Engineering” (9th ed.), Chapter 2: Software
processes: Software process models and Chapter 3: Agile Software
Development, Boston, MA, 2011.
J. Pieper, “Learning Software Engineering Processes through Playing
Games,” IEEE International Workshop on Games and Software
Engineering (GAS) at Intl. Conf. on Software Engineering, June 2012.
N. Tillman, et al, “Pex4Fun: Teaching and Learning Computer Science
via Social Gaming”, IEEE Conf. on Software Engineering Education
and Training (CSEET) at Intl. Conf. on Software Engg., June 2012.
D. Ismailovic, et al, “Adaptive Serious Game Development”, IEEE
International Workshop on Games and Software Engineering (GAS) at
International Conference on Software Engineering, June 2012.

1890

[9]

[10]

[11]

[12]
[13]

A. Baker, E. O. Navarro, A. Hoek, “An experimental card game for
teaching software engineering processes”, Journal of Systems and
Software, IEEE, pp. 3–16, 2005.
E. O. Navarro, “SimSE: A software engineering simulation environment
for software process education”, Doctoral Dissertation, School of
Information & Computer Sciences, Univ. of California, Irvine, 2006.
A. Rusu, R. Russell, R. Cocco, “Simulating the Software Engineering
Interview Process using a Decision-based Serious Computer Game”,
IEEE International Conference on Computer Games (CGAMES), 2011.
A. Chua, “The Design and Implementation of a Simulation Game for
Teaching Knowledge Management”, Journal of the American Society
for Information Science and Technology , pp. 1207–1216, 2005.
G. Taran, “Using Games in Software Engineering Education to Teach
Risk Management”, IEEE Conf. on Software Engineering Education &
Training (CSEET), pp. 211-220, 2007.
A. Ampatzoglou, A. Chatzigeorgiou, “Evaluation of object-oriented
design patterns in game development”, Elsevier Informaiton and
Software Technology, Vol. 49, pp. 445-454, 2007.
D. Maggiorini, L. A. Ripamonti, E. Zanon, “Supporting Seniors
Rehabilitation through Videogame Technology”, IEEE Workshop on
Games & Software Engg. (GAS) at Intl. Conf. on Soft. Engg, June 2012.
“jQuery”[Online]. Available: http://jquery.com/ [Accessed: June-2013]
“Drupal”[Online]. Available:http://drupal.org/ [Accessed: June-2013]

2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)

SCIENCE CHINA
Information Sciences

. RESEARCH PAPER .

July 2012 Vol. 55 No. 7: 1639–1649
doi: 10.1007/s11432-012-4598-3

Eﬃcient construction of provably secure
steganography under ordinary covert channels
ZHU Yan1,2 ∗ , YU MengYang3 , HU HongXin4 , AHN Gail-Joon4 & ZHAO HongJia1,2
1Beijing

Key Laboratory of Internet Security Technology, Peking University, Beijing 100871, China;
of Computer Science and Technology, Peking University, Beijing 100871, China;
3School of Mathematical Sciences, Peking University, Beijing 100871, China;
4School of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, AZ 85287, USA
2Institute

Received January 6, 2011; accepted October 15, 2011; published online May 10, 2012

Abstract Steganography is the science of hiding information within seemingly harmless messages or innocent media. This paper addresses the problems of eﬃcient construction of secure steganography in ordinary
covert channels. Without relying on any sampling assumption, we provide a general construction of secure
steganography under computational indistinguishability. Our results show that unpredictability of mapping
function in covertext sampler is indispensable for secure stegosystem on indistinguishability against adaptive
chosen hiddentext attacks. We completely prove that computationally secure steganography can be constructed
on pseudorandom function and unbiased sampling function under ordinary covert channels, that is, its security is inversely proportional to the sum of errors of these two functions, as well as the legth of hiddentexts.
More importantly, our research is not dependent upon pseudorandom ciphertext assumption of cryptosystem or
perfect sampling assumption. Hence, our results are practically useful for construction and analysis of secure
stegosystems.
Keywords

steganography, cryptography, indistinguishability, sampler, unpredictability, adversary models

Citation
Zhu Y, Yu M Y, Hu H X, et al. Eﬃcient construction of provably secure steganography under ordinary
covert channels. Sci China Inf Sci, 2012, 55: 1639–1649, doi: 10.1007/s11432-012-4598-3

1

Introduction

Steganography is the science of hiding information within seemingly harmless messages or innocent media
to supply various applications, such as anonymous communications [1], anonymous online transactions,
covert channels in computer systems [2], covert or subliminal communications, invisible watermarking
and ﬁngerprinting [3] for protection of intellectual property rights, etc. It is quite obvious that hidinginformation messages (stegotext) must be indistinguishable from harmless messages (covertext) if we
expect that communication is unconscious for human beings or undetectable for computers. Thereby, the
starting point of this paper is also indistinguishability.
The security, as robustness and imperceptiveness, is a vital aspect of the research of steganography.
Cachin [4] ﬁrstly formalized an information-theoretic model for steganography in 1998. They introduced
∗ Corresponding

author (email: yan.zhu@pku.edu.cn)

c Science China Press and Springer-Verlag Berlin Heidelberg 2012


info.scichina.com

www.springerlink.com

1640

Zhu Y, et al.

Sci China Inf Sci

July 2012 Vol. 55 No. 7

a relative entropy function as a basic measure of information contained in an observation to deﬁne
unconditionally secure stegosystem. Up to 2002, Hopper et al. [5,6] ﬁrstly attempted to formalize
steganography from a complexity-theoretic point of view. However, the security proofs only hold under
perfect sampling oracle assumption, which requires the samplers are independent each other. However, it
is not practical in most circumstances. Moreover, this paper obtains the precondition for the existence of
secure steganography on a strong assumption that IND-CPA security cryptosystems have pseduorandom
ciphertext, as well as the perfect sampling without bias. Unfortunately, this assumption does not hold in
some existing cryptosystems (see [7]).
Following these works, many scholars began to research various technologies of semantic security for
steganography. For instance, Dedic et al. [8] further lowered the requirement for hiding-channel and provided ‘black-box steganography’. This paper provided two improved algorithms based on rejection-sample
encoder of [5]. Ahn et al. [9] created a public-key steganography and chosen-stegotext attacks (SS-CSA).
Backes et al. [10], as well as Hopper [11], considered adaptive chosen-covertext attacks. Lysyanskaya
et al. [12] discussed the problem of imperfect sample by weakening assumption, in which the covertext
distribution is modeled as a stateful Markov process. But further discussions of the underlying questions
and unconditional imperfect sample are still needed in order to construct more practical stegosystems.
As is known to all, indistinguishability is a signiﬁcant factor in cryptographic security analysis. We
also believe that it is the foundation of security analysis in steganography. Although this point has
already been discussed in prior papers [5,8,11,12], there is still a lack of thoughtful attention to show
how to construct secure stegosystems without sampling assumption. The real-world stegosystems are
most often broken because they make invalid assumptions about the adversary’s abilities. Typically,
this is an assumption about an adversary’s lack of knowledge about covertext distribution. There are
many ways to characterize this sort of abilities: perfect sampling for independent distribution [5], semiadaptive sampling for α-memoryless distribution from some Markov process of order α [12], etc. These
assumptions are still too strong for real-world applications. Hence, it is essential to examine whether it is
possible to weaken or eliminate these assumptions. Fortunately, pseudorandomness and unpredictability
methods on complexity-theory, as well as randomized unbiased sampling functions will help settle this
issue in ordinary covert channels.
In this paper, we focus on the construction of secure steganography under ordinary covert channels
without sampling assumption. By constructing a more general model of stegosystem, we prove that
unpredictability of mapping function in covertext sampler is indispensable for secure stegosystem on
indistinguishability against adaptive chosen hiddentext attack (IND-CHA). Furthermore, computationally secure steganography is feasible if there exist 1 -pseudorandom function and 2 -unbiased sampling
function, where 1 and 2 are two negligible errors. Although some previous studies have induced similar
conclusions, our results provide more complete line of investigations and a more general conclusion. Especially, our research was not based upon the strong IND-CPA assumption and perfect sampling assumption,
in which the ciphertext of cryptosystem used to build stegosystem must be pseudorandom. Hence, our
construction reveals important insights into more general constructibility of secure steganography, and
these results are also practically useful for construction and analysis of stegosystems.
The rest of the paper is organized as follows. In Section 2, we describe some basic notions and general
deﬁnition of stegosystem. In Section 3, we deﬁne a formal model of IND-CHA security based on left-right
oracle. In Section 4, a practical construction of stegosystem is proposed for IND-CHA security, and a
security analysis of this construction is described as well. Finally, we conclude this paper in Section 5.

2
2.1

Preliminary and deﬁnition
Notions and preliminaries

The security in this paper is stated as a game in which an adversary has the ability to select two
messages. One of the messages is randomly selected and hidden. The stegosystem is then called secure

Zhu Y, et al.

Sci China Inf Sci

July 2012 Vol. 55 No. 7

1641

if no adversary can do better than a random guess in ﬁnding out which message was hidden. Before we
formalize the game, we recall the concept of negligible function:
Definition 1 (Negligible). A function f : N → [0, 1] is called negligible if for every polynomial p(·)
there exists an N such that |f (n)| < 1/p(n) for all n > N .
That is, f is asymptotically smaller than any inverse polynomial. This paper will focus on languages in
BPP (bounded-error probabilistic polynomial time), which can be recognized by probabilistic polynomialtime machines with a negligible error probability.
Computational indistinguishability, introduced by Goldwasser and Bellare [13] and deﬁned in full
generality by Yao [14], is deﬁned as follows:
Definition 2 (Computational indistinguishability).
Two probability ensembles, X = {Xn }n∈N and
Y = {Yn }n∈N , are computationally indistinguishable, if for every probabilistic polynomial-time (PPT)
distinguisher D, every positive polynomial p(·), and all suﬃciently large n’s,
|Pr[D(1n , Xn ) = 1] − Pr[D(1n , Yn ) = 1]| <

1
.
p(n)

(1)

Note that, |Xn | = Ω (n) denotes the length of samples in Xn . Clearly, if two ensembles are statistically
close then they are also computationally indistinguishable. The converse, however, is not true. Let Un
denote uniform distribution on n-bit strings. The ensemble {Un }n∈N is called standard uniform ensemble.
Given the above deﬁnition, it is easy to deﬁne pseudorandomness:
Definition 3 (Pseudorandom ensembles). An ensemble X = {Xn }n∈N is called pseudorandom if there
exists a uniform ensemble U = {Un }n∈N such that X and U are indistinguishable in polynomial time.
Furthermore, let f : {0, 1}s × {0, 1}L → {0, 1}l denote a family of functions. We call the function f is a
pseudorandom function if f is a deterministic polynomial-time algorithm and the ensemble {fk (Un )}n∈N
is also pseudorandom1) .
Other important notions involve sampling and unbiased function. The simplest method for sampling
from a complex combinatorial structure is Monte Carlo method, often known as rejection sampling.
Without lost of generality, we will also use the method in this paper and write SampleCf (k, b) to denote a
rejection-sampling algorithm, which samples from a covertext distribution C according to oracle OC such
that multi-bit symbol b can be embedded in it, for a function fk (·) with a certain key k. However, this
algorithm diﬀers greatly from general Monte Carlo method due to Oracle.
To evaluate the statistical property of mapping function in sampling space, we deﬁne the unbiased
function as follows:
Definition 4 (Unbiased function). A function f : K × C → R is a (tb , )-unbiased function on the
distribution K, C, R, if for all r ∈ R, |Prx←C [fk (x) = r] − 1/|R||  , where |R| is the number of elements
in R and tb denotes the time-complexity in f .
We say that a function f is unbiased if  = 0, that is, Prx←C [fk (x) = r] = 1/|R|. An -biased function
is called an unbiased function if  is a negligible function.
2.2

Definition of stegosystem

We assume that there is a public channel in which a sender can communicate with a receiver and to
which an adversary can have perfect read-only access. In addition, there is a covertext source by which
any covertext is generated to hide message. The model of stegosystems [5,6] can be formally deﬁned as
follows:
• C is a covertext space with probability distribution PrC . We assume that C can be denoted by all
ﬁnite sequences {(c1 , . . . , cl )|l ∈ N, ci ∈ Σ, 1  i  l} after sampling, where Σ is a ﬁnite source alphabet2) .
1) The extensive property of pseudorandom function in cryptography may be unnecessary for steganography, thus a
universal hash function is often used in practice.
2) Without loss of generality, we can also map a symbol ri in a ﬁnite alphabet Σ into an element ci in a ﬁnite ﬁeld Fq .
For example, the image watermarking is regarded as the modular operation in GF (28 ) ﬁnite ﬁeld.

1642

Zhu Y, et al.

Sci China Inf Sci

July 2012 Vol. 55 No. 7

• P is a hiddentext space with probability distribution PrP , which is statistically independent of PrC .
• S is a stegotext space with probability distribution PrS . We assume that S is equal to C as sets
(S = C) according to the requirement of imperceptibility, but PrS might be diﬀerent from PrC , sometimes.
We assume that any sequence of covertexts over the public channel consists of independent repetitions
of covertexts. Similar to cryptography, let K = {0, 1}∗ denote a key space. Then, a stegosystem Φ can
be deﬁned in terms of the above (C, P, S, K), as follows:
Definition 5 (Stegosystem [11,12]). An eﬃcient stegosystem is a triple, SS = (G, E, D), of probabilistic
polynomial-time algorithms satisfying the following conditions:
• On input 1n , algorithm G (called key-generator) outputs a bit strings k as the private-key, where n
is a security parameter.
• For a private-key k ∈ K in the range of G(1n ), a covertext distribution C, and every hiddentext
m ∈ P, the encoding algorithm E outputs a stegotext s ∈ S, i.e., s ← E C (1n , k, m).
• For a private-key k ∈ K in the range of G(1n ), a covertext distribution C, and every stegotext s ∈ S,
the decoding algorithm D outputs a message m ∈ P or an invalid symbol ⊥, i.e., {m, ⊥} ← DC (1n , k, s),
and the probability that


 
Pr DC 1n , k, E C (1n , k, m) = m
is negligible in n, where the probability is taken over the internal coin tosses of algorithms E and D, as
well as the covertext distribution C.
The algorithm D returns either a symbol ⊥ indicating failure (an empty message), or a hiddentext
m ∈ P. Especially, we require that Pr[DC (1n , k, E C (1n , k, ⊥)) =⊥] = 1 holds. We sometimes rewrite the
encoding and decoding algorithms as EkC (m) and DkC (s) for short. In this paper, we focus on an eﬃcient
algorithm to detect whether a hiddentext is presented instead of decrypting the stegotext. In this case,
the decoding algorithm D is also called a deterministic algorithm if it outputs a bit b ∈ {0, 1}, where
b = 1 denotes the presence of hiding information, otherwise b = 0. We call this kind of D a detector.
According to the deﬁnition in [5,8,11,12], we assume that there exists an encoding algorithm to realize
C
cn ← EG(1
n ) (⊥) for an empty message, and then we have the deﬁnition of hiding property, as follows:
Definition 6 (Computational hiding property [5,12]). A stegosystem (G, E, D) has computational hiding
C
property if the following holds: Given a covertext distribution C, both a covertext cn ← EG(1
n ) (⊥) and its
C
any stegotext sn ← EG(1n ) (m) are indistinguishable in polynomial-time if for any probabilistic polynomialtime algorithm D, every hiddentext m ∈ P, every polynomial p(·), and all suﬃciently large n, the following
inequation holds,
 


	



	


1


C
C
.
= 1 − Pr DC 1n , EG(1
=1 <
Pr D C 1n , EG(1
n ) (m)
n ) (⊥)
p(n)
The probability is taken over the internal coin tosses of algorithms G, E and the property of C.

3

Deﬁnition of IND-CHA security

The security in this paper is stated as a game in which an adversary has the ability to select two
messages. One of the messages is randomly selected and hidden. The stegosystem is then called secure
if the adversary cannot do better than a random guess in ﬁnding out which message is hidden. Loosely
speaking, this game means that it is hard to distinguish between covertext and stegotext.
We will use a formal deﬁnition to describe the IND-CHA attack, in which indistinguishability is measured via the “left-or-right” model [13]. Deﬁne the left-or-right oracle EkC (LR(·, ·, b)), where b ∈ {0, 1},
to take input (x0 , x1 ) and do the following: if b = 0 it computes c0 ← EkC (x0 ) and returns c0 ; else it
computes c1 ← EkC (x1 ) and returns c1 . In this case, the goal of such an adversary is to distinguish
whether he is seeing the encodings of the hiddentext that he supplied to the encoder, or simply random
samples from the channel. This choice is decided by a left-or-right oracle EkC (LR(·, ⊥, b)) except that the
second variable is replaced by the null strings ⊥. We ﬁx a speciﬁc stegosystem SS = (G, E, D). Such an
experiment consists of four stages as follows:

Zhu Y, et al.

Sci China Inf Sci

July 2012 Vol. 55 No. 7

1643

1. Key generation: A key k is generated by the key generation algorithm G.
2. Training stage: Malice prepares some hiddentext messages and sends to the oracle O, O encodes
them and returns the results to Malice. After repeating many times, Malice halts and chooses a messages
m ∈ P;
3. Challenge stage: Malice sends m to the oracle O. O tosses a fair coin b ∈U {0, 1}, then provides
c ← EkC (LR(m, ⊥, b)) to Malice;
4. Guessing stage: Upon receipt of c, Malice must answer a bit b as his guess of O’s coin tossing b.
Definition 7 (Hiding attack of stegosystem). Let SS = (G, E, D) be a symmetric stegosystem. Let
b ∈ {0, 1} and n ∈ N. Let Acha be an adversary that has access to one oracle. We consider the following
experiments:
ind-cha-b n
Algorithm ExpSS,A
(1 ).
cha
R

k ← G(1n ),

E C (LR(·,·,b))

k
x ← Acha

(1n ).

Return x.
C
Above it is mandated that the two messages queried by EK
(LR(·, ·, b)) always have equal length, and
that ⊥ denotes a null messages. We deﬁne the advantage of the adversaries via
 







ind-cha-0 n
n
n
(1
)
=
Advind-cha
Pr Expind-cha-1
SS,Acha
SS,Acha (1 ) = 1 − Pr ExpSS,Acha (1 ) = 1  .

We deﬁne the advantage functions of the scheme as follows. For any integers t,qe and ue , InSecind-cha
(t, qe ,
SS
n
ue ) = maxAcha {Advind-cha
SS,Acha (1 )} where the maximum is over all Acha with time-complexity t, each making
to the Ek (LR(·, ·, b)) oracle at most qe queries the sum of those lengths is at most ue bits of hiddentext.
So that, we deﬁne the secure stegosystem under the hiding attack:
Definition 8 (IND-CHA security). The stegosystem SS = (G, E, D) is called (t, qe , ue , )-indistinguisha(t, qe , ue ) is negligible for
bility against adaptive chosen-hiddentext attack if the function  = InSecind-cha
SS
any adversary A whose time-complexity is bounded by a polynomial in n.

4

Construction of secure steganography

In this section we address the problems of existence and construction of secure stegosystem on computational complexity. Intuitively, the more unpredictable the appearance of stego-objects is, the more
diﬃcult it would be to distinguish from them in covert channels. In support of this idea, we focus on the
relationship of unpredictability and hiding property to provide a general construction of secure stegosystem, and then we analyze the security requirements of this construction to explain the preconditions for
the existence of IND-CHA stegosystem.
4.1

Construction of IND-CHA stegosystem

In this subsection, we turn attention to the eﬃcient construction of secure steganography with computationally hiding property. Before giving our stegosystem, we present a simple sampling algorithm
(sampler), which is a key part in our construction. In this algorithm, we still use the similar construction
suggested in [5,6], but just do a little bit of change to satisfy the requirements of an imperfect sampling
oracle model. The sampling algorithm can be brieﬂy described as follows:
Algorithm SampleC
f (k, b).

Require: a key k, a value b ∈ {0, 1}e ,
1: repeat
2:

s ←R O C ,

3: until fk (s) = b.
4: Return s.

Zhu Y, et al.

1644

Sci China Inf Sci

July 2012 Vol. 55 No. 7

In this construction, given a mapping function fk (·) : K × C → R with the key k and R = {0, 1}e , the
encoding algorithm in our stegosystem is based on the rejection-sampling algorithm SampleCf (k, b). This
algorithm can sample a covertext distribution C according to oracle OC , such that an e-bit symbol b would
been embedded in it. For the sake of simpliﬁcation, this algorithm is sometimes abbreviated to gkC (b).
S = {s1 , s2 , . . . , sm } is a set of all diﬀerent candidate samples, which are usually some inconspicuous
samples based on models of human (auditory and visual) perception. And then, the algorithm randomly
chooses a sample s from them until fk (s) = b, where ←R denotes the random choosing of oracle O. Our
objective is to analyze the property of mapping function fk (·) by this simple construction.
The rejection-sampling function has the following characters to induce greater usability and ﬂexibility
in ordinary covert channels: we do not assume the existence of a perfect oracle OC , “one that can
perform independent draws, one that can be rewound, etc”. This kind of independence assumption is
so strong that it is seldom satisﬁed in real-life applications 3) . Instead, the sampling oracle OC can
be executed under arbitrary channel in our rejection-sampling algorithm, so that the samples may be
related to each other for multi-times samplings. The reason for making this assumption is that the proof
of our subsequent theorem also does not rely on the distribution characteristic of samplers but the choice
of mapping function fk (·), when analyzing the indistinguishability between two sample sequences with
polynomial-size.
Algorithm EkC (m).

1: z ← Encoder (m).

Algorithm DkC (s).

1: Parse s as {s1 s2  · · · sl },

2: Parse z as {z1 z2  · · · zl },

2: for i = 1 To l do

3: for i = 1 To l do

3:

4:

si ← gkC (zi ),

zi ← fk (si ),

4: end for,

5: end for.

5: m ← Decoder(z1 z2  · · · zl ).

6: Return s ← {s1 s2  · · · sl }.

Return m.

We now turn to the description of the stegosystem. Algorithm E ﬁrst encodes an input message m using
the given encoding function Encoder, which outputs z. And then it repeatedly invokes Sample to embed
z into a sequence of covertext symbols. Algorithm D proceeds analogously: the message is extracted
from each constant-size symbols in a covertext s by mapping function f , then the concatenation of these
messages is decoded by algorithm Decoder, and the resulting value is returned.
4.2

Unpredictability and steganography

Now we devote into an analysis of the above stegosystem. Apparently, these algorithms (E and D)
construct a valid stegosystem. Without loss of generality, we assume that our analysis is based on binary
ﬁeld {0, 1}, namely the output of Encoder is a binary string and the output of fk (·) is a value in {0, 1}.
We ﬁrst consider the character of the encoder Encoder. We ﬁnd that the output of Encoder ought to
be unpredictable in order to realize the hiding property. This kind of unpredictability is deﬁned by
Goldreich [15] as follows:
Definition 9 (Unpredictability [15]). An ensemble {Xn }n∈N is called (tp , )-unpredictable in polynomial
time if for every probabilistic polynomial-time algorithm A and a negligible ,
Pr[A(1|Xn | , Xn ) = NextA (Xn )] <

1
+ ,
2

where NextA (x) returns the i + 1 bit of x if on input (1|x| , x) algorithm A reads only i < |x| of the bits
of x, and returns a uniformly chosen bit otherwise (i.e., in case A reads the entire string x), tp denotes
time-complexity in A.
3) As [5,6] had said, “Our deﬁnitions (this assumption) do not rule out eﬃcient constructions for channels where more is
known about the distribution”, “In practice, this oracle is also the weakest point of all our constructions”, and “A real-world
warden would use this to his advantage”.

Zhu Y, et al.

Sci China Inf Sci

1645

July 2012 Vol. 55 No. 7

In the above stegosystem, we require that each stegotext block be unpredictable in order to reach
indistinguishability between covertext and stegotext. In the same way, we also require the encoding
message hidden in each block is unpredictable. Without loss of generality, we will consider the binary
hiding, that is, zi ∈ {0, 1} for any i ∈ {1, 2, . . . , l} in z. We have the following theorem.
Theorem 1 (Unpredictability implies security of steganography). If the output of the encoding function
1
1
)-unpredictable (it can pass all next-bit tests) and the mapping function f is (tb , q(n)
)Encoder is (tp , p(n)

unbiased distributed, then the construction (SampleCf , EkC , DkC ) over the covertext distribute C is a (tp −
l(n)
+ 5l(n)
l(n)tb , l(n), l(n)|Σ|, p(n)
q(n) )-secure stegosystem against IND-CHA attack for all suﬃciently large n,
where l(n) is a polynomial in n, and |Σ| denotes the length of source alphabet Σ in C.
Proof. Assume that there is a probabilistic polynomial-time algorithm AC that can distinguish the
1
. Then, for an arbitrary hiddentext
output of algorithm E from the covertext c under C with at least p (n)
m, and all suﬃciently large n, we may drop the absolute value and assume that


 


 
Pr AC 1n , EkC (m) = 1 − Pr AC 1n , EkC (⊥) = 1 

1
.
p (n)

For any polynomial l(·), let the encoding result of the hiddentext m be denoted by z = (z1 , . . . , zn ) by
Encoder, where each zi is a bit and the length of z is n = l(n). We also parse the covertext c = EkC (⊥) as
{c1 c2  . . . cn }. For all k ∈ G(1n ), we consider the following sequence of distributions {pk,0 , . . . , pk,n }

on the symbol sets Σn :
⎧
⎪
pk,0 = {c1 , c2 , . . . , cn : c ← C},
⎪
⎪
⎪
⎪
⎪
⎪
pk,1 = {gkC (z1 ), c2 , . . . , cn : c ← C, z ← P},
⎪
⎪
⎪
⎪
..
⎪
⎪
⎪
.
⎪
⎨
pk,r = { gkC (z1 ), . . . , gkC (zr ), cr+1 , . . . , cn : c ← C, z ← P },
⎪
⎪
⎪
⎪
⎪
pk,r+1 = { gkC (z1 ), . . . , gkC (zr+1 ), cr+2 , . . . , cn : c ← C, z ← P },
⎪
⎪
⎪
⎪
..
⎪
⎪
⎪
.
⎪
⎪
⎪
⎩ p  = {g C (z ), g C (z ), . . . , g C (z  ) : z ← P}.
k,n
k 1
k 2
k n

We start with the true covertext (see pk,0 ), and in each step we replace one more sample of the covertext
from the left by a stegotext block, which is encoded by gkC . Finally, in pk,n we have the distribution of
the stegotext. So that, we observe that



 



pk,0
Pr AC 1n , EkC (⊥) = 1 = Pr AC (1n , y 0 ) = 1 : y 0 ←−− Σn ,





 
pk,n



Pr AC 1n , EkC (m) = 1 = Pr AC (1n , y n ) = 1 : y n ←−−− Σn .
According to our assumption, algorithm A is able to distinguish between the distribution of stegotext
pk,n and the distribution of covertext pk,0 . We say that A is able to distinguish between two subsequent
distributions pk,r and pk,r+1 , for some r ∈ N, as follows:
1
p (n)



 Pr[AC (1n , EkC (m)) = 1] − Pr[AC (1n , EkC (⊥)) = 1]






pk,n




pk,0
= Pr AC (1n , y n ) = 1 : y n ←−−− Σn − Pr AC (1n , y 0 ) = 1 : y 0 ←−− Σn
=


n
−1 






	


pk,r+1
pk,r
Pr AC (1n , y r+1 ) = 1 : y r+1 ←−−−− Σn − Pr AC (1n , y r ) = 1 : y r ←−− Σn
.

(2)

r=0

Since we notice that pk,r diﬀers from pk,r+1 only at one position,
 namely at r + 1, algorithm
 A can
also be used to successfully predict the next sample gkC (zr+1 ) from gkC (z1 ), gkC (z2 ), . . . , gkC (zr ) for some

Zhu Y, et al.

1646

Sci China Inf Sci

July 2012 Vol. 55 No. 7
4)

r; that is, for randomly chosen r, we expect that the r-th term in the sum is  n ·p1 (n) .
More precisely, we will derive a probabilistic polynomial-time algorithm D from algorithm A, which

can predict the next bit with higher probability from the inputs z = (z1 , . . . , zn ) ∈ {0, 1}n for the
inﬁnitely many k ∈ G(1n ). We start with a more precise description of algorithm D as follows:
1. Choose r uniformly in {0, 1, . . . , n − 1}.
2. Choose a covertxt sequence {cr+1 , . . . , cn } under distribution C, and sets


y r = gkC (z1 ), . . . , gkC (zr ), cr+1 , . . . , cn .
3. If AC (i, y r ) = 1, then output fk (cr+1 ) as a prediction, and otherwise output 1 − fk (cr+1 ), namely

AC (1n , y r ) = 1,
fk (cr+1 ),
C
D (r, z) =
AC (1n , y r ) = 0.
1 − fk (cr+1 ),
1
-unbiased function (the output of
Without loss of generality, if we can assume that fk (·) is a q(n)
function is approximately uniformly distributed with a negligible bias), then we assume min(Pr[fk (cr+1 ) =
1
in terms of Lemma 1, where z r+1 = 1 − zr+1 . Moreover, using the
zr+1 ], Pr[fk (cr+1 ) = z r+1 ])  12 − q(n)
deﬁnition of A, we get




sD (n ) = Pr DC (1n , z) = NextD (z) : z ← {0, 1}n



⎫
⎧
pk,r


−1 ⎨ Pr AC (1n , y r ) = 1, fk (cr+1 ) = zr+1 : y r ←−− Σn
n
⎬
1



= 
pk,r

⎩ + Pr AC (1n , y r ) = 0, 1 − f (c ) = z
n
: y r ←−− Σn ⎭
r=0

k

r+1

r+1




⎧
C
n r
r pk,r
n
⎪
Pr
A
·
(1
,
y
)
=
1
:
f
(c
)
=
z
,
y
←
−
−
Σ
⎪
k
r+1
r+1
⎪
⎪

−1 ⎪
n
⎨
Pr[fk (cr+1 ) = zr+1 : cr+1 ← OC ]
1



= 
pk,r

n r=0 ⎪
⎪ + Pr AC (1n , y r ) = 0 : fk (cr+1 ) = z r+1 , y r ←−− Σn ·
⎪
⎪
⎪
⎩ Pr[f (c ) = z
C
k r+1
r+1 : cr+1 ← O ]
 


⎧

C
n r+1
r+1 pk,r+1
n
⎪
Pr
A
·
1
=
1
:
y
,
y
←
−
−
−
−
Σ
⎪
⎪
⎪

n
−1 ⎪
⎨
C
Pr[fk (cr+1 ) = zr+1 : cr+1 ← O ]+
1
	

	

 
= ·
pk,r+1

n r=0 ⎪
·
1 − Pr AC 1n , y r+1 = 1 : y r+1 ←−−−− Σn
⎪
⎪
⎪
⎪
⎩ Pr[f (c ) = z
C
k r+1
r+1 : cr+1 ← O ]
1
1
 −
+
2 q(n)



1
1
−
2 q(n)



⎫
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎭

⎫
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎭

⎫
⎧
 



pk,r+1

C
n r+1
n −1
⎬
= 1 : y r+1 ←−−−− Σn
1  ⎨ Pr A 1, y
	


· ·
p

k,r+1
n r=0 ⎩ − Pr AC 1n , y r+1 = 1 : y r+1 ←−
−−− Σn ⎭

⎫
 



pk,r+1

⎬
Pr AC 1n , y r+1 = 1 : y r+1 ←−−−− Σn
	


 
,
p

k,r+1
⎩ − Pr AC 1n , y r+1 = 1 : y r+1 ←−−−− Σn ⎭
⎧



2
1
1
−
+
·
2 q(n) 2n


n
−1 ⎨

r=0

pk,r

(3)



where y r+1 = (gkC (z1 ), . . . , gkC (zr ), gkC (z r+1 ), cr+2 , . . . , cn ). Note that, y r ←−− Σn includes cr+1 ← OC .
On the assumption that fk (·) is an -unbiased function, we know that y r is distributed identically to the
1
1
and 12 − q(n)
, respectively. Thus,
distribution obtained by taking y r+1 and y r+1 with probability 12 + q(n)
we obtain




pk,r
Pr AC (1n , y r ) = 1 : y r ←−− Σn




pk,r+1
pk,r


4) Note that, we cannot have Pr AC (1n , y r+1 ) = 1 : y r+1 ←−−−−− Σn − Pr AC (1n , y r ) = 1 : y r ←−−− Σn 
considering that some elements may be 

1
n ·p (n)

but it is not necessary for others.

1
,
n ·p (n)

Zhu Y, et al.

Sci China Inf Sci

1647

July 2012 Vol. 55 No. 7





pk,r
= Pr AC (1n , y r ) = 1 : fk (cr+1 ) = zr+1 , y r ←−− Σn Pr[fk (cr+1 ) = zr+1 : cr+1 ← OC ]




pk,r
+ Pr AC (1n , y r ) = 1 : fk (cr+1 ) = z r+1 , y r ←−− Σn Pr[fk (cr+1 ) = z r+1 : cr+1 ← OC ]
 



pk,r+1
1
1
−
Pr AC (1n , y r+1 ) = 1 : y r+1 ←−−−− Σn
2 q(n)

	


pk,r+1
.
+ Pr AC (1n , y r+1 ) = 1 : y r+1 ←−−−− Σn




(4)

This equation implies




pk,r+1
Pr AC (1n , y r+1 ) = 1 : y r+1 ←−−−− Σn





 

1
C n r
r pk,r
n
C n r+1
r+1 pk,r+1
n
− Pr A (1 , y ) = 1 : y
←−−−− Σ
· Pr A (1 , y ) = 1 : y ←−− Σ
.

1
( 12 − q(n)
)
Thus, in term of Eq. (2), we get
⎫
⎧
 



pk,r+1

n
−1 ⎨ Pr AC 1n , y r+1 = 1 : y r+1 ←
n
⎬
−
−
−
−
Σ
2
1
1
	


 
sD (n ) = −
+ 
p

k,r+1
2 q(n) 2n r=0 ⎩ − Pr AC 1n , y r+1 = 1 : y r+1 ←−
−−− Σn ⎭



⎧
⎫
C n r+1
r+1 pk,r+1
n
⎪
⎪
Pr
A
−
(1
,
y
)
=
1
:
y
←
−
−
−
−
Σ
⎪
⎪

−1 ⎪
n

 ⎞ ⎪

⎨
⎬
⎛
p

k,r
2
1
1
1
C n r
r
n
· Pr A (1 , y ) = 1 : y ←−− Σ −
+ 
 −
1
1
−
2 q(n) 2n r=0 ⎪
⎝ ( 2  q(n) )
⎠ ⎪


⎪
⎪
pk,r+1

⎪
⎪
⎩
⎭
Pr AC (1n , y r+1 ) = 1 : y r+1 ←−−−− Σn
⎧ 



C n r+1
r+1 pk,r+1
n
⎪
2
Pr
A
−
(1
,
y
)
=
1
:
y
←
−
−
−
−
Σ
⎪
n −1 ⎪

	

1
2
1 ⎨
C n r
r pk,r
n
= −
+
−
2 Pr A (1 , y ) = 1 : y ←−− Σ
2 q(n) 2n r=0 ⎪



⎪
p

k,r
⎪
4
⎩
Pr AC (1n , y r ) = 1 : y r ←−− Σn


q(n)−2

⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭

1
2
1
2
−
+
−
2 q(n) n p (n) q(n) − 2
5
1
1
+  
,
 −
2 q(n) n p (n)


5
where q(n)  6. This means that the advantage of algorithm D is at least n p1 (n) − q(n)
in this experiment

for n = l(n). This is in contradiction to our hypothesis that the embedded sequence z is unpredictable
in a polynomial-time. The time-complexity t of our construction is at most tp − n tb since tp  t + n tb .
l(n)
1
1
is at most p(n)
+ 5l(n)
Moreover, the advantage of adversaries for this construction p (n)
q(n) since p(n) 
1
5
n p (n) − q(n) for at most l(n) queries and at most l(n)|Σ| bits of hiddentext. The theorem follows.
The proof made use of a so-called “hybrid” argument. More importantly, this proof is provided with
the predictability in worst case. This means that secure steganography must be unpredictable for all bits
of hiddentext. Conversely, a stegosystem should be insecure only if an adversary can guess successfully on
some bits. In the practical applications, we are able to use these bits to predict the message of hiddentext
or to implement the steganalysis [16].

4.3

Preconditions of secure stegosystems

In order to prove the existence of computational hiding property in ordinary covert channels, we propose a
new construction (SampleCf , EkC , DkC ) with imperfect oracle, in which the samples are not necessarily to be
independent of each other. The conclusion of the above theorem is signiﬁcant to steganography analysis;
moreover the process of proof has some interesting meanings for the construction of the computationally
secure stegosystems. In the construction we know that there exist two kernel parts: mapping function f
and encoding function Encoder. According to the proof process, we have the following preconditions:

Zhu Y, et al.

1648

Precondition 1.

Sci China Inf Sci

July 2012 Vol. 55 No. 7

The mapping function fk (·) is an unbiased function for a key k.

Precondition 2. The encoding function Encoder(·) can generate an unpredictable sequence.
1
-unbiased
Precondition 1 is derived from the assumption of Eqs. (3) and (4): the function fk (·) is a q(n)
1
1
1
over
function with a polynomial negligible bias q(n) . This means that | Prc←C [fk (c) = b] − 2 |  q(n)
distribution C for any b ∈ {0, 1}. It is easy to deduce

1
2

and max{Prc←C [fk (c) = b], Prc←C [fk (c) = b]} 
following simple lemma:

+

1
2

1
− q(n)
 min{Prc←C [fk (c) = b], Prc←C [fk (c) = b]}
1
q(n)

for arbitrary distribution C in terms of the

1
|  . Let S
Lemma 1. Given an -unbiased function f : K × C → R, that is, | Prx←C [fk (x) = r] − |R|
be an arbitrary distributed random variable with values in R. Assume that S and f are independent.
1
1
−   Prx←C [fk (x) = S]  |R|
+ .
Then |R|
1
−   Prx←C [fk (x) = r] 
Proof. In terms of the deﬁnition of -unbiased function, we have |R|
Since fk (x) is independent of S, we have

Pr [S = r|fk (x) = r] · Pr [fk (x) = r]
Pr [fk (x) = S] =
x←C

r←R




x←C

1
|R|

+ .

x←C

1
+
|R|


r←R

Pr[S = r] =

1
+ .
|R|

Thus, we also have Prx←R [fk (x) = S] 
− , and the lemma follows.
Normally, the above lemma means that the mapping function f is independent of the covertext distribu5
+ n p1 (n) .
tion C.5) In Theorem 1, the success probability of prediction of the algorithm D is at least 12 − q(n)
1
It is obvious that the bias of mapping function q(n)
decreases the success probability of prediction. Moreover, as a special case of this theorem, the success probability of prediction is at least 12 + n p1 (n) when
the mapping function fk (x) is an unbiased function. Note that, we emphasize that this function ought
to employ a key k to realize the randomness of the sampling process. Obviously, we can use one-way
function with a hard-core predicate or balanced boolean function to construct an unbiased function.
Precondition 2 is the direct consequence of cryptographic theory. Yao has indicated that an ensemble
is pseudorandom if and only if it is unpredictable in polynomial time as follows:
1
|R|

Theorem 2 (Yao’s Theorem [14,15]). Let I = (In )n∈N be a key set with security parameter n, and
G = (Gk : Xk −→ {0, 1}l(n))k∈I be a pseudorandom bit generator with polynomial stretch function l and
key generator K. Then G is computationally perfect if and only if G passes all next-bit tests.
Yao’s theorem means that the proposed scheme is able to realize the Precondition 2 of computational
hiding stegosystem if there exists a pseudorandom bit generator. Furthermore, according to relationship
between one-way function and pseudorandom bit generator, the weaker assumption of the computational
hiding stegosystem is that there exists a strong one-way function, which can induce a pseudorandom bit
generator [15].
Taking these two preconditions together, we claim that a (block-based) stegosystem with IND-CHA
security enjoys the following characteristics:
Corollary 1. There exists a computationally secure stegosystem with IND-CHA security, if the following two conditions are satisﬁed: 1) there exists a pseudorandom function with a negligible error, which
is used to construct the encoder of the hiddentext message; 2) there exists an unbiased function with a
negligible bias, which is used to construct the rejection-sampling function.
l(n)
+ 5l(n)
Note that, the advantage of adversaries in winning IND-CHA game is at most p(n)
q(n) in terms
of Theorem 1, where the length of hiddentext l(n) must be far less than the polynomial value p(n) and
q(n). This means that the adversary’s advantage is proportional to the length of hiddentext l(n) and is
inversely proportional to the polynomials p(n) or q(n). The less l(n) or the larger p(n) and q(n) there
are, the more secure the stegosystem is.
5) The reason is that we have Pr[fk (x) = r] =
function fk (x).



c∈C

Pr[fk (c) = r] · PrC [c] =

1
|R|



c∈C

PrC [c] =

1
|R|

for the unbiased

Zhu Y, et al.

5

Sci China Inf Sci

July 2012 Vol. 55 No. 7

1649

Conclusions and further work

In this paper, we provide a more general construction of secure steganography without any special assumptions. We prove that our construction is a computationally secure stegosystem with indistinguishability
against adaptive chosen hiddentext attacks. Based on it, the security analysis proves that the computationally secure steganography is feasible if there exist pseudorandom function and unbiased sampling
function. Similarly, both trapdoor one-way permutation and unbiased sampler also are the foundation
of construction of secure private-key and public-key steganography schemes. These results are practically useful for construction and analysis of stegosystems. As part of future work, we will construct a
proof-of-concept implementation of our method with extensive system evaluation.

Acknowledgements
This work of Zhu Yan, Yu Mengyang, and Zhao Hongjia was supported by National Natural Science Foundation of
China (Grant Nos. 61170264, 10990011). Ahn Gail-Joon and Hu Hongxin were partially supported by US National
Science Foundation (Grant Nos. NSF-IIS-0900970, NSF-CNS-0831360) and Department of Energy (Grant No.
DE-SC0004308). Many thanks to our collaborators, Stephen S. Yau, from Arizona State University for very
valuable comments and suggestions.

References
1 Feamster N, Balazinska M, Wang W, et al. Thwarting Web censorship with untrusted messenger discovery. In:
Dingledine R, ed. Privacy Enhancing Technologies. LNCS 2760. Berlin: Springer, 2003. 125–140
2 Serjantov A, Danezis G. Towards an information theoretic metric for anonymity. In: Dingledine R, Syverson P F, eds.
Privacy Enhancing Technologies. LNCS 2482. Berlin: Springer, 2002. 41–53
3 Boneh D, Shaw J. Collusion-secure ﬁngerprinting for digital data. IEEE Trans Inf Theory, 1998, 44: 1897–1905
4 Cachin C. An information-theoretic model for steganography. In: Aucsmith D, ed. Proceedings of the 2nd Workshop
on Information Hiding (IH 1998). LNCS 1525. Portland: Springer, 1998. 306–318
5 Hopper N J, Langford J, Ahn L V. Provably secure steganography. In: Yung M, ed. Advances in Cryptology CRYPTO 2002. LNCS 2442. Santa Barbara: Springer, 2002. 77–92
6 Hopper N J, Ahn L V, Langford J. Provably secure steganography. IEEE Trans Comput, 2009, 58: 662–676
7 Möller B. A public-key encryption scheme with pseudo-random ciphertexts. In: Samarati P, Ryan P Y A, Gollmann
D, et al, eds. Proceedings of the 9th European Symposium on Research in Computer Security, ESORICS 2004. LNCS
3193. Sophia Antipolis: Springer, 2004. 335–351
8 Dedic N, Itkis G, Reyzin L, et al. Upper and lower bounds on black-box steganography. In: Backes M, Cachin C, eds.
Theory of Cryptography, Second Theory of Cryptography Conference, TCC 2005. LNCS 3378. Cambridge: Springer,
2005. 227–244
9 Ahn L V, Hopper N J. Public-key steganography. In: Cachin C, Camenisch J, eds. Advances in Cryptology EUROCRYPT 2004. LNCS 3027. Interlaken: Springer, 2004. 323–341
10 Backes M, Cachin C. Public-key steganography with active attacks. In: Kilian J, ed. Theory of Cryptography, Second
Theory of Cryptography Conference (TCC 2005). LNCS 3378. Cambridge: Springer, 2005. 210–226
11 Hopper N J. On steganographic chosen covertext security. In: Caires L, Italiano G F, Monteiro L, et al, eds. Proceedings
of the 32nd International Colloquium on Automata Languages and Programming (ICALP 2005). LNCS 3580. Lisbon:
Springer, 2005. 311–323
12 Lysyanskaya A, Meyerovich M. Provably secure steganography with imperfect sampling. In: Yung M, Dodis Y, Kiayias
A, et al, eds. Proceedings of the 9th International Conference on Theory and Practice of Public-Key Cryptography
(PKC 2006). LNCS 3958. New York: Springer, 2006. 123–139
13 Goldwasser S, Bellare M. Lecture Notes on Cryptography. 2008. Http://www-cse.ucsd.edu/ mihir/papers/gb.html
14 Yao A C. Theory and application of trapdoor functions. In: Proceedings of the 23rd IEEE Symposium on Foundations
of Computer Science (FOCS 1982). Chicago: IEEE Computer Society, 1982. 80–91
15 Goldreich O. Foundations of cryptography volume I, Basic tools. London: Cambridge University Press, 2001
16 Luo X Y, Liu F L, Yang C F, et al. Image universal steganalysis based on best wavelet packet decomposition. Sci
China Inf Sci, 2010, 53: 634–647

Software Enterprise Pedagogy for Project-Based Courses
Kevin Gary, Srividya Bansal, Arbi Ghazarian
Arizona State University
Mesa, AZ 85212
USA
{kgary*, srividya.bansal, arbi.ghazarian} @ asu.edu

Abstract
The Software Enterprise is a pedagogical model combining traditional lecture with
project-based learning. The Enterprise model leads students through a modular series
of lessons that combine foundational concepts with skills-based competencies. In this
tutorial, software engineering educators in higher education or industry will learn the
methodology, get exposed to assessment techniques such as e-portfolios and concept
maps, and become familiarized with the open resources available to educators that
adopt the pedagogy. This tutorial should be of interest to any educator interested in
making their project-based courses more engaging and more relevant to students
needing to be ready to practice the profession from the first day they exit the Enterprise
environment.

1. Overview of the Software Enterprise
The Software Enterprise has been in use at Arizona State University since 2004, and
now forms the "project spine" that runs the full four years of the new B.S. in software
engineering degree program at ASU and the two-year M.S. in Computing Studies
graduate program. The primary goal of the Enterprise is to move students rapidly from
foundational concepts to industry best practices, so students completing the degree
program are prepared from day one to meaningfully contribute to the profession. The
Enterprise pedagogy takes students from introduction of a concept to scalable practice
in a real project in the span of a 3-week "sprint." This is in contrast to typical degree
programs that introduce a concept with toy problems in one course and then expect
students to synthesize the multiple concepts in a capstone project in a different course
(perhaps years later).
In the Enterprise model, students are first exposed to concepts via traditional
dissemination activities; lectures, readings, and discussion. They then put the skill into
practice using an industry-accepted tool in a mentored lab exercise. The practice is then
adapted and incorporated into the current iteration of the lifecycle process model on a
scalable project. Students complete the learning cycle by reflecting on the lessons
learned while applying the technique on their project. The key is these activities take
place closely in time as opposed to over a semester (or more).
As an example, consider the topic configuration management (CM). CM concepts
such as quality thresholds for codelines, good and bad branching patterns, and change
management are presented and discussed in class meetings. At the end of the same
week, CM is grounded via a half-day problem-centered lab in a CM tool. The following
week, CM is incorporated into the project. At the end of the three-week project
iteration, teams are required to write in their team journal (a Wiki), a rationale for how

368
c 2013 IEEE
978-1-4673-5140-9/13/$31.00 

CSEE&T 2013, San Francisco, CA, USA

they applied the technique into the project and an evaluation of how well the approach
worked. Finally, individual students are also required to log an individual journal entry
on her/his thoughts on the utility of the technique at the end of the iteration
The Software Enterprise is a flexible model for delivery, and as such allows a wide
variety of techniques to be incorporated into the approach. The facilitators have
experimented with peer mentoring, entrepreneurial projects, outsourcing, and other
variations. The study of what teaching and learning innovations may be incorporated
into a project-centric framework such as the Software Enterprise is a significant area of
future research for the team. Further, existing and freely available course materials
(from the Enterprise website, textbooks, or 3 rd party sites such as SWENET) may easily
be utilized in the framework. The primary desired outcomes for attendees of this
tutorial are to 1) understand how the instructor-as-facilitator can re-orient their existing
approaches to one that is project-centered and contextualized, and 2) communicate
lessons learned from a near-decade of experience, 3) suggest alternative forms of
assessment for project-centered learning, and 4) promote a project-centered form of
curricular design for the future.

2. Presenters
Kevin Gary, Associate Professor, Department of Engineering, Arizona State University
kgary@asu.edu, http://dcs.asu.edu/faculty/KevinGary
Dr. Gary designed and created the Software Enterprise in 2004. He has received over $200K
in support funding and mentored over 40 student team projects for industry. Prior to joining
ASU, Dr. Gary spent several years in industry as an enterprise systems architect. His
motivation is to promote the education of software engineering through rigorous pedagogy
and close attention to current best practices.
Srividya Bansal, Assistant Professor, Department of Engineering, Arizona State University
Srividya.bansal@asu.edu, http://dcs.asu.edu/faculty/skbansal
Dr. Bansal teaches the sophomore year of the Software Enterprise, and mentors senior
Enterprise projects. Dr. Bansal's research interests are in the area of semantic web services.
Before joining the ASU faculty in 2010, Dr. Bansal was a Visiting Professor at Georgetown
University and earned her Ph.D. in Computer Science from the University of Texas-Dallas.
Arbi Ghazarian, Assistant Professor, Arizona State University
Arbi.ghazarian@asu.edu, http://technology.asu.edu/directory/1627371
Dr. Ghazarian teaches the senior year Enterprise capstone project sequence, leading and
mentoring several industry projects. His research interests include software requirements and
software design. Prior to joining the ASU faculty in 2009, Dr. Ghazarian held a postdoctoral
appointment at the University of Toronto, and spent several years doing industry consulting.

3. Intended Audience and Pre/Post-Conference Expectations
This tutorial is primarily intended for faculty at 2- or 4-year colleges and universities,
which offer software engineering degrees, minors, certificates, or courses. Industry
professionals involved in corporate training will also benefit from the presentation as the
modular approach of the Software Enterprise translates to this environment. However, the

369

project aspects of the tutorial will not be as beneficial for these attendees, though professional
experience is a viable substitute for project-based learning as a contextualizing experience.
Prior to the conference attendees will be asked to review materials on the Software
Enterprise community site, perform a simple assessment exercise, and take a short online
survey. Total anticipated preparation time pre-conference is 30-60 minutes. After the
conference attendees will be asked to provide feedback via an online quantitative and
qualitative survey. Educators interested in pursuing deeper collaboration will be provided
instruction on how to engage the larger Software Enterprise community in a variety of
potential roles.

4. Agenda
The requested amount of time for the tutorial is 120 minutes. The suggested agenda is as
follows:
1. Introductions and objectives of participants (10 minutes)
2. Overview of the Software Enterprise model and website (10 minutes)
3. The sequence of a software engineering module in the Software Enterprise
(interactive, 25 minutes)
4. Assessment in the Software Enterprise (interactive, 20 minutes)
5. Incorporating pedagogical innovations in the Enterprise framework (20 minutes)
6. Pitfalls and lessons learned in project-centered teaching (10 minutes)
7. Resources available to the Software Enterprise community (20 minutes)
8. Future Directions and Wrap-up (5 minutes)

5. About this Tutorial
A recently concluded NSF CCLI Phase I grant, an IBM Jazz Innovations Award, a
Kaufmann Pathways to Entrepreneurship Grant (PEG), and an Arizona Board of
Regents Learner-Centered Education (LCE) award have supported this work. This
tutorial was previously presented at the Software Engineering and Applications (SEA)
conference in November 2012, and feedback from that presentation has been used to
focus this proposal on the most beneficial areas for attendees. Specifically, an insession assessment piece has been refactored into the pre-conference activity, as it was
too time consuming during the session. Time was added at the end to allow for
discussion and input from the attendees on project-centered software engineering
education. Finally, an activity was added (#5) to discuss how variations on Enterprise
delivery have been (or we anticipate will be) integrated in the delivery model. These
variations include cross-year mentoring, service learning, team formation,
entrepreneurial projects, software process models, and several more.

370

Building Faculty Expertise in Outcome-based
Education Curriculum Design
Srividya Bansal, Ashraf Gaffar

Odesma Dalrymple

School of Computing, Informatics, Decision Systems Engg.
Arizona State University, Mesa, AZ, USA
{srividya.bansal, ashraf.gaffar}@asu.edu

Shiley-Marcos School of Engineering
University of San Diego, San Diego, CA, USA
odesma@sandiego.edu

Abstract— An information technology (IT) tool that can guide
STEM educators through the complex task of course design
development, ensure tight alignment between various
components of an instructional module, and provide relevant
information about research-based pedagogical and assessment
strategies will be of great value. A team of researchers is engaged
in a User-Centered Design (UCD) approach to develop the
Instructional Module Development System (IMODS), i.e., a
software program that facilitates course design. In this paper the
authors present the high-level design of the IMODS and
demonstrate its use in the development of the curriculum for an
introductory software engineering course.
Keywords—instruction design; outcome-based
semantic web-based application; user-centered design

I.

education;

INTRODUCTION

Felder, Brent and Prince [1] have made a strong argument in
support of instructional development training for engineering
faculty. This argument, which cites among other reasons:
shortfalls in graduation rates, changing demographics and
attributes of the student body, and modifications in the
expectations of graduates; can be extended to encompass all
STEM fields. Furthermore, studies show that for 95% of new
faculty members, it takes four to five years, through trial and
error (the most common method of gaining expertise in
teaching), to deliver effective instruction [2]. While there are
a number of options available to faculty for receiving
instructional development training (i.e., training focused on
improving teaching and learning), most share similar format,
features, and shortcomings. For example: workshops, courses
and seminar series, the most common program structures, are
often offered at a cost to the institution, department or
individual attendee; delivered face-to-face at specified times;
and accessible to a restricted number of persons. Even when
interest is high, these factors can become obstacles to
participation.
Outcome-based Education (OBE) is a result-oriented approach
where the product defines the process. The learning outcomes
guide what is taught and assessed [3], [4]. This approach
contrasts the preceding “input-based” model that places
emphasis on what is included in the curriculum as opposed to
the result of instruction. There is a growing demand and
interest in faculty professional development in areas such as
OBE, curriculum design, and pedagogical and assessment
strategies.
In response to these challenges and needs, a group of faculty

978-1-4799-8454-1/15/$31.00 ©2015 IEEE

researchers from two south-western universities have
undertaken a project to design and develop the Instructional
Module Development System (IMODS) that will facilitate
self-paced instructional development training while the user
creates his/her course design with the added benefits of being
free to all who are interested, accessible almost anywhere
through a web browser, and at any time that is convenient.
Additional key features of the IMODS are as follows:
1. Guides individual or collaborating users, step-by-step,
through an outcome-based education process as they
define learning objectives, select content to be covered,
develop an instruction and assessment plan, and define
the learning environment and context for their course(s).
2. Contains a repository of current best pedagogical and
assessment practices, and based on selections the user
makes when defining the learning objectives of the
course, the system will present options for assessment and
instruction that align with the type/level of student
learning desired.
3. Generates documentation of a course designs. In the same
manner that an architect’s blue-print articulates the plans
for a structure, the IMODs course design documentation
will present an unequivocal statement as to what to expect
when the course is delivered.
4. Provides just-in-time help to the user. The system will
provide explanations to the user on how to perform course
design tasks efficiently and accurately. When the user
explores a given functionality, related explanations will
be made available.
5. Provides feedback to the user on the fidelity of the course
design. This will be assessed in terms of the cohesiveness
of the alignment of the course design components (i.e.,
content, assessment, and pedagogy) around the defined
course objectives.
In this paper the authors present the high-level design of the
IMODS and demonstrate its use in the development of the
curriculum for an introductory software engineering course.
The rest of the paper is organized as follows. Background
material for this research project is presented in section 2.
Section 3 presents the high-level design of the IMODS
software system. Section 4 presents a case study that
demonstrates the use of the IMODS framework in the
development of an introductory software engineering course.
The paper concludes with future work and acknowledgements.

II.

BACKGROUND

A. Related Work
To justify the need for the development of IMODS we
conducted a competitive analysis to determine the strengths
and weaknesses of tools and approaches currently used to
support course design and related training. The tools and
approaches that were evaluated were categorized into five
groups based on primary functions and features.
Knowledge/Learning Management System (KMS/LMS):
This group contains a number of proprietary and open-source
solutions that are delivered either as desktop or web-based
applications. These tools mainly facilitate the administration
of training, through the (semi-) automation of tasks such as:
registering users, tracking courses in a catalog, recording data,
charting a user’s progress toward certification, and providing
reports to managers. These tools also serve as a platform to
deliver eLearning to students. In that context, their main
purpose is to assemble and deliver learning content,
personalize content and reuse it.
Examples: Blackboard, Moodle, Sakai, Canvas, WeBWorK,
and Olat
Educational Digital Libraries: These tools contain
collections of learning and educational resources in digital
format. They provide services that support the organization,
management, and dissemination of the digital content for the
education community.
Examples: National Engineering Education Delivery System
(NEEDS), National Science Digital Library (NSDL) and
Connexions
Personalized Learning Services: There are a number of elearning tools that leverage semantic web technologies to
support personalized learning services for their users with an
ontology based framework [5]. Some of these tools function
by initially profiling the learner and then, based on that
profile, identifying the best strategies for presenting resources
to them. They can also provide feedback to instructors on
student learning, so improvements to the content and structure
of the course can be incorporated. For many of these tools the
ontology framework is used to bridge learning content with
corresponding pedagogy; however, they seldom address
assessments and learning objectives. Examples: Content
Automated Design and Development Integrated Editor
(CADDIE), Intelligent Web Teacher (IWT), LOMster [6], and
LOCO-Analyst [7].
Understanding by Design Exchange (UbD Exchange): This
is a software framework based on Wiggins’s and McTighe’s
Backward Design principle [8] that is used for designing
curriculum, assessments, and instruction, and integrates K-12
state and provincial standards in the design of units [9]. It
provides a form-based user interface to fill in the details of the
course unit that is being designed.

Professional Development Workshops, Courses &
Seminars: Face-to-face training sessions in teaching and
learning that are facilitated by experts in the field of
instructional design.
Examples: National Effective Teaching Institute, Connect
Student Learning Outcomes to Teaching, Assessment, and
Curriculum, Content, Assessment ant Pedagogy [10].
Our search identified very few tools and approaches that
contained features or functionality that explicitly facilitated
the design of course curriculum. Of these tools few of them
facilitated the generation of design documentation and
feedback to the user on the fidelity of the design. These are
two key deficiencies that IMODS will address.
B. IMODS Framework – PC3 Model
The IMOD framework adheres strongly to the OBE approach
and treats the course objective as the spine of the structure.
New constructs (not included in the models previously
discussed) are incorporated to add further definition to the
objective. The work of Robert Mager [11] informs the IMOD
definition of the objective. Mager identifies three defining
characteristics of a learning objective: Performance –
description of what the learner is expected to be able to do;
Conditions – description of the conditions under which the
performance is expected to occur; and the Criterion – a
description of the level of competence that must be reached or
surpassed. For use in the IMOD framework an additional
characteristic was included, i.e., the Content to be learned –
description of the factual, procedural, conceptual or metacognitive knowledge; skill; or behavior related to the
discipline. The resulting IMOD definition of the objective is
referred to as the PC3 model [12].
The other course design elements (i.e., Content, Pedagogy,
and Assessment) are incorporated into the IMOD framework
through interactions with two of the PC3 characteristics.
Course-Content is linked to the content and condition
components of the objective. The condition component is
often stated in terms of pre-cursor disciplinary knowledge,
skills or behaviors. This information, together with the content
defined in the objective, can be used to generate or validate
the list of course topics. Course-Pedagogy is linked to the
performance and content components of the objective. The
types of instructional approaches or learning activities used in
a course should correspond to the level of learning expected
and the disciplinary knowledge, skills or behaviors to be
learned. The content and performance can be used to validate
pedagogical choices. Course-Assessment is linked to the
performance and criteria components of the objective. This
affiliation can be used to test the suitability of the assessment
strategies since an effective assessment, at the very least, must
be able to determine whether the learner’s performance
constitutes competency. Figure 1 shows a visual
representation of the IMOD framework. Learning domains
and domain categories defined by Bloom’s revised taxonomy
[11] are used to describe learner performance. Learning
domains are categorized into Cognitive, Affective, and

Psychomotor, which are further classified under various
Domain Categories (Remember, Understand, Apply, Analyze,
Evaluate, Create). Each Domain Category has performance
verbs associated to it. Learning objective in the PC3 model is
described in terms of Performance, Content, Condition, and
Criteria. Performance is described using an appropriate action
verb from revised Bloom’s taxonomy based on the learning
domain and domain category.
Criteria: Learning objective assessment criteria are
categorized as quality, quantity, speed, and accuracy. Criteria
for learning objectives are described in terms of one or more
of these categories with a criteria value defined or determined
later when the assessment is defined.
Knowledge Dimensions: The revised Bloom’s taxonomy
introduced an additional dimension called the knowledge
dimension that was categorized as Factual, Conceptual,
Procedural and Metacognitive.
Topic Prioritization: The IMODS framework uses a
prioritization framework that classifies topics and subtopics of
a particular course as one of the following:
• Critical
• Important
• Good to know
Achieving the right mix of the three levels of learning
(priorities) is essential to planning a good course.

enduring foundation of knowledge, skills and habits of mind
about curriculum development.
UCD is an emerging design method that focuses on both
operational and technical requirements by observing and
understanding user needs and wants, as well as by prototyping
and testing software throughout all phases of software
lifecycle. It enables the capturing and resolution of any
mismatches between users and software early on. The main
objective of UCD is to allow for a closer match between users
and the software, leading to a more intuitive interaction. As a
design process, it also has the objective of reaching that goal in
the most resource-efficient way, in terms of time and cost
through careful planning and execution [14].
The UCD process can be divided into five main phases: Plan,
User Research, Design, Develop, and Measure. Thus far, the
research team has completed the user research and design
phases of the UCD process. In this respect, 4 focus group
sessions were conducted with prospective users of IMODS to
gather insights on how faculty approach the task of designing a
course. At the beginning of each session all participants were
asked to fill an electronic background survey that collected
demographic information, primary areas of interest in teaching
and research, time spent on teaching, number of courses taught
per year (at both undergraduate and graduate levels), and
number of new courses developed (both at undergraduate and
graduate levels). Participants were also asked to fill an
electronic questionnaire about curriculum design tools that they
currently use to create and manage their courses (e.g. preparing
syllabi; communicating with students; developing teaching
materials; preparing, assigning, and delivering grades, etc.).
The results of this phase were published in ASEE 2014 and
FIE 2014 [15], [16]. The results from the focus group helped
identify the key features of the software system; an ontology
that defines the relevant terms of the domain and identifies
their specific meaning as well as the potential relationships
between them; and mental model, which is a tool to improve
understanding of the user needs and activities. Figure 2 shows
the ontological concepts and relationships between concepts as
a hierarchy. Figure 3 shows the mental model that depicts an
affinity diagram of similar activities organized sequentially into
9 towers in the upper half with detailed relevant activities in the
lower half.

Figure 1: IMODS Framework - PC3 model
C. User-centered design methodology
The IMODS system is being developed using a user-centered
design (UCD) methodology, as opposed to technology focused,
methodology [13]. This approach is well suited for the project
given the high cognitive nature of outcome-based course
design tasks, and the high levels of interactions required
between the user and the system to not only facilitate the
development of course designs, but to help users build an

Figure 2: IMODS Ontological Concepts and Relationships

Figure 5: IMODS Learning Objective Overview
Then as we planned and executed the testing, we were able to
directly ask the user to work on a “Criteria” within “learning
objectives”. Our dual purpose is to let the user know that those
are two concepts of the IMOD, as well as knowing the
structural relationship between them.

Figure 3: Mental Model
III.

HIGH-LEVEL DESIGN OF IMODS

One of the biggest challenges of software design is to make
sure the user has sufficient understanding to use the
application successfully and accomplish the required tasks.
Our User-centered design approach followed two main phases:
A. Conceptualization Phase
After the user research provided a relatively clear idea and
understanding of domain- and user needs, this initial design
phase provides a high-level design with concepts
identification, conceptual modeling and early prototyping.
During initial conceptualization and high-level design, we
focused on Brainstorming sessions and contextual analysis to
build an initial concept of the application. We gradually
consolidated it into a set of requirements on flip charts and
PowerPoint slides. The main goal of high-level design is to
plot down schematic ideas and steps into visual graphs and
models; an early blueprint. We started by investigating
different options and providing design alternatives to make
sure we have a broad view before identifying a good design.
Doing this early on, at high-level, sketchy, paper-based only,
and without going into details help provide several solution
alternatives at a very low cost.
The IMODS system is conceptualized such that a course
design is centered around the learning objectives of the course
defined by the instructor (user) as shown in Figure 4. Learning
objectives are directly associated with the course content,
assessments, and pedagogical activities as defined in the PC3
model (Figure 5).

Figure 4: IMODS System Overview
The Learning Objectives component of the IMODS system
was conceptualized with it various components based on the
PC3 model as shown in Figure 5.

B.
Development Phase
In order to validate that we were proceeding in the right
direction, we ran a series of usability tests on the application.
We chose between multiple good designs instead of focusing
on only one early on. The high-level design sketches were
discussed with the users to make sure what they said in
unstructured dialogs and vague ideas and imaginations can
now be concretely captured in design artifacts for further
validation and clarifications [17]. Our main goal was to
evaluate the simplicity and clarity of the application structure
to allow for an easy-to-recognize mental model. A mental
model can be loosely defined as the user perception of the
application. The opposite is the developers’ perception of the
application. While the latter one is the actual structure that
developers use to build the application, typically as their
interpretation of the requirements, the user mental model is
not necessarily the same. With the fact that users don’t
normally have access to the actual structure of the application,
or detailed and prolonged access to the application to know
any of it’s internal structure, they can only perceive what’s
exposed to them from the UI, and can build an “imagination:
of what the application structure might look like. In ideal
situations, this “imaginary” structure should match the actual
structure build by developers. In reality, though, it is rarely the
case. A discrepancy or vagueness on the user mental model
(we can call it a delta) is typically present and expected. The
problems arise when this delta is large, indicating an
application whose structure is not comprehensible by the user.
We have identified 2 tools that are most suitable for this
project in this phase.
C.
Tools
Navigation Model is one of the essential methods of design
that we used. A significant challenge in complex software is
not the contents of each screen, but how the user mentally
build a mental view of how all screens are connected (like a
city road map), and how to navigate between hundreds of
screens to accomplish their task. In this regard, we have
developed an effective technique, elastic prototyping, an
implementation of a participatory design to help designers and
users build a navigation model together, greatly reducing time
and effort needed. Figure 6 shows the navigation model for the
primary application. Figures 7 and 8 show the navigation
model for new user registration and user login. One of the

Figure 6: Primary IMODS Application - Navigation Model

Figure 7: New User Registration - Navigation
Model

Figure 8: User Login - Navigation Model

Figure 9: Course Overview - Navigation Model

Figure 10: Course Details - Navigation Model

Figure 11: Course Overview Mockup

Figure 12: Learning Objectives Mockup

main components of course design is describing course
overview information that includes data about course title,
description, schedule, instructors, course policies, etc. Figures
9 and 10 show the navigation model for course overview data
entry. In a similar manner navigation model for other screens
of IMODS that are used for design of Learning Objectives,
Content, Assessments, and Pedagogy were created.
Prototyping (PT) is extensively used in UCD to visualize and
validate all otherwise vague ideas and unclear expectations at
low cost and high effectiveness. We focused on three main
categories of prototyping: Paper (low-level) PT, low-fidelity
electronic (medium level) PT, and high-fidelity, detailed PT
[18]. Paper prototypes are very inexpensive and help us
capture several initial ideas and concepts, and validate them.
After explaining their needs, users often change their minds
when they see them on paper. Therefore multiple paper PT
sessions gives a head start in validating what users actually
mean and need. After initial concepts, design ideas and
directions were identified, we moved into a medium fidelity
prototyping stage where we provided a sketchy visualization
of key screens without contents and gradually validated them
and added initial contents. Figures 11 and 12 show the user
interface mockups of Course Overview and Learning
Objective components of the IMODS system
D. System Architecture
The development phase of the project included identifying
appropriate technologies to be used for the development of the
IMODS semantic web application, design of the back-end
database schema, installation and configuration of the serverside and client-side technologies, and development of the user
interface screens for login, registration, index, and creation of
an instructional module and the connectivity of these web
pages with the backend database. An Agile software
development methodology called Scrum is being used for the
development of this project. Scrum is an iterative and
incremental framework for managing product development.
The technologies chosen included Groovy on Grails, an open
source, full stack, web application framework for the Java
Virtual Machine. It takes advantage of the Groovy
programming language and convention over configuration to
provide a productive and streamlined development experience.
Grails uses Spring Model-View–Controller (MVC)
architecture as the underlying framework. MVC is a software
architecture pattern that separates the representation of
information from the user's interaction with it. PostgreSQL
was chosen as the database management system. It is a
powerful, open source object-relational database system with
more than 15 years of active development and a proven
architecture that has earned it a strong reputation for
reliability, data integrity, and correctness. Git was chosen for
source code version control. It is a distributed revision control
and source code management (SCM) system with an emphasis
on speed.

E. Testing
For the testing phase of the project, we opted to not have a
complete discovery test, where the user would be asked to do
a blind discovery of the application without any prior
knowledge [19]. That would be a simulation of a completely
novice user without any application background. Instead, we
decided to test for an average user with some level of
knowledge about the application structure. The reason is that
we already have a concrete navigation model, and we can
typically bring it to any user’s attention in few minutes to help
them in building a correct navigation model. That is one of the
main advantages of using the navigation modeling method.
Currently our research team is working on software
development and testing phases of the project.
IV.

CASE STUDY

The IMODS framework was applied to design an introductory
software engineering course titled “Software Enterprise I:
Personal Software Process” in B.S. in Software Engineering
program. This section describes the use of IMODS – PC3
model for course design.
A. About the Course
Software Enterprise I: Personal Software Process is a
sophomore course in the Software Engineering program that
introduces software engineering and object-oriented software
design principles using a modern programming language.
Students are introduced to Software Engineering, Software
Life Cycle models, Object-Oriented Programming, Personal
Software process, Effort estimation, effort tracking, defect
estimation and defect tracking. Students learn personal
software process for individual professionalism, time and
defect estimation; yield and productivity. A project-based
pedagogical model is used for delivery of all our courses in
Software Engineering program. Students in this course worked
on a game project using Java programming language.
B. Learning Objectives
Learning objectives of this course were defined using the PC3
model. The course has 6 objectives that are categorized under
Performance, Content, Condition, and Criteria as shown in the
Table 1. The objectives are as follows:
•

•

•

LO1: Design a software solution using Object-Oriented
Design principles of encapsulation, information hiding,
abstraction, inheritance, and polymorphism
LO2: Develop a software solution in an object-oriented
programming language employing standard naming
conventions and making appropriate use of advanced
features such as exception handling, I/O operations, and
simple GUI
LO3: Use object-oriented design tools such as UML class
diagrams to model problem solutions and express classes
and relationships such as inheritance, association,
aggregation, and composition

•

•

•

LO4: Use personal software process for individual
development productivity through time estimation and
tracking
LO5: Use personal software process for individual
development quality through defect estimation and
tracking
LO6: Demonstrate teamwork
Table 1: Learning Objectives based on PC3 Model

category, knowledge dimension, and criteria type that each
method is suitable for.
E. Instructional Activities
Pedagogical activities used in this course are listed in Table 4
along with the knowledge dimension and learning domain
category that they are suitable for. The list of activities
includes a mix of lectures, lab activities, Q&A discussions,
and problem solving activities.
Table 3: Course Assessments

** DPA Determined Per Assessment

C. Content
The list of Content topics and subtopics are listed in Table 2.
For each topic the knowledge dimension and topic priority is
defined. This information is used to find assessments and
instructional activities that best fit for delivering a topic.

F. Results
Software Enterprise I: Personal Software Process course in the
Software Engineering program in School of Computing,
Informatics, Decision Systems Engineering (CIDSE) at
Arizona State University was designed using the IMODS –
PC3 model and offered as a face-to-face section (with 82
students) as well as an online section (with 87 students) by the
same instructor (one of the co-authors). Using the IMODS
framework ensured the alignment between various course
elements and thereby ensuring high-quality course design.
Table 4: Course Pedagogical activities

Table 2: Content Topics based on PC3 Model

D. Assessments
Assessments chosen for this course include a mix of both
formative and summative assessments. The PC3 model aligns
assessments chosen for the course with the learning objectives
by checking compatibility of learning domains, performance,
and criteria requirements. Table 3 provides the list of
assessments with their corresponding learning domain

Alignment between various course components:
The framework supports the checking of alignment between
course assessments and learning objectives. The course
assessments are linked to the performance and criteria
elements of the learning objective as shown in Figure 1. The
framework supports the checking of alignment between course
instructional activities and learning objectives. The course
pedagogical activities are linked to the performance and
content of the learning objective as shown in Figure 1.
Topic Prioritization:
Use of the PC3 model ensured a balanced distribution of the
topics under Critical, Important, and Good to know as shown
in figure below.

V.

FUTURE WORK

Following the high-level design phase of the project, the next
step will be software development and testing of IMODS. We
will conduct usability testing of the prototype with instructors
and solicit feedback using surveys, observation, and user
interviews. The feedback will be incorporated into the iterative
software development lifecycle model. The scope of this
project will also include the evaluation of its novel approach to
self-guided web-based professional training in terms of: 1) user
satisfaction with the documentation of course designs
generated; and 2) impact on users’ knowledge of the outcomebased course design process.
ACKNOWLEDGMENT
The authors gratefully acknowledge the support for this project
under the National Science Foundation's Transforming
Undergraduate Education in Science, Technology, Engineering
and Mathematics (TUES) program Award No. DUE-1246139.
REFERENCES
[1] R. M. Felder, R. Brent, and M. J. Prince, “Engineering
Instructional Development: Programs, Best Practices, and
Recommendations.,” Journal of Engineering Education,
vol. 100, no. 1, pp. 89–122, Jan. 2011.
[2] R. Boice, Advice for new faculty members. Allyn &
Bacon, 2000.
[3] R. M. Harden, J. R. Crosby, M. H. Davis, and M.
Friedman, “AMEE Guide No. 14: Outcome-based
Education: Part 5--From Competency to MetaCompetency: A Model for the Specification of Learning
Outcomes.,” Medical Teacher, vol. 21, no. 6, pp. 546–
552, 1999.
[4] W. G. Spady and K. J. Marshall, “Beyond Traditional
Outcome-Based Education.,” Educational Leadership,
vol. 49, no. 2, pp. 67–72, 1991.
[5] G. Adorni, S. Battigelli, D. Brondo, N. Capuano, M.
Coccoli, S. Miranda, F. Orciuoli, L. Stanganelli, A. M.
Sugliano, and G. Vivanet, “CADDIE and IWT: two
different ontology-based approaches to Anytime,
Anywhere and Anybody Learning,” Journal of eLearning and Knowledge Society-English Version, vol. 6,
no. 2, 2010.
[6] S. Ternier, E. Duval, and P. Vandepitte, “LOMster: peerto-peer learning object metadata,” in Proceedings of
EdMedia, 2002, pp. 1942–1943.

[7] “LOCO-Analyst.” [Online]. Available:
http://jelenajovanovic.net/LOCO-Analyst/index.html.
[Accessed: 28-May-2012].
[8] G. P. Wiggins and J. McTighe, Understanding by design.
Association for Supervision & Curriculum Development,
2005.
[9] J. Warren, “Changing community and technical college
curricula to a learning outcomes approach,” Community
College Journal of Research &Practice, vol. 27, no. 8,
pp. 721–730, 2003.
[10] R. A. Streveler, K. A. Smith, and M. Pilotte, “Aligning
Course Content, Assessment, and Delivery: Creating a
Context for Outcome-Based Education,” K. Mohd Yusof,
S. Mohammad, N. Ahmad Azli, M. Noor Hassan, A.
Kosnin and S. K, Syed Yusof (Eds.), Outcome-Based
Education and Engineering Curriculum: Evaluation,
Assessment and Accreditation. Hershey, Pennsylvania:
IGI Global, 2012.
[11] R. F. Mager, “Preparing Instructional Objectives: A
critical tool in the development of effective instruction
3rd edition,” The Center for Effective Performance, Inc,
1997.
[12] K. Andhare, O. Dalrymple, and S. Bansal, “Learning
Objectives Feature for Instructional Module Development
System,” presented at the PSW American Society for
Engineering Education Conference, San Luis Obispo,
California, 2012.
[13] A. Gaffar, “Enumerating mobile enterprise complexity 21
complexity factors to enhance the design process,” in
Proceedings of the 2009 Conference of the Center for
Advanced Studies on Collaborative Research, 2009, pp.
270–282.
[14] A. Gaffar, N. Moha, and A. Seffah, “User-Centered
Design Practices Management and Communication,” in
Proceedings of HCII, 2005.
[15] S. Bansal, O. Dalrymple, A. Gaffar, and R. Taylor, “User
Research for the Instructional Module Development
(IMOD) System,” in American Society for Engineering
Education Annual Conference (ASEE), Indianapolis, IN,
2014.
[16] O. Dalrymple, S. Bansal, A. Gaffar, and R. Taylor,
“Instructional Module Development (IMOD) System: A
User Study on Curriculum Design Process,” in Frontiers
in Education (FIE), Madrid, Spain, 2014.
[17] J. Lazar, J. H. Feng, and H. Hochheiser, Research
methods in human-computer interaction. John Wiley &
Sons Inc, 2009.
[18] W. Lidwell, K. Holden, and J. Butler, Universal
principles of design: 125 ways to enhance usability,
influence perception, increase appeal, make better design
decisions, and teach through design. Rockport Pub, 2010.
[19] N. Moha, A. Gaffar, and G. Michel, “Remote usability
evaluation of web interfaces,” Human Computer
Interaction Research in Web Design and Evaluation. P.
Zaphiris and S. Kurniawan. Hershey, PA, Idea Group
Publishing, pp. 273–289, 2007.

2009 IEEE Conference on Commerce and Enterprise Computing

WSC-2009: A Quality of Service-Oriented Web Services Challenge
Srividya Kona

Ajay Bansal

M. Brian Blake

Georgetown University
Washington, DC 20057

Georgetown University
Washington, DC 20057

Georgetown University
Washington, DC 20057

kona@cs.georgetown.edu

bansal@cs.georgetown.edu

blakeb@cs.georgetown.edu

Steffen Bleul

Thomas Weise

Universityof Kassel
D-34121,Kassel Germany

Universityof Kassel
D-34121,Kassel Germany

bleul@vs.uni-kassel.de

weise@vs.uni-kassel.de

automated composition software and facilitates the
dissemination of results that advance this field.
The WSC-09 represents the fifth event of the
matchmaking and composition challenge series. It
extends the original criteria of the first two
competitions which focused on service discovery and
service composition based on the syntactic matching of
WSDL part names. It also further extends the third and
fourth competition in 2006 and 2007 which provided
taxonomy of parameter types represented using the
natural hierarchies that are captured using simple and
complex types within XML documents. WSC-08
further evolved with the adoption of ontologies written
in OWL to provide semantics.
The 2009 competition is a continuation of the
evolution of the challenge considering non-functional
properties of a web service. The Quality of Service of a
web service is expressed by its response time and
throughput. It has been the tradition of the WSC events
to adhere to technology-independent approaches to
semantics. In this way, the competition attempts to
circumvent debates on representations, such as
differences between approaches like OWL-S [8],
WSML [7], and WSDL-S [12]. In 2006, the use of
pure XML-based semantics allowed for a bi-partisan
approach. In 2007, we have evolved the challenge by
mirroring the XML-based semantics with equivalent
representations using OWL ontology. During the
previous three years, web service challenge focused on
optimizing the discovery and composition process
solely using abstractions from real-world situations.
The taxonomies of semantic concepts as well as the
involved data formats were purely artificial. In 2008,
we have further evolved the challenge by using data
formats and contest data based on OWL, WSDL, and
WSBPEL schemas for ontologies, services, and service
orchestrations.

Abstract
With the growing acceptance of service-oriented
computing, an emerging area of research is the
investigation of technologies that will enable the
discovery and composition of web services. The Web
Services Challenge (WSC) is a forum where academic
and industry researchers can share experiences of
developing tools that automate the integration of web
services. In the fifth year (i.e. WSC-09) of the Web
Services Challenge, software platforms will address
several new composition challenges. Requests and
results will be transmitted within SOAP messages.
Semantics will be represented as ontologies written in
OWL, services will be represented in WSDL, and
service orchestrations will be represented in WSBPEL.
In addition, non-functional properties (Quality of
Service) of a service will be represented using WSLA
format.

1. Introduction
The annual Web Services Challenge has provided a
platform for researchers in the area of Web Service
Composition since 2005. Succeeding the EEE-05
[2][11], WSC-06, WSC-07, and WSC-08 Challenges
[3][4][5], the 2009 Web Services Challenge (WSC-09)
[6] is the fifth year of this SOA venue [1] that looks to
benchmark software applications that automatically
manage web services. WSC-09, held at the 11th IEEE
Conference on Commerce and Enterprise Computing
(CEC 2009) continues to provide a forum where
researchers can collaborate on approaches, methods
and algorithms in the domain of web service discovery
and automated composition. This forum provides
quantitative and qualitative evaluation results on the
performance of participating matchmaking and

978-0-7695-3755-9/09 $25.00 © 2009 IEEE
DOI 10.1109/CEC.2009.80

487

file during the bootstrapping process. This file contains
the taxonomy of concepts used in this challenge in
OWL format. The bootstrapping process includes
loading the relevant information from these files. The
challenge task will then be sent to the composer via a
client-side GUI very similar to last year’s challenge.
After the bootstrapping on the server side is finished,
the GUI queries the composition system with the
challenge problem definition. The contestant’s
software must now compute a solution – one or more
service compositions – and answer in the solution
format which is a subset of the WSBPEL schema.
When the WSBPEL document is received by the GUI,
we will stop a time measurement and afterwards
evaluate the compositions themselves.

In the WSC-08, concurrency was introduced into
the composition problem sets. The combination of
complex, workflow-based heuristics with semanticallyrich representations required participants to create new
software that is both robust and efficient. The problem
sets in WSC-09 also are inherently concurrent.

2. Related Venues
Although WSC is perhaps the first venue, other
unique venues have been established to investigate the
need for solutions to the Service Composition Problem.
The Semantic Web Services Challenge [9] is less of a
competition and more of a challenge. Both business
cases and solution applications are the focus of the
venue. Participants are placed in a forum where they
can incrementally and collaboratively learn from each
other. While WSC venues are more driven by
application, the SWS venues concentrate more on the
environment. As such, the SWS venues place more
focus on semantics where the WSC favors applied,
short-term solutions.
Alternatively, the SOA Contest [10] held at the
International Conference on Services Computing
(SCC2006, SCC2007, and SCC2008) allows
participants to openly choose the problems that best
demonstrate their approach. The benefit of this venue
is that participants can show the best approach for a
particular domain-specific problem. In contrast, the
WSC venue attempts to set a common problem where
approaches can be evaluated side-by-side.
There is a unique niche for each forum and the
composition of the results from all the venues will
undoubtedly advance the state-of-the-art in serviceoriented computing.

Bootstrap Phase

Challenge Server Side
WSDL
File

OWL
File

Challenge Client Side

WSLA
Files

Evaluation

WSDL of
Required
Service
Parse WSDL
Service Descriptions

Parse OWL
Ontology

Parse WSLA
QoS Description

Compute Service
Composition

WSDL of
Required
Service

Generate
WSBPEL

WSBPEL
File

Interface
Package

WSBPEL
File

Figure 1: The procedure of the Web Service
Challenge 2009.

3. The Challenge

3.1. What’s New





In the competition, we adopt the idea of so-called
Semantic Web Services that represent Web Services
with a semantic description of the interface and its
characteristics. The task is to find a composition of
services that produces a set of queried output
parameters from a set of given input parameters. The
overall challenge procedure is shown in Figure 1. The
composer software of the contestants is placed on the
server side and started with a bootstrap procedure. First
of all, the system is provided with a path to a WSDL
file. The WSDL file contains a set of service
descriptions along with annotations of input- and
output parameters. The number of services will change
from challenge to challenge. Every service will have an
arbitrary number of parameters. Additional to the
WSDL file, we also provide the address of the OWL

•
•

Document Formats: WSLA format
Quality of Service (QoS): Each service
will be annotated with its non-functional
properties on response time and
throughput. The contestants do not have to
find the shortest or minimal composition
considering the amount of services. The
contestants should, instead, find the
composition with the least response time
and the highest possible throughput.

3.2. Semantics
Ontologies are usually expressed with OWL’s XML
format [13][14]. We use OWL format in the 2009
challenge, but like in the previous years, we limit

488

milliseconds and the throughput (invocations per
minute) of each service in a challenge set. Metrics can
be omitted as they do not contain relevant information
for the service composition. They are interesting
nonetheless as they present the capabilities of WSLA.

semantic evaluation strictly to taxonomies consisting
of sub and super class relationship between semantic
concepts only. OWL is quite powerful. In addition to
semantic concepts (OWL-Classes), OWL allows to
specify instances of classes called individuals. While
we also distinguish between individuals and classes in
the competition, the possibility to express equivalence
relations between concepts is not used. In OWL, the
semantics are defined with statements consisting of
subject, predicate, and object, e.g. ISBN-10 is_a
ISBN (ISBN subsumes ISBN-10). Such statements
can be specified with simple triplets but also with
XML-Hierarchies
and
XML-References.
The
implementation of an OWL-Parser is hence not trivial.
In order to ease the development of the competition
contributions, we will stick to a fixed but valid OWLSchema.

3.4. Evaluation
The Web Service Challenge awards the most
efficient system and also the best architectural solution.
The best architectural effort will be awarded according
to the contestant’s presentation and system features.
The evaluation of efficiency consists of two parts as
shown in Figure 2.
The BPEL checking software evaluates the results
of the participant’s composition system. The BPEL file
is examined for a solution path and its correctness with
respect to the challenge task.
1. Three challenge sets are provided and each
composition system can achieve up to 18 points.
2. The time limit for solving a challenge is five
minutes. Every composition system replying with a
solution later than five minutes will receive 0 points for
the challenge set.
3. The task is to find the service composition
solution with the lowest response time. Additionally
the composition system that finds the service
composition with the highest throughput in the fastest
time will be rewarded.

3.3. Quality of Service
The Quality of Service for a service can be
specified using the Web Service Level Agreements
(WSLA) [15], language from IBM. In contrast to the
Web Service Agreements (WS-A) language, WSLA is
in its final version. Furthermore, WSLA offers more
specific information than WS-A. We can not only
specify the Service Level Objectives (SLO) of a
service and its service operations, but also the
measurement directives and measurement endpoints
for each quality dimension. WSLA represents a
configuration for a SLA management and monitoring
system. In contrast to WS-A, WSLA enables the
automated discovery and deployment of service
contracts inside SOAs. In the WSC-09, we define the
following quality dimensions for a Web Service. They
can be accessed in this document format and must be
calculated for a whole BPEL process.
• Response Time: In a data system, the
system response time is the interval between the
receipt of the end of transmission of an inquiry
message and the beginning of the transmission of a
response message to the station where the inquiry
originated.1 Example: 200 milliseconds.
• Throughput: In communication networks
such as Ethernet or packet radio, throughput is the
average rate of successful message delivery over a
communication channel.2 Example: 10.000 (successful)
invocations per minute.
We define one WSLA document containing
the specification of the average response time in

WSDL of
available
Services

Compute Service
Composition
Interface
Package

Time
Measurement

WSBPEL
file

Composition
Evaluation

Generate
WSBPEL

Challenge Score

Figure 2: Evaluation in the WS-Challenge 2009.

4. Logistics of the Web Services Challenge
WSC-09 has attracted international teams which
come from universities and research organizations in
countries including the Slovak Republic, China,
Vienna, Netherlands, South Korea, and the United
States. The organization of this event is divided into
two phases: The first phase focuses on evaluating the

1

http://en.wikipedia.org/wiki/Response_time_(technology)
[accessed on 2009-05-12]

2

http://en.wikipedia.org/wiki/Throughput [accessed on 200905-12]

489

technical viability of the methodologies proposed by
the participating teams. A four-page technical
description submitted from each team is peer-reviewed
and included in the proceedings of the conference CEC
2009. Once the reviewing and acceptance notification
phases have completed, teams that successfully
complete this first step are asked to submit a version of
their software for evaluation. Preliminary tests are
conducted using this evaluation version to ensure the
compatibility and applicability during the final
competition. The main objective is to avoid, in
advance, potential format related problems that may
otherwise occur when the competition takes place.
The second phase is the final competition which is
scheduled for two days at CEC-09. On the first day,
all the participating teams will present their approaches
in a specialized session of the conference. On the same
day, the participants are allotted times to install the
latest version of their software on the evaluations
stations located onsite at the conference. On the second
day, the teams must execute their software using a
customized data set prepared specifically for the
competition. Participating software is measured for
performance during any indexing phases and during
the actual composition routine. Composition results
are evaluated against known solutions for correctness
and completeness. In 2009 there will be multiple sets
of correct answers with variable length chains.
Applications will be judged with weighted scores
based on the best solutions that they present.
The solution application with the best qualitative
and quantitative scores when run against several
datasets is awarded first place. The competition
typically has a winner and several runner-ups.

[2]

[3]

[4]
[5]
[6]
[7]
[8]

[9]
[10]
[11]
[12]
[13]

5. Acknowledgements

[14]

The authors would like to acknowledge the efforts of
Georgetown student Brian Miller who facilitated the
web site development for the challenge. Georgetown
graduate student, John Adams, organized the travel
logistics of the web service challenge. The authors also
acknowledge Hong Kong Baptist graduate student,
Kai-Kin Chan, for preparing the OWL representations.
The Web Service Challenge has been extensively
funded by the National Science Foundation under
award number 0548514 and 0723990. The Hewlett
Packard Corporation and Springer-Verlag have also
supported an award to the winners of the competition.

[15]

References
[1] Blake, M.B., Cheung, W., and Wombacher, A. “Web
Services Discovery and Composition Systems”

490

International Journal of Web Services Research, Vol. 4,
No. 1, pp iii – viii, January 2007
Blake, M.B., Tsui, K.C., Cheung, W., “The EEE-05
Challenge: A New Web Service Discovery and
Composition Competition” Proc. of the IEEE Intl.
Conference on E-Technology, E-Commerce, and EServices (EEE-05), Hong Kong, March 2005.
Blake, M.B., Cheung, W., Jaeger, M.C., and
Wombacher, A., “WSC-06: The Web Service
Challenge”, Joint Proceedings of the CEC/EEE 2006,
San Francisco, California, USA, June 2006.
The
Web
Services
Challenge
(2007).
http://www.wschallenge.org/wsc07/
The
Web
Services
Challenge
(2008).
http://cec2008.cs.georgetown.edu/wsc08/
The Web Services Challenge (2009). http://www.wschallenge.org/wsc09
Fensel, D. and Bussler, C. “The Web Service Modeling
Framework”, Electronic Commerce: Research and
Applications, 1(2): 113-137, 2002
Martin, D. et al. “Bringing Semantics to Web Services:
The OWL-S Approach”, Proc. of the First Intl.
Workshop on Semantic Web Services and Web Process
Composition (SWSWPC-04), San Diego, USA, July ‘04.
The Semantic Web Services Challenge (2007):
http://sws-challenge.org/wiki/index.php/Main_Page
The
Services
Computing
Contest
(2007):
http://iscc.servicescomputing.org/2007/
The Web Services Challenge at the IEEE Conference on
e-Business
Engineering
(ICEBE-05)
(2007):
http://www.comp.hkbu.edu.hk/simctr/wschallenge/
WSDL-S
(2007):
http://www.w3.org/Submission/WSDL-S/
Bechhofer, S., Harmelen, F., Hendler, J., Horrocks, I.,
McGuinness, D., Patel-Schneider, P., and Stein L.
OWL Web Ontology Language Reference. World Wide
Web Consortium (W3C), February 10, 2004. Online
available at http://www.w3.org/TR/2004/REC-owl-ref20040210/
Bray, T., Paoli, J., Sperberg-McQueen, C., Maler, E.,
and Yergeau, F. Extensible Markup Language (XML)
1.0 (Fourth Edition). World Wide Web Consortium
(W3C), September 29, 2007. Online available at
http://www.w3.org/TR/2006/REC-xml-20060816
Keller, A., Ludwig, H. The WSLA Framework:
Specifying and monitoring service level agreements for
Web services. Journal of Network System Management,
11(1), 2003.

Proceedings of the 20 IS IEEE 9th International Conference on Semantic Computing (IEEE ICSC 20 IS)

MOOCLink: Building and Utilizing Linked Data
from Massive Open Online Courses
Sebastian Kagemann

Srividya Bansal

Indiana University
School of Informatics and Computing
Bloomington, IN 47405, USA
sakagema@indana.edu

Arizona State University
School of Computing, Informatics, and Decision Systems
Engineering, Mesa AZ 85212, USA
srividya.bansal@asu.edu

and syllabus details. Although Coursera's course catalog data
is easily accessible as JSON, there is no option to retrieve and
use it in a Linked Data format such as the Resource
Description Framework (RDF). Moreover, there is little to no
Linked Data available for MOOCs or an ontology that denotes
properties unique to MOOCs.

Abstract-Linked Data is an emerging trend on the web with
top companies promoting their own means of marking up data
semantically, publishing and connecting data on Web. Despite
the increasing prevalence of Linked Data, there are a limited
number of applications that implement and take advantage of its
capabilities, particularly in the domain of education. In this
project we are using Semantic technologies to create a semantic

In order to incorporate MOOC data into the Linked Data
cloud as well as demonstrate the potential of Linked Data when
applied to education, we propose to (i) build or extend an RDF
ontology that denotes MOOC properties and relationships (ii)
use our ontology to generate Linked Data from multiple
MOOC providers and (iii) implement this data in a practical
web application that allows users to discover courses across
different MOOC providers.

data model for educational data, more specifically data about
Massive Open Online Courses (MOOCs) and publishing this data
as linked data on the Web. Data from various MOOC providers
is integrated and published as Linked Data.

We present a web

portal called MOOCLink that utilizes the data to discover and
compare open courseware.

Keywords-semantic

application;

linked

data;

education;

ontology engineering

I.

II.

INTRODUCTION

A. Resource Description Framework

Linked Data involves using the Web to create typed links
between data from different sources [1]. Source data may vary
in location, size, subject-matter and how congruously it is
structured. Typed links produced from this data create
uniformity and define properties explicitly in an effort to make
data easier to read for machines. This is opening up
opportunities for applications that were previously impractical
such as Berners-Lee's intelligent agents [2].

The most notable model for Linked Data is the Resource
Description Framework (RDF), which encodes data as subject,
predicate, object triples [7]. The subject and object of a triple
are both Uniform Resource Identifiers (URIs), while the
predicate specifies how the subject and object are related, also
using a URI. For the purposes of this paper, Linked Data is
presented as RDF/XML, an XML syntax for RDF, which IS
also used in the implementation of our application.

Linked Data is relatively unexplored in the domain of
education. Although there are several data models for
structuring educational data as well as repositories adopting
these models [3], Linked Data-driven educational applications
are far and few between. As a result, initiatives such as the
LinkedUp Challenge have surfaced to encourage innovative
applications focused on open educational data [4].

B. Simple Protocol and RDF Query Language

Simple Protocol and RDF Query Language or SPARQL is
an RDF query language that allows users to retrieve and
manipulate data stored as RDF [8]. SPARQL is used as our
application's query language in order to retrieve data to
populate web pages with course information. A SPARQL
endpoint
for
our
data
can
be
accessed
at
http://sebk.me:3030/sparq J.tp I.

Massive Open Online Courses or MOOCs are online
courses accessible to anyone on the web. Hundreds of
institutions have joined in an effort to make education more
accessible by teaming up with MOOC providers such as
Coursera and edX [5]. Delivering course content through
lecture videos as well as traditional materials such as readings
and problem sets, MOOCs encourage interactivity between
professors and students around the world by way of discussion
forums and graded assessments.

C. Linked Data Principles

Tim Berners-Lee published a set of rules for publishing
data on the Web so that data becomes part of a global space in
which every resource is connected. The rules are as follows:

Coursera, a leading MOOC provider, offers a RESTful API
[6] for most information associated with their course catalog.
This includes properties such as a courses's title, instructor,

IEEE ICSC 2015, February 7-9, 2015, Anaheim, California, USA
978-1-4799-7935-6/15/$31.00 ©20I 5 IEEE

BACKGROUND

373

1.

Use URIs as names for things

2.

Use HTTP URIs so that people can look up those
names

Proceedings of the 20 IS IEEE 9th International Conference on Semantic Computing (IEEE ICSC 20 IS)
3.

When somone looks up a URI, provide useful
information (RDF, SPARQL)

4.

Include links to other URIs, so that they can discover
more things

Coursera is the largest MOOC provider in the world with
7.1 million users in 641 courses from 108 institutions as of
April 2014 [14]. These courses span 25 categories including 4
subcategories of computer science. All course details are
retrievable by HTTP GET method using Coursera's RESTful
course catalog API and returned in JSON.

These principles provide a basis for contributing to a
Linked Data cloud in which a variety of datasets from different
fields of human knowledge are interconnected. Our project
aims to abide by these principles.

edX is another premier MOOC provider with more than 2.5
million users and over 200 courses as of June 2014 [15]. edX
courses are distributed among 29 categories, many of which
overlap with Coursera's. edX does not provide an API for
accessing their course catalog, however, as of June 2013,
edX's entire platform is open-source.

D. Linked Education Data

Many models have been devised for structuring educational
data, among the most popular are the IEEE Learning Object
Metadata (LOM) specification and Sharable Content Object
Reference model (SCORM). LOM is encoded in XML and
includes nine categories with sub-elements that hold data. An
RDF binding for LOM exists [9], however development is
halted at the time of this paper's writing [10]. SCORM is an
extensive technical standard, typically encoded in XML, that
defines how educational content should be packaged, how it is
delivered, and how learners navigate between different parts of
an online course [11]. An RDF binding of SCORM has yet to
be developed. Rather than using the defunct LOM binding or
creating a new binding of SCORM to RDF, we chose to extend
a vocabulary meant for Linked Data, Schema.org, for which an
RDF mapping exists [12], to include properties unique to open
courseware.

Udacity, founded by Google VP, Sebastian Thrun, is a
vocational course-centric MOOC provider with l.6 million
users in 12 full courses and 26 free courseware as of April
2014 [16]. The majority of Udacity courses are within the field
of computer science. Udacity does not provide an API for
accessing their course catalog data.
B.

Data Model

Schema.org is organized as a hierarchy of types, each
associated with a set of properties. CreativeWork is
Schema.org's type for generic creative work including books,
movies, and now educational resources. The LRMI
specification adds properties to CreativeWork including the
time it takes to work through a learning resource
(timeRequired), the typical age range of the content's intended
audience (typicaIAgeRange), as well as specifying properties
previously available in Schema.org's CreativeWork type like
the subject of the content (about) and the publisher of the
resource (publisher) [17].

In 2013, the Learning Resource Metadata Initiative (LRMI)
specification was incorporated into Schema.org's vocabulary
for tagging educational content [13]. The properties added in
this adoption introduced fields for online course details
including the type of learning resource, time required, and so
on. While there is significant overlap between LRMI's
additions to Schema.org, Schema.org's Creative Work
properties and MOOC course details like those provided in
Coursera's API, several crucial missing data fields such as
syllabus details, course difficulty, and predicates linking
courses to other objects, make it necessary to extend the
vocabulary for MOOC data.

CreativeWork provides a base type for our ontology
extension, which adds types Course, Session, Category and
their associated properties drawn from MOOC data. The
extension is made in Stanford Protege [18], which we use to
import the Schema.org vocabulary mapped to RDF at
Schema.RDFS.org. In the GUI of Protege, types and
properties are added as well as sample individuals to be used
as a model for data generation. The final product is an
ontology in which classes are defined using OWL, the Web
Ontology Language [19], and in which data and object
properties are defined using RDFSchema, which provides
basic elements for the description of ontologies [20]. The
hierarchy of the ontology extension is outlined in the
Implementation section of this paper.

E. Building Ontologies

After determining that there was no educational resource
ontology that denoted every property needed to create linked
MOOC data, we chose to extend Schema.org's ontology. In
order to support uniformity in our data, we use RDF/XML
from ontology creation to final data generation.
Schema.rdfs.org hosts an RDF/XML version of Schema.org's
ontology, which we imported into Stanford Protege to extend
with additional types and properties.

C. Data Generation

Coursera's list of courses is accessible using their RESTful
API and JSON as the data exchange format. Using Requests, a
Python HTTP library [21], we retrieved a full list of courses,
universities, categories, instructors and course sessions in
JSON using the GET method.

III. MOOCLINK
MOOCLink is a web application which aggregates online
courses as Linked Data and utilizes that data to discover and
compare online courseware. This section of the paper outlines
our approach including choosing our providers, modeling the
data, data generation, and the development of our web
application.

edX and Udacity do not have an API therefore it became
necessary to use a scraper to obtain course data. We used
Scrapy, an open-source screen scraping and web crawling
framework for Python [22] to retrieve properties from edX
and Udacity course pages with XPath selectors. Scrapy

A. MOOe Providers
2014 CRA-W Distributed Research Experience for Undergraduates

374

Proceedings of the 20 IS IEEE 9th International Conference on Semantic Computing (IEEE ICSC 20 IS)
supports mUltiple formats for storing and serializing scraped
items; we export our data as JSON to maintain uniformity
with the data we retrieve from Coursera.

IV.

IMPLEMENTATION

This section outlines and provides relevant illustrations of
(i) our extension of Schema.org's ontology to include MOOC
classes and properties, (ii) web crawling methods, (iii) RDF
data from three different MOOC providers and (iv) our web
application.

After collecting the data, we create Linked Data from
JSON using Apache Jena, an open source Semantic Web
framework for Java [23]. First, we import the ontology model
we created in Protege to retrieve the types and properties to be
assigned. We use three methods, one for each course provider,
to read JSON specific to each provider (using Google Gson
[24]), map property names from JSON to our ontology's
properties, and write each property to the RDF graph. The
RDF is output in a flat file, serialized as RDF/XML.

A.

Ontology Schema

As mentioned in the previous section on our data model,
Schema.org is organized as a hierarchy of types. Much like
object-oriented programming, types inherit properties. Figure
1 displays relevant properties from CreativeWork and other
Schema.org types although our new types, Course, Session
and Category, inherit more properties that are not shown.
Sebastian's personal domain, sebk.me, is used as a temporary
namespace for the ontology.

As recommended by Berners-Lee in his Linked Data
Principles, each property is assigned a URI. HTTP URIs are
used so these resources can be looked up. Because the
majority of our data points to URIs within the same RDF,
hash URIs are used to identify the local resources. More
details on the naming schemes of these URIs are available in
the Implementation section of this paper, where our RDF data
for Coursera, edX, and Udacity is discussed in detail.

Thing
schema,orgIThlng

r---- +

Object Property

+ Object Property

D. Web Application

To create an application that is appealing to the eye, we
used Bootstrap [25], a responsive HTML, CSS, and JavaScript
framework to create a UI consistent on both mobile and
desktop devices. Rather than designing each component of the
website, we repurposed a business template for displaying
online courses. The template incorporates many current web
design trends including parallax scrolling and image carousels,
which give it an up-to-date look and feel.

1 : name
2: URL

Person

I

Creative Work

schema.orgIPerson

schema.orQICreatlveWork

+ Object Property
+ Object Property
+ otlject Property

I

1: amliatlon(Person, Organization)
2: email
3: teaches(Person, Course)

+ Object Property
+ Object Property

+

Object Property

1 : about
2: author
3: video

+ Object Property4: Inlanguage

5: IsBasedOnUri (syllabus)
6: timeRequired
7: keywords
+ Object Property 8: discusslonURL
+ Object Property 9: aggregateRating
+ Object Property 10: publisher

+ Object Property

+

Object Property

+ Object Property

OrganlLatlon
schema.orgJOrganlzatlon
+ Data Property

Fuseki is a SUb-project of Jena that provides an HTTP
interface to RDF data. For the web component of our project,
we upload our data into a stand-alone Fuseki server from
which we can query and update our RDF by <what>. Fuseki
includes a built-in version of TDB (Tuple Data Base), which
indexes the contents of the file as opposed to storing RDF as a
flat file [26].

1: Organization ID

I

Course
sebk.meIMOOC.owI#Course
+ Data Property 1: Difficulty
+ Data Property 2: Course 10
+ Data Property 3: Recommended Background
+ Data Property4: Course Format
+ Data Property 5: Course Syllabus
+ Data Property 6: Subtitle Languages

Google App Engine was initially explored as the web
development framework for its automatic scaling and Java
support. Unfortunately an incompatibility between App Engine
and Apache Jena was found as App Engine supports URL
Fetch to access web resources [27] while Jena's SPARQL
processor, ARQ, implements their HTTP requests using their
own HTTP operation framework [28]. Additionally, App
Engine's limits on request and response size (10 megabytes
and 32 megabytes respectively) as well as their maximum
deadline on request handlers (60 seconds) could potentially
cause problems if our queries executed slower than anticipated.

+ Object Property
+ Object Property
+ Object Property

1: isTaughtBy(Person, Course)
2: hasCategory(Course, Category)
3: hasSesslon(Course, Session)

I

I

Session
sebk.me/MOOC.owI#Sesslon

+ Data Property 1: Category ID
+ Data Property 2: Short Name

+ Data Property
+ Data Property
+ Data Property

+ Object Property

1: inciudesCourse(Category, Course)

1: Session ID
2: Session Status
3: Active

+ Data Property4: Start Date
+ Data Property
+ Data Property
+ Data Property

5: Eligible for Signature Track
6: Eligible for Certificates
7: Duration

+ otlject Property

Because of App Engine's constraints, we opted for a
combination of Apache Tomcat as our web server / servlet
container and JavaServer Pages (JSPs) and Java Servlets for
dynamic web page generation over App Engine's similar Java
solution. Each JSP uses HTML, CSS and JS from the
Bootstrap template for the UI. Jena ARQ then queries our RDF
on the Fuseki server and writes the course detaiIs onto
respective pages. Screenshots of the application demo can be
found in the Implementation section of this paper.

I

Category
sebk.meIMOOC.owI#Category

1: belongsToCourse(Session, Course)

Fig. I. Diagram of Schema.org ontology extension.

B.

Web Crawling

We retrieve Coursera's course properties via their course
catalog API but use screen scrapers for edX and Udacity. This
section details the process of writing a Scrapy crawler for edX
in Python. After starting a new Scrapy project in the terminal,

375

Proceedings of the 20 IS IEEE 9th International Conference on Semantic Computing (IEEE ICSC 20 IS)
for site in sites:
item = EdxItem()
item['url'] = site
item['category'] =response.request.ur 1. split(" /") [5]
request=scrapy . Request( site, callback=self. parse_details)
request.meta[' item'] = item
requests.append(request)
return requests

we define the Item or container that will be loaded with
scraped data. This is done by creating a scrapy.Item class and
defining its attributes as scrapy.Field objects as shown in
Figure 2. These attributes, which are ultimately output as
JSON, are named slightly different than our RDF properties.
As a consequence, the naming scheme is mapped to the types
defined in our ontology later in our Jena methods, which
convert the collected JSON into RDF/XML.

Fig. 4. Parsing start_uris

The next method, parse_details, shown in Figure 5, collects
attributes from individual course pages using XPath selectors.
The majority of these selectors are tailored to the HTML
classes provided by edX, which denote course properties
provided on the course page.

class Edxltem(Item):
name
FieldO
about
Field 0
instructor
Field()
school
Field()
courseCode
Field()
startDate
Field()
ur 1
Field 0
length
Field()
effort
Field()
prereqs
Field()
video
Field()
category
Field()
=

=

=

def parse_details(self, response):
filename = response.url.split("/")[-2]
open(filename, 'wb').write(response.body)
sel = Selector(response)
item = response.meta['item']
item['name'] = sel.xpath('//h2/span/text()').extract()
item['about']=
sel.xpath('//*[@itemprop="description"]').extract()
item['instructor']=sel.xpath('//*[@class="staff­
title"]/text()').extract()
item['school']=sel.xpath('//*[@class="course-detail-school
item"]/a/text()').extract()
item['courseCode']=sel.xpath('//*[@class="course-detai1number item"]/text()').extract()
item['startDate']=sel.xpath('//*[@class="course-detail­
start item"]/text()').extract()
item['length']=sel.xpath('//*[@class="course-detail-length
item"]/text()').extract()
item['effort']=sel.xpath( , //*[@class="course-detail-effort
item"]/text()').extract()
item['prereqs']=sel.xpath('//*[@class="course-section
course-detail-prerequisites-full"]/p/text()').extract()
item['video']=
sel.xpath( , /html/head/meta[@property="og:video"]/@content') . extrac
t()
return item

=

=

=

=

=
=

=

=

=

Fig. 2. Defining attributes for the EdXltem container

In
Figure
3
we
subclass
CrawlSpider
(scrapy.contrib.spiders.CrawISpider), and define its name,
allowed domains, as well as a list of URLs for the crawler to
visit first. The subsequent lines of code open a list of edX
categories, iterate through those categories and append URLs
to start_uris. These appended URLs lead to paginated lists of
courses in each category. This method was chosen over using
edX's "All Courses" listing as a start URL in order to gather
category names from each edX course. Category names are
currently not explicitly defined on each edX course page.

Fig. 5. Parsing Indlviudal course pages

class EdXSpider(CrawISpider):
name = "edx"
allowed_domains = ["edx.org")
start_urIs = [)
file = open('data/category_map', 'r')
for line in file:
url="https://www.edx.org/course-list/allschools/"
+ line.strip() + "/allcourses"
start urls.append(url)

Navigating to the Scrapy project folder in the terminal and
issuing the command "scrapy crawl edx -0 items.json" yields a
JSON file containing all items scraped from edX. This works
for initializing our data although an item pipeline component
will need to be developed in the future to validate our scraped
data and check for duplicates.
C. RDF Data

Fig. 3. Defining the spider and start_uris

The first section of our RDF (Fig. 6) invokes the
namespaces associated with (line 2) our ontology, (line 3)
RDF, (line 4) OWL, (line 5) XMLSchema, (line 6)
Schema.org and (line 7) RDFSchema. These namespaces are
used as prefixes to abbreviate URIs throughout the data.

Figure 4 shows our first parse method which takes a URL
from our list of start URLs defined in Figure 3, selects each
course URL listed on the page using the XPath selector
"Ilstrong/a/@href', then iterates through each site. For each
site, a new EdxItem is declared, "uri" and "category" are
assigned, and a new scrapy.Request is made for the URL
selected, calling the parse_details method detailed in Figure 5.
The complete EdxItem is then assigned to the Request.meta
attribute, and the request is appended to a list of every request
made during the crawl.

<rdf:RDF xmlns=''http://sebk.me/MOOC.owl#''
xml:base=''http://sebk.me/MOOC.owl''
xmlns:rdfs=''http://www.w3.org/200010 lIrdf-schema#"
xmlns:owI=''http://www.w3.org/2002/07low 1#"
xmlns:xsd=''http://www.w3.org/200 l/XMLSchema#"
xmlns:schema=''http://schema.org''
xmlns:rdf=''http://www.w3.orgI1999102/22-rdf-syntaxns#">

def parse_sites(self, response):
filename = response.url.split("l")[ -2]
open(filename, 'wb').write(response.body)
sel = Selector(response)
sites = sel.xpath( , //strong/a/@href' ) . extract()
sites.pop(0) # remove home directory link
requests = []

Fig. 6

376

RDF/XML namespace declarations

Proceedings of the 20 IS IEEE 9th International Conference on Semantic Computing (IEEE ICSC 20 IS)
<hasCategory>http://sebk.meIMOOC.owl#category_308<l1lasCategory>
<Course ID>OEEI 0I x</Course ID>
<Recommended_Background>None.</Recommended_Background>
<schema:about>&lt;span itemprop�"description"&gt;&lt;h4&gt;
*Note - This is an Archived course*&lt;1114&gt;
&It;p&gt;&lt;span&gt;This is a past/archived course. At this time, you can only
explore this course in a self-paced fashion. Certain features of tllis course may not be
active...</schema:about>
<rdfrype rdfresource�''http://sebk.meIMOOC.owl#Course''/>
</rdfDescription>

In Figure 7 line 1, Coursera subject URIs (rdf:about)
are set to "coursera_course_" concatenated with the course ID.
They are preceded by the ontology's namespace explicitly.
Line 5 sets the type of resource to Course, a class provided by
our MOOC ontology. Data and object properties mapped from
Coursera
data
to
the
ontology
such
as
Recommended_Background are assigned values one by one.
These properties using a "schema" prefix where the property
is inherited from Schema.org and no prefix where the property
is borrowed from our ontology. Lines 4 6, and 8 link the
course to Coursera sessions, categories and instructors
respectively. These linked classes follow a similar subject URI
naming scheme to Coursera courses. They are also instantiated
in RDF/XML in the same way, but with their own properties.

Fig. 8. Sample of edX course data in RDFIXML

Udacity follows the same subject URI scheme as
Coursera and edX but with the course ID provided by Udacity.
Similar to edX, instructors and sessions do not have their own
IDs. We once again use the course IDs in place of instructor
and session IDs for their subject URIs. We do not assign these
course IDs to Session_ID data properties however; this is to
avoid any conflicts in future SPARQL queries.

<rdfDescription rdfabout=''http://sebk.meIMOOC.owl#coursera course 83">
<Course_Fonnat>The class consists of lecture videos, 8 - 15 mi.1Utes in length. These
contain 2-3 integrated quiz questions per video. There are standalone quizzes each
week. Total lecture time is - 14 hours.&lt;br&gt;</Course-Fonnat>
<Course ID>83</Course rD>
<hasSesSion>http://sebk.l;; eIMOOC.owl#coursera session 410</hasSession>
<rdftype rdfresource�''http://sebk.me/MOOC.owT#Cours';;'/>
<hasCategory>http://sebk.melMOOC.owl#category_I O</hasCategory>
<schema:name>Drugs and the Brain</schema:name>
<isTaughtBy>http://sebk.me/MOOC.owl#courserajnstructor_640696</isTaughtBy>
<schema:timeRequired>4-6 hoursiweek</schema:timeRequired>
<schema:inLanguage>en</schema:inLanguage>
<hasSession>http://sebk.me/MOOC.owl#coursera session 971466</hasSession>
<Recommended_Background>Neuroscience, the I;;ost inte�disciplinary science of the
21st century, receives inputs from many other fields of science, medicine, clinical
practice, and technology. Previous exposure to one or more Of tlle subjects listed in
"Suggested Readings" will provide a good vantage point, as we introduce material
from tlleSe subjects.</Recommended_Background>
<hasCategory>http://sebk.meIMOOC.owl#category_3</hasCategory>
<schema:about>What happens in the body when a person smokes a cigarette? After
several weeks of smoking? When a person takes antidepressant or antipsychotic
medication? A drug for pain, migraine, or epilepsy? A recreational drug? Neuroscientists
are beginning to understand tlleSe processes. You'll learn how drugs enter the brain, how
tlley act on receptors and ion channels, and how "molecular relay races" lead to changes
m nerve cells and neural circuits that far outlast the drugs themselves. "Drugs and the
Brain" also describes how scientists are gathering the knowledge required for the next
steps in preventing or alleviating Parkinson's, Alzheimer's, schizophrenia, and drug
abuse.</schema:about>
</rdfDescription>

<rdfDescription rdfabout=''http://sebk.meIMOOC.owl#udacity course cs259">
<Recommended _Background>&lt;p&gt;Basic knowledge of program�ning and Python
at the level of Udacity CSI 0I or better is required. Basic understanding of Object­
oriented programming is helpful.&lt;lp&gt;</Recommended_Background>
<hasCategory>http://sebk.melMOOC.owl#category_12</hasCategory>
<hasSession>http://sebk.me/MOOC.owl#udacity session-cs259<l1lasSession>
<schema:name>Software_Debugging</schema:nmne>
<Course rD>cs259</Course ID>
<rdftype rdfresource�"http:7/sebk.me/MOOC.owl#Course"/>
<schema:inLanguage>English</schema:inLanguage>
<schema:timeRequired>Assumes 6hr/wk</schema:timeRequired>
<schema:about>&lt;div&gt;&#xD;
&It;p&gt;!n tllis class you will leam how to debug programs systematically, how to
automate the debuggmg process and build several automated debugging tools in
Python.&lt;lp&gt;&#xD;
&It;ldiv&gt;&lt;strong&gt;Why Take This Course?&lt;lstrong&gt;&lt;div&gt;&#xD;
&It;p&gt;At the end of this course you will have a solid understanding about systematic
debuggmg, WIll know how to automate debugging and will have built several functional
debugging tools in PytllOn.&lt;lp&gt;&#xD;
&It;ldiv&gt;&lt;strong&gt;Prerequisites
and
Requirements&lt;lstrong&gt;&lt;div&gt;&#xD;
&It;p&gt;Basic knowledge of programming and Python at tlle level of Udacity CS101 or
better is required. Basic understanding of Object-oriented programming is
helpful.&lt;lp&gt;&#xD;
&It;ldiv&gt;&lt;p&gt;See the &It;a href�.. https://www.udacity.com/tech-requirements..
target�"_blank"&gt;Technology
Requirements&lt;la&gt; for
using
Udacity&lt;lp&gt;</schema:about>
<schema:video>http://www.youtube.com/channeIIUCOVOktSaVdm </schema:video>
<isTaughtBy>http://sebk.meIMOOC.owl#udacity_instructor_cs259</isTaughtBy>
</rdfDescription>

Fig. 7. Sample of Coursera course data in RDFIXML

edX courses (Fig. 8) follow the same subject URI format as
Coursera's with "edx_course_" appended to edX's mixed
�h�racter and integer IDs. Note that on line 8 that the category
IS In the 300s rather than the single or double-digit category
URIs seen in the previous RDF snippet. Although most of
edX's courses are mapped to categories drawn from Coursera
JSON, some edX categories did not have an equivalent
category. As a result, additional edX categories were added to
our RDF with IDs starting at 300. Support to map relevant
courses from other providers to these new categories is
currently not implemented. An additional difference in this
data is that the session and instructor (lines 5 and 6) do not
have unique IDs. To accommodate this change, we append
course IDs to the subject URIs of the edX session and
instructor.

Fig. 9. Sample of Udacity course data in RDFIXML

D. UJ Screenshots

MOOCLink's home page, shown in Figure 10, is headed by
a navbar which contains links to "Providers", "Subjects", and
"Upcoming" pages. "Providers" drops down into a list of
Coursera, edX, and Udacity links when hovered over. Each of
these pages is a paginated list of courses from their respective
provider. "Subjects" leads to a table of links to our 36
categories. Clicking on one of these retrieves a paginated list of
courses in that category. "Upcoming" is an extension of the
starting soon table featured at the bottom of the home page,
listing courses starting within the next month. The last item on
the navbar is a search bar which retrieves courses by SPARQL
query, an example of which is shown in Figure 14.

<rdfDescription rdfabout-''http://sebk.meIMOOC.owl#edx course-OEEI 0I x">
<schema:name>Our Energetic Eartll</schema:name>
<schema:video>http://www.youtube.com/vI1Qc13bg2io?version�3&amp;amp;autohide�I </schema:video>
<schema:inLanguage>English</schema:inLanguage>
<hasSession>http://sebk.meIMOOC.owl#edx session OEEI 0I x<i1lasSession>
<isTaughtBy>http://sebk.meIMOOC.owl#ed�instructor_OEEI 0I x</isTaughtBy>
<schema:timeRequired>2-3 hours per week 16 weeks)</schema:timeReauired>

The "Course Spotlight" is an image carousel which features
courses that might be popular with average users. Currently it
is a mix of introductory courses we have hand-picked. In the
future we hope to base these courses on statistics incorporated

377

Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015)
into our RDF such as how many users have enrolled or
completed a specific course. This will depend on the future
availability of MOOC statistics via provider API or otherwise.

Search results are shown in a 4-column image grid as
shown in Figure 11. The images of these courses are retrieved
by the same method as the home page carousel. Courses on the
search results page are filterable by course provider.
Checkboxes are provided below course titles to flag which
courses one would like to compare in more detail. In this
example, the bottom three courses, "Calculus: Single
Variable", "Preparing for the AP Calculus AB and BC Exams"
and "Principles of Economics with Calculus" are flagged for
comparison.

The course images in this carousel are retrieved from
YouTube. The IDs for these videos are taken from our RDF
which collects links to introductory videos. Where an ID is not
provided, a placeholder logo for the provider, such as the
Coursera logo in the rightmost course, is shown. Hovering over
a course prompts links to the MOOCLink course details page
and the introductory video. The course title, provider, start
date, as well as the beginning of the course summary are
provided below the course images.
Below the course spotlight, an image with a quote from
Daphne Koller is shown. On further scrolling (which creates a
parallax effect with the image) the "Starting Soon" section
follows with 7 courses starting in the next month. This is
followed by a link to our SPARQL endpoint and a footer
featuring a brief project description, links to related work as
well as our contact information.

'--_ ...... -.
-� .... .......- .. -

_ .... _l1li-

�--,.-- --.--

�-- .. -

--

-

-

--...--..

.. -

o CLINK

�

COURSE SPOTUGKT

=::-- .....

.-

.-

a,-

a.- _____

�--.--

-,- .......... -

- .......... -

-_ ..... --.---

....... _ .. --

.....
. ..........

-'-'

"'---

a.._ImlA'

---

._-

l ..... __

--

---

--

-�----

-'-

-- -

- ... �

--

... -- ....

"""----

<...-It-. ...

-- ...

--

-- ..... -.

---

--

"-

-

--

_ ..... " ....

-

Fig. II.

In Figure 12, the table for the comparison of courses is
shown. Information pertaining to each course is displayed as an
"accordion" in which clicking on a property opens the field for
each course being compared allowing for simple detail-by­
detail comparison. Courses are once again filterable by course
provider and hovering over the course image brings up a link to
its corresponding MOOCLink "course details" page as well as
a link to enroll in the course.

STARTING SOON

1

�_.""""-"01_",."" __ """",,

Screenshot of search results

......
.......

.----­
--�
. __ ._ .... __ c.-Ma-

t.k ...

Fig. 10.

Screenshot of MOOCLink home page

Fig 12.

378

�v_.

Screenshot of course comparison table

Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015)
A course page on MOOCLink, shown in Figure 13, is
headed by the course title and start date as well as a
breadcrumb for navigation. Next to the introductory video is
the same accordion found on the course comparison table,
listing relevant course properties. Below the video is the full
course summary and a link to enroll in the course on its
provider's webpage.

syllabi details, we can perform a SPARQL query which
returns any course that mentions writing business plans
allowing the user to compare relevant courses side-by-side to
see that they enroll in the course that covers this topic most
extensively. We aim to incorporate more robust search such as
this example in future iterations of MOOCLink.
V.

..... �.

.,_

•.

--., - _.

CONCLUSION AND FUTURE WORK

We have presented an extension of Schema.org's Linked
Data vocabulary, which incorporates types and properties
found in online courses. The extended ontology allows for
assignment of data properties relevant to MOOCs as well as
object properties which link MOOC sessions, course details,
categories and instructors together.

.. 0

Also presented is our approach to collecting and generating
Linked Data from three MOOC providers: Coursera, edX, and
Udacity. Using Coursera's API and two Scrapy crawlers for
edX and Udacity, we collect MOOC data in JSON and convert
it to RDF with Apache Jena.
We describe a prototype implementation of MOOCLink, a
web application which utilizes the Linked MOOC Data to
allow users to discover and compare similar online courses. A
semantic web stack of Apache Tomcat as the web server and
servlet container, Fuseki as the SPARQL server, TDB as the
RDF store, and JavaServer Pages and Java Servlets to
dynamically create webpages is proposed to achieve this. The
functionality as well as the look and feel of the application are
highlighted with VI screenshots.

THIS COURSE BEGINS
11 JullOH
,--

Fig 13.

Screenshot of course details page

Our future work will focus on: incorporating demographic
data, reviews, developing an item pipeline for our crawlers,
automating website updates, enabling user profiles, course
tracks, natural language processing of syllabi and summaries
for more robust data and search.

In order to generate search results and retrieve course
properties from our RDF, SPARQL queries are performed.
These are called on our application server using Jena's ARQ
API which queries the RDF from a separate Fuseki Server.
Currently we search for names in the RDF that contain the
words searched using regular expressions. Figure 14 illustrates
a search for "calculus". All items in the RDF are selected
where they are of type mooc:Course and every course with a
schema:name or course title containing "calculus" (ignoring
case as defined by the regular expression filter) is returned.

ACKNOWLEDGMENTS

This work is supported in part by the Distributed Research
Experience for Undergraduates (DREU) program, a joint
project of the CRA Committee on the Status of Women in
Computing Research (CRA-W) and the Coalition to Diversify
Computing (CDC).

PREFIX mooc: <http://sebk.me/MOOC.owl#>
PREFIX schema: <http://schema.org/>
SELECT

*

REFERENCES

WHERE {

?course rdf:type mooc:Course.

[I)

Bizer, C., Heath, T., & Berners-Lee, T. (2009). Linked data-the story so
far. International journal on semantic web and information systems,
5(3), 1-22. Chicago

[2)

Berners-Lee, T., Hendler, J., & Lassila, O. (200I). The semantic
web. Scientific american, 284(5), 28-37.

[3)

Dietze, S. , Yu, H. Q. , Giordano, D., Kaldoudi, E. , Dovrolis, N. , & Taibi,
D. (2012, March). Linked Education: interlinking educational Resources
and the Web of Data. In Proceedings of the 27th Annual ACM
Symposium on Applied Computing (pp. 366-371). ACM.

[4)

Linked Up Challenge. (n.d.). LinkedUp Challenge. Retrieved July 22,
2014, from http://linkedup-challenge.org/

[5)

Pappano, L. (2012). The Year of the MOOe. The New York Times,
2(12),2012. Chicago

[6)

Coursera Catalog API. (n.d.). - Cow'sera Technology. Retrieved July 22,
2014, from https:lltech.coursera.org/app-platform/catalog/

[7)

Klyne, G. , & Carroll, J. J. (2006). Resource description framework
(RDF): Concepts and abstract syntax.

?course schema:name ?iname.
FILTER (regex(?iname, "calculus", "i"».

}
Fig. 14.

Sample of a SPARQL query behind MOOCLink search

Although keyword search of course titles may yield
relevant results, there is more work to be done in order to take
advantage of the more extensive searches we can perform with
SPARQL. For example, we might want to know what courses
cover writing business plans. There are few courses which
concentrate around business plan writing or have "business
plan" in the title, although many business courses might have
lectures which cover it. Because our MOOC RDF contains

379

Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015)
[8]

Perez, J. , Arenas, M., & Gutierrez, C. (2006). Semantics and
Complexity of SPARQL. In The Semantic Web-ISWC 2006 (pp. 30-43).
Springer Berlin Heidelberg.

[9]

Nilsson, Mikael, Matthias Palmer, and Jan Brase. "The LOM RDF
binding: principles and implementation." Proceedings of the Third
Annual ARlADNE conference, Leuven Belgium, 2003. 2003.

[19] McGuinness, D. L. , & Van Harmelen, F. (2004). OWL web ontology
language overview. W3C recommendation, 10(10), 2004.
[20] RDF Schema 1.1. (n.d.). RDF Schema 1.1. Retrieved July 23, 2014,
from hnp://www.w3.org/TRlrdf-schema/
[21] Requests: HTTP for Humans. (n.d.). Requests: HTTP for Humans Requests 2.3. 0 documentation. Retrieved July 22, 2014, from
http://docs. python-req uests. org/en/Iatest/

[10] IEEE Learning Object Metadata RDF binding. (n.d.). IEEE Learning
Object Metadata RDF Binding. Retrieved July 22, 2014, from
http://kmr.nada.kth.se/static/ims/md-Iomrdf.html

[22] Scrapy I An open source web scraping framework for Python. (n.d.).
Scrapy I An open source web scrapingji'ameworkfor Python. Retrieved
July 22, 2014, from http://scrapy.org/

[II] Bohl, 0., Scheuhase, J., Sengler, R., & Winand, U. (2002, December).
The sharable content object reference model (SCORM)-a critical review.
In Computers in education, 2002. proceedings. international conference
on (pp. 950-951). IEEE. Chicago

[23] Apache Jena. (n.d.). Apache Jena. Retrieved July 22, 2014, from
https://jena.apache.org/
[24] google-gson - A Java library to convert JSON to Java objects and vice­
versa - Google Project Hosting. (n.d.). google-gson - A Java librmy to

[12] Schema.rdfs.org: a mapping of Schema.org to RDF. (n.d.). - Home.
Retrieved July 22, 2014, from hnp://schema.rdfs.org/

convert JSON to Java objects and vice-versa - Google Project Hosting.

[13] Learning Resource Metadata Initiative. (2013, April 9). :: World's

Retrieved July 22, 2014, from http://code.google.com/p/google-gson/

Leading Search Engines Recognize LRMI as Education Metadata

[25] Bootstrap. (n.d.). Bootstrap.
http://getbootstrap.com/

Standard. Retrieved July 22, 2014, from http://www.lrmi.net/worlds­

leading-search-engines-recognize-Irmi-as-education-metadata-standard/

Retrieved

July

22,

2014,

from

[26] TDB Architecture. (n.d.). Apache Jena. Retrieved July 22, 2014, from
https://jena.apache.org/documentation/tdb/architecture.html

[14] Coursera. (n.d.). Retrieved July 22, 2014, from http://coursera.org/
[15] edX. (n.d.). Retrieved July 22, 2014, from http://edx.org/.
[16] Udacity. (n.d.). Retrieved July 22, 2014, from http://udacity.com/

[27] Google App Engine. (n.d.). URL Fetch API Overview. Retrieved July 22
2014, from https://developers.google.com/appengine/docs/java/urlfetch

[17] Learning Resource Metadata Initiative. (n.d.). :: The Specification.
Retrieved July 22, 2014, from http://www.lrmi.net/the-specification/

[28] HTTP Authentication in ARQ. (n.d.). Apache Jena -. Retrieved July 22,
2014, from http://jena.apache.org/documentation/query/http-auth.html

[18] A free, open-source ontology editor and framework for building
intelligent systems. (n.d.). protege. Retrieved July 22, 2014, from
http://protege.stanford.edu/

380

Information Assurance in Federated Identity
Management: Experimentations and Issues
Gail-Joon Ahn1, Dongwan Shin1, and Seng-Phil Hong2
1

University of North Carolina at Charlotte, Charlotte, NC 28232, USA
{gahn,doshin}@uncc.edu
2
Information and Communications University, Taejon, Korea
philhong@icu.ac.kr

Abstract. Identity management has been recently considered to be a
viable solution for simplifying user management across enterprise applications. When users interact with services on the Internet, they often
tailor the services in some way for their personal use through their personalized accounts and preferences. The network identity of each user
is the global set of such attributes constituting the various accounts. In
this paper, we investigate two well-known federated identity management
(FIM) solutions, Microsoft Passport and Liberty Alliance, attempting to
identify information assurance (IA) requirements in FIM. In particular,
this paper focuses on principal IA requirements for Web Services that
plays an integral role in enriching identity federation and management.
We also discuss our experimental analysis of those models.

1

Introduction

Surveys and polling data conﬁrm that the Internet is now a prime vehicle for
business, community, and personal interactions. The notion of identity is the
important component of this vehicle. Identity management (IM) has been recently considered to be a viable solution for simplifying user management across
enterprise applications. As enterprises have changed their business operation
paradigm from brick-and-mortar to click-and-mortar, they have embraced a variety of enterprise applications for streamlining business operations such as emailing systems, customer relationship management systems, enterprise resource
planning systems, supply chain management systems, and so on. However, a
non-trivial problem has been compounded by this reinforcing line of enterprise
applications, the problem of managing user proﬁles. The addition of such applications has proved to be subject to bringing in a new database for storing user
proﬁles, and it was quite costly and complex to manage all those proﬁles, which
were often redundant. Considering business-to-business environments, where a
set of users consists of not only their employees or customers but also those of
The work of Gail-J. Ahn and Dongwan Shin was supported by the grants from
Bank of America through e-Business Technology Institute at the University of North
Carolina at Charlotte.
X. Zhou et al. (Eds.): WISE 2004, LNCS 3306, pp. 78−89, 2004.
 Springer-Verlag Berlin Heidelberg 2004

Information Assurance in Federated Identity Management

79

their partners, this problem became even worse. As a set of underlying technologies and processes overarching the creation, maintenance, and termination
of user identities, IM has attempted to resolve such issues.
Furthermore, the prevalence of business alliances or coalitions necessitates
the further evolution of IM, so called federated identity management (FIM).
The main motivation of FIM is to enhance user convenience and privacy as well
as to decentralize user management tasks through the federation of identities
among business partners. As a consequence, a cost-eﬀective and interoperable
technology is strongly required in the process of federation. Web Services (WS)
can be as a good candidate for such requirement as it has served to provide the
standard way to enable the communication and composition of various enterprise
applications over distributed and heterogeneous networks.
Since identity federation is likely to go along with the exchange of sensitive
user information in a highly insecure online environment, security and privacy
issues with such exchange of information are key concerns in FIM. In this paper,
we describe a comparative study of FIM to investigate how to ensure information assurance (IA) for identity federation. We ﬁrst discuss key beneﬁts of
FIM and how WS can play an integral role in enriching IM through federation.
Then, we investigate two well-known FIM solutions, Liberty Alliance [HW03]
and Microsoft Passport [tr103], attempting to identify IA requirements in FIM.
In addition, we describe our experimental study on those models.
The rest of this paper is organized as follows. Section 2 overviews three approaches involved in IM, along with the prior research works related to our work.
Section 3 describes FIM, particularly, Liberty and Passport in detail. Section 4
discusses the role of WS in federating identities in the two models. Section 5
articulates IA requirements for FIM followed by the experimentation details in
Section 6. Section 7 concludes this paper.

2

Identity Management and Related Works

In this section, we start with the discussion of IM approaches. We categorize
IM approaches into the following three styles: isolated IM, centralized FIM, and
distributed FIM. Thereafter, we discuss the core components of WS architectures.
The isolated IM model is the most conservative of the three approaches.
Each business forms its own identity management domain (IMD) and has its
own way of maintaining the identities of users including employees, customers,
and partners. Hence, this model is simple to implement and has a tight control
over user proﬁles. However, it is hard to achieve user convenience with this
model since diﬀerent IMDs are likely to have diﬀerent authentication processes
or mechanisms for their users and corresponding authentication policies may
vary between players.
The centralized FIM model has a single identity provider (IDP) that brokers
trust to other participating members or service providers (SP) in a Circle of
Trust (CoT). IDP being a sole authenticator has a centralized control over the

80

G.-J. Ahn, D. Shin, and S.-P. Hong

identity management task, providing easy access to all SP domains with simplicity of management and control. The drawback of this approach is a single
point of failure within a CoT infrastructure in case that IDP fails to provide
authentication service. User convenience can be also achieved partially in that
the single sign-on (SSO) for users is only eﬀective within SPs which belong to
the same CoT.
The distributed FIM model provides a frictionless IM solution by forming a
federation and making authentication a distributed task. Every member agrees
to trust user identities vouched for by other members of the federation. This
helps users maintain their segregated identities, making them portable across
autonomous policy domains. It also facilitates SSO and trust, thereby allowing
businesses to share the identity management cost with its partners. Microsoft
Passport is based on the centralized FIM model, while Liberty Alliance aims to
be the distributed FIM model.
Earlier works related to user identity management were mostly focused on a
user-centric approach [DPR99], where users have control over IM functions. A
simple idea of managing user identities is described in [Cha85]. They proposed
the use of personal card computers to handle all payments of a user, thereby
ensuring the privacy and security of the user’s identity on the Web. Hagel and
Singer [HS99] discussed the concept of infomediaries where users have to trust
and rely on a third party to aggregate their information and perform IM tasks
on their behalf while protecting the privacy of their information. The Novell
digitalme technology [Cra] allows users to create various identity cards that can
be shared on the Internet according to users’ preferences. Users can control both
what information is stored in each card and conditions under which it may be
shared.

3

Federated Identity Management

In this section, we discuss FIM in general, Liberty Alliance and Microsoft Passport in particular. Federated identity gives the ability to securely recognize and
leverage user identities owned by trusted organizations within or across CoTs,
and identity federation allows organizations to securely share conﬁdential user
identities with trusted ones, without requiring users to re-enter their name and
password when they access their network resources. Additionally, identity federation provides the ability to optionally and securely share user information
such as their proﬁles or other data between various trusted applications which
is subject to user consent and organizational requirements.
Two well-known FIM solutions, Liberty Alliance and Microsoft Passport have
fundamentally the same goal of managing web-based identiﬁcation and authentication. Both enable organizations to build IM systems that can be federated
across many disparate sources. Therefore, each user can have a single network
identity that provides SSO to the web sites that have implemented either or
both of the systems.

Information Assurance in Federated Identity Management

3.1

81

Liberty Alliance

Liberty Alliance is a consortium of more than 150 companies working together
towards developing an open, interoperable standard for FIM. It is aimed towards
realizing the notion of a cohesive, tangible network identity, which can facilitate
SSO and frictionless business operations. It is a distributed FIM model, relying
on the notion of IDP and SP, as we discussed earlier. IDP is responsible for carrying out identity federation. Authentication messages or authentication requests
are passed between IDP and SP. IDP and SP in Liberty Alliance Model actually
facilitate WS to discover service locations and handle incoming messages from
other IDP and SP.

3.2

Microsoft Passport

Microsoft Passport provides authentication services for Passport-enabled sites
called participating sites. It was initially released as a service and not an open
speciﬁcation and precedes Liberty Alliance by at least a year. It is the underlying
authentication system of Microsoft Hotmail and Microsoft Network, and it is
integrated for use in Windows XP. A centralized Passport server is the only
IDP in Passport model and contains users’ authentication credentials and the
associated unique global identiﬁer called Passport Unique Identiﬁer (PUID).
Passport is an example of a centralized FIM model. Unlike Liberty Alliance,
cookies play a major role in Passport architecture where Passport server stores
and reads identity information in the form of session and browser cookies stored
securely at a client side.

4

Role of Web Services in FIM

In this section, we describe the role of WS in identity federation. Identity federation usually involves three actors: IDP, SP, and users. IDP in a CoT performs
the task of authentication and SP relies on IDP for authentication information of a user before granting the user access to its services. Identity federation occurs with the user’s consent to federate his local identity at SP with
his identity at IDP which further facilitates SSO. In this process of federation,
WS architecture has four key components: consumer, SOAP, WSDL and UDDI
and provides SOAP/HTTP-based standard communication vehicles among the
providers [tr201]. SP can discover IDP either statically or by querying a UDDI
registry. Afterwards, SP communicates with IDP by reading its WSDL from
UDDI, whereby SP can exchange authentication request/response through service endpoints (SEP) speciﬁed in WSDL.

82

G.-J. Ahn, D. Shin, and S.-P. Hong

4.1

Web Services in Liberty Alliance

In Liberty Alliance, each CoT has one or more providers using SOAP/HTTP
based communication channels for exchanging authentication-related information between WS endpoints. Both SP and IDP follow agreed-upon schema for
federation and SSO. Security Assertion Markup Language (SAML) [HBM02] is
an essential component in this process for the purpose of asserting authentication status of users between the providers. A federated sign-in at IDP would
provide users with a valid session that is respected by all the SPs in its CoT.
Figure 1(a) shows the WS-enabled FIM architecture for Liberty Alliance which
hosts two WS components, SSO Login and Global Logout.
Federation requires a user to opt-in by providing consent for mapping his
identities at IDP and SP. As a result, both IDP and SP store a pseudonym as a
name identiﬁer for the user. Pseudonyms are used by IDP later when the user
requests an SSO. IDP vouches for SAML-based user authentication request from
SP by providing SAML-based authentication response.
Global Logout WS endpoints, also called Single Logout endpoints, receive
and process logout events from SP and IDP. Typically, when a user logs out
from one provider, the user’s SSO session which is active at the rest of providers
is invalidated by sending a message to these WS endpoints. The user agent
accesses Global Logout WS at IDP and indicates that all SPs, which the IDP
has provided authentication for during the current session, must be notiﬁed of
the session termination. Then, the user agent receives an HTTP response from
IDP that conﬁrms the completion of a global logout.

SEP
SEP

Passport Server
Identity Provider

SEP

/H

P
TT
/H
P
A
S
O
L}

P
Participating site
A

TT

M

PM

A
O

FEDERATION
CIRCLE
(CoT)

S
P

{S

A

s

SSO/
Global
Logout

SSO
HTTP POST
profiles
and Web
Redirection

SEP

Service Provider

it h
w
e
r it p t
/w r y
ad C
re o r t
p
e
ki ss
oo P a

L}

SEP
SEP

oo P a

it h
w
e
r it pt
/w r y
ad t C
or
p
s

M

SEP

re

A

Service Provider

C

{S

SSO/
Global
Logout

k ie

C o okie re ad /w rite w ith
P a ssp ort C rypt

SSO/Global
Logout

C

Identity Provider

PM
Participating site
C

PM

SEP

SEP
SEP

SEP

SEP

Participating site
B

SEP
SEP
SEP

SEP

SEP
SEP

(a) Liberty Alliance model

(b) Passport model
Fig. 1. FIM Models

Information Assurance in Federated Identity Management

4.2

83

Web Services in Microsoft Passport

Figure 1(b) shows the Passport architecture with WS endpoints. There are WS
components that make up Passport authentication service and involve the implementation of the authentication service [tr103]. The primary WS component
for Passport authentication model is Login Service.
As implied by its name, Login WS is mainly in charge of the user authentication service. For instance, a user logging in to any Passport-enabled site
is automatically authenticated by all other Passport-enabled sites, thereby enabling SSO. Subsequent sites receive the authentication status of the user from
Login WS through a Component Conﬁguration Document (CCD). CCD is an
XML document used by Passport to facilitate the synchronization of the user’s
authentication status in participating sites.

5

Experimentations for Information Assurance: Metrics
and Details

In this section, we describe our experimentations and results. Our goal is to
measure the performance of the two models of federated identity management,
particularly focusing on authentication issue which is a critical component to
maintain information assurance. To measure the performance of LibertyAlliance
and M icrosof tP assport models, we developed a set of tools to generate and
monitor loads. The performance for various key operations or services–such as
federation of identities and SSO–are measured for the generated workload.
To identify those key operations, we ﬁrst introduce an imaginary company,
called MegaBank. Then we attempt to have MegaBank play one of the following
three roles as shown in Figure 1: a) MegaBank as Identity Provider, b)MegaBank
as Service Provider with single third-party Identity Provider, and c)MegaBank
as Service Provider with two third-party Identity Providers. There are various
unpredictable factors such as the delay from user’s end, which prevent us from
producing a workload that is exactly similar to the real life traﬃc. Moreover,
the workloads that we are using may diﬀer over the scenarios depending upon
the role played by the MegaBank in various scenarios. 1
Finally we develop metrics that are used to evaluate the performance of a
system. The comparison and analysis of the systems can be done by comparing
the values obtained for these measured metrics. Therefore, metrics can be termed
as the key points that reﬂect the impact of the changes in system state. We have
identiﬁed certain metrics for measuring the performance of the two FIM Models.
These metrics are common for both models. The measurement of these metrics
is performed by applying monitors at various locations in the systems. Those
monitors are embedded in the codes as software modules. A typical dialog that
1

Workload can be categorized into test workload and real workload. Real workload is
observed on a system being used for normal operations. Test workload denotes any
workload used in performance studies.

84

G.-J. Ahn, D. Shin, and S.-P. Hong

occurs between the communicating parties in each FIM model consists of various
time factors. The dialog between a service provider and identity provider may
consist of diﬀerent time factors as follows:
– Communication Time, T c[f rom,to] : The time an entity takes to send a request
to another entity and get a response back from that entity. T c[f rom,to] denotes
where “from” is the entity at which the time is measured and “to” is the
entity which sends back a response to the request made by a “from” entity.
The response sent back by the “to” entity completes the communication
cycle.
– Data Access Time, T dat : The time that a service provider or an identity
provider takes to retrieve a user’s information or attributes from the local
storage for the purpose of authentication is called the Data Access time. In
T dat , “at” signiﬁes the entity at which data access time is measured. The
data access time may vary depending upon the type or directory servers and
data access mechanism employed.
– Message Processing Time, T mat : The time taken by the entities to process
the message received. In T mat , “at” denotes the entity or the communicating
party at which message processing time is measured.
– Request Redirect Time, T r[sp1,sp2] : The time required for redirecting a service
request from one service provider to another service provider. T r[sp1,sp2]
denotes the time between the source service provider sp1 and the destination
service provider sp2.
This section describes additional metrics, their signiﬁcance, and the composition. By composition, we mean that one or more of these metrics may be
composite. They may contain one or more of the time factors and the actual measurement of these time factors may depend upon the case scenario. The steps for
measuring these metrics are diﬀerent for Liberty Alliance and Microsoft Passport because of the diﬀerences in their architecture. Though there are a number
of sub-factors that we can measure, we have limited our scope to the most important, required and relevant factors to the scope of our research.
– Local Login Time/Service Provider Authentication Time, Asp : Asp is the
time taken by a principal to get authenticated at the Service Provider. This
time neither facilitates federation nor SSO. The measurement of this metric
is important in situations where one wants to measure the data access time
at the Service Provider.
– Identity Provider Authentication Time, Ai : When a principal chooses to
logon using the identity providers credentials, the service provider directs
the principal to the identity provider site, which is one time process, when
the principal signs in for the ﬁrst time. Ai is the time taken by a principal to
get authenticated just after when he signs in at the identity providers’ site.
In other words, it is obtained from T dsp
– Federation Time, Fi,sp : For attempting a single sign-on, a principal is required to federate her/his identity at the service provider with its identity at
the Identity provider. Fi,sp consists of Ai and T c[sp,idp] the communication
time, data access time and the message processing time.

Information Assurance in Federated Identity Management

85

– Single Sign-On Time, S[idp,sp] : Once principal’s identities at various service
providers are federated with her/his identity at the identity provider, s/he
can access the resources at any of the service providers without re-logging
within a common authentication context. This is a very important metric
and is the most crucial in studying the performance of the two systems.
S[idp,sp] consists of the communication time, the message processing time
and the data access time including Ai , T c[sp1,idp] , T c[sp2,idp] and T r[sp1,sp2] .
Figure 2 shows our experimentation results on authentication and federation
issues based on the aforementioned metrics.
As we mentioned earlier, our experimental analysis represents a proportion
or a sample of the population that may exist in the real life for both the models.
Moreover, the factors that aﬀect the performance of the system may vary with
location and deployments across enterprise applications. In such cases, deﬁnitive statements cannot be made about the characteristics of all systems, but a
probabilistic statement about the range in which the characteristics of most systems would ﬁt can be made. Therefore, we have adopted a statistical approach
for performance evaluation. Rather than making any directive statement about
the superiority or inferiority of one of the two models, we are summarizing the
results based on their characteristics. We have adopted a statistical approach
whereby we can state with a certain amount of conﬁdence that the values of the
proposed metrics can lies within a speciﬁed range. Moreover, we can compare
these conﬁdence intervals (CIs) for various metrics with respect to the two models. We use the method to calculate the CIs for unpaired observations. The brief
steps for calculating the CIs that we used in this work are as follows:
1. We ﬁrst calculate the sample mean Xlam and Xpm for Liberty and Passport,
where n is the number of observations.
n
n
Xlam = n1 i=1 Xi , Xpm = n1 i=1 Xi
2. Next, we derive the sample standard deviations Slam and Spm and it gives
us the standard deviation S of the mean diﬀerence.
2
Spm
S2
S = ( lam
n + n )
3. Using the standard deviation, we compute the eﬀective number of degrees
of freedom V .
V =

S2
S2
lam + pm )2
n
n
2
S
S2
1
1
lam )2 +
2
(
( pm
n
n )
(n+1)
(n+1)

(

4. Finally we identify the conﬁdence interval CI for the mean that can be used
to determine the performance characteristics.
CI = (Xlam − Xpm ) ± t[1− a2 ,v] S
Unfortunately our results are not permitted to be available in public but we
brieﬂy describe lessons learned from this work. Our analysis demonstrated the
followings: a) FIM leads us to consider several trade-oﬀs between security and

86

G.-J. Ahn, D. Shin, and S.-P. Hong

Federation Time
IDP Authn. Time
1400
1400

1200
1200

1000
1000

Time (ms)
800

Time (ms)
800

600

600

400

400

IDP Authn. Time for X

200

Federation Time for X
Federation Time for Y

200

IDP Authn. Time for Y

0

0
1

2

3

4 5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Runs

Runs

(a) IDP authentication time

(b) Federation Time

Fig. 2. Experimentation Results

system overheads; b) organizational roles in FIM are very important to identify
additional requirements related to performance factors; and c) it gives us an
idea on which system and workload parameters mostly aﬀect the performance
of FIM models in given case scenarios. We believe this work can be helpful to
IA practitioners for designing the enhanced FIM architectures.

6

Information Assurance Issues in FIM

As an eﬀort to identify principal IA requirements for FIM, we discuss security
and privacy concerns relevant to WS in FIM in this section. We also describe
how Liberty Alliance and Microsoft Passport deal with these concerns to fulﬁll
such requirements in their architectures.
6.1

Security Concerns in FIM

Security concerns in FIM can be observed from the perspective of the general
objectives of information security: availability, integrity, and conﬁdentiality. In
addition, authorization is also an important aspect to be considered in that
controlled access to federated identity information is strongly required.
The availability of information in FIM models concerns system reliability and
timely delivery of information. In FIM models, the availability of information
can be ensured by not only having a common protocol or mechanism for communicating authentication and other information between parties but also securing
communication channels and messages. Channel security can be achieved using
protocols like TLS1.0/SSL3.0 or other protocols like IPsec with security characteristics that are equivalent to TLS or SSL. However, these protocols can only

Information Assurance in Federated Identity Management

87

provide security at the transport level and not at the message level. Liberty speciﬁcations strongly recommend TLS/SSL with well-known cipher suites [Wat03]
for channel security. More details has been discussed in [SSA03].
Message security is important in FIM for preventing attackers and intermediaries from tampering the messages that are in transit. Improper message
security generates concerns like identity theft, false authentication, and unauthorized use of resources. Web Services Security (WSS) [IBM02] tries to address
these issues by providing security extensions such as digital signature and encryption to SOAP messages. Signing a SOAP payload using XML Digital Signature [ERB+ 02] ensures the integrity of the message. The sender can sign a
SOAP message with his private key. The receiver can then verify the signature
with the sender’s public key to see if the message has been modiﬁed. In WS
architecture, public key infrastructure (PKI) can be leveraged to have organizations sign security assertions instead of issuing certiﬁcates. Liberty Alliance
speciﬁcations recommend XML Digital Signature and Encryption [IDS02] for
encrypting a complete SOAP message or a part of the SOAP message to maintain the integrity and conﬁdentiality of its contents. Microsoft Passport takes an
approach to encrypting cookies for securing data contained within them. Cookies store sensitive information like user proﬁles that can be securely accessed by
authorized parties.
FIM requires communicating parties to provide controlled access of information to legitimate users. Authorization deals with what information a user
or an application has access to or which operations a user or an application
can perform. Proper authorization mechanisms are necessary in WS communication especially when the communication endpoint is across multiple hops.
Liberty speciﬁcations recommend a permission-based attribute sharing mechanism, which enables users to specify authorization policies on their information
that they want to share. Similarly, Microsoft Passport allows users to have their
choices regarding the information they want to share with participating sites.
6.2

Privacy Concerns in FIM

Privacy is a growing concern with FIM models due to the voluminous exchange
of sensitive information that occur across enterprises. Securing communication
channels and encrypting messages may help preserve the privacy of relevant information only up to some extent. The security concerns that we discussed in
the previous section are obviously applicable to privacy as well. In WS-enabled
FIM where the receiver of a message may not be its ultimate destination, improper security measures may result in unauthorized access of user’s personal
information which leads to violation of privacy.
Protection of user identities and personal information can be achieved by
using the principle of pseudonymity. Obfuscating message payloads can also
preserve their privacy by making them accessible only by authorized parties
having proper credentials or keys [MPB03]. Privacy enhancing technologies like
Platform for Privacy Preference (P3P) [CCL+ 02] provide a solution for point-

88

G.-J. Ahn, D. Shin, and S.-P. Hong

to-point privacy protection based on user preferences. However, such solutions
do not scale for a more open, interoperable WS architecture.
Liberty’s SAML implementation uses pseudonyms constructed using pseudorandom values that have no discernable correspondence with users’ identiﬁers at
IDP or SP. The pseudonym has a meaning only in the context of the relationship
between the two communicating parties. The intent is to create a non-public
pseudonym so as to contravene the linkability to users’ identities or activities,
thereby maintaining the privacy.
Organizations using FIM models is required to follow four key principles of
fair information practices which are discussed in [tr102]:
– Notice: Users should receive prior notice of the information practices.
– Choice: Users have a choice to specify what information will be used and
the purpose for which the information is collected.
– Access: Users should be able to access and modify their personal information
as and when needed.
– Security: Users should be assured that the organizational system is capable
of securing their personal information.
Liberty speciﬁcations have recently proposed an approach to sharing user
attributes on the basis of user’s permission. The speciﬁcations also provide a
set of guidelines that will help businesses adhere to these principles. Microsoft
Passport’s approach to online privacy is also based on adherence to these aforementioned principles.

7

Conclusion and Future Works

Information security and privacy issues are the key concerns in FIM because
identity federation requires the exchange of sensitive user information in a highly
insecure and open network. In this paper, we discussed two well-known FIM solutions, Microsoft Passport and Liberty Alliance and how WS can play an integral
role in FIM. In addition, we have identiﬁed certain metrics that are crucial when
considering a FIM model. These metrics are composite metrics which may consist
of measuring one or more of the time factors. Also, we identiﬁed and discussed
core IA requirements in FIM focusing on WS-relevant issues. We believe our
work can be leveraged by the research and industry communities working on
issues in identity management.
Our future work will focus on a privacy attribute management framework
within Liberty Alliance which can provide users with a high level of conﬁdence
in the privacy of their personal data. Developing IA metrics for FIM is another
issue that we intend to work on in the near future. It is generally believed that
no single perfect set of IA metrics can be applied to all systems. Thus, we would
attempt to investigate IA metrics speciﬁcally designed for FIM systems.

Information Assurance in Federated Identity Management

89

References
[CCL+ 02] Lorrie Cranor, Lorrie Cranor, Marc Langheinrich, Massimo Marchiori, Martin Presler-Marshall, and Joseph Reagle.
The platform
for privacy preferences 1.0 (p3p1.0) speciﬁcation.
Technical report,
www.w3.org/TR/2002/REC-P3P-20020416/, 2002.
[Cha85] David Chaum. Security without identiﬁcation: Card computers to make big
brother obsolete. Communications of the ACM, 28(10):1030–1044, 1985.
[Cra]
Lorrie Faith Cranor. Agents of choice: Tools that facilitate notice and choice
about web site data practices.
[DPR99] Herbert Damker, Ulrich Pordesch, and Martin Reichenbach. Personal reach
ability and security management - negotiation of multilateral security. In
Proceedings of Multilateral Security in Communications, Stuttgart, Germany, 1999.
[ERB+ 02] D. Eastlake, J. Reagle, J. Boyer, B. Fox, and E. Simon. XML - signature syntax and processing. Technical report, http://www.w3.org/TR/2002/RECxmldsig-core-20020212/, 2002.
[HBM02] Phillip Hallam-Baker and Eve Maler. Assertions and protocols for OASIS
SAML.
Technical
report,
http://www.oasisopen.org/committees/security/docs/cs-sstc-core-01.pdf, 2002.
[HS99]
John Hegel and Marc Singer, editors. Net Worth: Shaping Market When
Customers Make the Rule. Harvard Business School Press, 1999.
[HW03] Jeﬀ Hodges and Tom Watson. Liberty architecture overview v 1.2-03.
Technical report, http://www.sourceid.org/docs/sso/liberty-architectureoverview-v1.1.pdf, 2003.
Technical
Web services security (WSS) speciﬁcations 1.0.05.
[IBM02] IBM.
report, http://www-106.ibm.com/developerworks/webservices/library/wssecure/, 2002.
[IDS02] Takeshi Imamura, Blair Dillaway, and Ed Simon. XML encryption syntax and processing. Technical report, http://www.w3.org/TR/2002/CRxmlenc-core-20020304/, 2002.
[MPB03] Marco Casassa Mont, Siani Pearson, and Pete Bramhall. Towards accountable management of identity and privacy: Sticky policies and enforceable
tracing
services.
Technical
report,
http://www.hpl.hp.com/techreports/2003/HPL-2003-49.pdf, 2003.
[SSA03] Prasad Shenoy, Dongwan Shin, and Gail-Joon Ahn. Towards IA-Aware
web services for federated identity management. In Proceedings of IASTED
International Conference on Communication, Network, and Information Security, pages 10–15, New York, USA, December 2003.
[tr102]
Federal Trade Commission. online proﬁling - a report to congress, part
2. Technical report, http://www.ftc.gov/os/2000/07/onlineproﬁling.htm,
2002.
[tr103]
Mircrosoft Corporations. Microsoft .Net Passport Review Guide. Technical
report, http://www.microsoft.com/net/services/passport/review guide.asp,
2003.
[tr201]
W3C note: Web services description language (WSDL) v 1.1. Technical
report, http://www.w3.org/TR/wsdl12/, 2001.
[Wat03] Tom Watson. Liberty ID-FF implementation guidlines v 1.2.02. Technical
report, Liberty Alliance Project, 2003.

Multiparty Authorization Framework for Data Sharing
in Online Social Networks
Hongxin Hu and Gail-Joon Ahn
Arizona State University, Tempe, AZ 85287, USA
{hxhu,gahn}@asu.edu

Abstract. Online social networks (OSNs) have experienced tremendous growth
in recent years and become a de facto portal for hundreds of millions of Internet
users. These OSNs offer attractive means for digital social interactions and information sharing, but also raise a number of security and privacy issues. While
OSNs allow users to restrict access to shared data, they currently do not provide
effective mechanisms to enforce privacy concerns over data associated with multiple users. In this paper, we propose a multiparty authorization framework that
enables collaborative management of shared data in OSNs. An access control
model is formulated to capture the essence of multiparty authorization requirements. We also demonstrate the applicability of our approach by implementing a
proof-of-concept prototype hosted in Facebook.
Keywords: Social network, Multiparty, Access control, Privacy, Data sharing.

1 Introduction
In recent years, we have seen unprecedented growth in the application of OSNs. For
example, Facebook, one of representative social network sites, claims that it has over
500 million active users and over 30 billion pieces of shared contents each month [2],
including web links, news stories, blog posts, notes and photo albums. To protect user
data, access control has become a central feature of OSNs [1,3].
A typical OSN provides each user with a virtual space containing profile information, a list of the user’s friends, and web pages, such as wall in Facebook, where users
and friends can post contents and leave messages. A user profile usually includes information with respect to the user’s birthday, gender, interests, education and work history,
and contact information. In addition, users can not only upload a content into their own
or others’ spaces but also tag other users who appear in the content. Each tag is an
explicit reference that links to a user’s space. For the protection of user data, current
OSNs indirectly require users to be system and policy administrators for regulating
their data, where users can restrict data sharing to a specific set of trusted users. OSNs
often use user relationship and group membership to distinguish between trusted and
untrusted users. For example, in Facebook, users can allow friends, friends of friends,
specific groups or everyone to access their data, relying on their personal authorization
and privacy requirements.


This work was partially supported by the grants from National Science Foundation (NSF-IIS0900970 and NSF-CNS-0831360) and Department of Energy (DE-SC0004308).

Y. Li (Ed.): Data and Applications Security and Privacy XXV, LNCS 6818, pp. 29–43, 2011.
c IFIP International Federation for Information Processing 2011


30

H. Hu and G.-J. Ahn

Although OSNs currently provide simple access control mechanisms allowing users
to govern access to information contained in their own spaces, users, unfortunately,
have no control over data residing outside their spaces. For instance, if a user posts a
comment in a friend’s space, s/he cannot specify which users can view the comment. In
another case, when a user uploads a photo and tags friends who appear in the photo, the
tagged friends cannot restrict who can see this photo, even though the tagged friends
may have different privacy concerns about the photo. To address such an issue, preliminary protection mechanisms have been offered by existing OSNs. For example,
Facebook allows tagged users to remove the tags linked to their profiles. However, removing a tag from a photo can only prevent other members from seeing a user’s profile
by means of the association link, but the user’s image is still contained in the photo.
Since original access control policies cannot be changed, the user’s image continues to
be accessed by all authorized users. Hence, it is essential to develop an effective and
flexible access control mechanism for OSNs, accommodating the special authorization
requirements coming from multiple associated users for collaboratively managing the
shared data.
In this paper, we propose a multiparty authorization framework (MAF) to model
and realize multiparty access control in OSNs. We begin by examining how the lack of
multiparty access control for data sharing in OSNs can undermine the protection of user
data. A multiparty authorization model is then formulated to capture the core features
of multiparty authorization requirements which have not been accommodated so far by
existing access control systems and models for OSNs (e.g., [6,7,13,14,19]). Meanwhile,
as conflicts are inevitable in multiparty authorization specification and enforcement,
systematic conflict resolution mechanism is also addressed to cope with authorization
and privacy conflicts in our framework.
The rest of the paper is organized as follows. Section 2 gives a brief overview of
related work. In Section 3, we present multiparty authorization requirements and articulate our proposed multiparty authorization model, including multiparty authorization
specification and multiparty policy evaluation. Implementation details and experimental
results are described in Section 4. Section 5 concludes this paper.

2 Related Work
Access control for OSNs is still a relatively new research area. Several access control
models for OSNs have been introduced (e.g., [6,7,13,14,19]). Early access control solutions for OSNs introduced trust-based access control inspired by the developments
of trust and reputation computation in OSNs. The D-FOAF system [19] is primarily a
Friend of a Friend (FOAF) ontology-based distributed identity management system for
OSNs, where relationships are associated with a trust level, which indicates the level of
friendship between the users participating in a given relationship. Carminati et al. [6]
introduced a conceptually-similar but more comprehensive trust-based access control
model. This model allows the specification of access rules for online resources, where
authorized users are denoted in terms of the relationship type, depth, and trust level between users in OSNs. They further presented a semi-decentralized discretionary access
control model and a related enforcement mechanism for controlled sharing of information in OSNs [7]. Fong et al. [14] proposed an access control model that formalizes

Multiparty Authorization Framework for Data Sharing in Online Social Networks

31

and generalizes the access control mechanism implemented in Facebook, admitting arbitrary policy vocabularies that are based on theoretical graph properties. Gates [8]
described relationship-based access control as one of new security paradigms that addresses unique requirements of Web 2.0. Then, Fong [13] recently formulated this
paradigm called a Relationship-Based Access Control (ReBAC) model that bases authorization decisions on the relationships between the resource owner and the resource
accessor in an OSN. However, none of these existing work could model and analyze
access control requirements with respect to collaborative authorization management of
shared data in OSNs.
Recently, semantic web technologies have been used to model and express finegrained access control policies for OSNs (e.g., [5,10,21]). Especially, Carminati et
al. [5] proposed a semantic web based access control framework for social networks.
Three types of policies are defined in their framework, including authorization policy,
filtering policy and admin policy, which are modeled with the Web Ontology Language
(OWL) and the Semantic Web Rule Language (SWRL). Access control policies regulate how resources can be accessed by the participants; filtering policies specify how
resources have to be filtered out when a user fetches an OSN page; and admin policies
can determine who is authorized to specify policies. Although they claimed that flexible
admin policies are needed to bring a system to a scenario where several access control
policies specified by distinct users can be applied to the same resource, the lack of formal descriptions and concrete implementation of the proposed approach leaves behind
the ambiguities of their solution.
The need of joint management for data sharing, especially photo sharing, in OSNs
has been recognized by the recent work [4,24,26]. The closest work to this paper is
probably the solution provided by Squicciarini et al. [24] for collective privacy management in OSNs. Their work considered access control policies of a content that is
co-owned by multiple users in an OSN, such that each co-owner may separately specify her/his own privacy preference for the shared content. The Clarke-Tax mechanism
was adopted to enable the collective enforcement of policies for shared contents. Game
theory was applied to evaluate the scheme. However, a general drawback of their solution is the usability issue, as it could be very hard for ordinary OSN users to comprehend
the Clarke-Tax mechanism and specify appropriate bid values for auctions. In addition,
the auction process adopted in their approach indicates that only the winning bids could
determine who can access the data, instead of accommodating all stakeholders’ privacy
preferences. In contrast, our work proposes a formal model to address the multiparty
access control issue in OSNs, along with a general policy specification scheme and
a simple but flexible conflict resolution mechanism for collaborative management of
shared data in OSNs.
Other related work include general conflict resolution mechanisms for access control [12,15,16,17,18,20] and learn-based generation of privacy policies for OSNs
[11,22,23]. All of those related work are orthogonal to our work.

3 Multiparty Authorization for OSNs
In this section, we analyze the requirements of multiparty authorization (Section 3.1)
and address the modeling approach we utilize to represent OSNs (Section 3.2). We also

32

H. Hu and G.-J. Ahn

(a) A shared data item has multiple stakeholders

(b) A shared data item is published by a contributor

Fig. 1. Scenarios of Multiparty Authorization in OSNs

introduce a policy scheme (Section 3.3) and an authorization evaluation mechanism
(Section 3.4) for the specification and enforcement of multiparty access control policies
in OSNs.
3.1 Requirements
OSNs provide built-in mechanisms enabling users to communicate and share data with
other members. OSN users can post statuses and notes, upload photos and videos in
their own spaces, and tag others to their contents and share the contents with their
friends. On the other hand, users can also post contents in their friends’ spaces. The
shared contents may be connected with multiple users. Consider an example where a
photograph contains three users, Alice, Bob and Carol. If Alice uploads it to her own
space and tags both Bob and Carol in the photo, we call Alice an owner of the photo,
and Bob and Carol stakeholders of the photo. All of these users may specify access
control policies over this photo. Figure 1(a) depicts a data sharing scenario where the
owner of a data item shares the data item with other OSN members, and the data item
has multiple stakeholders who may also want to involve in the control of data sharing.
In another case, when Alice posts a note stating “I will attend a party on Friday night
with @Carol” to Bob’s space, we call Alice a contributor of the note and she may
want to make the control over her notes. In addition, since Carol is explicitly identified
by @-mention (at-mention) in this note, she is considered as a stakeholder of the note
and may also want to control the exposure of this note. Figure 1(b) shows another data
sharing scenario where a contributor publishes a data item to someone else’s space and
the data item may also have multiple stakeholders (e.g., tagged users). All associated
users should be allowed to define access control policies for the shared data item.
OSNs also enable users to share others’ data. For example, when Alice views a photo
in Bob’s space and selects to share this photo with her friends, the photo will be in turn
posted to her space and she can specify access control policies to authorize her friends
to see this photo. In this case, Alice is a disseminator of the photo. Since Alice may
adopt a weaker control saying the photo is visible to everyone, the initial access control requirements of this photo should be complied with, preventing from the possible
leakage of sensitive information via the procedure of data dissemination. For a more
complicated case, the disseminated data may be further re-disseminated by disseminator’s friends, where effective access control mechanisms should be applied in each

Multiparty Authorization Framework for Data Sharing in Online Social Networks

33

procedure to regulate sharing behaviors. Especially, regardless of how many steps the
data item has been re-disseminated, the original access control policies should be always enforced to protect the data dissemination.
3.2 Modeling Social Networks
An OSN can be represented by a relationship network, a set of user groups and a collection of user data. The relationship network of an OSN is a directed labeled graph, where
each node denotes a user, and each edge represents a relationship between users. The
label associated with each edge indicates the type of the relationship. Edge direction denotes that the initial node of an edge establishes the relationship and the terminal node
of the edge accepts the relationship. The number and type of supported relationships
rely on the specific OSNs and its purposes. Besides, OSNs include an important feature that allows users to be organized in groups [28,27], where each group has a unique
name. This feature enables users of an OSN to easily find other users with whom they
might share specific interests (e.g., same hobbies), demographic groups (e.g., studying
at the same schools), political orientation, and so on. Users can join in groups without any approval from other group members. Furthermore, OSNs provide each member
with a web space where users can store and manage their personal data including profile
information, friend list and user content.
We now formally model and define an online social network as follows:
Definition 1 (Online Social Network). An online social network is modeled as a 9tuple OSN =< U, G, P C, RT, RC, T T, CC, U U, U G >, where
– U is a set of users of the OSN. Each user has a unique identifier;
– G is a set of groups to which the users can belong. Each group also has a unique identifier;
– P C is a collection of user profile sets, {p1 , . . . , pn }, where pi = {pi1 , . . . , pim } is the
profile set of a user i ∈ U . Each profile entry is a <attribute: profile value> pair, pij =<
attrj : pvaluej >;
– RT is a set of relationship types supported by the OSN. Each user in an OSN may be connected with others by relationships of different types;
– RC is a collection of user relationship sets, {r1 , . . . , rn }, where ri = {ri1 , . . . , rim } is the
relationship set of a user i ∈ U . Each relationship entry is a <user: relationship type> pair,
rij =< uj : rtj >, where uj ∈ U and rtj ∈ RT ;
– T T is a set of content types supported by the OSN. Supported content types are photo, video,
note, event, status, message, link, and so on;
– CC is a collection of user content sets, {c1 , . . . , cn }, where ci = {ci1 , . . . , cim } is a set
of contents of a user i ∈ U . Each content entry is a <content: content type> pair, cij =<
contj : ttj >, where contj is a content identifier and ttj ∈ T T ;
– U U is a collection of uni-directional binary user-to-user relations, {U Urt1 , . . . , U Urtn },
where U Urti ⊆ U × U specifies the pairs of users in a relationship type rti ∈ RT ; and
– U G ⊆ U × G is a binary user-to-group membership relation;

Figure 2 shows an example of social network representation. It describes relationships
of five individuals, Alice (A), Bob (B), Carol (C), Dave (D) and Edward (E), along
with their groups of interest and their own spaces of data. Note that two users may be
directly connected by more than one edge labeled with different relationship types in
the relationship network. For example, in Figure 2, Alice (A) has a direct relationship

34

H. Hu and G.-J. Ahn

Fig. 2. An Example of Social Network Representation

of type colleagueOf with Bob (B), whereas Bob (B) has a relationship of friendOf with
Alice (A). Moreover, in this example, we can notice there are two groups that users can
participate in: the “Fashion” group and the “Hiking” group, and some users, such as
Alice (A) and Edward (E), may join in multiple groups.
3.3 Multiparty Authorization Specification
To enable a collaborative authorization management of data sharing in OSNs, it is essential for multiparty access control policies to be in place to regulate access over shared
data, representing authorization requirements from multiple associated users. Our policy specification scheme is built upon the above-mentioned OSN model (Section 3.2).
Recently, several access control schemes (e.g., [6,13,14]) have been proposed to support fine-grained authorization specifications for OSNs. Unfortunately, these schemes
can only allow a single controller (the resource owner) to specify access control policies. Indeed, a flexible access control mechanism in a multi-user environment like OSNs
is necessary to allow multiple controllers associated with the shared data item to specify access control policies. As we discussed in Section 3.1, in addition to the owner of
data, other controllers, including the contributor, stakeholder and disseminator of data,
also desire to regulate access to the shared data. We formally define these controllers as
follows:
Definition 2 (Owner). Let d be a shared data item in the space of a user i ∈ U in the
social network. The user i is called the owner of d, denoted as OWdi .
Definition 3 (Contributor). Let d be a shared data item published by a user i ∈ U in
someone else’s space in the social network. The user i is called the contributor of d,
denoted as CBdi .
Definition 4 (Stakeholder). Let d be a shared data item published in the space of a
user in the social network. Let T be the set of tagged users associated with d. A user
i ∈ U is called a stakeholder of d, denoted as SHdi , if i ∈ T .

Multiparty Authorization Framework for Data Sharing in Online Social Networks

35

Fig. 3. Hierarchical User Data in OSNs

Definition 5 (Disseminator). Let d be a shared data item disseminated by a user i ∈ U
from someone else’s space to her/his space in the social network. The user i is called a
disseminator of d, denoted as DSdi .
In the context of an OSN, user data is composed of three types of information: User
profile describes who the user is in the OSN, including identity and personal information, such as name, birthday, interests and contact information. User relationship shows
who the user knows in the OSN, including a list of friends to represent the connections
with family members, coworkers, colleagues, and so on. User content indicates what
the user has in the OSN, including photos, videos, statuses, and all other data objects
created through various activities in the OSN. Formally, we define user data as follows:
Definition 6 (User Data). The user data is a collection of data sets, {d1 , . . . , dn },
where di = pi ∪ ri ∪ ci is a set of data of a user i ∈ U representing the user’s profile
pi , the user’s relationship list ri , and the user’s content set ci , respectively.
User data in OSNs can be organized as a hierarchical structure, whose leaves represent
the instances of data, and whose intermediate nodes represent classifications of data.
Figure 3 depicts a hierarchical structure of user data where the root node, user data, is
classified into three types, profile, relationship and content. The content is further divided into multiple categories, such as photo, video, note, event, status, etc. In this way,
access control policies can be specified over both data classifications and instances. Especially, access control policies specified on classifications can be automatically propagated down in the hierarchy. For instance, if access for the parent node photo is allowed,
access for all children nodes of photo is also allowed. As a consequence, such a hierarchical structure of user data can be used to improve the expressiveness of access control
policies and simplify the authorization management.
To summarize the aforementioned features and elements, we introduce a formal definition of multiparty access control policies as follows:
Definition 7 (Multiparty Access Control Policy). A multiparty access control policy is
a 7-tuple P =< controller, ctype, accessor, atype, data, action, ef f ect >, where

36

H. Hu and G.-J. Ahn

– controller ∈ U is a user who can regulate access to data;
– ctype ∈ {OW, CB, SH, DS} is the type of the controller (owner, contributor,
stakeholder, and disseminator, respectively);
– accessor is a set of users to whom the authorization is granted, representing with
a set of user names, a set of relationship types or a set of group names. Note that
patterns are allowed to specify any set by using the the wildcard (*) instead of a
specific name;
– atype ∈ {U N, RN, GN } is the type of the accessor specification (user name,
relationship type, and group name, respectively);
– data ∈ di ∪ T T ∪ DT is a data item dij ∈ di , a content type tt ∈ T T , or a data
type dt ∈ DT = {prof ile, relationship, content}, where i ∈ U ;
– action = view is an action being authorized or forbidden;1 and
– ef f ect ∈ {permit, deny} is the authorization effect of the policy.
Note that different representations of accessor in our policy specification scheme have
different semantics. If the accessor is represented with a set of user names {u1 , . . . , un },
the semantics of this user name set can be explained as u1 ∨ . . . ∨ un , which means that
any user contained in the user name set is treated as an authorized accessor. On the other
hand, if the accessor is expressed as a set of relationship types {rt1 , . . . , rtn } or a set
of group names {g1 , . . . , gn }, the semantics of the relationship type set or group name
set are interpreted as rt1 ∧ . . . ∧ rtn or g1 ∧ . . . ∧ gn . Examples of multiparty access
control policies are as follows:
1. p1 = (Alice, OW, {f riendOf }, RN, < statusId, status >, view, permit):
Alice authorizes her friends to view her status identified by statusId. In this policy, Alice is an owner of the status.
2. p2 = (Bob, CB, {colleageOf }, RN, photo, view, permit): Bob authorizes his
colleagues to view all photos he publishes to others’ spaces. In this policy, Bob is
a contributor of the photos.
3. p3 = (Carol, ST, {f riendOf, colleageOf }, RN, < photoId, photo >, view,
permit): Carol authorizes users who are both her friends and her colleagues to
view one photo photoId she is tagged in. In this policy, Carol is a stakeholder of
the photo.
4. p4 = (Dave, OW, {Bob, Carol}, U N, < eventId, event >, view, deny): Dave
disallows Bob and Carol to view his event eventId.
5. p5 = (Edward, DS, {f ashion, hiking}, GN, < videoId, video >, view,
permit): Edward authorizes users who are in both groups, f ashion and hiking,
to view a video videoId that he disseminates. In this policy, Edward is a disseminator of the video.
3.4 Multiparty Policy Evaluation
In our proposed multiparty authorization model, each controller can specify a set of
policies, which may contains both positive and negative policies, to regulate access of
1

We limit our consideration to view action. The support of more actions such as post,
comment, tag, and update does not significantly complicate our approach proposed in this
paper.

Multiparty Authorization Framework for Data Sharing in Online Social Networks

37

Fig. 4. Conflict Identification for Multiparty Policy Evaluation

the shared data item. Two steps should be performed to evaluate an access request over
multiparty access control policies. The first step checks the access request against policies of each controller and yields a decision for the controller. Bringing in both positive
and negative policies in the policy set of a controller raises potential policy conflicts.
In the second step, decisions from all controllers responding to the access request are
aggregated to make a final decision for the access request. Since those controllers may
generate different decisions (permit and deny) for the access request, conflicts may
occurs again. Figure 4 illustrates potential conflicts identified during the evaluation of
multiparty access control policies. In order to make an unambiguous final decision for
each access request, it is crucial to adopt a systematic conflict resolution mechanism to
resolve those identified conflicts during multiparty policy evaluation.
Policy Conflict Resolution in One Party. In the first step of multiparty policy evaluation, policies belonging to each controller are evaluated in sequence, and the accessor
element in a policy decides whether the policy is applicable to a request. If the user who
sends the request belongs to the user set derived from the accessor of a policy, the policy is applicable and the evaluation process returns a response with the decision (either
permit or deny) indicated by the effect element in the policy. Otherwise, the response
yields NotApplicable. In the context of OSNs, controllers generally utilize a positive policy to define a set of trusted users to whom the shared data item is visible, and
a negative policy to exclude some specific untrusted users from whom the shared data
item should be hidden. Some general conflict resolution strategies for access control
have been introduced [12,15,16]. For example, deny-overrides (this strategy indicates
that “deny” policy take precedence over “allow” policy), allow-overrides (this strategy states that “allow” policy take precedence over “deny” policy), specificity-overrides
(this strategy states a more specific policy overrides more general policies), and recencyoverrides (this strategy indicates that policies take precedence over policies specified
earlier). We can adopt these strategies to resolve policy conflicts in our conflict resolution mechanism when evaluating a controller’s policies. Since some strategies, such
as specificity-overrides and recency-overrides are nondeterministic, and deny-overrides
strategy is too restricted in general for conflict resolution, it is desirable to combine
these strategies together to achieve a more effective conflict resolution. Thus, a strategy
chain can be constructed to address this issue, which has been discussed in our previous
work [17,18].

38

H. Hu and G.-J. Ahn

Resolving Multiparty Privacy Conflicts. When two users disagree on whom the
shared data item should be exposed to, we say a privacy conflict occurs. The essential reason leading to the privacy conflicts is that multiple controllers of the shared data
item often have different privacy concerns over the data item. For example, assume that
Alice and Bob are two controllers of a photo. Each of them defines an access control
policy stating only her/his friends can view this photo. Since it is almost impossible
that Alice and Bob have the same set of friends, privacy conflicts may always exist
considering multiparty control over the shared data item.
A naive solution for resolving multiparty privacy conflicts is to only allow the common users of accessor sets defined by the multiple controllers to access the data. Unfortunately, this strategy is too restrictive in many cases and may not produce desirable
results for resolving multiparty privacy conflicts. Let’s consider an example that four
users, Alice, Bob, Carol and Dave, are the controllers of a photo, and each of them
allows her/his friends to see the photo. Suppose that Alice, Bob and Carol are close
friends and have many common friends, but Dave has no common friends with them
and also has a pretty weak privacy concern on the photo. In this case, adopting the naive
solution for conflict resolution may turn out that no one can access this photo. Nevertheless, it is reasonable to give the view permission to the common friends of Alice,
Bob and Carol.
A strong conflict resolution strategy may provide a better privacy protection. In the
meanwhile, it reduces the social value of data sharing in OSNs. Therefore, it is important to consider the tradeoff between privacy and utility when resolving privacy conflicts. To address this issue, we introduce a flexible mechanism for resolving multiparty
privacy conflicts in OSNs based on a voting scheme. Several simple and intuitive strategies can be derived from the voting scheme as well.
Our voting scheme contains two voting mechanisms, decision voting and sensitivity
voting. In the decision voting, an aggregated decision value from multiple controllers
with respect to the results of policy evaluation is computed. In addition, each controller
assigns a sensitivity level to the shared data item to reflect her/his privacy concern.
Then, a sensitivity score for the data item can be calculated as well through aggregating each controller’s sensitivity level value. Based on the aggregated decision value
and the sensitivity score, our decision making approach provides two conflict resolution solutions: automatic conflict resolution and strategy-based conflict resolution. A
basic idea of our approach for automatic conflict resolution is that the sensitivity score
can be utilized as a threshold for decision making. Intuitively, if the sensitivity score
is higher, the final decision is likely to deny access, taking into account the privacy
protection of high sensitive data. Otherwise, the final decision is very likely to allow
access. Hence, the utility of OSN services cannot be affected. In the second solution,
the sensitivity score of a data item is considered as a guideline for the owner of shared
data item in selecting an appropriate strategy for conflict resolution. Several specific
strategies can be used for resolving multiparty privacy conflicts in OSNs. For example,
owner-overrides (the owner’s decision has the highest priority), full-consensus-permit
(if any controller denies the access, the final decision is deny), majority-permit (this
strategy permits a request if over 1/2 controllers permit it), strong-majority-permit (this
strategy permits a request if over 2/3 controllers permit it), and super-majority-permit
(this strategy permits a request if over 3/4 controllers permit it).

Multiparty Authorization Framework for Data Sharing in Online Social Networks

39

Requests

Privacy Policy

Owner
Control

Sensitivity Setting

Strategy Selection

Privacy Policy

Privacy Policy

Contributor
Control

Sensitivity Setting

Stakeholder
Control

Sensitivity Setting

Disseminator
Control

Privacy Policy

Recommendation
Strategy
Repository

Sensitivity Aggregation

Decision Aggregation

Decision

Decision Making

Final Decisions

Fig. 5. System Architecture of Decision Making in MController

Conflict Resolution for Disseminated Data. A user can share others’ contents with
her/his friends in OSNs. In this case, the user is a disseminator of the content, and the
content will be posted in the disseminator’s space and visible to her/his friends or the
public. Since a disseminator may adopt a weaker control over the disseminated content
but the content may be much sensitive from the perspective of original controllers of
the content, the privacy concerns from the original controllers of the content should
be always complied with, preventing inadvertent disclosure of sensitive contents. In
other words, the original access control policies should be always enforced to restrict
access to the disseminated content. Thus, the final decision for an access request to
the disseminated content is a composition of the decisions aggregated from original
controllers and the decision from the current disseminator. In order to eliminate the risk
of possible leakage of sensitive information from the procedure of data dissemination,
we leverage the restrictive conflict resolution strategy, Deny-overrides, to resolve
conflicts between original controllers’ decision and the disseminator’s decision. In such
a context, if either of those decisions is to deny the access request, the final decision is
deny. Otherwise, if both of them are permit, the final decision is permit.

4 Prototype Implementation and Evaluation
To demonstrate the feasibility of our authorization model and mechanism, we implemented a Facebook-based application called MController for supporting collaborative
management of shared data. Our prototype application enables multiple associated users
to specify their authorization policies and privacy preferences to co-control a shared
data item. We currently restrict our prototype to deal with photo sharing in OSNs. Obversely, our approach can be generalized to handle other kinds of data, such as videos
and comments, in OSNs as long as the stakeholders of shared data can be identified
with effective methods like tagging or searching.

40

H. Hu and G.-J. Ahn

Fig. 6. MController for Owner Control on Facebook

MController is deployed as a third-party application of Facebook, which is hosted
in an Apache Tomcat application server supporting PHP and MySQL database. MController application is based on the iFrame external application approach, adopting the
Facebook REST-based APIs and supporting Facebook Markup Language (FBML),
where Facebook server acts as an intermediary between users and the application server.
Facebook server accepts inputs from users, then forwards them to the application server.
The application server is responsible for the input processing and collaborative management of shared data. Information related to user data such as user identifiers, friend lists,
user groups, and user contents are stored in the MySQL database.
Once a user installs MController in her/his Facebook space, MController can access
user’s basic information and contents. In particular, MController can retrieve and list
all photos, which are owned or uploaded by the user, or where the user was tagged.
Then, the user can select any photo to define the privacy preference. If the user is not
the owner of selected photo, s/he can only edit the privacy setting and sensitivity setting
of the photo. Otherwise, if the user is an owner of the photo, s/he can further configure
the conflict resolution mechanism for the shared photo.
A core component of MController is the decision making module, which processes
access requests and returns responses (either permit or deny) for the requests. Figure 5 depicts a system architecture of the decision making module in MController. To
evaluate an access request, the policies of each controller of the targeted content are enforced first to generate a decision for the controller. Then, the decisions of all controllers
are aggregated to yield a final decision as the response of the request. During the procedure of decision making, policy conflicts are resolved when evaluating controllers’

Multiparty Authorization Framework for Data Sharing in Online Social Networks

41

Fig. 7. Performance of Policy Evaluation Mechanism

policies by adopting a strategy chain pre-defined by the controllers. In addition, multiparty privacy conflicts are resolved based on the configured conflict resolution mechanism when aggregating the decisions of controllers. If the owner of the content chooses
automatic conflict resolution, the aggregated sensitivity value is utilized as a threshold
for making a decision. Otherwise, multiparty privacy conflicts are resolved by applying
the strategy selected by the owner, and the aggregated sensitivity score is considered as
a recommendation for the strategy selection. Regarding access requests to the disseminated contents, the final decision is made by combining the disseminator’s decision and
original controllers’ decision through a deny-overrides combination strategy.
A snapshot of MController for owner control is shown in Figure 6, where an owner
of a photo can assign weight values to different types of controllers of the shared photo,
and select either automatic or manual mechanism for conflict resolution. If the owner
chooses manual conflict resolution, s/he can further select an appropriate conflict resolution strategy referring to the recommendation derived from the sensitivity score of
the photo. Note that MController currently requires all controllers of a shared photo
should define their privacy preferences before applying our authorization mechanism
to evaluate the requests. Otherwise, the photo is only visible to the controllers. Since
a user may be involved in the control of hundreds of photos, manual input of the privacy preferences is a time-consuming and tedious task. As part of our future work, we
would study inference-based techniques [11] for automatically configuring controllers’
privacy preferences.
To evaluate the performance of the policy evaluation mechanism in MController,
we changed the number of the controllers of a shared photo from 1 to 20. Also, we
considered two cases for our evaluation. In the first case, each controller has only one
positive policy. The second case examines two policies (one positive policy and one
negative policy) of each controller. Figure 7 shows the policy evaluation cost while
changing the number of the controllers. For both cases, the experimental results show
that the policy evaluation cost increased slightly with the increase of the number of the

42

H. Hu and G.-J. Ahn

controllers. Also, we can observe that MController performs fast enough to handle even
a large number of controllers for collaboratively managing the shared data.

5 Conclusion and Future Work
In this paper, we have proposed a novel authorization framework that facilitates collaborative management of the shared data in OSNs. We have given an analysis of multiparty
authorization requirements in OSNs, and formulated a multiparty access control model.
Our access control model is accompanied with a multiparty policy specification scheme
and corresponding policy evaluation mechanism. Moreover, we have described a proofof-concept implementation of our approach called MController, which is a Facebook
application, along with performance analysis.
As our future work, we will incorporate a logic-based reasoning feature into our approach to provide a variety of analysis services for collaborative management of the
shared data. Also, we are planning to conduct extensive user studies to evaluate the usability of our proof-of-concept implementation, MController. In addition, as effective
automated algorithms (e.g., facial recognition [9,25]) are being developed to recognize people accurately in contents such as photos and then generate tags automatically,
access and privacy controls will become even more problematic in the future. Consequently, we would extend our work to explore more sophisticated and effective solutions
to address emerging security and privacy challenges for sharing various data in OSNs.

References
1. Facebook Privacy Policy, http://www.facebook.com/policy.php/
2. Facebook Statistics,
http://www.facebook.com/press/info.php?statistics
3. Myspace Privacy Policy,
http://www.myspace.com/index.cfm?fuseaction=misc.privacy/
4. Besmer, A., Lipford, H.R.: Moving beyond untagging: Photo privacy in a tagged world. In:
Proceedings of the 28th International Conference on Human Factors in Computing Systems,
pp. 1563–1572. ACM, New York (2010)
5. Brands, S.A.: Rethinking public key infrastructures and digital certificates: building in privacy. The MIT Press, Cambridge (2000)
6. Carminati, B., Ferrari, E., Perego, A.: Rule-based access control for social networks. In:
Meersman, R., Tari, Z., Herrero, P. (eds.) OTM 2006 Workshops. LNCS, vol. 4278, pp. 1734–
1744. Springer, Heidelberg (2006)
7. Carminati, B., Ferrari, E., Perego, A.: Enforcing access control in web-based social networks.
ACM Transactions on Information and System Security (TISSEC) 13(1), 1–38 (2009)
8. Carrie, E.: Access Control Requirements for Web 2.0 Security and Privacy. In: Proc. of Workshop on Web 2.0 Security & Privacy (W2SP), Citeseer (2007)
9. Choi, J., De Neve, W., Plataniotis, K., Ro, Y., Lee, S., Sohn, H., Yoo, H., Neve, W., Kim,
C., Ro, Y., et al.: Collaborative Face Recognition for Improved Face Annotation in Personal
Photo Collections Shared on Online Social Networks. IEEE Transactions on Multimedia,
1–14 (2010)
10. Elahi, N., Chowdhury, M., Noll, J.: Semantic Access Control in Web Based Communities.
In: Proceedings of the Third International Multi-Conference on Computing in the Global
Information Technology, pp. 131–136. IEEE, Los Alamitos (2008)

Multiparty Authorization Framework for Data Sharing in Online Social Networks

43

11. Fang, L., LeFevre, K.: Privacy wizards for social networking sites. In: Proceedings of the
19th International Conference on World Wide Web, pp. 351–360. ACM, New York (2010)
12. Fisler, K., Krishnamurthi, S., Meyerovich, L.A., Tschantz, M.C.: Verification and changeimpact analysis of access-control policies. In: ICSE 2005: Proceedings of the 27th International Conference on Software Engineering, pp. 196–205. ACM, New York (2005)
13. Fong, P.: Relationship-Based Access Control: Protection Model and Policy Language. In:
Proceedings of the First ACM Conference on Data and Application Security and Privacy.
ACM, New York (2011)
14. Fong, P., Anwar, M., Zhao, Z.: A privacy preservation model for facebook-style social network systems. In: Backes, M., Ning, P. (eds.) ESORICS 2009. LNCS, vol. 5789, pp. 303–
320. Springer, Heidelberg (2009)
15. Fundulaki, I., Marx, M.: Specifying access control policies for XML documents with XPath.
In: Proceedings of the Ninth ACM Symposium on Access Control Models and Technologies,
pp. 61–69. ACM, New York (2004)
16. Jajodia, S., Samarati, P., Subrahmanian, V.S.: A logical language for expressing authorizations. In: IEEE Symposium on Security and Privacy, Oakland, CA, pp. 31–42 (May 1997)
17. Jin, J., Ahn, G., Hu, H., Covington, M., Zhang, X.: Patient-centric authorization framework
for sharing electronic health records. In: Proceedings of the 14th ACM Symposium on Access Control Models and Technologies, pp. 125–134. ACM, New York (2009)
18. Jin, J., Ahn, G.J., Hu, H., Covington, M.J., Zhang, X.: Patient-centric authorization framework for electronic healthcare services. Computers & Security 30(2-3), 116–127 (2011)
19. Kruk, S., Grzonkowski, S., Gzella, A., Woroniecki, T., Choi, H.: D-FOAF: Distributed identity management with access rights delegation. In: Mizoguchi, R., Shi, Z.-Z., Giunchiglia, F.
(eds.) ASWC 2006. LNCS, vol. 4185, pp. 140–154. Springer, Heidelberg (2006)
20. Li, N., Wang, Q., Qardaji, W., Bertino, E., Rao, P., Lobo, J., Lin, D.: Access control policy
combining: theory meets practice. In: Proceedings of the 14th ACM Symposium on Access
Control Models and Technologies, pp. 135–144. ACM, New York (2009)
21. Masoumzadeh, A., Joshi, J.: Osnac: An ontology-based access control model for social networking systems. In: IEEE International Conference on Privacy, Security, Risk and Trust,
pp. 751–759 (2010)
22. Shehab, M., Cheek, G., Touati, H., Squicciarini, A., Cheng, P.: User Centric Policy Management in Online Social Networks. In: 2010 IEEE International Symposium on Policies for
Distributed Systems and Networks, pp. 9–13. IEEE, Los Alamitos (2010)
23. Squicciarini, A., Paci, F., Sundareswaran, S.: PriMa: an effective privacy protection mechanism for social networks. In: Proceedings of the 5th ACM Symposium on Information,
Computer and Communications Security, pp. 320–323. ACM, New York (2010)
24. Squicciarini, A., Shehab, M., Paci, F.: Collective privacy management in social networks. In:
Proceedings of the 18th International Conference on World Wide Web, pp. 521–530. ACM,
New York (2009)
25. Stone, Z., Zickler, T., Darrell, T.: Autotagging Facebook: Social network context improves
photo annotation. In: IEEE Computer Society Conference on Computer Vision and Pattern
Recognition Workshops, CVPRW 2008, pp. 1–8. IEEE, Los Alamitos (2008)
26. Wishart, R., Corapi, D., Marinovic, S., Sloman, M.: Collaborative Privacy Policy Authoring
in a Social Networking Context. In: 2010 IEEE International Symposium on Policies for
Distributed Systems and Networks, pp. 1–8. IEEE, Los Alamitos (2010)
27. Wondracek, G., Holz, T., Kirda, E., Kruegel, C.: A practical attack to de-anonymize social
network users. In: 2010 IEEE Symposium on Security and Privacy, pp. 223–238. IEEE, Los
Alamitos (2010)
28. Zheleva, E., Getoor, L.: To join or not to join: the illusion of privacy in social networks with
mixed public and private user profiles. In: Proceedings of the 18th International Conference
on World Wide Web, pp. 531–540. ACM, New York (2009)

Role-Based Privilege Management
Using Attribute Certificates and Delegation
Gail-Joon Ahn, Dongwan Shin, and Longhua Zhang
University of North Carolina at Charlotte, Charlotte, NC 28232, USA
{gahn,doshin,lozhang}@uncc.edu
Abstract. The Internet provides tremendous connectivity and immense
information sharing capability which the organizations can use for their
competitive advantage. However, we still observe security challenges in
Internet-based applications that demand a uniﬁed mechanism for both
managing the authentication of users across enterprises and implementing business rules for determining user access to enterprise applications
and their resources. These business rules are utilized for privilege management or authorization in a security context. In this paper, we design
a role-based privilege management leveraging access control models and
X.509 attribute certiﬁcate. We attempt to develop an easy-to-use, ﬂexible, and interoperable authorization mechanism. Also, we demonstrate
the feasibility of our architecture by providing the proof-of-concept prototype implementation using commercial oﬀ-the-shelf technologies.

1

Introduction

Many organizations have transited from their old and disparate business models
based on ink and paper to a new, consolidated ones based on digital information
on the Internet. The Internet is uniquely and strategically positioned to address
the needs of a growing segment of population in a very cost-eﬀective way. It
provides tremendous connectivity and immense information sharing capability
which the organizations can use for their competitive advantage. However, we
still observe security challenges in Internet-based applications that demand a
uniﬁed mechanism for both managing the authentication of users across enterprises and implementing business rules for determining user access to enterprise
applications and their resources. These business rules are utilized for privilege
management or authorization in a security context [13]. In this paper, we often
use the term authorization and access control as an identical notion of privilege
management. Authentication mechanisms have been practiced at considerable
length and various authentication schemes such as SSL, LDAP-based, or secure
cookies-based have been widely accepted. Unlike authentication mechanisms,
authorization mechanisms which can conveniently enforce various business rules
from diﬀerent authorization domains among various applications still need to be
investigated.
Role-based access control (RBAC) has been acclaimed and proven to be a
simple, ﬂexible, and convenient way of managing access control [6, 15]. This extremely simpliﬁes management of privileges, reducing complexity and potential
S. Katsikas, J. Lopez, and G. Pernul (Eds.): TrustBus 2004, LNCS 3184, pp. 100–109, 2004.
c Springer-Verlag Berlin Heidelberg 2004


Role-Based Privilege Management

101

errors in directly assigning privileges to users. Another issue is to support such
a simpliﬁed privilege management among distributed Internet-based enterprise
applications. Privilege management infrastructure (PMI) [4, 5] has recently been
introduced allowing us to establish the trustworthiness among diﬀerent authorization domains as long as each of them keeps the meaning of attributes intact.
Our objective in this paper is to design a role-based privilege management
leveraging RBAC features and X.509 attribute certiﬁcate in PMI. We attempt to
develop an easy-to-use, ﬂexible, and interoperable authorization mechanism. We
also seek to address the issue of how to advocate selective information sharing in
internet-based enterprise applications while minimizing the risks of unauthorized
access.
The rest of this paper is organized as follows. Section 2 shows previous researches related to our work. Section 3gives an overview of background technologies. Section 4 describes our approach to designing a role-based privilege
management with attribute certiﬁcates and delegation including system architecture and authorization policies. Implementation details are described in Section 5. Section 6 discusses lessons learned from our experiment and concludes
the paper.

2

Related Works

Several researchers have been trying to accommodate RBAC features into largescale systems of intranet or extranet focusing on various applications such as
database systems, web servers, or web-based workﬂow systems. At the same
time, delegation has been studied by a number of researchers as an important
factor for secure distributed computing environment [7].
In the OSF/DCE environment [11], privilege attribute certiﬁcate (PAC) that
a client can present to an application server for authorization was introduced.
PAC provided by a DCE security server contains the principal and associated
attribute lists, which are group memberships. This approach focused on the
traditional group-based access control.
Similarly, Thompson et al. [18] developed a certiﬁcate-based authorization
system called Akenti for managing widely distributed resources. It was especially
designed for system environments where resources have multiple stakeholders
and each stakeholder wants to impose conditions for access. Their approach
emphasized the policy-based access control in a distributed environment.
Also, several studies have been carried out to make use of RBAC features
with the help of public-key certiﬁcates [1, 12]. Public-key certiﬁcates were used
to contain attribute information such as role in their extension ﬁeld. To add role
information into public key certiﬁcates, however, may cause problems such as
shortening of certiﬁcates’ lifetime and complexity of their management [17].
In general, delegation is referred to as one active entity in a system delegates
its authority to another entity to carry out some functions. In role-based systems, the delegated authorities are roles. The requirements related to role-based
delegation have been identiﬁed in the literature [2, 8, 21]. A work closely related

102

Gail-Joon Ahn, Dongwan Shin, and Longhua Zhang

to ours is RBDM0 model proposed by Barka and Sandhu [2]. They developed
a simple role-based delegation model. They explored some issues including revocation, delegation with hierarchical roles, partial delegation, and multi-step
delegation. One limitation of RBDM0 is that this work does not address the relationships among each component of a delegation, which is a critical notion to
the delegation model. A number of researchers have looked at the semantics of
authorization, delegation, and revocation. Li et al. proposed a logic for authorizing delegation in large-scale, open, distributed systems [3, 10]. But in their logic,
role-based concepts were not fully adopted; neither did they address revocation
adequately.

3
3.1

Background Technologies
Role-Based Access Control

RBAC is an alternative policy to traditional mandatory access control (MAC)
and discretionary access control (DAC). As MAC is used in the classical defense
arena, the policy of access is based on the classiﬁcation of objects such as topsecret level [14]. The main idea of DAC is that the owner of an object has
discretionary authority over who else can access that object [9]. But RBAC
policy is based on the role of the subjects and can specify security policy in a
way that maps to an organization’s structure. A general family of RBAC models
called RBAC96 was deﬁned by Sandhu et al [15]. Motivation and discussion
about various design decisions made in developing this family of models is given
in [15, 16]. Also, there are variations regarding distributed systems [20].
Figure 1(a) shows (regular) roles and permissions that regulate access to data
and resources. Intuitively, a user is a human being or an autonomous agent, a
role is a job function or job title within the organization with some associated
semantics regarding the authority and responsibility conferred on a member of
the role, and a permission is an approval of a particular mode of access to one
or more objects in the system or some privilege to carry out speciﬁed actions.
Roles are organized in a partial order ≥, so that if x ≥ y then role x inherits the
permissions of role y. Members of x are also implicitly members of y. In such
cases, we say x is senior to y. Each session relates one user to possibly many
roles. The idea is that a user establishes a session and activates some subset
of roles that he or she is a member of (directly or indirectly by means of the
role hierarchy). A user may have multiple sessions open at the same time, each
in a diﬀerent window on the workstation screen for instance. Each session may
have a diﬀerent combination of active roles. The concept of a session equates
to the traditional notation of a subject in access control. A subject is a unit of
access control, and a user may have multiple subjects (or sessions) with diﬀerent
permissions active at the same time.
3.2

Privilege Management Infrastructure

PMI is based on the ITU-T Recommendation of directory systems speciﬁcation
[4], which introduced PKI in its earlier version. Public-key certiﬁcates are used

Role-Based Privilege Management

103

in PKI while attribute certiﬁcates are a central notion of PMI. Public-key certiﬁcates are signed and issued by certiﬁcation authority (CA), while attribute
certiﬁcates are signed and issued by attribute authority (AA). PMI is to develop
an infrastructure for access control management based on attribute certiﬁcate
framework. Attribute certiﬁcates bind attributes to an entity. The types of attributes that can be bound are role, group, clearance, audit identity, and so on.
Attribute certiﬁcates have a separate structure from that of public key certiﬁcates.
PMI consists of four models: general model, control model, delegation model,
and roles model. General and control models are required, whereas roles and
delegation models are optional. The general model provides the basic entities
which recur in other models.

RH
UA

UAO

ROLE
HIERARCHY

USER
ASSIGNMENT
U
USERS

user

S

R

P

ROLES

PERMISSIONS

roles
.
.
.

PA

CONSTRAINTS

ODLGT

PERMISSION

UAD

ASSIGNMENT

DDLGT
DLGT

CONSTRAINTS
SESSIONS

(a) RBAC Model

(b) Delegation Relation
Fig. 1. RBAC and Delegation.

4
4.1

Role-Based Privilege Management
Adopting Attribute Certificate

Our approach is based on basic entities in PMI. It consists of three foundation entities: the object, the privilege asserter, and the privilege veriﬁer. The
control model explains how access control is managed when privilege asserters
request services on object. When the privilege asserter requests services by presenting his/her privileges, the privilege veriﬁer makes access control decisions
based upon the privilege presented, privilege policies, environmental variables,
and object methods. PMI roles model also introduces two additional components: role assignment and role speciﬁcation. Role assignment is to associate
privilege asserters with roles, and its binding information is contained in attribute certiﬁcate called role assignment attribute certiﬁcate. The latter is to
associate roles with privileges, and it can be contained in attribute certiﬁcate
called role speciﬁcation attribute certiﬁcate or locally conﬁgured at a privilege
veriﬁer’s system. Our approach is based upon PMI roles model. Accordingly, two
diﬀerent attribute certiﬁcates are employed: role assignment attribute certiﬁcate
(RAAC ) and role speciﬁcation attribute certiﬁcate (RSAC ). The integrity of
the bindings is guaranteed through digital signature in attribute certiﬁcate.

104

4.2

Gail-Joon Ahn, Dongwan Shin, and Longhua Zhang

Constrained Role-Based Delegation

Zhang et al. [21] introduced RDM2000 (role delegation model 2000) for userto-user delegation in role-based systems. Our work is based on RDM2000. It
formalizes the relationship between two user assignments that form a delegation relation (DLGT), as shown in Figure 1(b). We ﬁrst deﬁne a new relation
called delegation relation (DLGT). It includes sets of three elements: original
user assignments UAO, delegated user assignment UAD, and constraints. The
motivation behind this relation is to address the relationships among diﬀerent
components involved in a delegation. In a user-to-user delegation, there are four
components: a delegating user, a delegating role, a delegated user, and a delegated role. A delegation relation is one-to-many relationship on user assignments.
It consists of original user delegation (ODLGT) and delegated user delegation
(DDLGT). We assume each delegation relation may have a duration constraint
associated with it. If the duration is not explicitly speciﬁed, we consider the
delegation as permanent unless another user revokes it. The function Duration
returns the assigned duration-restriction constraint of a delegated user assignment. If there is no assigned duration, it returns a maximum value. Our delegation model has the following components and theses components are formalized
from the above discussions.
– T is a set of duration-restricted constraint.
– DLGT ⊆ UA × UA is one to many delegation relation. A delegation relation
can be represented by (u, r, u , r ) ∈ DLGT, which means the delegating
user u with role r delegated role r to user u .
– ODLGT ⊆ UAO × UAD is an original user delegation relation.
– DDLGT ⊆ UAD × UAD is a delegated user delegation relation.
– DLGT = ODLGT ∪ DDLGT.
In some cases, we may need to deﬁne whether or not each delegation can be further delegated and for how many times, or up to the maximum delegation depth.
We introduce two types of delegation: single-step delegation and multi-step delegation. Single-step delegation does not allow the delegated role to be further
delegated; multi-step delegation allows multiple delegations until it reaches the
maximum delegation depth. The maximum delegation depth is a natural number deﬁned to impose restriction on the delegation. Single-step delegation is a
special case of multi-step delegation with maximum delegation depth equal to
one.
Also, we have an additional concept, delegation path (DP) that is an ordered list of user assignment relations generated through multi-step delegation.
A delegation path always starts from an original user assignment. We use the
following notation to represent a delegation path.
uao0 → uad1 → uadi → uadn
Delegation paths starting with the same original user assignment can further
construct a delegation tree. A delegation tree (DT) expresses the delegation
paths in a hierarchical structure. Each node in the tree refers to a user assignment
and each edge to a delegation relation. The layer of a user assignment in the tree

Role-Based Privilege Management

105

is referred as the delegation depth. The function Prior maps one delegated user
assignment to the delegating user assignment; function Path returns the path
of a delegated user assignment; and function Depth returns the depth of the
delegation path.
Constraints are an important aspect of RBAC and can lay out higher-level
organizational policies. In theory, the eﬀects of constraints can be achieved by
establishing procedures and sedulous actions of security administrators [6]. Constraints are enforced by a set of integrity rules that provide management and
regulators with the conﬁdence that critical security policies are uniformly and
consistently enforced. In the framework, when a user delegates a role, all context
constraints that are assigned to the user and anchored to the delegated role are
delegated as well.
Rule-Based Policy Specification Language. We also deﬁne policies that
allow regular users to delegate their roles. It also speciﬁes the policies regarding
which delegated roles can be revoked. A rule-based language is adopted to specify
and enforce these policies. It is a declarative language in which binds logic with
rules. The advantage is that it is entirely declarative so it is easier for security
administrator to deﬁne policies.
A rule takes the form:
H ← F 1&F 2& . . . &F n
where H, F1, F2,. . . , Fn are Boolean functions.
There are three sets of rules in the framework: basic authorization rules specify
organizational delegation and revocation policies; authorization derivation rules
enforce these policies in collaborative information systems; and integrity rules
specify and enforce role-based constraints. For example, a user-user delegation
authorization rule forms as follows:
can delegate(r, cr, n) ← .
where r, cr, and n are elements of roles, prerequisite conditions, and maximum delegation depths respectively.
This is the basic user-to-user delegation authorization rule. It means that a member of the role r (or a member of any role that is senior to r) can assign a user
whose current membership satisﬁes prerequisite condition cr to role r (or a role
that is junior to r) without exceeding the maximum delegation depth n.
Constraints Specification. In order to represent role-based privilege management constraints, we deﬁne rules that are extremely suited for constraints
speciﬁcation as well as enforcement. We articulate several constraints and specify them using a rule-based language introduced in [21].
A static separation of duty (SSOD): incompatible roles assignment
constraint states that no common user can be assigned to conﬂicting roles in
the incompatible role set ira = {r1 , r2 , ... }. This constraint can be represented
as:

106

Gail-Joon Ahn, Dongwan Shin, and Longhua Zhang

cannot assign(u, r) ←
senior(r, one element(ira))&
member of (u, one element(all other(ira, one element(ira)))).
where u ∈ U, r ∈ R, and ira ∈ IRA.
The rule says if r equals one element of a set of the incompatible role assignments ira, and a user u is already member of another role other than r in the
incompatible role set, then u cannot be assigned role r.
An incompatible users constraint states that two conﬂicting users in the
incompatible user set iu={u1, u2, ...} cannot be assigned to the same role. This
constraint can be represented as:
cannot assign(u, r) ←
equals(u , one element(all other(iu, u)))&
member of (u , r).
An incompatible permissions constraint states that two conﬂicting permissions in the incompatible user set ip={p1, p2, ...} cannot be assigned to the same
role. This constraint can be represented as:
cannot assignp(r, p) ←
equals(p , one element(all other(ip, p)))&
in(p , permissions role(r)).
A role cardinality constraint states that a role can have a maximum number
N of user members. This constraint can be represented as:
cannot assign(u, r) ←
greater than(cardi(r), maxcardi(r) − 1).
A user cardinality constraint states that a user can be member of a maximum
number N of roles. This constraint can be represented as:
cannot assign(u, r) ←
greater than(cardi(u), maxcardi(u) − 1).
We have demonstrated how diﬀerent constraints can be speciﬁed using rules.

5

Implementation Details

Our implementation leverages role-based delegation features and X.509 attribute
certiﬁcate. We attempt to implement the proof-of-concept prototype implementation of our architecture. An overview of the preliminary architecture is shown
in Figure 2.
It consists of a number of services and management agents together with the
objects to be managed. The enforcement agents are based on a combination of
roles and rules for specifying and interpreting policies. Since delegation and revocation services are only part of a security infrastructure, we choose a modular
approach to our architecture that allows the delegation and revocation services
to work with current and future authentication and access control services. The

Role-Based Privilege Management

Privilege Asserter

Cert.

Security
Officer

Attribute
Certificate
Server

Client

107

Request/Issue RAAC

AC Storage

Rule Editor

Access/delegation
Request & Decision
Role Database

Rule Service

Role Service

PMI Attribute Authority

Request/Issue RSAC

Delegation
Agent

Access Control
Agent

Authentication
Agent

Server
Access/
delegation
Request &
Decision

Reference
Monitor
Policy Database

Privilege Verifier

Fig. 2. Operational architecture for role-based EAM.

modularity enables future enhancements of our approach. The role service is
provided by a role server and a role server maintains RBAC database and provides user credentials, role memberships, associated permissions, and delegation
relations of the system. The rule service is provided by a rule server, which manages delegation and revocation rules. These rules are always associated with a
role, which speciﬁes the role that can be delegated. They are implemented as
authorization policies that authorize requests from users. The rule editor is developed to simplify the management of these rules. As a portion of an integrated
RBAC administration platform to manage various components, the rule editor
is used to view, create, edit, and delete delegation and revocation rules. The delegation agent is an administrative infrastructure, which authorizes delegation
and revocation requests from users by applying derivation authorization rules
and processes delegation and revocation transactions on behalf of users. We
implement these components as the delegation/revocation service: users’ delegation/revocation requests are interpreted, authorized, and processed by the
service; it creates RDM2000 elements based upon users’ requests and maintains
the integrity of the database by checking and enforcing consistency rules. The
core of this service is a rule engine. We implemented the rule inference engine by
extending SWI-Prolog [19] using its C++ interface. The rule engine has three
functional units: a pre-processor, an inference engine, and a post-processor.
In Figure 2, three components are identiﬁed for managing attribute certiﬁcates: privilege asserter, privilege veriﬁer, and PMI attribute authority as we
described in Section 4. A privilege asserter is developed by using ActiveX control, named attribute certiﬁcate manager. The manager enables a user to import
downloaded BER-encoded RAACs into Windows registry. Internet Information
Server (Version 5.0) is used as a privilege veriﬁer. An HTTP raw data ﬁlter,
called AC ﬁlter, was developed using Microsoft ISAPI (Internet Server API)
technology. An attribute certiﬁcate server was developed to generate RAAC s

108

Gail-Joon Ahn, Dongwan Shin, and Longhua Zhang

and RSAC s. The programming library, called AC SDK, was built for supporting
the functionality related to the generation of the attribute certiﬁcates. Netscape
Directory Service 5.0 was used for both a role database and an AC storage. We
also developed an application working as an access control policy server. This
application has been developed in C++. An engine for making access control
decisions is a major component in this application.

6

Conclusion and Future Works

Authentication mechanisms have been practiced at considerable length and various authentication schemes have been widely accepted. Unlike authentication
mechanisms, privilege management which can conveniently enforce various business rules from diﬀerent authorization domains among various applications still
need to be investigated. In this paper, we have discussed issues of privilege management. We also attempted to utilize an existing delegation framework and
attribute certiﬁcates in PMI. In addition, we demonstrated the feasibility of our
architecture through a proof-of-concept implementation. We believe that this
work would lead Internet-based applications to consider privilege management
as a core component in their design and deployment.

Acknowledgements
This work was partially supported at the Laboratory of Information of Integration, Security and Privacy at the University of North Carolina at Charlotte by the grants from National Science Foundation (NSF-IIS-0242393) and
Department of Energy Early Career Principal Investigator Award (DE-FG0203ER25565).

References
1. G. Ahn, R. Sandhu, M. Kang, and J. Park. “Injecting RBAC to secure a Webbased workﬂow system,” In Proceedings of 5th ACM Workshop on Role-Based
Access Control. Berlin, Germany, July 2000.
2. E. Barka and R. Sandhu. Framework for role-based delegation model. In Proceedings of 23rd National Information Systems Security Conference, pages 101–114,
Baltimore, MD, October 16-19 2000.
3. E. Bertino, E. Ferrari and V. Atluri. The speciﬁcation and enforcement of authorization constraints in workﬂow management systems. ACM Transactions on
Information and System Security, Vol.2 No.1, p.65-104, Feb. 1999
4. ITU-T Recommendation X.509. Information Technology: Open Systems Interconnection - The Directory: Public-Key And Attribute Certiﬁcate Frameworks, 2000.
ISO/IEC 9594-8:2001.
5. S. Farrell and R. Housley. An Internet Attribute Certiﬁcate Proﬁle for Authorization, PKIX Working Group, June 2001.

Role-Based Privilege Management

109

6. D. Ferraiolo, J. Cugini, and D.R Kuhn. “Role Based Access Control: Features
and Motivations,” In Annual Computer Security Applications Conference, IEEE
Computer Society Press, 1995.
7. M. Gasser and E. McDermott. An Architecture for Practical Delegation a Distributed System. In Proceedings of IEEE Computer Society Symposium on Research in Security and Privacy, Oakland, CA, May 7-9,1990.
8. A. Hagstrom, S. Jajodia, F. P. Presicce, and D. Wijesekera. Revocations - a
classiﬁcation. In Proc. 14th IEEE Computer Security Foundations Workshop, pages
44–58, Nova Scotia, Canada, June 2001.
9. S. Jajodia, P. Samarati, V. Subrahmanian, and E. Bertino. A uniﬁed framework
for enforcing multiple access control policies. In Proceedings of the ACM SIGMOD
international conference on management of data, pages 474–485, 1997.
10. N. Li and B. N. Grosof. A practically implementation and tractable delegation
logic. In Proceedings of IEEE Symposium on Security and Privacy, May 2000.
11. OSF DCE 1.0 Introduction to DCE, Open Software Foundation, Cambridge, MA,
1999.
12. J. Park, R. Sandhu, and G. Ahn. “Role-based Access Control on the Web,” ACM
Transactions on Information and System Security, 4(1), February 2001.
13. John Pescatore. Extranet Access Management Magic Quadrant, Gartner Research
Note (ID: M-13-6853), Gartner INC., May 2001.
14. R. S. Sandhu. Lattice-based access control models. IEEE Computer, 26(11):9–19,
November 1993.
15. Ravi S. Sandhu, Edward J. Coyne, Hal L. Feinstein, and Charles E. Youman.
Role-based access control models. IEEE Computer, 29(2):38–47, February 1996.
16. R. Sandhu. Rationale for the RBAC96 family of access control models. In Proceedings of the 1st ACM Workshop on Role-Based Access Control. ACM, 1997.
17. D. Shin, Gail-J. Ahn, and S. Cho. Role-based EAM Using X.509 Attribute Certiﬁcate. In Proceedings of Sixteenth Annual IFIP WG 11.3 Working Conference on
Data and Application Security, King’s College, University of Cambridge, UK July
29-31, 2002.
18. M. Thompson, W. Johnston, S. Mudumbai, G. Hoo, K. Jackson, and A. Essiari.
“Certiﬁcate-based Access Control for Widely Distributed Resources,” In Proceedings of the 8th USENIX Security Symposium, Washington, D.C., August 1999.
19. Wielemaker. “J. SWI-Prolog,” http://www.swi.psy.uva.nl/projects/SWI-Prolog/
20. N. Yialelis, E. Lupu, and M. Sloman. Role-based security for distributed object
systems. In Proceedings of the IEEE Fifth Workshops on Enabling Technology:
Infrastructure for collaborative enterprise. IEEE, 1996.
21. L. Zhang, Gail-J. Ahn and B. Chu. A Rule-Based Framework for Role-Based Delegation and Revocation. ACM Transactions on Information and System Security,
Vol.6, No.3, August 2003.

Access Control Model for Sharing Composite
Electronic Health Records
Jing Jin1 , Gail-Joon Ahn2 , Michael J. Covington3 , and Xinwen Zhang4
1

University of North Carolina at Charlotte,
jjin@uncc.edu
2
Arizona State University,
Gail-Joon.Ahn@asu.edu
3
Intel Corporation,
michael.j.covington@intel.com
4
Samsung Information Systems America,
xinwen.z@samsung.com

Abstract. The adoption of electronically formatted medical records, so
called Electronic Health Records (EHRs), has become extremely important in healthcare systems to enable the exchange of medical information
among stakeholders. An EHR generally consists of data with diﬀerent
types and sensitivity degrees which must be selectively shared based on
the need-to-know principle. Security mechanisms are required to guarantee that only authorized users have access to speciﬁc portions of such
critical record for legitimate purposes. In this paper, we propose a novel
approach for modelling access control scheme for composite EHRs. Our
model formulates the semantics and structural composition of an EHR
document, from which we introduce a notion of authorized zones of the
composite EHR at diﬀerent granularity levels, taking into consideration
of several important criteria such as data types, intended purposes and
information sensitivities.

1

Introduction

Healthcare is an increasingly collaborative domain involving a wide range of individuals and organizations. Seamless electronic communication infrastructure
that allows patients, physicians, hospitals, public health agencies and other authorized users to share clinical information in real-time, under stringent security
and privacy protections, has become extremely important to improve the quality
of healthcare while simultaneously reducing costs and administrative complexity [1]. In particular, the adoption of electronically formatted medical records, so
called Electronic Health Records (EHRs), has become the primary concern for
a broad range of health information technology applications and practitioners.
Critical concerns about the privacy and security of personal medical information remain high in healthcare information sharing systems. More than ever,
there is a strong need to deﬁne access control models that conform to legal principles and regulations, while limiting access to information to those entities on
E. Bertino and J.B.D. Joshi (Eds.): CollaborateCom 2008, LNICST 10, pp. 340–354, 2009.
c ICST Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering 2009


Access Control Model for Sharing Composite EHRs

341

need-to-know basis. However, an EHR includes complex health information such
as the patient demographics, medical histories, examination reports, laboratory
test results, radiology images (X-rays, CTs), and so on. Supporting the authorized and selective sharing of EHRs among several parties with diﬀerent duties
and objectives is indeed a great challenge.
1.1

A Motivation Scenario

In order to better illustrate access control challenges on sharing of EHRs, we
consider a typical clinical EHR document, and we demonstrate our proposed
approach using the same document throughout the rest of this paper.
Suppose Good Health Clinic is a member of a particular Regional Health Information Organization (RHIO) [2], where health information can be exchanged
through an established infrastructure with other involved organizations. Figure 1
illustrates a sample Consultation Note in the clinic for a patient named Henry
Levin [3]. The consultation note includes Henry’s past medical history, medications, physical examination, labs, etc. The medical information is recorded in
various data types such as texts, numbers and images. Some ﬁelds inside the
document may refer to other external clinical documents. For example, Henry’s
HIV/AIDS disease history may be maintained in another folder of the patient,
and Henry’s current medications may be directly linked to the records operated
by his pharmacist. Given the complexity of this EHR document, the information contained in the consultation note should be legitimately exchanged to
satisfy needs of diﬀerent parties in RHIO. In particular, the lab orders need to
be communicated with appropriate laboratories and speciﬁc test codes are used
to trigger the billing process. The doctor’s prescriptions, on the other hand, are
necessary to be ﬁlled by the pharmacist, and proper referrals are exchanged with
specialists for complex medical problems. However, ensuring the patient’s privacy
and data security is still vital for the EHR exchange system. The need-to-know

Good Health Clinic Consultation Note
Consultant: Robert Dolin, MD
Date: 04/07/2000

Patient: Henry Levin Sex: Male PID: 12345
DOB: 09/24/1971

Good Health Clinic
HIV CDA
•HIV History
•HIV ROS

History of Present Illness
Henry Levin is …
Past Medical History

Medications

•Asthma
•HIV

Allergies
•Penicillin - Hives
•Aspirin - Wheezing
Labs

Physical Exam
Date

4/7/2000 14:30

4/7/2000 15:30

Weight

194.0lbs

Pulse

86 /min

84/min

…

…

…

Assessment
•Asthma, with prior smoking history…
•….

•CXR 02/03/1999:
--Order:
-- Result:
Hyperinflated…
•CD4 07/03/1998.
Instructions
•Complete PFTs with lung ….
•Medication
•Plan

Fig. 1. Motivation EHR Document

…

XXX Pharmacy
Medication CDA
•Theodur 200mg BID
•…

AAA Laboratory
CD4 Count Test
•Order: …
•Result: …

342

J. Jin et al.

principle must be strictly enforced for each responsible party to obtain only the
necessary information to carry out its task. For instance, only the test codes and
patient’s insurance information are necessary for a billing clerk to fulﬁll her responsibility. The document also has sensitive ﬁelds, such as Henry’s HIV/AIDS
medical history, which may be hidden from general medical information sharing
unless a speciﬁc treatment purpose is indicated.
The example clinical document has demonstrated several unique characteristics of an EHR, including the composition of various data types, connections
among diﬀerent pieces of information from multiple sources, and navigational
aspects of the information linkage and exchange. We thus refer the EHRs with
such features as composite EHRs. In supporting partial sharing of a composite EHR, only a portion of the document needs to be shared with authorized
users. Without explicitly identifying the protection objects and their associations
within a composite EHR, the authorization speciﬁcation referring to speciﬁc protection parts is diﬃcult. In addition, these protection objects must be classiﬁed
with regard to diﬀerent purposes, data types, and sensitivity levels to guide the
selection of speciﬁc parts with various protection granularity levels within the
document. Finally, as an EHR document may link to other EHRs, the navigation paradigm would aﬀect the authorization model, while the navigational links
serve as a visual representation of associations between the EHR documents and
need to be protected in a secure manner.
In this paper we propose a novel approach for modeling access control in
composite EHRs. Our model ﬁrst introduces a level of abstraction to formulate
the logical structure of a composite EHR in terms of its internal protection
objects and relationships among them. The protection objects are categorized
by three dimensional properties – sensitivity, intended purpose and object type
– to facilitate the authorization model and accommodate the composition and
selective sharing requirements. By manipulating the selection criteria of these
properties, various authorized zones including diﬀerent protection objects can
be dynamically collected to share with recipients.
The rest of the paper is organized as follows. In Section 2, we provide a
brief overview of the emerging EHR standards. We also review existing security
solutions for EHR systems and access control models related to conventional
structured or semi-structured data. In Section 3, we present the logical composite
EHR model. The proposed authorization model and speciﬁcation are discussed
in Section 4. We conclude the paper in Section 5 with future research directions.

2

Related Work

Related EHR Standards. There are several standards currently under development to specify EHRs, such as openEHR [4] and Health Level 7 (HL7)
Clinical Document Architecture (CDA) [5,3]. These standards aim to structure
and markup the clinical content of an EHR for the purpose of exchange. The
most important concept introduced in openEHR is the archetype, which is used
to model healthcare concepts such as blood pressure and lab results. These

Access Control Model for Sharing Composite EHRs

343

archetypes serve as fundamental building blocks to form various clinical EHR
documents. Meanwhile, these archetypes and the contents contained in them
are exactly what need to be protected in the process of information exchange
across healthcare systems. Similarly, CDA deﬁnes the structure and semantics
of medical documents in terms of a set of coded components (called vocabulary) to model basic medical concepts. A common feature of all emerging EHR
standards is that the clinical concepts are modeled and expressed independently
from how the data is actually stored in underlying database. By implementing or
converting to the EHR standards, a “common language” is established between
diﬀerent medical information systems to communicate and share standardized
medical information with each other. Therefore, instead of being carried out at
the lower level in underlying database, authorization and selective sharing of
medical information should be deﬁned and enforced with common understanding of EHR standards. In our motivation example and the rest of the paper, we
assume the composite EHR document conforms to CDA standard format.
Access Control for EHR Systems. A number of solutions have been proposed to address the security and access control concerns associated with EHR
systems. In [6], the authors propose a set of authorization policies enforcing
role-based access control for the electronic transfer of prescriptions. In [7], the
paper demonstrates an implementation of EHR prototype system including a
basic network and role-based security infrastructure for the United Kingdom
National Health Service. In [8], the authors present a trust management and
role-based policy speciﬁcation language, called Cassandra, for expressing access
control policies in large-scale distributed systems. A case study discusses how
the language can be used to specify security policies for a UK national EHR
system. In [9], the paper presents a policy-based security management framework to enforce context based authorizations for federated healthcare databases.
Role-based access control [10], with its superior advantages in reducing administration complexity, has become the common theme applied in these approaches.
However, the EHR considered in these approaches is either a general abstract object or an isolated primitive object. None of these approaches took into account
of the composition feature of EHR documents, and thus cannot support a more
ﬁne-grained access control to selectively share composite EHRs as illustrated in
our motivational example.
Related Authorization Models for Structured Data. Sharing of composite EHRs requires clear understanding of the internal protection objects/clinical
concepts and their structural relationships. There has been a considerable amount
of work in regulating access to structured or semi-structured data.
The access control models proposed in [11] and [12] are especially tailored to
object-oriented databases storing conventional structured data, where information is represented in the form of objects. These models consider a rich semantic
structure of objects incorporating inheritance, aggregation, and composition associations. The relationship of objects in the database is modelled as a hierarchical structure so that the validity of an authorization rule written at some level
can be eﬃciently propagated to its descendants. Such features can be adopted

344

J. Jin et al.

in modelling the logical structure of composite EHRs. However, these models
have several shortcomings in providing eﬀective access control for information
exchange of EHRs. On the one hand, EHR documents are stored and exchanged
based on standards, which are deﬁned independently from underlying database
structures. The object relationships and navigational patterns deﬁned in standards may be totally diﬀerent from the ones enforced by access control mechanisms. On the other hand, as identiﬁed in our motivation scenario, the medical
information may be distributed at diﬀerent sites, from which a particular composite EHR document is derived. This unique feature cannot be addressed by a
localized object-oriented database.
XML has become the de facto mechanism for sharing data between disparate information systems. It is essentially adopted by HL7 to carry out its
standardization eﬀorts to describe, store and exchange health records. Regulating access to XML documents has attracted considerable attentions in recent
years [13,14,15]. All these work represent an XML document as a hierarchical
tree structure and its authorizations are propagated along with the association
links to achieve diﬀerent granularity levels. However, all these approaches deﬁne
access control rules for particular elements and attributes of an XML document.
The selection of a portion of the document requires a number of authorization
rules to be deﬁned and evaluated. This is obviously not eﬀective and eﬃcient in
practice to authorize and share a speciﬁc part of the document to fulﬁll the speciﬁc functional purpose of the requesting party. In addition, an XML document
itself is not semantically enough to represent a variety of data types as encountered in composite EHRs (e.g., image, audio and video). Thus the access control
mechanisms proposed for XML documents cannot meet the special requirements
for sharing of composite EHRs.

3

Logical Composite EHR Model

In order to enable the selective sharing of speciﬁc parts of a composite EHR, we
must allow the document to be logically divided into subcomponents so that ﬁnegrained authorization can be applied. Therefore, we consider the basic building
blocks of a composite EHR as the pieces of information or clinical concepts
that might be individually exchanged. A piece of information is represented as a
sub-object within a composite EHR, where each sub-object should be uniquely
identiﬁed. Sub-objects can be nested at any depth within the EHR and can link
to other sub-objects or even other EHRs. In our example, the blood pressure
can be modeled as a sub-object in the Good Health Clinic’s EHR document and
it is nested under the physical examination object category, while the patient’s
current medication is linked to another EHR document in the pharmacy. We
could further diﬀerentiate these two types of links between sub-objects and/or
composite EHR documents as inclusion link and navigation link, respectively.
The inclusion link realizes the typical “is a part of” relation, and the navigation
link represents the “reference to” relation between sub-objects within or across
composite EHR documents.

Access Control Model for Sharing Composite EHRs

345

To cope with the essential features of diﬀerent object types and their sensitivity levels within a composite EHR, we associate such information as properties
for each sub-object within the document. The properties can be categorized into
three dimensions: sensitivity, intended purpose and object type. The sensitivity property is designed to label a sub-object based on the sensitivity of the
content contained in it, which eventually can prevent sensitive medical information from being disclosed unintentionally. In the practice of Iowa HISPC [16],
the sensitivity classiﬁcations of medical data include general medical data, drug
and alcohol treatment, substance abuse treatment, mental health, communicable
disease (HIV, STDs, etc.), decedent, immunizations, and so on. Based on these
classiﬁcations, the sub-objects representing Henry’s HIV medical history and the
speciﬁc CD4 lab test should be marked with “communicable disease” property
(“HIV” for simplicity). The intended purpose property is necessary to address
privacy concerns to guide the exchange of data based on speciﬁc purpose(s)
and it is also essential to determine necessary pieces of information to fulﬁll the
need-to-know requirement of a speciﬁc job function. According to [17], business
practices for health information exchange can be organized by 11 purposes including payment, treatment, research, etc. These purposes could be achieved by
exchanging diﬀerent portions of a composite EHR document. The object type
property gives another dimension on sub-object selection and protection. The
sub-objects can be primitive types such as plain texts, dates and time, images
and reference links. They can also be a composite type in the hierarchical structure including other types of sub-objects. Considering the navigational pattern
within the document, the starting point of a navigation link should always be
associated with an object labelled with the type of reference link.
As a summary, a composite EHR is modeled in terms of the composition
of sub-objects and their relationships as links in a hierarchical structure. Each
sub-object is labelled with properties of sensitivity, intended purpose and object
type. These properties are used along with authorization policies to determine
whether a speciﬁc sub-object is allowed to be exchanged or not. This can be
formally deﬁned as follows.
Definition 1 (Composite EHR). A composite EHR is a tuple C
(vc , Vo , Eo , γEo , τVo ), where

=

–
–
–
–

vc is the root representing the whole composite resource object;
Vo is a set of sub-objects within the composite document under protection;
Eo ⊆ Vo × Vo is a set of edges between sub-objects;
γEo : Eo → {I, N } is an edge labelling function indicating whether an edge
is inclusion (I) or navigation (N ) type;
– τVo : Vo → P is a sub-object labelling function to specify the property of a
sub-object. P is a set of properties defined in Definition 2.
Definition 2 (Property). Let S, P U , and T be the sets of sensitivity classifications, intended purposes, and object types, respectively.
– Ps is a collection of sensitivity classification sets, {ps1 , . . . , psm }, where
psi = {s1 , . . . , sn } ⊆ S, i ∈ [1, m];

346

J. Jin et al.

– Pp is a collection of intended purpose sets, {pp1 , . . . , ppm }, where ppi =
{pu1 , . . . , pun } ⊆ P U , i ∈ [1, n];
– P = Ps × Pp × T is a set of three dimensional properties of sensitivity,
intended purpose and the type.
Given a property label p for a sub-object, we use the dot notation to refer to a
speciﬁc property dimension. For instance, p.ps refers to the sensitivity property;
p.pp refers to the intended purpose; and p.t refers to the object type. The function
τ (vi ) is used to retrieve the property label for a speciﬁc sub-object vi inside the
composition C.
According to Deﬁnition 1, a composite EHR can be represented as a labelled
hierarchical graph. The root of the tree graph indicates a particular composite EHR document. The nodes represent the sub-objects within the document
and speciﬁc properties are associated with each node for authorization and selection. Edges represent the inclusion or navigational relationships between the
nodes. Within the structure, nodes can be explicitly denoted by their identiﬁers, or can be implicitly addressed by means of path expressions. We apply
a simpliﬁed XPath [18] expression for the path representation 1 . Simpliﬁcation
comes from the fact that each node is uniformly treated without a type, whereas
XPath diﬀerentiates between “children” and “attributes” of an object due to the
diﬀerence between elements and attributes in XML. We do not make such an
explicit distinction because nodes in an EHR are the logical representation of
clinical concepts under protection. By using our model, the logical structure of
an EHR document in Figure 1 can be represented as a rooted tree as shown in
Figure 2(a). The links inside the tree are labelled with I and N indicating types
of inclusion and navigation, respectively. Figure 2(b) illustrates the example of
node labelling for the section of Labs in the document. We use paths to select
some of sub-objects in the graph as follows:
–
–
–
–

/ConsultationNote: the whole composite EHR document;
//Labs/CXR: this CXR lab test;
//Labs/CXR/*: the child nodes of CXR lab test;
//Labs/CXR//*: all the descendants of CXR lab test.

Another main design issue for the sub-object labelling scheme is the level of
granularity an object should be associated with. As sub-objects are managed in
a hierarchical structure of the composite EHR document, it enables us to provide
a ﬁne-grained labelling scheme yet achieves storage eﬃciency. In particular, we
could explicitly label sub-objects with properties at a certain granularity level
and allow the properties to be implicitly labelled through proper propagation
and aggregation along with the hierarchical links. As properties are categorized
in three dimensions, each has special characteristics for diﬀerent authorization
requirements. Therefore, the property propagation and aggregation for each dimension should be designed individually. We propose the following rules.
1

For brevity, we omit the formal deﬁnition of the path speciﬁcation here.

Access Control Model for Sharing Composite EHRs

347

ConsultationNote
I

I
I

Demographics

History
I

Present
Illness

I

I

I

Examination
I

Medical
History

I

Medication

I

Asthma

I

N

I

Medi
CDA

HIV

...

I

Physical
Exam

Allergies
I

I

I

Pulse
I

HIV
CDA

Order
I

Code

I

Instr

I

Referal

CD4

I

N

CD4
CDA
I

Note

I

Medication

I

Result
I

Note

CXR

N

Instructions

I

Labs

I

Weight

Assessment

I

X-ray
img

I

Order
I

Result
I

Code

Instr

(a) Composite EHR Document in a Hierarchical Structure
{general}

{payment,RHIO,treatment}
composite

ao=[/ConsultationNote//*, <{*}, {payment}, *>]

ConsultationNote

Exact
mode

{general}
{payment,RHIO,treatment} Examination
composite
I

I
{general}
{payment,RHIO,treatment}
composite

Code
Labs

{general}
I
{payment,RHIO,treatment}
composite
CXR
I

{general}
{payment,RHIO} Order
composite
I

Code

ConsultationNote

I

CD4

I

Instr

I

Note

{general} {general} {general}

{HIV}

Examination

{payment,RHIO,treatment}

ref

N

{general}
Result {treatment}
composite
I

Code

I

{HIV}
CD4
CDA {payment,RHIO,treatment}
composite
I

X-ray
img
{general}

{payment} {RHIO} {treatment} {treatment} I
code
text
text
image

I

Order
Result
{HIV}
{payment,RHIO}
{HIV}
composite {treatment}
I

text

Code

Instr

{HIV}

{HIV}

Labs

Subset
mode

CD4
CDA

Order
Code

Order
Code

{payment} {RHIO}
code
text

(b) Property Labeling and Propagation

CD4

CXR

(c) Property Match

Fig. 2. Composite EHR Document Structure

Rule 1. The property of sensitivity is automatically propagated downwards in
the hierarchy until a more sensitive label is explicitly speciﬁed and overridden. We denote this as Ps ↓.
Rule 2. The property of intended purpose is aggregated upwards in the hierarchy. We denote this as Pp ↑.

348

J. Jin et al.

Rule 3. The property of object type is aggregated upwards along with inclusion
and navigation links and labelled as “composite” and “ref”, respectively.
I
N
We denote these types as T ↑→“composite” and T ↑→“ref”.
In Rule 1, the structure represents an inheritance hierarchy, so that a property deﬁned at the parent can be automatically inherited by its children, and a
child may deﬁne new properties to override the ones inherited from its parent. In
our example, we assume the “general” label is the least-sensitive property, and
other labels such as “HIV” and “mental” are more sensitive ones. As shown in
Figure 2(b), the root of the clinical consultation note is labelled as “{general}”
and this label is implicitly propagated downwards to all sub-objects within the
structure. However, as CD4 is a special lab test related to HIV/AIDS disease,
the “HIV” sensitivity is explicitly speciﬁed to override the original “general”
label. It is then implicitly inherited by its children nodes (e.g., CD4 CDA) in
the hierarchy. In Rule 2, the hierarchical structure is treated as an aggregation
association, where the purposes served by children nodes are aggregated by their
parents. In our example, the code of the CXR lab test is used for “{payment}”
purpose and the instruction instr is used for “{RHIO}” purpose to be communicated with the laboratory. Therefore, their parent node, order of CXR lab
test, aggregates the purposes as “{payment, RHIO}”. In Rule 3, the hierarchical
structure reﬂects both the “is a part of” and “reference to” relations between
the sub-objects. The parent node associated with inclusion links actually forms
a type of “composite” to all its children nodes. And the parent node associated
with navigation links referring to all its children nodes through a type of “ref”.
In our example, the root and all internal nodes are labelled as “composite”,
while CD4 is labelled as “ref” since it is associated with a navigation link. In
Figure 2(b), the properties using bold and underlined font indicate the explicitly speciﬁed properties and the ones with regular font indicate the implicitly
assigned properties according to the rules.

4

Authorization Model

The fundamental question towards the selective sharing of a composite EHR
is what portion of a document can be exchanged with whom. The role of an
authorization model is then to articulate and specify policies to determine the
authorized zone of a source tree that a given subject is permitted to access 2 .
4.1

Authorization Subject

The role-based access control model has gained a lot of attention in healthcare
security research [19,6,7,8,9] because of its ability to provide practical security
administration for a large number of users. Users are authorized through their
roles (e.g., patient, physician, nurse) to access EHR documents within a healthcare infrastructure. In our approach, we also adopt a notion of role, considering
2

In this paper, we mainly focus on read-only permission in our authorization model.

Access Control Model for Sharing Composite EHRs

349

authorization subjects as roles directly. We assume a system-wide set of roles
(R) has been established within a healthcare system and each individual user
is a member of one or more roles. Access control policies are then speciﬁed as
what role is authorized to access which part of an EHR document.
4.2

Authorization Objects and Property Match

The ﬁne-grained authorization speciﬁcation should support a set of protection
objects with the broader coverage, ranging from a set of interrelated EHR documents to a speciﬁc portion of an EHR document. In our hierarchical composite
EHR model, XPath-like path expressions can be utilized to specify the scope
of the sub-objects to which an authorization policy applies. Meanwhile, properties provide the ﬂexibility to group sub-objects and to establish authorized
zones within a document scope for meeting various access control requirements.
Therefore, the selection of objects can be indirectly achieved by specifying authorized properties. These authorized properties serve as the ﬁltration criteria
to be compared with labels of the sub-objects. The matched sub-objects are
then selected to share with speciﬁc role(s). In specifying authorized properties,
we allow patterns to be used instead of enumerating each property. Patterns are
expressed by using the wildcard character. Two kinds of patterns are introduced:
pattern “*” is to indicate any property type(s) within a property dimension; and
pattern “{*}” is to specify any collection(s) of property sets within a property
dimension. For example, <{* },{payment },* > speciﬁes the object(s) that have
any collections of sensitivity levels, for payment purpose with any object type(s).
The notion of authorized property speciﬁcation is formally deﬁned as follows.
Definition 3 (Authorized Property Specification). An authorized property
is specified as a tuple prop =< ps, pp, pt >, where ps ∈ Ps or ps = {∗} is the
authorized sensitivity property; pp ∈ Pp or pp = {∗} is the authorized purpose
property; and pt ⊆ T or pt = ∗ is the authorized object type property.
As each sub-object is labelled with both explicitly speciﬁed properties and implicitly inherited or aggregated properties as the result of property propagation,
diﬀerent semantics must be identiﬁed to accommodate such features by incorporating the cascading options to guide the matching process. We further introduce
two matching modes as exact mode and subset mode. The exact mode can be
utilized to specify access control policies for certain sub-object(s) with speciﬁc
properties, while the subset mode can be speciﬁed to select a large collection of
sub-objects related to the speciﬁed properties, considering the property propagation and aggregation along the hierarchical links.
Definition 4 (Property Match). Suppose prop =< ps, pp, pt > is an authorized property specification and p = (ps , pp , t ) is the object property label,
– In exact match mode, two properties match if the following is true:
[(prop.ps = {∗})?true : (prop.ps = p .ps )]&&[(prop.pp = {∗})?true :
(prop.pp = p .pp )]&&[(prop.pt = ∗)?true : (p .t ∈ prop.pt)], that is, if patterns are not used, the sensitivity and intended purpose properties must be

350

J. Jin et al.

exactly equal in the authorized property and the object’s label, and the object type must be contained in the authorized types. Otherwise, any pattern
used in a property dimension returns a true for that property dimension.
– In subset match mode, the two properties match if the following is true:
[(prop.ps = {∗})?true : (prop.ps ⊇ p .ps )]&&[(prop.pp = {∗})?true :
(prop.pp ⊆ p .pp )]&&[(prop.t = ∗)?true : (p .t ∈ prop.pt)], that is, if patterns are not used, the sensitivity property of the object must be contained
in the authorized sensitivity property, the authorized purpose property must
be contained in the object’s purpose property, and the object type must be
equal.
We also deﬁne an authorization object that is used in an access control policy.
Definition 5 (Authorization Object Specification). Let scp expr be a scope
expression to denote a set of authorization objects, and prop be an authorized
property specification. An authorization object is specified as a tuple ao =
(scp expr, prop).
Given Deﬁnition 4 and the example in Figure 2(b), an authorization object
speciﬁed as
ao = [/ConsultationN ote//∗, < {∗}, {payment}, ∗ >]
means those sub-objects within the whole consultation note with any collections
of sensitivities, for payment purpose, and for any object type. In the exact match
mode, only the two Code objects are the matched ones, while in the subset
match mode, all the parent nodes upwards to the root are additionally included.
Figure 2(c) illustrates the property match example.
4.3

Information Sharing Privileges

Our model supports the read-only privilege which allows subjects to read the information in an EHR document and to navigate across EHR documents through
navigation links. As identiﬁed in our example, navigation links serve as the visual
representation of associations between EHR documents and such links should be
appropriately protected. In particular, special protection mechanisms can be applied to restrict users’ navigational behaviors by not revealing the existence of
a navigation link, or by revealing and allowing a subject to explore the objects
referenced by a navigation link. Therefore, two diﬀerent sharing privileges are
derived for the protection options, navi− and navi+ , respectively. By distinguishing these two protection options, it is possible to grant subjects the access
permission to a particular EHR document without disclosing links to other EHR
documents. For instance, by navi− privilege, a family physician may be aware
of Henry’s HIV/AIDS disease from his medical history documented in the consultation note. However, he cannot see the existence of the link to another EHR
document for the details of Henry’s HIV/AIDS treatment history since acquiring
such information requires navi+ being assigned. This feature provides another
spectrum for the selection of information across composite EHRs.

Access Control Model for Sharing Composite EHRs

4.4

351

Access Control Policy Specification

To summarize the above-mentioned approach, we introduce the deﬁnition of an
access control policy as follows.
Definition 6 (Access Control Policy). Let R be the system-wide set of roles in
a healthcare system. An access control policy is a tuple acp =< role, ao, match −
mode, priv >, where
–
–
–
–

role ∈ R is a specific role in the system;
ao is an authorization object;
match-mode ∈ {exact, subset} is the match mode for object properties;
priv ∈ {navi− , navi+ } is the sharing privilege for which the authorization is
granted.

The semantics of an access control policy is that, a certain role is only authorized
with certain priv to share the sub-objects whose property labels match the prop
using the speciﬁed match-mode. The followings are examples of access control
policies and relative authorization zones created against Figure 2(b).
P1: (“billing clerk”, [//Labs//*, <{*},{payment},“code”>], exact, navi+ );
P2: (“physician”, [//Labs//*, <{general}, {treatment}, *>], subset, navi− );
P3: (“lab technician”, [//Labs//*, <{general}, {RHIO}, *>], subset, navi− );
These policies select the same scope as the Labs category in the clinical consultation note. P1 states that the billing clerk can only access to the two Code
objects for both CXR and CD4 lab tests. With P2, the physician can access to
the CXR lab results, where the content of CD4 lab test is hidden because of its
sensitivity restriction. In P3, the lab technician can only access to the CXR lab
test order with detailed instructions.
With given access control policies, the target scope and corresponding authorization zones are generated as illustrated in Figure 3. The authorization zones
are created based on an algorithm as shown in Appendix. The algorithm takes
the composite EHR source tree and an access control policy as inputs, and returns the authorized zone including only the authorized portion of the source
tree for a given role. The algorithm ﬁrst retrieves the target subtree from the
source tree based on the scope speciﬁcation and the navigation privilege. Then
the properties of each object inside the subtree need to be matched against the
authorized property speciﬁcation in the access control policy, and unmatched
ones are pruned from the subtree. Taken the property propagation and aggregation into consideration, the algorithm traverses the target tree in pre-order
and post-order, respectively. Overall, the algorithm achieves a time complexity
of O(n) for traversing and pruning the target source tree.

5

Concluding Remarks

In this paper, we have presented an access control model for selectively sharing composite EHR documents. Essential features of the model are built with

352

J. Jin et al.
ConsultationNote

Demographics
Present
Illness

History
Medical
History

Examination

Medication

Allergies

Physical
Exam

Assessment

Labs

Note

Instructions

Medication

Referal

Zone 2

Asthma

HIV
HIV
CDA

Medi
CDA

...

Weight

Pulse

Order

CXR

CD4
CD4
CDA

Result

Zone 3

Code

Instr

Zone 1

Note

X-ray
img
Code

Order

Result

Instr
Target Subtree

Fig. 3. Authorization Zones

the logical abstraction of a composite EHR through a hierarchical structure,
where internal sub-objects are distinguished and organized through their interrelationships. The design of three dimensional properties for each sub-object
addresses the generic concerns for medical data sharing by enabling privacy protection and need-to-know principle for multiple data types, data relationships
and access modes. And the property-based authorization zone ﬁltration mechanism provides a ﬂexible yet eﬃcient means to select and authorize a collection
of sub-objects with speciﬁc property criteria.
Our future work seeks to develop a policy evaluation engine based on our
proposed model and standard EHR implementations. Experiments will be conducted on real healthcare systems to demonstrate the applicability and possible
extension of our work. Meanwhile, performance and storage eﬃciency need to
be measured and evaluated. Another issue concerns investigating more sophisticated authorization policies to deal with various access types in sharing composite EHRs. For example, a policy may allow a lab technician to directly submit
test results to a clinic’s EHR, while her access privileges on the medical record
should remain intact. Finally, an eﬀective policy propagation and enforcement
scheme is necessary to maintain the control power of its original domain after
an EHR is distributed and disseminated.

Acknowledgments
The work was partially supported by the grants from National Science Foundation (NSF-IIS-0242393) and Department of Energy Early Career Principal
Investigator Award (DE-FG02-03ER25565).

Access Control Model for Sharing Composite EHRs

353

References
1. IEEE-USA’s Medical Technology Policy Committee Interoperability Working
Group (ed.): Interoperability for the National Health Information Network (NHIN).
IEEE-USA EBOOKS (2006)
2. Bartschat, W., Burrington-Brown, J., Carey, S., Chen, J., Deming, S., Durkin, S.:
Surveying the RHIO landscape, a description of current rhio models, with a focus
on patient identiﬁcation. J. AHIMA 77(1), 64A–64D (2007)
3. Dolin, R.H., Alschuler, L., Boyer, S., Beebe, C., Behlen, F.M., Biron, P.V.: Hl7
clinical document architecture, release 2.0. ANSI Standard (2004)
4. openEHR Community: openEHR, http://www.openehr.org
5. HL7: Health level 7 (HL7), http://www.hl7.org
6. Chadwick, D.W., Mundy, D.: Policy based electronic transmission of prescriptions.
In: Proceedings of the 4th International Workshop on Policyies for Distributed
Systems and Networks (POLICY 2003), pp. 197–206 (2003)
7. Eyers, D.M., Bacon, J., Moody, K.: OASIS role-based access control for electronic
health records. In: IEE Proceedings – Software, pp. 16–23 (2006)
8. Becker, M.Y., Sewell, P.: Cassandra: ﬂexible trust management, applied to electronic health records. In: Proceedings of IEEE 17th Computer Security Foundations Workshop, pp. 139–154 (2004)
9. Bhatti, R., Moidu, K., Ghafoor, A.: Policy-based security management for federated healthcare databases (or RHIOs). In: Proceedings of the international workshop on Healthcare Information and Knowledge Management, pp. 41–48 (2006)
10. Ferraiolo, D., Sandhu, R., Gavrila, S., Kuhn, R.: Proposed NIST standard for
role-based access control. ACM Transactions on Information and System Security
(TISSEC) 4, 224–274 (2001)
11. Fernández, E.B., Gudes, E., Song, H.: A model for evaluation and administration of
security in object-oriented databases. IEEE Trans. Knowl. Data Eng. 6(2) (1994)
12. Rabitti, F., Bertino, E., Kim, W., Woelk, D.: A model of authorization for
next-generation database systems. ACM Transactions on Database Systems
(TODS) 16(1), 88–131 (1991)
13. Bertino, E., Castano, S., Ferrari, E., Mesiti, M.: Specifying and enforcing access
control policies for xml document sources. World Wide Web Journal 3(3), 139–151
(2000)
14. Damiani, E., di Vimercati, S.D.C., Paraboschi, S., Samarati, P.: A ﬁne-grained
access control system for XML documents. ACM Transactions on Information and
System Security (TISSEC) 5(5), 169–202 (2002)
15. Gabillon, A., Bruno, E.: Regulating access to XML documents. In: Proceedings of
the 15th Annual Working Conference on Database and Application Security (2001)
16. Iowa Foundation for Medical Care: HISPC state implementation project summary
and impact analysis report for the state of Iowa (2007),
http://www.ifmc.org/news/State%20Impact%20Report_11-27-07.doc
17. Dimitropoulos, L.L.: Privacy and security solutions for interoperable health information exchange: Interim assessment of variation executive summary (2007),
http://www.rti.org/pubs/avas_execsumm.pdf
18. Clark, J., DeRose, S.: XML path language (XPath) version 1.0. World Wide Web
Consortium (W3C) (1999), http://www.w3.org/TR/xpath
19. Science Applications International Corporation (SAIC): Healthcare RBAC task
force charter, v1.1 (2003),
http://www.va.gov/RBAC/docs/HealthcareRBACTCharterv1_1.pdf

354

J. Jin et al.

Appendix

Algorithm Zone Control
Input: C = (vc, Vo, Eo,γEo, τVo )

/* C is the composite EHR source tree */
acp = <role, ao, match-mode, priv> /* acp is an access control policy */
Output: Z /* Z is the authorized zone for role including a list of nodes from the source tree*/

/* Step 1: Select the scoped subtree for evaluation */
1. LET scope = acp.ao.scp_expr /* retrieve the scope specification from the access control policy */
2. LET Z = select (C, scope, acp.priv)
/* retrieve the subtree Z from C based on the scope and privilege spec */
/* Step 2: Traverse the subtree and match authorized properties */
3. LET prop = acp.ao.prop
/* retrieve the authorized property specification from the access control policy */
4. LET N = vc
/* Step 2.1: Handle exact match mode */
5. IF match-mode = exact THEN /* handle exact match mode */
6. IF prop.ps ≠ {*} THEN /* match sensitivity property label */
7.
WHILE preorder(N).hasnext() DO /* traverse the subtree Z in postorder */
8.
LET ps’ = τ(N).ps
9.
IF ps’ ≠ prop.ps THEN
10.
remove N and all its descendant nodes from Z
/* prune unmatched nodes from the tree */
11. ELSE IF prop.pp ≠ {*} THEN /* match purpose of use property label */
12.
LET N = root of Z
13.
WHILE postorder(N).hasnext() DO /* traverse the remaining tree in postorder */
14.
LET pp’ = τ(N).pp
15.
IF pp’ ≠ prop.pp THEN
16.
remove N and all its ancestor nodes from Z
/* prune unmatched nodes from the tree */
17. ELSE IF prop.pt ≠ * THEN /* match object type property label */
18.
LET N = root of Z
19.
WHILE postorder(N).hasnext() DO /* traverse the remaining tree in postorder */
20.
LET pt’ = τ(N).t
21.
IF pt’ prop.pt THEN
22.
IF prop.pt contains “composite” THEN
23.
remove N from Z
/* prune unmatched nodes from the tree */
24.
ELSE remove N and all its ancestor nodes from Z /* prune unmatched nodes from the tree */
/* Step 2.2: Handle subset match mode */
25. IF match-mode=subset THEN /* handle subset match mode */
26. IF prop.ps ≠ {*} THEN /* match sensitivity property label */
27.
WHILE preorder(N).hasnext() DO /* traverse the subtree Z in postorder */
28.
LET ps’ = τ(N).ps
29.
IF ps’ prop.ps THEN
30.
remove N and all its descendant nodes from Z
/* prune unmatched nodes from the tree */
31. ELSE IF prop.pp ≠ {*} THEN /* match purpose of use property label */
32.
LET N = root of Z
33.
WHILE postorder(N).hasnext() DO /* traverse the remaining tree in postorder */
34.
LET pp’ = τ(N).pp
35.
IF pp’ prop.pp THEN
36.
remove N from Z
/* prune unmatched node from the tree */
37. ELSE IF prop.pt ≠ * THEN /* match object type property label */
38.
LET N = root of Z
39.
WHILE postorder(N).hasnext() DO /* traverse the remaining tree in postorder */
40.
LET pt’ = τ(N).t
41.
IF pt’ prop.pt THEN
42.
IF prop.pt contains “composite” THEN
43.
remove N from Z
/* prune unmatched nodes from the tree */
44.
ELSE remove N and all its ancestor nodes from Z /* prune unmatched nodes from the tree */
45. RETURN Z
Fig. 4. Zone Control Algorithm

Zhu Y, Hu HX, Ahn GJ et al. Provably secure role-based encryption with revocation mechanism. JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY 26(4): 697–710 July 2011. DOI 10.1007/s11390-011-1169-9

Provably Secure Role-Based Encryption with Revocation Mechanism
Yan Zhu1,2,∗ (朱 岩), Member, CCF, Hong-Xin Hu3 (胡宏新), Gail-Joon Ahn3 , Senior Member, ACM, IEEE
Huai-Xi Wang4 (王怀习), and Shan-Biao Wang4 (王善标)
1

Institute of Computer Science and Technology, Peking University, Beijing 100871, China

2

Beijing Key Laboratory of Internet Security Technology, Peking University, Beijing 100871, China

3

School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287, U.S.A.

4

School of Mathematical Sciences, Peking University, Beijing 100871, China

E-mail:{yan.zhu, hxwang, xbwang}@pku.edu.cn; {hxhu, gahn}@asu.edu
Received December 5, 2010; revised May 15, 2011.
Abstract Role-Based Encryption (RBE) realizes access control mechanisms over encrypted data according to the widely
adopted hierarchical RBAC model. In this paper, we present a practical RBE scheme with revocation mechanism based
on partial-order key hierarchy with respect to the public key infrastructure, in which each user is assigned with a unique
private-key to support user identification, and each role corresponds to a public group-key that is used to encrypt data.
Based on this key hierarchy structure, our RBE scheme allows a sender to directly specify a role for encrypting data, which
can be decrypted by all senior roles, as well as to revoke any subgroup of users and roles. We give a full proof of security of
our scheme against hierarchical collusion attacks. In contrast to the existing solutions for encrypted file systems, our scheme
not only supports dynamic joining and revoking users, but also has shorter ciphertexts and constant-size decryption keys.
Keywords

1

cryptography, role-based encryption, role hierarchy, key hierarchy, collusion security, revocation

Introduction

Role-based access control (RBAC), as a proven alternative to traditional access controls, including discretionary access control (DAC) and mandatory access
control (MAC), has been widely adopted in various
information systems over the past few years[1] . Even
though RBAC can tremendously help us minimize the
complexity in administering users, it is still inevitable
to adopt various cryptographic capabilities of managing resources in RBAC systems[2] , so as to protect the
resources that deviate from the access control system.
However, the existing cryptographic schemes based on
common asymmetric cryptosystem have several limitations to address above-mentioned features since those
schemes cannot accommodate access control features
of RBAC and lack scalability and interoperability for
enterprise application environments with a large number of users[3] .
Role-Based Cryptosystem, proposed by Zhu et al.[4] ,
is a key management system that realizes encryption, signature and authentication according to role

hierarchy (RH) in RBAC model. As an important
part of role-based cryptosystem, Role-Based Encryption (RBE) enables an access control mechanism over
encrypted data by hiding access permissions and assigned roles into private keys and ciphertexts. One of
the advantages of RBE is the ability to easily link up
with the existing RBAC models and systems. In an
RBE system, a resource owner specifies an access permission, which could be a security clearance (SC) requirement, to encrypt the resource directly by encryption algorithm (which can be run by anyone knowing
the universal public key issued by an authority). Each
user in this system possesses a unique private key associated with a role, stating the identity and privilege
level within the organization. Such a user can decrypt
a ciphertext if the user’s privilege level is equal to or
higher than the clearance requirement associated with
the ciphertext. Different from traditional cryptosystems, a key feature of RBE is that the encryption algorithm is not for individual user, but for the group of
authorized users.

Regular Paper
This work of Yan Zhu, Huai-Xi Wang and Shan-Biao Wang were partially supported by the National Development and Reform
Commission under Project “A Cloud-based service for monitoring security threats in mobile Internet” and “A monitoring platform for
web safe browsing”. This work of Gail-J. Ahn and Hong-Xin Hu were partially supported by the National Science Foundation of USA
under Grant Nos. NSF-IIS-0900970 and NSFCNS-0831360.
∗ Corresponding Author
©2011 Springer Science + Business Media, LLC & Science Press, China

698

J. Comput. Sci. & Technol., July 2011, Vol.26, No.4

Role hierarchy is a natural means of structuring roles
to reflect an organization’s lines of authority and responsibility. In RBAC model, role hierarchy defines an
inheritance relationship among roles, which is required
to be a partial order. As role hierarchy is the foundation
of RBAC, it is necessary to implement such a mechanism in the construction of RBE. It requires us not
only to hide the information of RH into public encryption keys and user’s keys, but also to find an efficient
cryptographical method to validate the partial ordering
relation among these keys. Moreover, it is necessary for
practical applications to ensure shorter length of user’s
key, as well as to support a large number of users.
1.1

Related Work

The research on cryptographic hierarchical structures has a long history since hierarchical structure is
a natural way to organize and manage a large number
of users. Several approaches on cryptographic partial
order relation supporting hierarchical structures have
been proposed. Akl and Taylor introduced a simple
scheme to solve multilevel security problem[5-6] . Since
then, several efficient methods have been studied. The
concept of logical key hierarchy (LKH) was proposed
by Wallner et al.[7] and Wong et al.[8] independently.
In this paradigm, common encryption key was organized into a tree structure to achieve secure group communication in a multicast environment. Several modifications have been proposed, such as complete subtree (CS)[9] , subset difference (SD)[10] , and layered subset difference (LSD)[11] . All of these schemes are constructed in symmetric-key setting, and the lengths of
ciphertext and user’s key are directly associated with
the number of users.
Public-key hierarchical cryptosystems have been recently proposed, e.g., Boneh and Franklin proposed the
first fully functional identity-based encryption scheme
(IBE)[12-13] in 2001, in which the public key can be
an arbitrary string such as an email address. In order to manage a large number of users, hierarchical
identity-based encryption (HIBE) mirrors an organizational hierarchy[14] . Another important area is hierarchy key management (HKM) that also organizes the key
into a hierarchy. For example, time-bound hierarchical
key assignment (THKA)[15] can assign time-dependent
encryption keys to a set of classes in a partially ordered hierarchy. This scheme is especially suitable for
the realtime broadcast system with time control. Since
all users with the same role share the same key, these
schemes are hard to realize certain useful security mechanisms, such as revocation, digital forensics, and traceability.
Since
Sahai
and
Waters[16]
introduced

attribute-based encryption (ABE) as a new means of
encrypted access control in 2005, ABE has received
much attention and many schemes have been proposed
recently, such as, key-policy ABE (KP-ABE)[17-19] ,
ciphertext-policy ABE (CP-ABE)[20-23] and dual policy
ABE (DP-ABE)[24-25] . ABE is based on the attributebased access control (ABAC)[26-27] or fine grained access control (FGAC)[28-29] model. In an ABE system,
ciphertexts are not necessarily encrypted to one particular user by an assigned key as in traditional public
key cryptography. Instead, both users’ private keys and
ciphertexts will be associated with a set of attributes
or a policy over attributes. A user is able to decrypt a
ciphertext if there is a “match” between his private key
and the ciphertext. Although it takes time to widely
adopt ABAC or FGAC model in practical management
information systems (MIS), it is worth learning the
features of access policy representation in ABE. At the
same time, the research on attribute-based signature
has also made great progress and various attributebased signature schemes[30-32] have been proposed.
There are several recent studies of revocation mechanism of ABE. For example, Attrapadung and Imai[33]
presented a revocation approach for a subset of at most
t users based on Linear Secret Sharing Scheme (LSSS)
with random polynomial of degree t. This approach
also leads to a longer size of public-key, which is direct
proportional to the number of revoked users t. Thus,
it is not an effective solution to practical applications,
especially to supporting a large number of users.
To overcome the limitations of the exiting solutions, we presented a practical cryptographic RBAC
model in [4], called role-key hierarchy model, to support
various security features based on role-key hierarchy.
In that work, we introduced a role-based cryptosystem construction, which includes a role-based encryption (RBE) scheme and a role-based signature (RBS)
scheme. With the help of role-key hierarchy, this RBE
scheme can be tightly integrated into the hierarchical
RBAC systems. However, this RBE scheme could not
support the revocations of users and roles. Thus, efficient revocation mechanism should be introduced to
improve the existing RBE scheme.
1.2

Contributions

In this paper, we focus on the construction of a cryptosystem compatible with hierarchical RBAC model.
With the help of bilinear pairings, we present an enhanced Role-Based Encryption with revocation mechanism. Our new scheme provides more flexible control
than other schemes, as well as an efficient revocation
mechanism to support any number of users (or identities) and roles. Furthermore, our scheme has following

Yan Zhu et al.: Provably Secure Role-Based Encryption

699

Table 1. Comparison Between Role-Based Encryption and Attribute-Based Encryption
RBE (our work)
Access Model
Access Policy

RBAC
Partial order relation of role hierarchy based on RBAC

Policy Structure

Lattice (tree and inverse tree)

Tree

Public Key

System parameters, role hierarchy and user label set

System parameters and attribute set

Private Key

Constant size, specified by role

Variable length, specified by a set of attributes

Ciphertext

Variable length, proportional to authorized role and
revoked user set

Variable length, proportional to the number of leaf
nodes in the policy tree

Encryption

Policy is implemented by RBAC system

Policy is specified by the encryptor

Decryption

Matching between derived role and assigned role in
ciphertext and aggregate algorithm

Policy tree retracing and matching between the
user’s attribute and assigned attribute in ciphertext

Revocation

Dynamic user revocation

None

Main Techniques

Partial ordering and bilinear map

Secret sharing and bilinear map

new properties: key hierarchy can support arbitrary
partial-order structures and an unlimited number of
roles; a manager can dynamically add infinitely many
users without revising the existing ciphertexts and
user’s private keys; and encryption is collusion-secure
for arbitrarily large collusion of users. Moreover, our
construction also achieves an optimal bound of overhead rate for both ciphertexts and decryption keys.
Most importantly, our RBE scheme has better performance and scalability than existing solutions for encrypted file systems (EFS).
To explain the features of RBE, in Table 1 we show
the main differences between RBE and ABE in comparison with BSW’s CP-ABE[20] . For example, our RBE
scheme has a user’s private key with constant size rather
than variable size; the RBE encryption is automatically
performed by RBAC systems, which serve to reduce the
burdens of regulation on managements; there exists an
efficient revocation method for a subset of users; and so
on.
The rest of the paper is organized as follows. Section 2 overviews some basic notions and complexity assumptions. Section 3 articulates the definition of RBE
and security models. In Section 4, we address our
RBE construction. We evaluate the security and performance of our schemes in Sections 5 and Section 6,
respectively. Finally, we conclude this paper in Section
7.
2

CP-ABE (Bethencourt et al.’s work)[20]
ABAC[26-27] and FGAC[28-29]

Preliminaries

In this section, we present a brief description of partial order relation and role hierarchy in RBAC model.
Next, we briefly review the necessary facts about bilinear maps, as well as a class of assumptions used in our
RBE scheme.

2.1

Policy based on the set of attributes, which does not
involve partial order relation

Partial Order Relation and Role Hierarchy

Let Ψ = hP, ¹i be a (finite) partially ordered set
(Poset) with partial order relation ¹ on a (finite) set
P . A partial order is a reflexive, transitive and antisymmetric binary relation. We provide some terminology for partial order relation. Two distinct elements
x and y in Ψ are said to be comparable if x ¹ y or
y ¹ x. Otherwise, they are incomparable, denoted by
xky. An order relation ¹ on P gives rise to a relation
≺ of strict partial order: x ≺ y in P iff x ¹ y and
x 6= y. We define the predecessors and successors of
elements in Ψ = hP, ¹i as follows: for an element x
in P , ↑ x = {y ∈ P : x ¹ y} denotes the set of predecessors of x, ↓x = {y ∈ P : y ¹ x} denotes the set
of successors. Two posets are said to be isomorphic
if their “structures” are entirely identical. Formally,
posets Ψ = hP, ¹i and Φ = hQ, ¹i are isomorphic if
there exists a bijection f from P to Q such that x ¹ y
iff f (x) ¹ f (y).
In hierarchical RBAC model, inheritance is reflexive
because a role inherits its own permissions, and transitivity is a natural requirement in this context. Also,
anti-symmetry rules out roles that inherit from one another and would therefore be redundant. For example,
we can represent Ψ = hP, ¹i by using circles (indicating
the elements of P ) and inter-connecting lines (indicating the covering relations). For example, Fig.1(a) shows
the diagrams for some simple ordered sets. Sub-figures
(a1), (a2) and (a3) are linear, tree, and inverted-tree
structure, respectively. Sub-figure (a4) is a three-layer
tree structure, in which the senior ra (top) inherits
the permissions from all other juniors (bottom). Especially, there exist two different paths {rf , rb , ra } and
{rf , rc , ra } from rf to ra . Sub-figure (a5) is a threelayer hybrid structure, which is composed of various

700

J. Comput. Sci. & Technol., July 2011, Vol.26, No.4

Fig.1. Diagrams for partial-order sets (a) and instance of role hierarchy (b).

different structures. Note that ra and râ are not related
through hierarchy.
Generally, a hierarchy in RBAC is mathematically a
partial order that defines an inheritance (or seniority)
relation between roles, whereby senior roles acquire the
permissions from their juniors. An example of role hierarchy is shown in Fig.1(b), in which more powerful (senior) roles are shown toward the top of the diagram and
less powerful (junior) roles toward the bottom. Based
on this, we present the definition of role hierarchy as
follows.
Definition 1 (Role Hierarchy). Given a set of users
U , a role hierarchy H is a triple hU, R, ¹i, if there exists a (finite) partially ordered set hR, ¹i, such that each
user belongs to and only belongs to a role, i.e., for all
ui,j ∈ U , there exists an ri ∈ R, such that ui,j ∈ ri .
2.2

Bilinear Maps and Some Assumptions

Let G1 and G2 be two additive groups and GT be
a multiplicative group with large prime order p.① A
computable bilinear map is a function e : G1 ×G2 → GT
with the following properties: for any G ∈ G1 , H ∈ G2
and all a, b ∈ Zp , we have 1) bilinearity: e([a]G, [b]H) =
e(G, H)ab ; 2) non-degeneracy: e(G, H) 6= 1 unless G
or H = 1; 3) computability: e(G, H) is efficiently
computable. A bilinear map group system is a tuple S = hp, G1 , G2 , GT , ei composed of the objects described above.




[F1 (x1 , . . . , xm )]G,
H(x1 , . . . , xm ) =  [F2 (x1 , . . . , xm )]H,  ∈
e(G, H)F3 (x1 ,...,xm )
Gs1 × Gs2 × GsT ,
and T ∈ GT , decide whether T = e(G, H)h(x1 ,...,xm ) ,
where h ∈ Zp [X1 , . . . , Xm ].
We refer to Theorem A.2 as a proof that GDDHE
has generic security when h 6∈ (F1 , F2 , F3 ) in [34]. In
Lemma 1, we restate the generic security in a more
concrete form for (n, t)-GDDHE1 problem[34-35] .
Definition 3 ((n, t)-GDDHE1 Problem). Let f (x)
and g(x) be two known random polynomials of degree t
and n − t with pairwise distinct roots respectively,

t
t
Y
X


 f (x) =
(ζi x + xi ) =
ai · xi


i=1
i=0
mod p
n−t
n−t

Y
X


0
i
 g(x) =
(ζt+i x + xi ) =
bi · x

i=1

Qt

i=0

Qn−t

Security of our system is based on a complexity assumption called the General Decisional bilinear DiffieHellman Exponent (GDDHE) assumption. We define
the GDDHE problem as follows.

where i=1 ζi = 1 and i=1 ζt+i = 1 mod p. Let
h(x, y) = yf (x)g(x) be a two-variable polynomial, and
S = (p, G1 , G2 , GT , e(·, ·)) be a group system. Given the
values in (F1 , F2 , F3 , T )-GDDHE problem with
µ
¶

G, [γ]G, · · · , [γ t−1 ]G,


F
(γ,
ς)
=
,
1


[γ · f (γ)]G, [ς · γ · f (γ)]G




µ
¶
H, [γ]H, · · · , [γ n ]H,
(1)
F
(γ,
ς)
=
,

2


[ς
·
g(γ)]H





2
F3 (γ, ς) = e(G, H)f (γ)g(γ) ,

Definition 2 (GDDHE Problem). Let F1 , F2 , F3 ∈
Zp [X1 , . . . , Xm ]s be three s-tuples of m-variate polynomials over Zp , where s, m ∈ Z+ . Given a vector

and T ∈ GT , decide whether e(G, H)ς·f (γ)·g(γ) = T ,
where γ, ς, ζi , xi , x0i ∈ Z∗p are two secret random variables and G, H are generators of G1 , G2 , respectively.

① We require that no efficient isomorphism G2 → G1 or G1 → G2 is known, or G2 → G1 is known but its inverse G1 → G2 is
unknown.

Yan Zhu et al.: Provably Secure Role-Based Encryption

3

Definitions

In this section, we begin by formally defining what
is a role-based encryption system. We then state the
security requirements and adversary’s attack models
needed for our proof of security. For the sake of clarity,
we list some notations used throughout this paper in
Table 2.
Table 2. Description of the Notations
No.

Notation

Description

1
2

κ
U, R

3

H, R

4

par , mk

5

ri , ui,j

6
7
8

pk i
dk i,j
M, Ci

Security parameter
U and R denote the set of users and
roles, respectively
H denotes the hierarchy hU, R, ¹i and
R denotes the set of revoked users
par and mk denote the public parameter and the master key, respectively
ri denotes the i-th role in R and ui,j
denotes the j-th user with role ri
Public role key for ri ∈ R
Decryption key of user ui,j
M and Ci denote the plaintext and the
ciphertext encrypted by pk i , respectively

3.1

Public-Key Role-Based Encryption

Given a role hierarchy H = hU, R, ¹i, a publickey Role-Based Encryption (RBE) is specified by five
polynomial-time algorithms hS, G, A, E, Di.
1) Setup(κ, H) → (mk , par ). Take a security parameter κ and a role hierarchy H as input. It produces a
manager key mk and a public parameter par.
2) GenRKey(par , ri ) → pk i . Take the parameter par
and a role index ri . It generates a public encryption key
pk i of ri .
3) AddUser(mk , ID, ri , ui,j ) → (lab i,j , dk i,j ). Take a
user identity ID, a user index ui,j in the role ri , and
the manager key mk. It outputs a user’s secret key,
which involves Sa private key dk i,j , a user label lab i,j ,
and par = par {lab i,j }.
4) Encrypt(R, pk i , M ) → Ci . Take as input the encryption key pk i , a message M and a set of revoked
users R ⊆ U . It returns a ciphertext Ci .
5) Decrypt(R, dk i,j , Ci ) → M . Take as input a ciphertext Ci , a subset R ⊆ H and a user decryption key
dk i,j . It returns a message M .
We need to realize the user revocation by the user label lab i,j , called identity-based revocation (IBR). With
the help of this revocation mechanism, some users
{ui,j } ∈ R can be revoked temporarily from the authorized users in ciphertexts. This paper does not highlight
the revocation of roles since we can realize this mechanism by using Control Domain addressed in Section 4.

701

Given an instance of RBE scheme E under a certain H, we define a key hierarchy K = {UK , RK , ¹}
from (E, H), where UK = {dk i,j }∀dk i,j ←A(·) , RK =
{pk i }∀pk i ←G(·) , and pk i ¹ pk j iff there exists a
polynomial-time algorithm F (H, pk i , rj ) = pk j . We
call F the derivation (or delegation[36] ) function of E,
which is used to realize the partial order relation in a
set of public encryption keys.
Note that, we require that a user belongs to a single
role rather than to multiple roles in this definition due
to the construction limitations in cryptography. Although this requirement is not true in general RBAC
model, it is necessary to require strict role-based authentication mechanisms to provide strong security. If
necessary the manager can assign multiple secret keys
to different roles, but they have the same label. In addition, in practice the RBAC model can automatically
employ the user’s current role to invoke the Encrypt
algorithm.
3.2

Security Notions and Adversary’s Attack
Models

The security requirements of RBE system, made up
of three properties, are defined as follows.
Definition 4 (RBE). Given a role hierarchy H =
hU, R, ¹i, an RBE scheme E = hS, G, A, E, Di (|R| =
m, |U | = n) satisfies the following conditions.
1) Consistency: The representations are equivalent
between the role hierarchy H and the reduced key hierarchy K, that is, {pki ¹ pkj }K ∼ {ri ¹ rj }H , where ∼
denotes isomorphism.
2) Viability: For every set of revoked users R, M ∈
M, and Ci = E(R, pki , M ),
·
Pr

¸
D(R, dk j,l , Ci ) = M :
= 1.
∀uj,l ∈ rj , ri ¹ rj ∧ uj,l 6∈ R

3) Security: For any probabilistic polynomial-time algorithm D0 , every polynomial p(·), all sufficiently large
k ∈ N, every R, M ∈ M, and Ci = E(R, pk i , M ),
·

¸
1
D0 (R, dk j,l , Ci ) = M :
Pr
<
.
∀uj,l ∈ rj , ri 6¹ rj ∨ uj,l ∈ R
p(k)
The principal attack on RBE system is the collusion attack between different users, that is, A corrupts
some uj,l ∈ R to decrypt Ci , even if uj,l ∈ rj and
ri ¹ rj . Hence, we define a semantic security under
Chosen Plaintext Attack against Hierarchical Collusion
(denoted by IND-hcCPA). Security is defined using the
following game between an attack algorithm A and a
challenger B. This game is defined as follows.
1) Initial. B constructs an arbitrary H (|R| = m),

702

J. Comput. Sci. & Technol., July 2011, Vol.26, No.4

and then runs Setup algorithm and gives A the resulting parameters par and H, keeping mk secret.
2) Learning. A adaptively issues n queries q1 , . . . , qn
to add the users and gets a set of collusion users R
(|R| = t) as follows.
(a) Public Label Query (ui,j 6∈ R): following
AddUser (mk , ui,j ), B generates a user label lab i,j and
sends it to A;
(b) Private Key Query (ui,j ∈ R): following
AddUser (mk , ui,j ), B generates a revoked user and returns this user’s lab i,j and dk i,j to A.
3) Challenge. A chooses two equal length plaintexts
M0 , M1 ∈ M and appoints a role ri on which it wishes
to be challenged. B picks a random bit b ∈ {0, 1} and
sends the challenge ciphertext E(R, pk i , Mb ) to A.
4) Guess. A outputs a guess b0 ∈ {0, 1} for b, and
wins if b = b0 .
The above game models an attack where all users,
who are not in the set R, collude to try and expose a
ciphertext intended for users in U \ R only. The set R
is chosen by the adversary. In this game, we define the
advantage of the adversary A in attacking the scheme
as
0
0
Adv ind
E,A (m, n, t) = | Pr[b = b] − Pr[b 6= b]|

= |2 Pr[b0 = b] − 1|,
where |R| = t, |R| = m, |U | = n, and the probability is
taken over the random coins of A and all probabilistic
algorithms in the scheme.
Definition 5 (Secure Role-Based Encryption). An
RBE scheme E is said to be an (m, n, t)-secure rolebased encryption if for any polynomial-time adversary
A, the total number of roles m, the total number of
users n, and at most t colluders, any computational advantage of adversary Adv ind
E,A (m, n, t) is negligible in the
above IND-hcCPA game. The scheme E is said to be semantically secure against full collusion if it is (m, n, n)secure.
4

Our Construction

In this section, we describe a public-key RBE scheme
with role hierarchy, which has new features including
O(m)-size ciphertexts and encryption key, as well as
O(1)-size decryption key for the number of roles m.
This construction also supports the revocation of any
number of users.
4.1

Role-Based Encryption Scheme

Let H = {U, R, ¹} be a role-key hierarchy. Without
loss of generality, we assume that the total number of
roles is m in H, i.e., R = {r1 , r2 , . . . , rm }. We construct
an RBE scheme as follows.

1) Setup(κ, Ψ ). Let S = (p, G1 , G2 , GT , e) be a bilinear map group system with randomly selected generators G ∈ G1 and H ∈ G2 respectively, where G1 , G2
are two bilinear groups of prime order p, |p| = O(κ).
This algorithm first picks a random integer τi ∈ Z∗p for
each ri in role-key hierarchy graph. We define
Di = [τi ]G ∈ G1 ,

∀ri ∈ R,

V = e(G, H) ∈ GT ,
where τi is the secret of each role ri and Di is called the
identity of this role. Furthermore, it defines D0 = [τ0 ]G
by using a random τ0 ∈ Z∗p . Thus, the public parameter is par = hH, V, D0 , D1 , . . . , Dm i and we keep
mk = hG, τ0 , τ1 , . . . , τm i secret.
2) GenRKey(par , ri ). This is an assignment algorithm for role key from the public parameter par. For
a role ri , the role key pk i can be computed as
pk i = hH, V, Wi , {Dk }∀rk ∈↑ri i,
X
Wi = D0 +
Dk ∈ G1 ,
ri 6¹rk

where {Dk }∀rk ∈↑ri is the P
identity set of all roles in ↑ ri .
It is clear that Wi = P
[τ0 + ri 6¹rk τk ]G. For sake of simplicity, let ζi = τ0 + ri 6¹rk τk , so we have Wi = [ζi ]G.
3) AddUser(mk , ID, ri , ui,j ). Given the manager key
mk = hG, {τi }m
i=0 i and a user index ui,j in the role ri ,
the manager generates a unique decryption key by randomly selecting a fresh xi,j = Hash(ID, ui,j ) ∈ Z∗p and
defining a public user label lab i,j = hxi,j , Vi,j , Bi,j i and
a decryption key dk i,j = Ai,j , where
x0i,j = xi,j −
h
Ai,j =
h
Bi,j =
Vi,j = V

X

τk ∈ Z∗p ,

ri 6¹rk
0
xi,j i
G
ζi + x0i,j

∈ G1 ,

i
1
H ∈ G2 ,
0
ζi + xi,j
1
ζi +x0i,j

∈ GT .

Note that, the total number of users is unlimited in
each role.
4) Encrypt(R, pk i , M ). To encrypt the message
M ∈ GT , given any pk i = hH, V, Wi , {Dk }rk ∈↑ri i and
a set of revoked users R = {ui1 ,j1 , . . . , uit ,jt }, the algorithm randomly picks ξ ∈ Z∗p and then computes

C1



C
2

C3



C4

= [ξ]Wi ∈ G1 ,
= [ξ]BR ∈ G2 ,
= M · (VR )ξ ∈ GT ,
= {[ξ]Dk }∀rk ∈↑ri ∈ Gm̄
1 ,

(2)

Yan Zhu et al.: Provably Secure Role-Based Encryption

where, m̄ is the number of elements in C4 , |R| = t, and

if R = ∅,
 H,
h
i
1
BR =
H, if R 6= ∅,
 Qt
0
l=1 (ζil + xil ,jl )

if R = ∅,
 V,
1
VR =
Qt

)
(ζ +x0
V l=1 il il ,jl , if R 6= ∅.
BR and VR can be efficiently computed by the aggregate algorithms from {Bil ,jl }uil ,jl ∈R and {Vil ,jl }uil ,jl ∈R
(see Subsection 5.2.1). Finally, it outputs the ciphertext Ci = hC1 , C2 , C3 , C4 , Ri.
5) Decrypt(dk j,k , Ci ). Given a ciphertext Ci from the
role ri , the user uj,k ∈ rj can utilize the following equation to recover M from Ci with private key dk j,k = Aj,k
when ri ¹ rj and uj,k 6∈ R:
³
´
X
R
V 0 = e C1 +
Dl0 , Bj,k
· e(Aj,k , C2 ), (3)
rl ∈Γ (rj ,ri )

where Γ (rj , ri ) denotes ∪ri ¹rl ,rj 6¹rl {rl }, Dl0 = [ξ]Dl ∈
C4 for all rl ∈ Γ (rj , ri ), and

Bj,k ,
if R = ∅,



i
h
1
R
H,
Q
Bj,k =
t
(ζil + x0il ,jl ) · (ζj + x0j,k )

l=1



if R 6= ∅,
from {Bil ,jl }uil ,jl ∈R and Bj,k . Finally, it outputs the
plaintext M = C3 /V 0 .
The derivation function of public keys, F (pk i , rl ) =
pk l for any ri ¹ rl , can be defined as F (pk i , rl ) =
hH, V, Wl , {Dk }∀rk ∈↑rl i = pkl , where Wl = Wi +
P
rk ∈Γ (rl ,ri ) Dk , due to rk ∈ Γ (rl , ri ) ⊆ (↑ ri ) when
ri ¹ rl . Note that, the part C4 of Ci is called Control
Domain of this ciphertext in (2). We can deal with access control constraints for roles by choosing the appropriate rk ∈↑ ri to insert into C4 . Furthermore, access
control constraints for users can be effectively carried
out by using the set of revoked users R. The number
of revoked users is unlimited in this scheme. Hence, we
can revoke any subgroup of roles and users in terms of
these two mechanisms.
5
5.1

Security Analysis
Analysis of Consistency

Since Di is chosen at random, we need to consider
the collision probability among the role keys {pk i }ri ∈R ,
i.e., Wi = Wj for i 6= j, Wi ∈ pki , and Wj ∈ pkj . The
following theorem tells us that this collision probability
is negligible if the security parameter κ is large enough.
Theorem 1. The collision probability among m in2
.
tegers chosen from Z∗p at random is less than (m+1)
4p

703

Proof. Firstly, the collision probability between λ
random integers {ai }λi=1 and µ random integers {bi }µi=1 ,
Pλ
Pµ
1
∗
i=1 ai =
j=1 bj , is p , where a1 , . . . , aλ ∈ Zp and
∗
b1 , . . . , bµ ∈ Zp . Secondly, the number of all possible unordered pairs {λ, µ} with λ + µ = k is b k2 c
for 1 6 λ, µ < m. Thus the number of all possible unordered pairs {λ, µ} with 3 6 λ + µ 6 m is
2
Pm k
Pm k
m(m+1)
< (m+1)
. Hence, with
k=3 b 2 c <
k=1 2 =
4
4
the help of Bernoulli’s inequality, the collision proba(m+1)2

2

2

(m+1)
1
bility is 1 − (1 − p1 ) 4
. Note
6 (m+1)
4
p =
4p
that we do not assume that ai and bj are different. ¤
Since the total number of roles is far less than the
size of space of keys, this theorem means that the collision probability is negligible for m ¿ p, e.g., given
m = 1000 and |p| = 2 × κ = 160 (κ = 80-bits), the
20
collision probability is less than 22162 = 2−142 . This implies that different roles almost always have different
keys. So we will neglect the collision probability hereinafter.
Theorem 2.
Under the above assignment,
∪rj 6¹rk {τk } ⊂ ∪ri 6¹rk {τk } if and only if rj ≺ ri , that
is, the consistency in Definition 4 holds.
Proof. Firstly if rj ≺ ri , then we have ri ∈
∪rj ¹rk {rk }, which implies that ∪ri ¹rk {rk } ⊂ ∪rj ¹rk
{rk }. So we have that ∪rj 6¹rk {rk } ⊂ ∪ri 6¹rk {rk }.
In terms of the corresponding relation between ri
and τi , we have ∪rj 6¹rk {τk } ⊂ ∪ri 6¹rk {τk }. Conversely, if ∪rj 6¹rk {τk } ⊂ ∪ri 6¹rk {τk }, then we know
∪rj 6¹rk {rk } ⊂ ∪ri 6¹rk {rk }.
This relation implies
∪ri ¹rk {rk } ⊂ ∪rj ¹rk {rk }. Since ri ∈ ∪ri ¹rk {rk }, we
have rj ≺ ri . Hence, the theorem holds.
¤
Given
a
pk
=
hH,
V,
W
=
[τ
]G
+
i
0
i
P
[ ri 6¹rk τk ]G, {Dk }∀rk ∈↑ri i and a polynomial-time
derivation function F (H, pki , rj ) = pkj , the relation
pk i ¹ pkj can be efficiently generated if and only if
ri ¹ rj in terms of Theorem 2. This gives the consistency between K and H, that is, {pki ¹ pkj }K ∼ {ri ¹
rj }H .

5.2

Analysis of Correctness

We analyze the validity of our scheme in two cases:
R = ∅ and R 6= ∅ respectively, as follows.
1) In the Case of R = ∅. By the definition of
Γ (rj , ri ), we have the equation
Γ (rj , ri ) =

[
rj 6¹rl

{rl } \

[
ri 6¹rl

{rl } =

[
ri ¹rl

{rl } \

[

{rl }

rj ¹rl

P
for ri ¹ rj . This means that Wi + rl ∈Γ (rj ,ri ) Dl = Wj
P
for WP
i = D0 +
ri 6¹rk Dk and ri ¹ rj , as well as
C1 + rl ∈Γ (rj ,ri ) Dl0 = [ζj · ξ]G. Therefore, the validity
of the RBE scheme can be guaranteed by (4).

704

J. Comput. Sci. & Technol., July 2011, Vol.26, No.4

µ
V = e C1 +

¶

X

0

Dl0 , Bj,k

· e(Aj,k , C2 )

rl ∈Γ (rj ,ri )

µ
h
= e [ζj · ξ]G,
= e(G, H)

¶
i ¶ µh x0
i
1
j,k
H e
G, [ξ]H
ζj + x0j,k
ζj + x0j,k

ζj ·ξ
ζj +x0j,k

ξ

· e(G, H)

ξ·x0j,k
ζj +x0j,k

ξ

= e(G, H) = V .
³
X
V 0 = e C1 +

(4)

´

R
Dl0 , Bj,k
· e(Aj,k , C2 )

rl ∈Γ (rj ,ri )

µ
h
= e [ζj · ξ]G, Qt

1

l=1 (ζil

= e(G, H)
= e(G, H)

(ζj +x0j,k )

+ x0il ,jl ) · (ζj + x0j,k )

ζ ·ξ
Qtj

0
l=1 (ζil +xil ,jl )

· e(G, H)

(ζj +x0j,k )

0
l=1 (ζil +xil ,jl )

X

i ¶
x0j,k i h
ξ
G,
H
Qt
0
ζj + x0j,k
l=1 (ζil + xil ,jl )

ξ·x0
Qt j,k
0
l=1 (ζil +xi

l ,jl

= (VR )ξ .

(5)

τk + x0i,j = τ0 + xi,j ,

ri 6¹rk

method (called aggregate algorithm) to solve this problem, as follows.
Given R = {x0i1 ,j1 , . . . , x0it ,jt } and their labels {labik ,jk } for k ∈ [1, t] and lab ik ,jk =
hxik ,jk , Bik ,jk , Vik ,jk i. In terms of (6), for all k, l ∈ [1, t],
it is easy to obtain the equation

where all xi,j are made public and all x0i,j , ζi , τi are kept
secret. Thus, for a revocation set R = {uil ,jl , uik ,jk }
and il 6= ik , it is easy to obtain
i
1
(Bik ,jk − Bil ,jl )
xil ,jl − xik ,jk
h
i
1
=
H = BR ,
0
0
(ζil + xil ,jl )(ζik + xik ,jk )

i
h
i
1
1
H−
H
τ0 + xik ,jk
τ0 + xil ,jl
h
i
xil ,jl − xik ,jk
=
H.
0
0
(ζil + xil ,jl )(ζik + xik ,jk )
h

Bik ,jk − Bil ,jl =

h

(6)

To expand this equation to multi-user cases, we dees,r for any pair (s, r),
fine the following denotation B
where 1 6 s < r 6 t,

and
1

(Vik ,jk /Vil ,jl ) xil ,jl −xik ,jk = V

(ζil +x0i

l ,jl

1
)(ζik +x0i

k ,jk

es,r =
B
)

= VR .

R
Similarly, BR , Bj,k
, and VR can be efficiently computed in an arbitrary revocation set R by a general
recursive method, which is defined in Subsection 5.2.1.
Therefore, we can prove (3) by (5).
The revocation mechanism can be supported by (6),
R
that is, for ui,j ∈ R, Bj,k
cannot be computed because
1
the denominator can be zero in a fraction xi,j −x
,
i0 ,j 0
where ui0 ,j 0 ∈ R.

5.2.1 Aggregate Algorithms for User Revocation
It is more important to compute the three values
R
B , V R , and Bj,k
from the labels of public parameter
par in an efficient way. We provide such a recursive
R

)

ξ

Qt

2) In the Case of R 6= ∅. By the definition of xi,j
and x0i,j , we have
ζi + x0i,j = τ0 +

i ¶ µh
H ·e

h

i
1
1
· Qs
H.
τ0 + xir ,jr
k=1 (τ0 + xik ,jk )

es,r =
same
way, we can compute B
i
es−1,s − B
es−1,r ). Hence, BR = B
et−1,t
(B
es,r for
can be completed by computing sequentially B
s = [1, t − 1] and r = [s + 1, t] using the equation
et−1,t ) and the induction
(BR = B
In
h

the

1
xir ,jr −xis ,js

 e
B0,r = Bir ,jr ,
∀r ∈ [1, t],



i
h
1
es−1,s − B
es−1,r ),
es,r =
(B
B

x
−
xis ,js
ir ,jr


s ∈ [1, t − 1], r ∈ [s + 1, t],
e0,r is defined as the initial input Bi ,j for
where B
k k
R
k = [1, r]. Obviously, we can get Bj,k
in the same
way, or it can be computed from the resulting sequence

Yan Zhu et al.: Provably Secure Role-Based Encryption

e0,1 , B
e1,2 , . . . , B
et−1,t i), where
(Bi,j , hB
 e
B0,t+1 = Bj,k ,



i
h

1

es−1,s − B
es−1,t+1 ),
es,t+1 =
B
(B
x0it+1 ,jt+1 − x0is ,js



∀s ∈ [1, t],



R
et,t+1 .
Bj,k
=B
1

· Qs

1

Similarly, we define Ves,r = V τ0 +xir ,jr k=1 (τ0 +xik ,jk ) ,
and then compute VR from Vi1 ,j1 , . . . , Vit ,jt , where
(VR = Vet−1,t ) and
 e
V0,r = Vir ,jr ,
∀r ∈ [1, t],



1

³ Ve
´
s−1,s xir ,jr −xis ,js
Ves,r =
,


Ves−1,r


∀s ∈ [1, t − 1], ∀r ∈ [s + 1, t].
5.3

Analysis of Security

We prove the semantic security of our RBE scheme
under the assumption of the GDDHE1 problem.
Lemma 1 assures that (n, t)-GDDHE1 problem is hard
in the generic bilinear groups.
Lemma 1 (Complexity Lower Bound in Generic
Bilinear Groups[34] ). Given an (n, t)-GDDHE1 problem, two s-tuples of 3-variate polynomials F1 , F2 ∈
Zp [x, y, z]s and a 1-tuple 2-variate polynomial F3 ∈
Zp [x, y], where s = n + t + 4② , then the maximum total degree of these polynomials is d =
max(2dF1 , dF2 , dF3 ) = max(2t + 4, n + t) 6 2n. If h
is independent of (F1 , F2 , F3 ) then for any algorithm A
that makes a total of at most q queries to the oracles
computing the group operation in G, GT and the bilinear
pairing e(·, ·), we have
Adv gddhe
(n, t)
A

(q + 2(n + t + 4) + 2)2 · d
6
2p
(q + 2(n + t + 4) + 2)2 · (2n)
.
6
2p

In terms of this lemma, we can prove that our RBE
scheme is semantically secure against dynamic colluders, the number of which is unlimited, as follows.
Theorem 3. The (m, n, t)-RBE is semantically secure against dynamic colluders (IND-hcCPA) assuming the (n, t)-GDDHE1 problem is hard in S. Concretely, for any probabilistic algorithm A that totalizes
at most q queries to the oracles performing group operations in S = (p, G1 , G2 , GT , e(·, ·)) and evaluations

705

of the bilinear map e(·, ·), we have Adv ind
E,A (m, n, t) 6
(q+2(n+t+4)+2)2 ·(2n)
.
p

Proof. We prove this theorem according to the INDhcCPA model as follows: suppose that there exists an
adversary A that can break RBE under collusion attack, we build a reduction algorithm B to solve above
(n, t)-GDDHE1 problem in terms of A.
Given S = (p, G1 , G2 , GT , e), the algorithm B is
given as input an (n, t)-GDDHE1 instance, which is defined by (1). In fact, B does not know γ, ς but knows
3n random integers ζi , xi , x0i , ai , bi ∈ Z∗p in f (x) and
g(x), where any pairwise {xi , x0i } are not equal to each
other, but some ζi may be equal. Let m be the number
of the different ζi . In terms of these known values, the
algorithm B will generate an arbitrary role hierarchy
H = hU, R, ¹i with the total number of users n and the
number of roles m, and then generates an encryption
environment based on A as follows.
1) Initial. B firstly sets G = [f (γ)]G. Note that
B cannot obtain the value of G. Then, B chooses an
integer ζi from {ζi }i∈[1,m] for each role ri in R, where
i ∈ [1, m]. Let ζ̄i = ζi γ and Wi = [ζ̄i ]G = [ζi γf (γ)]G
(which can be computed from the input [γf (γ)]G) for
each ri (i ∈ [1, m]), B computes the generation matrix
M m×(m+1) reduced from H, and then extends it to
M 0(m+1)×(m+1) by appending a row vector h1, 0, . . . , 0i
in the top of it. So that B defines the vector W =
hW0 , W1 , . . . , Wm i and D = hD0 , . . . , Dm i, satisfying
(7),



W0
 W1 

W = M0 · D = 
 ... 
Wm


1
1

=.
 ..

0
M1,2

0
M1,3

1 Mm,2

Mm,3

 

0
D0
M1,m 
D1 
 
·
.. 


. 
Dm
· · · Mm,m
(7)

···
···
..
.

where W0 = [γ]G = [γ · f (γ)]G. Obviously, B can com(m+1)×(m+1)
pute D = (M 0 )−1 ·W and (M 0 )−1 ∈ Z2
for
0
rank (M ) = m + 1 or a feasible solution hD0 , . . . , Dm i
to W = M 0 · D for rank (M 0 ) 6 m, thus B can define
τ̄i = τi γ and compute Di = [τ̄i ]G =P[τi · (γ · f (γ))]G
to realize W0 = D0 and Wi = D0 + ri 6¹rk Dk , where
Pm
τ0 = γ, τi can be obtained by ζi = 1 + k=1 Mi,k τk .

② In fact, F1 and F2 are (n + t + 4)-tuples of 3-variate polynomials and F3 is 1-tuple 2-variate polynomial, such that,
Adv

gddhe

(t, n, A) 6

(q+(n+t+4)+1+2)2 ·(2n)
.
2p

706

J. Comput. Sci. & Technol., July 2011, Vol.26, No.4

Then it computes easily the public parameter as follows:

n
X


 H = [f (γ)g(γ)]H =
di · [γ i ]H,


i=0

2


V = e(G, H) = e(G, H)f (γ)g(γ) ,



D = hD0 , D1 , . . . , Dm i from M 0 , W ,
Pn
where f (x) · g(x) = i=0 di · xi . Therefore, B can run
A on these parameters.
2) Learning. In this phase, the adversary A can issue up to t private key queries and n − t label queries to
gain the information of this cryptosystem. Let R ⊂ U
be a subset that indicates at most t corrupted users.
Algorithm B considers three types of queries as follows.
(a) Hash Query (ID, ui,j ). At any time, A can query
the hash function Hash(ID, ui,j ) and B replies a random integer in Z∗p . B can maintain tables to ensure that
repeated queries are answered consistently.
(b) Private Key Query (ui,j ∈ R). B generates the
keys of the corrupted user as follows: for the j-th user in
f (x)
=
role ri , B sets x0i,j = xi and defines fi,j (x) = ζi x+x
0
i,j
Qt
k=1,k6=i (ζk x + xk ). Thus for some ei ∈R Zp ,

fi,j (γ) =

t−1
t
X
Y
f (γ)
ei · γ i .
(ζ
γ
+
x
)
=
=
k
k
ζ̄i + x0i,j
i=0
k=1,k6=i

Since this equation is a polynomial of degree t − 1, B
can compute

h x0
i
h x f (γ) i

i
i,j

A
=
G
=
G

i,j

0

ζ
γ
ζ̄i + xi,j
i + xi




= [xi · fi,j (γ)]G,
h
h f (γ)g(γ) i
i

1


B
=
H
=
H
i,j


ζi γ + xi
ζ̄i + x0i,j




= [fi,j (γ) · g(γ)]H,
as the decryption key dk i,j = Ai,j . To generate the laQt
f (x)
0
bels of users, B defines fi,j
(x) = x+
xi =
k=1,k6=i (x +
ζi

xk
ζk )

and xi,j = xζii mod p. The value xi,j is stored
as Hash(ID, ui,j ), and lab i,j = hxi,j , Vi,j , Bi,j i can be
computed by
Vi,j = V

1
γ+xi,j

=V

f 2 (γ)g(γ)
γ+xi /ζi

0
= e([fi,j
(γ)]G, [f (γ)g(γ)]H).

Finally, B sends dk i,j and lab i,j to A. Note that, dk i,j
is available for the ciphertext which is encrypted by the
public encryption key.
(c) Public Label Query (ui,j 6∈ R). If there exists an
unused (ζi x + x0i ) in g(x) and ui,j ∈ ri , B computes the
honest user’s label but not their private keys: B first

fixes and records xi,j = Hash(ID, ui,j ) = x0i /ζi and
Qn−t
x0
g(x)
0
gi,j
(x) = x+x
= k=1,k6=i (x + ζkk ), and then com0 /ζ
i
i
putes the user’s label lab i,j = hxi,j , Vi,j , Bi,j i, where

f 2 (γ)g(γ)
1


γ+xi,j = V γ+x0i /ζi

V
V
=

i,j


2
0


= e(G, H)f (γ)gi,j (γ) ,
h
h f (γ)g(γ) i
i
1



B
=
H
=
H
i,j


γ + xi,j
γ + x0i /ζi



0
= [f (γ) · gi,j
(γ)]H,
0
where f 2 (γ)·gi,j
(γ) is the polynomial of degree n+t−1.
l

It can be computed because we can obtain e(G, H)γ for
l ∈ [0, n + t − 1] by [γ l ]G for l ∈ [0, t − 1] and [γ l ]H for
l ∈ [0, n]. Finally, lab i,j is given to A.
3) Challenge. A produces two messages M0 , M1 ∈
GT , ri ∈ R, and returns them to B, where ri denotes
the expected position of encryption. B picks a random
b ∈ {0, 1} and constructs a ciphertext Cib as follows:

C1 = [ς · ζ̄i ]G = [ζi · (ς · γ · f (γ))]G,



h
i
h f (γ)g(γ) i


1


H
C
=
ς
·
H
=
ς·
Q
2

t

f (γ)
i=1 (ζi γ + xi )

= [ς · g(γ)]H,





C3 = Mb · T,



C4 = {[ς · τ̄k ]G}∀rk ∈↑ri = {[τi · (ς · γ · f (γ))]G}∀rk ∈↑ri .
Finally, B sends the challenge ciphertext Cib to A.
4) Guess. A returns b0 ∈ {0, 1}. If b = b0 , B outputs
1 (True), otherwise 0 (False).
This completes the description of algorithm B. It
is easy to describe the advantage of adversary in both
instances. To do so, we recall two polynomials
f (x) =
g(x) =

t
Y

(ζi x + xi ) =

t ³
Y

x+

i=1
n−t
Y

i=1
n−t
Y³

i=1

i=1

(ζt+i x + x0i ) =

xi ´
ζi

x+

x0i ´
ζt+i

Qt
Qn−t
where i=1 ζi = i=1 ζt+i = 1 (mod p). If T is equal
to e(G, H)ςf (γ)g(γ) , the challenge ciphertext Ci is available since
C3 = Mb · e(G, H)

Qt

ς·

= Mb · e(G, H)

ς

i=1 (ζi γ+xi )

f 2 (γ)g(γ)
f (γ)

= Mb · T.

Hence, we have
Pr[b =b0 : T ← e(G, H)ςf (γ)g(γ) ]
= Pr[A(Cib ) = b : T ← e(G, H)ςf (γ)g(γ) ].

Yan Zhu et al.: Provably Secure Role-Based Encryption

Otherwise, we have Pr[b = b0 |T ←R GT ] = Pr[b 6=
b0 |T ←R GT ] = 1/2 and the equation③
Adv gddhe
(n, t)
¯ B
¯
¯ Pr[B((F1 , F2 , F3 , T )] = 1 ¯
¯
¯
=¯
− Pr[B((F1 , F2 , F3 , T )] = 0 ¯

707

scheme has a short constant-size private user key even
in large scale systems. Moreover, the size of ciphertext
is O(m + t), which is proportional to the number of
granted roles and revoked users. Hence, our construction achieves the optimal bound of overhead rate for
both ciphertexts and decryption keys.

= | Pr[b = b0 ] − Pr[b 6= b0 ]|
¯
¯
¯ Pr[b = b0 : T ← e(G, H)ςf (γ)g(γ) ] ¯
¯
= ¯¯
¯
− Pr[b 6= b0 : T ←R GT ]
= | Pr[A(Cib ) = b : T ← e(G, H)ςf (γ)g(γ) ] − 1/2|.
Therefore, according to Subsection 2.2, we have
Adv ind
E,A (m, n, t)
¯
¯
¯ Pr[b = b0 : T ← e(G, H)ςf (γ)g(γ) ]− ¯
¯
¯
=¯
Pr[b 6= b0 : T ← e(G, H)ςf (γ)g(γ) ] ¯
= |2 Pr[b = b0 : T ← e(G, H)ςf (γ)g(γ) ] − 1|
= 2| Pr[A(Ci ) = b : T ← e(G, H)ςf (γ)g(γ) ] − 1/2|.
Summing up, we get that Adv gddhe
(n, t) =
B
Adv ind
(m,
n,
t)/2.
In
terms
of
Lemma
1,
it
is easy to
E,A
2

(q+2(n+t+4)+2) ·(2n)
see that Adv ind
. This
E,A (m, n, t) 6
p
implies the algorithm can decide GDDHE1 problem
with a non-negligible success probability, which would
contradict with the assumption. Moreover, we can
prove the same result when t = n, that is, full collusion security.
¤

6

Performance Analysis

Following our terminology, we denote |R| = m and
|R| = t. We use E to denote a multiplication operation in G1 , G2 or an exponentiation operation in GT ,
P to denote the pairing operation e : G1 × G2 →
GT . We analyze the performance of each phase in
our scheme as shown in the second column of Table
3. We neglect the hash operation, the operations in
Zp , an addition in G1 , G2 and a multiplication in GT ,
since these operations are much more efficient compared
to the pairing operation and exponentiation. Considering the
our scheme requires
¡ revocation mechanism,
¢
1
2 t(t + 1) E(G2 ) + E(GT ) in the encryption algorithm,
and (2t + 1)E(G2 ) is required in the decryption algorithm. This indicates that our scheme has a low computation overhead, related to the number of granted roles
and revoked users.
We also present the communication complexity in
the third column of Table 3. This indicates that our

Fig.2. Comparison between exiting encrypted file system in Windows NT (a) and our scheme (b).

Our scheme can construct an efficient encrypted file
systems (EFS) based on RBAC, which enables the users
to encrypt files on disks in terms of the user’s role(s).
Many existing encrypted file systems implement the
straight forward encryption system where the number
of ciphertexts in the file header grows linearly in the
number of users that can access the file. As a result,
there is often a hard limit on the number of users that
can access a file, and the headers of all files must be
changed to permit the user’s access when a new user
joins the system. For example, the following quote is
from Microsoft’s knowledge base: “EFS has a limit of
256 KB in the file header for the EFS metadata. This
limits the number of individual entries for file sharing
that may be added. On average, a maximum of 800
individual users may be added to an encrypted file.”[37]
We show such a structure in Fig.2(a).
However, the RBAC systems built on our RBE
scheme can automatically use the role key to encrypt
the files in terms of the user’s role ri in a transparent way for users. Such a file header is shown in
Fig.2(b), in which “Cipher” consists of the constantsize C1 , C2 , C3 ∈ Ci , “Granted roles table” consists of
C4 ∈ Ci , and “Revoked users table” consists of the list

③ Let R and Z denote T ← G and T ← e(G, H)ςf (γ)g(γ) , respectively. It is easy to show that
R
T
| Pr[b = b0 ] − Pr[b 6= b0 ]| = | Pr[b = b0 : R] Pr[R] + Pr[b = b0 : Z] Pr[Z] − Pr[b 6= b0 : R] Pr[R] − Pr[b 6= b0 : Z] Pr[Z]|
1
= | (2 Pr[b = b0 : Z] − 1 + 1 − 2 Pr[b 6= b0 : R])| = | Pr[b = b0 : Z] − Pr[b 6= b0 : R]|.
2

708

J. Comput. Sci. & Technol., July 2011, Vol.26, No.4
Table 3. Performance Analysis for RBE

Setup (Public parameter)
GenRKey (Public role key)
AddUser (Private user key)
Encrypt (Ciphertext)
Decrypt

Computation Complexity

Communication Complexity

(m + 1)E(G1 ) + 1P

(m + 1)G1 + 1G2 + 1GT
1Z∗p + mG1 + 1G2 + 1GT
1Z∗p + 1G2 + 1GT
(m + 1)G1 + 1G2 + 1GT

1E(GT )
(m + 1)E(G1 ) + 1E(G2 ) + 1E(GT )
2P

of user’s labels in R ∈ Ci for an RBE ciphertext Ci .
Here, both the number of users and the number of revoked users are not limited in this EFS system. Moreover, for a new user who joins this system, all existing
files need not be changed to permit the access of these
files.
To reduce the length of list of revoked users in file
header, our RBE scheme has the capability of fixing
the user’s labels into the public role key by using the
Aggregate algorithms in Subsection 5.2, such that these
users will be permanently revoked. Furthermore, as a
practical cryptosystem, the keys in RBE system should
be regularly renewed by the system manager (such as 3
months or half year). The users who leave the system
should be permanently revoked after updating keys.
Hence, these mechanisms can avoid the accumulation
of revoked users in RBE systems, as well as can reduce
the length of R in file headers.
For the sake of clarity, we evaluate the performance
of EFS on our RBE scheme as follows: suppose the
security parameter κ is 80-bits[38-39] , we need the elliptic curve domain parameters over Zp with |p| =
160-bits. Elliptic curve domain parameters over Zp
with dlog2 pe = 2κ supply approximately κ bits of
security[40] , which means that solving the logarithm
problem on associated elliptic curve is believed to take
approximately 2κ operations. This means that the
length of the integer is l0 = 2κ in Zp . Similarly, we
have the length of the element in G1 , G2 , GT , which
satisfy l1 = 4κ, l2 = 20κ, and lT = 10κ. We assume that the embedding degree of elliptic curve is 5.
In the RBE scheme, the length of the file header is
(m+1)·l1 +1·l2 +1·lT +t·l = 4κ·(m+1)+20κ+10κ+
128t = 320m + 128t + 2720 bits, where l is the length of
user’s label and is set to 128 bits presumed. Considering
a system where each role contains 40 users on average,
with 800 users and 100 revoked users, the file header is
just 320 × 20 + 128 × 100 + 2720 = 21920 bit ≈ 2.68 KB.
In contract to the existing EFS structure of Windows
NT[41] , this storage cost is far less than 256 KB, which
is the exact size of file header of encrypted files in Windows EFS. Furthermore, in RBE-based EFS, the file
header with 256 KB can support the system with about
2000 roles, and each role contains 300 users on average

and 11 000 revoked users, where the length of the user
label is 128 bit. In theory, the above-mentioned system
can support unlimited number of users, which is much
better than existing EFS.
7

Conclusion and Future Work

In this paper, we introduced a generic role-based encryption over RBAC model to support a flexible encryption of resources in RBAC systems. The proposed
scheme supports fully collusion security under a special case of the GDDHE problem and implements the
revocation at minimal cost and constant-size ciphertexts and decryption keys. Our scheme has better performance and scalability than existing solutions in encrypted file systems.
In our future work, we will investigate a more comprehensive role-based cryptosystem to support various
secure mechanisms, such as encryption, signature, and
authentication. Meanwhile, we would exploit the partial ordering relation in ABE with respect to the work
addressed in this paper. We will also optimize our solution to improve the performance of revocation algorithms in our scheme. Finally, based on our exiting
work, we will propose a complete cryptosystem to realize massive-scale conditional access systems for the
practical RBAC applications of large-scale organizations.
References
[1] Sandhu R, Ferraiolo D F, Kuhn D R. The nist model for
role-based access control: Towards a unified standard. In
Proc. the 5th ACM Workshop on Role Based Access Control
(RBAC), Berlin, Germany, Jul. 26-27, 2000, pp.47-63.
[2] Li Q, Zhang X W, Xu M W, Wu J P. Towards secure dynamic
collaborations with group-based RBAC model. Computers &
Security, 2009, 28(5): 260-275.
[3] Shafiq B, Joshi J, Bertino E, Ghafoor A. Secure interoperation in a multidomain environment employing RBAC policies. IEEE Transactions on Knowledge and Data Engineering, 2005, 17(11): 1557-1577.
[4] Zhu Y, Ahn G J, Hu H X, Wang H X. Cryptographic rolebased security mechanisms based on role-key hierarchy. In
Proc. the 5th ACM Symposium on Information, Computer
and Communications Security (ASIACCS), Beijing, China,
Apr. 13-16, 2010, pp.314-319.
[5] Akl S G, Taylor P D. Cryptographic solution to a problem of access control in a hierarchy. ACM Transactions on

Yan Zhu et al.: Provably Secure Role-Based Encryption
Computer System, 1983, 1(3): 239-248.
[6] Akl S G, Taylor P D. Cryptographic solution to a multilevel security problem. In Proc. Advances in Cryptology:
CRYPTO, Santa Barbara, USA, 1982, pp.237-249.
[7] Wallner D M, Harder E G, Agee R C. Key management for
multicast: Issues and architecture. Internet Draft, draftwaller-key-arch-01.txt, 1998.
[8] Wong C K, Gouda M, Lam S S. Secure group communications
using key graphs. In Proc. the Annual Conference of the Association for Computing Machinery’s Special Interest Group
on Data Communication (SIGCOMM), Vancouver, Canada,
Sept. 2-4, 1998, 28, pp.68-79.
[9] Asano T. Reducing receiver’s storage in CS, SD and LSD
broadcast encryption schemes. IEICE Transactions on Fundamentals of Electronics, Communications and Computer
Sciences, 2005, 88(1): 203-210.
[10] Naor D, Naor M, Lotspiech J. Revocation and tracing schemes
for stateless receivers. In Proc. the 21st Annual International Cryptology Conference (CRYPTO), Santa Barbara,
USA, Aug. 19-23, 2001, pp.41-62.
[11] Halevy D, Shamir A. The LSD broadcast encryption scheme.
In Proc. the 22nd International Cryptology Conference
(Crypto), Santa Barbara, USA, Aug. 18-22, 2002, pp.47-60.
[12] Boneh D, Franklin M. Identity-based encryption from the weil
pairing. In Proc. the 21st Annual International Cryptology
Conference (CRYPTO), Santa Barbara, USA, Aug. 19-23,
2001, pp.213-229.
[13] Yuen T H, Susilo W, Mu Y. How to construct identity-based
signatures without the key escrow problem. International
Journal of Information Security, 2010, 9(4): 297-311.
[14] Gentry C, Silverberg A. Hierarchical ID based cryptography. In Proc. the 8th International Conference on the Theory and Application of Cryptology and Information Security
(ASIACRYPT), Queenstown, New Zealand, Dec. 1-5, 2002,
pp.548-566.
[15] Tzeng W G. A time-bound cryptographic key assignment
scheme for access control in a hierarchy. IEEE Transactions
on Knowledge and Data Engineering, 2002, 14(1): 182-188.
[16] Sahai A, Waters B. Fuzzy identity-based encryption. In Proc.
the 24th Annual International Conference on the Theory and
Applications of Cryptographic Techniques (EUROCRYPT),
Aarhus, Denmark, May 22-26, 2005, pp.457-473.
[17] Goyal V, Pandey O, Sahai A, Waters B. Attribute-based encryption for fine-grained access control of encrypted data. In
Proc. the 13th ACM Conference on Computer and Communications Security (CCS), Alexandria, USA, Oct. 30-Nov. 3,
2006, pp.89-98.
[18] Ostrovsky R, Sahai A, Waters B. Attribute-based encryption
with non-monotonic access structures. In Proc. the 14th
ACM Conference on Computer and Communications Security (CCS), Alexandria, USA, Oct. 28-31, 2007, pp.195-203.
[19] Chase M. Multi-authority attribute based encryption. In
Proc. the 4th Theory of Cryptography Conference (TCC),
Amsterdam, The Netherlands, Feb. 21-24, 2007, pp.515-534.
[20] Bethencourt J, Sahai A, Waters B. Ciphertext-policy
attribute-based encryption. In Proc. 2007 IEEE Symposium
on Security and Privacy (S&P), Oakland, USA, May 20-23,
2007, pp.321-334.
[21] Waters B. Ciphertext-policy attribute-based encryption:
An expressive, efficient, and provably secure realization. Cryptology ePrint Archive, Report 2008/290, 2008,
http://eprint.iacr.org/.
[22] Goyal V, Jain A, Pandey O, Sahai A. Bounded ciphertext
policy attribute based encryption. In Proc. the 35th International Colloquium on Automata, Languages and Programming, Part II — Track B: Logic, Semantics, and Theory of Programming & Track C: Security and Cryptography

709

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

Foundations (ICALP(2)), Reykjavik, Iceland, Jul. 7-11, 2008,
pp.579-591.
Ibraimi L, Tang Q, Hartel P H, Jonker W. Efficient and
provable secure ciphertext-policy attribute-based encryption
schemes. In Proc. the 5th International Conference on Information Security Practice and Experience (ISPEC), Xi’an,
China, Apr. 13-15, 2009, pp.1-12.
Attrapadung N, Imai H. Dual-policy attribute based encryption. In Proc. the 7th International Conference on Applied
Cryptography and Network Security (ACNS), Paris, France,
Jun. 2-5, 2009, pp.168-185.
Attrapadung N, Imai H. Dual-policy attribute based encryption: Simultaneous access control with ciphertext and key
policies. IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, 2010, E93A(1): 116-125.
Wang L Y, Wijesekera D, Jajodia S. A logic-based framework for attribute based access control. In Proc. the 2004
ACM Workshop on Formal Methods in Security Engineering
(FMSE), Washington DC, USA, Oct. 29, 2004, pp.45-55.
Frikken K B, Atallah M J, Li J T. Attribute-based access
control with hidden policies and hidden credentials. IEEE
Transaction on Computers, 2006, 55(10): 1259-1270.
Schoinas I, Falsafi B, Lebeck A R, Reinhardt S K, Larus J R,
Wood D A. Fine-grain access control for distributed shared
memory. In Proc. the 6th International Conference on Architectural Support for Programming Languages and Operating
Systems (ASPLOS), San Jose, USA, Oct. 4-7, 1994, pp.297306.
Damiani E, Vimercati S D C D, Paraboschi S, Samarati P. A
fine-grained access control system for xml documents. ACM
Transactions on Information and System Security, 2002,
5(2): 169-202.
Shahandashti S F, Naini R S. Threshold attribute-based signatures and their application to anonymous credential systems. In Proc. the 2nd International Conference on Cryptology in Africa (AFRICACRYPT), Gammarth, Tunisia,
Jun. 21-25, 2009, pp.198-216.
Maji H, Prabhakaran M, Rosulek M. Attribute-based
signatures:
Achieving attribute-privacy and collusionresistance. Cryptology ePrint Archive, Report 2008/328,
2008, http://eprint.iacr.org/.
Wang H X, Zhu Y, Feng R Q. Attribute-based signature with
policy-and-endorsement mechanism. Journal of Computer
Science and Technology, 2010, 25(6): 1293-1304.
Attrapadung N, Imai H. Attribute-based encryption supporting direct/indirect revocation modes. In Proc. the 12th
IMA International Conference on Cryptography and Coding,
Cirencester, UK, Dec. 15-17, 2009, pp.278-300.
Boneh D, Boyen X, Goh E J. Hierarchical identity based encryption with constant size ciphertext. In Proc. the 24th
Annual International Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT), Aarhus,
Denmark, May 22-26, 2005, pp.440-456.
Boneh D, Gentry C, Waters B. Collusion resistant broadcast encryption with short ciphertexts and private keys. In
Proc. the 25th Annual International Cryptology Conference
(CRYPTO), Santa Barbara, USA, Aug. 14-18, 2005, pp.258275.
Toahchoodee M, Xie X, Ray I. Towards trustworthy delegation in role-based access control model. In Proc. the 12th International Conference on Information Security (ISC), Pisa,
Italy, Sept. 7-9, 2009, pp.379-394.
Microsoft Corporation.
How encrypting file system
works. Microsoft TechNet Report, 2009, http://technet.microsoft.com/en-us/library/cc781588(WS.10).aspx.

710

J. Comput. Sci. & Technol., July 2011, Vol.26, No.4

[38] SEC1. Standards for efficient cryptograhy group: Elliptic
curve cryptography, Version 1.0, 2000.
[39] SEC2. Standards for efficient cryptograhy group: Recommended elliptic curve domain parameters, Version 1.0, 2000.
[40] Su D, Lv K W. A new hard-core predicate of paillier’s trapdoor function. In Proc. the 10th International Conference
on Cryptology in India (INDOCRYPT), New Delhi, India,
Dec. 13-16, 2009, pp.263-271.
[41] Schultz E E. Windows 2000 security: A postmortem analysis.
Network Security, 2004, 2004(1): 6-9.

Yan Zhu is an associate professor
of computer science in the Institute
of Computer Science and Technology
at Peking University since 2007. He
worked at the Department of Computer Science and Engineering, Arizona State University as a visiting associate professor from 2008 to 2009.
His research interests include cryptography and network security.
Hong-Xin Hu is currently working toward the Ph.D. degree at the
School of Computing, Informatics
and Decision Systems Engineering,
Ira A. Fulton School of Engineering,
Arizona State University, Tempe. He
is also a member of the Security
Engineering for Future Computing
Laboratory, Arizona State University. His current research interests include access control models and mechanisms, security in social network and cloud computing, network and distributed
system security and secure software engineering.

Gail-Joon Ahn received the
Ph.D. degree in information technology from George Mason University,
Fairfax, USA, in 2000. He was an
associate professor at the College of
Computing and Informatics, and the
Founding Director of the Center for
Digital Identity and Cyber Defense
Research and Laboratory of Information Integration, Security, and Privacy, University of North Carolina, Charlotte. He is currently an associate professor in the School of Computing, Informatics, and Decision Systems Engineering, Ira A. Fulton
School of Engineering and the Director of Security Engineering for Future Computing Laboratory, Arizona State University, Tempe. His research interests include information
and systems security, vulnerability and risk management,
access control, and security architecture for distributed systems, which has been supported by the U.S. National Science Foundation, National Security Agency, U.S. Department of Defense, U.S. Department of Energy, Bank of America, Hewlett Packard, Microsoft, and Robert Wood Johnson
Foundation. He is a recipient of the U.S. Department of Energy CAREER Award and the Educator of the Year Award
from the Federal Information Systems Security Educators
Association.
Huai-Xi Wang received his B.S.
degree from the School of Mathematical Sciences, Peking University in
2006. He is a Ph.D. candidate at
Peking University. His research interests include attribute based system and pairing based cryptography.

Shan-Biao Wang received his
B.S. degree from the School of Mathematical Sciences, Peking University
in 2007. He is a Ph.D. candidate
at Peking University. His research
interests include secure computation
and lattice based cryptography.

2011 IEEE World Congress on Services

Reputation-based Web service selection for Composition
Srividya K Bansal
Department of Engineering
Arizona State University
Mesa, Arizona 85212, USA
Email: srividya.bansal@asu.edu

Ajay Bansal
Department of Computer Science
Georgetown University
Washington, DC 20057, USA
Email: bansal@cs.georgetown.edu

rest of the network. So our rationale is that these central
ﬁgures who play a fundamental role in the network are
trusted by others in the network who are connected (directly
or indirectly) to them.
Our work investigates the following research issues: (i)
compute the reputation score of composition solutions based
on individual scores of service providers obtained using the
centrality measure of social networks (ii) set a threshold for
reputation that each and every Web service involved in the
composition has to satisfy. Failure to meet the threshold will
result in ﬁltering out the Web service and it will not be used
in any composition solution.

Abstract—The success and acceptance of Web service composition depends on computing solutions comprised of trustworthy services. In this paper, we extend our Web service Composition framework to include selection and ranking of services
based on their reputation score. With the increasing popularity
of Web-based Social Networks like Linkedin, Facebook, and
Twitter, there is great potential in determining the reputation
score of a particular service provider using Social Network
Analysis. We present a technique to calculate a reputation score
per service using centrality measure of Social Networks. We
use this score to produce composition solutions that consist of
services provided by trust-worthy and reputed providers.
Keywords-service composition; reputation; social networks;

I. I NTRODUCTION

II. C ENTRALITY M EASURE IN S OCIAL N ETWORKS :

The next milestone in the evolution of the World Wide
Web is making services ubiquitously available. We need
infrastructure that applications can use to automatically
discover, deploy, compose, and synthesize services. Along
with the functional attributes there is a need to consider
non-functional attributes (Quality of Service parameters) of
Web services in the process of building composite Web
services. The current challenge in automatic composition of
Web services also includes ﬁnding a composite Web service
that can be trusted by consumers before using it. In this
paper, we present our approach that uses analysis of Social
Networks to calculate a reputation score for each service
involved in the composition and further prune results based
on this score.
Web-based Social Networks have become increasingly
popular these days. Social Network Analysis is the process
of mapping and measuring the relationships between connected nodes. These nodes could represent people, groups,
organizations, computers, or any knowledge entity. We propose to measure the reputation of a service by measuring
the centrality of a service provider and/or a service provider
organization in a well-known Social Network. We adopt our
idea of computing a reputation score using centrality measure based on the notion of centrality and prestige being key
in the study of social networks [1], [2]. The role of central
people (nodes with high centrality) in a network seems to
be fundamental as they adopt the innovation and help in
transportation and diffusion of information throughout the

Social Network Analysis focuses on the structure of relationships ranging from casual acquaintance to close bonds.
It involves measuring the formal and informal relationships
to understand information/knowldege ﬂow that binds the
interacting units that could be a person, group, organization,
or any knowledge entity. In order to understand social
networks and their participants, the location of an actor in a
network is evaluated. The network location is measured in
terms of centrality of a node that gives an insight into the
various roles and groupings in a network.
Centrality gives a rough indication of the social power
of a node based on how well they “connect” the network.
The graph-theoretic conception of compactness has been
extended to the study of Social Networks and simply renamed “graph centrality” [1]. Their measures are all based
upon distances between points, and all deﬁne graphs as
centralized to the degree that their points are all close
together. Based on research on communication in Social
Networks, the centrality of an entire network should index
the tendency of a single point to be more central than all
other points in the network. Measures of a graph centrality
are based on differences between the centrality of the most
central point and that of all others. Thus, they are indexes
of the centralization of the network [4]. The three most
popular individual centrality measures are Degree Centrality,
Betweenness Centrality, and Closeness Centrality.
Degree Centrality: The network activity of a node can
be measured using the concept of degrees, i.e., the number

978-0-7695-4461-8/11 $26.00 © 2011 IEEE
DOI 10.1109/SERVICES.2011.96

95

ServiceProvider
Provider A
Provider B
Provider C
Provider D
Provider E
Provider F
Provider G

Degree
2
3
1
8
3
4
5

ServiceProvider
Provider H
Provider I
Provider J
Provider K
Provider L
Provider M

Degree
3
1
2
4
1
1

CD (sk ) =

a(si , sk )

i=0

where a(si , sk ) = 1 iff si and sk are connected
0 otherwise
As such it is a straightforward index of the extent to which
sk is a focus of activity. CD (sk ) is large if service provider
sk is adjacent to, or in direct contact with, a large number of
other service providers, and small if sk tends to be cut off
from such direct contact. CD (sk ) = 0 for a service provider
that is totally isolated from any other point. Our algorithm
ﬁlters out any services whose provider has a zero degree
centrality in a social network, i.e., such services will not be
used in building composition solutions. Reputation of the
entire composite service is computed as an average of the
individual reputation score of the services involved in the
composition.
We also need to set a reputation threshold and any service
with a reputation score that is below this threshold is not
used while generating composition solutions. In our initial
prototype implementation we set the reputation threshold
to zero, i.e., degree centrality of the service provider in
the network is zero. A service provider or service provider
organization that is not connected to any other nodes in
the Social network is not known to anyone else and is
an immediate reason to be pruned out from composition
solutions as the service cannot be trusted. Composition
solutions can be ranked such that solutions with highest
reputation score appear on top of the list.

Table I
D EGREE C ENTRALITY OF N ODES IN F IGURE 1

Figure 1.

n


A Social Network of Web service Providers

of direct connections a node has. In the example network
shown in ﬁgure 1 and table I, Provider D has the most direct
connections in the network, making it the most active node
in the network. In personal Social Networks, the common
thought is that “the more connections, the better”.
Betweenness Centrality: Though Provider D has many
direct ties, Provider H has fewer direct connections (close
to the average in the network). Yet, in many ways, Provider
H has one of the best locations in the network by playing
the role of a “broker” between two important components.
Closeness Centrality: Provider F and G have fewer connections than Provider D, yet the pattern of their direct and
indirect ties allow them to access all the nodes in the network
more quickly than anyone else. They have the shortest paths
to all other and hence are in an excellent position to have
the best visibility into what is happening in the network.
Individual network centralities provide insight into the individual’s location in the network. The relationship between
the centralities of all nodes can reveal much about the overall
network structure.

IV. C ONCLUSIONS AND F UTURE W ORK
In this paper, we presented our approach to compute
reputation of services and use this score to select services
for composition. A reputation score is computed for every
service in the repository based on degree centrality of
the service provider in a well-known Web-based Social
Network. Our future work includes exploring other measures
of centrality such as betweenness centrality and closeness
centrality and analyzing the possibility of using a combination of all three measures of centrality to compute reputation
of a service and/or provider.
R EFERENCES

III. R EPUTATION - BASED W EB SERVICE SELECTION FOR

[1] L. C. Freeman, Centrality in Social Networks Conceptual
Clariﬁcation, in Social Networks, Vol. 1, No. 3. (1979), pp.
215-239.

COMPOSITION

We extend our previous work on Web service composition
[3] (that uses both functional and non-functional parameters
to compute composition solutions) by using reputation
to ﬁlter services. The reputation score of each service
in a Web service repository is computed as a measure
of the degree centrality (CD ) of the social network to
which the service provider belongs. It is calculated as the
degree or count of the number of adjacencies for a node, sk :

[2] S. Wasserman, and K. Faust, Social Network Analysis: Methods
and Applications, Cambridge: Cambridge Univ. Press, 1994.
[3] S. Kona, A. Bansal, M. Blake, and G. Gupta, Generalized
Semantics-based Service Composition in Proceedings of IEEE
Intl. Conference on Web Services (ICWS), September 2008.
[4] H. J. Leavitt, Some effects of communication patterns on group
performance in Journal of Abnormal and Social Psychology,
pp. 46:38-50, 1951.

96

Web Service Discovery and Composition using USDL
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
University of Texas at Dallas
Richardson, TX 75083
Abstract
For web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize
services automatically. In this paper, we present the
design of an automatic service discovery and composition engine using USDL (Universal Service-Semantics
Description Language) [1, 2], a language for formally
describing the semantics of web-services. The implementation will be used for the WS-Challenge 2006 [3].

1. Introduction
A web-service is a program available on a website that “eﬀects some action or change” in the world
(i.e., causes a side-eﬀect). The next milestone in the
Web’s evolution is making services ubiquitously available. As automation increases, these web-services will
be accessed directly by the applications themselves
rather than by humans. To achieve this, we need a
semantics-based approach such that applications can
reason about a service’s capability to a level of detail
that permits their discovery and composition.
In this paper we present the design of a software system for automatic service discovery and composition.
This system uses web service descriptions written in
USDL [1, 2]. In section 2 we present a brief overview
of USDL. Section 3 shows the design of our software
with brief descriptions of the diﬀerent components of
the system followed by conclusions and references.

2. Overview of USDL
USDL is a language that provides formal semantics of web-services thus allowing sophisticated conceptual modeling and searching of available services, automated composition, and automated service integration.

This is an ongoing project with Metallect Corp. The
design and formal speciﬁcation in OWL was published
in European Conference On Web Services, 2005 [1].
The WSDL [4] syntax and USDL semantics of web services can be published in a directory which applications
can access to discover services. To provide formal semantics, a common denominator must be agreed upon
that everybody can use as a basis of understanding the
meaning of services. Additionally, the semantics should
be given at a conceptual level that captures common
real world concepts. USDL uses an ontology based on
OWL WordNet [5] for a universal ontology of basic
concepts upon which arbitrary meets and joins can be
added in order to gain tractable ﬂexibility.
USDL describes a service in terms of portType and
messages, similar to WSDL. The semantics of a service
is given using the OWL WordNet ontology: portType
(operations provided by the service) and messages (operation parameters) are mapped to disjunctions of conjunctions of (possibly negated) concepts in the OWL
WordNet ontology. Additional semantics is given in
terms of how a service aﬀects the external world. Formal semantic description of any external conditions
(pre-conditions or post-conditions) acting on the service can also be provided using USDL.

3. Design
Our discovery and composition engine is written
using Prolog [6]. Our software system for the WSChallenge [3] comprises of the following components
shown in Figure 1.

3.1. USDL Generator
This module parses all WSDL descriptions from the
given repository and converts them into corresponding
USDL descriptions. For the WS-Challenge, this module maps part names of a service to their corresponding
type deﬁnitions from the XML Schema ﬁle, as opposed

Proceedings of the 8th IEEE International Conference on E-Commerce Technology and the 3rd IEEE
International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE’06)
0-7695-2511-3/06 $20.00 © 2006

IEEE

Thomas D. Hite
Metallect Corp.
2400 Dallas Parkway
Plano, TX 75093

3.3. Semantic Relations Generator
For the semantic part of the challenge, we have to
match the parmater types. The XML Schema will provide type hierarchy. We will obtain the semantic relations from the XML Schema ﬁle provided instead of
OWL WordNet ontology. Complex types will be described in the schema ﬁle using simple data types, thus
providing a type hierarchy. A supertype will subsume
its subtype which is nothing but the hyponym and hypernym relation. A hyponym is a word that is more
speciﬁc than a given word, also called the sub-ordinate.
A hypernym is a word that is more generic than a given
word, also called the super-ordinate.
This module will extract the type deﬁnitions from
the XML Schema ﬁle and create the semantic relations
(hypernym, hyponym, etc.) [7] between the diﬀerent
types in a format that the discovery and composition
query processors will understand. For example, if the
XML Schema has a type p66a9128258 which is a supertype of p56a4809967, then this module will add the
prolog fact
hyponym(p56a4809967,p66a9128258).
to the list of facts.
to pointing them to concepts in OWL WordNet ontology. Our approach does not need a separate query language. It parses the query ﬁle and converts each query
for the desired service also into a USDL description.

3.2. Triple Generator
The triple generator module converts each service
description into a triple as follows:
(Pre-Conditions, aﬀect-type(aﬀected-object, I, O),
Post-Conditions).
The function symbol aﬀect-type is the side-eﬀect of the
service and aﬀected object is the object that changed
due to the side-eﬀect. I is the list of inputs and O
is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions
are the conditions on the output parameters of the
service. Services are converted to triples so that they
can be treated as terms in ﬁrst-order logic and specialized uniﬁcation algorithms can be applied to obtain exact, generic, speciﬁc, part and whole substitutions [7].
For the WS-Challenge, conditions on a service will not
be provided and hence the Pre-Conditions and PostConditions in the triple will be null. The aﬀect-type
will also not be available and so this module will assign
a generic aﬀect to all services.

3.4. Discovery Query Processor
This module compares the discovery query with
all the services in the repository. It uses an extended/special uniﬁcation algorithm to ﬁnd a matching
term. The uniﬁcation mechanism is diﬀerent depending on the type of match (Exact, Speciﬁc, Generic,
Part or Whole) required. The processor works as follows:
1. On the input parts of a service, the processor ﬁrst
looks for an exact substitutable. If it does not
ﬁnd one, then it looks for a type with hypernym
relation [7], i.e., a generic substitutable.
2. On the output parts of a service, the processor ﬁrst
looks for an exact substitutable. If it does not ﬁnd
one, then it looks for a type with hyponym relation
[7], i.e., a speciﬁc substitutable.
The discovery engine is written using prolog (a logic
programming language) [6]. It uses a repository of
facts, which contains a list of all the services and the
semantic relations. The query is converted into a prolog query that looks as follows:
discoverServices(queryService, solutionService).
The engine will try to ﬁnd a list of solutionServices that
match the queryService. Our code for the engine will
have various rules to solve the discoverServices query.

Proceedings of the 8th IEEE International Conference on E-Commerce Technology and the 3rd IEEE
International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE’06)
0-7695-2511-3/06 $20.00 © 2006

IEEE

3.5. Composition Query Processor
For service composition, the ﬁrst step is ﬁnding the
set of composable services. If a subservice S1 is composed with subservice S2 , then the output parts of S1
must be the input parts of S2 . Thus the processor has
to ﬁnd a set of services such that the outputs of the
ﬁrst service are inputs to the next service and so on.
These services are then stitched together to produce
the desired service.
If the complete semantics of a web service are described using USDL and OWL WordNet ontology, Part
substitution technique [7] can be used to ﬁnd the different parts of a whole task and the selected services
can be composed into one by applying the correct sequence of their execution. The correct sequence of execution can be determined by the pre-conditions and
post-conditions of the individual services. For the challenge, we do not have with mappings to OWL WordNet
Ontology and description of conditions. So we will be
using only Exact, Generic, and Speciﬁc substitution [7]
techniques to ﬁnd the list of composable services. Similar to the discovery engine, composition engine is also
written using prolog. The query is converted into a
prolog query that looks as follows:
composeServices(queryService, listOfServices).
The engine will try to ﬁnd a listOfServices that can be
composed into the requested queryService. Our code
for the engine will have various rules to solve the composeServices query. Our prolog code will be setup in
such a way that all possible listofServices that can be
used for composition will be returned.

3.6. Output Generator
After the Discovery/Composition Query processor
ﬁnds a matching service, or the list of atomic services
for a composed service, the results are sent to the output generator in the form of triples. This module generates the output ﬁles using the speciﬁed XML format
for the WS-Challenge[3].

4. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure to publish services, document services and
query repositories for matching services. Our approach
uses USDL to formally document the semantics of services and our discovery and composition engines ﬁnd

substitutable services that best match the desired service.
Our solution will produce very good results when semantic descriptions of web services are provided using
USDL. While matching real-world services, our discovery and composition engine will look at OWL WordNet
ontology for the meanings. For the challenge, type hierarchy is the only semantics provided and hence we will
not be able to use all the semantic relations available in
WordNet. We apply some optimization techniques to
our system so that it is eﬃcient on the WS-Challenge
repositories. We use constraint-logic programming [8]
for better pruning of the search space. We have put
in eﬀorts for testing the eﬃciency of our system and
identifying the correct optimization strategies.

References
[1] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In European Conference On
Web Services, pp. 214-225, 2005.
[2] S. Kona, A. Bansal, G. Gupta, and T. Hite.
USDL - Formal Deﬁnitions in OWL. Internal Report, University of Texas, Dallas, 2006. Available
at http://www.utdallas.edu/~srividya.kona/
USDLFormalDefinitions.pdf.
[3] WS Challenge 2006.
tu-berlin.de/wsc06.

[4] Web Services Description Language. http://www.
w3.org/TR/wsdl.
[5] Ontology-based information management system,
wordnet OWL-Ontology. http://taurus.unine.
ch/knowler/wordnet.html.
[6] L. Sterling and S. Shapiro. The Art of Prolog. MIT
Press, 1994.
[7] S. Kona, A. Bansal, L. Simon, A. Mallya, G. Gupta,
and T. Hite. USDL: A Service-Semantics Description Language for Automatic Service Discovery
and Composition. Technical Report UTDCS-1806, University of Texas, Dallas, 2006. Submitted to
JWSR. Available at http://www.utdallas.edu/
~srividya.kona/USDL.pdf.
[8] K. Marriott and P. J. Stuckey. Programming with
Constraints: An Introduction. MIT Press, 1998.

Proceedings of the 8th IEEE International Conference on E-Commerce Technology and the 3rd IEEE
International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE’06)
0-7695-2511-3/06 $20.00 © 2006

IEEE

http://insel.flp.cs.

Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises

An Agent-based Approach for Composition of Semantic Web Services
Ajay Bansal, Srividya Kona, M. Brian Blake
Department of Computer Science,
Georgetown University
Washington, DC 20057

Abstract

Approaches leveraging semantic annotations and information management techniques are necessary to enable reasoning about service-oriented capabilities to a
level of detail that permits discovery, composition, deployment, and synthesis [3]. This paper emphasizes
composition. Service composition [6, 7, 10] is the integration of distributed coarse-grained applications.using
four phases: (i) Planning, (ii) Discovery, (iii) Selection,
and (iv) Execution [12]. These four phases incorporate
the aggregation of services into a plan, the discovery of
candidate services, the selection of the most relevant
services, and the enactment of the process based on
service invocations.
In this paper, we extend our earlier work [4] towards
service composition with the introduction of intelligent
agents that manage automatic Web service composition across decentralized repositories. This approach
performs planning, discovery, and selection without
manual intervention. Note that service discovery is
a special case of composition of n services, i.e., when
n=1. Thus, we mainly study the general problem of
automatically composing n services to satisfy the demand for a particular service, posed as a query by the
user.
The rest of the paper is organized as follows. In
Section 2 we present the related work in the area of
service composition and agent-based composition and
discuss their limitations. In Section 3, we present the
Web service Composition problem with an example of
a travel reservation composite service. Next we present
a discussion of our agent-based technique for automatic
Web service composition in Section 4. Finally, we
present our conclusions and future work.

The paradigm of Service-oriented computing (SOC)
introduces emerging concepts for distributed- and ebusiness processing enabling the sharing and reuse of
service-centric capabilities. The underpinning for an
organization’s use of SOC techniques is the ability to
discover and compose Web services. Leading industry
approaches rely heavily on syntactical approaches for
managing service-based business processes. As such,
these approaches are limited since the true functionality
of ambiguous capabilities (i.e. web service operations)
cannot be inferred. We introduce approaches that disambiguate services by interleaving process-based control
with semantic annotations. In this paper, we introduce a generalized architecture where intelligent software agents control process-oriented composition that
leverages the descriptiveness of semantics. An outcome
of this work is the specification of a multiple agent system where a query agent interacts with multiple repository agents to perform business-oriented service composition.
Keywords: Semantic Web Services, Agents, Web
Service Composition, Service-Oriented Architecture.

1. Introduction
Service-oriented computing is changing the way software applications are being designed, developed, delivered, and consumed [13]. A Web service is an
autonomous, platform-independent computational element that can be described, published, discovered and
accessed over the web using standard web protocols.
Sample of Web services include flight, hotel, rental car
reservation services or device controls like sensors or
satellites. As automation increases, these services will
be accessed directly by applications rather than by humans [5].
978-0-7695-3315-5/08 $25.00 © 2008 IEEE
DOI 10.1109/WETICE.2008.19

Gopal Gupta
Department of Computer Science,
The University of Texas at Dallas
Richardson, TX 75083

2. Related Work
Composition of Web services has been active area
of research recently [11, 12]. Most of these approaches
present techniques to solve one or more phases listed
in Section 1. There are many approaches [8, 9, 10] that
12

3. Web Service Composition

solve the first two phases of composition namely planning and discovery. These are based on capturing the
formal semantics of the service using action description languages or some kind of logic (e.g., description
logic). The service composition problem is reduced to
a planning problem where the sub-services constitute
atomic actions and the overall service desired is represented by the goal to be achieved using some combination of atomic actions. A planner is then used to
determine the combination of actions needed to reach
the goal. With this approach an explicit goal definition has to be provided, whereas such explicit goals
are usually not available. The authors in [8] present
a composition technique by applying logical inferencing on pre-defined plan templates. There are industry
solutions based on WSDL and BPEL4WS where the
composition flow is obtained manually. BPEL4WS can
be used to define a new Web service by composing a set
of existing ones. It does not assemble complex flows of
atomic services based on a search process. They select
appropriate services using a planner when an explicit
flow is provided. In contrast, our agent-based technique automatically determines these complex flows using semantic descriptions of atomic services distributed
across various repositories.

A Web service can be regarded as a “programmatic
interface” that makes application to application communication possible. Informally, a service is characterized by its input parameters, the outputs it produces,
and the actions that it initiates. The input parameter may be further subject to some pre-conditions, and
likewise, the outputs produced may have to satisfy certain post-conditions. Given a repository (or a set of
repositories) of service descriptions, and a query with
the requirements of the requested service, the composition problem involves, in case a matching service is not
found, automatically finding a directed acyclic graph of
services that can be composed in correct order of execution to obtain the desired service. Figure 1 shows an example composite service made up of five services S1 to
S5 . In the figure, I 0 and CI 0 are the query input parameters and pre-conditions respectively. O0 and CO0 are
the query output parameters and post-conditions respectively. Informally, the directed arc between nodes
Si and Sj indicates that outputs of Si constitute (some
of) the inputs of Sj .

Shenghua et. al. [2] propose an agent-based solution
to web service composition problem using JADE and
JXTA. The approach presented achieves automation in
composing services but it is shown to handle only the
sequential case of composition problem. It falls short
of handling the general case of non-sequential composition with conditions. Our approach produces compositions that are not only sequential but also nonsequential that can be represented in the form of a
directed acyclic graph (DAG). Buhler et. al. [1] use
multi-agents to solve the more general case of web service composition problem. The approach presented by
them works in the distributed environment as well. But
their approach is syntactic in nature and does not make
use of the semantics provided in the descriptions of
web services. So their approach is limited to syntactic
composition whereas our approach works for the composition of Semantic Web services as well. A processlevel composition solution based on OWL-S is proposed
in [9]. In this work the authors assume that they already have the appropriate individual services involved
in the composition, i.e., they are not automatically discovered. They use the descriptions of these individual
services to produce a process-level description of the
composite service. In contrast, we present a technique
that automatically finds the services that are suitable
for composition based on the query requirements for
the new composed service.

Figure 1. Composite Service as a Directed
Acyclic Graph
Next we illustrate the composition problem by presenting an example of a non-sequential composition
with if-then-else conditions i.e., the composition flow
varies depending on the result of the post-conditions
of a service. We believe that the non-sequential conditional composition is the most general case of the
composition problem.
Example: Suppose we are looking for a service to
make international travel arrangements. We first need
to make a tentative flight and hotel reservation and
then apply for a visa. If the visa is approved, we can
buy the flight ticket and confirm the hotel reservation,
else we will have to cancel both the reservations. Also
if the visa is approved, we need to make a car reservation. The repository contains services ReserveFlight,
ReserveHotel, ProcessVisa, ConfirmFlight, ConfirmHotel, ReserveCar, CancelFlight, and CancelHotel. Table
1 shows the input/output parameters of the user-query
and the services.

13

Service

PreConditions

Query
Reserve
Flight
Reserve
Hotel
Process
Visa
ConfirmFlight
ConfirmHotel
CancelFlight
CancelHotel
Reserve
Car

VisaApproved
VisaApproved
VisaDenied
VisaDenied

Input Parameters

Output Parameters

PasngrName,OriginArprt,DestArpt,StartDate,ReturnDate
PasngrName,OriginArprt,DestArpt,StartDate,ReturnDate
PasngrName, StartDate,
ReturnDate
PasngrName,VisaType,
FlightConfNum,HotelConfNum
FlightConfNum,CreditCardNum
HotelConfNum,CreditCardNum
PasngrName,FlightConfNum
PasngrName,HotelConfNum
PasngrName,ArrivalDate
ArrivalFlightNum

FlightConfNum,HotelConfNum,CarConfNum
FlightConfNum

PostConditions

HotelConfNum
ConfirmationNum

VisaApproved
∨ VisaDenied

ArrivalFlightNum
ConfirmationNum
CancelCode
CancelCode
CarConfirmNum

Table 1. Example Scenario of (Non-Sequential Conditional) Web Service Composition
effect, AO is the affected object, O is the output list,
and CO is the list of post-conditions. The pre- and
post-conditions are ground logical predicates.

In this example, service ProcessVisa produces the
post-condition VisaApproved ∨ VisaDenied. The services ConfirmFlight and ConfirmHotel have the precondition VisaApproved. In this case, one cannot determine if the post-conditions of service ProcessVisa implies the pre-conditions of services ConfirmFlight and
ConfirmHotel until the services are actually executed.
In such a case, a condition can be generated which will
be evaluated at runtime and depending on the outcome of the condition, the corresponding services will
be executed. The vertex VisaApproved in the graph
is a condtional node with the outgoing edges representing the generated conditions and the outputs. In
this case conditions (VisaApproved ∨ VisaDenied) ⇒
VisaApproved and (VisaApproved ∨ VisaDenied) ⇒
VisaDenied are generated. Depending on which condition holds, the corresponding services ConfirmFlight or
CancelFlight are executed. Figure 2 shows this conditional composition example as an conditional directed
acyclic graph.
Next, we present our formalization of the generalized
composition problem. This generalization is an extension of our previous notion of composition [4] which can
handle non-sequential conditional composition (which
we believe is the most general case of composition).

Definition (Query): The query service is defined as
Q = (CI 0 , I 0 , A0 , AO0 , O0 , CO0 ) where CI 0 is the list of
pre-conditions, I 0 is the input list, A0 is the service
affect, AO0 is the affected object, O0 is the output list,
and CO0 is the list of post-conditions. These are all the
parameters of the requested service.
Definition (Generalized Composition): The generalized Composition problem can be defined as automatically finding a directed acyclic graph G = (V, E)
of services from repository R, given query Q =
(CI 0 , I 0 , A0 , AO0 , O0 , CO0 ), where V is the set of vertices and E is the set of edges of the graph. Each vertex in the graph either represents a service involved
in the composition or post-condition of the immediate
predecessor service in the graph, whose outcome can
be determined only after the execution of the service.
Each outgoing edge of a node (service) represents the
outputs and post-conditions produced by the service.
Each incoming edge of a node represents the inputs and
pre-conditions of the service. The following conditions
should hold on the nodes of the graph:
1. ∀i Si ∈ V where Si has exactly one incoming
edge that represents
the query inputs and preS
conditions, I 0 w i I i , CI 0 ⇒∧i CI i .
2. ∀i Si ∈ V where Si has exactly one outgoing
edge that represents
S the query outputs and postconditions, O0 v i Oi , CO0 ⇐∧i COi .
3. ∀i Si ∈ V where Si represents a service and has
at least one incoming edge, let Si1 , Si2 , ..., Sim be

Definition (Repository of Services): Repository
(R) is a set of Web services.
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs
and post-conditions. S = (CI, I, A, AO, O, CO) is the
representation of a service where CI is the list of preconditions, I is the input list, A is the service’s side-

14

Figure 2. Example Web Service Composition
the nodes such that there is a directed
Sedge from
each of these nodes to Si . Then Ii v k Oik ∪ I 0 ,
CI i ⇐ (COi1 ∧COi2 ... ∧ COim ∧ CI 0 ).
4. ∀i Si ∈ V where Si represents a condition that is
evaluated at run-time and has exactly one incoming edge, let Sj be its immediate predecessor node
such that there is a directed edge from Sj to Si .
Then the inputs and pre-conditions at node Si are
Ii = Oj ∪I 0 , CI i = COj . The outgoing edges from
Si represent the outputs that are same as the inputs Ii and the post-conditions that are the result
of the condition evaluation at run-time.
The meaning of the v is the subsumption (subsumes)
relation and ⇒ is the implication relation. In other
words, a service at any stage in the composition can
potentially have as its inputs all the outputs from its
predecessors as well as the query inputs. The services
in the first stage of composition can only use the query
inputs. The union of the outputs produced by the services in the last stage of composition should contain all
the outputs required by the query.
Next, we present our agent-based approach to solve
the above described generalized composition problem.

agents to realize a solution for the generalized composition problem, that works in presence of decentralized
repositories over a distributed environment.
We
introduce two types of agents to perform composition.
• Repository Agent: Each repository will be
accompanied by a Repository agent that has
knowledge of all the services in its respective
repository and their semantic descriptions. This
agent perceives its environment for two kinds of
requests: (i) any updates to the services in the
repository (any new service being added or an
old service being deleted), (ii) a request for all
services that can be invoked by a set of inputs and
pre-conditions. The Repository agent may have
the ability to discover services (based on exact
match) but does not have an ability to perform
composition of services.
• Query Agent: The Query agent performs the
composition task by communicating with various
repository agents for the services involved in the
composition. This agent perceives its environment
for composition query requests. Once it receives
a query request, it requests the various repository
agents for the services that can be invoked using
the available query inputs. Upon receiving the information of the services, the query agent computes the set of outputs of all these services and
checks if the query outputs are satisfied by this
set. If yes, then it filters out the redundant services that do not contribute to the query outputs.
If no, then it adds this set of outputs to the set

4. Agent-based Service Composition
Our previous work presented in [4] solves the
automatic Web service composition problem using a
centralized repository. With the service domain being
inherently distributed in nature, in the real-world it
would be a wishful thinking to imagine a centralized
repository for a particular or all the domains. In this
section we extend our previous work with the use of

15

repositories at multiple stages. Figure 4 shows the composition technique for the particular instance shown in
Figure 1. The query agent performs the composition
task upon receiving a composite service request from
the user. The query agent starts with the query input
parameters. It contacts the repository agents to find all
those services from their respective repositories which
can be satisfied with a subset of the query input parameters and whose pre-conditions can be implied by the
query pre-conditions. In Figure 4, CI, I are the preconditions and the input parameters provided by the
query. S1 and S2 are the services found after step 1.
O1 is the union of all outputs produced by the services
at the first stage. For the next stage, the inputs available are the query input parameters and all the outputs
produced by all the services in the previous stage, i.e.,
I2 = O1 ∪ I. I2 is used by the query agent to find
services at the next stage, i.e., it queries the repository
agents again for all those services that require a subset
of I2 . In order to make sure we do not end up in cycles,
the query agent keeps only those services which require
at least one parameter from the outputs produced in
the previous stage. This filtering continues until all
the query output parameters are produced. At this
point the query agent makes another pass in the reverse direction to remove redundant services which do
not directly or indirectly contribute to the query output parameters. Once the query agent has obtained
all the services required to produce the solution for the
query, it puts them in the correct order of execution to
produce the composite service. The generalized composition algorithm performed by the query agent is shown
in table 2. The repository agent, on the other hand,
performs nothing more than a service matching task.
It provides the query agent all the services that can be
satisfied with the input parameters and pre-conditions
provided by the query agent.

Figure 3. Agent-based Web Service Composition

of query inputs and sends out new requests to the
repository agents with this new query input set.
It keeps on repeating this until the set of outputs
of the services returned by the repository agent
subsumes the query output set. At this point the
query agent performs a backward pass through all
the services it has collected so far at various stages
to filter out redundant services that do not contribute directly or indirectly to the query outputs.
The query agent itself does not have any information about the services but it does have the ability
to solve the composition problem by collecting services from the repositories (via repository agents)
and putting them together in the correct order of
execution to provide a composite service.
Next, we present our agent-based generalized composition algorithm.

5. Conclusions and Future Work
In this paper we present an agent-based approach for
automatic service composition that extends the common practice of using centralized repositories for Web
service composition by introducing the use of agents
for multiple repositories in a distributed environment.
Furthermore, we introduce a query agent that interacts
with the repository agents to obtain services that can
be composed to produce the requested composite service. Our current implementation of this approach is at
a prototype stage. Our future work includes building
a full-fledged agent-based generalized composition engine and evaluating it using decentralized repositories
in a distributed enviroment.

4.1. Agent-based Generalized Composition
Algorithm
In order to produce the composite service which is
the graph, as shown in the example Figure 1, our agentbased approach selects relevant services from multiple

16

Figure 4. Multi-step Narrowing Technique for Web Service Composition
[3] S. McIlraith, T.C. Son, H. Zeng. Semantic Web
Services. In IEEE Intelligent Systems Vol. 16, Issue
2, pp. 46-53, March 2001.
[4] S. Kona, A. Bansal, and G. Gupta. Automatic
Composition of Semantic Web Services. In Intl.
Conf. on Web Services (ICWS) pp 150-158, 2007.
[5] A. Bansal, K. Patel, G. Gupta, B. Raghavachari,
E. Harris, and J. Staves. Towards Intelligent Services: A case study in chemical emergency response. In ICWS, pp.751-758, 2005.
[6] D. Mandell, S. McIlraith Adapting BPEL4WS for
the Semantic Web: The Bottom-Up Approach to
Web Service Interoperation. In ISWC, 2003.
[7] M. Paolucci, T. Kawamura, T. Payne, and
K. Sycara Semantic Matching of Web Service Capabilities. In ISWC, pages 333-347, 2002.
[8] S. McIlraith, T. Son Adapting golog for composition of semantic Web services. In KRR, pp.482–
493, 2002.
[9] M. Pistore, P. Roberti, and P. Traverso ProcessLevel Composition of Executable Web Services In
European Semantic Web Conf., pp 62-77, 2005.
[10] J. Rao, D. Dimitrov, P. Hofmann, and N. Sadeh
A Mixed-Initiative Approach to Semantic Web Service Discovery and Composition In International
Conference on Web Services, 2006.
[11] J. Rao, X. Su. A Survey of Automated Web Service Composition Methods In Workshop on Semantic Web Services and Web Process Composition, 2004.
[12] J. Cardoso, A. Sheth. Semantic Web Services,
Processes and Applications. Springer, 2006.
[13] G. Castagna, N. Gesbert, L. Padovani, et al. A
Theory of Contracts for Web Services. In Symposium on Principles of Programming Languages,
January 2008

Algorithm: Query Agent
Input: QI - QueryInputs, QO - QueryOutputs,
QCI - Pre-Cond, QCO - Post-Cond
Output: Result - ListOfServices

1. L ← CollectServicesFromRepositoryAgents(QI,
QCI);
2. O ← GetAllOutputParameters(L);
3. CO ← GetAllPostConditions(L);
4. While Not (O w QO)
5.
I = QI ∪ O; CI ← QCI ∧ CO;
6.
L’ ← CollectServicesFromRepositoryAgents(I,
CI);
7. End While;
8. Result ← RemoveRedundantServices(QO, QCO);
9. Return Result;

Table 2. Agent-based Generalized Composition Algorithm

References
[1] P. Buhler, D. Greenwood, A. Reitbauer. A Multiagent Web Service Composition Engine. In Proceedings of First International Workshop on Engineering Service Compositions (WESC), 2005.
[2] S. Liu, P. Kungas, M. Matskin. Agent-Based Web
Service Composition with JADE and JXTA. In
Proceedings of the 2006 International Conference
on Semantic Web and Web Services, SWWS 2006,
Las Vegas, Nevada, USA, June 26-29, 2006.

17

IEICE TRANS. INF. & SYST., VOL.E91-D,

NO .10 OCTOBER

2008
2449

PAPER

Access

Control

Management

Seng-Phil

for SCADA

HONG•õa),

Member,

SUMMARY The informationtechnologyrevolutionhas transformed
all aspectsof our siety includingcriticalinfrastructuresand led a significantshiftfromtheir old anddisparatebusinessmodelsbasedonproprietaryandlegacyenvironmentsto more openandconsolidatedones.SupervisoryControlandData Acquisition(SCADA)systemshavebeen widely
used not only for industrialprocessesbut also for some experimentalfacilities. Due to the natureof open environments,managingSCADAsystems shouldmeet varioussecurityrequirementssince systemadministrators need to deal with a large number of entitiesand functionsinvolved
in criticalinfrastructures.In this paper,we identifynecessaryaccesscontrol requirementsin SCADAsystemsand articulateaccesscontrolpolicies
for the simulatedSCADAsystems. We also attemptto analyzeand realize those requirementsand policiesin the contextof role-basedaccess
control that is suitablefor simplifyingadministrativetasks in largescale
enterprises.
key words: accesscontrol,securitypolicy,supervisorycontrol and data
acquisition(SCADA).
1.

Introduction

Supervisory control and data acquisition (SCADA) systems
are used not only for critical infrastructures such as telecommunication, transportation, power generation and distribution, but also for some experimental facilities such as nuclear fusion. By allowing collection & analysis of data and
control of equipments, SCADA systems have been widely
deployed for managing critical infrastructures.
Recently,
critical infrastructures have transited from their old and disparate business models based on proprietary and legacy environments to a new, consolidated ones based on open environments such as Internet-based distributed environments.
Due to the nature of open environments, the security of
SCADA systems is inherently a challenging problem. We
also observed the cyber threat in the SCADA systems is one
of critical incidents that made significant impact to our society [28]. In addition, managing SCADA systems is a cmcial task, requiring system administrators to access and control a large number of entities and functions involved in distributed critical infrastructures. Consequently, it also results
in the complexity of security policies and creates a demand
to seek a systematic way for articulating, analyzing and simManuscript received October 26, 2007.
The author is with Sungshin Women's University, 136-742,
249-1 Dongdeon-Dong, 3 Ga, SeongBuk-Gu, Seoul, Korea.
Corresponding Author. The author is with Arizona State University, Tempe, Arizona, 85287, USA.
The author is with University of North Carolina at Charlotte,
Charlotte, North Carolina, 28223, USA.
a) E-mail: philhong@sungshin.ac.kr
DOI: 10.1093/ietisy/e91-d.10.2449
Copyright (c)

2008 The Institute

of Electronics,

Gail-loon

Systems

AHN•õ•õ,

and

Wenjuan

XU•õ•õ•õ, Nonmembers

ulating such policies.
In general, security policies are developed to protect the critical assets in SCADA systems along with corresponding security mechanisms.
There are many elements that are part of an enterprise-wide security policy.
Few papers introduced high level frameworks for managing SCADA security policies [29]-[31]. However, none of
those works provided a systematic way to articulate all the
elements needed for specifying and enforcing security policies. In this paper, we focus on access control policies for
SCADA resources which are one of crucial security mechanisms to administer system entities and assets in critical infrastructures. Access control policies for SCADA systems
start by identifying critical and important resources, then
determining who can access the identified resources, and
knowing exactly what kind of access should be granted. In
addition, appropriate credential attributes such as identities
and roles within SCADA systems need to be defined. Also,
we need to define the type of access to critical resources,
activities, and operations based on such attributes.
Our approach is generic and policy-neural so that each
organization using SCADA systems only needs to customize our model to fit their own specific access control requirements such as roles and corresponding privileges based
on organizational characteristics. To achieve this goal, we
adopt role-based access control (RBAC) as a basis of our
approach. One of the main design principles in RBAC is to
minimize potential insider threats by providing an effective
control over users' access to applications, information, and
resources with the notion of role [20]. We utilize modeling
and analysis features of Colored Petri Net and Unified Modeling Language to support our approach. In our framework,
we first identify necessary access control requirements in
SCADA systems. Then, we articulate and analyze access
control policies for the simulated SCADA systems. We also
attempt to realize those requirements and policies in the context of role-based access control that is suitable for simplifying administrative tasks in large scale enterprises.
This paper is organized as follows. In Sect. 2, we
overview related works and background technologies. Section 3 discusses our access management framework for
SCADA systems as well as how role-based policies can be
modelled and analyzed in CPN and UML. Section 4 concludes the paper.

Information

and Communication

Engineers

IEICE

TRANS.

INF.

& SYST.,

VOL.E91-D,

NO.10

OCTOBER

2008

2450

2.

Related Works and Background Technologies

In this section, we briefly overview related technologies and
prior works related to our approach.
2.1

Supervisory Control and Data Acquisitions (SCADA)

SCADA systems [7] are based on a real-time process control system that consists of modern microprocessors and network technologies. It is used to monitor and control a plant
or equipment in industries such as telecommunications, water and waste control, oil and gas refining, and transportation. SCADA systems normally gather information, transfer back to a central site, and carry out necessary analyses [8]. It is composed of four major components such as
operation terminal, control center, remote stations and field
instruments as shown in Fig. 1.
Operation terminal is the place where a client operates
to view information and send commands. Control center
is the core component of SCADA systems. It mainly includes master station, historian database and engineering
workstation: master station is responsible for monitoring,
configuring and operating field devices through RTUs; historian database is used to store the data generated during
the operation; and engineering workstation is the platform
for engineers to work on. Remote stations are consisted of
equipments that are co-located or directly interfaced with
the field instrumentations to either provide control functions
or handle data back to the control center. Field instrumentations are devices that are connected to the equipment being
controlled and monitored by SCADA systems. In this paper, we would derive access control components including
users, roles, objects, operations and permissions from such
SCADA components and their operations.
2.2 Colored Petri Nets (CPN)
CPN is a well-known tool based on a powerful graph theory

in the system modeling area [13], [14]. Petri nets include
three basic components: places, transitions and arcs [15] .
The formal definitions of CPN are given in Table 1. The
types determine data attributes and operations that used in
arc, guards, and initialization expression functions. The arc
expressions specify a collection of token, which are added to
or removed from the places. If each transition's input place
contains at least one token that is equal to the corresponding arc expression then the transition is enabled. In general,
there are two main analysis methods in CPN, called simulation and state space analysis for execution of CPN models
and construction of directed graph, respectively.
There are few works on CPN-based analysis for access
control policies. Mandatory access control policies was validated using CPN in [2], [3]. Knorr et al. [4] adopted CPN
to analyze separation of duty policies in workflow systems
and Shafiq et al. [5] presented a role-based access control
policy verification framework using CPN. Also, Sampaio et
al, simulated the process flow of SCADA systems [6]. Although these prior works attempted to verify certain policies, their works lack a systematic framework to identify,
simulate and analyze access control policies.
2.3

Unified Modeling Language (UML)

UML is a general-purpose visual modeling language in
which we can specify, visualize, and document the components of a software system. It captures decisions and understanding about systems that must be constructed [11]. It has
been a standard language in the field of software engineering.
The UML consists of use case modeling, static modeling, and dynamic modeling. In use case modeling, the
functional requirements of systems are specified with use
cases and actors. A use case is initiated by actors and it defines interactions between the actors and the systems. Static
modeling provides a structural view of information in the
systems. In such a view, classes are defined in terms of attributes as well as relationships with other classes. The relationships include association, generalization/specialization,

Table 1
CPN=(ƒ°,

Formal definition of colored Petri nets (CPN).
P, T, A, N, C, G, E, I),

ƒ° is a finite

set

of non-empty

P

is a finite

set

of places.

T

is a finite

set

of transitions.

A

is a finite

set

of arcs,

where:
types,

where

also

called

color

sets.

P•¿T=P•¿A=

T•¿A=0
N

is a node

function,

where

C

is a color

function,

C:

G

is a guard
t•¸T:

function,

N:

A•¨P•~T•¿T•~P

P•¨ƒ°

G:

T•¨expressions,

such

that •Í

[Type(G(t))=B•ÈType(Var(G(t)))

•ºƒ°]
E

is an

arc expression

such

that •Ía•¸A:

function,

Type(Var(E(a)))•ºƒ°],
I

is an

initialization

where

Fig. 1

Overview

of SCADA

systems.

P:

A•¨expressions,

p is the

function, I

: P•¨closedexpressions,
•Íp•¸

E:

[Type(E(a))=C(p)MS•È

[TYPe(I(P))=C(p)MS]

and

place

of N(a).

HONG et al.: ACCESS CONTROL

MANAGEMENT

FOR SCADA SYSTEMS
2451

model
are

has

the

following

formalized

from

•EU is a set

of

•ER

is

components
the

above

and

these

discussions

components

.

users,

disjoint

sets

of

roles

and

administrative

roles

re-

spectively,
•EP is disjoint

sets

of permissions

and

administrative

per-

missions,
•E

UA•ºU•~R,

is a many-to-many

user

to role

assignment

relation,
•E

PA•ºP•~R
ment

•E
Fig. 2

RBAC

RH•ºR•~R

model.

aggregation

of

classes.

Dynamic

infix

modeling

shows

a set

•E user:

a be-

view

of the

collaboration
Object

systems.

diagrams,

collaboration

developed

to

to

the

It can

diagrams,

diagrams

show

be described

sequence

how

and

objects

with
or

sequence

are

defined

use

in

cases.

statecharts

dependent

introduced

[10].

in

[9],

In

[11],

this

with

each

views

of

paper,

we

are

Access

is

an

alternative

access

control

(MAC)

(DAC)

[22],

arena,

[23].

the

objects

such

is

the

that

over

else

is

based
policy

of

can

an

is

other

UML

the

on

level

[16].
has

that

object

classical

the

The

In

control

by

general

model

family

Sandhu

is

a

in

of RBAC
al [20].

this

family.

about

various

design

ily of

models

is given

main

[18],

of

idea

of

discretionary

models

[21].

But

DAC

RBAC
specify
struc-

RBAC96

2 illustrates

Motivation
made

ations

regarding
Figure

distributed

2 shows

[19],

in

was

the

and

most

access

to data

being

or

an

sented

3.

and

by

using

title

mantics

within

regarding

Also,

and

resources.

this

fam-

are

vari-

that

reg-

there

organization

the

authority

This

section

a member

of the

particular
tem

mode

or

are

some

in
the

implicitly
y.

The

a user

a role
with

and

is

a job

some
or

some

associated

responsibility

of

relates

that
of
by

y.

a user
roles
means

objects

or

role

In

such

he

or
the

specified
so that

of

user

establishes

of

more

out

one

that

session

si

r")•¸

stipulating

of

the

which

RBAC

model

are

she
role

y.

in

our

jor

in

we
possibly

a

session

is

a member

hierarchy).

CPN

RBAC

this
control

and

formal
require-

UML.

model

Management

Our

could

be

prior
repre-

shown

Articulating
all
and

control

control

elicit

for

components
has

three

issues

ma-

in SCADA

3.

contexts

requirements:

and

with

We

components

necessary

associated

framework

necessary

framework

access

Fig.

access

identify

all
Our

with

in

SCADA

management

elaborate

to cope

as

for

an access
We

framework.

objectives

obviously

in

access

those

required

components,

tions,

of a

detailed

the

of
say

sys-

then

role

x are

also

x is
many

and
of
The

in

identify

need

SCADA

control

require-

components.

to

gather

software

procedures

users

this

process,

tem
trol

each

which

which

Roles

if x•†y

to

accommodate
access

using
how

describes

empirical

to

se-

actions.

Members

cases,

to

Therefore,

information

and

such

of

corresponding

as

each

function,

funcand

users

conferred

one

order•†,

session
is

subset

indirectly

a partial

that

is a hu-

is an approval

carry

permissions

so

function

a permission

to

to

members

Each
idea

and

access

privilege

organized

x inherits

to

of

role,

si to
r')•¸

time)

analyzing

systems

systems.

involved
on

session

UML.

systems

permissions

Intuitively,
agent,

the

each

constraints

attempt
and

Control

system
or job

of

demonstrated

Access

[24].

roles

autonomous

with

components

we

SCADA

it is
man

change

various

paper,

ments
ulate

mapping

or forbidden.

[27]

systems

discussion

developing

[20].

systems

(regular)

can

modeling

of

work

•E
[17],

si to

session's

authority

called

Figure

decisions
in

is a function

collection

of

in

ments

defense

classification

of the subjects
and
can
maps
to an organizational

et

the

and

SCADA
A general

session

for

permissions •¾r•¸roles(si){P|(•Îr"•…r)[(p,

this

model

ture.

defined

each

is constant

roles(si)•º{r|(•Îr'•†r)[(user(si),

(which

has

mandatory
access

in the

based

object

role
that

mapping

and

objects

use

traditional

used

is

access

on the
in a way

to

discretionary

MAC
access

as top-secret
owner

who

policy
security

of

(written

(RBAC)

policy

As

policy

hierarchies

[12].

Control

and

user(si)

roles

allowed
RBAC

role

is a function
user

of

values
Role-Based

ordered

sessions,

S•¨2R

a set

•Ethere
2.4

assign-

lifetime,
•E roles:

PA]},
notations

to role

object

statecharts.

diagrams

collaborate

State

of

single

UA]}
execute

permission

notation),

S•¨U,

the
havioral

is partially

as•†in
•ES is

and

is many-to-many

relations,

context.
functions

are

to

we

use

modeling

tool,

be

such

need

to

regulated

UML,
to

With

elaborate

we

controlled

and

be

by

which

information,

certain

rules.

For

is a well-known
identified

sys-

access

con-

requirements.

senior
roles.

•E

Defining

access

activates

cess

(directly

In this

objective,

control

model

RBAC

control

control

policies
we
and

from
first
attempt

policies:
access
examine
to realize

Then
control

we

an appropriate
such

derive

ac-

requirements.
access

requirements

IEICE

TRANS.

2452

Fig. 3

Access

control

in the context of a particular access control model. Like
we discussed in the previous section, RBAC model is
used to articulate such requirements.
We utilize CPN
to build executable system modeling components
to reflect role-based policies for SCADA systems as well
as RBAC model that is a basis for access control policies. The CPN-based representation
enables us to explore and simulate all control flows and relevant
control policies in SCADA systems.
•E

Analyzing

access

control

policies:

Finally,

we

& SYST ., VOL.E91-D,

NO.10

OCTOBER

2008

framework.

Table

2

Monitor

against

limit values.

access

need

to analyze access control policies and detect possible policy conflicts. We leverage the features of state
space analysis in CPN to construct a directed occurrence graph that has a node for each reachable marking
and an arc for binding elements and to execute CPNbased policy expressions. At the end of simulation, the
user's operations on system objects are validated and
displayed. From viewing this, we can systematically
find the situation that violates access control requirements. After discovering issues, the occurrence graph
corresponding to these objects is generated, indicating
the problems of policies. Therefore, we can accommodate the appropriate assurance level to some extent by
validating policies before they are activated.
3.1

management

INF.

Articulating Access Control Requirement in SCADA

To define access control requirements of SCADA, we need
to gather sufficient information of SCADA systems and
identify all components entitled with access control issues.
Inevitably, we need to describe an overview of each component in SCADA systems. There exist five core software components: human machine interaction (HMI) that
provides interfaces for system analyzers and system operators, TCP/IP communication protocol that provides the
network connectivity between clients and operation terminals, control center subsystems (CCS) that maintains several
databases for historical system data, current system data,

and alarm message. Also, CCS includes supervisory control subsystems that are responsible for controlling the operations of each component, graphic builder subsystems for
generating graphical results, and run-time subsystems that
are responsible for getting messages from HMI and then
distributing them to appropriate software modules as well
as run-time configuration & maintenance modules.
Also, we identify corresponding functional elements
in each component such as monitoring & alarming, controlling, and configuring & maintaining. Monitoring and
alarming elements support monitoring trends, monitoring
against limit values, and monitoring automatic changes in
critical pieces of an equipment. Also, such monitoring capabilities use predefined values to monitor the status of critical
equipments. For instance, it monitors the water level and
sends a signal to the control center and operators in order
to indicate recent changes of water level. Also, it also generates an alarm message when the water level exceeds the
predefined limit. Based on control characteristics on the devices, controlling elements have three different modes such
as simple control, complex control and sequential control.
For example, a simple control provides functions to close
and open the gate of reservoir. Configuring and maintaining
elements are to configure SCADA systems such as handling
a programming module for RTU operations and checking
the system equipments periodically.
With given information of system components and
functional elements, we focus on the following functional

HONG

et al.: ACCESS

CONTROL

MANAGEMENT

FOR

SCADA

SYSTEMS

2453

(a) Use case diagram of monitor against limit values function.

(b) Sequence diagram of monitor against limit values function.
Fig. 4

Analysis of SCADA systems in UML.

element to examine and analyze access control requirements: monitor against limit values which is part of the
monitoring and alarming element. Due to the page limit, we
use this component to explain all other tasks in our approach
throughout the rest of this paper. We first articulate several

privileges as shown in Table 2. We also identify several corresponding users such as senior operator, junior operator,
super operator, engineer and technician. Each user plays
different role(s) in SCADA systems. We represent our analysis results using use case and sequence diagrams in UML

IEICE TRANS. INF. & SYST., VOL.E91-D,

NO.10 OCTOBER

2008

2454

as

shown

in Fig.

From
quirements
partially
•E

the
•E

•E

the

analysis

for

the

list
Any

Table

4.
above,

monitor

such

requirements

legitimate
ability

Critical

view

users

such

problems
Different

as

knowledged
for

sages,
trolling

be

senior

or

updated

and

not

for

updating

values

control

against

limit values

shown

in Table

function

.

re-

element.

systems

as

changing

performed

in

to

tasks.

acknowledge
but

SCADA

We

should

to
SCADA
should
users

For

maintenance
other

messages
SCADA

alarm
only

operators

occurring
messages

corresponding

should

in

such

should

from
alarm

access

limit

for monitor

have

screen;

functions,
value,

define

Permissions

as follows:

users

to

limit

we

against

3

by

prevent

points

and

authorized
unexpected

systems;
and
be accordingly
who

are

example,

a technician

related
such

ac-

responsible

alarm

as tasks

mesin

con-

systems.
limit

3.2 Defining Access Control Policies in SCADA

values

straints

function

are

requirements

In this section, we define access control policies in SCADA
systems based on the requirements identified in section 3.1.
As we mentioned earlier, we attempt to express access control policies in the context of RBAC. Obviously, we need
to go through an engineering process to identify roles in
the system. A role can be defined as a set of actions and
responsibilities associated with a particular working activity. Then, instead of specifying all the access each user
is allowed to execute, access authorizations on objects are
specified for roles. Since roles in an organization are relatively persistent with respect to user turn over and task reassignment, RBAC provides a powerful mechanism for reducing the complexity, cost, and the potential for error of
assigning users permissions within the organization. Because the roles within an organization typically have overlapping permissions, RBAC models include features to establish role hierarchies, where a given role can include all
of permissions of another role. Another fundamental aspect
of RBAC is constraints. Constraints are an important aspect
of access control and are a powerful mechanism for laying
out higher level organizational policy. There are the major classes of constraints in RBAC such as Prohibition Constraints including separation of duty policies and Obligation
Constraints including prerequisite policies as we identified
before in [25], [26].
Based on our analysis for the monitor against limit values function, we apply our engineering steps and identify
the following roles that can be assigned to authorized users
in SCADA systems: junior operator, engineer, technician,
senior operator and super operator. Role hierarchy is a
natural means to reflect an organizational structure of authority and responsibility. To specify the permissions, we
first characterize several operations such as create, read,
update, delete and execute. Then we associate those operations with system objects. Permissions are represented as an
abstract form of such associations. All permissions and corresponding operations and objects for the monitor against

First,

(a)
and

super

sage•h

Duty

Duty

addition,

all

Each

tion

needs

tecture.

to

same

role,

provide

user
Sep-

and

called

(b)

alarm

mes-

Static

Sep-

(SSOD_CP).

prerequisite

policies

should

called

as

same

Static

Permissions

is validated

be

such

a prerequi-

Prerequisite

module

Permis-

to

contact

components
be

for

nents

in

such

the

to

For

brevity,

tional

as

users,

all

this

transition

checks

operations

that

request
responsible

paper
and

a user's
the

to the

privileges,

user

can

control
for

on

center

making

center.
an

in

and
the

two

components:
Operation
selects
and

Control
authorization

RH

need

compo-

Fig.

simulate

context

to

policies.
RBAC

is

units.

execute,

in fact

and

components

trigwith

is

system

discussed

focuses

operare

modifying
between

operations

control

permissions

model

all

such

in

archi-

session(s),

request

permissions,

requirements

sys-

communicate

articulate

for

SCADA

in

subsystems

control

roles,

the

authoriza-

system

the

eventually

we

of

appropriate
performs

architecture

control

terminal

roles

viewing
and
relationships

representation

identify

access

in the

access

Besides,
SCADA

CPN-based

fied

that

defining,
and
detailed

included.

flow

functional

RTUs

on

flow

assigned

module

Simulating

provide
a way
The
transitions

based

1. The

determines

corresponding

devices.

enforced

system

activates

subsystem

Then,

field

and
in Fig.

the

a user

decision

gered

then

called
(SSOD_CR)

permission•h

emulate

run-time

ations.

tion

to

discussed

When

policy

out

to the

such
engineer,

to the

other,
Roles

permissions,

policy

architecture

to

in place

technician,

assigned

to

objects.

(PRE_Y).

tem

and

on

about •gacknowledge

screen

other

be
each

Conflicting

need

alarm

for

cannot

assigned

be

as

control

components.

operation

should

Conflicting

with

we

that •gview

each

con-

access

RBAC

such

conflicts

with

In addition,

appropriately

policies
roles

be assigned

of

site

for

permissions

cannot

sions

be

roles

roles

conflicting

aration

identified

duty

operator

of

the

three

3.

role-based

should

responsible
of

these

aration
the

on

following

because

In

are

separation
the

represent

permissions

that

Also,

to

based

all

role(s)

are

needed

1.

The

carried
identi-

of

RBAC.
operaterminal

corresponding
sends

center

the

activa-

transition
decision,

is
deal-

HONG

et al.: ACCESS

CONTROL

MANAGEMENT

FOR

SCADA

SYSTEMS

2455

(a) SCADA policy expression in CPN.

(b) Control center in CPN.
Fig. 5

SCADA Systems in CPN.

ing with all necessary activities for the requested operation
and exchanging information from TCP/IP Communication
and I/O Driver. Figure 5 shows the above-mentioned components and functional flows in SCADA systems.
3.3

Analyzing Access Control Policies

In this section, we validate and verify policies that our approach identified for SCADA systems through simulation
and state space analysis. To pursue state space analysis, we

consider several components as follows:
1. Statistics information: The statistics information ensures that there does not exist an infinite occurrence
graph in RBAC simulation.
2. Boundary properties: It shows that all places have upper and lower bounds so that the occurrence graph is
bounded. Therefore, RBAC in the occurrence graph is
represented by a finite list of operations.
3. Home properties: For simulating RBAC, it is not desir-

IEICE TRANS. INF. & SYST., VOL.E91-D,

NO.10 OCTOBER

2008

2456

Fig. 6

Simplified

occurrence

able to return to a previous state. Thus, the state space
analysis ensures that the net of occurrence states should
not include home marking(s).
4. Liveness properties: We also need to determine dead
marking in the graph.
5. Fairness properties: This is a step by step operation and shows whether there is an infinite occurrence
graph.
Our analysis module represents those components and
contains 144 dead markings according to the state space
report. These dead markings are caused by the guard
for the operations that are not normally allowed, such as
view_assigned_permissionlist and revoke_role_from user.
To identify the existing policy conflicts, we also execute the
designed CPN model and discover any nodes that occur access control policy problems, generating the simplified occurrence graph and tracing the path of objects as shown in
Fig. 6. This graph shows a test case with two roles, technician and super operator. It first shows all permissions that
are assigned to each role and clearly indicates how we can
detect the violation of SSOD_CP policy which implies no
conflicting permissions to the same role.
4.

Concluding Remarks

graph

a

of SCADA.

proven

access

policies

in

leverage

clearly

to

to

part

ical

simulate

of

detect

security

policies

for

the

simulation

as

min-

manage

providing

several
to

control

infras-

proactively.

explore

templates

and

critical

more

would

help

crit-

describe

issues.

module

analysis

to

functions
all

functions

access

as

to

need

designed

generation

well

we

enable

develop

scenario

could

complex

manner,

analyze

including

study

that

possibly

we

and

analyze
problems

systematic

directions,

we

infrastructures.

unexpected

future

infrastructures

would

will

those

how

and

out

a

and
it

expressed

possible

points
in

Eventually,

tructures

manage

critical

policies

capabilities

Also,

that

can

we

be

used

tasks.

References

[1]

D.A.

Shea, •gCritical

threat,•h
[2]

K.

CRS

abling
[3]

[4]

with

Y. Jiang

and
Oper.

in

Knorr

St.

and

H.

R.R.F.

A. Masood,

Sampaio

A.

2052:

control

of

duties

in

Information

Models,

2001/V.I.

access

2004.

and

Workshop

and

A.

petri

Assurance

Architectures

for

MMM-ACNS

Gorodetski,

Ghafoor, •gA

framework
on

and

system

plication
ulation,

En-

2001,

V.A.

Skormin,

for

role-based

real-time

Object-oriented

access

systems,•h

con-

Proc.

Real-Time

IEEE

Depenable

2005.

supervisory

[7]

on

2001.

Workshop

Systems,
[6]

May

verification

International

control

Workshop

of mandatory

separation

LNCS

International

eds.,

the terrorist

access

Intl.

Jan.-Feb.

- Methods,

Russia,

Shafiq,
policy

analysis

Weidner, •gAnalyzing

Networks

and

mandatory

Proc.12th

pp. 82-93,

pp. 102-114,

Popyack,

systems

2003.

2003.

vol.13,

Security

L.J.

nets,•h

IEEE,

Res.,

Petersburg,

J.B.
trol

petri

C. Lin. •gSecurity

Computer

and
[5]

Feb.
enterprise's

coloured

workflows,•h

Network

Control

RL31534,

Technologies,

model,•h
K.

infrastructure:

Report

Juszczyszyn, •gVerifying

policies

net

In this article, we briefly gave an overview of current trends
in critical infrastructures that are seriously facing security
problems. Also, we have proposed an empirical framework
to ensure access control management for SCADA systems.
Our case study utilized UML to represent components and
functional elements in SCADA systems. Based on the UML
analysis, we have successfully demonstrated how we could
identify access control requirements for SCADA systems.
To identify access control requirements, we adopted rolebased access control model and derived access control policies from the identified requirements in the context of such

article
control

events.

to

with

have

demonstrated

detecting

involved

This

We

have

CPN

policies,

risks

access

we

of

control

imize

model.

Also,

features

access

As

control

CPN.

using

G.C.
of

colored

an

Barroso, •gAn
electrical

petri

nets,•h

advanced
distribution

Proc.

(380)

function
substation:
Modelling

for
An
and

the
apSim-

2003.

Daneels

and

W.

Salter, •gWhat

is

scada,•h

availalbe

at

cn-

HONG et al.: ACCESS CONTROL

MANAGEMENT

FOR SCADA SYSTEMS
2457

lart.
[8]

[9]

web.

A.T.

cern.

recommendations,•h
Association,

G.

Booch,

H.

with

G.

[16]

1.1.

J.L.

Peterson,

tice

Hall,

R.S.

R.S.

Petri

Net

Unified

Modeling

Reading,

MA,

1999.

Notation

concepts,

analysis

Theoretical

and

The

methods

Computer

and

Science,

applications,•h

Modeling

access
Nov.

Proc.

of Systems,

Pren-

ACM

control

models,•h

Computer,

1993.

for

1st

the

RBAC96

Workshop

family

on

Role-Based

Subrahmanian,

and

of access

control

Access

Control.

P. Samarati,
for

SIGMOD
1997.

R.S.

and

Sandhu

R.S.

V.S.

enforcing

multiple

International

pp. 474-485,

E.J.

access

T.Y. Lin

Coyne,

control

H.L.

and

model

in

Database

Feinstein,

models,•h

Management

X. Qian,

Proc.
of Data,

for

C.E.

R.S.

Sandhu

and

using

R.S.

R.S.

Munawer, •gHow

Proc.

pp. 47-54,

and

vol.32,

board

members

by John Wiley

no.2,

in The

E.

access

con-

Role-Based

Access

and

M.

access
and

CRC

control

IEEE

Press,

Infrastructure

for

Fifth

prac-

and

Engineering

in-

Hand-

1997.

Sloman. •gRole-based

Proc.

and

1994.

Science

security

Workshops

Collaborative

for

on

dis-

Enabling

Enterprise,

IEEE,

1996.
G.J.

Ahn

and

separation
based
G.J.

R.S.

Access
Ahn

and

G.J.

R.S.

rity

model,•h

The

ACM

Nov.

Hong,

role-based
on

authorization

Information

and

System

E.

Role-

and

Software

jailed

for

a formal

Technology,

revenge

sewage

vol.44,

no.11,

attacks,•h

available

hacker-jailed-for-revenge-sewage
NERC

Security

Controls,
[30]

W.F.

Young,

J.E.
and

Am.

Energy

Wind
Austin,

B. Young
igations

Stamp,
mitigation

TX,

and
in

for

the

Electricity

Sector:

Cyber

Access

2002.

nerabilities

2003,
[31]

Guidance

June

M.

wind

Assoc.

and
in

J.D.
wind

Dillinger, •gComrnunication
power

WINDPOWER

SCADA
2003

vul-

systems,•h
Conf.,

May

Proc.
18-21,

USA.
Rumsey, •gCommunication
power

supervisory

vulnerabilities
control

and

an

Networks

Journal

Ltd.

Wenjuan Xu
is currently PhD student at
UNC Charlotte. She joined the Laboratory for
Information Integration, Security and Security
since 2005, and has been actively involved in
research in information security during her PhD
study at UNC Charlotte. Her research interests
include access control and security-enhanced
operating systems. She received her BE degree
in Mechanical Engineering from Shandong University of Technology and ME degree in Information Technology from Capital University of

secu-

at www.theregister.co.uk/2001/10/31/

[29]

ar-

vol.3,

2002.

Register, •gHacker

security

is currently

constraints
Security,

Shin, •gReconstructing

and Communication

and Sons,

control,

Dr Hong

currently serves as an information director of ACM Special Interest Group
on Security, Audit and Control. In addition, he is a recipient of Department of Energy CAREER Award and the Educator of the Year Award from
the Federal Information Systems Security Educators' Association. Prior
to ASU, he was an Associate Professor of College of Computing and Informatics and Founding Director of Center for Digital Identity and Cyber
Defense Research and Laboratory of Information Integration, Security and
Privacy at UNC Charlotte.

2000

and

Aug.

for

Workshop

1999.

Trans.

Information

pp. 649-657,

language

4th

Sandhu, •gRole-based

pp. 207-226,
S.

Proc.

ACM,

ACM

Ahn,

RSL99

constraints,•h

Control,

specification,•h
no.4,

Sandhu, •gThe

of duty

access

security.

Gail-Joon Ahn
is an Associate Professor of
Department of Computer Science and Engineering at Arizona State University. His principal research and teaching interests are in information
and systems security. His research foci include
vulnerability and risk management, access control, and security architecture for distributed systems. His research has been supported by NSF,
NSA, DoD, DoE, Bank of America, Hewlett
Packard, Microsoft and Robert Wood Johnson
Foundation. He is an IEEE Senior member and

pp. 38-47,

Principles

pp. 40-48,

Computer

systems,•h

Technology:

control:

no.9,

pp. 1929-1948,

Lupu,

object

on

Security

include

XI:
1997.

do discretionary

Workshop

interests

PKI, and e-business

Youman, •gRole-

vol.29,

P. Samarati, •gAuthentication,

E. Bertino,

Yialelis,

tributed

to

ACM

P. Samarati, •gAccess

detection,•h
ed.

3rd

Commun.,

Sandhu

book,

[28]

published

His research

1998.

and

IEEE

trusion

N.

Q.

roles,•g

Sandhu

tice,•h

[27]

editorial

act.

privacy,

1996.

Control,

[26]

chitecture,

from Information
and Communications
University
in Koresearch
and teaching interests are in information
security

role-based

Security

North-Holland,

and

Computer,

uni-

policies,•h

URA97

assignment,•h

ed.

E. Bertino, •gA

control
on

V. Bhamidipati, •gThe

Prospects,

Sandhu,

access

Conference

of user-role

and

computer
science
rea. His principal
and privacy

1997.

framework

trol

[25]

Seng-Phil Hong (CISSP, CISM)
is an
Associate Professor of Media and Information
School at Sung Shin Women's University and
He received BS degree in computer science
from Indiana State University, and MS degree
in Computer Science from Ball State University
at Indiana, USA. He researched the information
security for PhD at Illinois Institute of Technology from 1994 to 1997. He joined the Research
and Development Center in LG-CNS Systems,
Inc since 1997, and he received Ph.D. degree in

Guide,

1989.

Theory

pp. 9-19,

Proc.

based

[24]

of

Mason

1981.

S. Jajodia,

Feb.

[23]

The

analysis

April

Sandhu, •gRationale

Status

[22]

family

George

Language

basic

Properties,

pp. 541-580,

administration

[21]

Modeling

for

Dept.

Wesley,

in

Sandlm, •gLattice-based

ACM

[20]

Jacobson,

nets.

Monographs

nets:

vol.77,

ACM,

[19]

modeling
ISE

Modeling

petri

use,•h

T. Murata, •gPetri

fied

Unified

Ameri-

1997.

models,•h

[18]

The

by

1997.

Jensen, •gColoured
practical

I.

Addison

Unified

Sept.

vol.26, no.11,
[17]

no.12

and

Report,

and

Manual,

Software,

IEEE,
[15]

Report

1999.

analysis

Technical

Booch,

Reference

Version

[14]

presentation slides, Am. Wind Energy Assoc. WINDPOWER 2003
Conf., May 18-21, 2003, Austin, TX, USA,
available at www.prod.sandia.gov/cgi-bin/techlib/access-control.pl/
2003/031398p.pdf

communications

1999.

Rational

vol.1,

scada

Jacobson,
Wesley,

oriented

J. Rumbaugh,

and

AGA

I.

Addison

the UML,•h

of

2004.
and

Guide,

Gomaa, •gObject

K.

Draft3,
Aug.

J. Rumbaugh,
User

Language

[13]

protection

Gas

University,

[12]

2000.

can

systems

[11]

Dec.

general

Language
[10]

ch,

Group, •gCryptographic

data

and

mit-

acquisition,•h

Economics

and Business,

China

in 2000

and 2003,

respectively.

Active Automation of the DITSCAP
Seok Won Lee, Robin A. Gandhi, Gail-Joon Ahn, and Deepak S. Yavagal
Department of Software and Information Systems,
The University of North Carolina at Charlotte, Charlotte, NC 28223
{seoklee, rgandhi, gahn, dsyavaga}@uncc.edu

Abstract. The Defense Information Infrastructure (DII) connects Department of
Defense (DoD) mission support, command and control, and intelligence computers and users through voice, data, imagery, video, and multimedia services,
and provides information processing and value-added services. For such a critical infrastructure to effectively mitigate risk, optimize its security posture and
evaluate its information assurance practices, we identify the need for a structured and comprehensive certification and accreditation (C&A) framework with
appropriate tool support. In this paper, we present an active approach to provide
effective tool support that automates the DoD Information Technology Security
C&A Process (DITSCAP) for information networks in the DII.

1 Introduction
The DoD increasingly relies on software information systems, irrespective of their
level of classification, in order to perform a variety of functions to accomplish their
missions. The DITSCAP provides an excellent platform to assess the security of software information systems from organizational, business, technical and personnel aspects while supporting an infrastructure-centric approach. However, the lack of an integrated C&A framework and tool support often diminishes its effectiveness.
DITSCAP itself can be quite overwhelming due to its long and exhaustive process of
cross-checks and analysis which requires sifting through a multitude of DITSCAP polices and requirements. The complex interdependencies that exist between information from such large and diverse sources, significantly restricts human ability to effectively comprehend, develop, configure, manage and protect these systems.
To address these shortcomings and enhance the effectiveness of DITSCAP, we
discuss our design principles, modeling techniques and supporting theoretical foundations that lead to the conceptual design of the DITSCAP Automation Tool
(DITSCAP-AT). DITSCAP-AT aggregates C&A related information from various
sources using a uniform representation scheme, and transforms static record keeping
repositories into active ones that link to each other from different perspectives, allowing for their reuse and evolution through all stages of the system C&A lifecycle.
DITSCAP-AT combines novel techniques from software requirements engineering
and knowledge engineering to leverage the power of ontologies [10] for representing,
modeling and analyzing DITSCAP-oriented requirements, while actively assisting the
discovery of missing, conflicting and interdependent pieces of information that are
critical to assess DITSCAP compliance.
P. Kantor et al. (Eds.): ISI 2005, LNCS 3495, pp. 479 – 485, 2005.
© Springer-Verlag Berlin Heidelberg 2005

480

S.W. Lee et al.

2 DITSCAP Overview and Objectives for Its Automation
DITSCAP is a standard DoD process for identifying information security requirements, providing security solutions, and managing information systems security activities [3] for systems in the DII. DITSCAP certification is a “comprehensive evaluation of the technical and non-technical security features of an information system and
other safeguards made in support of the accreditation process, to establish the extent
to which a particular design and implementation meets a set of specified security requirements” [3]. Ensuing certification, the accreditation statement is an approval to
operate the information system in a particular security mode using a prescribed set of
safeguards at an acceptable level of risk by a designated approving authority.
DITSCAP distributes its activities over four phases that range from the initiation of
the C&A activities to its maintenance and reaccreditations. The level of rigor in each
phase depends on the certification level chosen for the information system among the
four levels available [2]. The security plan for DITSCAP is documented in the Software Security Authorization Agreement (SSAA) to “guide actions, document decisions, specify IA requirements, document certification tailoring and level-of-effort,
identify potential solutions, and maintain operational systems security” [3].
Although the DITSCAP application manual [2] outlines the C&A tasks and activities along with associated roles and responsibilities of C&A personnel, they are expressed at an abstract level to maintain general applicability. Such abstractness makes
it hard to ensure objectivity, predictability and repeatability in interpreting and enforcing DITSCAP requirements and policies. Furthermore, an entirely manual approach
to cross-reference a multitude of DITSCAP-oriented directives, security requisites
and policies in the light of user/system criteria to determine applicable security requirements raises serious concerns about the accuracy and comprehensiveness of such
assessments. A structured and comprehensive method to assess and monitor the operational risk of information systems is also missing in the current approach.
To address the above shortcomings, the first and foremost objective of DITSCAP
automation is to effectively assess the extent to which an information system meets
the DITSCAP-oriented security requirements by supporting the process of identifying, interpreting and enforcing the applicable requirements based on user/system criteria. To reduce the amount of long and exhaustive documentation, carefully designed
interfaces need to be developed that guide the user interactions through the DITSCAP
tasks and activities. These interfaces should leverage thoroughly designed questionnaires and criteria, extracted from DITSCAP related C&A goals, directives, security
requisites and other widely accepted best practices. These questionnaires along with
predefined answers become the basis for building well defined metrics and measures
that encompass the scope of the C&A goals addressed by them. The DITSCAP automation also demands structured, justifiable and repeatable methods to have for a
comprehensive risk assessment, providing a firm basis to create cost versus risk
measures. To actively discover and monitor network vulnerabilities, DITSCAP automation requires network self-discovery capabilities that allow comparison between
the intended and the actual operational environment. Currently we limit the scope of
DITSCAP-AT to level one DITSCAP certification as applied to Local Area Network
(LAN) systems only. In the following section, we present the DITSCAP-AT

Active Automation of the DITSCAP

481

conceptual architecture conceived through our analysis to accomplish the aforementioned objectives.

3

DITSCAP-AT Conceptual Architecture

The conceptual architecture of DITSCAP-AT is shown in Fig. 1. The Process-driven
Workflow module guides the DITSCAP through a well-defined course of action that
results in the elicitation of required user criteria and generation of the SSAA. The
tasks contained in each process component (P1, P2…Pn) are extracted from the
DITSCAP application manual [2] and homogenously grouped based on their interdependent goals/objectives. Each task is then further expressed using carefully designed
questionnaires/forms embedded in wizard-based interfaces to gather and establish
well-defined C&A metrics and measures.
The Requirements Repository module provides a complete ontological engineering
support for DITSCAP-AT. It provides utilities to support representation of security
requirements, meta-knowledge creation, ability to query pre-classified and categorized information structures and other browsing and inference functionalities. The requirements repository is a specialized module built upon the GENeric Object Model
(GenOM) [6], an integrated development environment for ontological engineering
processes with functionalities to create, browse, access, query and visualize associated
knowledge-bases (Ontology + Rules).
The Multi-strategy Machine Discovery module supports network self-discovery
capabilities that allow the comparison of intended and operational environments. A
set of network tools are selected on the basis of the information required to assess
DITSCAP compliance, such as hardware, software and firmware inventories, configurations of network devices and services, and vulnerability assessment using penetration testing. A combination of network discovery tools and scripts enables to gather
and fuse aggregated information as meta-knowledge in the requirements repository,
which is then suitably transformed for inclusion in the SSAA.
Creation of DITSCAP
related Documentation

DITSCAP-AT

Populating,
Browsing,
Searching and
Editing the
Requirements
Repository

P1

P2

Pn-1

P3

Pn

Analysis of reports
obtained from various
Network Tools

Process-driven Workflow

Network discovery information
integration
Natural Language
Requirements
+ Domain Knowledge

T1
Requirements
Repository

Tm

Multi-strategy Machine Discovery

P: DITSCAP Process Component
T: Network information discovery Tool

Fig. 1. DITSCAP-AT Conceptual Architecture

Network

Network
Environment
Environment

482

S.W. Lee et al.

The process components of the Process-driven Workflow module retrieve applicable requirements/policies/meta-knowledge from the Requirements Repository and
network discovery/monitoring information from the Multi-strategy Machine Discovery module, to actively assist the user in the C&A process.
In the following section, we discuss the use of information aggregated by
DITSCAP-AT to achieve the objectives of DITSCAP automation.

4 The DITSCAP Automation Framework
In order to actively support the C&A process, uniformly across the DII, we create a
DITSCAP Problem Domain Ontology (PDO) that provides the definition of a common language and understanding of the DITSCAP domain at various levels of abstractions through the application domain concepts, properties and relationships between them. The PDO is a machine understandable, structured representation of the
DITSCAP domain captured using an object oriented ontological representation in the
Requirements Repository. We elaborate more on methods and features for deriving
the PDO in [7]. To satisfy the objectives of DITSCAP automation, the PDO specifically includes structured and well defined representations of: 1) A requirements hierarchy based on DITSCAP-oriented directives, security requisites and policies; 2) A
risk assessment taxonomy that includes links between related risk sources and leaf
node questionnaires with predictable answers that have risk weights and priorities assigned to them; 3) Overall DITSCAP process aspect knowledge that includes C&A
goals/objectives; 4) Meta-knowledge about information learned from network discovery/monitoring tools; and 5) Interdependencies between entities in the PDO.
One of the objectives of DITSCAP-AT is to assess the extent to which an information system meets the DITSCAP-oriented security requirements by supporting the
process of identifying, interpreting and enforcing the applicable requirements based
on user criteria. The PDO supports such features through a requirements hierarchy
that is constructed by extracting requirements from DITSCAP-oriented security directives, instructions, requisites and policies. A hierarchical representation includes highlevel Federal laws, mid-level DoD/DoN policies, and site-specific requisites in the
leaf nodes, which naturally corresponds to generic requirements, domain spanning requirements and sub-domain requirements in the requirements hierarchy. Also, there
exists several non-taxonomic links that represent relationships within the requirements hierarchy as well as with other entities in the PDO.
A requirements hierarchy, therefore, allows the determination of applicable security requirements by successively decomposing the high-level generic requirements
into a set of specific applicable requirements in the leaf nodes based on user criteria.
Furthermore, the non-taxonomic links can be utilized to effectively interpret and enforce requirements by identifying the related requirements in other categories as well
as relationships with entities in various dimensions from the PDO to ensure a comprehensive coverage of the C&A process.
To address the needs for a structured, justifiable and repeatable method for a comprehensive risk assessment, the PDO includes a risk assessment taxonomy which aggregates a broad spectrum of all possible categories and classification of risk related
information. The risk assessment goals expressed in the higher level non-leaf nodes of

Active Automation of the DITSCAP

483

this taxonomy can be achieved using specific criteria addressed in the leaf nodes. For
example, the risk taxonomy in the upper level non-leaf nodes consists of threat, vulnerabilities, countermeasures, mission criticality, asset value and other categories related to risk assessment. Each non-leaf node is then further decomposed into more
specific categories. In addition, several non-taxonomic links identify relationships
with other risk categories as well as with other entities in the PDO. Current scope of
the risk related categorization is mainly based on the National Information Assurance
Glossary [1] as well as other sources such as the DITSCAP Application Manual [2]
and the DITSCAP Minimal Security Checklist [2]. We also utilize the information security metrics that have been established by the National Institute of Standards and
Technology [8], [9].
A predictable and quantitative risk assessment is carried out using weights assigned to pre-classified answers for specific questions/criteria in the leaf nodes. These
answers can be elicited from a variety of sources such as DITSCAP-AT users, network self-discovered information, or other sources. Furthermore, the questions/criteria in the leaf nodes of the risk assessment taxonomy naturally relate to
various security requirements in the requirements hierarchy by expressing their testability in the form of criteria to measure their level of compliance. Such relationships
along with the priorities/weights/criticalities associated with answers to questions/criteria in the leaf nodes of the risk assessment taxonomy can be used to develop
complex risk calculation algorithms and establish metrics and measures that enable
further articulation of critical weakest points in the system. The risk assessment taxonomy also promotes a uniform and comprehensive interpretation of different risk
categories that are established through a common understanding of the concepts,
properties and relationships that exist in the DITSCAP PDO. Such a shared understanding is inevitable to effectively estimate the collective impact of residual risk
from all supporting systems on the overall critical infrastructure.
To populate the models discussed here, we have designed several core mock interfaces for DITSCAP-AT to realize a complete course of action for gathering and analyzing the required information [7]. Such mock interfaces provide a thorough understanding of the important aspects of DITSCAP-AT user interaction and offer valuable
insight and assurance in realizing the theoretical aspects of DITSCAP automation.

5 Multi-dimensional Link Analysis
The root of Multi-Dimensional Link Analysis (MDLA) lies in the concept of proxy
viewpoints model from the PVRD methodology proposed by Lee [5] to discover
missing requirements and relationships. Lee suggests that “Individual pieces of information finally become valuable knowledge when they establish ‘links’ with each other
from various aspects/dimensions based on a certain set of goals”. Following this
paradigm, MDLA can be carried out from different dimensions such as user criteria,
viewpoints [4], system goals, business/mission requirements, regulatory requirements,
specific operational concepts, and risk categories based on the DITSCAP C&A goals
which can help understand various interdependencies between DITSCAP-oriented requirements, facilitating their interpretation and enforcement. The DITSCAP PDO that
resides in the requirements repository fosters such analysis due to its ontological

484

S.W. Lee et al.

characteristics that provides inherent properties for an active approach to link requirements and other entities from different perspectives and dimensions. MDLA’s
integrated framework for analytical analysis promotes assurance for a comprehensive
coverage of the certification compliance space by actively assisting the process of
discovering missing, conflicting, and interdependent pieces of information as well as
establishing C&A metrics and measures based on common understanding and the reflected language from various dimensions.

6 Conclusion and Future Work
DITSCAP-AT contributes to the automation of DITSCAP in several ways. Firstly, it
provides an effective tool support to identify, interpret and enforce DITSCAP polices
and requirements. Secondly, it provides a structured and comprehensive approach to
risk assessment from a broad spectrum of categories contributing to risk and finally,
the ability to perform multi-dimensional link analysis provides the opportunity to reveal the “emergent” or “missing” information pieces that in-turn provides the assurance of a comprehensive coverage of the certification compliance space.
Our future work includes the software realization of DITSCAP-AT mock interfaces while systematically realizing all its core functional components. Although we
limit the current scope of DITSCAP-AT to include DoD directives, security requisites
and best practices for secure software development, it can be easily scaled to accommodate general security requirements, policies and practices in any domain of interests. We also realize that development of appropriate metrics and measures for a
comprehensive and uniform risk assessment in the DITSCAP domain is an area that
requires significant attention for the success of DITSCAP-AT.

Acknowledgements
This work is partially supported by the grant (Contract: N65236-05-P-0597) from the
Critical Infrastructure Protection Center (CIPC), Space and Naval Warfare
(SPAWAR) Systems Center, Charleston, SC. USA. We acknowledge the support and
encouragement from Scott West, John Linden, Bill Bolick, and Bill Chu. Finally, we
thank Divya Muthurajan and Vikram Parekh for their contributions to this research.

References
1. Committee on National Security Systems (CNSS) Instruction No. 4009.: National Information Assurance (IA) Glossary. (2003)
2. DoD 8510.1-M: DITSCAP Application Manual (2000)
3. DoD Instruction 5200.40.: DITSCAP (1997)
4. Kotonya, G. and Sommerville, I.: Requirements Engineering with Viewpoints. BCS/IEEE
Software Engineering Journal, Vol. 11, Issue 1 (1996) 5-18
5. Lee, S.W. and, Rine D.C.: Missing Requirements and Relationship Discovery through
Proxy Viewpoints Model. Studia Informatica Universalis: International Journal on Informatics, December (2004)

Active Automation of the DITSCAP

485

6. Lee, S.W. and, Yavagal, D.: GenOM User’s Guide. Technical Report: Dept. of Software
and Information Systems, UNC Charlotte (2004)
7. Lee, S.W., Ahn, G. and Gandhi, R.A.: Engineering Information Assurance for Critical Infrastructures: The DITSCAP Automation Study. To apprear in: Proceedings of the Fifteenth Annual International Symposium of the International Council on Systems Engineering (INCOSE ‘05), Rochester New York July (2005)
8. Swanson, M., Nadya, B., Sabato, J., Hash, J., Graffo, L.: Security Metrics Guide for
information Technology Systems. NIST #800-55 (2003)
9. Swanson, M.: Security Self-Assessment Guide for Information Technology Systems.
NIST #800-26 (2001)
10. Swartout, W. and Tate, A.: Ontologies. In: Intelligent Systems, IEEE, Vol. 14(1) (1999)

SCIENCE CHINA
Information Sciences

. RESEARCH PAPERS .

August 2011 Vol. 54 No. 8: 1608–1617
doi: 10.1007/s11432-011-4293-9

Zero-knowledge proofs of retrievability
ZHU Yan1,2 ∗ , WANG HuaiXi3 , HU ZeXing1 , AHN Gail-Joon4 & HU HongXin4∗
1Institute

of Computer Science and Technology, Peking University, Beijing 100871, China;
Key Laboratory of Internet Security Technology, Peking University, Beijing 100871, China;
3School of Mathematical Sciences, Peking University, Beijing 100871, China;
4School of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, AZ 85287, USA

2Beijing

Received April 26, 2010; accepted December 14, 2010; published online May 31, 2011

Abstract Proof of retrievability (POR) is a technique for ensuring the integrity of data in outsourced storage
services. In this paper, we address the construction of POR protocol on the standard model of interactive proof
systems. We propose the ﬁrst interactive POR scheme to prevent the fraudulence of prover and the leakage of
veriﬁed data. We also give full proofs of soundness and zero-knowledge properties by constructing a polynomialtime rewindable knowledge extractor under the computational Diﬃe-Hellman assumption. In particular, the
veriﬁcation process of this scheme requires a low, constant amount of overhead, which minimizes communication
complexity.
Keywords
cryptography, integrity of outsourced data, proofs of retrievability, interactive protocol, zeroknowledge, soundness, rewindable knowledge extractor
Citation
Zhu Y, Wang H X, Hu Z X, et al. Zero-knowledge proofs of retrievability. Sci China Inf Sci, 2011, 54:
1608–1617, doi: 10.1007/s11432-011-4293-9

1

Introduction

A proof of retrievability (POR) [1] is a cryptographic proof technique for a storage provider to prove that
clients’ data remain intact. In other words, the clients can fully retrieve their data and have conﬁdence
to use the recovered data. This highlights a strong need to seek an eﬀective solution for checking whether
their data have been tampered with or deleted without downloading the latest version of data. This
technique is important for the storage-outsourced data, especially large ﬁles or achieves. For example,
with a wide spread of cloud computing, cloud storage service has become a new proﬁt growth point
by providing a comparably low-cost, scalable, location-independent platform for managing clients’ data.
However, if such an important service is vulnerable to malicious attacks, it would bring irretrievable
losses to the clients since their data and archives are stored into an uncertain storage pool outside the
enterprises. Therefore, it is necessary for cloud service providers to make use of the POR technique to
provide a secure management of their storage services.
Since a formal model for the proof of retrievability was introduced by Juels and Kaliski [1], some
schemes [2–5] have been proposed in recent years. In these schemes, Shacham and Waters [6] proposed
the compact proofs of retrievability (CPOR) schemes, considered as a representative work with a general
∗ Corresponding

authors (email: yan.zhu@pku.edu.cn; hxhu@asu.edu)

c Science China Press and Springer-Verlag Berlin Heidelberg 2011


info.scichina.com

www.springerlink.com

Zhu Y, et al.

Sci China Inf Sci

August 2011 Vol. 54 No. 8

1609

framework and diverse characters: 1) a ﬁle is split into blocks and each block corresponds to a signature
tag; 2) a veriﬁer can verify the integrity of ﬁle in a random sampling approach, which is of utmost
importance for large or huge ﬁles; and 3) a homomorphic property is used to aggregate the tags into a
constant size response, which minimizes network communication.
Although various adversary models have been proposed to prove the security of POR schemes, these
existing schemes do not follow the standard model of interactive proof systems (IPS) [7] so that the
security of veriﬁcation process, especially the soundness of veriﬁcation, cannot be guaranteed. This
means that a prover could deceive a veriﬁer for the forged data through the veriﬁcation protocol. More
importantly, the data conﬁdentiality of outsourced storage cannot be ensured by the public veriﬁcation
processes, in which the veriﬁer can easily gain all veriﬁed data by analyzing the responses of public
challenges. Hence, it is necessary to construct an eﬃcient POR scheme on standard model of interactive
proof systems so as to prevent the prover fraud and protect the data privacy.
Related works.
To check the availability and integrity of the storage-outsourced data, Juels and
Kaliski [1] ﬁrst presented a proof of retrievability (POR) scheme which largely relies on preprocessing
steps the client conducts before sending a ﬁle to the server: “sentinel” blocks are randomly inserted to
detect corruption; the ﬁle is encrypted to hide these sentinels; and error-correcting codes are used to
recover data from corruption. Unfortunately, this scheme can only handle a limited number of queries,
which have to ﬁx a priori and can only be applied to encrypted ﬁles.
Similar to POR, Ateniese et al. [2] proposed a PDP model for ensuring possession of ﬁles on untrusted
storages and provided a RSA-based scheme for the static case where it achieves O(1) communication
costs. They also proposed a publicly veriﬁable version, which allows anyone, not just the owner, to
challenge the server for data possession. This property greatly extends application areas of PDP protocol
due to the separation of data owners and users. However, similar to replay attacks, these schemes are
insecure in a dynamic scenario because of the dependence on the index of blocks. To solve this problem,
Chris Erway et al. [8] introduced two Dynamic PDP schemes with a Hash function tree to realize O(log n)
communication and computational costs for a ﬁle consisting of n blocks.
Based on the works of Juels et al. [1] and Ateniese et al. [2], Shacham and Waters [6] proposed a general
model based on a data fragmentation idea, called Compact POR (CPOR), which uses homomorphic
property to aggregate a proof into O(1) authenticator value and O(t) computation costs for t challenge
blocks. In fact, this model, considered to be a general representative for existing schemes, is readily
converted to MAC-based, ECC or RSA schemes, which are built from BLS signature [9] and random
oracle model, and have the shortest query and response with public veriﬁability. However, this model
was not constructed on interactive proof systems and an adversary can make use of the public veriﬁcation
protocol to gain the storage-outsourced data.
Furthermore, some other POR schemes and models, such as [4, 5, 10], have been recently proposed.
Dodis et al. [4] discussed several variants of this problem (such as bounded-use vs. unbounded-use,
knowledge soundness vs. information-soundness), and gave nearly optimal POR schemes for each of these
variants. Wang et al. [5] presented a dynamic scheme with O(log n) costs by integrating above CPOR
scheme and Merkle Hash Tree (MHT) in DPDP. Bowers et al. [10] proposed a theoretical framework for
the design of POR based on Juels-Kaliski and Shacham-Waters works, which supports a fully Byzantine
adversary model on the adversarial noisy channel assumption and the error-correction coding methods.
Contributions.
In this paper, we focus on the construction of POR protocol to prevent the fraudulence of prover and the leakage of veriﬁed data. We introduce the ﬁrst formal deﬁnition of interactive
proofs of retrievability (IPOR) on the standard model of interactive proof systems. In terms of this
deﬁnition, we provide a practical zero-knowledge POR (ZK-POR) solution to prevent data leakage in the
public veriﬁcation process. We also prove the soundness and zero-knowledge propertis of this scheme by
constructing a polynomial-time knowledge Extractor, having rewindable black-box access to the prover,
under the computational Diﬃe-Hellman (CDH) assumption. The performance analysis shows that our
commitment/challenge/response protocol transmits a small, constant amount of data, which minimize
network communication. Thus, our scheme supports a public remote checking for the large-size private

Zhu Y, et al.

1610

Sci China Inf Sci

August 2011 Vol. 54 No. 8

archive ﬁles in widely-distributed storage systems.
Organization.
The rest of the paper is organized as follows. In section 2, we describe some basic
notations, common POR structure, and the attack for existing schemes. In section 3, we deﬁne a formal
model of IPOR based on interactive proof systems. A practical ZK-POR scheme is proposed for the
IPOR model in section 4. We describe the security analysis and performance evaluation of our scheme
in section 5 and section 6, respectively. Finally, we conclude this paper in section 7.

2

Preliminaries

Let H = {Hk } be a keyed hash family of functions Hk : {0, 1}∗ → {0, 1}n index by k ∈ K. We say that
algorithm A has advantage  in breaking the collision-resistance of H if
Pr[A(k) = (m0 , m1 ) : m0 = m1 , Hk (m0 ) = Hk (m1 )]  ,
where the probability is over the random choice of k ∈ K and the random bits of A. This hash function
can be obtained from the hash function of BLS signatures [9].
Definition 1 (Collision-resistant hash). A hash family H is (t, )-collision-resistant if no t-time adversary has advantage at least  in breaking the collision-resistance of H.
We set up our systems using bilinear pairings proposed by Boneh and Franklin [11]. Let G and GT
be two multiplicative groups using elliptic curve conventions with large prime order p. The function e
is a computable bilinear map e : G × G → GT with the following properties: for any G, H ∈ G and all
a, b ∈ Zp , we have 1) bilinearity: e(Ga , H b ) = e(G, H)ab ; 2) non-degeneracy: e(G, H) = 1 unless G or
H = 1; and 3) computability: e(G, H) is eﬃciently computable.
Definition 2 (Bilinear map group system). A bilinear map group system is a tuple S = p, G, GT , e
composed of the objects as described above.
Shacham and Waters [6] proposed a general CPOR model as follows: Given a ﬁle F , the client splits F
into n blocks (m1 , . . . , mn ) and each block mi is further split into s sectors (mi,1 , . . . , mi,s ) ∈ Zsp for some
suﬃciently large p. Let e : G × G → GT be a bilinear map, g be a generator of G, and H : {0, 1}∗ → G
be the BLS hash. The secret key is sk = x ∈R Zp and the public key is pk = (g, v = g x ). The client
chooses s random u1 , . . . , us ∈R G as the veriﬁcation information t = (F n, u1 , . . . , us ), where F n is the
s
m
ﬁle name. For each i ∈ [1, n], the tag at the ith block is σi = (H(F n||i) · j=1 uj i,j )x . On receiving

query Q = {(i, vi )}i∈I for an index set I, the server computes and sends back σ  ← (i,vi )∈Q σivi and

μ = (μ1 , . . . , μs ), where μj ← (i,vi )∈Q vi mi,j . The veriﬁcation equation is
e(σ  , g) = e



(i,vi )∈Q

H(F n||i)vi ·

s
j=1


μ
uj j , v .

This scheme is not secure for the leakage of ﬁle information as follows:
Attack 1. An adversary can get the ﬁle and tag information by running or wiretapping n times veriﬁcation communication for a ﬁle with n × s sectors.
Proof. Let s be the number of sectors in each block. After running or wiretapping n times queries, an ad(1)
(n)
versary can get n times challenges (Q(1) , . . . , Q(n) ) and their the responses ((σ  , μ(1) ), . . . , (σ  , μ(n) )),
(k)
(k)
(k)
where μ = (μ1 , . . . , μs ) for k ∈ [1, n]. For each i ∈ [1, s], these responses can generate the equations
⎧ (1)
(1)
(1)
⎪
= v1 m1,i + · · · + vn mn,i ,
⎪ μi
⎨
..
..
.
.
⎪
⎪
⎩ (n)
(n)
(n)
= v1 m1,i + · · · + vn mn,i ,
μi
(k)

(k)

where Q(k) = {(j, vj )}j∈I are known and ∀j ∈ I, vj

= 0 for k ∈ [1, n]. The adversary can compute
(j)

{m1,i , . . . , mn,i } by solving the equations iﬀ the coeﬃcient matrix {vi }n×n of equations is invertible.

Zhu Y, et al.

Sci China Inf Sci

August 2011 Vol. 54 No. 8

1611
i∈[1,n]

After s times solving these equations (i ∈ [1, s]), the adversary can obtain the whole ﬁle, F = {mi,j }j∈[1,s] .
Similarly, the adversary can get all tags σ1 , . . . , σn by using σ  , . . . , σ  . Denote the inverse matrix of

wi,j
(j)
{vi }n×n by {wi,j }n×n , all the tags can be easily computed following the equations σj = ni=1 σ (i)
for j ∈ [1, n] .
(1)

3

(n)

Interactive proofs of retrievability

3.1

Definition

We present the deﬁnition of interactive proofs of retrievability (IPOR) based on interactive proof systems:
Defintion 3 (Interactive-POR). An interactive proof of retrievability scheme S is a collection of two
algorithms and an interactive proof system, S = (K, T , P):
KeyGen(1κ ): It takes a security parameter κ as input, and returns a secret key sk or a public-secret
keypair (pk, sk);
T agGen(sk, F ): It takes as inputs the secret key sk and a ﬁle F , and returns the triples (ζ, ψ, σ), where
ζ denotes the secret used to generate the veriﬁcation tags, ψ is the set of public veriﬁcation parameters
u and index information χ, i.e., ψ = (u, χ); σ denotes the set of veriﬁcation tags;
Proof (P, V ): It is a protocol of proof of retrievability between a prover (P) and a veriﬁer (V). At the
end of the protocol run, V returns {0|1}, where 1 means the ﬁle is correctly stored on the server. It
includes two cases:
• P (F, σ), V (sk, ζ) is a private proof, where P takes as input a ﬁle F and a set of tags σ, and V takes
as input a secret key sk and a secret of tags ζ;
• P (F, σ), V (pk, ψ) is a public proof, where P takes as input a ﬁle F and a set of tags σ, and a public
key pk and a set of public parameters ψ are the common input between P and V ,
where P (x) denotes the subject P holds the secret x and P, V (x) denotes both parties P and V share
a common data x in a protocol.
This is a more generalized model than existing POR models. Since the veriﬁcation process can be
considered as an interactive protocol, this deﬁnition is not limited to the speciﬁc steps of veriﬁcation,
including scale, sequence, and the number of moves in protocol, so it can provide greater convenience
for the construction of protocol. Further, this paper will only consider the construction of public proof
protocol.
3.2

Security requirements

According to the standard deﬁnition of interactive proof system proposed by Bellare and Goldreich [7],
the protocol Proof (P, V ) has two requirements:
Definition 4 (Security of IPOR). A pair of interactive machines (P, V ) is called an available proof of
retrievability for a ﬁle F if P is a (unbounded) probabilistic algorithm, V is a deterministic polynomialtime algorithm, and the following conditions hold for some polynomial p1 (·), p2 (·), and all κ ∈ N:
• Completeness: For every σ ∈ T agGen(sk, F ),
Pr[P (F, σ), V (pk, ψ) = 1]  1 − 1/p1 (κ);

(1)

• Soundness: For every σ ∗ ∈ T agGen(sk, F ), every interactive machine P ∗ ,
Pr[P ∗ (F, σ ∗ ), V (pk, ψ) = 1]  1/p2 (κ);

(2)

where p1 (·) and p2 (·) are two polynomials, and κ is a security parameter used in KeyGen(1κ ).
In this deﬁnition, the function 1/p1 (κ) is called completeness error, and the function 1/p2 (κ) is called
soundness error. For non-triviality, we require 1/p1 (κ) + 1/p2 (κ)  1 − 1/poly(κ).
The soundness means that it is infeasible to fool the veriﬁer into accepting false statements. The
soundness can also be regarded as a stricter notion of unforgeability for the ﬁle tags. Thus, the above

Zhu Y, et al.

1612

Sci China Inf Sci

August 2011 Vol. 54 No. 8

deﬁnition means that the prover cannot forge the ﬁle tags or tamper with the data if soundness property
holds.
In order to protect the conﬁdentiality of checked data, we are more concerned about the leakage of
private information in the veriﬁcation process. It is easy to ﬁnd that data blocks and their tags could be
obtained by the veriﬁer in some existing schemes. To solve this problem, we introduce zero-knowledge
property into IPOR system, as follows:
Definition 5 (Zero-knowledge). An interactive proof of retrievability scheme is computational zero
knowledge if there exists a probabilistic polynomial-time algorithm S ∗ (called Simulator ) such that for
every probabilistic polynomial-time algorithm D, for every polynomial p(·), and for all suﬃciently large
κ, it holds that




Pr[D(pk, ψ, S ∗ (pk, ψ)) = 1]−


  1/p(κ),

 Pr[D(pk, ψ, P (F, σ), V ∗ (pk, ψ)) = 1] 
where S ∗ (pk, ψ) denotes the output of simulator S on common input (pk, ψ) and P (F, σ), V ∗ (pk, ψ)
denotes the output of interactive protocol between V ∗ and P (F, σ) on common input (pk, ψ). That
is, for all σ ∈ T agGen(sk, F ), the ensembles S ∗ (pk, ψ) and P (F, σ), V ∗ (pk, ψ) are computationally
indistinguishable.
Actually, zero-knowledge is a property that captures P ’s robustness against attempts to gain knowledge
by interacting with it. For the POR scheme, we make use of the zero-knowledge property to guarantee
the security of data blocks and signature tags.
Definition 6 (ZK-POR). An IPOR is called zero-knowledge proof of retrievability (ZK-POR) if the
completeness, knowledge soundness, and zero-knowledge property hold.

4

Construction of zero-knowledge proofs of retrievability

In our construction, the veriﬁcation protocol has a 3-move structure: commitment, challenge and response. This protocol is similar to Schnorr’s Σ protocol [12], which is a zero-knowledge proof system. We
present our IPOR construction as follows:
KeyGen(1κ ): Let S = (p, G, GT , e) be a bilinear map group system with randomly selected generators
g, h ∈R G, where G, GT are two groups of large prime order p, |p| = O(κ). Generate a collision-resistant
hash function Hk (·) and chooses two random α, β ∈R Zp and computes H1 = hα and H2 = hβ ∈ G.
Thus, the secret key is sk = (α, β) and the public key is pk = (g, h, H1 , H2 ).
TagGen(sk, F ): Splits the ﬁle F into n × s sectors F = {mi,j } ∈ Zn×s
. Chooses s random τ1 , . . . , τs ∈
p
τi
Zp as the secret of this ﬁle and computes ui = g ∈ G for i ∈ [1, s] and ξ (1) = Hξ (“F n”), where
s
ξ = i=1 τi and F n is the ﬁle name. Builds an index table χ = {χi }ni=1 and ﬁlls out the item χi in χ for
i ∈ [1, n], where the index table χ = {χi }i∈[1,n] can be used to support some dynamic data operations,
for example, we deﬁne χi = (Bi ||Vi ||Ri ) and initially set χi = (Bi = i, Vi = 1, Ri ∈R {0, 1}∗), where Bi
is the sequence number of block, Ri is the version number of updates for this block, and Ri is a random
integer to avoid collision. Then calculates its tag as
σi ← (ξi )α · g
(2)

s

j=1

τj ·mi,j ·β

∈ G,

(2)

where ξi = Hξ(1) (χi ) and i ∈ [1, n]. Finally, sets u = (ξ (1) , u1 , . . . , us ) and outputs ζ = (τ1 , . . . , τs ),
ψ = (u, χ) to a trusted third part (TTP), and σ = (σ1 , . . . , σn ) to a storage service provider (SSP).
Proof(P, V ): This is a 3-move protocol between Prover (SSP) and Veriﬁer (client) with the common
input (pk, ψ), which is stored in a TTP as follows:
• Commitment (P → V ): P chooses a random γ ∈R Zp and s integers λj ∈R Zp for j ∈ [1, s], and
s
λ
sends theirs commitments C = (H1 , π) to V , where H1 = H1γ and π ← e( i=1 uj j , H2 ) ∈ GT .

Zhu Y, et al.

Sci China Inf Sci

1613

August 2011 Vol. 54 No. 8

• Challenge (P ← V ): V chooses a random challenge set I of t indices along with t random coeﬃcients
vi ∈ Z∗p , where t = |I|. Let Q = {(i, vi )}i∈I be the set of challenge index coeﬃcient pairs. V sends Q to
P.
• Response (P → V ): P calculates the response θ and μ as


σiγ·vi , μj ← λj + γ ·
vi · mi,j ,
σ ←
(i,vi )∈Q

(i,vi )∈Q

where μ = {μj }j∈[1,s] . P sends θ = (σ  , μ) to V .
Verification:

Now the veriﬁer V can check whether or not the response was correctly formed by
 

 
s
?
μ
(2)
π · e(σ  , h) = e
(ξi )vi , H1 · e
uj j , H2 .

(3)

j=1

(i,vi )∈Q

In order to prevent the leakage of the stored data and tags in the veriﬁcation process, the secret data
{mi,j } are protected by a random λj ∈ Zp and the tags {σi } are randomized by a γ ∈ Zp . Moreover,
s
λ
the values {λj } and γ are protected by the simple commitment methods, i.e., H1γ and e( i=1 uj j , H2 ),
to avoid the adversary from gaining them.

5

Security proof of construction

Our scheme is an eﬃcient interactive proof system with completeness and soundness properties as follows:
(1) Completeness: for every available tag σ ∈ T agGen(sk, F ) and a random challenge Q = (i, vi )i∈I ,
the completeness of protocol can be elaborated as follows:
 
α·γ
s


(2) vi
β sj=1 τj ·λj

·e
(ξi ) , h
· e(g, h)γ·β j=1 (τj · (i,vi )∈Q vi ·mi,j )
π · e(σ , h) = e(g, h)
= e(g, h)β

=e

s

j=1

τj ·λj



(i,vi )∈Q



·e

(ξi )vi , h
(2)

α·γ

· e(g, h)β

s

j=1 (τj ·μj −τj ·λj )

(i,vi )∈Q



(ξi )vi , hα·γ

(i,vi )∈Q

(2)

 
s
μ
·
e(uj j , hβ ).
j=1

There exists a trivial solution when vi = 0 for all i ∈ I. In this case, the above equation could not
μ
determine whether the processed ﬁle is available, because σ  = 1, μj = λj , and πj = uj j . Hence, the
completeness of protocol holds
Pr[P (F, σ), V (pk, ψ) = 1]  1 − 1/pt ,
where t is the number of index coeﬃcient pairs in Q. In fact, we require vi ∈R Z∗p .
(2) Soundness: For every tag σ ∗ ∈ T agGen(sk, F ), in order to prove the nonexistence of fraudulent
P ∗ , to the contrary, we make use of P ∗ to construct a knowledge extractor M [7, 13], which gets the
common input (pk, ψ) and rewindable black-box accesses to the prover P ∗ , and then attempts to break
the computational Diﬃe-Hellman (CDH) assumption in G: given G, G1 = Ga , G2 = Gb ∈R G, output
Gab ∈ G. We have the following theorem:
Lemma 1. Our IPOR scheme has (t,  ) knowledge soundness in random oracle and rewindable knowledge extractor model assuming the (t, )-computational Diﬃe-Hellman (CDH) assumption holds in the
group G for   .
Proof. For some unavailable tags {σ ∗ } ∈ T agGen(sk, F ), we assume that there exists an interactive
machine P ∗ that can pass veriﬁcation with noticeable probability, that is, there exists a polynomial p(·)
and all suﬃciently large κ’s,
Pr[P ∗ (F, {σ ∗ }), V (pk, ψ) = 1]  1/p(κ).

(4)

Zhu Y, et al.

1614

Sci China Inf Sci

August 2011 Vol. 54 No. 8

Using P ∗ , we build a probabilistic algorithm M (called knowledge Extractor) that breaks the Computational Diﬃe-Hellman CDH problem in a cyclic group G ∈ S of order p. That is, given G, G1 , G2 ∈R G,
output Gab ∈ G, where G1 = Ga , G2 = Gb . The algorithm M is constructed by interacting with P ∗ as
follows:
Setup: M chooses a random r ∈R Zp and sets g = G, h = Gr , H1 = Gr1 , H2 = Gr2 as the public key
pk = (g, h, H1 , H2 ), which is sent to P ∗ .
i∈[1,n]

Learning: given a ﬁle F = {mi,j }j∈[1,s] , M ﬁrst chooses s random τi ∈R Zp and ui = Gτ2i for i ∈ [1, s].
Secondly, M assigns the indices 1, . . . , n into two sets T = {t1 , . . . , t n2 } and T  = {t1 , . . . , tn }. Let
2

mti ,j = mti ,j for all i ∈ [1, n/2] and j ∈ [1, s]. Then, M builds an index table χ and ξ (1) in terms of the
original scheme and generates the tag of each block, as follows:
s
τj ·mti ,j
(2)
.
• For each ti ∈ T , M chooses ri ∈R Zp and sets ξti = Hξ(1) (χti ) = Gri and σti = Gr1i · G2 j=1
r

• For each ti ∈ T  , M uses ri and two random ri , ζi ∈R Zp to sets ξt = Hξ(1) (χti ) = Gri · G2i and
(2)

s

σti = Gζ1i · G2

j=1

τj ·mt ,j
i

i

.

mt ,j
s
?
(2)
M checks whether e(σti , h) = e(ξt , H1 ) · e( j=1 uj i , H2 ) for all ti ∈ T  . If the result is true, then
i
 −1

outputs Gab = Ga2 = (Gζi · Gr1i )(ri ) , otherwise M sends (F, σ ∗ = {σi }ni=1 ) and ψ = (ξ (1) , u = {ui }, χ)
to P ∗ .
Hash queries: At any time, P ∗ can query the hash function Hξ(1) (χk ), M responds with ξti or ξt
i
while ensuring consistency, where k = ti or ti .

Output: M chooses an index set I ⊂ [1, n2 ] and two subsets I1 and I2 , where I = I1 I2 , |I2 | > 0. M
constructs the challenges {vi }i∈I and all vi = 0. Then M simulates V to run an interaction P ∗ , M as
follows:
• Commitment. M receives (H1 , π  ) from P ∗ ;
• Challenge. M sends the challenge Q1 = {(ti , vi )}i∈I to P ∗ ;
• Response. M receives (σ  , {μj }sj=1 ) from P ∗ .
M checks whether or not each response is an eﬀective result by eq. (3). If it is true, then M completes
a rewindable access to the prover P ∗ as follows:
• Commitment. M receives (H1 , π  ) from P ∗ ;

• Challenge. M sends the following challenge to P ∗ , Q2 = {(ti , vi )}i∈I1 {(ti , vi )}i∈I2 ;
• Response. M receives (σ  , {μj }sj=1 ) or a special halting-symbol from P ∗ .
If the response is not a halting-symbol, then M checks whether the response is eﬀective by eq. (3),
(2)

?

(2)

?

H1 = H1 , and π  = π  . If they are true, then M computes
γ=

μj − μj

i∈I2 vi · (mti ,j − mti ,j )

for any j ∈ [1, s] and veriﬁes H1 = H1γ to ensure this is an eﬀective rewindable access. Finally, M outputs
?

1



γ·(φ−1) i∈I ri vi γ· i∈I ri ·vi
2
,
Gab = Ga2 = σ  · σ −φ · G1



where
φ=

i∈I1

(5)


s

i∈I2
j=1 τj mti ,j vi +
j=1 τj mti ,j vi
 s
i∈I
j=1 τj mti ,j vi

s

and ψ = 1.
It is obvious that we set α = a and β = b in the above construction. Since the tags σti are available
for any ti ∈ T , the response in the ﬁrst interaction satisﬁes the equation:
 


s
μj
(2) vi



π · e(σ , h) = e
(ξti ) , H1 · e
uj , H2
i∈I

j=1

Zhu Y, et al.

Sci China Inf Sci


= e(G

i∈I

1615

August 2011 Vol. 54 No. 8

ri ·vi

, H1 ) · e


μ
uj j , H2 .



s
j=1

However, the values of {σti } are unavailable for all ti ∈ T  . In the second interaction, we require that
M can rewind the prover P ∗ , i.e., the chosen parameters are the same in two protocol executions [7, 13].
In above construction, this property ensures H1 = H1 , π  = π  , and for all i ∈ [1, s],


μj − μj = γ ·
vi · (mti ,j − mti ,j ) = γ ·
vi · (mti ,j − mti ,j ).
i∈I

i∈I2

By checking H1 = H1γ for all γ computed from this equation, we can make sure of the consistence of λi =

s
s
μ
−1
λi for i ∈ [1, s] in two executions. Thus, we have e( j=1 uj j , H2 ) · π  = e(G2 , H2 ) i∈I j=1 τj mti ,j vi
and


s

s

s
μ
τ m v
−1
j
e
uj , H2 · π  = e(G2 , H2 ) i∈I1 j=1 τj mti ,j vi · e(G2 , H2 ) i∈I2 j=1 j ti ,j i .
j=1

s
s
μ
μ
−1
−1
This means that e( j=1 uj j , H2 ) · π  = (e( j=1 uj j , H2 ) · π  )φ . In terms of the responses, we have


e(σ , h) = e



(2)
(ξti )vi

·

i∈I1

=e



i∈I2

=e



(Gri )vi ·

i∈I1





Gri ·vi , H1

=e

Gri ·vi , H1

i∈I

= e(σ , h) · e(G2
−φ

·e


s
j=1

(Gri · G2 )vi , H1



·e


i∈I2


·e





·e

μ
uj j , H2


s
j=1



· (π  )−1

μ

i∈I2

ri vi

· π 

−1

  

φ
s
μ
−1
r  ·v
G2i i , H1 · e
uj j , H2 · π 
j=1

  
−φ 

ri ·vi


φ
ri ·vi

G2 , H1 · e σ , h) · e( G
, H1
i∈I

· G(1−φ)


We have the equations e(σ  · σ  , h) = e(G2
thus eq. (5) holds. Furthermore, we have



uj j , H2

i∈I2



φ



ri

i∈I2

i∈I



(2)
(ξt )vi , H1
i

i∈I2


i∈I

ri ·vi

ri vi

, H1 ).

· G(1−φ)


i∈I

ri vi

, H1 ), H1 = haγ , and G1 = Ga ,

Pr[M(CDH(G, Ga , Gb )) = Gab ]  Pr[P ∗ (F, {σ ∗ }), M(pk, ψ) = 1]  1/p(κ).
It follows that M can solve the given -CDH challenge with advantage at least , as required. This
completes the proof of Theorem.
Lemma 2. The veriﬁcation protocol P roof (P, V ) is a computational zero-knowledge system in our
IPOR scheme.
Proof. For the protocol P roof (P, V ), we construct a machine S ∗ , which is called a simulator for the
interaction between V and P . Given the public key pk = (g, h, H1 , H2 ), for a ﬁle F , a public veriﬁcation
information ψ = (ξ (1) , u1 , . . . , us , χ), and a index set I (t = |I|), the simulator S ∗ (pk, ψ) executes the
following:
1. Chooses a random σ  ∈R G and computes e(σ  , h).
2. Chooses t random coeﬃcients {vi }i∈I ∈R Ztp and a random γ ∈R Zp to compute H1 ← H1γ and

A1 ← e( i∈I Hξ(1) (χi )vi , H1 ).
s
μ
3. Chooses s random {μi } ∈R Zsp to A2 ← e( j=1 uj j , H2 ).
4. Calculates π ← A1 · A2 · e(σ  , h)−1 .
5. Outputs S ∗ (pk, ψ) = (C, Q, θ) = ((H1 , π), {(i, vi )}ti=1 , (σ  , μ)).
It is obvious that the output of simulator S ∗ (pk, ψ) is an available veriﬁcation for eq. (3). Let
P (F, σ), V ∗ (pk, ψ) = ((H1 , π), {(i, vi )ti=1 }, (σ  , μ)) denote the output of the interactive machine V ∗
after interacting with the interactive machine P on common input (pk, ψ). In fact, every pair of variables

Zhu Y, et al.

1616
Table 1

Sci China Inf Sci

August 2011 Vol. 54 No. 8

The storage/communication and computation overheads in our IPOR scheme

Algorithm

Computation overheads

KeyGen

2[E]

2l0

TagGen

(2n + s)[E]

nsl0 + nl1

[B] + (s + 1)[E]

l2 + lT

t[E]

sl0 + l1

Commitment
Protocol

Challenge
Response
Veriﬁcation

Communication overheads

2tl0
3[B] + (t + s)[E]

is identically distributed in two ensembles, for example, H1 , {(i, vi )} and H1 , {(i, vi )} are identically
distributed due to the fact that the variables γ, {vi } ∈R Zp , as well as (σ  , μ) and (σ  , μ) are identically

distributed since σ  ∈R G, λj ∈R Zp and uj ← λj + γ i∈I vi · mi,j for i ∈ [1, s]. Two variables, π and
π, are computational indistinguishable because the π is identically distributed in terms of the random
choice of all λi and the distribution of π is decided on the randomized assignment of the above variables.
Hence, two ensembles, S ∗ (pk, ψ) and P (F, σ), V ∗ (pk, ψ), are computationally indistinguishable, thus
for every probabilistic polynomial-time algorithm D, for every polynomial p(·), and for all suﬃciently
large κ, it holds that




Pr[D (pk, ψ, S ∗ (pk, ψ)) = 1]−


  1/p(κ).

 Pr[D (pk, ψ, P (F, σ), V ∗ (pk, ψ))] = 1 
The fact that such simulators exist means that V ∗ does not gain any knowledge from P since the same
output could be generated without any access to P . That is, the protocol P roof (P, V ) is zero-knowledge.
According to Lemmas 1 and 2, we have the following theorem:
Theorem 1. Under CDH assumption, our IPOR scheme is a zero-knowledge proof of retrievability in
random oracle and rewindable extractor model.

6

Performances

We ﬁrst analyze the computation cost of IPOR scheme. For the sake of clarity, Table 1 presents the
results of our analyisis. In this table, we use [E] to denote the computation cost of an exponent operation
in G, namely, g x , where x is a positive integer in Zp and g ∈ G or GT . We neglect the computation cost
of algebraic operations and simple modular arithmetic operations because they run fast enough [14]. The
most complex operation is the computation of a bilinear map e(·, ·) between two elliptic points (denoted
as [B]).
Secondly, we analyze the storage and communication costs of our schemes. We deﬁne the bilinear
pairing taking the form e : E(Fpm ) × E(Fpkm ) → F∗pkm (we give here the deﬁnition from [15, 16]), where p
is a prime, m is a positive integer, and k is the embedding degree (or security multiplier). In this case, we
utilize asymmetric pairing e : G1 × G2 → GT to replace symmetric pairing in original schemes. Without
loss of generality, let the security parameter κ be 80-bits, we need the elliptic curve domain parameters
over Fp with |p| = 160-bits and m = 1 in our experiments. This means that the length of integer is
l0 = 2κ in Zp . Similarly, we have l1 = 4κ in G1 , l2 = 24κ in G2 , and lT = 24κ in GT for the embedding
degree k = 6. Based on these deﬁnitions, we describe storage or communication cost in Table 1. For a 1M
bytes ﬁle and s = 200, the extra storage of tags is 250 × 40 = 10K bytes (n = 250) and the commitment
and response overheads are 240 + 240 = 480 bytes and 200 × 20 + 40 ≈ 4K bytes, respectively. It is
obvious that the communication overhead has a constant size in the commitment and response steps of
veriﬁcation protocol. Furthermore, given a ﬁle with sz = n · s sectors and the probability ρ of sector
corruption, the detection probability of our scheme has P  1 − (1 − ρ)sz·ω , where ω denotes the sampling
probability in the veriﬁcation protocol.

Zhu Y, et al.

7

Sci China Inf Sci

August 2011 Vol. 54 No. 8

1617

Conclusions

In this paper, we addressed the construction of POR scheme on interactive proof systems. Based on an
interactive zero-knowledge proof, we proposed an interactive POR (IPOR) scheme to support soundness
property and zero-knowledge property. Our analysis showed that our schemes require a small, constant
amount of overhead, which minimizes computation and communication complexity.

Acknowledgements
This work was supported by the National Natural Science Foundation of China (Grant No. 61003216), and the
US National Science Foundation (Grant Nos. NSF-IIS-0900970, NSF-CNS-0831360). The authors gave thanks
to the collaborators at Arizona State University: Dijiang Huang and Stephen S. Yau for discussing the research
direction and the method for proofs, also to the intern student, Kainan Liu, at Peking University for verifying
the scheme by C++ Language.

References
1 Juels A, Kaliski-Jr B S. Pors: Proofs of retrievability for large ﬁles. In: Proceedings of the 2007 ACM Conference on
Computer and Communications Security, CCS 2007. Alexandria: ACM, 2007. 584–597
2 Ateniese G, Burns R C, Curtmola R, et al. Provable data possession at untrusted stores. In: Proceedings of the 2007
ACM Conference on Computer and Communications Security, CCS 2007. Alexandria: ACM, 2007. 598–609
3 Bowers K D, Juels A, Oprea A. Proofs of retrievability: Theory and implementation. In: Proceedings of the 2009 ACM
Workshop on Cloud Computing Security, CCSW 2009. Chicago: ACM, 2009. 43–54
4 Odis Y, Vadhan S P, Wichs D. Proofs of retrievability via hardness ampliﬁcation. In: Reingold O, ed. Theory of
Cryptography, 6th Theory of Cryptography Conference, TCC 2009. Lecture Notes in Computer Science, vol. 5444. San
Francisco: Springer-Verlag, 2009. 109–127
5 Wang Q, Wang C, Li J, et al. Enabling public veriﬁability and data dynamics for storage security in cloud computing.
In: Proceedings of the 14th European Symposium on Research in Computer Security, ESORICS 2009. Saint-Malo:
Springer-Verlag, 2009. 355–370
6 Shacham H, Waters B. Compact proofs of retrievability. In: Advances in Cryptology - ASIACRYPT 2008, 14th International Conference on the Theory and Application of Cryptology and Information Security. Melbourne: Springer-Verlag,
2008. 90–107
7 Goldreich O. Foundations of Cryptography: Basic Tools. Volume Basic Tools. Cambridge: Cambridge University Press,
2001
8 Christopher Erway C, Küpçü A, Papamanthou C, et al. Dynamic provable data possession. In: Proceedings of the 2009
ACM Conference on Computer and Communications Security, CCS 2009. Chicago: ACM, 2009. 213–222
9 Boneh D, Boyen X, Shacham H. Short group signatures. In: Proceedings of CRYPTO 2004, LNCS series. Santa Barbara:
Springer-Verlag, 2004. 41–55
10 Bowers K D, Juels A, Oprea A. Hail: A high-availability and integrity layer for cloud storage. In: ACM Conference on
Computer and Communications Security, CCS 2009. Chicago: ACM, 2009. 187–198
11 Boneh D, Franklin M. Identity-based encryption from the weil pairing. In: Advances in Cryptology (CRYPTO’2001),
vol. 2139 of LNCS. Santa Barbara: Springer-Verlag, 2001. 213–229
12 Schnorr C P. Eﬃcient signature generation by smart cards. J Cryptol, 1991, 4: 161–174
13 Cramer R, Damgård I D, MacKenzie P D. Eﬃcient zero-knowledge proofs of knowledge without intractability assumptions. In: Public Key Cryptography. Melbourne: Springer-Verlag, 2000. 354–373
14 Barreto P S L M, Galbraith S D, O’Eigeartaigh C, et al. Eﬃcient pairing computation on supersingular abelian varieties.
Des Codes Cryptogr, 2007, 42: 239–271
15 Beuchat J L, Brisebarre N, Detrey J, et al. Arithmetic operators for pairing-based cryptography. In: Cryptographic
Hardware and Embedded Systems - CHES 2007, 9th International Workshop. Vienna: Springer-Verlag, 2007. 239–255
16 Hu H G, Hu L, Feng D G. On a class of pseudorandom sequences from elliptic curves over ﬁnite ﬁelds. IEEE Trans Inf
Theory, 2007, 53: 2598–2605

2015 IEEE International Congress on Big Data

Distributed SPARQL over Big RDF Data
A Comparative Analysis using Presto and MapReduce
Mulugeta Mammo

Srividya K. Bansal

School of Computing, Informatics, and Decision
Systems Engineering (SCIDSE)
Arizona State University
Mesa, AZ, USA
mmammo@asu.edu

School of Computing, Informatics, and Decision
Systems Engineering (SCIDSE)
Arizona State University
Mesa, AZ, USA
srividya.bansal@asu.edu

query processing models work well to deliver faster response
times for SPARQL queries, which must be translated to
SQL. This paper makes the following novel contributions:
(i) Architecture of Presto-RDF framework that uses a
distributed in-memory query execution model, based on
Presto, to evaluate the performance of SPARQL queries over
big RDF data.
(ii) RDF-Loader component of Presto-RDF that uses mapreduce to load RDF data into the different storage structures
based on three storage schemes - triple-store, vertical and
horizontal scheme.
(iii) SPARQL to SQL compiler based on Flex and Bison.
The compiler is also unique in that it generates SQL for the
three RDF storage schemes.
(iv) Evaluation of query performance for the three RDF
storage schemes. Horizontal storage scheme had better
performance than the triple-store as the size of data
increases. No published results were found on the horizontal
storage scheme to the best of our knowledge.
The rest of the paper is organized as follows: Background
and related work is presented in section 2. The architecture
of Presto-RDF framework and RDF storage strategies are
presented in section 3. Section 4 describes the SPARQL to
SQL compiler. Section 5 describes the experimental setup
for performance evaluation of Presto-RDF and results.
Section 6 presents conclusions and future work.

Abstract—The processing of large volumes of RDF data
require an efficient storage and query processing engine that
can scale well with the volume of data. The initial attempts to
address this issue focused on optimizing native RDF stores as
well as conventional relational databases management systems.
But as the volume of RDF data grew to exponential
proportions, the limitations of these systems became apparent
and researchers began to focus on using big data analysis tools,
most notably Hadoop, to process RDF data. This paper
presents a comparative analysis of performance of Presto
(distributed SQL query engine) in processing big RDF data
against Apache Hive. To evaluate the performance Presto for
big RDF data processing, a map-reduce program and a
compiler, based on Flex and Bison, were implemented. The
map-reduce program loads RDF data into HDFS while the
compiler translates SPARQL queries into a subset of SQL that
Presto (and Hive) can understand.
Keywords - Big Data processing; Database Performance;
Evaluation; Querying; Semantic Web data

I.

INTRODUCTION

Resource Description Framework (RDF) enables the
representation of data as a set of linked statements, each of
which consists of a subject, predicate, and object called a
triple. RDF datasets, consisting of millions of triples, form a
network of directed graph (DG) and are stored in systems
called triple-stores. A query language standard, SPARQL,
has also been developed to query RDF datasets. For the
Semantic Web to work, both triple-stores and SPARQL
query processing engines have to scale well with the size of
data. This is especially true when the size of RDF data is too
big such that it is difficult, if not impossible, for conventional
triple-stores to work with [1] - [3] In the past few years,
however, new advances have been made in the processing of
large volumes of data sets, aka big data, which can be made
to use for processing big RDF data [4] – [6].
In the past two and half-years, new trends in big data
technology have emerged that use distributed in-memory
query processing engines based on SQL syntax. Some of
these tools include: Facebook Presto [7], Apache Shark [8],
and Cloudera Impala [9]. These tools promise to deliver high
performance query execution than traditional Hadoop system
like Hive [10]. It is the motivation of this paper to validate
this claim for big RDF data – i.e. if these new in-memory
978-1-4673-7278-7/15 $31.00 © 2015 IEEE
DOI 10.1109/BigDataCongress.2015.15

II. BACKGROUND AND RELATED WORK
This section presents the background on various RDF
storage schemes and review of related works that propose
and evaluate different distributed SPARQL query engines. It
also presents a review of two systems, Apache Spark and
Cloudera Impala, which are similar to Facebook Presto.
A. RDF Stores
RDF stores, also known as triple stores, are data
management systems that are used to store and query RDF
data. RDF stores also provide an interface, called a SPARQL
end–point, which can be used to submit SPARQL queries.
Some triple stores, like Sesame, also provide APIs that can
programmers can use to submit SPARQL queries and get
results. RDF storage managers can be broadly classified into
three categories:
33

• Am: resource-sharing architecture  memory, disk, or
nothing.
RDF triples can be stored and accessed in Hadoop
Distributed File System (HDFS) by creating a relational
layer on top of HDFS that maps triples into relational
schemas. Hive, for example, allows storing data in HDFS
based on a relational schema that defined by the user.
Though there are some discrepancies among researchers
regarding the naming and classification of relational schemas
for RDF data, most researchers classify these schemas in to
three groups [1], [4], [5], [17]:
• Triple table – the entire RDF data is stored as a single
table with three columns – subject, predicate and object.
Each triple is stored as a row in this table.
• Property-table – triples are grouped together by predicate
name. In this scheme, all triples with the same predicate are
stored in a separate table. Some researchers call property
tables vertical partitioning.
• Cluster-property tables – in this scheme triples are
grouped into classes based on correlation and occurrence of
predicates. A triple is stored in the table based on the
predicate class it belongs to.

• Native triple stores – are stores that are built from scratch
to store RDF triples. Native triple stores make a direct
use of the RDF data model – a labeled directed graph – to
store and access RDF data. These triple stores usually
store RDF data in a custom binary format. Examples are
4store [11], Jena TDB [23], and Sesame [24].
• Relational–backed triple stores – are triple stores that are
built on top of traditional RDBMs. Because RDBMs have
a number of well-advanced features that have developed
over the years, triple stores based RDBMs benefit from
these features. Examples are 3store [25], Jena SDB [23].
• NoSQL triple stores – are triple stores based on NoSQL
systems and by far are the largest triple stores in number.
This group includes triples stores based on NoSQL
systems as well as systems based on the Hadoop
ecosystem. Examples include: Hive+HBase [26],
CumulusRDF, Couchbase, and many more. Because
NoSQL systems include a wide range of systems,
including graph databases, some researches have
classified AllegroGraph [27], a graph database, both as a
native as well a NoSQL store.
Despite the large number of RDF triple stores that have been
developed and proposed by researchers, very few attempts
have been made to systematically parameterize triple stores
based on specific implementation parameters. Kiyoshi Nitaa
and Iztok Savnik [28] proposed a parameterized view of
RDF stores that is based on “single” and “multi–process”
attribute sets. In this view, an RDF Store Manager (RSM)
can be parameterized as function of single–process, S, and
multi–process attributes, M:

B. Distributed SPARQL
A distributed SPARQL query engine based on Jena ARQ
[12] has been proposed [13]. The query engine extends Jena
ARQ and makes it distributed across a cluster of machines.
Document indexing and pre-computation joins were also
used to optimize the design. The results of the experiments
that were conducted showed that the distributed query engine
scaled well with the size of RDF data but its overall
performance was very poor. The query engine, unlike
Facebook Presto, uses MapReduce similar to Hussain et al.
[31] approach of using Hadoop MapReduce framework to
store large RDF graphs and query them.
Marcello Leida et al. [14] propose a query processing
architecture that can be used to efficiently process RDF
graphs that are distributed over a local data grid. They
propose a sophisticated non-memory query planning and
execution algorithm based on streaming RDF triples. Presto
uses a distributed in-memory query-processing algorithm.
Xin Wang et al. [15] discuss how the performance of
distributed SPARQL query processing can be optimized by
applying methods from graph theory. The results of their
experiment show that a distributed SPARQL processing
engine based on MST-based algorithms performs much
better than other non-graph traversal algorithms. The
framework presented in this paper translates a SPARQL
query into its equivalent SQL query, and hence the query
optimization that is done by Presto is for the SQL query and
not for the SPARQL query. A distributed RDF query
processing engine based on a message passing has been
proposed [16]. The engine uses in-memory data structures to
store indices for data blocks and dictionaries. Just like
Presto, query-processing engine avoids disk I/O operations.

RSM = f (S, M)
= (<Ts, Is, Qs, Js, Cs, Ds, Fs>, <Dm, Qm, Sm, Am>)
The single–process attributes, S, this paper identifies are:
• Ts: type of triple table structure the store uses - vertical,
property-table, or horizontal
• Is: index structure type – 6 independent, GSPO-OGPS,
O matrix
• Qs: indicates whether a SPARQL endpoint is
implemented.
• Ss: indicates the translation method type of IRI and
literal strings  URI, literal, long, or none
• Js: join optimization method  RDBMS-based, columnstore based, conventional ordering, pruning, or none
• Cs: the cache type used  materialized path index,
reified statement, or none
• Ds: whether the store relies on traditional RDBMS or
uses its own custom database engine
• Fs: type of inference it supports  TBOX, ABox, or
none.
While multi–process attributes, M, are:
• Dm: data distribution method type  hash, data source
or none.
• Qm: query process distribution method type  data
parallel, data replication, or none.
• Sm: stream process type  pipeline or none.

C. Apache Spark and Cloudera Impala
Apache Spark [8] and Cloudera Impala [9] are two opensources systems that are very similar to Facebook Presto.

34

Both Apache Spark and Cloudera Impala offer in-memory
processing of queries over a cluster of machines. Spark uses
advanced Directed Acyclic Graph (DAG) execution engine
with cyclic data flow and in-memory processing to run
programs up to 100 (for in-memory processing mode) or 10
times faster (for disk processing mode) than Hadoop
MapReduce [8]. Cloudera Impala is an open-source
massively parallel processing (MPP) engine for data stored
in HDFS. Cloudera Impala is based on Cloudera’s
Distribution for Hadoop (CDH) and benefits from Hadoop’s
key features – scalability, flexibility, and fault tolerance.
Cloudera Impala, just like Presto, uses Hive Metastore to
store the metadata information of directories and files in
HDFS [9].

Once the raw RDF data is uploaded, RDF–Loader runs
several MapReduce jobs and stores the output back into
HDFS. The structure of data is defined by the schema that
can be specified by users of the system. In order for the
RDF–Loader to run and process raw RDF, the following
input parameters are required:
• database – is the name of the database that will be created.
• target – is the type of RDF storage structure, i.e. the type
of schema. There are four options: triples, vertical, wide,
and horizontal.
• expand – this option indicates if qnames are to be
expanded.
• server – is the DNS name or IP address of the master node,
NameNode, of the Hadoop cluster.
• port – is the port number Hadoop listens to connections.
• input – is the path of the HDFS directory that holds the
raw RDF data.
• output – is the path of the HDFS directory the processed
RDF data will be stored.
• format – defines the format of the output files as they are
stored in HDFS. The current version of the Hive meta–
store supports five different formats: SEQUENCEFILE,
TEXTFILE, RCFILE, ORC, and AVRO. This study
makes use of the TEXTFILE format.
The following sections discuss four different RDF storage
strategies implemented by the RDF–Loader.
In the triple-store storage scheme, an RDF triple is stored
as is – resulting in a table with three columns: subject,
predicate and object. If the raw RDF data has 30 million
triples, the triple store strategy will have one table with 30
million rows. The map–reduce algorithm that transforms the
raw RDF data into the triples table is quite simple and shown
below.

III. PRESTO-RDF ARCHITECTURE
This section proposes architecture, called Presto–RDF,
which can be used to store and query big RDF data using the
Hadoop Distributed File System (HDFS) and Facebook
Presto. It also presents RDF–Loader, one of the key
components of the architecture, which is used to read, parse
and store RDF triples.

Figure 1: Presto-RDF Architecture

map (String key, String value)
// key: RDF file name
// value: file contents
for each triple in value
emit_intermediate (subject + '\t ' + predicate, object)
reduce (String key, Iterator values)
// key: subject and predicate delimited by tab
// values: list of object values
for each v in values
emit (subject + '\t ' + predicate + object);

A. Architecture
Presto–RDF consists of the following components: a
command line interface (CLI), a SPARQL to SQL compiler
(RQ2SQL), Facebook Presto, Hive Metastore, HDFS, and
RDF–Loader. Figure 1 illustrates the different components
of the architecture. RDF data that is extracted from the
Semantic Web is parsed and loaded into HDFS using a
custom–made RDF-loader, which will also store metadata
information on Hive Thrift Server. When a user submits a
SPARQL query over a command line interface, the query is
processed by a custom–made SPARQL to SQL converter,
RQ2SQL, that translates the SPARQL query into SQL which
would then be submitted to Facebook Presto. Presto, using
its Hive connector and Hive Thrift Server, runs the SQL
against HDFS and returns the result back to the CLI.

In the wide table RDF storage scheme, the raw RDF data
is parsed and stored as a single table having one column for
subject values, and multiple predicate columns for object
values. Because it is unlikely that a subject has all the
predicates found in the data set, this storage strategy will
have a number of null values. For an RDF data set that has
unique object values for a subject–predicate pair, this scheme
would result in a table that has s number of rows, where s is
the number of subjects in the data set. If the dataset,
however, contains multiple values for the same subject–
predicate pair, the table will have multiple rows for the same
subject. The storage scheme, thus, forces new rows to be
created for each unique subject–predicate pair. The map–
reduce algorithm for the wide table storage scheme, as
implemented in this study, is shown in the table below:

B. RDF–Loader
The purpose of the RDF–Loader is to load, parse, and store
RDF data in HDFS. RDF–Loader implements four different
RDF storage schemes and creates external Hive tables whose
metadata is stored in the Hive Thrift server. Before the RDF–
Loader is executed the raw RDF data to be first processed is
loaded into HDFS using this command:
hadoop fs –put file hdfs–dir

35

IV. RQ2SQL - SPARQL TO SQL COMPILER

map (String key, String value)
// key: RDF file name
// value: file contents
for each triple in value
emit_intermediate (<subject, predicate>, <predicate, object>)

This section presents a SPARQL to SQL mini-compiler that
is developed as part of Presto–RDF. RQ2SQL (RDF Query
to SQL) converts SPARQL queries into SQL statements that
can be run on Presto and Hive. RQ2SQL is implemented in
Flex and Bison using C++11.

reduce (String key, Iterator values)
// key: a <subject, predicate> pair
// values: list of <predicate, object> pairs
String subject = key.getSubject();
String[] row = new String[1 + num_unique_predicates];
int i = 0
for each v in values
row[i] = v.getObject();
i++;
emit (subject, row);

A. SPARQL Graph Patterns
SPARQL query processing is based on graph pattern
matching. Complex graph patterns can be constructed by
combining few basic graph pattern techniques. The W3C
classifies SPARQL graph pattern matching into five smaller
patterns [14]: Basic Graph Pattern, Group Graph Pattern,
Optional Graph Pattern, Alternate Graph Pattern and Named
Graph Pattern.
• Basic graph patterns: are set of triple patterns where the
pattern matching is defined in terms of joining the results
from individual triples. A single graph pattern is composed
of a sequence of triples that may optionally be interrupted
by filter expressions. The below example is a basic graph
pattern.

The horizontal storage scheme is similar to the wide table
storage scheme in terms of the schema of the table.
However, unlike the wide–table scheme, it optimizes the
number of rows stored for subjects that have multiple object
values for the same predicate. In this scheme, it is not
necessary to create new rows for each unique subject–
predicate pair. Instead, rows that are already created for the
same subject, but for a different predicate will be used.
In the vertical storage scheme implemented in this
research, the raw RDF data is partitioned into different tables
based on the predicate values of the triples in the data with
each table having two columns – the subject and object
values of the triple. Thus, if the raw RDF data has 30 million
triples that have 20 unique predicates, the vertical storage
scheme will create 20 tables and stores the subject and object
values of triples that share the same predicate in the same
table. The map–reduce algorithm works with predicate as a
key value and a pair of subject and object values as value:

PREFIX foaf:
<http://xmlns.com/foaf/0.1/>
SELECT ?name ?age
WHERE {
?x foaf:name ?name .
?x foaf:age ?age .
}

RQ2SQL translates the above SPARQL into the SQL :
SELECT T0.object,
T1.object
FROM http___xmlns_com_foaf_0_1_name T0
JOIN http___xmlns_com_foaf_0_1_age T1
ON (T1.subject = T0.subject)

map (String key, String value)
// key: RDF file name
// value: file contents
for each triple in value
emit_intermediate (predicate, <subject, object>);
reduce (String key, Iterator values)
// key: predicate
// values: list of <subject, predicate> pairs
String table = key.replace_unwanted('_');
MultipleOutputs<String, String> mos;
for each v in values
// create a directory table
// write the subject, values inside the directory
mos.write (v.getFirst(), v.getSecond(), table);

• Group graph pattern: is specified by delimiting it with
braces. The below example specifies one graph pattern
with two basic graph patterns.
PREFIX foaf:
<http://xmlns.com/foaf/0.1/>
SELECT ?name ?age
WHERE {
{ ?x foaf:name ?name . }
{ ?x foaf:age ?age . }
}

The RQ2SQL translation, for the vertical store, is the same
as the previous:
SELECT T0.object,
T1.object
FROM http___xmlns_com_foaf_0_1_name T0
JOIN http___xmlns_com_foaf_0_1_age T1
ON (T1.subject = T0.subject)

Because predicate values are URIs that contain non–alpha
numeric characters, e.g. http://www.w3.org/1999/02/22–rdf–
syntax–ns#, which cannot be used in naming directories, the
reducer has to replace these characters with some other
character, for example the underscore character, and creates
the directory (which is considered as a table for the Hive
Metastore). In the vertical storage scheme, for a raw RDF
data that contains n number of triples, the mapper runs at
O(n) while the reducer runs at O(p*x) where p and s are the
number of unique predicates and subjects in the data set,
respectively. In the worst case scenario, where there are as
many unique predicates and subjects, the number of triples,
the map-reduce algorithm for the vertical storage scheme
runs at O (n2).

•

Optional graph patterns: re specified using the
OPTIONAL keyword. The semantics of the optional
graph pattern matching is that it either adds additional
binding to the solution or would leave it unchanged.

Given the following RDF data:
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix rdf: <http://www.w3.org/1999/02/22–rdf–syntax–ns#> .
_:a rdf:type
foaf:Person .
_:a foaf:name "Michael" .

36

_:a
_:a
_:b
_:b

foaf:email
foaf:email
rdf:type
foaf:name

SELECT DISTINCT T0.object
FROM http___xmlns_com_foaf_0_1_name T0

<mailto:michael@example.com> .
<mailto:michael@hahusofware.com> .
foaf:Person .
"Mulugeta" .

•

A SPARQL optional graph pattern query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name ?email
WHERE {
?x foaf:name ?name .
OPTIONAL {
?x foaf:email ?email
}
}

PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT REDUCED ?name WHERE {
?x foaf:name ?name
}

Would give the following result:
name
“Michael”
“Michael”
“Mulugeta”

Reduced modifier: unlike the distinct modifiers that
ensures that duplicate solutions are eliminated from the
solution sequence, the reduced modifier, specified by
the REDUCED keyword, permits them to be eliminated.
The result set of a solution sequence with a reduced
modifier is at least one and at most the cardinality of the
solution sequence without the distinct and reduce
modifiers.

email
<mailto:michael@example.com>
<mailto:michael@hahusoftware.com>

•

The RQ2SQL translation, for the vertical store, is the same
as the previous. Constraints can also be applied to optional
graph patterns.
• Alternate graph Patterns: are constructed by specifying
the keyword UNION between two graph patterns.
• Named graph patterns: are constructed by specifying a
FROM NAMED IRI where each IRI is used to provide
one named graph in the RDF dataset. Using same IRI in
two or more NAMED clauses would result in one named
graph.

•

RQ2SQL does not support the REDUCED keyword.
Offset modifier: just like SQL, the offset modifier,
specified by the OFFSET keyword, returns results of the
solution sequence starting at the specified offset value.
Offset value of 0 has no effect. Both Presto and Hive do
not support the OFFSET keyword.
Limit modifier: just like SQL, the LIMIT modifier puts
an upper bound to the number of solution sequences
returned. A limit value of 0 would return no results. A
negative limit value is not valid.
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE {
?x foaf:name ?name
}
LIMIT 20

# Graph: http://example.org/bob
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
_:a foaf:name "Mulugeta" .
_:a foaf:email <mailto:mulugeta@hahusoftware.com> .

RQ2SQL translation:
•

# Graph: http://example.org/alice
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
_:a foaf:name "Michael" .
_:a foaf:email <mailto:michael@example.com> .
...
FROM NAMED <http://example.org/michael>
FROM NAMED <http://example.org/mulugeta>

SELECT TOP 20 T0.object
FROM http___xmlns_com_foaf_0_1_name T0

ASK query modifier – SPARQL queries specified using
the ASK form test whether or not a SPARQL query has
a solution. Given the following triples:
@prefix foaf:
<http://xmlns.com/foaf/0.1/> .
_:a foaf:name
"Michael" .
_:a foaf:homepage <http://work.example.org/michael/> .
_:b foaf:name
"Mulugeta" .
_:b foaf:mbox
<mailto:mulugeta@hahusoftware.com> .

...
RQ2SQL does not support Named graph patterns.

Running the SPARQL query:
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
ASK { ?x foaf:name "Michael" }

B. SPARQL solution sequences and modifiers
The results returned from a SPARQL query are unordered
collection of single or composite values that, according the
W3C, can be regarded as solution sequences with no specific
order. SPARQL defines six solution modifiers: order,
projection, distinct, reduced, offset and limit.
• Order modifier: is specified by the ORDER BY clause
and forms the order of a solution sequence. Ordering can
be qualified as ASC for ascending or DESC for
descending.
• Projection modifier: is specified by listing a subset of
variables defined in the pattern-matching clause.
• Distinct modifier: is specified by the DISTINCT
keyword and filters out duplicates from the solution
sequence.

Returns the value: yes.
RQ2SQL does not support the ASK query modifier.
C. RQ2SQL
RQ2SQL is a mini SPARQL to SQL compiler built using
Flex – a lexical analyzer creator – and Bison – a parser
generator creator. RQ2SQL supports basic SPARQL queries
including OPTIONALS, FILTERS as well as ORDER BY,
DISTINCT, projection and LIMIT modifiers. However, it
does not support UNION, ASK, named graph patterns as
well as group graph patterns. RQ2SQL generates SQL
queries for the four different RDF storage schemas explained
in previous section – triple, wide, horizontal, and vertical.
Translating LUBM queries to SQL: RQ2SQL was tested for
correctness by compiling the 14 LUBM benchmark queries
against Presto and then comparing the result with the output

PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT DISTINCT ?name WHERE { ?x foaf:name ?name }

RQ2SQL translation:

37

Q12:

generated after running same queries on 4store. This section
presents selected queries from LUBM and their RQ2SQL
translation for the vertical storage scheme.

PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX ub: <http://www.lehigh.edu/~zhp2/2004/0401/univbench.owl#>
SELECT ?X ?Y
WHERE {
?X rdf:type ub:Chair .
?Y rdf:type ub:Department .
?X ub:worksFor ?Y .
?Y ub:subOrganizationOf <http://www.University0.edu>
}

Q1:
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX ub: <http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#>
SELECT ?x WHERE {
?x rdf:type ub:GraduateStudent.
?x ub:takesCourse
<http://www.Department0.University0.edu/GraduateCourse0>.
}

Graph representation:

Graph representation:

Figure 4 – LUBM Query 12 [30]

Figure 2 – LUBM Query 1 [30]

RQ2SQL translation:

RQ2SQL translation – for the vertical storage scheme:

Q13:
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX
ub:
<http://www.lehigh.edu/~zhp2/2004/0401/univbench.owl#>
SELECT ?x WHERE {
?x rdf:type ub:Person.
<http://www.University0.edu> ub:hasAlumnus ?x.
}

SELECT T1.Subject
FROM http___www_w3_org_1999_02_22_rdf_syntax_ns_type T0
JOIN http___www_lehigh_edu__zhp2_2004_0401_univ_bench_
owl_takesCourse T1 ON (T1.Subject = T0.Subject)

Q4:
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX ub: <http://www.lehigh.edu/~zhp2/2004/0401/univbench.owl#>
SELECT ?x ?y1 ?y2 ?y3 WHERE {
?x rdf:type ub:Professor.
?x ub:worksFor
<http://www.Department0.University0.edu>.
?x ub:name ?y1.
?x ub:emailAddress ?y2.
?x ub:telephone ?y3.
}

RQ2SQL translation:
SELECT T1.Object FROM
http___www_w3_org_1999_02_22_rdf_syntax_ns_type T0 JOIN
http___www_lehigh_edu__zhp2_2004_0401_univ_bench_owl_has
Alumnus T1 ON (T1.Object = T0.Subject)

V. COMPARATIVE ANALYSIS OF PRESTO-RDF AND HIVE
This section presents the experiments and the results
conducted to benchmark the performance of Presto-RDF
against Hive. A comparative measurement was also done on

Graph representation:

SELECT T2.Subject, T3.Subject
FROM http___www_w3_org_1999_02_22_rdf_syntax_ns_type T0
JOIN http___www_w3_org_1999_02_22_rdf_syntax_ns_type T1
JOIN
http___www_lehigh_edu__zhp2_2004_0401_univ_bench_owl_works
For T2 ON (T2.Object = T1.Subject AND T2.Subject = T0.Subject)
JOIN
http___www_lehigh_edu__zhp2_2004_0401_univ_bench_owl_subOr
ganizationOf T3 ON (T3.Subject = T2.Object)

Figure 3 – LUMB Query 4 [30]

4store – a native RDF store. For the experiment, four
benchmark queries (# 1, 6, 8, 11) from SP2Bench [18], [19]
were used and three different RDF storage schemes were
evaluated – triple, vertical and horizontal stores. SP2Bench
is a SPARQL benchmark that is designed to test SPARQL
queries over RDF triples stores as well as SPARQL–to–
SQL re–write systems. SP2Bench focuses on how well an
RDF store supports the different SPARQL operators and
their combination – known as operator constellations [18],
[19]. SP2Bench data model is based on the DBLP,
http://www.informatik.uni–trier.de/~ley/db/, a computer
science bibliography created in the 1980s and currently

RQ2SQL translation:
SELECT T4.Subject, T2.Object, T3.Object, T4.Object
FROM http___www_w3_org_1999_02_22_rdf_syntax_ns_type T0
JOIN
http___www_lehigh_edu__zhp2_2004_0401_univ_bench_owl_works
For T1 ON (T1.Subject = T0.Subject) JOIN
http___www_lehigh_edu__zhp2_2004_0401_univ_bench_owl_name
T2 ON (T2.Subject = T1.Subject) JOIN
http___www_lehigh_edu__zhp2_2004_0401_univ_bench_owl_email
Address T3 ON (T3.Subject = T2.Subject) JOIN
http___www_lehigh_edu__zhp2_2004_0401_univ_bench_owl_teleph
one T4 ON (T4.Subject = T3.Subject)

38

featuring more than 2.3 million articles. The SP2Bench data
generator can generate any number of triples based on what
a user specifies. For the experiments conducted in this
study, for example, triples of size 10, 20, and 30 million
were generated. The experimental setup that was conducted
involved setting up four and eight node clusters on
Microsoft Windows Azure Platform. Each node in the
cluster had a 2-core x86-64 processor, 14GB of memory,
and 1TB of hard disk. Measurements were conducted for the
four-benchmark queries for 10, 20, and 30 million triples.
Figures 5 and 6 show the performance of Presto-RDF on
Query 1 (Q1) over a 4-node and 8-node cluster respectively.

C. Presto vs. Hive for Q6 and Q8
For Q6 as well, Presto-RDF has a much higher performance
than Hive. The SQL translations for Q8 involve multiple
JOINs (just as the case were in Q6) and a UNION. The
results have the same behavior as Q6 – the vertical store has
a much better performance than the triple-store and
horizontal stores, and Presto-RDF has a much higher
performance than Hive. Figure 11 below shows the results
of running the above queries over 10, 20 and 30M triples.

Figure 8. Q6 Performance on
Presto-RDF with 4 Nodes

Figure 5: Presto-RDF Query
Processing Time of Ql over a 4
node Cluster

Figure 9. Q6 Performance on PrestoRDF with 8 Nodes

Figure 6: Presto-RDF Query
Processing Time of Ql over a 8
node Cluster

A. Presto vs. Hive for Q1
Compared to Hive, Presto once again has a much higher
performance. Figure 7 shows a comparison of Presto and
Hive for 30M triples.
Figure 10. Performance
Comparison of Presto-RDF and Hive,
for 30M Triples on 8-node Cluster

Figure 11. Q8 performance, on
Presto-RDF with 4 nodes

Figure 7: Q1 performance for 30M triples on 8-node cluster

B. Result for Q6:
The SQL translations for query Q6, unlike Q1, involve
multiple JOINs for each of the three storage. The results of
the evaluation on a 4-node and 8-node cluster are shown in
Figure 8 and 9 below. The results of the evaluation above
indicate that the performance increased with increase in the
number of nodes. The vertical store, has a much better
performance than the triple-store and horizontal store.
Unlike Q1, however, where the horizontal store had a
slightly better performance than the triple-store, the triplestore in Q6 had a slightly better performance than the
horizontal store, especially as the size of the triples
increases. This result can be explained by the fact that the
horizontal store SQL for Q6, unlike the triple-store, involves
multiple selections before making JOINs. For Hive, unlike
Presto-RDF, as the number of nodes was increased there
was a drop in performance – which can be attributed to
increase in replication across nodes and disk I/O operations.

Fig. 12. Q11 Performance, on PrestoRDF with 4 Nodes

Fig. 13. Q11 Performance, on PrestoRDF with 8 Nodes

D. Result for Q11:
Because Q11 involves just one table that has less number of
rows for the vertical and horizontal storage schemes than the
triple-store (which is one table), the results shown above are
expected. For 8 nodes, there is a performance improvement –
see Figure 13.
VI. CONCLUSIONS AND FUTURE WORK
This paper proposed a Presto-based architecture, PrestoRDF that can be used to store and process big RDF data and
SPARQL to SQL compiler. This paper also presented a
comparative analysis of big RDF data using Presto, which
uses in-memory query processing engine, and Hive, which
uses MapReduce to evaluate SQL queries. From the
experiments conducted, following conclusions can be
drawn:

39

• For all queries, Presto-RDF has a much higher
performance than Hive.
• The vertical storage scheme has a consistent
performance advantage than both the triple-store or
horizontal storage schemes.
• As the size of data increases, the horizontal storage
scheme performed relatively better than the triple-store
scheme. This is unlike the articles reviewed during this
research study, which ignore the horizontal scheme as
being not efficient (because it has many null values).
• Increasing the number of nodes improved query
performance in Presto but not in Hive. This can be
explained by the fact that Hive replicates data across
clusters and does IO operations – which increase as
the size of nodes increase.
There are a number of areas to extend this study:
this paper used a single benchmark, SP2Bench. This work
can be investigated on different benchmarks such as
LUBM [21], BSBM [22], and DBPedia [6]. There are
different optimization techniques that can be applied to
the three storage schemas as well as to the RDF data
directly. The RDF data is stored as a text file, which is not
optimal. This work can be extended to test using RCFILE,
ORC, AVRO formats, which are better optimized than
text file.

[9]

[10]
[11]
[12]
[13]

[14]

[15]

[16]
[17]
[18]

[19]

[20]
[21]

REFERENCES
[1]

[2]

[3]
[4]
[5]

[6]

[7]

[8]

[22]

Y. Luo, F. Picalausa, G. H. Fletcher, J. Hidders, and S.
Vansummeren, “Storing and indexing massive RDF datasets,” in
Semantic Search over the Web, Springer, 2012, pp. 31–60.
P. Cudré-Mauroux, I. Enchev, S. Fundatureanu, P. Groth, A. Haque,
A. Harth, F. L. Keppmann, D. Miranker, J. F. Sequeda, and M.
Wylot, “Nosql databases for rdf: An empirical evaluation,” in The
Semantic Web–ISWC 2013, Springer, 2013, pp. 310–325.
RDF, “Efficient RDF Storage and Retrieval in Jena2,” 2003.
S. Sakr and G. Al-Naymat, “Relational processing of RDF queries: a
survey,” ACM SIGMOD Record, vol. 38, no. 4, pp. 23–28, 2010.
D. J. Abadi, A. Marcus, S. R. Madden, and K. Hollenbach, “Scalable
semantic web data management using vertical partitioning,” in
Proceedings of the 33rd international conference on Very large data
bases, 2007, pp. 411–422.
M. Morsey, J. Lehmann, S. Auer, and A.-C. N. Ngomo, “DBpedia
SPARQL benchmark–performance assessment with real queries on
real data,” in The Semantic Web–ISWC 2011, Springer, 2011, pp.
454–469.
“Presto: Interacting with petabytes of data at Facebook.” [Online].
Available: https://www.facebook.com/notes/facebookengineering/presto-interacting-with-petabytes-of-data-atfacebook/10151786197628920. [Accessed: 02-Dec-2014].
“Apache Spark” [Online]. Available: https://spark.apache.org/.
[Accessed: 02-Dec-2014].

[23]
[24]

[25]
[26]
[27]
[28]

[29]
[30]
[31]

40

“The Platform for Big Data and the Leading Solution for Apache
Hadoop in the Enterprise - Cloudera.” [Online]. Available:
http://www.cloudera.com/content/cloudera/en/home.html.
“Apache Hive.” [Online]. Available: https://hive.apache.org/.
[Accessed: 02-Dec-2014].
“4store-Scalable RDF storage.” [Online]. Available:
http://www.4store.org/.
“Apache Jena SPARQL Processor” [Online].
http://jena.apache.org/documentation/query/.
P. Kulkarni, “Distributed SPARQL query engine using MapReduce,”
Master of Science, Computer Science, School of Informatics,
University of Edinburgh, 2010.
M. Leida and A. Chu, “Distributed SPARQL Query Answering over
RDF Data Streams,” in Big Data (BigData Congress), 2013 IEEE
International Congress on, 2013, pp. 369–378.
X. Wang, T. Tiropanis, and H. C. Davis, “Evaluating graph traversal
algorithms for distributed SPARQL query optimization,” in The
Semantic Web, Springer, 2012, pp. 210–225.
A. K. Dutta, M. Theobald, R. Schenkel, “A Distributed In-Memory
SPARQL Query Processor based on Message Passing,” 2012.
A. Harth, K. Hose, and R. Schenkel, Linked Data Management. CRC
Press, 2014.
M. Schmidt, T. Hornung, G. Lausen, and C. Pinkel, “SP^ 2Bench: a
SPARQL performance benchmark,” in Data Engineering, 2009.
ICDE’09. pp. 222–233.
“The SP2Bench SPARQL Performance Benchmark.” [Online].
Available: http://dbis.informatik.unifreiburg.de/forschung/projekte/SP2B/. [Accessed: 02-Dec-2014].
“DBLP.” [Online]. Available: http://dblp.org/search/index.php.
[Accessed: 02-Dec-2014].
Y. Guo, Z. Pan, and J. Heflin, “LUBM: A benchmark for OWL
knowledge base systems,” Web Semantics: Science, Services &
Agents on WWW, vol. 3, no. 2, pp. 158–182, 2005.
“Berlin SPARQL Benchmark.” [Online]. Available: http://wifo503.informatik.uni-mannheim.de/bizer/berlinsparqlbenchmark/.
[Accessed: 02-Dec-2014].
K. Wilkinson, C. Sayers, H. A. Kuno et al., “Efficient rdf storage and
retrieval in jena2.” in SWDB, vol. 3, 2003, pp. 131–150
Jeen Broekstra, Arjohn Kampman, Frank van Harmelen, "Sesame: A
Generic Architecture for Storing and Querying RDF and RDF
Schema", June 2002
Stephen Harris, Nicholas Gibbins, "3store: Efficient Bulk RDF
Storage", 2003
Albert Haque, Lynette Perkins, "Distributed RDF Triple Store Using
HBase and Hive", December 2012
AllegroGraph, "Semantic Graph Technologies",
http://franz.com/agraph/allegrograph, 2014
Y. Theoharis, V. Christophides, and G. Karvounarakis.
"Benchmarking database representations of RDF/S stores”. In Proc.
of ISWC, 2005
World Wide Web Consortium (W3C), "SPARQL Query Language
for RDF", 2006
The Lehigh University Benchmark (LUBM),
http://swat.cse.lehigh.edu/projects/lubm, 2014
M. F. Husain, J. P. McGlothlin, M. M. Masud, L. R. Khan, B. M.
Thuraisingham, “Heuristics-Based Query Processing for Large RDF
Graphs Using Cloud Computing”, IEEE Trans. Knowledge Data
Engineering, 23(9): 1312-1327, 2011

Annotating UDDI Registries to Support the Management of
Composite Services
M. Brian Blake, Michael F. Nowlan, Ajay Bansal, and Srividya Kona
Department of Computer Science
Georgetown University
Washington, DC
202 687-3084

{mb7,mfn3,ab683,sk558}@georgetown.edu
Moreover, the syntactic and semantic metadata that accompanies
these services [9][11] enable the discovery of these capabilities,
on-demand. Discovery, in this environment, largely depends on
the accessibility and capabilities of the repositories for which
these services are stored. Universal Description, Discovery, and
Integration (UDDI) is the leading specification for the
development of service-based repositories or registries. UDDI
registries of the future should facilitate fast search and discovery
of relevant web services akin to the performance currently
associated with resolving a domain name. Although, performance
and federation are two important aspects of UDDI, an additional
requirement for UDDI should be effective process-oriented
storage and retrieval. Currently, the abilities to browse and
discover independent services as characterized by their
overarching business name and/or their capability name are
important fundamental operations. However, as service-oriented
architectures mature, composite services (i.e. capabilities based
on the workflow composition of multiple atomic services) will
also be important to persist and manage. UDDI currently has
limited support for managing business processes [6]. Although
not all federated service registries will need business process
annotations, we suggest that a subset of registries frequently used
by partnering organizations would benefit from maintaining
historical process information.

ABSTRACT
The future of service-centric environments suggests that
organizations will dynamically discover and utilize web services
for new business processes particularly those that span multiple
organizations. However, as service-oriented architectures mature,
it may be impractical for organizations to discover services and
orchestrate new business processes on a daily, case-by-case basis.
It is more likely that organizations will naturally aggregate
themselves into groups of collaborating partners that routinely
share services. In such cases, there is a requirement to maintain
an organizational memory with regards to the capabilities offered
by other enterprises and how they fit within relevant business
processes. As a result, registries must maintain information about
past business processes (i.e. relevant web services and their
performance, availability, and reliability). This paper discusses
and evaluates several hybrid approaches for incorporating
business process information into standards-based service
registries.

Categories and Subject Descriptors
D.2.11 [Software]: Software Engineering: Software Architectures
– domain specific architectures.

General Terms
Performance,
Design,
Standardization

Experimentation,

Security,

Currently, there are numerous languages and protocols
that support the specification and execution [3][4][5][9] of
composite web services.
Unfortunately, techniques for
incorporating the underlying process information into UDDI
registries are limited. In this work, we address several questions
as listed below.

and

Keywords
Keywords are your own designated keywords.

x

1. INTRODUCTION

x

Service-oriented computing [10] promotes the development
of modular domain-specific capabilities that can be advertised to
and shared among collaborating organizations.

x
x

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SAC’09, March 8-12, 2009, Honolulu, Hawaii, U.S.A.
Copyright 2009 ACM 978-1-60558-166-8/09/03…$5.00.

What are the relevant descriptive attributes of composite web
services that must be represented in web service registries?
What are the relevant use cases for process-oriented service
registries?
What are the state-of-the-art methods for incorporating
process information into registries and the corresponding
challenges?
What are the most efficient and effective approaches, both
qualitatively and quantitatively, for process management of
services in UDDI registries?

This paper proceeds in the next section with a survey of related
work and a discussion of the state of the practice. The next
section formalizes a composite service by detailing the most
relevant descriptive attributes.
Next, we describe the

2146

the process information can be valuable. Luo et al. [8] and
Srinivasan et al. [14] use semantic notations embedded in UDDI
tModels to associate services. This approach allows for more rich
definitions, but the underlying limitations caused by distribution
also apply. Other approaches look at the development of external,
integrated software mechanisms that run parallel with the UDDI
registry [18] [19]. These approaches tend to depart from the
essence of the SOA paradigm as they promote proprietary, less
standardized solutions.

requirements of a process-oriented registry. We next introduce
several hybrid approaches for adding process information to web
service repositories. Finally, we describe a case study and
experimental evaluation of both approaches.

2. RELATED WORK
Universal Description, Discovery and Integration (UDDI)
describes the data model associated with a web-accessible registry
for the storage and management of web services. Three core
hierarchical objects specify the service provider (businessEntity),
the web service (businessService), and information about how the
service is binded (bindingTemplate). These foundational objects
can be extended with the use of technical models or tModels.
TModels facilitate the further description of businessEntities,
businessServices, and bindingTemplates through classification
based on metadata. Each tModel of a businessService represents
a certain behavior or classification system that the service must
implement. An example would be a web service that takes a state
abbreviation as input. This web service would probably choose to
reference the tModel that represents the US-State abbreviation
classification system.
A person looking at the service’s
bindingTemplate would then be able to see a reference to the USState System and know that a state should be entered with its
abbreviation. UDDI also supports other structures called
keyedReferences that allow previously mentioned core objects to
be associated to tModels. KeyedReferences consist of a
tModelKey, keyValue, and keyName. The tModelKey identifies
the referenced tModel. The keyValue allows a categorization of
the link between the core object, and the tModel and the keyName
is a text string readable for humans. In UDDI v3,
keyedReferences can be aggregated with keyedReferenceGroups.

In this paper, we experiment with a combination of external
process documentation and annotations that are embedded
directly in the UDDI registry. Prerequisite to any solution, it is
important to understand the required aspects of the web servicesbased business process

3. ANATOMY OF WEB SERVICES-BASED
PROCESSES
Web-services based business processes also referred to as web
service workflows are similar to traditional processes that are
established between human stakeholders. We propose a model
that intersects business process semantics with web service-based
data management techniques. There are few approaches that
formally describe this intersection. Figure 1 illustrates the
metamodel of a web services-based business process using a
Unified Modeling Language (UML) class diagram. A difference
is, as opposed to human-managed tasks or steps, web services
enact the underlying steps. A web services-based business
process, BP, contains user data endpoints, DE , defined below.

The strength of the tModels and keyedReferences is that
further information about the main UDDI objects (i.e.
businessEntities, businessServices, and bindingTemplates) are not
populated in the repository. Tmodels merely point to webaccessible documents. This paradigm both reduces maintenance
of the registry and promotes overall robustness. However, this
paradigm also makes it difficult to associate web services that are
stored in the registry, which is a necessary requirement for
describing business processes within the registry.
The most common research projects tend to address the
problem of federating UDDI registries [1] [2][12], although, of
most relevance to our work, are the projects that directly address
the problem of business process annotations that associate
services. Perhaps the leading approach to inserting business
processes is the construction of a tModel classification system
that mirrors a particular taxonomy of business processes. These
tModels can then be used as pointers to the corresponding
business process description documents. In industry, several
OASIS technical reports [15] [17] describe high-level approaches
to integrating tModel classifications with ebXML and BPEL4WS
descriptions. Other research projects detail specialized domainspecific methods that leverage the same basic approach [6] [13]
These are reasonable approaches, but all business descriptions are
defined by external business process documents that are
decoupled from the individual services. In fact, since services
may be captured at individual locations, replacing services or
even discontinuing the offering of a particular business process
becomes difficult. In these cases, centralizing some vital part of

Figure 1. Metamodel of a Web Service-Based Business Process.
Definition (UserData EndPoint):
The user data end point is defined as a pair DE = (ID, OD) where
ID represents the input information of the business process
provided by the user and OD represents the output information,
ultimately generated by the completion of the business process.
The business process also has a sequence of tasks (realization of
the steps) that are implemented by a set of services,  = {S1, S2,
S3, ….Sn}. Each service Si has its own input, ISi, and output, OSi,
information; however the set of all input/output information of a
service is less relevant than the subset of inputs and outputs that

2147

are relevant to the business process. In addition, a service can also
be defined with its quality of service information and its URI
location.

4. BUSINESS PROCESS AND SERVICE
REGISTRIES

Definition (Service):
A service is a tuple of its inputs, outputs, QoS parameters, and its
URI location. S = (IS , OS , QS , US) is the representation of a
service where IS is the input list, OS is the output list, QS is the list
of quality of service parameters and US represents the URI
location. Each step in the business process is defined by a
transition, T, that defines the shared information between the
output, OT, of the preceding step that connects to the input, IT , of
the subsequent step.

4.1 Potential Use Cases

In order for organizations to understand their business processes
defined with web services, it is important that their process
databases include relevant process information as defined in the
previous section. Organizations should be able to generally access
process information in addition to the service-specific details.
There are several functions required by a registry that supports
composite services as business processes. Figure 3 illustrates the
functions of such a repository depicted as a UML use case
diagram. The basic registry actors follow the SOA paradigm (i.e.
service providers and consumers). Incorporating business process
metadata into the registry also supports the interaction of
intelligent software components or agents to autonomously
maintain the integrity of the information. Service providers should
be able to insert one or more services into the registry. Service
consumers should be able to either browse or explicitly search the
repository based on several attributes such as the name/type of
business, service, or process. Although only available using
specialized approaches, consumers may also want to search by
service/process message names. We focus on three major features
of such a repository.

Definition (Transition):
A transition is represented as a tuple of its inputs, outputs, flowtype, pre-conditions, and post-conditions. T = (IT , OT ,FT , CPre ,
CPost) is the representation of the transition where IT is the input
list, OT is the output list, FT is the flow type, CPre represents the
pre-conditions of the transition and CPost represent the postconditions of the transition. Ultimately, the business process can
be formally defined as follows:
Definition (BusinessProcess):
A BusinessProcess is defined as a tuple BP = (DE, , ) where DE
represents the user data endpoint,  is the set of services involved
in the business process, and  is the set of transitions in the
workflow .

DE = (ID, OD);
 = {S1, S2, ..., Sn} where Si = (ISi , OSi , QSi , USi),
for all i = 1 to n;
 = {T1, T2, .…, Tn} where Ti = (ITi , OTi ,FTi , CPrei ,
CPosti), for all i = 1 to n.

x

Advertising a set of services aggregated as a process

When partnering organizations decide to share services, there
may be a predefined understanding for orchestration. As such,
these organizations must insert their relevant services into shared
UDDI registries annotated by process-based information (i.e. the
underlying control flow and data flow).
x

The following conditions should hold for a valid business
process:
For all Si   and Ti  , the inputs of Si are
1.
subsumed by the inputs of Ti , i.e., ISi C ITi
For all Si   and Ti  , the outputs of Si
2.
subsumes the outputs of Ti , i.e., OTi C OSi
For all Ti , Ti+1  , the post-conditions of Ti imply
3.
the pre-conditions of Ti+1 , i.e., CPosti => CPre i+ 1
For all Ti , Ti+1  , the outputs of Ti along with
4.
the user data inputs of the business process
subsume the inputs of Ti+1 , i.e., (OTi U ID) C IT i+ 1
The inputs of the business process subsume the
5.
inputs of the first transition, T1 (where T1   ),
i.e., IT1 C ID
The outputs of the business process are subsumed
6.
by the outputs of the last transition Tn (where Tn 
), i.e., OD C OTn

Discovering services associated with processes
discovering processes associated with services

and

Current UDDI registries facilitate browsing of services by
business name and by service name. A process-oriented registry
will also support browsing by process name or type. Consumers
should also be able to find all services associated with a particular
process.
x Managing process information by software agents
Once process information is annotated into a registry, intelligent
agents can regularly check the health of the underlying services.
Agents can look for indexing configurations that best support the
storage of the process information. In addition, agents can record
QoS information supplied by service consumers. This QoS
information can represent individual services or the process as a
whole.

4.2 Alternative Hybrid Approaches
By extending and leveraging the UDDI specification, we have
identified several approaches for annotating business processes
within service registries. Every service in a UDDI registry has a
bindingTemplate structure, which stores references to tModels.
TModels are “sources for determining compatibility of Web
services and keyed namespace references” [16]. This means that
tModels identify how to interact with a web service by describing
the technologies it implements. Although, the UDDI registry

The cardinality of data endpoints, services, and transitions vary
for each step, such that is necessary to develop containers to
aggregate the information into sets. The notion of containers is
central to business process languages, such as BPEL4WS and
BPML [3] [4], for aggregating information related to
subprocesses.

2148

usually implements default tModels, such as the State
Abbreviation System, it is also possible for an administrator of the
registry to create tModels, on-demand. There are two approaches
for using tModels to describe web services-based business
processes.

<tModel tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14">
<description>This tModel represents the process starting with
firstService and ending with thirdService. </description>
<categoryBag>
<keyedReference keyName="uddi:Process-Representing"
keyValue="categorization"
tModelKey="uddi:uddi.org:categorization:types"/>
</categoryBag>
</tModel>

x Annotating business process information directly into the
UDDI registry
A new tModel is created for every business process that is
identified in the registry. In addition, a parent tModel is created
that simply classifies any tModel that annotates a business process
as such. In this way, when a new process chain is added or
identified in the registry, a tModel that points to the parent tModel
is created to represent the process. Furthermore, the categoryBag
element is used to store references to all the processes of which
the service is a part. Figure 2 shows an example tModel. Notice
that the keyName of the keyedReference contains the service
name and additional control flow information.
Also, the
keyValue maintains the sequence number of the service in the
process. Figure 4 demonstrates the actions taken by the registry
to identify and store the existence of a composite web service.
x

<businessService>
<name>firstService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV=firstService, TRAN = Sequence"
keyValue="1"/>
</categoryBag>
</businessService>
<businessService>
<name>secondService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV = secondService, TRAN = Sequence"
keyValue="2"/>
</categoryBag>
</businessService>

Defining business process information using external
markup documents

Figure 5 details the steps for annotating a business process within
the registry using the UDDI data structures. Perhaps the leading
approach in related work is the use of an external document (e.g.
BPEL4WS and ebXML) to store the process information. This
method involves simply adding an entry to each of the process
documents associated with the relevant services. The document
could exist centrally on a main server, or locally with each service
provider.

<businessService>
<name>thirdService</name>
<categoryBag>
<keyedReference
tModelKey="176a3131-0c20-45d1-b31d-efb4f61b8b14"
keyName="SERV=thirdService, TRAN = Sequence"
keyValue="3"/>
</categoryBag>
</businessService>

Figure 2. Sample Process-Oriented TModel.

Figure 3. Process-Oriented Service Repository Use Cases
(** Shaded areas are not currently well supported in the state-of-the-art)

2149

Annotate( VS): Annotating Function
Vector of serviceKeys for services in chain
VS
The first service in the vector
GSFIRST
The last service in the vector
GSLAST
TM
The newly created process-tModel
The newly created tModel’s key
TMKEY
PM
The tModel Process-Classification System
CB
The CategoryBag of an individual structure
S
An individual service
KR
KeyedReference – A pointer to a tModel
Function to add a KR to CB of Services
A(VS, TM)
MakeTM(GSFIRST , GSLAST ) Function to make a new TM

cause the process workflow chain to be incomplete. In this case,
all the services in the chain are affected and they must modify
their process annotation method (either XML document or
CategoryBag) by removing the process that is broken. When a
service is deleted, all the tModels that it points that represent
composite web services (excluding simple classifications
represented in the bindingTemplate) must be deleted. Any other
services in the registry that reference these tModels must remove
the reference from their categoryBags. In the future, registries
may be able to search for replacements services as opposed to
deleting the process.

A(VS , TM)
KR = TMKEY
forAll S in VS
S.CB += KR
return VS
MakeTM(GSFIRST , GSLAST )
TM.CB += PM
TM.description = GSFIRST , GSLAST
return TM

x Service Deletions or Changes when Using External Process
Documentation
External process documents contain one entry for each process.
Irrespective of the process document notations, each entry will
define the serviceKeys of the services in the particular process. In
this approach, these serviceKeys are used to find all services that
share a process with the deleted service. The registry then edits
the relevant XML documents by removing the entry that
corresponds to the broken process.

Annotate(VS)
TM = MakeTM(GSFIRST , GSLAST )
return A(VS , TM)

x Query for Processes by Service Name using UDDI Data
Structure Method

Figure 4. Annotating a Business Process of Web Services within
the UDDI Data Model.

The last action that can be taken on a registry, and perhaps the
most desired ability, is the ability to aggregate the information
and list of services for every process in which a particular service
plays a role. The serviceKey is used to get the service’s
categoryBag. The categoryBag contains references to all the
tModels, that represent all relevant processes. The registry is then
queried to find all services which point to any one of the tModels
in their categoryBags. The chain of services is then sorted by the
tModels and is returned as process.

Annotate(VS):
Annotating Function
MakeEntry(VS ) Function to make a new TM
Function to add entry to XML Doc.
A(VS , E)
VS
Vector of serviceKeys for services in chain
E
An entry containing a list of services
S
An individual service
S.KEY
Service key
D
A service’s description element
A service’s XML document file
FS
A(VS , E)
forAll S in VS
S. FS += E
return VS

x Query for Processes by Service Name using External Process
Documentation
The external document method is quicker because it does not
require querying the registry. The XML document for a service is
retrieved. In this document is a list of entries, each pertaining to
one composite web service. Each entry contains the serviceKey
for each service in that particular chain. These services can be
retrieved with a single call to the registry by compiling a vector of
the serviceKeys. The retrieved service information is then sorted
by the order of the entries in the initial service’s process
document and displayed.

MakeEntry(VS)
forAll S in VS
E += S.KEY
return E
Annotate(VS)
forAll S in VS
S.D = FS
E = MakeEntry(VS )
return A(VS , E)

Figure 5. Leveraging External Process Documents.

5. CASE STUDY: A GENERAL UDDI
BUSINESS EXPLORER

4.3 Alternative Hybrid Approaches
Both of the hybrid approaches also have other functions relevant
to a business-oriented registry.

Once process information is associated with or incorporated into a
UDDI registry, new graphical user interfaces can be created to
support enhanced service discovery and manipulation. As a part
of this work, we designed and experimented with a new user
interface front-end (i.e. UDDI-P) to the jUDDI registry Error!
Reference source not found.. A screenshot of the design of the
interface is shown in Figure 6. Using this new interface, a user

x Service Deletions or Changes when using UDDI Structure
Method
As the registry is updated when a process is identified, it will also
change when a service is removed or replaced. These actions

2150

has the capability of browsing known process names as shown on
the left side of Figure 6. Once a process is identified, the
consumer can further decide to analyze the process to see if it
meets the organization need.
The interface can display
parameters such as estimated delivery date and price range based
on service level objectives associated with the process stored in
the registry. By embedding process information into the registry,
classification of processes can occur to the point where they
themselves function as individual services. On the right side of
the interface, the user has the ability to browse each process
meeting the search criteria by price and delivery. This sort of
interface would enable a separate interface that allows the
services to be executed.

6. EXPERIMENTATION & EVALUATION
In previous sections, two distinct hybrid approaches for
aggregating UDDI services into business processes were
introduced. The leading approaches in published works suggest
using external mark-up documents to describe business processes.
An alternative approach is incorporating specific business process
information directly into the registry. We experimented to
characterize the performance of these approaches on our
prototype repository. The performance is illustrated in Table 1
and Figure 7 based on the use cases illustrated in Figure 3.
In general, the difference in performance between adding
individual services, adding a chain, and annotating services with
business process annotations were only negligibly different
between the two approaches. However, deleting a service was
approximately three times faster when external business process
documents were used. Another variation in performance is
associated with retrieving all service IDs associated with a
particular process (or aggregating the services). The aggregation
time increased linearly with the increase in size of the UDDI
registry embedded process information. The main reason for
disparity between the approaches is due to the fact that the latter
approach requires a significant query within the repository. This
approach must retrieve the categoryBag for all services in the
repository and search those structures for a particular
keyedReference. The external business process document does
not require this step since the process information for a service is
stored in one location: the XML-based document. The retrieval
of this document is much faster when compared to the query that
must take place within the registry. Although the performance for
an external file is more favorable, having external BPEL4WS
files causes the duplication of process information (i.e. the same
process entry could appear in multiple files that may be attached
to the underlying services). Depending on the management of the
BPEL4WS files, centralizing the process within the repository
may be more advantageous. In such cases, embedding processes
with the registry represents an effective solution.

Figure 6. UDDI-P: A Prototype User Interface for Process-Based
Service Management.
Table 1. Performance of External Document and Annotated UDDI Repository (milliseconds)

Repo
Size

Add
Serv.
(ms)

Add
Composite
(ms)

Annotate
(Note)
(ms)

Collect
Service IDs
by Process
(Aggregate)
(ms)

Ext.
Process
Doc

150
330
600
850

1573
1573
1573
1573

2500
2700
2600
2600

1500
1700
1600
1600

1500
1500
1500
1500

1900
1900
1900
1900

Internal
UDDI
Data
Structure

150
330
600
850

1573
1573
1573
1573

2800
3000
3000
3000

2100
2100
2100
2100

2600
4000
5700
7450

7790
7790
7790
7790

2151

Del. Serv.
(ms)

9000
8000
7000
6000
5000
4000
3000
2000
1000
0

Add Service
Add Composite
Note
Aggregate

External XML Document

Repo=850

Repo=600

Repo=330

Repo=150

Repo=850

Repo=600

Repo=330

Repo=150

Delete Service

Annotated UDDI

Figure 7. Comparison of the Performance (ms) for Storing Process Information in External Document versus Annotating the UDDI
Repository (by number of services).
[3] WS-BPEL(2008 ):
http://www.ibm.com/developerworks/library/specification/w
s-bpel/

7. CONCLUSIONS
An innovation in this paper is the formalized model for web
services-based business process and the relevant use cases for
using this information. In addition, we introduce the design of a
new interface for business-based UDDI interactions.
Our
experimentation evaluates the two leading approaches for
capturing process information in UDDI registries. Overall
performance information does not suggest a quantitative
advantage for embedding process information directly into the
repository. However, qualitatively, maintenance is less extensive
since process information is centralized in a potentially federated
registry. As future work, we plan to continue developing a
process-oriented UDDI explorer and experiment on new
approaches for interface design.

[4] BPML (2008): http://www.ebpml.org/bpml.htm (currently
moved to OMG)
[5] BPMN (2008): http://www.bpmn.org/
[6] Dogac, A., Tambag, Y., Pembecioglu, P, Pektas, S., Laleci,
G., Gokhan, K., Toprak, S., and Kabak, Y. “An ebXML
infrastructure implementation through UDDI registries and
RosettaNet PIPs” Proceedings of the 2002 ACM SIGMOD
Conference (SIGMOD 2002), Madison, Wisconsin, June
2002
[7] jUDDI (2008): http://ws.apache.org/juddi/
[8] Luo, J., Montrose, B., Kim, A., Khashnobish, A., Kang, M.
“Adding OWL-S Support to. the Existing UDDI
Infrastructure” Proceedings of the 4th International
Conference on Web Services (ICWS2006), Chicago, Ill,
November 2006.

8. ACKNOWLEDGMENTS
We acknowledge fruitful conversations with Brian Schott and
Robert Graybill of the University of Southern California, ISI-East
and Suzy Tichenor of the Council of Competitiveness. This
material is based on research sponsored by DARPA under
agreement number FA8750-06-1-0240. This U.S. Government is
authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation thereon. The
views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing the
official policies or endorsements, either expressed or implied, of
DARPA or the U.S. Government.

[9] OWL-S(2008): http://www.daml.org/services/owl-s/
[10] Papazoglou, M. “Service-oriented computing: Concepts,
characteristics and directions. In Proc. of WISE ‘03
[11] RDF (2008): http://www.w3.org/RDF/
[12] Sivashanmugam, K., Verma, K., and Sheth, A. Discovery of
Web Services in a Federated Registry Environment,
Proceedings of 4th IEEE International Conference on Web
Services (ICWS), pp. 270-278, 2004.
[13] Spies, M., Schoning, H., and Swenson, K. “Publishing
Interoperable Services and Processes in UDDI” The 11th
Enterprise Computing Conference (EDOC 2007), Annapolis,
MD, October 2007

9. REFERENCES
[1] Al-Masri, E. and Mahmoud, Q.H., “Crawling Multiple UDDI
Business Registries”, Proceedings of the 16th International
Conference on the World Wide Web, Banff, Alberta, Canada,
2007

[14] Srinivasan, N., Paolucci, M. and Sycara, K. "Adding OWL-S
to UDDI, implementation and throughput," Proceedings of
First International Workshop on Semantic Web Services and
Web Process Composition (SWSWPC 2004), San Diego,
California, USA, 2004

[2] Blake, M.B., Sliva, A.L., zur Muehlen, M., and Nickerson, J.
“Binding Now or Binding Later: The Performance of UDDI
Registries”, IEEE Hawaii International Conference of
System Sciences (HICSS-2007), Track on Technology and
Strategies for Realizing Service-oriented Architectures with
Web services, January 2007

[15] UDDI as the registry for ebXML Components, OASIS
Technical Note, February 2004, Accessed (2008):
http://www.oasis-open.org/committees/uddispec/doc/tn/uddi-spec-tc-tn-uddi-ebxml-20040219.htm

2152

Representation Chain”, In Proceedings of the International
Conference on Web Services (ICWS 2004), 2004

[16] Universal Description, Discovery, and Integration (UDDI)
(2008): http://www.uddi.org/pubs/uddi_v3.htm

[19] Zhang, M., Cheng, Z., Zhao, Y. Huang, J.Z. Yinsheng,
L., Zang, B. “ADDI: an agent-based extension to UDDI for
supply chain management” Proceedings of the Ninth Int
Conference on CSCW in Design, Shanghai, China, May 2005

[17] Using BPEL4WS in UDDI Registry, OASIS Technical Note,
July 2005 Accessed (2008): http://www.oasisopen.org/committees/uddi-spec/doc/tn/uddi-spec-tc-tn-bpel20040725.htm
[18] Zhang, L-J., Zhou, Q., and Chao, T., “A Dynamic Services
Discovery Framework for Traversing Web Services

2153

Semantics-based Web Service Composition engine
Srividya Kona, Ajay Bansal, Gopal Gupta
Department of Computer Science
The University of Texas at Dallas
Richardson, TX 75083

Abstract
Service-oriented computing is gaining wider acceptance. We need an infrastructure that allows users and applications to discover, deploy, compose and synthesize services automatically. In this paper we present an approach
for automatic service discovery and composition based on
semantic description of Web services. The implementation
will be used for the WS-Challenge 2007 [1].

1. Introduction
In order to make services ubiquitously available, we need
a semantics-based approach such that applications can reason about a service’s capability to a level of detail that permits their discovery, deployment, composition and synthesis [6]. Informally, a service is characterized by its input
parameters, the outputs it produces, and the side-effect(s)
it may cause. The input parameter may be further subject
to some pre-conditions, and likewise, the outputs produced
may have to satisfy certain post-conditions. For discovery and composition, one could take the syntactic approach
in which the services being sought in response to a query
simply have their inputs syntactically match those of the
query. Alternatively, one could take the semantic approach
in which the semantics of inputs and outputs, as well as a
semantic description of the side-effect is considered in the
matching process. Several efforts are underway to build an
infrastructure for service discovery, composition, etc. These
efforts include approaches based on the semantic web (such
as USDL [4], OWL-S [7], WSML [8], WSDL-S [9]) as well
as those based on XML, such as Web Services Description Language (WSDL [5]). Approaches such as WSDL are
purely syntactic in nature, that is, they only address the syntactical aspects of a Web service. In this paper we present
our approach for automatic service composition which is
an extension of our implementation that we used at WSChallenge 2006 [3].
In section 2 we present the formal definition of the ComThe 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

Thomas D. Hite
Metallect Corp.
2400 Dallas Parkway
Plano, TX 75093

position problem. We describe our Service Composition algorithm in section 3. Section 4 presents the design of our
software with brief descriptions of the different components
of the system followed by conclusions and references.

2. Automated Web service Discovery and Composition
Discovery and Composition are two important tasks related to Web services. In this section we formally describe
these tasks. We also develop the requirements of an ideal
Discovery/Composition engine.

2.1. The Discovery Problem
Given a repository of Web services, and a query
requesting a service (we refer to it as the query service
in the rest of the text), automatically finding a service
from the repository that matches these requirements is the
Web service Discovery problem. Valid solutions to the
query satisfy the following conditions: (i) they produce
at least the query output parameters and satisfy the query
post-conditions; (ii) they use only from the provided input
parameters and satisfy the query pre-conditions; (iii) they
produce the query side-effects. Some of the solutions may
be over-qualified, but they are still considered valid as
long as they fulfill input and output parameters, pre/post
conditions, and side-effects requirements.

2.2. The Composition Problem
Given a repository of service descriptions, and a query
with the requirements of the requested service, if a matching service is not found, then the composition task can be
performed. The composition problem involves automatically finding a directed acyclic graph of services that can be
composed to obtain the desired service. Figure 1 shows an
example composite service made up of five services 1 to
0
0
5 . In the figure, I and C I are the query input parameters
and pre-conditions respectively. O0 and C O0 are the query

S

S

S2
S5
CI’,I’

CO’,O’

S3
S1

S4

Figure 1. Example of a Composite Service as
a Directed Acyclic Graph

S

S

output parameters and post-conditions respectively. Informally, the directed arc between nodes i and j indicates
that outputs of i constitute (some of) the inputs of j .
Discovery and composition can be viewed as a single
problem. Discovery is a simple case of composition where
the number of services involved in composition is exactly
equal to one.
Definition (Service): A service is a 6-tuple of its preconditions, inputs, side-effect, affected object, outputs and
post-conditions.
=( ; ; ;
;
;
) is the representation of a service where
is the pre-conditions,
is the input list,
is the service’s side-effect,
is the
affected object,
is the output list, and
is the postconditions.
Definition (Repository of Services): Repository is a set of
Web services.
Definition (Query): The query service is defined as
0
0
0
0
= ( 0; 0; 0;
;
;
) where
is the pre0
0
is the input list,
is the service affect,
conditions,
0
0
is the affected object, 0 is the output list, and
is the post-conditions. These are all the parameters of the
requested service.
Definition (Composition): The Composition problem can
be defined as automatically finding a directed acyclic graph
= ( ; ) of services from repository , given query =
0
0
0
( 0; 0; 0;
;
;
), where is the set of vertices
and is the set of edges of the graph. Each vertex in the
graph represents a service in the composition. Each outgoing edge of a node (service) represents the outputs and postconditions produced by the service. Each incoming edge of
a node represents the inputs and pre-conditions of the service. The following conditions should hold on the nodes of
the graph:
where i has zero incoming edges,
1. i i
0
0
,
i
i.
i i
2. i i
where i has zero outgoing edges,
0
0
,
i
i.
i
i
3. i i
where i has at least one incoming edge,
let i1, i2 , ..., im be the nodes such that there is
a directed edge from each of these nodes to i . Then
0
0
Ii
I , CI i
( i1
C I ).
i2 :::
im
k ik
The meaning of the is the subsumption (subsumes) reis the implication relation. Figure 2 explains
lation and
one instance of the composition problem pictorially. When

S

S

S CI I A AO O CO
CI
A
AO
O
CO

Q CI I A AO O CO
I
A
AO
O
G VE
CI I A AO O CO
E

CI

V

R

I

CO
Q

8 SS 2 V
S
I w I CI )^ CI
8 SS 2 V
S
O v O CO (^ CO
8 S 2V
S
S S S
S
vS O [ ( CO ^CO ^CO ^
v
)

The 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

Figure 2. Composite Service
the number of nodes in the graph is equal to one, the composition problem reduces to the discovery problem. When all
nodes in the graph have not more than one incoming edge
and not more than one outgoing edge, the problem reduces
to a sequential composition problem.

2.3. Requirements of an ideal Engine
The features of an ideal Discovery/Composition engine
are:
Correctness: One of the most important requirement for
an ideal engine is to produce correct results, i.e, the services discovered and composed by it should satisfy all the
requirements of the query. Also, the engine should be able
to find all services that satisfy the query requirements.
Small Query Execution Time: Querying a repository of
services for a requested service should take a reasonable
amount of (small) time, i.e., a few milliseconds. Here we
assume that the repository of services may be pre-processed
(indexing, change in format, etc.) and is ready for querying.
In case services are not added incrementally, then time for
pre-processing a service repository is a one-time effort that
takes considerable amount of time, but gets amortized over
a large number of queries.
Incremental Updates: Adding or updating a service to an
existing repository of services should take a small amount
of time. A good Discovery/Composition engine should not
pre-process the entire repository again, rather incrementally
update the pre-processed data (indexes, etc.) of the repository for this new service added.
Cost function: If there are costs associated with every service in the repository, then a good Discovery/Composition
engine should be able to give results based on requirements
(minimize, maximize, etc.) over the costs. We can extend
this to services having an attribute vector associated with
them and the engine should be able to give results based
on maximizing or minimizing functions over this attribute
vector.
These requirements have driven the design of our

S1
I=I
CI, I

1

S1
S2

O1

I=IUO
2

1

1

S

O

2

I=IUO
3

2

2

3

.
.

.
.

O
S

3

I=IUO
4

3

3

4

S

O

4

O

5

.
.

.
.

Query
described
using
USDL
(S)

.
.
.
Sn

Infer
Sub-queries

Discovery Module
(Discovery Engine + Service
Directory + Term Generator)
S1

Figure 3. Composite Service

..........................

Sn

Composition Engine
(implemented using
Constraint Logic
Programming

Composed Service

semantics-based Composition engine described in the following sections.
Pre-Cond(S)
S1
Pre-Cond( S)1

3. A Multi-step Narrowing Solution
We assume that a directory of services has already been
compiled, and that this directory includes semantic descriptions for each service. In this section we describe our Service Composition algorithm.
Service Composition Algorithm: For service composition, the first step is finding the set of composable services.
The correct sequence of execution of these services can be
determined by the pre-conditions and post-conditions of the
individual services. That is, if a subservice 1 is composed
with subservice 2 , then the post-conditions of 1 must imply the pre-conditions of 2. The goal is to derive a single
solution, which is a directed acyclic graph of services that
can be composed together to produce the requested service
in the query. Figure 4 shows a pictorial representation of
our composition engine.
In order to produce the composite service which is represented by a graph as shown in figure 1, we filter out services that are not useful for the composition at multiple
stages. Figure 3 shows the filtering stages for the particular
instance shown in figure 1. The composition routine starts
with the query input parameters. It finds all those services
from the repository which require a subset of the query input parameters. In figure 3, C I ; I are the pre-conditions and
the input parameters provided by the query. 1 and 2 are
the services found after step 1. 1 is the union of all outputs produced by the services at the first stage. For the next
stage, the inputs available are the query input parameters
and all the outputs produced by the previous stage, i.e., 2
= 1 I . 2 is used to find services at the next stage, i.e., all
those services that require a subset of 2. In order to make
sure we do not end up in cycles, we get only those services
which require at least one parameter from the outputs produced in the previous stage. This filtering continues until all
the query output parameters are produced. At this point we
make another pass in the reverse direction to remove redundant services which do not directly or indirectly contribute
to the query output parameters. This is done starting with
the output parameters working our way backwards.
Algorithm: Composition
Input: QI - QueryInputs, QO - QueryOutputs, QCI -

S

S

S

S

O

O[ I

S

I

S

I

The 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

Post-Cond( S)1

S2

................................. S n

Post-Cond( S)n
Post-Cond(S)

Pre-Cond( S)2

Figure 4. Composition Engine
Pre-Cond, QCO - Post-Cond
Output: Result - ListOfServices
1. L NarrowServiceList(QI, QCI);
2. O GetAllOutputParameters(L);
3. CO GetAllPostConditions(L);
4. While Not (O QO)
5.
I = QI O; CI QCI CO;
6.
L’ NarrowServiceList(I, CI);
7. End While;
8. Result
RemoveRedundantServices(QO, QCO);
9.Return Result;

[

w

^

4. Implementation
Our composition engine is implemented using Prolog
[10] with Constraint Logic Programming over finite domain
[11], referred to as CLP(FD) hereafter. In this section we
briefly describe our software system and its modules. The
details of the implementation along with performance results are shown in [2].
Triple Generator: The triple generator module converts each service description into a triple as follow:
(Pre-Conditions, affect-type(affected-object, I, O), PostConditions).
The function symbol affect-type is the side-effect of the service and affected object is the object that changed due to
the side-effect. I is the list of inputs and O is the list of outputs. Pre-Conditions are the conditions on the input parameters and Post-Conditions are the conditions on the output
parameters. Services are converted to triples so that they
can be treated as terms in first-order logic. In case conditions on a service are not provided, the Pre-Conditions and
Post-Conditions in the triple will be null. Similarly if the
affect-type is not available, this module assigns a generic
affect to the service.
Query Reader: This module reads a query file (in XML
format, possibly different from the XML format used for a

service) and converts it into a triple used for querying in our
engine.
Semantic Relations Generator: We obtain the semantic
relations from the provided ontology. This module extracts
all the semantic relations and creates a list of Prolog facts.
Composition Query Processor: The composition engine is
written using Prolog with CLP(FD) library. It uses a repository of facts, which contains list of services, their input and
output parameters and the semantic relations between the
parameters. The following is the code snippet of our composition engine:
composition(sol(Qname,A)) :dQuery(Qname,_,_),
minimize(compTask(Qname,A,SeqLen),SeqLen).
compTask(Qname, A, SeqLen) :dQuery(Qname,QI,QO), encodeParam(QO,OL),
narrowO(OL,SL), fd_set(SL,Sset),
fdset_member(S_Index,Sset),
getExtInpList(QI,InpList),
encodeParam(InpList,IL), list_to_fdset(IL,QIset),
serv(S_Index,SI,_), list_to_fdset(SI,SIset),
fdset_subtract(SIset,QIset,Iset),
comp(QIset,Iset,[S_Index],SA,CompLen),
SeqLen #= CompLen + 1, decodeS(SA,A).
comp(_, Iset, A, A, 0) :- empty_fdset(Iset),!.
comp(QIset, Iset, A, SA, SeqLen) :fdset_to_list(Iset,OL),
narrowO(OL,SL), fd_set(SL,Sset),
fdset_member(SO_Index,Sset), serv(SO_Index,SI,_),
list_to_fdset(SI,SIset),
fdset_subtract(SIset,QIset,DIset),
comp(QIset,DIset,[SO_Index|A],SA,CompLen),
SeqLen #= CompLen + 1.

The query is converted into a Prolog query that looks as follows:
composition(queryService, ListOfServices).
The engine will try to find a ListOfServices that can be composed into the requested queryService. Our engine uses the
built-in, higher order predicate “bagof” to return all possible ListOfServices that can be composed to get the requested
queryService.
Output Generator: After the Composition Query processor finds a matching service, or the graph of atomic
services for a composed service, the results are sent to
the output generator in the form of triples. This module
generates the output files in any desired XML format. For
the WS-Challenge, this module will produce output files in
the format provided [1].
For this year’s challenge, the software has to receive requests and return results via SOAP. Hence our software will
work as a Web service whose interface will accept the discovery/composition query.
The 9th IEEE International Conference on E-Commerce
Technology and The 4th IEEE International Conference
on Enterprise Computing, E-Commerce and E-Services(CEC-EEE 2007)
0-7695-2913-5/07 $25.00 © 2007

5. Conclusion
To catalogue, search and compose services in a semiautomatic to fully-automatic manner we need infrastructure
to publish services, document services and query repositories for matching services. We presented our approach for
Web service composition. Our composition engine can find
a graph of atomic services that can be composed to form the
desired service as opposed to simple sequential composition
in our previous work [3]. Given semantic description of
Web services, our solution produces accurate and quick results. We are able to apply many optimization techniques to
our system so that it works efficiently even on large repositories. The use of Constraint Logic Programming (CLP)
helped greatly in obtaining an efficient implementation of
this system. We used a number of built-in features such as
indexing, set operations, and constraints and hence did not
have to spend time coding these ourselves. These CLP(FD)
built-ins facilitated the fast execution of queries.

References
[1] WS Challenge 2007 http://ws-challenge.
org.
[2] S. Kona, A. Bansal, G. Gupta, and T. Hite. Efficient
Web Service Discovery and Composition using Constraint Logic Programming. In ALPSWS Workshop at
FLoC 2006.
[3] S. Kona, A. Bansal, G. Gupta, and T. Hite. Web
Service Discovery and Composition using USDL. In
CEC/EEE, June 2006.
[4] A. Bansal, S. Kona, L. Simon, A. Mallya, G. Gupta,
and T. Hite. A Universal Service-Semantics Description Language. In European Conference On Web Services, pp. 214-225, 2005.
[5] Web Services Description Language. http://www.
w3.org/TR/wsdl.
[6] S. McIlraith, T.C. Son, H. Zeng. Semantic Web Services. In IEEE Intelligent Systems Vol. 16, Issue 2, pp.
46-53, March 2001.
[7] OWL-S www.daml.org/services/owl-s/1.
0/owl-s.html.
[8] WSML: Web Service Modeling Language. www.
wsmo.org/wsml/.
[9] WSDL-S: Web Service Semantics. http://www.
w3.org/Submission/WSDL-S.
[10] L. Sterling and S. Shapiro. The Art of Prolog. MIT
Press, 1994.
[11] K. Marriott and P. J. Stuckey. Programming with Constraints: An Introduction. MIT Press, 1998.

ShareEnabler: Policy-Driven Access
Management for Ad-Hoc Collaborative Sharing
Jing Jin1 , Gail-Joon Ahn1 , and Mukesh Singhal2
1

Department of Software and Information Systems
University of North Carolina at Charlotte
Charlotte, NC 28223, USA
{jjin, gahn}@uncc.edu
2
Department of Computer Science
University of Kentucky
Lexington, KY 40506, USA
singhal@cs.uky.edu

Abstract. The rise of the Internet has introduced dramatic changes in
managing and sharing digital resources among widely dispersed groups.
This paper presents a policy-driven access management approach for
ad-hoc collaboration to enable secure information sharing in heterogeneous network environments. In particular, we attempt to incorporate
the features of distributed role-based access control, delegation and dissemination control to meet the fundamental access control requirements
associated with resource originators. These features are realized in a set
of XACML-based Role-based Originator Authorization policies (ROA).
We propose a security architecture, called ShareEnabler, to achieve eﬀective authorization and enforcement mechanisms in the context of Peerto-Peer (P2P) networking oriented ﬁle sharing. We brieﬂy discuss our
proof-of-concept prototype implementation based on an existing P2P ﬁle
sharing toolkit developed by Lawrence Berkeley National Laboratory.

1

Introduction

The rise of Internet has led collaborators to face dramatic changes in managing and sharing their resources. Subsequently, it has extremely inﬂuenced to the
traditional information sharing fashion. Firstly, collaborative information sharing has increasingly turned outward to connect distributed participants across
enterprises and research institutes. By removing the barriers of the time and geographical distance from research collaborations, people are able to work together
regardless of their locations. And new terms such as virtual organization, virtual
laboratory, and collaboratorium have been introduced consequently. Also, the
heterogeneous network environments demand more open and ﬂexible infrastructures as well as system architectures to enable collaborative sharing. In addition,
there is a need for ad-hoc collaborative sharing systems to support autonomous
and spontaneous collaboration among diverse participants, minimizing administrative complexity.
Traditionally, collaborative information sharing heavily relies on client-server
based approach or email systems. By recognizing the inherent deﬁciencies such
T. Grust et al. (Eds.): EDBT 2006 Workshops, LNCS 4254, pp. 724–740, 2006.
c Springer-Verlag Berlin Heidelberg 2006


ShareEnabler: Policy-Driven Access Management

725

as a central point of failure and scalability issue, several alternatives have been
proposed to support collaborative sharing of resources, including Grid computing
[1] and Peer-to-Peer (P2P) networking [2]. While Grid suits for highly structured
collaborations with centralized infrastructures, P2P works well on heterogeneous
network environments and promises to be more ﬂexible and reliable for smaller adhoc collaborative interactions [3]. Especially, with the decentralized structure and
load balancing feature, P2P based ﬁle sharing system oﬀers better scalability and
robustness. As demonstrated in the newly proposed SciShare system [3,4] from
Lawrence Berkeley National Laboratory (LBNL), P2P ﬁle sharing has great potentials to support collaborative sharing. However, most P2P technologies mainly
focus on sharing services such as availability and scalability. Ad-hoc collaborative
sharing requires the resource sharing be highly controlled and the conﬁdentiality
and integrity be properly protected during sharing sessions. On one hand, systematic techniques such as secure group communication protocols are needed to
protect the communication traﬃc for each sharing session. On the other hand, access control mechanisms should be in place to allow resource owners, also called
originators, to deﬁne and enforce access control policies for participating peers.
Although some researchers have investigated secure group communication protocols and technologies [5,6,7], there are few attempts in exploring practical access
control models and mechanisms for such environments. Our immediate motivation of this paper is to provide eﬀective and practical policy-driven access management mechanisms for fulﬁlling access control requirements associated with adhoc collaborative sharing environments. Our approach emphasizes the originator
as the principal source of policy to determine the collaboration control space and
delegate ﬁne-grained access capabilities to collaborators. The policy framework
incorporates the features of distributed role-based access control, delegation and
dissemination control. The policy enforcement system is then proposed to guarantee the policies being propagated and enforced appropriately.
The rest of the paper is organized as follows. In Section 2, we give an overview
of motivation and background technologies. Section 3 introduces our access management framework, including the originator-initiated approach and role-based
management framework followed by the underlying policy speciﬁcation framework. We then realize the proposed policy framework in a concrete collaborative
sharing example and show the detailed policy evaluation procedures. Our proposed ShareEnabler system and implementation issues are also discussed in this
section. Section 4 concludes the paper.

2

Problem Statements and Background Technologies

To better understand the ad-hoc collaborative sharing environments, we proceed with a typical example of collaborative sharing [8], from which we identify
the key concepts involved in the environment and derive generic access control
requirements for our approach:
NIH sponsored large-scale biomedical science collaborations involve a consortium of universities and research groups participating in several testbed projects

726

J. Jin, G.-J. Ahn, and M. Singhal

related to the brain imaging of human neurological disease and associated animal
models. Researchers from any of the groups may contribute their research results
and data to be shared by other members in the collaboration group. Suppose Regional Medical Center (RMC), jointly initiated with Bioinformatics Department
at University of XYZ, administers a local magnetic resonance imaging (MRI)
data repository and would like to share the data with other collaborators for testing new hypotheses on human neurological diseases.
RMC needs to protect and control the data access and dissemination during
the collaborative sharing. Due to the large group of collaborators, RMC would
like to have a ﬂexible and easy way to deﬁne the sharing collaborators as well as
the access privileges for them. For faster and more convenient sharing, instead of
contacting all the researchers in collaborating labs, RMC may need to notify the
director or the coordinator of each collaborator’s lab. The data are then shared
with all other lab members through them. Furthermore, to protect the patentable
data, any dissemination of the data should be under RMC’s agreement. Meanwhile, Bioinformatics Department at University of XYZ as a co-owner of the
data also would like to have the control on the data.
From the example above, we ﬁrst identify several key concepts in an ad-hoc
collaborative sharing environment that are used through the rest of this paper:
– Originator: In collaborative sharing environments, we refer the resource
owner or the initial information provider as an originator. An originator
plays a critical role in providing the resource to be shared and in controlling
how the resource is shared among collaboration participants. The originator
could be an individual principal or an organizational entity. For a particular
resource, there may be one or multiple joint originators. In our scenario,
RMC and XYZ University, both as organizational entities, act as joint originators for the MRI data repository.
– Collaborative sharing space: In general, collaborative sharing space
refers to the control domain of the collaborative sharing. An originator needs
to deﬁne her collaborative sharing space by including a collection of, mostly
distributed, people or organizations and granting ﬁne-grained access privileges to them. In our example, the whole NIH sponsored biomedical science
consortium or a subgroup of consortium could be considered as the collaboration space. This should be determined at the originator’s discretion.
– Collaborator: Each entity that is included inside the collaborative sharing
space is referred as a collaborator. These collaborators are the actual recipients or consumers of the shared resource(s). Similar to the originator, a
collaborator could be an individual principal such as independent researcher
or an organizational research lab.
– Disseminator: We deﬁne two types of disseminators, namely, the root disseminator and the designated disseminator. The root disseminator refers to
the originator since the originator triggers the initial sharing process with
other ad-hoc collaborators. Designated disseminator, on the other hand, refers
to a group of collaborators, with the consent of originator, to further distribute the resource. This can be achieved through the notion of delegation.

ShareEnabler: Policy-Driven Access Management

727

Indeed, designated disseminator is a subgroup of ad-hoc collaborator. In our
case, the directors/coordinators of collaboration laboratories are the designated disseminators.
2.1

Access Control Requirements

From the above-mentioned example, we derive several generic access management requirements for ad-hoc collaborative sharing:
– Flexible and manageable access control: Collaborative sharing may
involve a large amount of distributed collaborators across domains. The diversity and unpredictability of the involved participants determine that the
authorization cannot be established on per-individual basis like the way ACL
does. The access control system needs to provide appropriate abstraction of
collaborators and privileges to achieve the ﬂexibility and reduce the complexity of security administration.
– Flexible delegation/revocation: The nature of distributed resource sharing requires delegation in place to allow the access privileges as well as administrative responsibilities of an originator to be distributed among diﬀerent
collaboration parties. Especially, it should also allow an originator to delegate not only all of the privileges, but also partial privileges. In addition,
revocation as the counterpart of delegation should be supported as well.
– Eﬀective originator-controlled dissemination: As the shared information leaves the originator’s domain, it is hard for the originator to have
control on such information. With the originator-initiated control, originators should be able to control and track down the re-dissemination of
their resources to make sure the dissemination happens within the collaborative sharing space, and only the legitimate collaborators could share the
resources.
2.2

Background Technologies

Role-based Access Control (RBAC): RBAC is a proven technology for managing and enforcing security in large-scale and enterprise-wide systems [9,10].
The essential idea of RBAC is that permissions are associated with roles, and
users acquire permissions by being members of appropriate roles. With the
abstraction between users and permissions, RBAC could tremendously reduce
the complexities of security management for system administrators. Meanwhile,
many role-based delegation models [11,12,13] have been proposed as a complementary to RBAC in leveraging an eﬀective way of propagating authorities
as well as responsibilities among various distributed entities. Our framework is
built on existing role-based delegation models by applying decentralized user
assignments.
ORCON, UCON and DCON: Originator control (ORCON) is a special access control policy deﬁned by a resource originator to control the dissemination
of restricted resources [14,15]. ORCON policy requires that resource recipients
obtain an originator’s permission to re-disseminate protected resources to users

728

J. Jin, G.-J. Ahn, and M. Singhal

who are not originally designated as authorized recipients by the originator. Traditional ORCON solutions used a non-discretionary access control list, which
limits the ability to enforce ORCON policies in a closed centralized control environment [16]. The concept of Usage Control (UCON) is introduced in [16,17] for
controlling access and usage of digital information objects. The re-dissemination
control in ORCON is also one of the key concerns in UCON. By introducing
license and ticket [16], UCON has the potential to support and enforce ORCON
policies in more versatile and ﬂexible ways for distributed environments. Most
recently, the notion of dissemination control (DCON) has been proposed in [18].
DCON involves a much richer and broader concept than ORCON and UCON
concerning with controlling information during the dissemination activities.
SciShare File Sharing Infrastructure: Traditional P2P sharing applications,
such as Gnutella [19], allow end users to search and download information from
other peers, and make their own information available to other peers. The search
component often broadcasts a query to all known peers, while sending response
and downloading information are unicast communications. LBNL’s framework,
called Scishare [4], is a security enhanced version of P2P ﬁle sharing system.
SciShare leverages X.509 public key certiﬁcate as the central security component. The certiﬁcate can be either self-signed or signed by a trusted organizational certiﬁcation authority (CA). To facilitate new peers joining the community
quickly, the system allows the new peers (called pseudo user) to create self-signed
X.509 certiﬁcates. However, the pseudo user cannot gain higher level of trust or
privileges in the system. The instantiation of secure and reliable multicast communication is provided by Secure Group Layer (SGL) [5], while TLS [20] is used
to achieve conﬁdentiality and integrity in unicast communication when peers
play traditional role of client in some cases and the traditional role of a server
in others. SciShare also supports access control that is primitive and limited to
group-based discretionary access control approach.

3

Policy-Driven Access Management Framework

In this section, we describe a policy-driven access management framework to
provide a means of comprehensive access management model beyond SciShare.
The framework emphasizes originator-initiated role-based access control and delegation. Originators dynamically create and include roles in their collaborative
sharing space while delegating ﬁne-grained access and dissemination capabilities to the roles. Distributed role-assignment is achieved through Delegation of
Delegation Authority. These features are expressed in a set of Role-based Originator Authorization policies (ROA). ROA policies serve as the foundation of
our framework and are further evaluated and enforced in our proposed security
architecture for P2P based ﬁle sharing.
3.1

Supporting Originator Control and Role-Based Approach

An originator, as the resource owner, is responsible for initiating the controls to
secure her respective resources over sharing and dissemination activities among

ShareEnabler: Policy-Driven Access Management

729

other peer collaborators. To accommodate the originator-initiated control approach, it is essential for an originator to deﬁne her collaborative sharing space in
a set of access management policies and delegate ﬁne-grained privileges through
these policies. The speciﬁed policies should be propagated and enforced properly
by the underlying security system during the resource re-dissemination.
RBAC provides an eﬀective way to abstract privileges using roles. Instead of
including every individual ad-hoc collaborator, the originator could simply deﬁne
the collaborative sharing space in a collection of speciﬁc roles, such as “engineer”
and “investigator”. And each peer collaborator is dynamically included in the
sharing space to gain access privileges by claiming their role. Therefore, bringing
“role” in our framework becomes a natural choice to achieve the manageability
in the ad-hoc collaboration environments. In addition, we introduce role-based
delegation as another layer of privilege and authority decentralization to accommodate the needs of distributed role assignment and ﬁne-grained privilege propagation in collaborative sharing environments. In particular, our framework incorporates the following types of delegation relationships for ad-hoc collaboration:
– Delegation of access capabilities: The permission-role assignment in traditional RBAC usually deals with the abstraction of privileges in a closed
organizational domain. In a distributed collaborative sharing environment,
an originator delegates ﬁne-grained access capabilities to certain roles in the
collaboration space so that the privileges are propagated and distributed
across various participating entities through these roles.
– Constrained dissemination delegation: To achieve better resource availability and continuous resource dissemination, besides the normal access
privileges, the resource dissemination privilege can be delegated by an originator to a certain set of roles, so that the collaborators who are assigned to
these roles are allowed to further disseminate the pre-obtained resources on
the originator’s behalf. These collaborators, in another words, are the designated disseminators. As “constrained” delegation, the scope of delegation
should be within the originators and the designated disseminators.
– Delegation of delegation authority: This is a special form of administrative delegation that enables an originator to partially delegate the role
assignment privilege to trusted third parties. In our example, the originator deﬁnes a set of roles in her collaborative sharing space and delegates the
role assignment authority to the directors/coordinators of each collaboration
group so that these directors/coordinators may assign roles to their members
on the originator’s behalf.
3.2

Designing Policies

In our policy framework, an originator deﬁnes her access management policies in
a set of authorization policies using XACML (eXtensible Access Control Markup
Language) [21]. We introduce two major types of policies, Root Meta PolicySet
(RMPS) and Role-based Originator Authorization PolicySet (ROA).
Root Meta Policy Set (RMPS) is the starting point of the originator’s authorization policies for the shared resources. Since the shared resources may

730

J. Jin, G.-J. Ahn, and M. Singhal

have single or multiple distributed originators, their authorization policies may
be maintained in multiple administrative domains. We need a policy to identify
these originators and locate their policies so that the underlying enforcement system could retrieve and enforce these distributed policies. RMPS is designed for
this purpose. In RMPS policy schema, the Target element speciﬁes the resource
to which ROA authorization policies are applied. The resource is represented as
a URI that conforms to RFC2396 standard format [22]. The PolicySet contains
one ownership Policy and one or more PolicySetIdReference elements to specify
ROA policy locations in the format of LDAP URLs. In the ownership Policy,
originators are identiﬁed as Subject attributes in their X.500 DNs. The ownership is speciﬁed as “own” in Action element. Figure 1(a) illustrates the schema
of RMPS.
ROA policy sets are the real role-based authorization policies where an originator deﬁnes her collaborative sharing space and delegates ﬁne-grained capabilities.
We extend OASIS RBAC proﬁle [23] to support the delegation and distributed
role assignment in our framework. ROA contains four major sub-components:
role speciﬁcation policy (RPS), capability speciﬁcation/role-capability assignment policy (CPS), user-role assignment policy (RAPS), and delegation of delegation authority policy (DoDPS).
– Role PolicySet (RPS) is a role speciﬁcation PolicySet. The originator deﬁnes
the collaborative sharing space in a set of RPSs, and associates each role RPS
with a Capability PolicySet (CPS) that actually contains capabilities of the
given role. The role is speciﬁed as a Subject attribute, the corresponding CPS
is referenced through PolicySetReference. Figure 1(b) shows the schema of
RPS.
– Capability PolicySet (CPS) speciﬁes the actual capabilities assigned to the
given role. CPS contains Policy and Rule elements that describe the delegated capabilities as the resources and actions. By granting the “disseminate” action to a speciﬁc role, the originator delegates her dissemination
privilege to the role. The collaborator who is assigned to that role then becomes a designated disseminator to re-disseminate the resources. The CPS
may also contain references to the CPSs associated with other roles that are
junior to the given role, thereby achieving the role hierarchies through the
capability aggregation. Figure 1(c) shows the schema of CPS.
– Role Assignment PolicySet (RAPS) is speciﬁed by an originator or a delegated third party authority to deﬁne which roles are assigned to which
collaborators. In RAPS, the principals are speciﬁed in their X.500 DNs as
Subject attributes. The assigned role is speciﬁed as Resource attribute. And
the term“enable” is used as Action attribute to indicate the assignment relationship. Figure 1(d) shows the schema of RAPS.
– Delegations of Delegation Authority PolicySet (DoDPS) reﬂects the type of
“delegation of delegation authority” with originators specifying which role assignments are delegated to which speciﬁc trusted authorities. The construction of DoDPS is similar to RAPS, except that the Subjects are the trusted

ShareEnabler: Policy-Driven Access Management

Resource is represented in URI
conform to [RFC2396]

Originator’s identity is
represented in X500 DN
Root Meta PolicySet
for particular resource
Put “own” to indicate
the ownership

Ownership policy

Default effect is “Permit”

Originator’s ROA policy LDAP URLs

(a) RMPS Policy Schema

Define the specific “xx” role
Role PolicySet
Refer to the PolicySetId of associated
Capability PolicySet or CPS

(b) RPS Policy Schema

Each specific capability is
encoded in one Rule

Capability PolicySet for
the specific role

Refer to CPS IDs of Junior roles to achieve
implicit role inheritance

(c) CPS Policy Schema

Collaborator’s identity in
X.500 DN

Specific role name

Role assignment
PolicySet

Put “enable” to indicate
the assignment

Each role assignment is
encoded in one policy

Default effect is “Permit”

(d) RAPS Policy Schema

Delegatee identity
in X.500 DN

Specific role name

Put “delegated_assign” to
indicate the delegation
relationship
DoD PolicySet
Default effect is “Permit”

Delegatee’s Role assignment PolicySet locations

(e) DoDPS Policy Schema

Fig. 1. Policy Set Schemas

731

732

J. Jin, G.-J. Ahn, and M. Singhal

delegatees and the delegation relationship is indicated as “delegated assign”
in Action. Figure 1(e) shows the schema of DoDPS.
3.3

Policy Framework Realization and Policy Evaluation

In this section, we extend the earlier discussed collaborative sharing scenario
into a concrete example and proceed implementing a set of access management
policies to realize our proposed policy framework. We then show how the authorization system evaluate these policies and make decisions.
Inside the NIH biomedical science research community, a team of biologists
from LIISP research lab, with John as the team leader and Dave as one of the
team members, is conducting research tasks related to animal modeling comparisons and analysis. John’s team needs to collaborate with RMC and use RMC’s
data to verify a new hypothesis drawn from their research.
As discussed earlier, both RMC and XYZ University are joint originators for
the MRI data resource. For simplicity, we focus on how RMC develops the ROA
policies and omit the control from the XYZ University. End each individual
member in LIISP lab, John and Dave, is considered as an ad-hoc collaborator
that needs to be authorized individually in RMC ’s collaborative sharing space.
To authorize accesses to the members in LIISP lab, RMC deﬁnes two roles
such as Coordinator role and Investigator role, where the Coordinator role is
senior to the Investigator role. RMC delegates the capabilities of “query” and
“acquire” to the Investigator role, and further delegates the capability of “redisseminate” to the Coordinator role. RMC then assigns the team leader John
to the Coordinator role and delegates John to perform the user-Investigator
role assignment through the delegation of delegation authority, so that John is
able to assign his other team members (i.e. Dave) to the Investigator role and
re-disseminate the resource to them as a designated disseminator.
Figure 2(a) shows the overall structure of our policy framework and the relationships among the individual policy components. In particular, RMPS speciﬁes
the resource with the originator(s) who “own” the resource, and locates the originators’ ROA policy sets. In the example scenario, RMC and XYZ University,
both represented as Subject attributes in their X.509 DNs, are joint originators
of the “MRI data” resource. Since we focus on the control of RMC, only the URL
location of RMC ’s ROA policy sets is referenced through the PolicySetIdReference element. In RMC’s ROA policies, there is a set of RPSs and CPSs, a RAPS
and a DoDPS. As shown in Figure 2(b), RPSs deﬁne two roles in RMC’s collaborative sharing space, namely the Coordinator role and the Investigator role.
CPSs specify the corresponding capabilities associated with these two roles. The
reference link between each pair of RPS and CPS reﬂects the permission-role
assignment relation where the originator delegates the ﬁne-grained access and
dissemination capabilities to the role. By referencing to the CPS of the Investigator role, the Coordinator role inherits all the capabilities that are assigned
to the Investigator role. In this context, the role hierarchy is achieved indirectly
through capabilities aggregation. As shown in Figure 2(c), the RAPS speciﬁes

ShareEnabler: Policy-Driven Access Management

733

RMC’s ROA policy sets

Root Meta <PolicySet>
<Target>
resource-id= MRI data

Role <PolicySet> -- R
<PolicySetIDReference>

<Policy>
<Target>
<Subjects>
subject-id= CN=RMC...
subject-id=CN=XYZ...
<Actions> own

Capability <PolicySet> -- P
<PolicySetIDReference>

PA
RH via
capability
aggregation

<PolicySetIdReference>
locates

Role Assignment <PolicySet> -- UAC

locates

XYZ’s ROA policy sets

DoD <PolicySet> -- DoD

RPS

locates

<PolicySetIDReference>

CPS

locates

RAPS

DoD Delegatee’s role assignment policy sets

DoDPS

DoD Role Assignment <PolicySet> -- UACDoD

(a) Overall Policy Framework
Role <PolicySet> -- Coordinator

Role <PolicySet> -- Investigator

<Target>
<Subjects>
role = Coordinator

<Target>
<Subjects>
role = Investigator

<PolicySetIdReference>

<PolicySetIdReference>

PA
Capability <PolicySet> -- Coordinator

RH

<Policy>
<Target>
<Actions>
redisseminate

PA
Capability <PolicySet> -- Investigator

<Policy>
<Target>
<Actions>
query
acquire
<Rule> Permit

<Rule> Permit
<PolicySetIdReference>

(b) RPS-CPS Details
RAPS

DoDPS

Role Assignment <PolicySet>--UAC
RA <Policy> -- Coordinator role
<Target>
<Subjects> subject-id = CN=John…
<Resources> role = Coordinator
<Actions> action = enable

<Rule> Permit

DoD <PolicySet>--DoD
DoD <Policy> -- Investigator role

<Target>
<Subjects> subject-id = CN=John…
<Resources> role = Investigator
<Actions> action = delegated assign

<Rule> Permit
<PolicySetIDReference>

DoD RAPS
Role Assignment <PolicySet> --UACDoD
DoD RA <Policy> -- Investigator role
<Target>
<Subjects> subject-id = CN=Dave…
<Resources> role = Investigator
<Actions> action = enable
<Rule> Permit

locates

(c) DoDPS-RAPS Details
Fig. 2. Policy Framework Realization

the user-role assignment relation that RMC assigns John to the Coordinator
role. By being assigned to the role, John is included in RMC’s collaborative
sharing space, and thus obtains the delegated capabilities. DoDPS realizes the
delegation of delegation authority where RMC delegates the user-Investigator
role assignment to John. And John’s RAPS policy is ﬁnally referenced in the
DoDPS where John assigns his team member Dave to the Investigator role.
As our policy framework conforms to the XACML standard, the policy evaluation and authorization decision making can be done as speciﬁed in [21]. The
typical setup is that the Policy Enforcement Point (PEP) forms an access request
based on the requester’s attributes (X.509 identities, roles, etc.), the resource in
question, and the action towards the resource. The request is sent to a Policy

734

J. Jin, G.-J. Ahn, and M. Singhal
PDP
PEP

Context Handler

PDP Engine

Policy Repository

user access request + RMPS
retrieve requester's role attributes
role attributes
DoD request
retrieve DoDPSs
DoDPSs
DoD decision
role assignment request

DoD evaluation
(DoDPSs)

retrieve RAPSs
RAPSs
role assignment decision
role access request

Role assignment evaluation
(RAPSs, DoD RAPSs)

retrieve RPSs and CPSs
RPSs and CPSs
role access decision
final decision

formuate final decision

Role access evaluation
(RMPS, RPSs, CPSs)

enforce decision

Fig. 3. Policy Retrieval and Evaluation

Decision Point (PDP) for policy retrieval and policy evaluation. Basically, the
PDP ﬁrst ﬁnds the top-level policy elements that the Target elements match
the attributes speciﬁed in the access requests, and then evaluates the boolean
expressions included in each Rule elements and ﬁnally combines the results using
the speciﬁed policy combination algorithms. A response with an access Decision
element of value “Permit ”, “Deny”, “Indeterminate” or “NotApplicable” will be
made and returned to the PEP for further authorization enforcement. In our
system, we introduce the Context Handler as a subcomponent of the PDP to
conduct a series of query-generation and decision-making process for a single
access query sent by the PEP. In this section, we focus on how the PEP, Context Handler and PDP interact with each other and how the PDP evaluates an
access request against the originator’s ROA policies. The detailed system design
and implementation will be discussed shortly in next section.
Figure 3 shows the detailed sequence diagram of the policy retrieval and policy evaluation. The PEP formulates an access request with the requester’s X.509
identity and the action towards the requested resource. For instance, the PEP
may generate an access request for the PDP to evaluate whether a requester
Dave (CN=Dave...) is allowed to “acquire” the “MRI data” resource. Along
with the associated RMPS for the “MRI data”, the request is sent to the PDP.
The Context Handler parses the RMPS and locates RMC ’s policy directory.
The role attributes that are assigned to the user’s identity are retrieved from
the originator’s policy repository. In our case, the Investigator role is assigned
to Dave by John. Since the attribute is assigned by an entity other than the
originator, the Context Handler will prompt to formulate a DoD request for the

ShareEnabler: Policy-Driven Access Management

735

PDP to evaluate whether the role attribute issuer (CN=John...) is a legitimate
delegated authority to conduct the user-Investigator role assignment. The PDP
Engine conducts the DoD Evaluation based on DoDPS and conﬁrms the delegation of delegation authority relationship. The Context Handler then formulates
the role assignment request for the PDP Engine to check whether the requester
(CN=Dave...) is “enabled ” with the Investigator role attribute that is retrieved
earlier. The PDP Engine conducts the Role Assignment Evaluation against
the retrieved RAPSs deﬁned by the originator and/or the DoD RAPSs deﬁned
by the DoD delegatee (in our case, only the DoD RAPS is evaluated). Finally,
the Context Handler formulates the role access request for the PDP Engine to
check whether the assigned Investigator role is able to perform the “acquire” action towards the “MRI data” resource as speciﬁed in the PEP’s access request.
The PDP Engine conducts the Role Access Evaluation against the RMPS,
RPSs and CPSs. Based on the authorization decisions of these three evaluations,
the Context Handler generates the ﬁnal user access decision and sends back to
the PEP for further decision enforcement process.
3.4

ShareEnabler System Architecture and Discussions

In this section, we give an overview of our system architecture, called ShareEnabler. ShareEnabler casts our proposed framework as detailed authorization services and mechanisms which are bound to speciﬁc communication infrastructure
from LBNL’s SciShare toolkit [4].
In our collaborative sharing system, each participant is represented by a ShareEnabler (SE) agent that executes sharing services on the collaborator’s behalf.
Similar to most of existing P2P ﬁle sharing systems, the resource discovery
involves broadcasting a query to all known peers. As shown in Figure 4, ShareEnabler Agent 1 sends a broadcasting query message to all known peers in the
collaborative sharing group. Upon receiving the query message, SE Agents 2 5 look up their own posted contents. SE Agent 2 ﬁnds the matched content(s),
evaluates the originator’s ROA policies and sends a unicast query response with
the metadata of the authorized content(s) to SE Agent 1, while SE Agents 3 5 are not necessary to respond to the requester. We call this process as metadata sharing. SE Agent 1 then can send out the download request, while the SE
Agent 2 will further check with the originator’s ROA policies and initiate the
data transferring process if the requester is authorized to download to resource.
Figure 4 also shows the detailed components inside the ShareEnabler Agent
and their interactions in the process of metadata sharing between the SE Agent
1 (as the requester) and the SE Agent 2 (as the responder). Each ShareEnabler
agent is composed of ﬁve components: Graphical User Interface (GUI), Executive Services, Access Management/Enforcement, SGL/IG and TLS/TCP. GUI is
the interface through which the user operates and executes the sharing services.
Executive Services are the real services required by collaborative sharing behaviors, which include Search, Download and Share Services. All these services are
based on the underlying Data Management Service, which provides data storage and cache functionalities. The Data Management Service also serves as the

736

J. Jin, G.-J. Ahn, and M. Singhal

Fig. 4. ShareEnabler System Architecture

background database in the system. The Access management/enforcement is the
central component for the core access and dissemination control. The PEP is responsible for the request processing and access decision enforcement. The PDP,
which consists of the Context Handler and the PDP Engine, is designed for the
policy retrieval and authorization decision making. Secure Group Layer (SGL)
and the underlying InterGroup protocol provide the secure group communication services. Similarly, Transport Layer Security (TLS) and the underlying TCP
protocol provide the secure communication between two ShareEnabler agents,
which in the category of unicast communication. The functionalities for both
SGL/IG and TLS/TCP are adopted from SciShare [4].
In the context of metadata sharing, on the requester agent side (ShareEnabler
Agent 1), a user interacts with the GUI to specify the keywords and search
criteria (step 1). GUI invokes the Search Service to formulate the query message
and broadcast to all known peers in the collaborative sharing group through the
SGL/IG protocol (step 2 - 4). Upon receiving responses from other peers, the
TLS/TCP module notices the Search Service with the response messages (step 5
- 6), and these responses are parsed and then shown in the GUI (step 7), through
which the user may further interact to download the data resource. The search
results are ﬁnally cached through the Data Management Service (step 8).
On the responder agent side (ShareEnabler Agent 2), the SGL/IG module
notices the Sharing Service (step 1’ - 2’) upon receiving the request. The Sharing
Service then invokes the Data Management Service to ﬁnd matched resources
against the query (step 3’). When a list of matched resources is returned back to
the Sharing Service, the Access Management/Enforcement component is invoked
for access checking (step 4’ - 6’). The PEP enforces the decision by removing

ShareEnabler: Policy-Driven Access Management

737

unauthorized resources from the list and returns the updated list back to the
Sharing Service (step 7’). Finally, the Sharing Service formulates the response
message with the metadata of a list of matched and authorized resources, and
sends back to the requester through the TLS/TCP module (step 8’ - 9’). The
metadata sharing result is shown in the GUI and cached in the Data Management
Service (step 10’ - 11’).
As also shown in Figure 4, ROA policies are deployed separately from the
major ShareEnabler application and its enforcement components. These ROA
policies will be retrieved and enforced at run time whenever the ShareEnabler
agent needs to respond to other peer’s requests. In doing so, an originator can
easily maintain and change the policies without requiring changes to sharing
service systems. We decide to apply X.509 attribute certiﬁcates to encapsulate
access management policies. X.509 attribute certiﬁcate (AC) is a basic data
structure in Privilege Management Infrastructure (PMI) [24] to bind a set of
attributes to its holder. With its portability and ﬂexibility, AC is considered
as an ideal container of subject attributes as well as authorization policies in
ShareEnabler. We also developed a Policy Administration Facility application
to provide the utility modules for originator to create and maintain ROA policies.
Especially, originators use the Policy Engine to create their ROA policy sets.
Attribute Certiﬁcate Engine is then invoked to generate the ROA policy ACs
and store them in distributed LDAP policy repositories.
The goal of access and dissemination control of ShareEnabler is to guarantee the resource is shared within the collaborative sharing space deﬁned by
ROA policies. Our system applies a distributed policy propagation and enforcement scheme with decentralized, self-enforcing, and self-monitoring features at
each ShareEnabler agent level. Especially, each disseminator ShareEnabler agent
should ensure that ROA policies are enforced locally by the Access Management/Enforcement component, and these ROA policies are propagated to other
ShareEnabler agents while those agents may further act as disseminators to respond to other peers’ requests. Since the Root Meta Policy Set (RMPS) plays
an important role for the ShareEnabler Agent to locate and enforce originator’s
policies. It is essential to make sure the RMPS is propagated along with the data
dissemination and the conﬁdentiality and integrity are properly protected . In
achieving these goals, we design a new data structure that strongly encapsulates
the data resource together with the associated RMPS policy. As the originator
initiates the sharing process, instead of sending out the original data resource,
originator’s ShareEnabler agent disseminates the encapsulated data structure
to other agents, which can only be decrypted at runtime by the ShareEnabler
Agent. By doing this, we leave the ShareEnabler Agent with full enforcement
power and make it extensible for more advanced dissemination tracking mechanisms.
In our prototype, we use JDK1.4 core packages as well as other necessary
libraries to develop the components speciﬁed in the system architecture. Especially, we adopt SciShare’s Reliable and Secure Group Communication (RSGC)
package for the implementation of SGL/TLS communication protocol as well

738

J. Jin, G.-J. Ahn, and M. Singhal

(a) New Search and Search Results

(b) Post New Resource

(c) Policy Creation

(d) Attribute Certiﬁcate Generation

Fig. 5. ShareEnabler User Interfaces

as the basic authentication mechanisms. We extend Sun’s XACML implementation to accommodate the functionalities in PDP and PEP. IAIK’s java crypto
library is used to implement the major components of cryptography and attribute certiﬁcate. And IPlanet Directory Server serves as the back-end LDAP
policy repository. The beta version of ShareEnabler system implementation has
been completed for further testing and evaluation. Figure 5(a) shows a user
interface of an SE Agent for searching for speciﬁc ﬁle resource and displaying search results based on the responses from other peers. Figure 5(b) shows
an originator posts new resource to be shared with other collaborator peers.
The Figure 5(c) shows the user interface of the policy creation that allows an
originator to create new roles in her collaborative sharing space and delegate
ﬁne-grained capabilities to the roles. The ROA policies will then be generated
automatically based on the originator’s input. Finally, Figure 5(d) shows the
interface of policy attribute certiﬁcate generation with the originator specifying
the validity period of the attribute certiﬁcate and using her private key (encapsulated in an X.509 Personal Information Exchange Certiﬁcate [25]) to sign the
attribute certiﬁcate.

ShareEnabler: Policy-Driven Access Management

4

739

Conclusion

In this paper, we have presented a policy-driven access control framework for
ad-hoc collaborative sharing. Especially, we articulated distinctive access control
requirements in ad-hoc collaborative sharing and proposed a family of XACMLbased policy schemas that are comprehensive and ﬂexible enough to meet the
identiﬁed requirements. In addition, we brieﬂy described the enforcement mechanisms as well as a proof-of-concept prototype of P2P based ﬁle sharing system,
called ShareEnabler. An important contribution of this work includes special features of originator control, delegation and dissemination control. Our approach
allows originators to authorize distributed collaborators and control over the resources being shared. The delegation of delegation authority was introduced to
systematically achieve user-role assignments in distributed environments.
Our future works are geared towards several directions. We would investigate
and apply more advanced system-level dissemination control enforcement mechanisms. In collaborative sharing environment, the resources are stored and updated in distributed places. This causes another control issue of how to maintain
the originator-initiated control of data usage and modiﬁcation after the dissemination, which in turn, relates to the enforcement mechanisms. Furthermore, the
inconsistency of data representation and instances needs to be dealt with while
the resources are shared and updated. Developing an integrated infrastructure
would be another research direction as well.

Acknowledgments
The work was partially supported by the grants from National Science Foundation (NSF-IIS-0242393). The work of Gail-J Ahn and Jing Jin was also supported
by the grants from Department of Energy Early Career Principal Investigator
Award (DE-FG02-03ER25565).

References
1. Baker, M., Buyya, R., Laforenza, D.: The Grid: International eﬀorts in global
computing. International Journal of Software Practice and Experience. (2002)
2. Oram, A. (ed.): Peer-to-peer: Harnessing the power of disruptive technologies.
O’Reilly. (2001)
3. Berket, K., Agarwal, D.: Enabling secure ad-hoc collaboration. In: Proceedings of
the Workshop on Advanced Collaborative Environments. (2003)
4. Berket, K., Essiari, A., Muratas, A.: PKI-based security for peer-to-peer information sharing. In: Proceedings of the Fourth IEEE International Conference on
Peer-to-Peer Computing. (2004)
5. Agarwal, D., Chevassut, O., Thompson, M.R., Tsudik, G.: An integrated solution
for secure group communication in wide-area networks. In: Proceedings of the 6th
IEEE Symposium on Computers and Communications. (2001) 22–28
6. Kihlstrom, K.P., Moser, L.E., Melliar-Smith, P.M.: The securering protocols for
securing group communication. In: Proceedings of 31st IEEE HICSS. (1998) 317–
326

740

J. Jin, G.-J. Ahn, and M. Singhal

7. Reiter, M.K.: Secure group membership protocol. In: Proceedings of IEEE Symposium on Research in Security and Privacy. (1994)
8. NIH:
NIH data sharing workbook.
http://grants.nih.gov/grants/policy/
data sharing/data sharing workbook.pdf (2004)
9. Sandhu, R., Coyne, E.J., Feinstein, H.L., Youman, C.E.: Role based access control
models. IEEE Computer 29 (1996)
10. Ferraiolo, D., Sandhu, R., Gavrila, S., R. Kuhn, R.: Proposed NIST standard for
role-based access control. ACM Transactions on Information and System Security
(TISSEC) 4 (2001) 224–274
11. Zhang, L., Ahn, G.J., Chu, B.T.: A rule-based framework for role-based delegation
and revocation. ACM Transactions on Information and System Security (TISSEC)
6 (2003) 404–441
12. Ahn, G.J., Mohan, B.: Secure information sharing using role-based delegation.
Journal of Network and Computer Applications 2 (2005)
13. Barka, E., Sandhu, R.: Framework for role-based delegation models. In: Proceedings of the 16th Annual Computer Security Applications Conference (ACSAC),
IEEE Computer Society (2000) 168
14. Abrams, M.D., Heaney, J., King, O., LaPadula, L.J., Lazear, M., Ol, I.M.: Generalized framework for access control: Towards prototyping the orgcon policy. In:
Proceedings of the 14th National Computing Security Conference. (1991) 257–266
15. McCollum, C.J., Messing, J.R., Notargiacomo, L.: Beyond the pale of MAC and
DAC — deﬁning new forms of access control. In: Proceedings of IEEE Symposium
on Security and Privacy. (1990) 190–200
16. Park, J., Sandhu, R.: Originator control in usage control. In: Proceedings of
the 3rd International Workshop on Policies for Distributed Systems and Networks
(POLICY’02). (2002)
17. Park, J., Sandhu, R.: Towards usage control models: beyond traditional access
control. In: Proceedings of the 7th ACM Symposium on Access Control Models
and Technologies (SACMAT 2002). (2002) 57–64
18. Thomas, R., Sandhu, R.: Towards a multi-dimensional characterization of dissemination control. In: Proceedings of the 5th IEEE International Workshop on Policies
for Distributed Systems and Networks (POLICY04). (2004)
19. Gnutella. http://www.gnutella.com/
20. RFC2246: The TLS protocol version 1.0. http://www.ietf.org/frc/rfc2246.txt
(1999)
21. OASIS: XACML 2.0 core: extensible access control markup language (xacml)
version 2.0. http://docs.oasis-open.org/xacml/2.0/access control-xacml-2.0-corespec-os.pdf (2005)
22. RFC2396:
Uniform resource identiﬁers (URI): Generic syntax. http://rfc.
net/rfc2396.html (1998)
23. OASIS:
Core and hierarchical role based access control (rbac) proﬁle of
xacml v2.0. http://docs.oasis-open.org/xacml/2.0/access control-xacml-2.0-rbacproﬁle1-spec-os.pdf (2005)
24. ITU-T: The directory: Public-key and attribute certiﬁcate frameworks. ISO/IEC
9594-8:2001 (2001)
25. RSA: PKCS #12: Personal information exchange syntax standard. ftp://ftp.
rsasecurity.com/pub/pkcs/pkcs-12/pkcs-12v1.pdf (1999)

Model-Based Conformance Testing for Android
Yiming Jing1 , Gail-Joon Ahn1 , and Hongxin Hu2
1

Laboratory of Security Engineering for Future Computing (SEFCOM)
Arizona State University, Tempe, AZ85281, USA
{ymjing,gahn}@asu.edu
2
Delaware State University, Dover, DE19901, USA
hxhu@asu.edu

Abstract. With the surging computing power and network connectivity
of smartphones, more third-party applications and services are deployed
on these platforms and enable users to customize their mobile devices.
Due to the lack of rigorous security analysis, fast evolving smartphone
platforms, however, have suﬀered from a large number of system vulnerabilities and security ﬂaws. In this paper, we present a model-based conformance testing framework for mobile platforms, focused on Android
platform. Our framework systematically generates test cases from the
formal speciﬁcation of the mobile platform and performs conformance
testing with the generated test cases. We also demonstrate the feasibility and eﬀectiveness of our framework through case studies on Android
Inter-Component Communication module.

1

Introduction

According to a recent report from research ﬁrm [5], the worldwide smartphone
market ballooned 65.4% year over year in the second quarter of 2011, indicating
the total shipments of 100 million units. In addition, with the surging computing
power and network connectivity of smartphones, more third-party applications
and services are deployed on these platforms and enable users to customize
their devices. Many legitimate applications tend to manipulate users’ sensitive
information such as contact list, locale information, and other credentials [14].
To protect such sensitive attributes, it is necessary to ensure that smartphones
are properly conﬁgured and rigorously validated.
Fast evolving smartphone platforms, however, have raised considerable security concerns due to the lack of rigorous security analysis. At the same time, a
large number of system vulnerabilities and security ﬂaws on smartphone platforms have continuously been reported. For instance, an unprotected component
was discovered in the phone application of Android version 1.1 [15]. This ﬂaw
allowed any malicious application to make phone calls without the permission
it ought to have. Another recent work [10] indicated that the message passing
system in Android can be a target for denial-of-service and hijacking if used
incorrectly.
Software developers often utilize conformance testing as an indispensable step
to check errors and ﬂaws in both developing and maintaining software systems.
G. Hanaoka and T. Yamauchi (Eds.): IWSEC 2012, LNCS 7631, pp. 1–18, 2012.
c Springer-Verlag Berlin Heidelberg 2012


2

Y. Jing, G.-J. Ahn, and H. Hu

Conformance testing attempts to bridge the gap between system implementation and design requirements. It compares the expected behaviors described
by the system requirements with the observed behaviors of an actual implementation. The observed results reﬂecting the conformance of implementation
strongly depends on the adopted test cases [12]. In addition, test automation [17]
has recently become quite common for reducing the cost of software testing
procedures. A typical automated testing harness mainly oﬀers automation in
managing, executing and evaluating tests. However, such an approach cannot
eﬀectively support automated test generation. Manually creating test cases is
tedious, error-prone, and often insuﬃcient for proving the conformance of system implementation [19]. Such a problem exists in the widely used test harness
for Android, Google’s Android testing framework [3] [1]. Android testing framework only adopts hand-crafted test cases for conformance testing and fails to
provide a comprehensive set of test cases.
Model-based testing involves developing a data model to generate tests. The
model is developed based on the design requirements, and reﬂects the expected
features of the System Under Test (SUT) [7]. Unlike hand-crafted tests, modelbased approach helps reuse the generated test cases and improves the eﬃciency
of testing procedures. If any requirement changes, a tester only needs to update
the model and get a new suite of test cases, avoiding the tedious work of changing
hand-crafted test cases.
In this paper, we present a model-based conformance testing framework for
evaluating Android platforms. Our framework automatically generates and executes test cases. Moreover, we demonstrate the feasibility and practicality of
our approach through case studies on Android Inter-Component Communication (ICC) module. We chose ICC for several reasons: (1) ICC is one of the core
modules of Android as it supports collective interactions of applications; (2) the
requirements of ICC are publicly available. To conduct conformance testing in
our framework, we ﬁrst derive the formal models and properties for Android ICC
from design requirements. The formal speciﬁcations of models and properties are
fed into an analysis module to automatically generate test cases, which systematically enable the rigorous conformance testing for the Android platform. MCTF
checks whether the SUT’s behaviors conform to functional and non-functional
requirements. For example, the requirements specify a set of desired behaviors.
Therefore, it is necessary to discover invalid and malformed inputs that may
violate those requirements and should be caught and handled properly. Having
comprehensive conformance testing would ensure the correctness and assurance
of ICC in Android.
The remainder of this paper is organized as follows. Section 2 gives an overview
of Android ICC. Section 3 discusses our framework and demonstrates how our
framework can be applied to examine the conformance of Android ICC. Section 4
presents a tool chain designed with our framework followed by the discussion on
performance analysis. Section 5 describes the related work. Section 6 concludes
this paper and elaborates the future directions.

Model-Based Conformance Testing for Android

2

3

Overview of Android ICC

Smartphone applications inherently tend to communicate with each other. Android ICC is a sophisticated messaging system designed to support such interactions. In this section, we give a brief overview of Android ICC as described in
Android documentation for SDK (SDKD) [2] and Android Compatibility Deﬁnition Document (CDD) [1].
2.1

Components

The basic unit in Android application communication is component. Each component is a logical building block that could support each other. Four types of
components are deﬁned with various requirements.
– Activities are components that provide graphic user interface (GUI). The
Android GUI is implemented as a stack of activities starting one after another, where each activity is presented as a window on the screen.
– Services are components that run in the background to perform long-running
operations. Unlike activities, a service does not have any graphic interface.
Instead, services provide Remote Procedure Call (RPC) interfaces.
– Broadcast Receivers are asynchronous components that receive and reply to
system-wide broadcasts from other components.
– Content Providers are components that provide public data interfaces to
other components. A content provider provides common database commands
such as query, insert, update and delete, through which other components
can retrieve and store data.
2.2

Intents and Intent Filters

Intents play a leading role in connecting the components of applications. An
intent object is a data structure carrying information about its desired recipients
and optional data. Applications communicate with each other by sending and
receiving intents. All intents are processed and delivered by a centralized “post
oﬃce”, the intent resolver.
Like a post oﬃce processing parcels in the real world, the intent resolver ﬁnds
qualifying recipients by checking the attributes of an intent object.
Primary intent attributes include action and data:
– Action is a string naming the general action to be performed. An intent can
contain at most one action.
– Data is a tuple consisting of both the URI of the data to be acted on and
its MIME media type. This attribute indicates the data to be processed by
the action.
Secondary attributes include component, category, extras and flags.
– Component Name is a string naming the component that should handle the
intent.

4

Y. Jing, G.-J. Ahn, and H. Hu

– Category is a string containing additional information about the kind of
component that should handle the intent.
– Extras is a key-value pair of additional information to be delivered to the
recipient component.
– Flags is a set of strings that instruct the Android system to launch an
activity.
Each component can be bound to one or more intent filters, which declare capabilities of the components. An intent ﬁlter includes three attributes describing
the intents it would accept, including action, category and data. Intents and
components are correlated via intent ﬁlters. Android maintains a map between
public components and intent ﬁlters. The intent resolver ﬁnds the matching
intent ﬁlters for a given intent, then delivers the intent to the corresponding
components based on the map.

3

Model-Based Conformance Testing Framework
(MCTF)

In this section, we present our conformance testing framework, called modelbased conformance testing framework (MCTF), which is depicted in Figure 1.
Our framework is designed for generating test cases and facilitating rigorous
conformance testing with the generated test cases. We divide the framework
into four steps as follows:
1. System Modeling: Android Modeling.
First, all parameters and properties of Android are derived from Android
CDD and Android SDKD. Based on the identiﬁed parameters and properties, a model is deﬁned. Parameters describe data objects and attributes of
the system. Properties lay out rules regulating interactions of parameters.
Android parameters and properties are then formally represented.
2. Test Case Generation.
The most signiﬁcant recent development in testing is the application of formal reasoning techniques, such as model checking [11], theorem proving [24]
and SAT solving [23], to generate test cases from the formal speciﬁcation. In
this step, the formal model is utilized to automatically derive abstract test
cases, leveraging a formal reasoning technique.
3. Test Case Translation.
The generated test cases from the previous step are not suitable for direct
execution, since they are generated in an abstraction level. Therefore, it is
crucial to bridge the gap between abstract test cases and executable test
cases. The translation is performed to extract necessary information from
abstract test cases and construct executable test cases.
4. Test Case Execution.
In this step, executable test packages are generated by compiling executable
test cases. With the executable test packages, an Android device or emulator
is tested. For each test case, the results are monitored and recorded. Finally,

Model-Based Conformance Testing for Android

5

Android CDD + SDK Document

Android
Modeling

Abstraction

Parameter Specification

Test Case
Generation

Property Specification

Formal Verifier
Abstract Test Cases

Translator

Test Case
Translation

Executable Test Cases

Compiler

Test Case
Execution

Test Runner
Result Report

Fig. 1. Model-based Conformance Testing Framework

a human readable report is generated once all the tests are executed. The
generated test report may contain supplemental information, such as screenshots, to further examine other functional and non-functional components.
In order to conduct model-based conformance testing, it is crucial to have a
well-designed and general purpose language to represent the model. Alloy [20] is
a structural modeling language based on ﬁrst order logic, and has been widely
used in the modeling community. The usage of Alloy for the representation of
models is an attractive aim. Our framework adopts Alloy to formally represent
an Android model. As we discussed earlier, the formal model is in turn utilized
by formal reasoning tools such as Alloy Analyzer, to generate abstract test cases,
which are then translated into executable test cases.
We now demonstrate how Android ICC can be rigorously tested through the
four steps shown in Figure 1, identifying speciﬁc mechanisms for each MCTF
task.
3.1

System Modeling: Android Modeling

A model for a speciﬁc software system is an abstract speciﬁcation of the system’s behaviors. Parameters and properties comprise a typical model for capturing such behaviors. The parameters are attributes or variables that appear
in a piece of requirements. After parameters are identiﬁed, their types and valid

6

Y. Jing, G.-J. Ahn, and H. Hu

Intent Resolver
IntentFilter1
Intent
Action

Action

Component

Category

Activity1

DataURI

Category
Data

IntentFilter2
Action

Component

Category

Activity2

DataURI

Fig. 2. Implicit Intent Resolution

value ranges should be identiﬁed as well. For example, if an input variable accepts integers in the range of 1 to 12, the identiﬁed parameters should use the
same valid range. Properties are identiﬁed from the information about the relationships among parameters.
Android modeling procedure consists of three steps: model construction from
requirements, speciﬁcation of model parameters, and speciﬁcation of model
properties.
Model Construction from Android ICC Requirements. For testing Android systems and applications, testers derive parameters and properties from
Android SDKD and Android CDD. Android SDKD deﬁnes the requirements
of Android system, including objects and logics of Android functions and packages. Android CDD complements Android SDKD by providing additional technical details of various versions of Android platform.
For example, a technical section in Android SDKD says that “there are three
Intent characteristics that can be ﬁltered on: actions, data and categories”. From
this, testers identify three parameters: action, category and data. The deﬁnition
of these three attributes also shows the data type of each parameter. That is,
action is any string, category is any string set and data is a pair (2-tuple) of
strings.
Android SDKD and Android CDD describe Android ICC in two categories:
Explicit Intent Resolution and Implicit Intent Resolution, depending on the target attributes for the resolution process. If the component name of an intent
is a non-empty set, this intent is an explicit intent because the recipient component is given explicitly. The intent resolver delivers explicit intents to the
recipients designated by the ComponentName attribute, regardless of other attributes in the intent. Such process is called Explicit Intent Resolution. Actually,
no resolution process is occurred because the recipient is already speciﬁed by the
sender.

Model-Based Conformance Testing for Android

7

Thus, intent, component and intent resolver are identiﬁed as parameters of
explicit intent resolution. The attribute ComponentName is consulted. The property of explicit intent resolution is trivial, as abstracted below:
– Property 1: The intent should be delivered to the recipient designated by
the component name attribute of the intent.
Implicit intents do not specify any recipient component but wait for the intent
resolver to determine which component they should be resolved to, based on the
action, data and category attributes speciﬁed in the intent. This process is called
Implicit Intent Resolution.
The parameters of implicit intent resolution include intent, intent filter, component, and intent resolver. Action, category and data are attributes that are
consulted during the resolution process. Each attribute corresponds to a test,
in which the attribute of the intent is matched against that of the intent ﬁlter.
To be delivered to the component, an implicit intent must pass all the three
tests on the intent ﬁlters bound with the component. Since a component can be
bound with multiple intent ﬁlters, an intent that does not pass through one of
a component’s intent ﬁlters may pass another.
In the action test, the Android Intent Resolver tests both the action of the
intent object and the action set of the intent ﬁlter. An intent names a single
action while the intent ﬁlter speciﬁes one or more actions. To pass the action
test, the action speciﬁed in the intent object must match at least one of the
actions speciﬁed in the intent ﬁlter. The action set of the intent ﬁlter object
must not be empty. A special case is an intent without actions, which passes all
action tests. The properties of action test can be summarized as follows:
– Property 2: The action speciﬁed in the Intent object must match one of
the actions listed in the ﬁlter.
– Property 3: An Intent object that does not specify an action automatically
passes the test as long as the ﬁlter contains at least one action.
The category ﬁelds in both the intent and intent ﬁlter are a set of category strings.
To pass the category test, the category set of the intent should be the subset of
the category set of the intent ﬁlter. The ﬁlter can list additional categories, but
it cannot omit any in the intent. An intent without category passes all category
tests by default. The properties of category test can be summarized as follows:
– Property 4: Every category in the Intent object must match a category in
the ﬁlter. The ﬁlter can list additional categories, but it cannot omit any in
the intent.
– Property 5: An Intent object with no category should always pass this test,
regardless of the attributes in the ﬁlter.
Data contains URI and type. The URI speciﬁes the location of the data in three
sub-attributes: scheme, authority and path. The data type speciﬁes the MIME
type of the data. Android also allows wildcards when specifying data subtype in
both the intent and intent ﬁlter.

8

Y. Jing, G.-J. Ahn, and H. Hu

– Property 6: An Intent object that contains neither a URI nor a data type
passes the test only if the ﬁlter likewise does not specify any URIs or data
types.
– Property 7: An Intent object that contains a URI but no data type passes
the test only if its URI matches a URI in the ﬁlter and the ﬁlter likewise
does not specify a type.
– Property 8: An Intent object that contains a data type but no URI passes
the test only if the ﬁlter lists the same data types and similarly does not
specify a URI.
– Property 9: An Intent object that contains both a URI and a data type
passes the data type part of the test only if its type matches a type listed in
the ﬁlter.
Figure 2 shows an example of implicit intent resolution. In this example, a public
component is bound with two intent ﬁlters. An intent resolver attempts to resolve
the intent shown on the left. If all of the tests pass for both intent ﬁlters, the
intent is delivered to the two components on the right.
Specification of Model Parameters. Based on Android SDKD and Android
CDD, we formulate the identiﬁed parameters. We ﬁrst deﬁne Component as
follows:
Definition 1. A component is represented with a (τ ), where τ is a unique name
of the component;
Intent can be deﬁned as follows:
Definition 2. An intent is represented with a 5-tuple (τ, α, Γ, σ), where τ is
the name of the recipient component; α is an action string that describes the
action to be performed; Γ is a set of category strings that represent the type
of components which should handle the intent; and σ is a 2-tuple (uri, type)
consisting of data URI and data type.
Intents can be classiﬁed into two categories: explicit intent and implicit intent,
as we discussed earlier. We formally deﬁne them as follows:
Definition 3. Explicit intents designate the target component by its component
name field. The set of explicit intents is denoted as EI. EI={i | i ∈ I ∧i.τ = null}
Definition 4. Implicit intents do not specify a target. The set of implicit intents
is denoted as II. II={i | i ∈ I ∧ i.τ = null}
Then, the intent filter can be deﬁned as:
Definition 5. An intent filter is represented with a 3-tuple (Λ, Γ, σ), where Λ
is a set of action strings; Γ is a set of category strings; and σ is is a set of
(uri, type) tuples consisting of data URI and data type.

Model-Based Conformance Testing for Android

9

We now formally deﬁne the intent resolver with sets and relations as:
–
–
–
–
–
–

C is a set of components, {c1 , · · · , cp };
I is a set of intents, {i1 , · · · , im };
F is a set of intent ﬁlters, {f1 , · · · , fq };
F C ⊆ F × C, a many-to-many ﬁlter-to-component assignment relation;
EIC, a one-to-one explicit intent-to-component assignment relation;
IIF , a one-to-many implicit intent-to-ﬁlter assignment relation;

Based on the above-deﬁned model, we now give the formal speciﬁcation of identiﬁed parameters with Alloy as follows:
module android/ICC
abstract sig Str {}
sig actionStr extends Str{}
sig categoryStr extends Str{}
sig uriStr extends Str{}
sig typeStr extends Str{}
sig dataTuple {
uri: lone uriStr,
type: lone typeStr }
abstract sig Object {}
sig Component extends Object {
componentName: lone componentStr }

sig Intent extends Object {
componentName: lone componentStr,
action: lone actionStr,
category: set categoryStr,
data: lone dataTuple }
sig Filter extends Object {
action: set actionStr,
category: set categoryStr,
data: set dataTuple }
sig Resolver {
IIF: Intent -> set Filter,
IIF_A: Intent -> set Filter,
IIF_C: Intent -> set Filter,
IIF_D: Intent -> set Filter,
FC: Filter -> set Component,
EIC: Intent -> lone Component }

The ﬁrst sig statement declares Str, which represents a string that can be
assigned to other objects. Then, we deﬁne component, intent and intent filter
which have all the necessary attributes for intent resolution. We then declare
a resolver, which deﬁnes several relations which map intents to sets of intent
ﬁlters. The value ranges of all the parameters are strings.
Specification of Model Properties. Based on Android SDKD and Android
CDD, we now formulate and specify properties of Android ICC. A fact statement in Alloy puts an explicit constraint on the model. In our cases, we need
to represent the identiﬁed properties of intent resolution with facts. According
to the properties identiﬁed from the requirements, we then give their formal
speciﬁcations.
The formal speciﬁcation of Property 1, which covers Explicit Intent Resolution, is shown below:
fact explicitIntentResolution {
all r: Resolver, i: Intent, c:Component |
i.componentName = c.componentName
<=> i->f in r.EIC }

The following shows formal speciﬁcations of Property 2-9, which cover Implicit
Intent Resolution:

10

Y. Jing, G.-J. Ahn, and H. Hu

fact implicitIntentResolutuion {
all r: Resolver, i: Intent, f:Filter |
i->f in r.IIF_A
and i->f in r.IIF_C
and i->f in r.IIF_D
<=> i->f in r.IIF }

fact actionTest {
all r:Resolver| all i:Intent |all f:Filter |
(f.action!=none and i.action!=none
and i.action in f.action)
or (f.action!=none and i.action = none)
<=> i->f in r.IIF_A }

3.2

fact categoryTest {
all r:Resolver| all i:Intent |all f:Filter |
(i.category!=none and i.category in f.category)
or (f.category!=none and i.category = none)
<=> i->f in r.IIF_C }
fact dataTest {
all r:Resolver| all i:Intent |all f:Filter |
(i.data.uri=none and i.data.type=none
and f.data.uri=none and f.data.type=none)
or (i.data.uri in f.data.uri
and i.data.type = none and f.data.type=none)
or (i.data.type in f.data.type
and i.data.uri = none and i.data.uri=none)
or (i.data.uri in f.data.uri
and i.data.type in f.data.type)
<=> i->f in r.IIF_D }

Test Case Generation

In conformance testing, testers need to generate positive and negative test cases
to examine the implementation thoroughly. Positive test cases test whether the
system behaves exactly as the speciﬁed properties when inputs are valid. Negative test cases test whether the system violates the properties when inputs
are invalid. Formal reasoning tools can generate abstract test cases accordingly.
They translate the model notations into boolean formulas. Then, the formulas
are analyzed to ﬁnd bindings of the parameters and their values that make the
formulas true or false. Such true and false bindings are positive and negative test
cases, respectively. To generate abstract test cases, we employ Alloy Analyzer to
generate instances that satisfy both facts and predicates.
Positive test cases for a given property are derived from the formal model
representation, in which the property speciﬁcation serves as a predicate for generating instances that conform to the very property. Similarly, negative test
cases are generated from the formal model representation, if we consider it as
a predicate to identify counterexamples, which satisfy the negated property. As
a model-based testing framework, MCTF can assist test activities at property
and behavior levels [13].
Property Testing. We take Property 2 as an example to demonstrate the
process of automated test generation for testing a given property from positive
and negative aspects. To simplify the test case generation process, we remove the
parameters and properties that are not related with action test. The following
predicate is deﬁned to derive the positive test cases for the corresponding facts
in the formal property speciﬁcation.
pred P2_pos(r: Resolver, i:Intent) {
all r: Resolver, i: Intent, f:Filter |
one i.action and i.action in f.action
<=> i->f in r.IIF_A}

This predicate checks Property 2 against the model representation of Android
ICC, then instances are generated. The generated instances are used to construct
positive test cases to ensure that the system should always permit a matched
pair of intent object and intent ﬁlter object.

Model-Based Conformance Testing for Android

11

The corresponding negative test cases for negated Property 2 are generated to
ensure the system never denies a matching pair or accepts a mismatching pair. In
order to derive negative test cases, we specify the negative property with Alloy
as follows:
pred P2_negDeny(r:Resolver, i:Intent,
f:Filter)
{i->f not in r.IIF_A
and i.action in f.action
and i.action!=none
}

pred P2_negAccept(r:Resolver, i:Intent,
f:Filter
){i->f in r.IIF_A
and i.action not in f.action
and i.action!=none
}

Alloy Analyzer requires a bounded input domain, speciﬁed by the number
of intents, intent ﬁlters, resolvers, action strings in our example, to generate
instances and counterexamples. The size of input domain determines the total
number of generated test cases. Then, we come up with the question of choosing
an appropriate size for generating test cases that achieve reasonable coverage.
Although testers can specify a large input domain and get millions of test cases
for a trivial property with respect to the coverage, it is not always the case. The
testers need to specify the input size based on practical test requirements1 .
For example, we specify the following input domain to test Property 2.
run P2_pos for
exactly 1 Resolver, exactly 2 actionStr,
exactly 2 Str, exactly 2 Intent,
exactly 2 Filter

run P2_negDeny for
exactly 1 Resolver, exactly 2 actionStr,
exactly 2 Str, exactly 2 Intent,
exactly 2 Filter
run P2_negAccept for
exactly 1 Resolver, exactly 2 actionStr,
exactly 2 Str, exactly 2 Intent,
exactly 2 Filter

Figure 3 depicts a positive test case generated by Alloy Analyzer for Property 2. Both Intent and Filter0 have the same action. Thus, Resolver allows
the interaction between them. Figure 4 and Figure 5 depict two negative test
cases. In Figure 4, Resolver unexpectedly denies Intent from accessing Filter1
(marked by (f) and (i)). In Figure 5, Resolver unexpectedly accepts Intent
and Filter1 (marked by (f) and (i)), which have diﬀerent actions.
Behavior Testing. After each property has been tested independently, we can
further check behaviors of the intent resolution module. Here, we give a more
complex scenario to test all modeled intent ﬁlter properties. Based on the aforementioned properties, we instruct Alloy Analyzer to enumerate all assignments,
simulating inter-component communications.
To test if a system always properly delivers the intent to correct recipients,
we need positive test cases that are composed of matched pairs of intents and
intent ﬁlters. In our model, it implies the set of iif relation should not be empty.
Therefore, we have the following speciﬁcation:
pred Positive(r: Resolver){
#r.IIF>0 }
1

The testers should balance the coverage and the input size, which are normally
obtained from subject matter experts and prior testing results.

12

Y. Jing, G.-J. Ahn, and H. Hu

Resolver
(r)

Filter1

IIF [Intent] IIF_A [Intent]

Intent

Filter0

action

action

actionStr0

action

actionStr1

Fig. 3. Abstract Test Cases for Property Testing: Positive

Resolver
(r)
IIF [Intent]IIF_A [Intent]

Filter1
(f)

Intent
(i)

action action

actionStr1

Filter0

action

action

actionStr0

Fig. 4. Abstract Test Cases for Property Testing: Negative Deny

On the contrary, negative test cases are those without paired intents and
intent ﬁlters. We simply set the size of iif to zero.
pred Negative(r: Resolver){
#r.IIF=0 }

Figure 6 depicts a positive test case for behavioral testing. In this example,
two successful intent deliveries can be identiﬁed from the arrows labeled with
“IIF[Intent]”: Intent0→Filter0, Intent1→Filter1.
In addition, the test case generation can be optimized to avoid generating
isomorphic test cases by adopting the approach proposed in [8]. Finally, each
abstract test case is exported to an independent ﬁle which contains the test
conditions and variables for further processing. Because we are using Alloy Analyzer, one of the available choices is to export test cases to DOT ﬁles, which
store test cases as hierarchical drawings of direct graphs. This is a perfect choice
for visualizing abstract test cases. Another choice is to export test cases into

Model-Based Conformance Testing for Android

Intent
(i)

Filter0

action

13

Resolver
(r)

action

IIF [Intent]IIF_A [Intent]

Filter1
(f)

actionStr1

action

actionStr0

Fig. 5. Abstract Test Cases for Property Testing: Negative Accept

Resolver
(r)

IIF [Intent0]

IIF_A [Intent0]

IIF_C [Intent0]

Filter0

action

actionStr1

IIF_D [Intent0]

IIF [Intent1]

Intent0

category

action

IIF_A [Intent1] IIF_C [Intent1] IIF_D [Intent1]

Filter1

data category data

categoryStr1

action

dataTuple1

type

typeStr0

actionStr0

Intent1

category

action

data

categoryStr0

category

dataTuple0

uri

uriStr0

data

type

typeStr1

uri

uriStr1

Fig. 6. A Positive Test Case for Behavior Testing

lightweight XML ﬁles, which are easy to parse with existing tools. We adopt the
latter for generating executable test cases.
3.3

Test Case Translation

Except for requirements, Android SDKD also provides guidelines of Android
testing framework and testing Android applications. Android CDD and Compatibility Test Suite (CTS) [1] provides additional guidelines for testing Android.
Android test suites are based on JUnit [18] and Android’s JUnit extensions. The
extensions provide component-speciﬁc test classes and helper methods to help
creating mock objects and controlling lifecycle of a component. In addition, CTS
is shipped with an automated test harness. Testers can choose to use the test
harness of Android CTS, use a third-party test harness, or write their own test
runner based on the APIs provided by Android testing framework.

14

Y. Jing, G.-J. Ahn, and H. Hu

Abstract test cases generated by Alloy Analyzer in our approach cannot be
directly integrated into test suites for execution as they are at diﬀerent abstraction levels. Thus, an additional step is required to translate abstract test cases
encoded in XML to executable test cases, involving information extraction and
source code construction.
Extraction. We employ a Python script to parse XML and regroup essential
information ﬁelds with cElementTree [4]. cElementTree is a Python package for
eﬃciently managing XML ﬁles.
In order to construct an executable test case for testing intent resolution, we
need to know all the variables, attributes and their assigned values. In our case,
the variables are intents and intent ﬁlters, and the attributes are component
name, action, category, data, URI and type. An XML-encoded abstract test
case is composed of several ﬁelds and tuples. Each ﬁeld stands for an attribute.
And each ﬁeld consists of some tuples, which store a variable and the value of
the attribute of that variable. Hence, information extraction can be achieved by
enumerating tuples and ﬁelds and reorganizing them.
Suppose we have a fragment of an XML-encoded abstract test case as shown
below:
<field label="action" ID="13" parentID="11">
<tuple> <atom label="Intent$2"/>
<tuple> <atom label="Intent$2"/>
<atom label="categoryStr$0"/> </tuple>
<atom label="actionStr$0"/> </tuple>
<tuple> <atom label="Intent$2"/>
</field>
<atom label="categoryStr$1"/> </tuple>
<field label="category" ID="14" parentID="11"> </field>

From this fragment we can identify an Intent object Intent2. Its action is assigned to actionStr0, its category is assigned to {categoryStr0, categoryStr1}.
Code Construction. The extracted information ﬁelds are utilized for a test
case template and Java code fragments for Android Compatibility Test Suite
(CTS). Our template is strictly complied with the format and syntax of test
cases deﬁned in Android CTS.
The sample code shipped with Android CTS oﬀers practical examples of how
to write executable test cases. We give a code template for testing Android ICC.
IntentFilter
String[]
String[]
String[]

filter = new Match(
actions, String[] categories,
checkMatches(filter, new MatchCondition[] {
dataTypes, String[] uriSchemes,
new MatchCondition(
uriAuthoroties, String[] uriPorts);
int expectedResult,
String action, String[] categories,
String dataType, String dataURI); }

With the extracted information in the template, we get several Java code
fragments at the end of this step.
3.4

Test Case Execution

After integrating the code fragments into existing test suites or a new test suite,
executable test cases are derived by compiling fragments. Such test suites are
run by a test runner that loads the test cases, runs and tears down each test.
We use Android’s Instrumentation Test Runner [3], which is a set of control

Model-Based Conformance Testing for Android

15

methods and hooks in Android platform, to run our generated test cases. For
each executable test case, the results are generated accordingly as we discussed
in our framework. Finally, a report is presented in an HTML page including test
results.

4

Implementation and Evaluation

In this section, we give a brief introduction of our tool set, which constitutes a
tool chain for model-based conformance testing. As depicted in Figure 7, our tool
chain consists of three tools: Alloy Analyzer, the Translator and the Android Instrumentation Test Runner. The formal representation of models and properties
are fed into Alloy Analyzer for automatically generating test cases. Alloy Analyzer exports the generated abstract test cases to intermediate XML ﬁles. Then,
our translator parses XML and constructs Java code fragments. The output of
test case translation is an Android application package containing compiled JUnit test cases. Finally, Android Instrumentation Test Runner executes test suite
and generates the test report.
Parameter
Specification
Alloy Analyzer

Abstract
Test Cases

Property
Specification
Translator

Test Result
Report

Android
Instrumentation
Test Runner

Executable
Test Cases

Fig. 7. A tool chain that supports MCTF

We provide a contrastive analysis between Android CTS and our generated
test cases to demonstrate eﬀectiveness of our framework in this section. For
property testing, every property of the three tests need to be rigorously checked.
We identiﬁed that Android CTS fails to check some properties from positive or
negative aspects. Table 1 shows a comparison between Android CTS and the
test cases generated by our approach. The table shows that Android CTS test
suites are not oﬀering suﬃcient test coverage. And our approach could achieve
better coverage than that of Android CTS.
To evaluate the eﬃciency of our approach, we also examined two core processes, test case generation and test case translation, in our implementation.
Figure 8(a) shows that the increase of the total number of generated test
cases is proportional to the number of intents and intent ﬁlters. Figure 8(b)
shows that the processing time taken for test case generation and translation
increases linearly with the increase of the number of the test cases, indicating
that our approach provides a feasible and promising solution to facilitate and
enhance conformance testing for Android platform.

16

Y. Jing, G.-J. Ahn, and H. Hu

(a) Amount of Generated Test Cases

(b) Processing Time

Fig. 8. Performance Evaluation
Table 1. Conformance testing achieved by Android CTS and our approach
Property Positive/Negative
Property 1
Property 2
Property 3
Property 4
Property 5
Property 6
Property 7
Property 8
Property 9

5

Positive
Negative
Positive
Negative
Positive
Negative
Positive
Negative
Positive
Negative
Positive
Negative
Positive
Negative
Positive
Negative
Positive
Negative

Android CTS
MCTF
Covered #Test cases Covered #Test cases
√
×
0
16
√
×
0
18
√
√
3
24
√
√
2
14
√
√
2
24
√
×
0
10
√
√
4
26
√
√
4
10
√
√
2
26
√
×
0
12
√
√
2
31
√
√
2
18
√
√
1
31
√
√
3
20
√
√
1
31
√
×
0
20
√
×
0
31
√
√
2
26

Related Work

Most recent work related to software testing in Android addresses automated
GUI testing for Android applications. Amalﬁtano et al. [6] proposed a crawlingbased approach to generate GUI test cases. They designed a tool to simulate
events on the user interfaces, generate event transition tree by capturing application responses, and predict future events at runtime. In contrast, our approach
is the ﬁrst attempt to explore rigorous conformance testing for Android. In particular, we adopt a model-based approach to automatically generate test cases.
Model-based approaches have been widely used for testing in various ﬁelds.
Several researchers proposed automated frameworks for testing Java programs,
such as Korat [8] and TestEra [21]. Korat constructs Java predicates and generates all non-isomorphic inputs for which the predicates return true, by searching
and enumerating a given bounded input space. TestEra works in a similar way
as Korat, but using a ﬁrst-order relational language and existing SAT solvers.

Model-Based Conformance Testing for Android

17

Both approaches use structural invariants on the input data to automatically
generate test cases and then test the output against a set of predicates. However, the generated test cases are abstract and need to perform the translation
task to generate the actual code. In our work, we attempt to extend model-based
approaches to testing Android platforms. We also demonstrate how test cases
can be integrated to perform conformance testing eﬀectively.
Security for mobile devices and applications is a growing concern recently.
TaintDroid [14] monitors and controls access to sensitive data by dynamic taintbased information ﬂow tracking. Stowaway [16] identiﬁes vulnerabilities in applications by static analysis on application packages, manifests and bytecodes.
Chaudhuri [9] proposed a formal language to describe applications and reason
about information ﬂows and the consistency of security speciﬁcations.

6

Conclusion

While several automated testing frameworks have been proposed and developed
for smartphone platforms, developers still need systematic approaches and corresponding tools to generate test cases for conformance testing eﬃciently and
eﬀectively. To address this issue, we have proposed a novel framework to enable
rigorous conformance testing for the Android platform. Our framework adopted
a model-based approach which utilizes formal veriﬁcation techniques to automatically generate test cases. In addition, we have demonstrated the feasibility
of our approach with Android ICC.
In our current framework, testers need to manually derive the model from
requirements. As part of our future work, we would explore an approach for
directly constructing model from the requirements, leveraging the capability of
NLP techniques [22]. Moreover, we would apply our approach to other Android
modules, such as Activity Manager and Package Manager.

References
1. Android compatibility, http://source.android.com/compatibility/
2. Android sdk cocument,
http://developer.android.com/reference/android/package-summary.html/
3. Android testing fundamentals, http://developer.android.com/guide/topics/
testing/testing android.html#Instrumentation
4. The elementtree xml api,
http://docs.python.org/library/xml.etree.elementtree.html
5. Apple rises to the top as worldwide smartphone market grows 65.4% in the second
quarter of 2011, idc ﬁnds (August 2011),
http://www.idc.com/getdoc.jsp?containerId=prUS22974611
6. Amalﬁtano, D., Fasolino, A.R., Tramontana, P.: A gui crawling-based technique for
android mobile application testing. In: IEEE Fourth International Conference on
Software Testing, Veriﬁcation and Validation Workshops (ICSTW), pp. 252–261.
IEEE (2011)
7. Beizer, B.: Software testing techniques. Dreamtech Press (2002)

18

Y. Jing, G.-J. Ahn, and H. Hu

8. Boyapati, C., Khurshid, S., Marinov, D.: Korat: Automated testing based on java
predicates. In: Proceedings of the 2002 ACM SIGSOFT International Symposium
on Software Testing and Analysis, pp. 123–133. ACM (2002)
9. Chaudhuri, A.: Language-based security on android. In: Proceedings of the ACM
SIGPLAN Fourth Workshop on Programming Languages and Analysis for Security,
pp. 1–7. ACM (2009)
10. Chin, E., Felt, A.P., Greenwood, K., Wagner, D.: Analyzing inter-application communication in android. In: MobiSys, pp. 239–252 (2011)
11. Clarke, E., Grumberg, O., Peled, D.: Model checking (2000)
12. Constant, C., Jéron, T., Marchand, H., Rusu, V.: Integrating formal veriﬁcation
and conformance testing for reactive systems. IEEE Transactions on Software Engineering 33(8), 558–574 (2007)
13. Dalal, S.R., Jain, A., Karunanithi, N., Leaton, J.M., Lott, C.M., Patton, G.C.,
Horowitz, B.M.: Model-based testing in practice. In: Proceedings of the 1999 International Conference on Software Engineering, pp. 285–294. IEEE (1999)
14. Enck, W., Gilbert, P., Chun, B.G., Cox, L.P., Jung, J., McDaniel, P., Sheth, A.N.:
Taintdroid: An information-ﬂow tracking system for realtime privacy monitoring
on smartphones. In: Proceedings of OSDI (2010)
15. Enck, W., Ongtang, M., McDaniel, P.: On lightweight mobile phone application
certiﬁcation. In: Proceedings of the 16th ACM Conference on Computer and Communications Security, pp. 235–245. ACM (2009)
16. Felt, A., Chin, E., Hanna, S., Song, D., Wagner, D.: Android permissions demystiﬁed. Technical report (2011)
17. Fewster, M., Graham, D.: Software test automation: eﬀective use of test execution
tools. ACM Press/Addison-Wesley Publishing Co. (1999)
18. Gamma, E., Beck, K.: Junit: A cook’s tour. Java Report 4(5), 27–38 (1999)
19. Hu, H., Ahn, G.: Enabling veriﬁcation and conformance testing for access control
model. In: Proceedings of the 13th ACM Symposium on Access Control Models
and Technologies, pp. 195–204. ACM (2008)
20. Jackson, D.: Alloy: a lightweight object modelling notation. ACM Transactions on
Software Engineering and Methodology (TOSEM) 11(2), 256–290 (2002)
21. Khurshid, S., Marinov, D.: Testera: Speciﬁcation-based testing of java programs
using sat. Automated Software Engineering 11(4), 403–434 (2004)
22. Lewis, D.D., Jones, K.S.: Natural language processing for information retrieval.
Communications of the ACM 39(1), 92–101 (1996)
23. Mitchell, D.G.: A sat solver primer. Bulletin of the European Association for Theoretical Computer Science 85(112-133), 12 (2005)
24. Robinson, J.A., Voronkov, A.: Handbook of automated reasoning, vol. 1. NorthHolland (2001)

