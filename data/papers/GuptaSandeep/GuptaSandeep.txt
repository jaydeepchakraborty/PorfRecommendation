The update complexity of selection and related problems

arXiv:1108.5525v1 [cs.DS] 29 Aug 2011

Manoj Gupta
Deptt. of Comp. Sc.,
IIT Delhi, New Delhi.

Yogish Sabharwal
IBM Research - India,
New Delhi.

Sandeep Sen
Deptt. of Comp. Sc.,
IIT Delhi, New Delhi.

October 8, 2013
Abstract
We present a framework for computing with input data specified by intervals, representing uncertainty
in the values of the input parameters. To compute a solution, the algorithm can query the input
parameters that yield more refined estimates in form of sub-intervals and the objective is to minimize
the number of queries. The previous approaches address the scenario where every query returns an exact
value. Our framework is more general as it can deal with a wider variety of inputs and query responses and
we establish interesting relationships between them that have not been investigated previously. Although
some of the approaches of the previous restricted models can be adapted to the more general model, we
require more sophisticated techniques for the analysis and we also obtain improved algorithms for the
previous model.
We address selection problems in the generalized model and show that there exist 2-update competitive algorithms that do not depend on the lengths or distribution of the sub-intervals and hold against
the worst case adversary. We also obtain similar bounds on the competitive ratio for the MST problem
in graphs.

1

Introduction

A common scenario in many computational problems is uncertainty about the precise values of one or
more parameters. Many different models have been considered in the database community for dealing with
uncertain data. In one of the commonly used models, the uncertain parameters are represented by probability
distributions (for a comprehensive survey, see[AY09]). In another model, the uncertain parameters are
represented by interval ranges, wherein the parameter may take on any value within the specified interval
(see [KT01]). In this paper, we focus on the latter model. More formally, we consider the model wherein
we want to compute a function f (x1 , x2 . . . xn ) where some (or all) xi ’s are not fully known. The xi ’s are
typically known to lie in some range (interval). Any assignment of xi = x′i consistent with the known range
of xi is a feasible realization. The algorithm can make queries about xi . This problem has been studied
before [KT01, HEK+ 08]. A common assumption made in the existing literature is that the exact value of xi
is returned by a single query. However, in many applications, a query about xi may only yield a more refined
estimate of the xi . As a matter of fact, in many such applications, it is not even possible to obtain the exact
value of the parameter. As an example, consider the case of handling satellite data such as maps. Due to
the large amount of data involved, the data is often stored hierarchically at different scales of resolutions.
Typically the data is presented at the highest level of resolution. Depending on the area of interest, data
may be retrieved for the next level of resolution for a smaller area (zoom in) by performing a query. Now
consider a query to find the closest hospital. Based on the highest scale of resolution, the distances to the
hospitals can be determined within a certain range of uncertainty. If the closest hospital cannot be resolved
at this level, then further queries are required for certain hospitals to determine which amongst them is the
closest. These queries proceed down the hierarchical scales of resolution until it is resolved which is the
closest hospital.

1

Let us illustrate this model using the problem of finding minimum when the exact values are not known
but each element is associated with a real interval [ℓi , ri ]. Consider the three elements x1 = [3, 17], x2 =
[14, 19], x3 = [15, 20]. Clearly any of these can be the minimum element as these are mutually overlapping
intervals. Suppose a query returns the exact value, then with three queries, we obtain the complete information and the problem is trivially solved. But the interesting question is - are three queries necessary ?
Suppose our first query yields that x1 = 10, then clearly we do not need to make any further queries. On the
other hand, the query may yield x1 = 16, so that we are forced to make further queries. In a more general
situation, where a query may return a sub-interval, we may obtain x1 = [8, 16] that doesn’t yield any useful
information about the identity of the minimum element. On the other hand, if the query returns [8, 10],
then we can conclude x1 to be the minimum even though we do not know the exact value of x1 .
It is natural to compare the number of queries made by the algorithm w.r.t. a hypothetical OP T
which can be thought of as a non-deterministic strategy that makes the minimum queries for any feasible
realization of the input. Moreover, the algorithm must contain a certificate of correctness of the final answer,
viz., that no more queries are necessary regardless of the number of unresolved parameters. This also brings
up the related verification problem, i.e., given an incompletely specified problem, does it contain sufficient
information for a solution to be computed (without further queries).

1.1

Related Previous Work

Kahan [Kah91] described a technique for maintaining data structures for online problems like flight-path
collisions using predictive estimates to obtain higher efficiency. The estimates could be used to prune objects
that couldn’t provably affect the solution and only those critical objects were updated that could affect the
answer. Kahan’s work laid the foundations for later work on kinetic data structures but in his paper,
he focussed on describing a framework for minimizing updates of critical objects. Kahan compared the
efficiency of his data structures with respect to a non-deterministic optimal algorithm, or more specifically,
the competitive ratio in the online setting. If our algorithm makes qS (n) queries for an input S of size n,
then it has competitive ratio c 1 iff for some constant α > 0,
qS (n) ≤ c · OP T (S) + α
where OP T may be thought of as a non-deterministic algorithm (coined as lucky in [Kah91]) Note that OP T
has an unfair advantage in being able to guess the optimal sequence of queries and ensure that it can be
verified in collusion with an adversary controlling the output of the queries.
For instance, if the given intervals are x1 = [2, 6], x2 = [2, 6], x3 = [2, 6], i.e., all of them are identical, OP T
may guess the answer to be x3 and if the query yields x3 = 2, then it is verified. On the other hand, an
algorithm has no means of distinguishing between the xi ’s. Even use of randomization does not appear to
provide any significant advantage in this scenario. Kahan [Kah91] tackled this issue (without acknowledging
as much) by changing the problem definition to that of reporting all values that are equal to the minimum.
Khanna and Tan [KT01] also used the competitive ratio as a measure of efficiency of their algorithms
but their parametrization didn’t yield O(1) bounds. Their algorithms for selection was related to the clique
number (maximum clique size) of the input. They compare with Non-deterministic optimal and show that,
no on-line algorithm can achieve a better competitive ratio than the clique number.
A somewhat different model was used by Erlebach et al.[HEK+ 08], who showed how to compute an exact
minimum spanning tree for graph with interval data using minimal number of queries. The final answer is
a combinatorial description (in this case a spanning tree) and not necessarily the weight of the spanning
tree. Erlebach et al.[HEK+ 08] proved that their algorithm has competitive ratio 2 when the edge weights
are initially specified as open intervals. One limitation of their result is the critical use of the property of
open intervals which is used to weaken the advantage of OP T in guessing and verifying the answer. Their
results on constant competitive ratio do not hold for closed or semi-closed intervals.
A recent motivation for this line of work came from caching problems in distributed databases, (Olston
and Widom [OW00]), where local cached copies are used for faster query processing where the cached values
are intervals that are guaranteed to contain the actual value called the master value. Their work showed
1 So

strictly speaking, the algorithm could take exponential time but may have a bounded competitive ratio.

2

O
C
OC
P
OP
CP
OCP

O
Category-1
(Note α)
(Note α)
trivial
Category-2
(Note α)
(Note α)

C
(Note α)
Category-1
(Note α)
(Note α)
Category-2
(Note α)

OC
(Note α)
(Note α)
Category-1
(Note α)
(Note α)
Category-2

P
(Note α)
(Note α)
(Note α)
OP-P
Category-3
Category-3

OP
(Note α)
(Note α)
(Note α)
OP-OP
(Note α)
(Note α)

CP
(Note α)
(Note α)
(Note α)
(Note α)
Category-3
(Note α)

OCP
(Note α)
(Note α)
(Note α)
(Note α)
(Note α)
Category-3

Figure 1: Models for studying uncertain data problems (see note for α below). The allowed input types
listed along the rows and the query return types listed along the columns. (The pure input point model is
trivial as no queries are required).
trade-off between the number of queries and the precision ∆ of the actual answer. This model was further
explored in the work of [FMP+ 03, FMO+ 03] that tackled fundamental problems like median-finding and
shortest-paths. They distinguished between the offline (oblivious) and online (adaptive) queries including
weighted versions where queries could have varying costs for different intervals. Unlike the previous work,
they compared their efficiency with respect to a worst case optimal rather than a non-deterministic inputspecific optimal. Therefore their results cannot be compared effectively with the previous work. Other
approaches like [AH04, KZ06] minimize the worst case deviation from actual values or minimizing queries
to get improved estimates of the expected solution when the distribution is known [GGM06, GM07].

2

Our contributions

In this paper, we generalize the query model in several directions. We classify models based on the types
of the inputs allowed and the return type of the queries. The input may specify a combination of points
(P), open intervals (I) and/or closed intervals (C). This leads to 7 variations , namely, O, C, P, OC, OP,
CP and OCP. Similarly queries on intervals (open/closed) may yield points (P), open intervals (I) and/or
closed intervals (C)2 . This also leads to seven variations. These models are specified in Figure 1. We denote
the models by X-Y where X denotes the type of the input allowed in the input instance and Y denotes the
query return types where X and Y can take values from O, C, P, OC, OP, CP and OCP (here the literals
O, C and P correspond to open intervals, closed intervals and points respectively). Thus for instance OP-P
denotes the model wherein the input can consist of open intervals as well as points and the queries can only
return points.
(Note α): Although there are 49 models possible, many of them are unnatural as they can lead to a
change of the input type after some initial queries. The framework of such models can be covered under the
framework of another suitable model. For instance, a problem under the O-P model would convert to OP-P
model after a single query and is thus better studied under the OP-P model. Similarly, the OC-C model can
be covered under the OC-OC model.
We categorize the valid models into 5 different categories (See Figure 1). The competitive ratios are
based on this categorization of the models. Category-1 corresponds to the models where the input and query
return types are only intervals (O-O, C-C, OC-OC models). Category-2 corresponds to the models where the
input may contain points by the queries only return intervals (OP-O, CP-C, OCP-OC models). Category-3
corresponds to the models where the input may contain closed intervals and the query may return points.
The other two categories correspond to the OP-P and OP-OP models themselves.
Our main results can be summarized as follows
1. We first generalize the models to practical scenarios wherein queries may return sub-intervals as answers
rather than exact values. The sub-intervals need not have any properties with respect to lengths or
distributions. In other words, with further queries, we obtain increasingly refined estimates of the
2 We can also handle semi-closed intervals but we have avoided further classification as they don’t lead to any interesting
results.

3

values until sufficient information has been obtained, i.e., the verification problem can be solved. We
show that the witness based approach used in the previous models can be adapted to the models
considered in this paper. More specifically, we establish interesting relationships between the various
models (see Figure 2).
2. We study the selection problem of finding the k th smallest value and present update competitive
algorithms with different guarantees for the different models for this problem. We also study the update
complexity of minimum spanning tree problem under the different models that is closely related to the
extremal selection problem (finding the heaviest edge in a cycle – also called the Red rule).
3. We also show that by deviating from the witness based approach studied in prior literature, we can
actually obtain improved bounds for the selection problem. These algorithms attain an additive overhead from optimal, that is similar to a competitive ratio of unity for some cases and are interesting in
their own right.
4. Given that closed intervals have not been successfully handled in prior literature[HEK+ 08] leading to
unbounded competitive ratios, is it possible to characterize the problem more precisely? For instance,
do we run into the same issues if we allow queries to return intervals? One approach for addressing
issues with closed intervals is to output all the optimal solutions[Kah91]. It can be quite expensive to
output all the solutions. Is there an alternate framework that addresses the issues with closed intervals
without determining all the solutions.
We show that this problem is a characteristic of models that allow closed intervals in the input and
points to be returned in the queries. We extend our models to handle closed intervals by using the
notion of lexicographically smallest solution (in case multiple solutions exist). This is a natural version
in many problems where the initial ordering is important and we will show later that this has the
desired effect of limiting non-deterministic guessing powers of OP T .
Another interesting variation could be assigning cost to a query depending on the the precision of the
answer given but we have not addressed this version in this paper. There is a growing body of work that
addresses the problem of computing exact answer with minimal queries [BEE+ 06, BHKR05] and coping with
more generalized queries is an important and fundamental direction of algorithmic research.
Problem

Extremal
selection

K-selection

MST

Competitive
ratio
OP T + 1
OP T + 1
2 · OP T
2 · OP T
OP T + 1
t · OP T
OP T + k
2 · OP T
2 · (OP T + k)
2 · OP T
2 · OP T
OP T + C
2 · OP T
2 · OP T

Models

Comment

Source

OCP-P
OP-P
Category-1,2 & OP-OP
Category-3
OCP-P
CP-P
OP-P
Category-1
OP-OP
Category-3
OP-P
OP-P
Category-1,2 & OP-OP
Category-3

Report all solutions
Value

Kahan [Kah91]
this paper
this paper
this paper
Kahan [Kah91]
Khanna-Tan [KT01]
this paper
this paper
this paper
this paper
Erlebach et al.[HEK+ 08]
this paper
this paper
this paper

lex first
Report all solutions
t = clique no.
Value, ≤ k · OP T
element
Value, lex first
C ≤ OP T C = no. of red rule
lex first

Figure 2: Known results in prior literature and our new results

3

Problem Definition

We consider a problem P where we are given an instance P = (C, A) that consists of
• an ordered set of data C = {c1 , c2 , . . . , cn } called a configuration; and
4

• an ordered set of data A = {a1 , a2 , . . . an } called areas of uncertainty such that ci ∈ ai ∀i.
The configuration C is not known to us – only the areas of uncertainty, A, are known. As an example
consider the problem, P, of finding the index of the minimum element. An example instance is given by
Pex = (C, A) where C is the ordered set of points C = {3, 7, 10} and A is the ordered set of intervals (areas
of uncertainties) A = {(2, 6), (5, 8), (9, 11)}.
We focus our discussion to problems where the input is Real data. Thus, the configuration consists of
points on the Real line ℜ, and the areas of uncertainty may be intervals on the Real line. The concepts can
be extended to higher-dimensional problems.
Verifier: We are also given a verifier V for the problem P, that takes as input the areas of uncertainty,
A and returns whether a solution of the problem P can be determined from A or not. For the example
instance, Pex , described above, the verifier would return false as it cannot determine a solution from the
given areas of uncertainty. However, if the intervals were A = {(2, 5), (6, 8), (9, 11)}, then the verifier would
return true as clearly the first interval has to contain the minimum.
Order-Invariance: An important characteristic of the problems we study is that the result of the
verifier is only dependent on the ordering of the areas of uncertainty. More formally, consider two instances
P = (C, A) and P ′ = (C ′ , A′ ) where A = {a1 , a2 , . . . , an } and A′ = {a′1 , a′2 , . . . , a′n } for the same problem
P. We say that P and P ′ are order-equivalent if for every pair of indices i, j ∈ {1, 2, . . . , n}, it can be
determined that ai ≤ aj iff it can be determined that a′i ≤ a′j . We say that a problem P is order-invariant if
the verifier returns the same value for any two order-equivalent configuration instances. It is easy to verify
that the problems such as selection (finding minimum, finding k th -minimum) and minimum spanning tree
are order-invariant.
Update operations: We are allowed to perform update operations on the areas. Performing an update
operation on area ai results in knowledge of the area to a greater degree of accuracy. More precisely, performing an update operation on ai in the instance P = (C, A), where A = {a1 , a2 , . . . , ai−1 , ai , ai+1 , . . . , an } results
in another instance P ′ = (C, A′ ), where A′ = {a1 , a2 , . . . , ai−1 , a′i , ai+1 , . . . , an } such that a′i is completely
contained in ai . An important characteristic of the models that we consider is that the results of updates
on an area are independent of updates on any other area. That is, given a multi-set S = {i1 , i2 , . . . , ik } of
indices of the areas, applying updates on the corresponding areas results in the same instance, irrespective
of the sequence in which these updates are applied. We refer to this as the update independence property.
Solution: Our goal is to solve the problem P by performing minimum number of updates, i.e., perform
the minimum number of updates that result in an instance for which the verifier returns true. For a problem
instance P = (C, A), a solution, S, is defined to be a multi-set of indices {i1 , i2 , . . . , ik } such that performing
updates on the areas ai1 , ai2 , . . . , aik results in a problem instance P ′ = (C, A′ ) for which V (A′ ) returns
true, i.e., a solution of the problem can be determined from A without performing any more updates. In
this case, we say that S solves the problem instance P . Let S(P ) denote the set of all such solutions. An
optimal solution is a solution, S ∈ S(P ) such that any other solution in S(P ) has at least as many indices,
i.e., |S| ≤ |S ′ | for all solutions, S ′ ∈ S(P ). Therefore, an optimal solution corresponds to a smallest set of
indices that need to be updated in order to solve the problem.
As mentioned before, the OP-P and the CP-P models have been studied before. We shall show now
show that the algorithms for the OP-P model can be generalized for the many other models for problems
that are order-invariant. These update competitive algorithms are based on the concept of witness sets. We
discuss these concepts in Section 4; these concepts are borrowed from [BHKR05] and presented here with
modifications suitable to discuss all our models. Then we discuss how to extend these algorithms to other
models.

4

The Witness Set Framework

For a problem instance P = (C, A), a set W is said to be a witness set of P if for every solution S ∈ S(P ),
W ∩ S 6= φ. Thus, no algorithm can solve P without querying any area from W .
Suppose that we have an algorithm, WALG, that given any instance P = (V, A) of the problem, finds a
witness-set of size at most k. Then there exists a k-update competitive algorithm for the problem. The

5

algorithm is presented in Figure 3. It simply keeps applying algorithm WALG to find a witness set of size at
most k and updates all the areas in the witness set. It keeps doing this until the problem is solved.
Algorithm SOLVE( Problem Instance P , Verifier V , Witness Algorithm WALG )
Input: - problem instance P = (C, A),
- a verifier algorithm V for the given problem,
- a witness algorithm WALG for the given problem.
Output: k-update competitive solution to problem instance P
Initialize solution S = {};
If ( V (A) returns false ) /* problem instance is not yet solved */
W = WALG(P);
Update the areas in W to reduce the problem instance P to P ′ ;
S = S ∪ SOLVE(P ′ , V, WALG);
Endif;
Output S;

Figure 3: Algorithm to determine k-update competitive solution given witness algorithm
We now formally show that the solution returned by this algorithm is k-update competitive. Note that
this result is independent of the model under consideration. The witness algorithm and verifier however are
dependent on the underlying model.
Theorem 4.1. The solution returned by the algorithm in Figure 3 is k-update competitive for the problem
instance P .
Proof. See Appendix.
Witness Algorithms For Different Models. Witness algorithms have been proposed for several problems
under the OP-P model. We now show that the same witness algorithms can be used for various other models
as well.
Theorem 4.2. A witness algorithm for a problem under the OP-P model is also a witness algorithm for the
same problem under the category-1, category-2 and OP-OP models (i.e., O-O, C-C, OC-OC, OP-O, CP-C,
OCP-OC and OP-OP models).
Proof. See Appendix.
Corollary 4.3. Algorithm 3 is k-update competitive under the category-1, category-2 and OP-OP models
with the same witness algorithms as that for the OP-P model.
Proof. See Appendix.
We make an important observation here. While the reduction might seem straightforward, it is important
to note many of these reductions are only one-way reduction. For instance, we can reuse the witness algorithm
for the OP-P model for the OP-O model but not vice-versa. We demonstrate this later for the k-min selection
problem, where we show that while it is possible to design a 2-update competitive algorithm under the OP-P
model, it is not possible to design an algorithm that is better than k-update competitive under the OP-O
model using witness sets.
Another important observation we make is that prior literature has shown that no algorithm can give
bounded update complexity guarantees for the selection problem under the CP-P models. However, we
have derived constant factor update-competitive algorithms for models involving closed intervals (i.e., the
CP-C, C-C, OC-OC and OCP-OC models). This highlights the fact that the problem is not in dealing with
closed intervals but rather with the combination of allowing closed intervals in the input and simultaneously
allowing queries to return points for such closed intervals.

5

The selection problem

In an instance P = (C, A) of the k-Min problem, C = {p1 , p2 , · · · , pn } is an ordered set of points in ℜ, and
A = {a1 , a2 , · · · , an } is an ordered set of intervals on ℜ. The nature of the intervals is determined by the
6

model under consideration. The goal is to find the index of the k th smallest element in C.
We denote by lj and uj , the lower and upper ends of the interval aj respectively. To avoid overloading
of notations, we will assume that lj and uj always refer to the latest known values for the interval ranges,
considering all the updates that have already been performed.

5.1

1-Min

In this section we look at the special case when k = 1, i.e., we are interested in finding the index of the
smallest value interval.
Witness Algorithm And Verifier. We first present the witness algorithm for the OP-P model. Consider
an instance P = (C, A). The witness algorithm chooses the interval with the “smallest l-value” and the along
with the interval with the next “smallest l-value” and returns them as the witness set. The verifier simply
determines if some interval can be determined to be smaller than all the other intervals. Let S = {1..n}
denote the set of indices of the intervals. For any subset S ′ ⊆ S, we define orderl (S ′ ) to be a permutation
of indices in S ′ in increasing order of the lower values of the corresponding intervals, i.e., orderl (S ′ ) =<
j1 , j2 , · · · , jm >, such that lj1 ≤ lj2 ≤ · · · ≤ ljm . Similarly define orderu (S ′ ) =< j1 , j2 , · · · , jm >, such that
uj1 ≤ uj2 ≤ · · · ≤ ujm .
The witness algorithm and the verifier are formally presented in Figure 4.
Witness Algorithm:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. Return ap1 and ap2 as the witness set

Verifier:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. If x ≤ y for all x ∈ ap1 and y ∈ apj , j 6= 1,
return the interval with index p1 as the solution
Else return false

Figure 4: Witness Algorithm and Verifier for 1-Min under the OP-P model
Note that an interval is declared to be the smallest interval only when no other interval can contain a
smaller value. Therefore the algorithm always outputs the correct interval.
Competitiveness. We now show that the algorithm is 2-update competitive under the OP-P model.
Lemma 5.1. The set W = {p1 , p2 } returned by the algorithm of Figure 4 is a witness set for the 1-Min
problem under the OP-P model.
Proof. See Appendix.
It follows from Theorem 4.2 and Corollary 4.3 that we can derive 2-update competitive algorithms for
the category-1, category-2 and OP-OP models.
Tight Example. We now show that the update-competitive bound of 2 is tight for all the models that allow
the queries to return intervals, i.e., for the category-1, category-2 and OP-OP models (but not the OP-P
model). This is demonstrated by the following example. We are given intervals A = {a0 , a1 , a2 , . . . , an }
where a0 = (1, 5) and aj = (3, 7) for all 1 ≤ j ≤ n. We argue that any algorithm can be forced to perform
2n queries while the OPT can determine the interval containing the minimum with only n queries. Let S
represent the set of intervals A \ {a0 }, i.e., S = {a1 , a2 , . . . , an }.
Suppose that the algorithm has already performed 2n − 1 queries. The adversary behaves as follows.
For the first n − 1 queries on a0 it returns the interval (1 + iε, 5) in the ith query, where ε is a small value
< 1/(2n). For the first n − 1 queries on intervals from the set S it returns the interval (6, 7). The remaining
actions of the adversary are based on whether the algorithm performs n queries on a0 or whether it queries
n intervals from S. Note that in performing 2n − 1 queries, the algorithm must encounter one of these cases.
These are considered in the following 2 cases:
• Case 1: The algorithm makes n queries to a0 .
In this case the adversary continues to return the interval (1+iε, 5) for the ith query on a0 where i ≤ 2n−1
and it returns the interval (6, 7) for each subsequent interval queried from S. Note that in this case, on
7

performing 2n − 1 queries, the algorithm could not have queried all the intervals from S. Therefore at
the end of 2n − 1 queries, as there is overlap between interval a0 and the unqueried intervals from S, the
algorithm is forced to make 2n queries. The OPT on the other hand can just query all the intervals in
S. The adversary will return the interval (6, 7) for OPT on the remaining intervals. Thus, OPT is able
to determine that a0 contains the minimum element by just performing n queries.
• Case 2: The algorithm makes n queries to intervals in S.
In this case, the adversary returns (3, 4) for the last (nth ) interval queried in S. For any subsequent
queries to a0 , the adversary continues to return (1 + iε, 5) for the ith query. Note that in this case, the
adversary performs less than n queries on a0 . Therefore at the end of 2n − 1 queries, as there is overlap
between interval a0 and the last queried intervals from S, the algorithm is forced to make 2n queries.
The OPT on the other hand can just query all the intervals in a0 . The adversary will return the value
(2, 3) for OPT on its nth query to a0 (recall that in this case the algorithm did not perform n queries
on a0 ). Thus, OPT is able to determine that a0 contains the minimum element by just performing n
queries.
It is surprising that though this tight example demonstrates that we cannot obtain better than 2-update
competitive algorithms for these models, it is possible to obtain a 1-update competitive algorithm for the
OP-P model; however, this is obtained by an approach different from the Witness Set framework. This is
discussed in more detail in Section 6.

5.2

K-Min

We now generalize the 1-min algorithm presented above to the k th -min problem, but under the O-O model.
We later discuss issues related to handling points under the OP-P model.
Witness Algorithm And Verifier. We now present a witness algorithm and verifier for this problem
under the O-O model.
Witness Algorithm:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk−1 }
3. If x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ S \ S ′
return the witness set of 1-Min algorithm
4. Else
let < q1 , q2 , · · · , q|S ′ | > = orderu (S ′ )
return apk and aq1 as the witness set

Verifier:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk−1 }
3. If (x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ apk ) and
(x ≥ y ∀ x ∈ ai , i ∈ S \ (S ′ ∪ apk ) and ∀ y ∈ apk )
return apk
else return false

Figure 5: Witness and Verifier Algorithm for K-Min under the O-O model
We say intervals ai and aj are disjoint if ∀x ∈ ai , y ∈ aj , x ≤ y or vice-verse. The witness algorithm
checks if the first k − 1 interval are disjoint with the last n − k + 1 interval. If that is the case, it returns
the witness set of the 1-Min algorithm. Else it chooses apk and an interval from S ′ with largest u value(aq1 )
as the witness set.
The verif ier takes the first k − 1 intervals(S ′ ) depending on their l values. The verif ier checks if these
k − 1 intervals are disjoint from the apk . Then it takes the last n − k intervals(S \ (S ′ ∪ apk )) and checks if
all of them disjoint with apk . If both the condition holds, it returns apk else it returns false.
Competitiveness. We now show that the algorithm is 2-update competitive for the O-O model. It follows
using proofs similar to Theorem 4.2 and Corollary 4.3 that we can derive 2-update competitive algorithms
for the other category-1 models.
Lemma 5.2. The witness set W returned by the algorithm of Figure 5 is a witness set for the k-Min problem
under the O-O model.
Proof. See Appendix.

8

Tight Example. It is not difficult to construct examples similar to that discussed for the 1-Min algorithm
to show that the update-competitive bound of 2 is tight under the category-1 models.
It is interesting to note here that while a 2-update competitive algorithm can be designed for the k-min
problem under the category-1 models, no algorithm can be better than k-update competitive for this problem
under models that allow points, i.e., the category-2 and OP-P models. This is illustrated by the following
example3 . Suppose we have 2k areas of which k are open intervals of the form (0, 5) and k are fixed points
of the value 3. For the first k − 1 intervals queried by any algorithm, the adversary returns 1 and for the
k th interval, the adversary returns 4 (or interval (3.5,4.5) as the case may be), thereby forcing k queries.
However, OPT only needs to update the interval with value 4 and can thereafter return any of the k fixed
points of value 3 as the k th smallest.
However, in the next section we show that it is possible to design algorithms for the k-Min problem
under these models that allow for points, obtaining update competitive bounds with additive factor k (i.e.,
the algorithm performs k more updates than OPT). This however is achieved by bypassing the Witness set
framework.

6

Bypassing the Witness Set framework

While the witness set framework, studied in prior literature, provides a general method for solving problems
with data uncertainty under the update complexity models, it has its limitations. We demonstrate this
by presenting algorithms that require to perform only k more queries than OPT for the k th -Min selection
problem. Note that, for the 1-Min problem this implies a 1-update competitive algorithm, as only one query
more than OPT is required to be performed.

6.1

1-Min

Consider the following algorithm. We note here that the set of intervals returned by the “witness” algorithm
‘‘Witness’’ Algorithm:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. Let A = {ap1 } and B = {p2 , · · · , p|S | }
3. Return interval in A.

Verifier:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. If x ≤ y for all x ∈ ap1 and y ∈ apj , j 6= 1,
return the interval with index p1 as
the solution
Else return false

Figure 6: “Witness” Algorithm and Verifier for 1-Min under the OP-P model
is not a true witness set. However, we stick to the terminology for the sake of consistency. The algorithm
remains the same, it updates the intervals returned by the “witness” algorithm until we obtain a solution.
Lemma 6.1. Let cOP T be the total number of queries made by OPT to find 1-Min, then total number of
queries made by algorithm in Figure 6 is at most cOP T + 1 in the OP-P model.
Proof. See Appendix.
Note that this simple algorithm for 1-Min in OP-P model fails for the OP-O model. Consider the following
example. Let there be two intervals I1 = (2,20) and I2 = (19,21) Suppose at the ith query of I1 , we get a
new interval (di , 20), where di < 19, so I1 and I2 will always intersect if we just query I1 . The algorithm
in Figure 6 always queries I1 , so it takes huge number of queries to find 1-Min. But if we just query I2 , it
returns a subinterval (20.5,21). This is what OPT does and uses just one query to find the answer.

6.2

k-Min

Consider the algorithm in Figure 7 for k-selection in the OP-P model which generalizes the result of the
algorithm in Figure 6.
3 This

was pointed out by an anonymous reviewer of a previous version

9

‘‘Witness’’ Algorithm:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk }
3. let < q1 , q2 , · · · , qk > = orderu (S ′ )
′
′
.
Let Smax
= aqk . Query Smax
4. If x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ S \ S ′
return the “witness set” of the
1-Max algorithm of S ′ (of Figure 4).

Verifier:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk−1 }
3. If (x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ apk ) and
(x ≥ y ∀ x ∈ ai , i ∈ S \ (S ′ ∪ apk ) and ∀ y ∈ apk )
return apk
else return false

Figure 7: Witness and Verifier Algorithm for K-Min under the OP-P model
Lemma 6.2. The algorithm of Figure 7 uses atmost cOP T + min{k, n − k} queries where cOP T is the
minimum number of queries required by the OPT.
Proof. See Appendix.
Now let us consider the OP-OP model. Note that since we have 2 · OP T algorithms for the OP-O model
and an OP T +k algorithm for the OP-P model, we can derive a 2·(OP T +k) algorithm for the OP-OP model
by combining these 2 algorithms. This is done by alternating the witness algorithms of the two models. This
ensures that we only need to perform at most twice the number of queries performed by the algorithms of
either of the two models.

7

Closed intervals with point returning queries

As discussed above, the competitive ratio is unbounded for the special cases where the input allows for closed
intervals and queries may return points (i.e., the category-3 models). For instance consider the problem of
finding the index of the minimum element. Further, consider the problem instance P = (C, A) where
ai = [1, 3] for all 1 ≤ i ≤ n. The adversary in this case acts as follows; for each of our queries except the last,
it returns 2. Finally, for our last query, say on interval ak , it returns 1. On the other hand, OPT directly
queries interval ak and obtains the optimal solution. This results in an unbounded competitive ratio.
The primary reason for this anomaly is the possibility of existence of multiple optimal solutions. In such
cases, the adversary is able to get away with few queries by just querying the necessary intervals that reveal
one of the optimal solutions. For any algorithm on the other hand, it is not able to distinguish from the areas
of uncertainty (as shown above) which are the necessary intervals to query to reveal the optimal solution.
One of the ways that has been suggested in prior literature to deal with this special case is to require all
the optimal solutions to be output. However, it can be quite expensive to output all these solutions. This
raises the question of whether other reasonable conditions can be laid on the structure of the required output
that are not so expensive but reasonable. We now consider such a condition, which we call the lexicographic
condition, for which we show that this special can be handled. Recall that the sets C and A that define
a problem instance are ordered sets. Thus, the set of indices that define a solution can be considered as a
string (called solution string) defined as follows: the length of the string is n and the ith element of the string
is set to 1 if it defines the solution and 0 otherwise. In the lexicographic setting, amongst all the optimal
solutions, we are interested in finding the solution for which the solution string has the smallest lexicographic
ordering.
Now consider again the example above. Note that, even though OPT queries ak and determines a solution
with optimal solution value, it cannot terminate without making further queries as it cannot decide whether
or not there exists another solution with the same value but a smaller lexicographic ordering.
We note that new witness algorithms may require to be developed for the lexicographic variants of the
problems. However, we show by case of examples that these are not very different from the corresponding
witness algorithms for the original problems.
It can be shown that once a witness algorithm is developed for a lexicographic variant of the problem
under the CP-P model, the same witness algorithm can be extended to other models along the same lines
as discussed in Section 4.

10

Now let us consider the lexicographic variant of the 1-Min problem. In order to obtain the witness algorithm for the lexicographic variant for the category-3 models, the notion of ordering of intervals, orderl (.),
needs to be extended to incorporate lexicographic ordering and closed intervals. As before, for any subset
S ′ ⊆ S, we define orderl (S ′ ) to be a permutation of indices in S ′ in increasing order of the lower values
of the corresponding intervals, i.e., orderl (S ′ ) =< j1 , j2 , · · · , jm >, such that lj1 ≤ lj2 ≤ · · · ≤ ljm . When
comparing two intervals with the same l-values, say lj and lj ′ , ties are resolved as follows: If aj contains a
point x such that x < y for all y ∈ aj ′ , then j precedes j ′ in the ordering; similarly if aj ′ contains such a
point, then j ′ precedes j; and if neither can be established, then the lexicographically smaller index precedes
the larger one in the ordering. Thus, if one of the intervals, say aj , is open from the left and another interval,
say aj ′ , is either closed from the left or a point, then j ′ precedes j in the ordering; in all other cases, the
lexicographic smaller of j and j ′ precedes the other in the ordering.
The witness algorithm and verifier are formally presented in Figure 8. Note that the verifier is also
modified so that it can check that the minimum interval can be determined or not based on the lexicographic
ordering.
Witness Algorithm:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. Return ap1 and ap2 as the witness set

Verifier:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. If (x ≤ y ∀ x ∈ ap1 and y ∈ apj , pj > p1 ) and
(x < y ∀ x ∈ ap1 and y ∈ apj , pj < p1 ),
return the interval with index p1 as the solution
Else return false

Figure 8: Witness Algorithm for 1-Min under the CP-P model
The proof of update competitiveness is similar to the case for the original problem.
Lemma 7.1. The set W = {p1 , p2 } returned by the algorithm of Figure 8 is a witness set for the lexicographic
1-Min problem under the CP-P model.
Proof. See Appendix.
The fact that no algorithm can be better than 2-update competitive for the 1-Min problem under the
CP-P model follows from the same reasoning as for the OP-P model.
We can extend this 2-update competitive algorithm for the other category-3 models using techniques
similar to that in Section 4.
Finally, we can design 2-update competitive algorithms for the k-min version as well under these models
by using similar techniques.

8

Minimum Spanning Tree

In the Lexicographic MST problem, we are given a graph G = (V, E). The edge lengths are specified with
uncertainty. Let E = {e1 , e2 , . . . , en } be the ordered set of edges. Then the ordered set C = {v1 , v2 , · · · , vn }
denotes the values of the edge lengths and the ordered set A = {a1 , a2 , · · · , an } denotes the intervals within
which the edge lengths are known to lie. The goal is to find the lexicographically smallest MST under the
category-3 models.
A 2-update competitive algorithm for the MST problem was given by [HEK+ 08] under the OP-P model.
By applying Theorems 4.2 and Corollary 4.3, we conclude that it is 2-update competitive for the Category-1,2
and OP-OP models as well. The Lexicographic MST problem can be solved under the Category-3 models
with few changes to the algorithm described in [HEK+ 08] (these changes are outlined in Appendix A). This
gives us the following result.
Theorem 8.1. There exists a 2-update competitive algorithm for the Lexicographic MST problem under the
Category-3 models.

11

Remark: It may be noted that the algorithm described in [HEK+ 08] in conjunction with Lemma 6.1 can be
used to derive an OP T + C update competitive algorithm for the MST problem under the OP-OP model
where C is the number of red-rules applied by the optimal algorithm. Note that C can be much less than
OP T .

9

Conclusion

We extended the one-shot query model to the more general situation where a query can return arbitrary subintervals as answers and established strong relationships between these models. Many of the previous results
in the restricted model can be generalized based on this relationship that simplifies the task of designing
algorithms for the more general model. This is far from obvious as the sub-interval query model presents
some obvious challenges because the uncertainty (in the values of any parameter) can take an arbitrary
number of steps to be resolved and can be controlled by an adversary. One drawback of this approach is
that the actual algorithmic complexity is overlooked and we only focus on the competitive ratio which is
justified on the basis of very high cost of a query. For future work, the algorithmic complexity needs to be
incorporated in a meaningful way.

References
[AH04]

Ionut D. Aron and Pascal Van Hentenryck. On the complexity of the robust spanning tree problem with
interval data. Oper. Res. Lett., 32(1):36–40, 2004.

[AY09]

Charu C. Aggarwal and Philip S. Yu. A survey of uncertain data algorithms and applications. IEEE
Trans. Knowl. Data Eng., 21(5):609–623, 2009.

[BEE+ 06] Zuzana Beerliova, Felix Eberhard, Thomas Erlebach, Alexander Hall, Michael Hoffmann 0002, Matús
Mihalák, and L. Shankar Ram. Network discovery and verification. IEEE Journal on Selected Areas in
Communications, 24(12):2168–2181, 2006.
[BHKR05] Richard Bruce, Michael Hoffmann, Danny Krizanc, and Rajeev Raman. Efficient update strategies for
geometric computing with uncertainty. Theory Comput. Syst., 38(4):411–423, 2005.
[FMO+ 03] Tomás Feder, Rajeev Motwani, Liadan O’Callaghan, Chris Olston, and Rina Panigrahy. Computing
shortest paths with uncertainty. In STACS, pages 367–378, 2003.
[FMP+ 03] Tomás Feder, Rajeev Motwani, Rina Panigrahy, Chris Olston, and Jennifer Widom. Computing the
median with uncertainty. SIAM J. Comput., 32(2):538–547, 2003.
[GGM06]

Ashish Goel, Sudipto Guha, and Kamesh Munagala. Asking the right questions: model-driven optimization using probes. In PODS, pages 203–212, 2006.

[GM07]

Sudipto Guha and Kamesh Munagala. Model-driven optimization using adaptive probes. In SODA, pages
308–317, 2007.

[HEK+ 08] Michael Hoffmann, Thomas Erlebach, Danny Krizanc, Matús Mihalák, and Rajeev Raman. Computing
minimum spanning trees with uncertainty. In STACS, pages 277–288, 2008.
[Kah91]

Simon Kahan. A model for data in motion. In STOC, pages 267–277, 1991.

[KT01]

Sanjeev Khanna and Wang Chiew Tan. On computing functions with uncertainty. In PODS, pages
171–182, 2001.

[KZ06]

A. Kasperski and P. Zielenski. An approximation algorithm for interval data minmax regret combinatorial
optimization problem. Information Processing Letters, 97(5):177–180, 2006.

[OW00]

Chris Olston and Jennifer Widom. Offering a precision-performance tradeoff for aggregation queries over
replicated data. In VLDB, pages 144–155, 2000.

Appendix A. Sketch of changes for Lexicographic MST
The following changes are required to the algorithm of [HEK+ 08]. Here we use the notation Ux for ux and
Lx for lx to remain consistent with [HEK+ 08].
12

1. The main change involves modifying the comparison operator. We modify the comparison operator
defined on the intervals as follows: Let x be the l-value or u-value of some interval, i.e., x = le or
x = ue for some interval e. Similarly, let y be the l-value or u-value of some interval, i.e., y = lf or
y = uf for some interval f 6= e. We say that x ≺ y if x < y or x = y and e is lexicographically smaller
than f .
2. An edge e of a cycle C is said to be always maximal if Uc ≺ Le for all c ∈ C − {e}. Note that the only
change introduced in this definition is in replacing the comparison operator.
3. We similarly modify the notion of comparing two edges e and f based on the comparison operator
as follows. We say that e ≺ f if Le ≺ Lf . While indexing the edges in the algorithm, the edges are
considered in the order defined by ≺ above.
4. The witness set is determined as follows. Once a cycle C is detected, if it contains an always maximal
edge, that edge is deleted. Otherwise let f ∈ C such that Uf = max{Uc |c ∈ C} where max is based on
the new ≺ operator. Further let g ∈ C − {f } such that Lf ≺ Ug . Then f and g form the witness set.
Theorem 8.1 can be proved with these changes along the same lines as presented in [HEK+ 08].

Appendix B. Proofs for the Witness Set Framework
9.1

Proof of Theorem 4.1

We first prove a claim that will be required in the proof of the above result.
Claim 9.1. Suppose that we are given a problem instance P = (C, A). Further, suppose that we know that
an optimal solution, So for P contains an index i, i.e., So queries the area ai . Let P ′ = (C, A′ ) be the
problem instance reduced from P on querying area ai . Then So′ = So \ {i} is an optimal solution for P ′ .
Here the operation \ on the multiset So removes only one instance of i from it in case there are multiple
instances.
Proof. See Appendix.
Proof. Recall that the update independence property implies that irrespective of the order in which the
updates are applied, applying all the updates in So solves the problem P . Therefore, clearly So′ solves the
problem instance P ′ . In order to argue that this is an optimal solution, all we need to show is that there
does not exist a solution of smaller size. Suppose otherwise. Then there exists a solution S of size smaller
than So′ that solves P ′ . But then, S ′ = S ∪ {i} solves P which contradicts the fact that So is an optimal
solution of P .
We now present the proof of Theorem 4.1.
Proof. The proof is by induction on the size of an optimal solution on instance P . For the base case, consider
a problem instance P for which any optimal solution has size 1. Let W be a witness set returned by algorithm
WALG. Clearly, W is k-update competitive by definition.
Now suppose that the claim holds for any problem instance P having optimum solution of size i or less.
Consider a problem instance P for which any optimum solution has size i + 1. Let W be a witness set of size
≤ k returned by WALG. Let the instance P be reduced to instance P ′ on applying updates on the areas in W .
By Claim 9.1, any optimal solution on P ′ has size ≤ i. By induction, the algorithm determines a k-update
competitive solution S ′ for P ′ . Hence |S ′ ∪ W | ≤ k(i + 1), and thus S ′ ∪ W is a k-update competitive solution
for P .

13

9.2

Proof of Theorem 4.2

Proof. We formally prove this for the CP-C model. The proofs for the other models follow similarly; we
point out the changes required.
Let P be any instance of the given problem under the CP-C model. Let P ′ be obtained from P by
modifying the configuration and areas of uncertainty as follows; (i) All the closed intervals are replaced with
open intervals; and (ii) The configuration is suitably modified in order to ensure that the configuration points
are always contained in the corresponding areas of uncertainty – this is explained in more detail later. Let
W be any witness set for P ′ under the OP-P model. We need to show that W is also a witness set for P
under the CP-C model. Suppose this is not so, i.e., W is not a witness set for P under the CP-C model.
We will then argue that there exists a set of queries excluding W that when applied to P ′ under the OP-P
model can result in an instance for which the verifier returns true; this implies that W is not a witness set
for P ′ under the OP-P model leading to a contradiction. Hence our supposition is incorrect and W must be
a witness set for P as well.
It remains to find a possible set of queries excluding W and query outcomes that when applied to
P ′ under the OP-P model results in an instance for which the verifier returns true under the assumption
that W is not a witness set for P under the CP-C model. Considering this assumption, there exists a
solution S = {i1 , i2 , . . . , ik } for P under the CP-C model that does not contain the index for any area
in W . Let P1 , P 2, . . . , Pk be the sequence of instances obtained on applying the updates in S where Pt
is obtained from Pt−1 on applying the update on ait for 1 ≤ t ≤ k. For any interval (not a point) aj
in Pk (the final configuration in the sequence above) let lj , uj denote the interval end points. Let ε =
min{uj − lj |aj aj is an interval in Pk }, i.e., ε is the minimum length of any interval in Pk .
As mentioned earlier, the configuration points in P ′ are also suitably modified in order to ensure that
they are always contained in the corresponding areas of uncertainty. This is done by setting a configuration
point cj to cj + ε/10 if cj = lj in Pk , and setting it to cj − ε/10 if cj = uj in Pk . This ensures that no
configuration point coincides with the interval end-points; this will allow us to replace closed intervals with
open intervals. Moreover, the modified configuration points are consistent with all the query outputs.
Now, consider the case where the same sequence of updates in S is applied to P ′ under the OP-P model.
A possible sequence of outcomes is P1′ , P2′ , . . . , Pk′ wherein Pt′ is the same as Pt with all closed intervals
′
on applying the update on a′it for 1 ≤ t < k). Now
replaced by open intervals (Pt′ is obtained from Pt−1
note that since S solves P , the verifier returns true for Pk . However, Pk and Pk′ are order-equivalent and
since the problem is order-invariant, the verifier must return true for Pk′ under the OP-P model as well. This
implies that W is not a witness set for P ′ under the OP-P model leading to the required contradiction.
We can similarly show that the witness set for the OP-P model can be reused for a variety of other
models, thereby resulting in comparable update-competitive algorithms. The proofs are similar to that of
Theorem 4.2 above; the only difference is in the way the the instances P1′ , . . . , Pk′ for the OP-P model are
constructed from the instances P1 , . . . , Pk for the new model.
For the C-C model, OC-OC model and the OCP-OC model, the instance Pt′ is obtained from instance
Pt by replacing the closed intervals in Pt with corresponding open intervals and modifying the configuration
points as described in the proof above.
For the O-O model, and the OP-O model, the instance Pt′ is obtained from instance Pt by replacing the
intervals in Pt corresponding to the areas of uncertainty having indices in the set {i1 , i2 , . . . , it } with the
corresponding configuration points ci1 , ci2 , . . . , cit . Note that in this case, it does not make sense to query
the interval on the same index more than once, therefore the number of queries can be reduced.
This completes the proof of the Theorem.

Proof of Corollary 4.3
Proof. Consider the CP-C model. By Theorem 4.2, we know that the witness algorithm for the OP-P model
is also a witness algorithm for the CP-C model. Moreover, the verifier for the OP-P model is also a verifier
for the CP-C model as the problem considered is order-invariant. The proof for the other models follows
similarly.

14

Appendix C. Proofs for the Selection Problem
Proof of Lemma 5.1
Proof. The proof is by contradiction. Suppose OPT updates neither ap1 nor ap2 . Let the index of the interval
returned by OPT as the answer be aq . We consider the 2 cases:
• aq = ap1 : As the witness algorithm is invoked only when the verifier returns false, by examining the
condition in Step 2 of the verifier (which must have failed for the current instance), we conclude that ∃
x ∈ ap1 and y ∈ apj , j 6= 1 such that y < x. Thus OPT has not fully demonstrated that ap1 contains the
point which is minimum as ap2 could be made to contain the minimum point.
• aq 6= ap1 : By the definition of orderl (.) applied in Step 1 of the witness algorithm and by examining the
condition in Step 2 of the verifier, we conclude that lp1 ≤ lpq . Thus, OPT has not demonstrated that apq
contains the point which is minimum as ap1 could be made to contain the minimum point.

Proof of Lemma 5.2
Proof. The proof is again by contradiction. There are two cases:
• The witness set returned is the witness set of the 1-Min Algorithm, W = {apk , apk+1 }:
Since the first k − 1 intervals are disjoint with the rest of the intervals, the problem of finding the k th
minimum interval becomes the problem of finding 1-Min in S \ S ′ . Using Lemma 6.1, W is a valid witness
set.
• W = {apk , aq1 }:
Suppose OPT updates neither apk nor aq1 . Let the index returned by OPT be aj . So aj has to be disjoint
with all the other intervals. Since the witness algorithm was called only because the verifier returned
false, so by examining the condition of step 3 of the verifier, we infer that ∃api with 1 ≤ i ≤ k − 1 such
that ∃x ∈ api , y ∈ apk for which x > y. So api and apk are not disjoint. If such api exists, then by the
definition of aq1 , we see that aq1 and apk are also not disjoint. So the solution returned by OPT cannot
be apk and aq1 as they both are not disjoint. As aj must be disjoint, we consider following cases:
– uj ≤ lpk and uj ≤ lq1 : Initially there were less than k − 2 intervals with l values ≤ lq1 . Since aq1 is
not updated, any update of other intervals cannot increase the number of intervals with l values ≤
lq1 . Since uj ≤ lpk , the number of intervals with l values ≤ lj is less than k − 3. So aj cannot be
the k th minimum interval.
– lj ≥ upk and lj ≥ uq1 : Initially there are k − 2 intervals with u values ≤ uq1 . Since aq1 is not
updated, any update of other intervals is not going to decrease the number of such intervals. These
intervals together with q1 and pk have u values ≤ lj . So there are k intervals with u values ≤ lj . So
aj cannot be the k th minimum interval.

Appendix D. Proofs for Bypassing the Witness set Framework
Proof of Lemma 6.1
Proof. Assume for contradiction that we have queried cOP T + j intervals where j ≥ 2. Let a1 and a2 be
any two intervals that algorithm in Figure 6 has queried but OPT has not queried such that l1 ≤ l2 . Since
OPT did not query a1 , we conclude that a1 is the interval which contains the minimum. Also since the
algorithm in Figure 6 queried a2 , ∃ x ∈ a1 and y ∈ a2 such that y < x. But we have assumed that OPT
does not query a2 , so OPT cannot demonstrate that a1 contains the point which is minimum. So we get a
contradiction.
15

Proof of Lemma 6.2
Proof. Let 1 < k ≤ n − k - the other case can be argued similarly and k = 1 is addressed by the algorithm in
′
′
′
Figure 6. If Smax
is not queried by OPT then Smax
has rank ≤ k. Smax
cannot have rank > k by definition
′
′
′
of S . Indeed, if Smax has rank > k, then there must be at least k points to the left of Smax
that violates
′
′
the definition of S . If Smax has rank ≤ k then atmost k − 1 such intervals can remain unqueried, otherwise
′
the rank of the element returned cannot provably be k. (If Smax
has rank = k, then the OPT must query
all except one, which is ≤ k − 1 for k > 1). For the second phase, to find out the maximum among S ′ , the
algorithm of Figure 6 needs at most cmax
OP T + 1 queries. So, overall, our algorithm makes at most k − 1 + 1 = k
queries more than the OPT.

Appendix E. Proofs for Closed intervals with point returning queries
Proof of Lemma 7.1
Proof. The proof is again by contradiction. Suppose OPT updates neither ap1 nor ap2 . Let the index of the
interval returned by OPT as the answer be aq . We consider the 2 cases:
• aq = ap1 : As the witness algorithm is invoked only when the verifier returns false, by examining the
condition in Step 2 of the verifier (which must have failed for the current instance), we conclude that
either (i) ∃ x ∈ ap1 and y ∈ apj , j 6= 1 such that y < x; or (ii) ∃ x ∈ ap1 and y ∈ apj such that y = x
and p2 < p1 . In either case, we observe that OPT has not fully demonstrated that ap1 contains the point
which is minimum as ap2 could be made to contain the minimum point.
• aq 6= ap1 : By the definition of orderl (.) applied in Step 1 of the witness algorithm and examining the
condition in Step 2 of the verifier, we conclude that lp1 ≤ lpq . Thus, OPT has not demonstrated that apq
contains the point which is minimum as ap1 could be made to contain the minimum point.

16

A LANGUAGE-BASED GENERATIVE MODEL FRAMEWORK FOR BEHAVIORAL ANALYSIS OF COUPLES' THERAPY Sandeep Nallan Chakravarthula1 , Rahul Gupta1 , Brian Baucom2 , Panayiotis Georgiou1
2

University of Southern California, Los Angeles, CA, USA The University of Utah, Department of Psychology, UT, USA
This work is the first step towards the creation of a more realistic model of human interaction that will try to address the following: (1) Human annotators employ a range of information to reach an integrated conclusion as to the behavior of interlocutors. While in past work we assumed that all observations (say talk-turns) belonged to the same class as the overall rating, we will expand our models to allow for the human subject to move through a range of behavioral states. We hypothesize that this more realistic model can both (a) provide more accurate overall judgment and (b) higher-resolution information about what happens in the interaction. (2) Humans do not integrate information in a linear manner. The saying "first impression counts" is an indicator of this non-linear process. This phenomenon is also studied extensively in the psychology literature. For example [25] investigates recency (thing observed last counts more) and primacy (thing observed first counts more). Integrating information requires an in-depth investigation of the algorithmic metrics of behavior (say log likelihood of a negative statement) versus the impact this has on the coding process. In our recent work we briefly touched on this [26] in investigating how human annotators employ isolated saliency or causal integration to make their decisions. Through the higher-resolution information achieved through (1) we can evaluate non-linear techniques in fusing local decisions for reaching a global (session) decision. This paper provides a proof of concept of the above by employing a 2-state model described below as Dynamic Behavioral Model (DBM) and contrasting it with the Static Behavior Model (SBM). The SBM works as our baseline assuming a constant behavioral state throughout interaction. A DBM allows for transitions between behavioral states within a session, and learns behavior representations from these transitions. Comparison of these models helps provide evaluation results for the goal (1a) in this work while we are working with our psychology partners to evaluate (1b). In order to evaluate fusion of turn-level decisions as stated in goal (2), we evaluate methods such as model log-likelihood comparison and comparing distribution of behavioral state occupancies. This paper is organized as follows: Section 2 provides a description of the database. We describe the SBM and DBMs in detail in Section 3, followed by the training methodology in section 4. The evaluation of our behavioral models and their results are detailed in Section 5, after which we discuss the results in Section 6. Finally, we present our conclusions in Section 7. 2. DATABASE We use the data of 134 couples from the UCLA/UW Couple Therapy Research Project [27]. The corpus consists of audio and video, recorded during sessions of real couples interacting, and the session transcripts. During a session, the husband and wife converse about a pre-decided topic (e.g. "why can't you leave my stuff alone ?") for a certain duration of time. Based on the Couples Interaction Rating System [1] and Social Support Interaction Rating System [2], each participant is later rated by annotators for a set of 33 behavioral codes including "Acceptance", "Blame", "Positivity" and "Negativity". Annotators provide subjective ratings on a Likert scale of 1-9,

1

ABSTRACT Observational studies for psychological evaluations rely on careful assessment of multiple behavioral cues. Recent studies have made good progress in automating the psychological evaluation, which often involved tedious manual annotation of a set of behavioral codes. However, the current methods impose strict and often unnatural assumptions for evaluation. In this work, we specifically investigate two goals: (1) Human behavior changes throughout an interaction and better models of this evolution can improve automated behavioral annotation and (2) Human perception of this evolution can be quite complex and non-linear and better techniques than averaging need to be investigated. For this purpose, we propose a Dynamic Behavior Modeling (DBM) scheme, which models a spouse as undergoing changes in behavioral state within a session, and contrast it against a Static Behavior Model (SBM) which allows only a constant session-long behavioral state. We use Negativity in a couples therapy task as our case study. We present results and analysis on both models for capturing the local behavior information and predicting the session level negativity label. Index Terms-- Behavioral Signal Processing, Static Behavioral Models (SBM), Dynamic Behavioral Model (DBM), Hard Expectation Maximization algorithms. 1. INTRODUCTION Observational studies are the basis of most psychological evaluations and they rely on careful observation and assessment of social, affective, and communicative behavioral cues. In family studies research and practice, our topic of interest, psychologists rely on a variety of established coding standards [13], with the aim of producing accurate, consistent ratings of human behavior by human annotators. This manual coding is a costly and time consuming process. First, a detailed coding manual must be created, which often requires several design iterations. Then, multiple coders, each of whom has his/her own biases and limitations, must be trained in a consistent manner. The process is mentally straining and the resulting human agreement is often quite low [4]. Great strides have been made over the last few years on establishing the ability of Signal Processing and Natural Language Processing methods as viable in estimating the behavioral state of the interlocutors. Overview papers [5,6] describe some of those advances. In couples therapy, example work includes [716], while similar work exists in other domains such as motivational interview therapy for addiction and autism [1721]. These methods however suffer from the simplistic assumption of estimating a single behavioral state from the whole interaction: the inherent underlying assumption is that the human is a behavioral state generator and he/she generates from the same state for the entire duration. On the other hand, previous work has also focused on dynamically modeling the behavioral constructs such as emotions [22, 23] and engagement [24]. Whereas these capture the evolution of behavioral states, they fail to provide a link to an overall perception.
This work was supported by the National Science Foundation (NSF).

978-1-4673-6997-8/15/$31.00 2015 IEEE

2090

ICASSP 2015

Utterance Utterance Utterance Utterance Utterance Static Behavior Model

Utterance Utterance Utterance Utterance Utterance Dynamic Behavior Model

For class Ci = {C0 , C1 }: Ci = = arg max
Cj

P ( U |C j ) P ( C j ) P (U )

(1) (2)

arg max P (U |Cj )P (Cj )
Cj

Fig. 1: Conceptualizing the proposed graphical model on the right versus the baseline on the left

Since our dataset is balanced (70 files per class), the class prior probabilities are equal, i.e. P (C0 )=P (C1 ). Therefore, we re-write the decision as: Ci = arg max P (U |Cj ) (3)
Cj

where 1 indicates absence of the behavior and 9 implies a strong presence. The sessions are rated by 2-12 annotators with majority of the sessions ( 90%) rated by 3-4 annotators. For our task, we analyze the `Negativity' behavioral code. In order to simplify the code learning, we only use sessions with mean annotator ratings in the top 20% (`High Negativity') and bottom 20% (`Low Negativity') of the code range and we binarize into C1 and C0 respectively. In this manner, we pose the learning problem as a binary classification one, as was also done in our earlier work [16]. Table 1 lists the statistics on the chosen set of data and we describe our modeling schemes in the next section. For more information, the reader can refer to [1, 2, 10]

Husband Class Label Code Range No. of Files C0 1.00-1.75 70 C1 5.33-9.00 70

Wife C0 1.00-2.00 70 C1 6.33-9.00 70

Equation 3 represents the final decision scheme for the SBM. The training and testing methodologies for this model are discussed in Sections 4.1 and 5.1 respectively. 3.2. Dynamic Behavior Models Dynamic Behavior Models (DBMs) allow for a person's behavior to change over time. This is modeled in the form of transitions between different behavioral states throughout a session, shown in Fig. 1 (right). In our work we will simplify the model to have 2 states and we assume that behavior remains short-term stationary (i.e. behavior does not change within an utterance, but only from one utterance to another). We will denote utterance states as Si = {S0 , S1 }. The behavioral state of the interlocutor, labeled by the human annotators as Low/High Negativity or C0 /C1 , does not provide a one-to-one correspondence any more. A person that is very negative (C1 ) can generate from both states (S0 , S1 ). The definition of the states S0 , S1 will be described in detail in Sec. 4.2. Given only one turn, similar to the formulation of the SBM, a ML model will result in: P (Si |U (m)) = P (U (m)|Si )P (Si ) (4)

Table 1: Data Demographics on 20% least/most negativity sessions

3. BEHAVIORAL MODELING Existing work [12] has assumed that an interlocutor is constantly in the same behavioral state. This is equivalent to modeling each interlocutor in the interaction as a single state generative model as shown in Fig. 1 (left). This is clearly limiting and does not reflect human behavior, that dynamically adapts based on the various stimuli, internal and external. Below we present two models: the Static Behavior Model in Sec. 3.1 as our baseline and similar to the one in [12], and the Dynamic Behavior Model in Sec. 3.2 that allows for transition between two behavioral states throughout the interaction and thus makes turn-level decisions instead of session level decisions. 3.1. Static Behavior Models In the Static Behavior Modeling (SBM) framework, a person's behavior is assumed to remain the same throughout the interaction, irrespective of external stimuli such as spouse's utterances, topic being discussed, etc. This is the same model we had proposed in [12] and in effect corresponds to behavioral averaging. Thus, all the utterances observed in that session are generated from the same behavioral state as in Fig. 1(left). Identifying the behavioral state of the interlocutor in this case is equivalent to identifying the class label Ci = {C0 , C1 } from the whole transcript. In this work we employ a Maximum Likelihood (ML) formulation for the binary classification. For a set of observed utterances, corresponding to the whole transcript, U ={U (1), . . . , U (M )}, we want to find: P (Code Low-Negatity or High-Negativity|U ) = P (C0 or C1 |U )

This method estimates turn-level state probabilities. Human coders integrate behavioral information and give a summative opinion on the session level. In order to reach inference from the DBM for session level behavioral descriptors, we need to also integrate such turn-level behavioral information. Below we provide two fusion methods for behavioral integration. 3.3. Fusion Methods for DBMs There are many perception inspired methods for behavioral integration. For instance in our work [26] we evaluated whether we could judge global behavior based on a locally isolated, yet highly informative event or whether integrating information over time was more effective. The premise of such work is that the human perception process is capable of integrating local events to generate an overall impression at the global level, but this process is not transparent and as such is difficult to replicate. What we can instead do is use local information to derive the same global decisions. In this work we will employ two fusion methods for the DBM: Activation-based and Likelihood-based. 3.3.1. Activation-based In the Activation-based DBM (ADBM), shown in Fig. 2 (left), local, state-label decisions are made for Si as shown below, based on (4) Si = arg max P (U (m)|Sj )P (Sj )
Sj

(5)

This fusion method uses a majority vote assumption, where the behavioral state which generates the largest proportion of utterances determines the session label decision. From the training data we can learn the mapping of Si  Cj ,  i, j  {0, 1} as explained in Sec.4.3.1

2091

Activation-Based Dynamic Behavior Model Utterance Utterance S1 Utterance Utterance Utterance Turn level decisions. Majority voting S1. Thus C1 selected due to association learned on training data C0
S1

Likelihood-based Dynamic Behavior Model Utterance Utterance Utterance Utterance Utterance Utterance Utterance Utterance Utterance Utterance C1 Best path from decoding from C0 model therefore C0 behavior selected (even if more S1 states in selected path)
S0

S0

S0

S1

Fig. 2: Activation-based DBM (left) versus the Likelihood-based DBM (right)

3.3.2. Likelihood-based In the Likelihood-based DBM (LDBM) we employ a Hidden Markov Model (HMM) representation of the behavioral classes C0 and C1 . This model assumes that an interaction is comprised of utterances U ={U (1), ..., U (M )} which are observations emitted by the hidden state sequence S ={S (1), ..., S (M )}, where S (i)  {S0 , S1 }, i  {1, ..., M }. It is also assumed that both classes C0 and C1 can produce both states, albeit with different likelihood, as shown in Fig. 2 (right). In human terms, negative persons are likely to remain mostly in negative states and express themselves negatively, although it is sometimes likely that they will express a positive attitude. In order to identify the most likely behavioral class, we decode U based on the HMMs of C0 and C1 and chose the model that is more likely to have generated it from the state sequence S . Ci = arg max P (S j |U , j ,  )
j

(6)

Where j is the HMM state transition matrix of Cj  is the initial state probability vector S j is state sequence decoded by HMM of Cj for U 4. BEHAVIOR MODEL TRAINING In this section, we describe the training procedure for the SBM and the two DBMs discussed in the previous section. For our implementation, we use language models to represent probabilistic models of lexical content, and build them using the SRILM toolkit [28]. We replace maximum likelihood schemes with minimum perplexity schemes wherever applicable. Perplexity is a measure of how well a probability model predicts an observation; lower the value, better the model. For an utterance U (m), it is calculated as: PP(U (m)) = P (w1 w2 ...wN )-1/N Where P (w1 w2 ...wN ) is joint probability of occurrence of words Algorithm 1 Hard-Assignment EM algorithm for state convergence in activation-based DBM Initialize utterances in C0 session  {S0 }, C1  {S1 } Build language model L0 from utterances  S0 , L1 from utterances  S1 while training perplexity does not converge do E-step: Classify every utterance U (m) in C0 ,C1 classes Get perplexities PP0 , PP1 of U (m) computed by L0 ,L1 PP0 {U (m)}
state=S1 state=S0

wi is the ith word of utterance U (m), i  {1, 2, ..., N } From (7) we see that minimizing perplexity is equivalent to maximizing probability 4.1. SBM From the SBM assumption, we know that the classes and states are equivalent. Therefore, for building a model of a particular behavioral state C0 /C1 , we collect all utterances from the corresponding sessions and train the language models L0 /L1 respectively. While doing so, we combine our trained LMs with a Universal Background Model (UBM), in order to smooth the language models. We use a parameter =0.1 as the interpolation weight for the UBM. Thus, the SBM consists of two LMs for each speaker that model his/her language structure corresponding to the least and the most negative behavior. 4.2. DBM As seen in Section 3.2, DBMs allow multiple transitions within an interaction and as a result, each session consists of utterances of multiple types of behavior. However, we have only the global session ratings C0 and C1 and not the utterance-level state labels. Therefore, we use two iterative semi-supervised methods to label turnlevel information into the two clusters corresponding to S0 and S1 , as described in Section 4.3. Due to the non-availability of utterancelevel labels, the convergence of these methods is verified indirectly through the overall training perplexity and the testing accuracy. 4.3. Fusion Methods for DBMs Fusion methods tie local utterance-level decisions to the global session-level behavior. Therefore, the type of fusion used defines the relation between behavioral states S0 /S1 and behavioral classes C0 /C1 , as explained below: 4.3.1. ADBM Just like in the SBM, we initialize the language models L0 /L1 for states S0 /S1 from utterances in C0 /C1 respectively, smoothed with a UBM. We then classify each utterance as S0 or S1 and re-train language models until the training perplexity converges. Since perplexities are not direct measures of log-likelihoods, we classify each ut-

(7)

Algorithm 2 Viterbi-EM algorithm for state and class parameter convergence in likelihood-based DBM Initialize utterances in C0 session  {S0 }, C1  {S1 } Build language model L0 from utterances  S0 , L1 from utterances  S1 Initialize  ,0 ,1 while training perplexity does not converge do E-step: Decode C0 utterances using 0 , L0 , L1 ,  for every session utterance U (m) do Get probabilities P0 ,P1 of utterance U (m) from L0 ,L1 Find probability that U (m) was generated by state Sk if m=1 (start of session) then k (m) = k *Pk {U (m)}; k  {0, 1} else k (m) = arg maxj [j (m-1) * 0 (j,k)] * Pk {U (m)}; j,k  {0, 1} end if 0 (m)
state = S0 state = S1



1 (m)



PP1 {U (m)}; m  {0, 1, ..., M }

M-step: Build L0 from S0 utterances,& L1 from S1 end while

end for Repeat E-step for C1 utterances; replace 0 with 1 M-step: Re-estimate states, class parameters Build Lk from all U (m) whose state = Sk ; k  {0, 1} (<i,j> state pairs in class Cn ) ; i,j,k  Update n (i,j ) = count k count(<i,k> state pairs in class Cn ) {0, 1}, n  {0, 1} end while

2092

terance independently of the rest, for which a Hard-Assignment Expectation Maximization (EM) scheme is better suited, as described in Algorithm 1. At the completion of this re-assignment, each class now contains both states. We, therefore, compute the proportions of state occupancies for C0 and C1 sessions as decoded using the models L0 and L1 . 4.3.2. LDBM We initialize our model parameters as in Section 4.3.1 and obtain converged estimates of behavioral states and the class parameters, using the Viterbi-EM algorithm, as described in Algorithm 2. In this model, the Hidden Markov Model (HMM) parameters need to be reestimated over iterations. Finally, each behavioral state is described by the initial vector  , which contains the probability of the spouse starting in each state, and its language model. Each class is now associated with a matrix that governs state transitions in that class; 0 for the C0 sessions, and 1 for C1 . 0 (i, j ) represents the probability of a C0 -rated spouse transitioning to state j , given that he/she was previously in state i. 5. BEHAVIORAL MODEL EVALUATION In this section, we explain our evaluation procedure for the two different models and the two different fusion techniques used in DBMs. 5.1. SBM For a given test session, we assign C1 /C0 label to a speaker after comparing the perplexities from his/her LMs. Given a set of M utterances U ={U (1), ..., U (M )} from a speaker during the test session, we compute the LM perplexities based on L0 and L1 , and the class label assignment is shown in equation 8. Ci = arg min
j m

Model SBM Activation-DBM Likelihood-DBM

Husband 79.29% 85.00% 83.57%

Wife 83.57% 79.29% 88.57%

Average 81.43% 82.15% 86.07%

Table 2: Classification Accuracy of Behavioral Models using 1grams

P Pj {U (m)}

(8)

Where P Pi {} represents perplexity score of utterance computed by LM Li The results for SBM, obtained using a 1-gram LM, are shown in Table 2. 5.2. DBM Although both ADBM and LDBM allow state transitions, the scoring mechanism is quite different in each case, as explained below. 5.2.1. ADBM For a given test session, we perform hard-assignment of state labels to utterances using L0 and L1 and compute the state distribution. For a set of utterances U ={U (1), ..., U (M )} where the most commonly occurring state is found to be Sk , the ADBM chooses the behavioral class that maximizes Ci = arg max P (Sk |Cj )
Cj

(9)

The performance of ADBM improves, on average, by around 1% relative to the SBM, and the results are shown in Table 2. 5.2.2. LDBM For a set of test utterances U , we decode based on HMMs of both C0 and C1 . The label of the class whose prediction is associated with the highest likelihood is assigned to the test file. This decision scheme is shown in equation (10) Ci = arg max P (S j |U , j ,  )
j

(10)

6. DISCUSSION The likelihood-DBM has the best average performance across both spouses, at 86.07%, while the SBM has the lowest, with 81.43%. This matches our expectations since the likelihood-DBM is less rigid in its assumptions about behavioral changes. Thus, it is better equipped to capture information about dynamic behavior, as compared to the other two models. While the SBM helps in identifying which state a person's behavior more likely corresponded to, it is limited in its ability to model changes in behavior within a session. The Activation-DBM can model behavioral changes within the same session, and performs well, but it allows rapid changes in behavioral states across successive utterances. We feel that such fast changes in behavior do not realistically mirror the usual changes in human behavioral expression. In addition, there is no dependence on previous utterances, meaning that context is ignored. The Likelihood-DBM avoids the pitfalls associated with the first two models, and while more complex, it is more accurate in predicting behavior from language. Note that both dynamic implementations can likely be improved through supervised learning and it is notable that the achieved performance is through only loose semi-supervised clustering. 7. CONCLUSION Automating the psychological evaluations can have a significant impact by providing less expensive and more accurate methods of behavioral coding. In this work, we proposed a Dynamic Behavior Model based scheme which models a spouse in couples' therapy as transitioning through multiple behavioral states to obtain an overall perception of negativity. We tested two models for dynamically modeling the behavior: Activation-based DBM and Likelihood-based DBM which outperformed the Static Behavioral Model. Furthermore, the LDBM performed slightly better than ADBM as it provided more freedom to the way behavior could be expressed. In this work, we addressed only two models of local information integration towards deriving global behavioral descriptions. We plan to extend this model to further incorporate dependencies amongst spouses to model their behavioral influences as well as dependencies among other codes apart from negativity. So far, we have only considered two state models to predict binary behavior levels. We will develop models with a finer behavior stratification and observe its relation with the number of states. Finally, this model can be employed in a range of application domains involving behavioral evaluations in multi-party interactions. 8. REFERENCES [1] C. Heavey, D. Gill, and A. Christensen, Couples interaction rating system 2 (CIRS2), University of California, Los Angeles, 2002. [2] J. Jones and A. Christensen, "Couples interaction study: Social support interaction rating system," Technical manual, University of California, Los Angeles, 1998. [3] RE Heyman and D. Vivian, "Rmics: Rapid marital interaction coding system: Training manual for coders, state university of new york, stony brook, ny (1993)," Unpublished manuscript. Available at, 1993.

Where U is set of test utterances, {U (1), ..., U (M )} S n is the test state sequence predicted by class Cn , {S (1), ..., S (M )} n is set of HMM parameters of class Cn , {n ,L0 ,L1 , } The performance of LDBM improves, on average, by around 5.7% relative to the SBM, and the results are shown in Table 2.

2093

[4] G. Margolin, P.H. Oliver, E.B. Gordis, H.G. O'Hearn, A.M. Medina, C.M. Ghosh, and L. Morland, "The nuts and bolts of behavioral observation of marital and family interaction," Clinical Child and Family Psychology Review, vol. 1, no. 4, pp. 195213, 1998. [5] Shrikanth S. Narayanan and Panayiotis G. Georgiou, "Behavioral signal processing: Deriving human behavioral informatics from speech and language," Proceeding of the IEEE, 2014. [6] Panayiotis G. Georgiou, Matthew P. Black, and Shrikanth S. Narayanan, "Behavioral signal processing for understanding (distressed) dyadic interactions: some recent developments," in Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding, New York, NY, 2011, JHGBU '11, pp. 712, ACM. [7] Bo Xiao, Panayiotis Georgiou, and S. Narayanan, "Data driven modeling of head motion towards analysis of behaviors in couple interactions," in International Conference on Audio, Speech and Signal Processing, 2013. [8] James Gibson, Bo Xiao, Panayiotis Georgiou, and S. Narayanan, "An audio-visual approach to learning salient behaviors in couples' problem solving discussions," in International Conference on Audio, Speech and Signal Processing, 2013. [9] Chi-Chun Lee, Athanasios Katsamanis, Matthew P. Black, Brian Baucom, Andrew Christensen, Panayiotis G. Georgiou, and Shrikanth S. Narayanan, "Computing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions," Computer, Speech, and Language, 2012. [10] M.P. Black, A. Katsamanis, B.R. Baucom, C.C. Lee, A.C. Lammert, A. Christensen, P.G. Georgiou, and S.S. Narayanan, "Toward automating a human behavioral coding system for married couples interactions using speech acoustic features," Speech Communication, 2012. [11] Matthew Black, Panayiotis Georgiou, Athanasios Katsamanis, Brian Baucom, and Shrikanth Narayanan, ""You made me do it": Classification of blame in married couples' interaction by fusing automatically derived speech and language information," in Proceedings of InterSpeech, Florence, Italy, August 2011. [12] P. G. Georgiou, M. P. Black, A. Lammert, B. Baucom, and S. S. Narayanan, ""That's aggravating, very aggravating": Is it possible to classify behaviors in couple interactions using automatically derived lexical features?," in Proceedings of Affective Computing and Intelligent Interaction, Memphis, TN, USA, 2011. [13] C.C. Lee, A. Katsamanis, M. Black, B. Baucom, P. Georgiou, and S. Narayanan, "Affective state recognition in married couples interactions using PCA-based vocal entrainment measures with multiple instance learning," Affective Computing and Intelligent Interaction, pp. 3141, 2011. [14] C.-C. Lee, A. Katsamanis, M. P. Black, B. R. Baucom, P. G. Georgiou, and S. S. Narayanan, "An analysis of PCA-based vocal entrainment measures in married couples' affective spoken interactions," in Proceedings of InterSpeech, Florence, Italy, 2011. [15] C.-C. Lee, M. P. Black, A. Katsamanis, A. Lammert, B. R. Baucom, A. Christensen, P. G. Georgiou, and S. Narayanan, "Quantification of prosodic entrainment in affective spontaneous spoken interactions of married couples," in Proceedings of InterSpeech, 2010.

[16] M. P. Black, A. Katsamanis, C.-C. Lee, A. Lammert, B. R. Baucom, A. Christensen, P. G. Georgiou, and S. S. Narayanan, "Automatic classification of married couples' behavior using audio features," in Proceedings of InterSpeech, 2010. [17] Bo Xiao, Daniel Bone, Maarten Van Segbroeck, Zac E. Imel, David Atkins, Panayiotis Georgiou, and Shrikanth Narayanan, "Modeling therapist empathy through prosody in drug addiction counseling," in In Proceedings of Interspeech, 2014. [18] Daniel Bone, Chi-Chun Lee, Theodora Chaspari, Matthew P. Black, Marian Williams, Sungbok Lee, Pat Levitt, and Shrikanth S. Narayanan, "Acoustic-prosodic, turn-taking, and language cues in child-psychologist interactions for varying social demand," in Proceedings of InterSpeech, Aug. 2013. [19] Theodora Chaspari, Chi-Chun Lee, and Shrikanth Narayanan, "Interplay between verbal response latency and physiology of children with autism during ECA interactions," in Proceedings of InterSpeech, 2012. [20] Theodora Chaspari, Emily Mower Provost, Athanasios Katsamanis, and Shrikanth Narayanan, "An acoustic analysis of shared enjoyment in ECA interactions of children with autism," in Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Kyoto, Japan, 2012. [21] E. Mower, C.C. Lee, J. Gibson, T. Chaspari, M.E. Williams, and S. Narayanan, "Analyzing the nature of ECA interactions in children with autism," in Twelfth Annual Conference of the International Speech Communication Association, 2011. [22] Rahul Gupta, Nikolaos Malandrakis, Bo Xiao, Tanaya Guha, Maarten Van Segbroeck, Matthew P Black, Alexandros Potamianos, and Shrikanth S Narayanan, "Multimodal prediction of affective dimensions and depression in humancomputer interactions," . [23] Bj orn Schuller, Gerhard Rigoll, and Manfred Lang, "Hidden markov model-based speech emotion recognition," in Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03). 2003 IEEE International Conference on. IEEE, 2003, vol. 2, pp. II1. [24] Gian-Marco Baschera, Alberto Giovanni Busetto, Severin Klingler, Joachim M Buhmann, and Markus Gross, "Modeling engagement dynamics in spelling learning," in Artificial Intelligence in Education. Springer, 2011, pp. 3138. [25] Dirk D Steiner and Jeffrey S Rain, "Immediate and delayed primacy and recency effects in performance evaluation.," Journal of Applied Psychology, vol. 74, no. 1, pp. 136, 1989. [26] Chi-Chun Lee, Athanasios Katsamanis, Panayiotis G. Georgiou, and Shrikanth S. Narayanan, "Based on isolated saliency or causal integration? toward a better understanding of human annotation process using multiple instance learning and sequential probability ratio test," in Proceedings of InterSpeech, Sept. 2012. [27] A. Christensen, D.C. Atkins, S. Berns, J. Wheeler, D.H. Baucom, and L.E. Simpson, "Traditional versus integrative behavioral couple therapy for significantly and chronically distressed married couples," Journal of Consulting and Clinical Psychology, vol. 72, no. 2, pp. 176191, 2004. [28] Andreas Stolcke et al., "Srilm-an extensible language modeling toolkit.," in INTERSPEECH, 2002.

2094

1

MT-Diet Demo: Demonstration of Automated
Smartphone Based Diet Assessment System
Junghyo Lee, Ayan Banerjee, and Sandeep K. S. Gupta
IMPACT Lab, CIDSE, Arizona State University, Tempe, AZ
Email: {jlee375,abanerj3,sandeep.gupta}@asu.edu

Abstract—Background: According to several recent research
results [1]–[4], obesity can increase the risk of many diseases
such as diabetes, chronic kidney disease, metabolic disease,
cardiovascular disease, etc. To prevent and treat the obesity
efficiently and effectively, diet monitoring is an important factor.
Purpose: Manual self-monitoring techniques for diet suffer from
drawbacks such as low adherence, underreporting, and recall
error [5]–[7]. Camera based applications that automatically
extract type and quantity of food from an image of the food plate
can potentially improve adherence and accuracy. However, stateof-the-art systems [8] have fairly low accuracy for identifying
cooked food (only 63%) and are not fully automatic. To overcome
these drawbacks such as low adherence, underreporting, recall
error, low accuracy, and semi-automatedness, we introduce MTDiet, a fully automated diet assessment system. It can identify
cooked food with an accuracy of 88.93%. This is a significant
improvement (over 20%) from the current state-of-the art system.
Method: MT-Diet is a smartphone-based system that interfaces a
thermal sensor with a smartphone. Using this system a user can
take both thermal and visual images of her food plate with just
one click. We used a database of 80 frozen meals which contain
several different types of foods so that the actual total number
of our food database 244 and the database has 33 different
types of foods. By using the database, we demonstrate two core
components: a) food segmentation, separating food items from
the plate and recognizing multiple food items as a single food
item, and b) food identification, determining the type of foods.
Result: MT-Diet food segmentation methodology is fully automatic and requires no user input as opposed to recent works,
the accuracy of separating food parts from the plate was 97.5%.
The accuracy of food identification using Support Vector Machine
with Radial Basis Function kernel based on color, texture, and
histogram of oriented gradients features is 88.5%.
Conclusion: We suggest a new and novel approach for diet
assessment, MT-Diet. Our approach can potentially be an inexpensive, real time for the feedback on calorie intake, easy-to-use,
privacy preservation, personalization based on eating habits of
individuals, and fully automated diet monitoring system. The tool
can also be used to conduct clinical studies to develop models of
meal patterns that can be incorporated to design better artificial
pancreas.

systems easy to use and more usable than text based records
[9]. Another study on a medium size cohort of adults aged 18
to 24 years has reported that nearly 87 % of users feel that they
will not be opposed using a food image based diet monitoring
mobile application for long term [10]. However, feedback from
a study on the usability of MyFitnessPal [11], an image based
diet monitoring application indicate that users would prefer
more information related to type of food and calorie intake
from just an image upload. According to recent surveys [12]–
[14], image based applications that automatically extract type
and quantity of food from an image of the food plate have
good accuracy for identifying fresh food. However, a larger
share of daily calorie intake comes from hot cooked food for
which these systems have fairly low identification accuracy
(nearly 63% accuracy at best). This is because hot/warm
foods also tend to be mixed dishes (e.g., lasagna), which are
difficult to assess using color images. We demonstrate MTDiet, a smartphone based automated cooked food identification
system that can determine the type of cooked food on plate
with nearly 90% accuracy.
MT-Diet interfaces a thermal sensor with a smartphone,
so that a user can take both thermal and visual images
of his/her food plate with just one click. The system uses
thermal maps of a food plate to increase accuracy of extraction
and segmentation of food parts, combines thermal and visual
images to improve accuracy in the detection of cooked food.
Preliminary testing results show that MT-Diet can determine
the type of food consumed with an average accuracy of 90%,
which is a significant improvement from the current stateof-the-art. MT-Diet is implemented as a part of the bHealthy
application suite for behavioral healthy monitoring [15].
II. D EMONSTRATION S ETUP AND P LAN
MT-Diet application
Requirements: a) cooked foods, b) a smartphone built in a camera, c) a
seek thermal sensor [16], and d) two small caps filled with cold water.

I. I NTRODUCTION
Accurate assessment of dietary intake is important for: i)
analyzing the relationship between caloric intake and health
outcomes such as obesity, ii) evaluating outcomes of dietary
change interventions, iii) ensuring compliance with dietary
recommendations, iv) self-motivated diet control, or v) identifying factors that induce change in dietary intake so that they
can be used in targeted interventions. Recent studies have reported that smartphone users find image based diet monitoring
This project is partially funded by NSF IIS 1116385 and NIBIB EB019202.

Inputs: a) a plate full of hot food, b) color image from smartphone
camera, and c) thermal image from infrared camera.
Outputs: a) Segmented food images, b) Food type in plate, and c)
Nutrition information into USDA website.
Platform: a) a smartphone interfaced with thermal camera and b)
reliable connection of the smartphone with a cloud server.
Assumptions:
a) Food temperature  Plate temperature;
b) Plate temperature > Background temperature;
c) The plate is not overflowing with food; and
d) A database of food items is prepared offline and available to MT-Diet.

2

Fig. 1. Actual requirements: Left are Nexus 5, Seek thermal sensor, and two
small caps. Right is a cooked frozen food.

Fig. 3. Flow Chart of MT-Diet by Socket.

Fig. 2. MT-Diet application steps for the demonstration.

Before launching the application, we need to prepare cooked
foods, a smartphone with built-in a camera, and a Seek
thermal sensor [16] interfaced with the smartphone such as
Fig. 1. Frozen foods are used since it is affordable and easy
to get. These foods are defrosted for 15 minutes using a
microwave. Then, we take the image of cooked food using
the smartphone camera and the thermal camera. The Seek
camera is connected to the Nexus 5 phone using a microUSB chord, due to which, the resulting images have different
angles and distances. Hence, we put two small caps filled with
cold water at two diagonally opposite ends of the plate for the
calibration purpose. After taking these two images by these
cameras, we are ready to launch the MT-Diet application. The
application needs both a color image and a thermal image of
the food as inputs. The application then provides the user with
(a) segmented food images (removed plate and background),
(b) food type in plate, and (c) each food’s nutrition information
into USDA (United States Department of Agriculture) website.
The demonstration consists of following steps: (a) The user
selects from color image and thermal image by clicking
two buttons: Color Image and Thermal Image such as
Step 3-1 and 3-2 in Fig. 2 . After clicking these buttons,
the application opens the gallery and the users can find the

images that they took. The application displays the images as
well as their absolute paths, the user can now visually verify
whether the selected images are correct. (b) The user clicks
the start button to send the images to the cloud server. The
cloud server connects the application using the socket communication. Fig. 3 illustrates the connection between the server
and the application. As seen in the figure, the application
sends the two images to the server. (c) The server processes
the food recognition included the food segmentation and
food identification. The application produces accurate food
segmentation without needing to to ask the user by combining
three algorithms: Dynamic Thermal Threshold (DTT), Hierarchical Image Segmentation (HIS) [17], and Grabcut [18]. The
approach and algorithms in the detail are provided in a paper
[19] Also, the execution time of the food identification is fast
since the server already has the trained parameters for SVM
classification. (d) The server sends the segmented images
and food types to the application. After receiving the food
types, the application can find the USDA database links of the
food type by matching with the USDA database.
The food types are displayed as a spinner button (Step 4-1 in
Fig. 2) and the segmented images clicking the Segmentation
button (Step 4-2 in Fig. 2). After looking at the segmented
images, the user can judge each segmented image quality,
which is critical for the identification accuracy. Moreover,
the user can access the USDA database about the food by
clicking the food type spinner button. Therefore, by taking
two images, the user can obtain the accurate food information
automatically. The application is envisioned to be inexpensive,
easy-to-use, and privacy preserving. In addition the application
can provide real-time feedback on calorie intake and can be
personalized based on eating habits of individuals.
A video of the demonstration is available in youtube [20].

III. S PECIAL REQUIREMENTS
For the demo, we need (a) Microwave with power outlet to
cook the frozen foods and (b) WiFi Network to connect to the
cloud server.

3

$392 Billion cost. In this project we evaluate the accuracy
and usability of MT-Diet, a cost effective smartphone based
automated diet monitoring application that uses the image of
a food plate in both the thermal and visual spectra to identify
food type. Such an easy-to-use and cost effective solution
to real time diet monitoring can potentially achieve higher
adherence to interventions which may in turn lead to beneficial
health impacts such as effective weight reduction.
R EFERENCES
Fig. 4. System Architecture of the extended MT-Diet food monitoring system
including the measuring food consumption.

IV. D ISCUSSION AND F UTURE W ORK
Since this demo is in an initial phase, there are some
insufficient implementations so in this section, we will discuss
the future phases. The first extension we propose is a technique
such that the two bottle caps that were used for calibration are
no longer needed. Using the bottle caps limits the usage of the
application so we suggest an extended implementation. In the
initial implementation, Nexus 5 needs the micro USB wire
because the direction of the thermal camera lens is opposite
to the direction of a built-in Nexus 5 camera. The micro USB
wire is an obstacle for calibration because it generates different
angles and distance between the thermal image and color
image. So we will change the smartphone to LG G2 which
does not need the micro USB wire to connect the thermal
camera. Then, we find the fixed different angle and distance
the thermal image and color image by executing only one
calibration task. Therefore, the small caps are not required for
the calibration process.
In addition, the the application requires high computation
time (almost 100 seconds for the segmentation) even when the
cloud server is employed. So, we consider parallel computation
as a potential solution of the issue. Moreover, if the user
is in a situation where there is no network connection, the
application cannot work, thus we pursuit an approach so that
the application will work in a non-network environment. To
deal with both issues, we consider changing the programming
language from Matlab to C++ because C++ not only supports
cross compiling with Android but also can be extended to
OpenCL for smartphone GPU implementation.
Food identification is a important task for diet monitoring,
but estimating food consumption is also a critical issue. Our
idea for the measuring the user’s food consumption is to
recognize the user hand movements using the wrist sensors
as seen in Step 2 in Fig. 4. To measure the food consumption
using the sensor, we need to consider three problems: a)
identifying utensils such as fork, spoon, chopstick, and knife
b) mapping food image location and actual food location and
c) identifying and counting user’s eating motion.
V. C ONCLUSION
Automated diet monitoring and caloric intake prediction is
an effective intervention for chronic diseases such as cardiac
problems, obesity and diabetes that affect more than onethird of US adults with a combined estimated economic

[1] C. Zhang, K. M. Rexrode, R. M. van Dam, T. Y. Li, and F. B. Hu,
“Abdominal obesity and the risk of all-cause, cardiovascular, and cancer
mortality sixteen years of follow-up in us women,” Circulation, vol. 117,
no. 13, pp. 1658–1667, 2008.
[2] M. T. Hamilton, D. G. Hamilton, and T. W. Zderic, “Role of low energy
expenditure and sitting in obesity, metabolic syndrome, type 2 diabetes,
and cardiovascular disease,” Diabetes, vol. 56, no. 11, pp. 2655–2667,
2007.
[3] H. Bays and C. Ballantyne, “Adiposopathy: why do adiposity and obesity
cause metabolic disease?” 2006.
[4] J. E. Hall, J. R. Henegar, T. M. Dwyer, J. Liu, A. A. da Silva, J. J. Kuo,
and L. Tallam, “Is obesity a major cause of chronic kidney disease?”
Advances in renal replacement therapy, vol. 11, no. 1, pp. 41–54, 2004.
[5] T. L. Burrows, R. J. Martin, and C. E. Collins, “A systematic review of
the validity of dietary assessment methods in children when compared
with the method of doubly labeled water,” Journal of the American
Dietetic Association, vol. 110, no. 10, pp. 1501–1510, 2010.
[6] C. M. Champagne, G. A. Bray, A. A. Kurtz, J. B. R. Monteiro, E. Tucker,
J. Volaufova, and J. P. Delany, “Energy intake and energy expenditure: a
controlled study comparing dietitians and non-dietitians,” Journal of the
American Dietetic Association, vol. 102, no. 10, pp. 1428–1432, 2002.
[7] J. R. Hebert, C. B. Ebbeling, C. E. Matthews, T. G. Hurley, M. Yunsheng,
S. Druker, and L. Clemow, “Systematic errors in middle-aged women’s
estimates of energy intake: comparing three self-report measures to total
energy expenditure from doubly labeled water,” Annals of epidemiology,
vol. 12, no. 8, pp. 577–586, 2002.
[8] M.-Y. Chen, Y.-H. Yang, C.-J. Ho, S.-H. Wang, S.-M. Liu, E. Chang,
C.-H. Yeh, and M. Ouhyoung, “Automatic chinese food identification
and quantity estimation,” in SIGGRAPH Asia. ACM, 2012, p. 29.
[9] K. Aizawa, K. Maeda, M. Ogawa, Y. Sato, M. Kasamatsu, K. Waki,
and H. Takimoto, “Comparative study of the routine daily usability
of foodlog a smartphone-based food recording tool assisted by image
retrieval,” Journal of diabetes science and technology, vol. 8, no. 2, pp.
203–208, 2014.
[10] S. Sharma and S. Fulton, “Diet-induced obesity promotes depressivelike behaviour that is associated with neural adaptations in brain reward
circuitry,” International journal of obesity, vol. 37, no. 3, pp. 382–389,
2013.
[11] [Online]. Available: https://www.myfitnesspal.com/.
[12] P. Pouladzadeh, P. Kuhad, S. V. B. Peddi, A. Yassine, and S. Shirmohammadi, “Mobile cloud based food calorie measurement,” in Multimedia
and Expo Workshops (ICMEW). IEEE, 2014, pp. 1–6.
[13] P. Pouladzadeh, S. Shirmohammadi, A. Bakirov, A. Bulut, and A. Yassine, “Cloud-based svm for food categorization,” Multimedia Tools and
Applications, pp. 1–18, 2014.
[14] P. Pouladzadeh, S. Shirmohammadi, and A. Yassine, “Using graph cut
segmentation for food calorie measurement,” in Medical Measurements
and Applications (MeMeA). IEEE, 2014, pp. 1–6.
[15] J. Milazzo, P. Bagade, A. Banerjee, and S. K. S. Gupta, “bhealthy:
A physiological feedback-based mobile wellness application suite,” in
Proceedings of the 4th Conference on Wireless Health, ser. WH ’13.
New York, NY, USA: ACM, 2013, pp. 14:1–14:2.
[16] Accessed: 2015-07-10. [Online]. Available: http://www.thermal.com/.
[17] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection
and hierarchical image segmentation,” Pattern Analysis and Machine
Intelligence, IEEE Transactions on, vol. 33, no. 5, pp. 898–916, 2011.
[18] C. Rother, V. Kolmogorov, and A. Blake, “Grabcut: Interactive foreground extraction using iterated graph cuts,” ACM Transactions on
Graphics (TOG), vol. 23, no. 3, pp. 309–314, 2004.
[19] J. Lee, A. Banerjee, and S. K. S. Gupta, “Mt-diet: Automated smartphone based diet assessment with infrared images,” in Pervasive Computing and Communications (PerCom). IEEE, 2016.
[20] [Online]. Available: https://www.youtube.com/watch?v=En8iyJ5JSI4.

Computational Methods for Dynamic Graphs
Corinna Cortes, Daryl Pregibon and Chris Volinsky AT&T Shannon Labs November 24, 2003

All authors are members of research sta at AT&T Labs, 180 Park Avenue, Florham Park, New Jersey 07934.

1

Abstract
We consider problems that can be characterized by large dynamic graphs. Communication networks provide the prototypical example of such problems where nodes in the graph are network IDs and the edges represent communication between pairs of network IDs. In such graphs, nodes and edges appear and disappear through time so that methods that apply to static graphs are not su cient. Our de nition of a dynamic graph is procedural. We introduce a data structure and an updating scheme that captures, in an approximate sense, the graph and its evolution through time. The data structure arises from a bottom-up representation of the large graph as the union of small subgraphs centered on every node. These subgraphs are interesting in their own right and can be enhanced to form what we call Communities of Interest (COI). We discuss an application in the area of telecommunications fraud detection to help motivate the ideas.

aging, fraud detection.

Keywords: transactional data streams, dynamic graphs, approximate subgraphs, exponential aver-

2

1 Introduction
Transactional data consists of records of interactions between pairs of entities occurring over time. For example, a sequence of credit card transactions consists of purchases of retail goods by individual consumers from individual merchants. Transactional data can be represented by a graph where the nodes represent the transactors and the edges represent the interactions between pairs of transactors. Viewed in this way, interesting new questions can be posed concerning the connectivity of nodes, the presence of atomic subgraphs, or whether the graph structure leads to the identi cation and characterization of \interesting" nodes. For example, Kleinberg 9] introduces the notion of \hubs" and \authorities" as interesting nodes on the Internet. The data used by Kleinberg di er signi cantly from the data we consider in that he uses static links to induce a graph over WWW pages. We use actual network tra c, as captured by interactions between pairs of transactors, to de ne our graph. Since nodes and edges appear and disappear through time, the graph we consider is dynamic. There are many challenging issues that arise for dynamic graphs and we have used a speci c application to focus our research, namely the graph induced by calls carried on a large telecommunications network. This application is interesting, both because of its size (i.e., hundreds of millions of nodes and edges) and its rate of change (i.e., hundreds of thousands of new nodes and edges each day). Like all networks, it is also diverse in the sense that some nodes are relatively inactive while others are superactive. The paper is organized as follows. Section 2 illustrates characteristics of large dynamic graphs using network tra c from a speci c telecommunications service. Section 3 introduces the de nition of a dynamic graph that we adopt and discusses the computational and statistical features of various alternatives. Section 4 describes an approximation to facilitate both interpretation and large scale computations, including updating and maintaining the dynamic graph over time. Section 5 introduces an example that illustrates how these subgraphs are used in practice. In Section 6 we expand on these notions and consider approximate subgraphs centered on nodes that can be further enhanced or pruned to de ne communities of interest. Section 7 discusses related work in and outside the data mining community. The nal section summarizes the ndings and discusses future work.

2 Motivation
The large graphs that we are concerned with are de ned from records of transactions on large telecommunications networks. We believe that characteristics of such graphs are shared by other large nancial and data networks, including the Internet. These characteristics include a large number of nodes and edges, sparse connectivity, and dynamics that have stable macro e ects but substantial variation in micro e ects. In this section we illustrate these characteristics using telecommunications tra c. Our data consists of hundreds of millions of nodes, each of which represents an account. We observe several billion edges on this network in an average week, presenting themselves in a continuous data stream. This section uses plots and tables to 3

present the salient features of this transaction stream.

2.1 Addition and attrition of nodes
Nodes in dynamic graphs appear and disappear through time. Figure 1 shows the addition and attrition of nodes throughout the study period. By \node addition" we mean the number of new nodes that we see in week i that we haven't yet seen through week i ? 1. By \node attrition" we mean the number of nodes that we see for the last time in week i. The service we consider has hundreds of millions of nodes and billions of edges. On any given day, tens of millions of these nodes are active and responsible for hundreds of millions of transactions. The exact numbers are considered proprietary and we illustrate the volatility of the service using relative change. Figure 1 attempts to capture this volatility by showing the addition and attrition rates of nodes on the network. The gure illustrates that after a steady state is reached, roughly after 18 weeks of observation, new nodes are observed (for the rst time) at a rate of slightly less than 1% per week. Similarly, discounting the end e ects of the observation period, old nodes are observed (for the last time) at the same rate. The parallel lines tted to the end/start of these sequences illustrate that the service is stable despite there being considerable turnover of transactors each and every week.

100

80 Cumulative % of All Nodes Seen o o o 60 o 40 o o

o

o

+ o o o o o o o o o o o o o o o o o

+ + + + + +

20

0

+ + + + + + + + + + + + + + + o + + 0 5 10 15

+

+

20

25

Study Week Observed

Figure 1: Node addition and attrition through time. The data correspond to 25 successive weeks of network activity. The upper curve shows the cumulative percent of unique nodes seen each study week that had not been seen before. The lower curve shows the cumulative percent of nodes seen for the last time in each study week. Steady state is reached after about 18 weeks and lines are tted to the remaining last and rst 7 weeks respectively. The slopes correspond to a addition and attritrion rates of just under 1%. This means that 1% of the nodes we observe each week had not been observed previously, and another 1% will never be seen again. 4

2.2 Addition and attrition of edges
We illustrate the addition and attrition of edges by following a random sample of 1000 residential accounts over a 180 day period. This period was broken up into six 30-day slices, and the edges from the rst month were tracked to see if they appeared in subsequent months. The results in Table 1 show that edges seen in one month often do not show up again. Of all the edges observed in the reference month, only 37.9% of them are seen the following month, and the number seen steadily decreases for subsequent months. The cumulative results show aggregation of all the months. By the end of the study period, only 53.7% of the edges from the rst month have been observed again, and the leveling o of the cumulative numbers indicate that there will be a reasonably large percent of edges that will never be seen again. Month Old Edges Seen Percent Cumulative Percent 1 5995 100 { 2 2272 37.9 37.9 3 2001 33.4 46.5 4 1734 28.9 50.0 5 1585 26.4 52.3 6 1376 23.0 53.7 Table 1: Edge attrition for activity on 1000 residential accounts over a 6 month period. Month 1 is a \reference month" for which we look for those edges in subsequent months. For each subsequent month the table shows how many of the edges are observed again. The column marked Cumulative aggregates the subsequent months to show how many of the edges we have seen overall. Table 2 shows edge addition e ects on the same sample over the same period. This time for each month, we note how many edges are seen for the rst time. In addition, we show the total number of unique edges seen up to and including that month. Note that after ve months, we have observed nearly 4 times more edges than we observed in the rst month.

2.3 Connectivity
A fully connected graph with N nodes has M = N (N ? 1) directed edges. We explore connectivity using the sample of 1000 residential accounts. Figure 2(a) displays the cumulative distribution of in-degree and out-degree for the nodes in this sample. The gure shows that 90% of all nodes have in-degree of 22 or less, and out-degree of 32 or less. This relative sparseness suggests a relationship of the form M / Nlog(N ), or maybe even M / N , rather than M / N 2 . In Section 4 we exploit this sparsity with an approximate representation for large network graphs. 5

Month New Edges Seen Percent New Cumulative Percent 1 5995 100 25.6 2 4461 66.3 44.7 3 4441 59.8 63.6 4 3130 50.8 77.0 5 3102 50.0 90.3 6 2274 44.4 100 Table 2: Edge addition for activity on 1000 residential accounts over a 6 month period. For each month we show the percentage of the edges that we observed that we had not seen yet. The column marked Cumulative Percent shows the unique edges we have seen through the entire study up until that point as a percentage of the union of all edges. Figure 2(b) shows that the distribution for the out-degrees roughly follows a power-law distribution (and hence shows up as linear on a log-log scale). There is a growing literature (e.g. Barabasi 2]) showing that power-law behavior exists in large networks, including the Internet, genetic networks, and social interactions { so it is not surprising to see evidence of this property in our data.

1.0

200

100
0.8

50
Cumulative % of Nodes

InDegree OutDegree

# of Nodes

0.6

20

10

0.4

5
0.2

2

0.0 1 2 5 10 20 50 100 200

1 1 2 5 10 Out-Degree 20 50 100

Edge Degree

(a)

(b)

Figure 2: Plots of the (in-) out-degree of 1000 residential accounts over a 180 day period. Panel (a) shows the cumulative percent of accounts having node degree less than or equal to k. Panel (b) shows the number of nodes having a speci c out-degree. 6

2.4 Activity
While nodes and edges arrive and depart in fairly large numbers, it is also interesting to consider how often they were observed. Figure 3 is a histogram of the number of days that the given nodes were active during the entire 180 day period. The median of this distribution is 29 days, indicating that a typical node was only active on one out of six days during the study.

200

150

Frequency

100

50

0 0 50 100 Days Active (out of 180) 150

Figure 3: Histogram of the number of days with activity for 1000 residential accounts over a 180 day period.

2.5 Implications
The plots and tables in this section illustrate that large numbers of nodes and edges appear or fail to appear on a daily basis. In some cases, their disappearance is temporary and in others it is permanent. Any procedure that attempts to capture network behavior will have to deal with node/edge addition and attrition in an automated fashion since there is little time to synchronize with databases of account information and yet process the current network activity. The relatively sparse connectivity of the graph is the aspect of the behavior that we attempt to exploit in devising a methodology to de ne, build, evolve, and maintain an approximate representation of the graph through time.

3 De nition of a Dynamic Graph
In this section we consider the de nition of Gt , a dynamic graph G at time t. We consider discrete time applications where new sets of nodes and edges corresponding to the transactions from time step t to t + 1 only become available at the end of the time step, for example once a day. Associated with every edge is a weight, w( ) 0, that is derived from an aggregation function applied to all (directed) transactions between 7

a pair of nodes at time step t. For example, the aggregation function can be the \total duration of calls" or the \number of calls" from one node to another. We rst de ne the sum of two graphs g and h
G= g h

where and are non-negative scalars. The nodes and edges in G are obtained from the union of the nodes and edges in g and h. The weight of an edge in G is w(G) = w(g) + w(h); where the weight of an edge is set to zero if the edge is absent from the graph. Let the graph corresponding to the transactions during time step t be gt . We can de ne Gt from gi where i = 1; : : : ; t in several ways, depending on the purpose to which the graph is intended to be used.

3.1 Summarizing historical behavior
The cumulative behavior of the graph through time can be de ned as

Gt = g

1

g2 : : : g t =

t M i=1

gi = Gt?1 gt ;

(1)

This de nition of Gt includes all historic transactions from the beginning of time. The last expression on the right hand side illustrates that for computational purposes, the cumulative summary at time t is the sum of the cumulative summary at time t ? 1 and the network activity at time step t. An alternative de nition that considers only recent network activity, say the last k time steps, is the moving window de nition

Gt = gt?k gt?k

+1

: : : gt =

t M

This de nition of Gt tracks the dynamics of the transactional data stream, and can be thought of as a characterization of network behavior at time t ? k=2, the center of the moving window. In contrast to Eq. (1), this de nition requires storage and retrieval of graphs characterizing network activity at each time step.

i=t?k

gi

(2)

3.2 Predicting future behavior
Gt can be thought of as a predictor of network activity at time t + 1. The simplest such prediction is Gt =
gt , the network graph corresponding to the transactions at time step t. The stability of this simple predictor

can be improved by blending in network activity in a way that discounts the past in favor of recent behavior:

where the weights !i satisfy !i = 1 and are an increasing function of i. Eq. (2) can be expressed in this form with !1 = : : : = !t?k?1 = 0 and !t?k = : : : = !t = 1=k. A particularly convenient form of the weights 8

P

Gt = ! g

1 1

!2 g2 : : : !t gt =

t M i=1

!i gi

(3)

800 Edge wt for a 60 minute call (in seconds)

600

 = 0.75  = 0.8  = 0.85  = 0.9  = 0.95

400

200

0 1

 = 0.1 2 5 10 Days 20 50 100

Figure 4: Contribution of a 60 minute call to edge weights as a function of time steps (days) in the recursive expansion (3) of Gt . The horizontal line at = 0:1 denotes an adjustable threshold whereby edges with weights less than this value are deleted. is !i = t?i (1 ? ) where 0 1 is a (scalar) parameter that allows more ( near 1) or less ( near 0) history to in uence the current graph. Figure 4 displays this graphically. If processing occurs daily, then with a value of = 0:85, the edge weight associated with a 60 minute call will be e ectively reduced to that of a 6 second call in about 30 days. This form of weight function is convenient in the sense that Eq. (3) can be expressed in recurrence form: Gt = Gt?1 (1 ? )gt : (4) This form is well-known in statistics as exponential smoothing 17]. It provides a smooth dynamic evolution of Gt without incurring the management and storage of graphs for many previous time periods. All that is needed is the graph through time period t ? 1 and the new set of transactions de ned by gt . In the following we adopt Eq. (4) as the de nition of a dynamic graph at time t. It's usefulness as a smoothing operator and as a prediction make it suitable for a wide range of applications.

4 Approximating Gt
The graph de ned by Eq. (4) is complete in the sense that it captures the entire graph, including edges with in nitesimally small weights. If such edges are maintained, the graph will eventually grow to be highly connected. This connectivity is undesirable as it preserves dated relationships that may be misleading (e.g., behavior 6 months ago may not be representative of current activity) or invalid (e.g., when accounts are closed their node labels are often reassigned, as is the case with telephone numbers and IP addresses). In addition to these considerations, the entire graph of the size we consider can be unwieldy, especially if it is 9

too large to t into main memory. In this section we propose approximations to the entire graph that enable sophisticated applications without compromising it's integrity. Section 2 demonstrated that over a 6 month period, the vast majority of nodes exhibit a low degree of connectivity. We propose an approximation to the entire graph that exploits this sparsity. The key to our approximation is the introduction of new aggregator edges to the graph that summarize edges that are eliminated for one of two reasons:
Global costs. Either the weight associated with an edge is too small, in an absolute sense, to justify the overhead of maintaining that edge, or Local costs. The weight associated with an edge is too small relative to other edges coming in or out of a node to justify the overhead of maintaining that edge.

The new aggregator edges are introduced at the subgraph level. For each node in Gt , consider the subgraph consisting of that node and the directed edges to its immediate neighbors. A new outbound aggregator edge e ectively replaces a subset of outbound edges of this subgraph such that it contains the same total weight of the edge subset. The node label on the terminating side of this edge is simply called other. A new inbound aggregator edge applies to a subset of inbound edges. The subsets of edges that are removed can be parameterized by a pair of thresholding functions, one applying to global thresholding of edge weights, and the other to local thresholding of edge weights. We rst describe the global thresholding function. In practice, edges in Gt with extremely small weights carry a disproportionate amount of overhead in maintenance and storage of the graphs relative to the information that they contain. Such edges come about from new calls with unusually small weights (e.g., realizing that one dialed a fax number instead of a voice number) or from old calls that had meaningfully large weights initially, but have decayed through time by exponential weighting. We apply a thresholding function to each edge in Gt such that all edges with weights less than a constant are eliminated prior to storing the updated graph. In our applications we use = 0:1. If edge weights re ect call durations (in seconds) then = 0:1 coupled with = 0:9 means that a one second call lasts one day in the updated graph (since w(e)=0:9 0 + 0:1 1ses). Alternatively a 60 minute call persists in the graph for 78 days (since w(e)=0:978 0:1 3600sec < 0:1). The local thresholding function that we use applies indirectly to the value of the edge weights. For a node n with outbound (inbound) edgeset fe : ei ; i = 1; :::N g, we retain only the top-k outbound (inbound) edges where \top" is relative to the value of the weight associated with each edge: top-kfeg = fe : !j (e) > ! N ?k] (e)g (5)

where ! i] (e) is the ith order statistic of !(e). This type of thresholding function leads to possible asymmetry in the sense that an edge eij : ni ! nj might be retained in the top-k outbound edgeset of ni but not in the top-k inbound edgeset of nj . This would be the case for example if ni corresponds to a \normal" residential 10

1.0

0.8 % accounts with k slots

0.6

0.4

0.75 0.8 0.85 0.9 0.95

0.2

0

5

10

15 Number of Slots (k)

20

25

30

Figure 5: Cumulative proportion of accounts that had k distinct outbound edges at the end of a 90 day study period. account and nj a toll free number for a large retailer. For all nite k, the only invariance between the complete graph and its top-k approximation is that the sum of the outbound edge weights equals the sum of the inbound edge weights, where the sum includes all nodes labeled as other. Considerations for selecting a value of k are discussed in the next subsection. In our experience we prefer a relatively small value of k that balances computational complexity (e.g., as regards speed and storage) with empirically determined accuracy (see below).

4.1 Tuning the approximation
The de nition of Gt given by Eq. (4) requires a value of that governs the degree to which new nodes and edges are blended in with recent activity. For top-k approximations, interplay between and k determines ^t and the degree to which it captures the evolving network graph. In theory, is also both the size of G a parameter which a ects the graph, however we choose to hold = 0:1 for its nice interpretation stated above, that a one second call lasts about one day, and any edge weight less than one second will be below threshold. In this section we explore the relationship between and k in top-k approximations. In practice, our recommended approach is to rst settle on a value of that makes sense for the service being modeled, and for that , choose k so that interesting detail on most of the nodes is adequately captured. We explore values of in the range 0.75 { 0.95. Reference to Figure 4 indicates that for this range of values, a one hour call persists in the evolving network graph from several weeks to several months. In our applications, where phone numbers can be reassigned 30 days after service is discontinued, a value of > 0:95 would lead to contamination of the new accounts subgraph with activity from the previous account. Smaller 11

1.0

% accounts with 90% of weight in top k slots

0.8

0.6

0.4

0.75 0.8 0.85 0.9 0.95

5

10 Number of Slots (k)

15

20

Figure 6: Cumulative proportion of accounts that have at least 90% of their edge weights captured by k edges. values of force more dynamics, with the result being that for many accounts with infrequent or sporadic usage, their subgraph is not adequately captured. We illustrate these points with several plots derived from the sample of 1,000 accounts introduced in Section 2. The graphs for these accounts were evolved using a range of values of , and the status of the accounts at the end of the period were used. Figure 5 shows the cumulative proportion of accounts that had k distinct outbound edges at the end of the 90 day study period. The gure shows that for = 0:90, 80% of all accounts have at most 15 distinct edges, and 90% of all accounts have at most 25 distinct edges. If edge preservation was critical in an application, choosing a value of k = 15(25) would lead to approximately 80% (90%) of all nodes having all their edges preserved by the top-k approximation. The curves displayed in Figure 5 relate to the presence or absence of edges without regard to the weights on those edges. An edge that corresponds to daily one hour calls between a pair of accounts is treated identically to an edge that resulted from a call lasting 1 second on the last day of the study period. The only role that edge weight played was that an edge was deleted if its weight dropped below = :1. Figure 6 addresses this issue by displaying the cumulative proportion of accounts that had at least 90% of their edge weights captured by k edges. If the weight function represents the number of calls, then the curves represent the cumulative proportion of accounts that had at least 90% of their calls captured by k distinct edges. If the weight function represents the length of calls, then the curves represent the cumulative proportion of accounts that had at least 90% of their \time on network" captured by k distinct edges. The gure shows that for = 0:90, 80% of all accounts have 90% of their edge weights captured in 5 distinct edges and 90% of all accounts have 90% of their edge weights captured in 8 distinct edges. If weight preservation was critical in an application, choosing a value of k = 5(8) would lead to approximately 80% (90%) of all nodes having 90% of their edge weights preserved by the top-k approximation. 12

0 B B B B B B B B B @

Old top-

k

edges:

node{labels

wts 5 2

X 5467 X 2656 X 4132 X 4231 X 3142 X 4212 X 1423 X 2312 X 4532
other

: 5:0 4:5 2:3 1:9 1:8 0:8 0:5 0:2 0:1

1 C C C C C C C C C A

+ (1

?

)

0 B B B B B B B B B @

Today's edges: node{labels wts 2 0

X 5467 X 2656 X 4132 X 6547

: 6:2 0:8 10:0

1 C C C C C C C C C A

=

0 B B B B B B B B B @

New top-

k

edges: wts 5 2

node{labels

X 2656 X 5467 X 4132 X 4231 X 3142 X 4212 X 6547 X 1423 X 2312
other

other

0 0

:

: 4:6 3:9 2:0 1:6 1:5 1:5 0:7 0:4 0:3

1 C C C C C C C C C A

Figure 7: Computing a new top-k edge set from the old top-k edge set and today's edges. Note how a new edge (X6547) enters the top-k edge set, forcing an old edge (X4532) to be added to other.

4.2 Implementing the approximation
We propose a constructive approach to implementing our approximation to a large time-varying graph. Consider a node in the graph, its associated directed edges, and weights associated with each edge. A data structure that consists of these weighted directed edge sets for each node is a representation of the complete graph. This data structure is redundant since it is indexed by nodes so that edges must be stored twice, once for the originating node and once for the terminating node. In contrast, a data structure that stores each edge once must be doubly indexed by nodes. The cost of edge duplication is often mitigated by gains in processing speed when subgraphs around nodes are expanded (see next subsection). For this reason we have chosen to represent our graphs as a singly-indexed list of nodes, each with an associated array of weighted directed edges. The singly-indexed node list represents our approximation but the de nition in Eq. (6) implies that new ^t?1 denote the top-k approximation to Gt?1 at activity must be blended into it at xed time steps. Let G time t ? 1 and let gt denote the graph derived from the new transactions at time step t. The approximation ^t?1 and gt , node by node, using a top-k approximation to Eq. (4): to Gt is formed from G (1 ? )gt g (6) ^t?1 (1 ? )gt . Then for each node we sort Thus, we rst calculate the edge weights for all the edges of G the edges according to their weight. (The over ow node other is not given any special treatment in these computations.) The top-k are preserved, and if there are more than k edges in the edge set for that node, the weights of the remaining edges are added to the weight of the edge going from the node to node other. These operations are displayed pictorially in Figure 7 using = :85. Notice that a new call today with a heavy edge weight (labeled X6547) replaces an old call with a low edge weight (labeled X4532). Between updating steps, transactions need to be collected and temporarily stored. At the end of that time period, the transactions are aggregated and the subgraph updated. The length of the time period represents a trade-o in accuracy: the longer the time period, the better an estimate of the top-k edge set,
1

^t = top-kf G ^t? G

13

but the more outdated the resulting subgraph. In the application discussed in Section 5, we perform daily updates, thereby maintaining reasonable accuracy while requiring temporary disk space for only one day of data. For a related discussion see Cortes and Pregibon 5]. Prior to storing the updated graph, we remove all edges that fall below the threshold. As argued above, this reaping process results in both a smaller and a more interpretable graph. For our large network graph this thresholding results in a 50% reduction in size, from 14gb (maximum size at saturation of top-9 approximation) to 7gb (stable size with average of 4.5 slots per indexed node).

4.3 Subgraph expansion
Our implementation of the subgraph consisting of the top-k inbound and the top-k outbound edges of a node is ideal for fast extraction of larger subgraphs centered on the node. The data structure containing the top-k approximation can be queried recursively for each node in the top-k edge sets of the center node. We grow the subgraphs in a breadth- rst traversal of the data structure. For notational purposes, we denote the top-k inbound and outbound nodes and edges of node n by R1 (n), the subgraph of radius 1 centered at node n. Similarly let R2 (n) denote the subgraph of radius 2 centered at node n. Note that R2 (n) can be formed from the \union" of R1 (n) and the radius-1 subgraph centered on each node contained in R1 (n). In general, we can de ne subgraphs of any size using the recursion
R1 (node) node2Rj (n) We use the quoted term \union" and the symbol ] instead of simply because of the aforementioned potential asymmetry in the edge weights. The top-k approximation may force an edge from/to a high activity node to be partially, or maybe even completely, absorbed by category other. However, a low activity node that is connected to a high activity node is likely to preserve the edge in its top-k approximation. In cases where both nodes preserve a common edge in their top-k list, the weights associated with that edge are potentially di erent for the two nodes. In those cases we use the maximum edge weight in the de nition of Rj+1 (n), since the maximum represents the weight least a ected by the top-k approximation. The index structure of our representation is critical since we often need to compute and compare many subgraphs on a daily basis. We have tuned our algorithms so that the average time for retrieving and rendering R2 (n) subgraphs from our data structure of several hundred million nodes is just under one second (on a single processor). In our applications, we rarely explore edge sets greater than R2 (n), as the edge sets become unmanageably large and remarkably uninformative. Subgraph expansion also reinforces arguments suggesting that a large value of k (in de ning the top-k approximation) is not necessarily desirable when network graphs are used to study relationships between nodes. Spurious edges (e.g., misdialed numbers) can have unintended consequences upon subgraph expansion. We also discuss this further in Section 6 where we suggest additional edge pruning in R2 (n) to further reduce \clutter" in a subgraph. Rj+1 (n) =

]

14

5 Application
In the telecommunications industry, there are many di erent types of fraudulent behavior. Subscription fraud is a type of fraud that occurs when an account is set up by an individual who has no intention of paying any bills. The enabler in such cases involves either awed processes for accepting and verifying customer supplied information, or identity-theft where an individual impersonates another person. In either case, if left undetected, the fraud is typically only discovered when the bill is returned to sender, often after thousands of dollars have been lost. Since speed is critical in reducing losses due to fraud, it is essential to essess inherent riskiness as each new account is activated on the network. In this section we explore this possibility by de ning a procedure that assesses risk on the basis of a node's connectivity to other nodes.

5.1 Subgraph-based account linkage
Consider the case where we have information on an account that was recently disconnected for fraud. If identity theft was the root cause of the fraudulent account, we might expect this same individual to appear with a new account bearing a di erent name and address. Basically the fraudster has now assumed the identity of a new victim. We attack this problem with the intuition that while the subscription information is not useful for matching network IDs to the same individual, the calling patterns of the new account, as characterized by its R2 subgraph, should not change very much from the previous account. Figure 8 provides an illustration where we overlay two R2 subgraphs apparently belonging to the same individual. The central nodes of the two subgraphs are indicated by the rectangle with two ID numbers. One of these numbers corresponds to a known fraudulent account, the other to a new account on our network. The amount of overlap between the two subgraphs is strong evidence that these numbers are related and increases the fraud risk of the new account. Subgraph-based account linkage is a non-standard problem that involves de ning a distance function to quantify the closeness of two accounts based on subgraphs centered on the two accounts. The distance between two subgraphs depends on both the quantity and the quality of the overlapping nodes. The quantity of the overlap can be measured by the percentage of overlapping nodes. However all overlapping nodes are not equally informative, so a measure of quality is needed as well. Many graphs will intersect at high-use nodes, such as large telemarketing rms or widely advertised customer service numbers. An informative overlapping node is one that has relatively low in- and out-degree, and in the best case, is shared only by the nodes under consideration for a match. We now describe a measure that captures these notions. Given a new account a and known fraudulent account b, let O = Rj (a) \ Rj (b) denote the set of all overlapping nodes in the two radius j subgraphs. We de ne Overlap(Rj (a); Rj (b)) =
o2O wo dao dbo

X wao wbo

1 1;

where wo is the overall weight of node o (the sum of all edge weights in R1 (o)), wao is the weight of edges 15

XXX2549490

XXX6721477 XXX9749066 XXX2320800 XXX3657504 XXX4243460 XXX3190804XXX9724031 XXX9350316 XXX3584547 XXX6835622 XXX9375596 XXX2965398 XXX4456363 XXX2057301 XXX9724301 XXX9687354 XXX6360824 XXX8534309 XXX9241699 XXX2768275 XXX9350055

XXX9724032

XXX4762258

Figure 8: Subgraph-based account linkage. The two individual subgraphs are superimposed (in the doublylabeled rectangle) to emphasize their similarity. Solid lines indicate edges common to both subgraphs, while dashed lines indicate edges belonging to only one subgraph.

100

80 % Correct matches

60

40

20

0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Predicted probabilities of a match 0.8 0.9 1.0

Figure 9: Success of subgraph matching. Observed percentage of matching node-pairs versus deciles of predicted matching probability.

16

between node a and node o in Rj (a), and dao is the minimal distance from node a to node o in Rj (a). The terms wbo and dbo are de ned similarly. In the case where dao > 1, it is not clear what the weight wao should be, since there is no direct edge between the two. For this application we elected to assign a small default weight in order to minimize the e ect of these overlaps]. Intuitively, the numerator measures the strength of the connection from a and b to the overlapping node, while the denominator corrects for an overlap node which is either common to many nodes or is further in the graph from a or b. This measure is large for overlapping nodes that have strong links to the nodes of interest, but otherwise have low overall volume. Even armed with this de nition of distance, subgraph-based matching is computationally di cult because of the dynamic nature of our network data as described in Section 2 { each day we see tens of thousands of new accounts. For each of the new accounts, we need to compute it's subgraph, and then the distance from it to the subgraph of all recently con rmed fraudulent accounts. Assuming for these purposes that we maintain a library of the most recent 1000 fraudulent accounts, tens of millions of pairwise distances need to be computed daily. We have harnessed the computations by maintaining R2 subgraphs for all accounts in our \fraud library" and computing R1 subgraphs for all new accounts. We obtained a training set of paired accounts where investigators were able to determine whether the old and new accounts belonged to the same individual. We built a decision tree using the overlap score de ned above as a predictor along with several covariates obtained from the information provided by the subscriber. The decision tree produces a \matching" probability for any set of node pairs. Figure 9 shows the performance of the decision tree on an independent test set of paired accounts. For the sample of 1537 pairs that we validated, the gure shows the observed proportion of matching node-pairs for each decile of predicted matching probability. As the plot shows, account pairs with a high predicted probability of matching based on our methodology were indeed usually associated with the same individual.

6 Communities of Interest
As stated earlier, edge sets larger than R2 (n) can be unmanageably large and remarkably uninformative. To help reduce this \clutter" we often apply a thresholding function to the edge weights in the expansion of subgraphs, so that any edge whose weight is below the threshold need not be expanded. This threshold function is the simplest operator in a series of functions one can apply to an edge set to bring out what we call the Community of Interest, or COI, for a given node. In telecommunications, a COI operationalizes the notion of a \calling circle", the group of accounts around a speci ed account, where there is some unobservable relationship between them (i.e., personal/professional/familial interests) that motivates accounts to communicate with each other. Intuition suggests that when such a relationship exists, that nodes involved in the relationship will be linked and that the weights along these links will be larger than weights along links to nodes not sharing in the relationship. There is also the notion of diameter of a calling circle since one can discuss \immediate calling circles" as well as \extended calling circles". Our subgraphs 17

captures these notions in a primitive fashion through R1 (n) and Rj (n); j > 1 edge sets respectively, but in applications, the raw edge sets are often treated as the starting point in deriving a COI. There are several reasons why the raw edge sets are often not su cient for capturing COIs. They can be summarized into two main categories: spurious edges in Rj (n) missing edges in Rj (n) Spurious edges arise from the fact that while we posit an unobservable relationship between accounts that encourages communication between them, additional calls are captured in the data that can be totally outside the relationship. For example, misdialed calls are captured by the edge sets, as well as unwanted telemarketing calls. The e ect of such calls on the edge sets can be enormous, since expanding the edge set to the next diameter, brings in all the accounts linked to this spurious node, and arguably, these are conceptually far removed from an accounts calling circle. Missing edges arise in several ways re ecting realities associated with large graphs arising from transactinoal data. One important way that edges are missing relates to the fact that in many applications there are numerous service providers so that any single network carries only a fraction of the transactions. A related way that edges are missing concern the locations of the devices in the network topology that records transactions. By this we mean that transactions are recorded when they cross certain network elements where recording equipment is located, and the corresponding records are then sent to a central repository for analysis. But many transactions could occur \below" the recording point and subsequent analysis is blind to these transactions. An example is the seperation between local and long distance calls in telecommunications whereby the long distance carrier is blind to any calls on the local level. Similarly for internet tra c monitoring, data collection equipment at internet gateway routers is blind to TCP/IP tra c between computers behind that network gateway. In our applications of COI for large dynamic graphs we address these de ciencies by introducing aspects of the problem not captured in the available data. For example, we would like to discount edges that might have large weight due simply to a single long call, for example, to a customer support center. One way we deal with such nuances is to build a large edge set, and then nd the strongly connected component containing the node of interest. This creates a subgraph where every node can be reached from every other node. Another way of removing spurious edges is to apply a high threshold , which will mitigate the e ects of one-time non-representative calls. Application of a thresholding function and applying a strongly connected component algorithm are both examples of operators we apply to prune edges and nodes from Rj (n). Alternatively, if we believe that edges may be missing from the observed set of transactional records, we may want to insert pseudo edges between certain nodes in Rj (n). For example, local phone calls between accounts would not appear in tra c collected from a long distance network. Similarly since most networks, telephony or otherwise, exist in competitive 18

markets, the observed edge set collected from a single network is blind to tra c carried on a competitors network. If the notion of COI is meant to capture the existence of implicit underlying relationships, adding certain pseudo edges to competitor nodes is a reasonable approach to uncovering such relationships. As is clear from this discussion, the transformation of an edge set into a COI is not a science. One normally lacks a reference for calibrating the process so that feed-back from COI-based applications is often the only guidance.

7 Related Work
The analysis of directed graphs goes by many names and has been studied in many elds, including sociology 16], epidemiology 10], information retrieval 15], statistics 3], operations research 13], and software engineering 12]. Clustering is a common goal in these elds, and is similar in spirit to our approach of de ning communities of interest. Arguably the oldest research eld in this area is the eld of social networks. Social networks model the interdependencies between \actors" or \agents" in a data set by analyzing the relationships between them, represented as edges in a graph. This type of analysis has grown to study such diverse topics as disease transmission, international trade, and computer networking. Social network theory can incorporate complex stochastic models, explanatory variables for both nodes and edges, and time dependent graphs. However, the eld has always focused on the study of small graphs. A popular textbook in the eld, Wasserman and Faust 16], contains ve datasets used to illustrate the methodology, the largest of which contains 32 nodes. The mathematically complex and computationally intensive methods generally do not scale, and to date, we have not used them in our research. Flake, Lawrence, and Giles 7] provide a de nition of communities of interest in terms of number of edges connecting a set of nodes that has the nice property that the COI can be e ciently enumerated by applying a maximum ow algorithm. Citation analysis of scienti c papers also aims at nding communities in large graphs. A distance between two documents is de ned using co-citation (the number of citations in common) or bibliographic coupling (the number of times both works are cited in other papers). Using this distance measure, clusters in the database can be found. A successful example of this work is the NEC Research Index 11] (http://citeseer.nj.nec.com/cs), which currently documents and cross references 7 million scienti c works, and for each of those works, lists the most similar books by several di erent metrics. The Internet is natural to treat as a massive graph, where web sites are nodes, and the links between them are edges. Current research 8, 9] uses \hubs" (sites that link to many others) and \authorities" (sites that are linked to by others) in order to identify clusters in the web that deal with a particular topic. Extensions of this work use network ow algorithms 7] and these are quite e ective in nding small subject clusters. Marketing has also inspired analysis of large graphs. The active research topics of market basket analysis 1], viral marketing 6], and collaborative ltering 14] all use graph algorithms to discover communities of 19

consumers with similar behavior. These popular methods have been used successfully at sites like Amazon.com, which suggests items to purchase based on purchases of other customers who recently purchased the same item. Despite the wealth of research into large network graphs, our research is unique in combining the following attributes:
Scale. Our network graphs contain hundreds of millions of nodes, and we are potentially interested in retrieving local subgraphs for any one of them. Speed. Our data structure, accessed recursively, along with o -line processing, allows us to compute subgraphs centered on any node of the graph in fractions of a second. Dynamic updating. The graph incorporates the continuous stream of incoming data, so that any analysis is as recent as the most recent data. Time is a crucial element, since today's network may contain tens of thousands of new nodes and edges than yesterday's graph did. Our exponential updating creates a smoothed view of network behavior, with the largest weights on the most recent events and the smallest weights on the oldest events. Condensed representation of the graph. Conceptually, we view a massive graph as the union of a massive number of small graphs R1 (n). The approximation we employ that limits node degree to the top-k is e ective because large dynamic graphs seem to be sparse as well.

Another appealing aspect of this work is that our applications measure direct interaction between the nodes. Accounts form a community by actually communicating, creating a richer basis on which to de ne clusters. In collaborative ltering or market basket analysis, the goal is to nd indirect links between people. Two people are similar not because of a direct interaction, but because they both purchased similar items. Similarly, large scale web mining explores static links between pages, but not user tra c along those links.

8 Conclusions
In this paper we introduced the concept of a dynamic graph and motivated the concept with network data from a sample of AT&T residential accounts. This led to our de nition of a dynamic graph Gt (at time t) as an exponentially weighted average of the previous graph (at time t ? 1) and a new network activity. We introduced a data structure that can be used to capture the evolution of a graph through time that was amenable to the exponential weighting scheme. This data structure allows the subgraph around any particular node to be quickly and e ciently expanded to an arbitrary diameter. An application was introduced that capitalized on this feature. We have concentrated on the computational aspects of building and evolving the data structure for real applications. We have not explored the statistical aspects of treating our data structure and the associated 20

^t (k) to the true graph Gt where k denotes the size of the top-k algorithm for traversal as an approximation G edge set maintained in the data structure. Similarly models and methods in social networks, while not applicable to massive graphs, are applicable to R2 (n) edge sets. These models might provide the rigorous justi cation for transforming edge sets into COI that we currently lack. We hope to explore these ideas in the near future. Another topic for further research is how to prune a subgraph so that only informative edges and nodes are retained. A common approach from (static) graph theory is to extract the strongly connected component. The strongly connected component algorithm has the advantage that it scales linearly in the order of nodes, and we have used it in our computationally intensive applications. However, we feel that certain features inherent to telecommunication networks such as asymmetric edges (due to some customers subscribing to a competitor), sinks (toll-free calling) and sources (large corporations), makes strongly connected components a less than ideal choice for pruning subgraphs Rj (n). Initializing, storing, and updating the data structures that we employ are facilitated by the programming language Hancock, 4]. Hancock is a domain-speci c C-based language for e cient and reliable programming with transactional data. Hancock is publicly available for non-commercial use at
http://www.research.att.com/~kfisher/hancock/

.

References
1] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Jorge B. Bocca, Matthias Jarke, and Carlo Zaniolo, editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB, pages 487{499. Morgan Kaufmann, 1994. 2] Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. Science, 286:509{ 512, 1999. 3] David Maxwell Chickering and David Heckerman. E cient approximations for the marginal likelihood of bayesian networks with hidden variables. Machine Learning, 29(2-3):181{212, 1997. 4] C. Cortes, K. Fisher, D. Pregibon, A. Rogers, and F. Smith. The Hancock language for signature processing. In Proceedings of the Sixth International Conference on Knowledge Discovery and Data Mining, 2000. 5] C. Cortes and D. Pregibon. An information-mining platform. In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining, 1999.

21

6] Pedro Domingos and Matt Richardson. Mining the network value of customers. In Proceedings of the Seventh International Conference on Knowledge Discovery and Data Mining, pages 57{66. ACM Press, 2001. 7] Gary Flake, Steve Lawrence, and C. Lee Giles. E cient identi cation of web communities. In Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 150{160, 2000. 8] David Gibson, Jon M. Kleinberg, and Prabhakar Raghavan. Inferring web communities from link topology. In UK Conference on Hypertext, pages 225{234, 1998. 9] Jon M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604{ 632, 1999. 10] Alden S Klovdahl. Social networks and the spread of infectious diseases, the aids example. Social Science and Medicine, 21:1203 { 1216, 1985. 11] Steve Lawrence, Kurt Bollacker, and C. Lee Giles. Indexing and retrieval of scienti c literature. In Eighth International Conference on Information and Knowledge Management, CIKM 99, pages 139{146, Kansas City, Missouri, November 1999. 12] S. Mancoridis, B. S. Mitchell, C. Rorres, Y. Chen, and E. R. Gansner. Using automatic clustering to produce high-level system organizations of source code. In IEEE Proceedings of the 1998 Int. Workshop on Program Understanding (IWPC'98), 1998. 13] A. Ravindran, D. Phillips, and J. Solberg. Operations Research-Principle and Practice. John Wiley and Sons, New York, USA, 1987. 14] P. Resnick, N. Iacovou, M. Suchak, P. Bergstorm, and J. Riedl. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Proceedings of ACM 1994 Conference on Computer Supported Cooperative Work, pages 175{186, Chapel Hill, North Carolina, 1994. ACM. 15] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, New York, NY, 1983. 16] Stanley Wasserman and Katherine Faust. Social Network Analysis. Cambridge University Press, 1994. 17] P. R. Winters. Forecasting sales by exponentially weighted moving averages. Management Science, 6:324{342, 1960.

22

Holistic Management of Sustainable Geo-Distributed Data Centers
Zahra Abbasi
Ericsson Research, San Jose, CA zahra.abbasi@ericsson.com

Sandeep K.S. Gupta
IMPACT Lab, Arizona St. Univ., Tempe, AZ sandeep.gupta@asu.edu power infrastructure [2][4]. Global workload management, i.e., intelligently distributing the workload across data centers according to their electricity price ($/J) and carbon footprint (CO2 /J) at a given time, can also be of significant aid to shave the peak power drawn without requiring large-scale ESDs. Although energy buffering and global workload management have been considerably studied in the literature [5][9], the solutions designed so far are piecemeal in the sense that each of which addresses only some aspects of the problem. In particular, prior work has independently considered (i) electricity cost minimization and carbon footprint capping through an intelligent global workload management, and (ii) peak power cost reduction via energy buffering. We argue that there is a need for a holistic approach, which combines all the available leverages and concurrently optimizes the potentially conflicting objectives. Accordingly, we propose a new holistic global workload management for large-scale Internet services running in geo-distributed data centers. Such a holistic management, however, introduces new challenges. First, the optimal solutions for the peak power cost minimization, energy buffering management and carbon capping can be only found offline. Prior online algorithms are designed to manage each (or two) of the aforementioned objectives separately, disregarding their implications on each other. Particularly, a window based predictive scheme, efficient for online management of peak power shaving [3], fails to competitively manage carbon capping with respect to the offline solution. This is because adjusting the carbon cap for each prediction window is difficult considering the intermittent nature of the available renewable energy. We use a combination of window based predictive scheme and T-slot Lyapunov optimization to jointly manage the energy cost (electricity and peak power cost) and the carbon footprint. The idea is to leverage the variability of data center parameters within the time frame T (e.g., a day) in order to smoothen the peak power drawn, and utilize Lyapunov technique to adjust the desired carbon footprint for each time frame over the entire budgeting period (e.g., a year). We hypothesize that such a solution achieves near optimal cost saving given the limited capacity of energy storage devices, and the daily variability of data center parameters. Nevertheless, the efficiency of the previously discussed solution depends on the prediction accuracy of data center parameters over T , e.g., workload, electricity prices, and the available renewable energy. In particular, the prediction error has a very harmful impact on the peak power cost. The reason is that the optimal approach is to utilize the data centers with low electricity cost as much as possible without

Abstract--This paper designs a holistic global workload management solution which explores diversities of a set of geo-distributed data centers and energy buffering in order to minimize the electricity cost, reduce the peak power drawn from utilities while maintaining the carbon capping requirement of the data centers. The prior work often designed solutions to address each of the aforementioned energy and cost optimization separately, disregarding the possible conflicts between the solutions' objectives. We propose a holistic solution to concurrently optimize the aforementioned potentially competing objectives. The proposed solution combines the techniques from Lyapunov optimization and predictive solution in order to manage the tradeoffs of electricity cost and carbon footprint reduction, and electricity cost and peak power cost reduction, respectively. The predicted data center parameters, being a significant aid to near optimally manage energy buffering and smoothing data centers' peak power draw, adversely affect the peak power cost due to the parameters' prediction error. The proposed holistic solution adapts stochastic programing to take the predicted parameters' randomness into consideration for minimizing the harmful impact of the prediction error. Our trace-based study confirms our analytical result that our holistic solution balances all the tradeoffs towards achieving energy and cost sustainability. Also our solution removes up to 66% of the prediction error impact in increasing the cost. Keywords--data centers, cloud computing, peak power, prediction error, carbon capping, electricity cost.

I.

Introduction

Internet data centers, typically distributed across the world in order to provide timely and reliable Internet service, have been increasingly pressurized to reduce their carbon footprint and electricity usage. Particularly, data centers will soon be required to abide by carbon capping polices which limit their maximum carbon footprint emission to encourage brown energy conservation [1]. Huge monthly electricity bill and costly power infrastructure of these data centers are other big concerns to the operators. Data centers spend 10 to 25 dollar per watt in provisioning their power infrastructure, regardless of the Watts actually consumed [2]. Since peak power needs arise rarely, provisioning power infrastructure for them can be expensive. Further, some utilities penalize data centers for their peak power consumption in addition to charging for the energy consumed (i.e., $/W). Energy buffering management using energy storage devices (ESDs), e.g., existing UPSes, has been shown to be promising to shave the power demand, allowing aggressive under-provisioning of the
This research has been funded by NSF CNS grant #1218505.  The work was done when this author was with Arizona State University.

Holistic global workload management solution
Prediction model Historical data Prediction error Stochastic scenarios Workload model Prediction models
Predicted data over future time window

Data center model

Power consumption model

Power supply model

Optimization model of global workload management Electricity cost Peak power cost Carbon footprint

footprint across data centers (Section IV-C). OnCMCCLyp is proven to operate near offline optimal solution when T is sufficiently large and the predicted data are accurately available (Theorem 1). Next, we present our concluding holistic solution which adapts stochastic programming to model and solve the online solutions, in the presence of the parameters' prediction error (Section V). Finally, we perform a real-world trace based study to complement our analysis (Section VI).

Lyapunov control
Workload distribution and server management policies

Workload and data center parameters (e.g., electricity price)

II.

Related work

Front-end Data center Data Control data

Geo-distributed data centers

Fig. 1.

Holistic global workload management solution.

increasing their peak power. Under any under prediction of those data centers' workload, for instance, their peak power most likely increases, resulting in an unexpected increase in the peak power cost. Previous prediction based schemes of peak power management are performed without considering the impact of the prediction error [4], [10]. We propose to make use of stochastic programming approach in order to mitigate the sensitivity of the solution to the prediction error. Accordingly, we formalize the cost minimization problem taking into account the stochastic scenarios, each of which represents the future realization of the cloud random parameters (e.g., input workload). A huge number of scenarios are required to completely describe the stochastic nature of the uncertainties associated with the cloud input parameters. Solving the stochastic cost minimization problem with those huge set of scenarios is computationally too expensive. So an appropriate scenario reduction technique must be used to limit the number of scenarios. We make use of the previously proposed scenario reduction algorithms [11] along with some problem-specific heuristics to reduce the number of scenarios. We hypothesize that a stochastic approach using a small set of stochastic scenarios, achieves a comparable performance against the solution with accurate data over T . In summary we make the following contributions. We design a holistic global workload management solution which combines the potential benefits from a set of geo-distributed data centers to concurrently optimize the electricity cost, the carbon footprint and the peak power cost of data centers. As shown in Fig. 1, the holistic solution arranges and employs diverse set of models and techniques including predictive solution, stochastic programming and Lyapunov optimization to tackle energy management tradeoffs and enable coordination management of the energy cost and the carbon footprint. Throughout the paper, we incrementally enhance the holistic solution. We first frame the holistic global workload management problem as a linear programming (Section III), and develop a predictive solution, Online Cost minimization and Carbon Capping (OnCMCC), which utilizes T future slots' information. Next, we design a predictive Lyapunov based solution (OnCMCCLyp), which uses T -slot Lyapunov optimization technique to jointly minimize the cost and the carbon

There have been related efforts on reducing electricity bill and carbon footprint of data centers through workload management for a single [4][6], [8], [9] and a set of geodistributed data centers [1], [7], [12]. In particular, related work proposed includes [1] and Lyapunov based optimization [9], [12], [13] for joint optimization of the electricity cost and the carbon capping. The efficiency of Lyapunov based solutions heavily depends on the value of the Lyapunov control parameter. There are also some recent works which explored the use of existing UPSes or any ESDs to reduce both the energy cost and the peak power cost for a single [2], [4] [6], [8] and a set of geo-distributed data centers [10] without considering the carbon capping requirements of data centers. The related work, thereby, lacks a holistic solution to jointly manage the energy cost, the peak power cost and the carbon capping, a key solution for today's data centers to operate under carbon capping policies. This is important since such a holistic solution introduces new challenges which need to be addressed. Existing studies mainly used offline and predictive solutions for energy buffering management in data centers [4], [6], [8]. However, Lyapunov technique is used to exploit batteries in data centers for energy cost minimization [5]. The performance of the solution in [5] is based on restricting the maximum value of the Lyapunov control parameter, and the minimum required ESD capacity (which is relatively a large value). However, first, we seek a practical solution without requiring large scale ESDs to avoid their space and financial overhead. Second, the proposed solution only accounts for the energy cost. However ESDs can be best utilized to shave the peak power drawn, where its online management is shown to be effective when using window based predictive approach [3]. Third, using Lyapunov optimization for online management of both the carbon footprint and the ESD dynamics becomes a tedious task (if possible at all) since it requires a Lyapunov control parameter adjustment that optimally manages the two. Further, the existing solutions on data center peak power shaving rely on the predictability of data centers' parameters over a window of time [2][4], [10], lacking analysis/solution to overcome the harmful impact of the prediction error on the peak power shaving. [14] designed an algorithm for single data centers to utilize Diesel Generators in order to compensate the impact of the prediction error in increasing the peak power cost. We use stochastic programming approach, to incorporate the randomness of the predicted parameters into the decision making process. Stochastic programming has been successfully applied in many applications, particularly, in grid power management and renewable energy optimization [11], [15]. However, we are the first (to our knowledge) to apply it for data center energy and power cost optimization.

INPUTS FROM FRONT-ENDS:

Control data

Workload

INPUTS FROM DATA CENTERS:

Workload arrival rate ()
OUTPUT TO FRONT-ENDS:

Front-ends
1 2 3

Online global workload management
Data centers

Workload distribution policies ()

11

Electricity cost () Peak power cost () Available renew. energy (r) Power efficiency (p) Cooling efficiency (PUE) Carbon emission factor () Data center capacity (Y) OUTPUT TO DATA CENTERS: # of active servers (y) Charging/discharging of batteries (R/D)

1

lution, however, is only effective when the parameters can be accurately predicted (zero prediction error). Given non-zero prediction error of the parameters, we design our final solution by taking into consideration the randomness of the predicted data (Section V), following the roadmap of the holistic solution given in Fig. 1. IV. Optimization problem formulation

2

Fig. 2.

System model. TABLE I. Symbols and definitions. Sym. Definition g r  =  S   E d c D C  V bmax  R grid carbon emission renew. carbon emission carbon cap time-avg carbon cap electricity price ($/J) peak power cost ($/W) ESD capacity ESD discharge rate ESD charge rate ESD max discharge rate ESD max charging rate ESD energy inefficiency Lyap. control parameter per-slot max carbon ESD cost per charge/discharge feasible set of y and 

Sym. Definition t S T j i N Y   p g r X S p0  W slot index total # of slots time frame (T << S ) frontend index data center index # of data centers total # of servers data centers' workload frontends' workload per server power cons grid power renewable power virtual queue peak power billing period stipulated peak power prediction error rand. var. set of stochastic scenarios

Our optimization problem uses models characterizing the power demand, derived from the workload distribution model, and the power consumption model of servers, and the power supply model which consists of models to describe the power drawn from the grid, batteries and the on-site renewables. The formulation and models build on the models used by the related work e.g., [4], [12]. The key change we make to [12] is to combine models of energy storage devices, peak power cost, and carbon capping in order to design a holistic solution for cost management and carbon capping of data centers. Data Center Power Demand: Let  j (t) denote the average workload arrival rate at front-end j, our algorithm decides on the workload distribution of front-end j to data center i, denoted by i, j (t), and the number of active servers at each data center i, denoted by yi (t), subject to a set of workload requirements (e.g., delay requirement, availability of computation data) and data centers capacity. To model these requirements, we assume that i (t) = (i,1 (t), . . . i,j (t), . . . , i,M (t)), and the corresponding yi (t) must be drawn from a feasible set, +1 (i (t), yi (t))  RM (t). Our analysis works for any convex set i M +1 of Ri (t), which contains the constraints that i i, j (t)= j (t) (workload processing requirement), and yi (t)  Yi (upper bound of number of servers). RiM+1 (t), for instance, can also contain the constraint that i, j (t)=0 to represent the constraint that workload arriving at front-ends j cannot be processed at data center i due to the network latency or data availability. To solve our problem in Section VI, we use models in [1] which account for finding i , and yi based on M/M/n queuing model of data centers and an additive slack for number of severs to deal with workload spikes. Given RiM+1 (t), the average one-slot energy consumption of an active server, denoted by pi , can be obtained by profiling. tot Then ptot i , where, pi =yi,t pi estimates the total one-slot energy consumed by active servers in data center i. Data Center Power Supply: Data centers get their primary power from the grid. We perform our study under the dynamic pricing managed by the wholesale electricity market (e.g., north America). Under this model, the electricity pricing is dynamic, significantly varies over time and has seasonal daily, and monthly pattern. In addition to the energy actually consumed, some utility providers penalize the excess power draw: imposing additional fee if the peak power draw over a certain time window e.g., average power every 15 minutes [4], seen in a billing period (denoted by S ) exceeds the stipulated power (denoted by p0 ). Hence, we consider that the electricity price from the grid includes i (t), the usage price, and i , the surcharge per excess power draw over S from p0 . To model energy storage, we denote the energy storage level at time t by ei (t), and the charge/discharge energy during time slot t by ci (t) and di (t), respectively. There is a limit on the maximum charging and discharging rate denoted by

III.

System model and problem formulation

We consider a cloud, which consists of N geographically distributed data centers (see Fig. 2), where data center i has Yi servers. We assume servers have only two states: active and inactive. Data centers get their required power from a mix of grid, on-site solar and wind renewable energy sources, and Energy Storage Devices (ESDs). We assume ESDs can be charged either from the grid or the available renewables. We assume Internet workload for data centers, which typically exhibit daily and weekly seasonality and require timely services. End users' requests arrive from M geographically distributed front-ends (i.e., the sources), as shown in Fig. 2. The geographical front-ends may be network prefixes, or geographic groupings (states and cities). The workload management system operates in slotted time i.e., t = 0 . . . S - 1 for the budgeting period of S slots where the time slot length matches the timescale at which the server provisioning and energy storage charging/discharging cycle can be updated. In this paper we aim to design an online holistic global workload management which decides on the workload distribution and power management polices over every T slots, where T << S . In the following sections, we incrementally design and enhance our holistic global workload management solution to conclude our final solution as shown in Fig. 1. Given models to describe the workload, the power demand and the power supply, we first frame the global workload management as an optimization problem. We argue that the joint management of the electricity cost, the peak power cost and the carbon footprint requires a combined technique of predictive solution and Lyapuonv optimization (Section IV-C). The resulting so-

C and D, respectively. An ESD has limited capacity, further it is associated with a cycle-life i.e., the average number of charging/discharging cycles in the lifetime of the device for a given depth of discharge. Furthermore, data centers reserver some of ESDs' capacity for use during the power outages. Therefore, we denote E the capacity of the ESD which can be used to manage the energy cost and the renewable energy utilization without affecting the data center availability and without violating the given depth of discharge. We assume that the efficiencies of ESD charging and discharging are the same, denoted by   [0, 1], e.g.,  = 0.8 means that only 80% of the charged or discharged energy is useful. Energy level of an ESD over time satisfies the following: i, t : ei (t + 1) = ei (t) + i ci (t) - 1  i di (t)[ESD energy level], i, t : 0  ei (t + 1)  Ei , 0  ei (0)  Ei , i, t : 0  ci  Ci , 0  di  Di . (1) ESDs have some other physical limitations such as self discharge rate, which are ignored for notation brevity. Finally, in any slot, one can either recharge or discharge the battery or do neither, but not both. Hence, for all t and i we have: i, t : ci (t)di (t) = 0. (2)

the integer constraint of number of active servers (yi ) and round the resulting solution with minimal increase in cost. Also observe that P1 disregards the non-convex and non-linear constraint (2), however the following lemma asserts that the optimal solution to P1 never chooses to simultaneously charge and discharge from ESDs. This is intuitively clear, because charging and discharging the ESD in the same slot incurs additional battery cost and energy cost due to the battery inefficiency. It is, thereby, beneficial to instead satisfy the demand from the grid or do either charging or discharging. Lemma 1. The optimal solution to P1 for every data center i and time t always chooses ci (t)di (t) = 0. Lemma 1 can be proved by construction, which is deleted due to the space limitation (see [16, Lemma 7.1.1]). The problem P1 as described above is a linear programing (given a linear model for Rm+1 ) which can be optimally solved using the existing linear programming solvers. However, the solutions of P1 over time are dependent due to the several sources of coupling factors: (i) the peak power cost is calculated over every S 1 slots (5), as a result it couples the solutions over S , (ii) the ESDs' dynamics (1) and the carbon capping constraint (4) couples the solutions over time. In practice, the billing period (S ) is typically a month, and the carbon cap is typically given over a year of operation of the data centers. This means that S is typically equals to the number of slots for a year. Therefore, in practice, it becomes impractical to solve P1 due to the unavailability of data as well as "curse of dimensionality". In this paper, we study and propose online solutions to solve P1. The performance of the online solutions are based on (i) the feasibility assumption which ensures that P1 has non-zero feasible solutions, (ii) the bounded assumption which ensures that the total one-slot cloud's carbon footprint is bounded by bmax , i.e., b(t)bmax t, and (iii) the predictability assumption which ensures that the data center parameters are predictable over T slots with reasonable accuracy, and their most variabilities fall within T slots. Observe that, the assumptions are not constraining in practice, and that the last assumption is consistent with the daily variability of the data center parameters. B. OnCMCC: predictive online solution We design the online solution, namely OnCMCC, as a reference solution to solve the problem P1 over T S , where T consists of slots for one or more days (e.g., T =24 or T =48 for hourly basis slots). In this solution we also use  for the T peak power cost where  = S . OnCMCC is inspired by the observation that the variation of the data center parameters across days is usually lower than their variation across slots within days. Given the limited ESDs' sizes, the ESDs are most likely to be best utilized to leverage the daily variation of the data center parameters. The availability of renewable energy, however, not only significantly varies during days (solar energy is available only when sufficient sunshine is there), but also significantly varies during a days and even months in a year depending on the weather conditions and geographical locations. However, due to the limited size of ESDs and their physical limitations (e.g., self-discharge), it is impractical to migrate renewable energy across such long periods, making the cost optimality distance of OnCMCC

Consistent with today's data centers, we assume data centers get their power partially from the available on-site renewable energy (wind and solar) denoted by ri (t)  Ri . For every data center i and all time t the energy demand and supply should be balanced as follows: i, t : gi (t) + ri (t) + di (t) = ptot i (t) + ci (t), (3) i, t : gi (t)  0. Cloud's Carbon Cap: To incorporate the carbon capping, we consider that each data center is associated with carbon emission intensities for the power source from utility r denoted by g i (t) and its on-site renewable denoted by i (t) in unit of CO2 g/J. The total carbon footprint of the cloud, within r slot t can be written as follows: bi (t) = g i (t)gi (t) + i (t)ri (t). The cloud desires to follow the long-term carbon capping target, denoted by , which is typically expressed for a year of operation of a data center.: S -1 1  bi (t)  = , (4) S t=0 i S where  denotes the time-averaged carbon cap. A. Operational Cost Minimization and Carbon Capping We set renewable energy operational cost to zero to maximize their utilization. In addition to the energy cost and the peak power cost, we consider that the data centers' operational cost accounts for the cost per maximum charging and discharging denoted by i,C , and i,D , respectively which depends on the ESD characteristics (e.g., cycle-life). The timeaveraged operational cost of data centers over S slots, can be written as the following optimization problem, namely P1: minimize subject to
ci (t) di (t) S t=1 i gi (t)i (t) + C i,C + D i,D S /S -1 + t =0 max(t -1)S t S -1 (gi () - p0 )+ i +1 (i (t), yi (t))  RM (t) (1), (3), and(4). i 1 S

,

(5) To simplify the problem P1, note that Internet data centers typically contain thousands of active servers. So, we can relax

negligible when carbon capping requirement is relaxed. Note, OnCMCC can only satisfy the carbon cap in a best-effort manner. Due to the intermittent nature of the renewable power, therefore, OnCMCC may significantly violate the carbon cap, making it inefficient particularly when cloud needs to perform under a relatively tight carbon capping requirement (i.e.,  is comparable to that of the minimum carbon footprint possible). To avoid this problem, we extend OnCMCC to leverage the T -slot Lyapunov optimization in order to account for the dynamics of the carbon footprint. C. OnCMCCLyp: T -slot Lyapunov based solution In accordance with Lyapunov optimization, we define a virtual queue [17] with occupancy X (t) equal to the maximum excess carbon footprint beyond the average carbon footprint over every T -slot. Using X (0)=0, we propagate the X (t) values over every T -slot as follows:
t0 +T -1

Algorithm 1 OnCMCCLyp Algorithm
1: Initialize the virtual queue X 2: for every k=1 . . . KT =S , where t=kT do 3: Predict the system parameters over the window t+T -1 4: Minimize:

V +

1 T

ci (t) i gi (t)i (t) + C i,C + i maxtt+T -1 (gi () - pi,0 ) i

t+T -1 =t

+ -

di (t) D i,D +T -1 X (t) T =t

i bi ()

(9)

+1 (t), (1), and (3). Subject to: (i (t), yi (t))  RM i 5: Update the virtual queue X using (6). 6: end for

Similar steps of [12, Theroem 1] can be taken to prove the Theorem. From (8) and (7) it can be concluded that OnCMCCLyp achieves near optimal performance for sufficiently large value of S . According to Theorem 1, the OnCMCCLyp achieves average cost no more than a factor of O(1/V ) above the optimal average cost of P1 under the Theorem's conditions. The large value of V comes at the expense of an O(V ) tradeoff in achieving the carbon cap. V. Stochastic Programming Approach

X (t0 + T ) = max[X (t0 ) - T , 0] +
=t0 i

bi ().

(6)

Building upon Lyapunov optimization technique we design OnCMCCLyp as given in Algorithm 1. The parameter V in Algorithm 1 is the Lyapunov control parameter which manages the electricity cost versus the carbon footprint reduction tradeoff. OnCMCCLyp requires only T slots ahead information. The algorithm removes the coupling property of P1 by (i) removing the constraint (4)), and (ii) managing the energy storage dynamics over window (t, t+T -1) rather than S and managing peak power reduction over (t, t+T -1), rather than S . OnCMCCLyp uses T future slots information to manage the operational cost according to the variation of the parameters within the frame T and the Lyapunov technique to stabilize the carbon footprint dynamics across T -slots. In order to evaluate OnCMCCLyp, we theoretically compare its performance against the offline optimal solution of problem P1 for the case of (i) S =T , and (ii) the energy storage dynamics only depends on the window of T . In other words, we consider that the operational cost and energy storage can be optimally managed using T future slots information, and evaluate how OnCMCCLyp can manage the carbon cap (i.e., ) without excessively increasing the operational cost. Theorem 1. (Performance Bound Analysis of OnCMCCLyp): Suppose X (0)=0, and that the maximum carbon footprint of the cloud over every T slot is upper bounded by T bmax . Also define cost T as the optimal solution to the special case of problem P1, where S =T , and for every t0 the beginning slot in every frame T, we have ei (t0 + T )+ = ei (t0 ). Further, suppose data center parameters are i.i.d. over every T -slots, and let cost() and b() denote the OnCMCCLyp cost and the carbon footprint, respectively for slot . Then for V > 0, and the integer variable k = 0, 1, . . . K where S = KT we have the following: costT =
1 lim supK  K B   costT + V , K -1 k =0

The performance of the online solutions depends on the predictability of the parameters over T . We use stochastic programming to take into consideration the randomness of the predicted input parameters. The major issue in developing the stochastic problem formulation is modeling of the uncertainties. We characterize and model uncertainties in the form of scenarios (possible outcomes of the data), a typical scheme in stochastic programming approach [18]. The goal is to find a policy that is feasible for all the possible parameter realizations (scenarios), and optimize the expectation of the objective functions given the probability associated with each scenario. Stochastic programming has many variants including stochastic dynamic programming. Stochastic dynamic programming in our problem requires discretization of the ESD states for every data centers in the cloud. This causes the number of states at each stage of the stochastic dynamic programming to dramatically increase. We, instead, choose to incorporate the stochastic scenarios in the original optimization problem and design the "deterministic equivalent" of the stochastic problem which is a typical stochastic programming approach [18]. Consider a deterministic optimization problem of the objective function f , the constraint function of h, and the decision variable of x, i.e., minimize f ( x), subject to h( x). Its stochastic programming counterpart over set of stochastic scenarios of W can be written as follows: Minimize wW Pr (w) f ( x, w), (10) Subject to: h( x, w) w  W, where W denotes the set of scenarios, w denotes a scenario in W , and Pr denotes the probability function. To characterize the scenarios, we model the prediction error of the input parameters, i.e., workload of each front-end,  j , the available renewable power at each data center i, ri (t), the electricity price at data center location i, i , and the carbon intensity of the grid power at each data center i, g i . We denote ri (t) the actual renewable energy available to data center i at time t and use r ^i (t) for the predicted generation. We denote

E{

kT +T -1 =kT

cost()}

(7)

S -1

bi (t)   +
t =0 i

 2

K -1 kT +T -1  KB+V (KcostT - k =0 =kT

cost()), (8)

2 2 2 2 where B = 1 2 (T bmax + T  ).

Stage 1

Z=z1
One scenario

Z=z2

Z=z3

Stage 2

Z=z1 Z=z2 Z=z3

Z=z1 Z=z2 Z=z3

Z=z1 Z=z2 Z=z3

(a)

Fuel coal PL NG NE wind solar

CO2 / kWh 986 890 (b) 440 15 22.5 18

Fig. 3. (a) A sample scenario tree for the random variable Z over two stages, (b) carbon emission of electricity fuels (CO2 / kWh).

ri (t)=(1+i,r )^ ri (t) , where i,r is the prediction error. We assume unbiased prediction error, i.e., E(r )=0, and denote the variance by 2 r which can be obtained from historic data. These are standard assumptions in statistics. We use similar assumptions for the prediction error of the other input parameters. We also consider that the random variables (e.g.,  j, ) are independent random processes. As a result, the evolution of these stochastic processes is modeled as a multivariate random process. The marginal distribution for each of these random processes at any time step is assumed to be a normal distribution in accordance with the nature of unbiased prediction error. To define the scenarios, we approximate the marginal distribution of the random parameters (i.e.,  ) into discrete samples. The mulM N N N Lr L L samples at tivariate random process has therefore, L each time step, where L , Lr , L , and L denote the number of discretization levels used for the workload demand, the renewable power, the electricity price, and the grid carbon intensity, respectively. The evolution of the random process for the entire T slots is a huge set of scenarios. This type of uncertainty modeling results in a multistage "scenario tree" M N N N with T branching stages and L Lr L L samples at each node of the tree (see Fig. 3(a)). Each scenario (i.e., a path from root to a leaf of the tree) represents a possible future realization of the random process. Observe that the scenario tree for our problem is huge. For instance for T =24, N =5, M =10, and L =Lr =L =L =5, the number of scenarios is 5600 . To solve the stochastic model, the multivariate random process with huge set of scenarios has to be approximated to a simple random process with small set of scenarios and should be as close as possible to the original scenario tree. There have been several scenario reduction algorithms in the literature which typically make use of probability metrics to choose a subset of scenarios [11], [19]. The scenario to be deleted is selected by comparing each scenario with the rest of the scenarios. Accordingly, the process of one to one comparisons of the scenarios needs to be repeated several times, which may not be feasible for a huge initial scenario tree. For instance, the scenario reduction algorithms in [11] make use of algorithms very similar to "kmeans" and "k-medoids" where the probabilistic measures are used to evaluate the distance between the scenarios. Similar to k-means, these solutions can be implemented efficiently using parallel programming to run on a huge set of initial scenarios. As a general case, where running scenario reduction algorithms on the complete scenario tree may not be feasible, we can use problem-speccific strategies to generate a scenario tree with reasonable size as follows. Staregy one, use stochastic aggregation rules to reduce the number of initial input

random processes (e.g., workload). Consider the random processes X and Y with normal distribution, X  N(1 , 2 1 ), Y  N(2 , 2 ), then aX + bY , where a and b are constant 2 numbers, also has a normal distribution as follows, aX +bY  2 2 N(a1 +b2 , a2 2 1 +b 2 ). Data centers may use a combination of wind and solar energy where an aggregated random process of the two can capture their randomness. Also, in practice, number of front-ends (i.e., M ) is very large. Suppose, every front-end can get service from all the available data centers in the cloud. Then, the entire input workload of all frontends can be aggregated into one single random process. In practice, however, there are always some restrictions such as network latency (proximity of front-ends to the data centers) and data availability, where every front-ends can get service from a subset of data centers. In this case we can group frontends depending on their feasible destination data centers and aggregate the workload of each group. Strategy two, ignore random processes which have small standard deviation. By removing such processes we significantly reduce the initial scenario tree size with negligible impact in the solution. Following the model of (10), and given the set of scenarios W and the associated probability to each scenario w  W , denoted by Pr(w), we formulate the stochastic counterpart of the problem P1, namely P2. Next, we design our final holistic solution OnCMCCLypstoch , i.e., the stochastic counterpart of the online solution OnCMCCLyp based on P2. Following our road-map, given in Fig 1, OnCMCCLypstoch combines leverages form data center and workload prediction models, stochastic programing, data center power consumption and power supply models, and Lyapunov optimization to concurrently optimize electricity cost, peak power cost and carbon footprint. Similar to OnCMCCLypstoch the stochastic counterpart of OnCMCC, namely OnCMCCstoch is designed. VI. Evaluation

We simulate a cloud consisting of six data centers located at CA, TX, GA, IA, NC and VA, most of which correspond to Google's data centers' locations, namely DC1, DC2, DC3, DC4, DC5, and DC6, respectively. The data centers are assumed to be homogeneous in terms of power consumption and computing characteristics, such that all the electricity cost savings and the carbon footprint reduction only comes from spatio-temporal variation of the electricity cost and the carbon footprint. Servers in each of the data centers are assumed to consume 300 W at peak utilization, and average response time and server slacks is set such that active servers have average utilization of 75% (active servers consume 250 W at this utilization). We set the slot length to one hour, S and S to one month, and use realistic hourly traces of the electricity price (see Fig. 4(a) where data is taken from Locational Marginal Prices available at the corresponding RTO/ISO websites 1 , and carbon intensity from Fig. 3(b) and fuel mix of data center locations. We also use the renewable traces of http://www.nrel.gov/midc/ for three sites located in the data center locations of CA, TX and GA. To ensure data consistency, all traces are chosen from the month of July and August (see Fig. 5(a)). According to [4] a typical peak power cost is 12 $/KW per averaged power over 15-minutes slots.
1 negative prices happen on the power wholesale market when a high power generation plant meets low demand.

100 0 -100 1000

700
CA

40 20

1

600

$/MWh

TX GA IA

0

CO2/kWh

500 400

Workload arrival rate (request/ms)

50
0 50
NC VA

300
200 100
CA TX GA IA NC VA

0 40 20 0 40 10 0 40 10 0 0

2

3

4

0 0

200

Time (every hour)

400

600

0 0

200

400 Time (every hour))

600

100

200

300 400 500 Time index (hour)

600

700

(a)

(b)

(c)

Fig. 4. Hourly traces for one month : (a) electricity price (data taken from corresponding RTO/ISO website), (b) carbon emission, (c) workload (data taken from [20]).

Given our hourly basis slots we amortize  to 30 $/KW. In most of the experiments we set p0 as 80% of data centers maximum power consumption, unless stated. We consider four front-ends, corresponding to four timezones in the U.S., and use two months (July and August) of NASA workload Internet trace [20]. The workload of each front-end is scaled proportionally to the number of Internet users and shifted according to the time zone for each frontend in the corresponding area, as shown in Fig. 4(c). Each data center has 280 servers, and the intensity of the workload is such that at peak, 95% of servers in the entire cloud are required to be activated. We assume that a data center can receive workload from any of the front-ends. We also consider a relatively large capacity for ESDs which can sustain the data centers for an hour. The results therefore, report a pessimistic performance of the online solutions, as large ESDs leverage the variabilities of prices and workload both within T slots and across T slots to minimize the cost. The physical characteristics of ESDs are set according to data sheet of Flood Lead Acidic (FLA) batteries used in data centers. We use GNU Linear Programming Kit (GLPK) to solve problems P1, P2 and the online algorithms. Prediction results:: We use one month of training data (July traces) and build weekly and daily Seasonal Auto Regressive Integrated and Moving Average (SARIMA) prediction model (using "forecast" library of "R" package) to predict workload, and electricity prices and solar energy, respectively. Further, we use ARMA prediction model for wind energy. The lag one (one hour-ahead) prediction error is 14%, 20% and 25% for workload, electricity prices, solar and wind energy respectively. The error goes up to 20%, 40% and 54% for 24 lag (24 hour ahead) prediction of workload, solar and wind energy, respectively. Observe that the prediction error of both the solar and the wind energy in our data set is very high which can be typically improved using sufficient training data (using historical data of about 2-3 years [22]). Since, sufficient training data is not always available, we perform a pessimistic analysis on the impact of high prediction error on our solution, and the way that stochastic programming can remove its harmful impact. The prediction results of the electricity prices are very different across data centers. In particular, the electricity prices of DC4, DC5, and DC6 are predicted with relatively high accuracy, exhibiting error of 5%

for lag one and 15% for lag 24. The electricity prices of DC1, DC2, and DC3, however, are predicted with low accuracy, exhibiting the error of 25% for lag one and 36% for lag 24. Due to the data insufficiency we do not build prediction model for carbon intensities and use accurate data. Experiments performed: We perform experiments under different configurations: the length of T , the magnitude of the stipulated power, p0 , the magnitude of the carbon cap , and the prediction error. To evaluate OnCMCC and OnCMCCLyp, we use three reference solutions namely Optimal (optimal offline solution to P1), MinCost and MinCarbon. MinCost performs global workload management over the cloud to first minimize the cost and then the carbon footprint. MinCarbon, on the contrary, first minimizes the carbon footprint across the cloud and then the cost. MinCost and MinCarbon can be viewed as representative of the previous schemes which solely focus on either cost minimization (e.g., [7]) or carbon footprint minimization. We perform experiments to evaluate the incremental solutions of the global workload management scheme, which altogether framed the holistic solution. We first, evaluate OnCMCC to assess the efficiency of predictive solution for joint optimization of the electricity cost, and the peak power cost. Next, we evaluate OnCMCCLyp and compare it against OnCMCC to assess the combined predictive and Lyapunov based technique for coordinated management of the electricity cost, the peak power cost and the carbon footprint. Finally, we evaluate our final holistic solution OnCMCCLypstoch to assess its performance for coordinated management of the electricity cost, the peak power cost and the carbon footprint in the presence of realistic predicted parameters and the prediction error. A. Joint optimization of cost and carbon capping We first relax the carbon capping constraint, and evaluate OnCMCC versus T and the magnitude of the stipulated power, p0 (as percentage of data centers' maximum power). In order to run Optimal in a reasonable time, we run the experiments of this section using only three data centers (DC1, DC2, and DC3). The results, shown in Fig. 5(b), depicts that the larger the value of T , the closer the performance of OnCMCC becomes to that of Optimal. A daily basis T (T =24) can competitively manage the cost compared to Optimal even for large ESDs as long as p0 is reasonably large. The magnitude of p0 is typically such that such a power consumption arises

Time averaged carbon (CO2 g)

Power (KW), CA

20 10

solar

wind

6000

x 10 2.5 2

4

5000

Optimal OnCMCC, T=6

Total Cost($)

Power (KW), TX

0 20 10 0 20 10 0 0 200 400 Time index (hour) 600

4000 3000 2000

OnCMCC, T=12 OnCMCC, T=24

Total cost ($)

1600

MinCarbon Optimal OnCMCCLyp

OnCMCC MinCost

Power (KW), GA

1300 1000
0 1 V value 2 3 11 x 10

1000

95% 90%

80% 70% 65%

50%

40%

Stipulated power (p0 ) given as percentage of the data centers` maximum power

(a)

(b)

(c)

Fig. 5. (a) Hourly traces of solar and wind power (data taken from [21]), (b) total cost of OnCMCC versus stipulated peak power ( p0 ), and (c) performance of OnCMCCLyp versus Optimal and OnCMCC for various V values with tight .
Time averaged carbon (CO2 g)
3.5 3 x 10

Request Request per sec. per sec.

4

2.5
MinCarbon Optimal OnCMCCLyp OnCMCC MinCost

Total cost ($)

2 1600 1300 1000

Renew power (W)

x 105 6 Workload scenarios of S1 4 2 x 10 5 6 Workload scenarios of S2 4 2 4 x 10 Renew scens Predicted Actual 2 of S2 Scenario 0 5 10 15 Time (hours) 20

8000

energy cost peak power cost
4%

Total Cost($)

6000
4000

11% 11% 23% 24%

2000
0

0

2

V value

4

6 11 x 10

Opt 1 3 5 7 10 15 20 25 30 35 40 Number of scenarios

(a)

(b)

(c)

Fig. 6. (a) Performance of OnCMCCLyp for various V values and for  is equal to the mean carbon footprint of MinCost and MinCarbon, (b) scenario tree of stochastic workload and renewable generation for a sample time frame T = 24, and (c) total cost of OnCMCCstoch versus OnCMCCopt (Opt) (the cost savings are calculated with respect to the one scenario case i.e., OnCMCCpred ) and number of S1 scenarios (zero renewable energy).

rarely, and a stipulated power which is 60% of the data center maximum power is an unrealistic value which is used to evaluate the worst-case performance of the solution. Next, we run MinCost and MinCarbon and perform some experiments to evaluate OnCMCC and OnCMCCLyp for T = 24, two values of the cap, , and various values of V , the Lyapunov control parameter. First, we set  to a value very close to the carbon footprint achieved by MinCarbon. This is an example of the case where the cloud is associated with a tight cap. Results, shown in Fig. 5(c), depict that OnCMCC fails to meet the cap, whereas OnCMCCLyp meets the cap for V values less than 1.51011 (see Fig. 5(c)). Interestingly, Fig 5(c) shows that for V values in the range [0.51011 1.51011 ], OnCMCCLyp yields lower carbon footprint and achieves lower energy cost (up to 7.5% lower cost) than that of OnCMCC. In particular, for a V value around 1.31011 , OnCMCCLyp performs very close to Optimal in terms of minimizing cost (sum of the electricity cost, the peak power cost and ESD cost) while satisfying the cap. Since OnCMCC independently manages the carbon footprint across T frames, it cannot opportunistically leverage the ups and downs of the cloud carbon footprint and the energy cost to optimally manage the two. OnCMCCLyp, however, takes the dynamics of the cloud carbon footprint into account and achieves a performance near to Optimal when V is appropriately adjusted. Second, we set  to the mean carbon footprint of MinCost and MinCarbon, example of the case where carbon cap is loose. Results, shown in Fig. 6(a), indicate that OnCMCC, in

this case, achieves a lower carbon than that of Optimal, albeit at the expense of increasing the cost by 10%. OnCMCCLyp, however, for V values less than 4.51011 meets the cap. Similar to the previous case, OnCMCCLyp, when run with appropriate V value, outperforms than OnCMCC and achieves near Optimal performance in terms of minimizing the cost (see Fig. 6(a) for V values in the range [2.51011 4.51011 ]). In practice, OnCMCCLyp is expected to yield higher performance against OnCMCC when performed for more than one month, since the carbon intensity variations over months are huge. Although the results of Theorem 1 is based on the assumption of T =S (in the experiment S =S =168 i.e., one month), the experimental results running for T =24<<S show that OnCMCCLyp achieves near one competitive ratio against Optimal for an appropriate V value. From the above results we conclude that T -slot Lyapunov based solution, OnCMCCLyp, is indeed effective for using as a holistic solution to manage the electricity cost, the peak power cost and the carbon capping. Note, the appropriate V value depends on the cloud parameters e.g., the carbon footprint. The parameter B in Theorem 1, gives a clue for adjusting V . The results so far, however, are given for the case where the T slot future information are accurately available. Next section evaluates the solutions when using predicted data over T slots. B. Stochastic optimization We characterize r,i ,  and ,i through the prediction results. Then the marginal distribution of each of them, cover-

12000 10000

Total Cost($)

8000 6000

stoch

6%

19%

7%

Cost($)

OnCMCC opt, energy cost OnCMCC pred , energy cost OnCMCC pred , peak power cost OnCMCC stoch , energy cost OnCMCC , peak power cost

15000

10000

OnCMCCopt energy cost OnCMCCopt peak power cost OnCMCCpred, energy cost OnCMCCpred, peak power cost OnCMCCstoch,energy cost OnCMCCstoch, peak power cost
30% 27%

12000 10000
8000 6000 4000

OnCMCCopt energy cost OnCMCCpred, energy cost OnCMCCpred, peak power cost OnCMCCstoch, energy cost OnCMCCstoch, peak power cost

4000 2000 0

23% 26%

30%

5000

30%

29%

23% 16%

32%
0 95% 90% 80% 70% 65% 50% 40%

5%

39% 42% 12% 27%

2000
0 0 6 12 30 50 60 Peak power cost over a month ( ), $/KW

0% 7% 15% 26% 41% 57% % of renew. energy when using OnCMCC opt

Stipulated power (p 0 ) as percentage of the data centers` peak power

(a)

(b)

(c)

Fig. 7. Total cost of OnCMCCstoch versus OnCMCCopt (Opt) using 15 scenarios of S2: (a) various renewable energy utilization, (b) various stipulated power ( p0 ), and (c) various peak power cost ().

ing 90% confidence interval, is approximated to five samples each with equal probabilities. Due to their large differences, parameters' samples are normalized between zero and one. We use r,i to represent the aggregated prediction error of both the wind and the solar energy at each data center and  to represent the aggregated prediction error of the workload for all front-ends (see Section V). Given a random process at one stage, we construct the scenario tree over T and apply [11, Algorithm 2] to construct two reduced scenario sets: (i) S1 solely from the discrete marginal distribution of  , and (ii) S2 from the discrete marginal distribution of  , r,i , and ,i . In order to run [11, Algorithm 2] in a reasonable time, we evolve the scenario trees of S1 over eight stages, and S2 over two stages. Fig. 6(b), shows that S1 and S2 capture the randomness of the predicted workload more accurately than that of the predicted renewable energy due to its high prediction error. We evaluate OnCMCC and OnCMCCLyp when using predicted data over T =24 (namely OnCMCCpred and OnCMCCLyppred ) versus when using stochastic programming approach (namely OnCMCCstoch and OnCMCCLypstoch ) and when using accurate data (namely OnCMCCopt and OnCMCCLypopt ). We run stochastic solutions for different number of scenarios (OnCMCCstoch of one scenario is identical to OnCMCCpred ). In the figures we show the sum of the electricity cost and the battery cost as energy cost. Number and type of scenarios: First, we set the renewable energy of all data centers to zero and use S1. From the results of Fig. 6(c), it can be seen that the prediction error has a harmful effect on the peak power cost. In particular, while OnCMCCopt can manage grid power draw to avoid the peak power cost, OnCMCCpred with one scenario incurs $2400 for the peak power, increasing the total cost by 66% compared to OnCMCCopt . The total cost of OnCMCCstoch is decreased from 6% for 3 scenarios up to 24% for 15 scenarios compared to the total cost of OnCMCCpred (i.e., OnCMCCstoch of one scenario). This means that OnCMCCstoch yielding $900 more cost than OnCMCCopt (as opposed to $2400 for OnCMCCpred ), can remove 62.5% of the harmful prediction error impact in increasing the cost. Hence, the results agree with our initial hypothesis that stochastic programming with small number of scenarios can significantly mitigate the harmful impact of the prediction error. Fig. 6(c) also shows that the peak power cost saving of OnCMCCstoch with multiple scenarios, compared to its deterministic counterpart (OnCMCCpred ), comes at the expense of a slight increase in the energy cost. Further,

the performance of OnCMCCstoch does not improve when number of scenarios increases beyond 15. Note that stochastic programming does not guarantee an optimal performance, and its performance heavily depends on the problem, the predicted error magnitude, and the scenarios. Next, we fix the number of scenarios of S2 to 15, and scale the renewable energy of DC1, DC2, and DC3 such that the total renewable energy utilization of the cloud varies from 0% to 57% when using OnCMCCopt . Results, as shown in Fig. 7(a), similar to that of Fig. 6(c), indicates that OnCMCCstoch when using S2 significantly removes the impact of the prediction error of the workload, the electricity prices, and the renewables (removing 66% and 89% of the prediction error impact for 15% and 57% renewable energy utilization cases, respectively). The less scenario coverage of S2 for the predicted workload compared to that of S1, causes the performance of OnCMCCstoch to downgrade by 5% (compare 24% cost saving of OnCMCCstoch in Fig. 6(c) with 19% in Fig. 7(a) for the case of 0% renewable utilization). The cost saving of OnCMCCstoch increases compared to its deterministic counterpart (OnCMCCpred ) with increasing the availability of the renewable energy. This is because taking the randomness of the renewable and workload prediction error into consideration results in higher utilization of the renewable energy and consequently decreasing the cost. The impact of such a management is higher for the higher availability of the renewable energy. We also evaluate the performance of OnCMCCstoch (when using S2 with 15 scenarios and 15% renewable energy utilization case) for various stipulated peak power ( p0 ) and peak power cost (). Fig. 7(b) shows that the cost saving of OnCMCCstoch against OnCMCCpred is higher for higher stipulated power where stochastic scenarios can significantly affect the decisions. Fig. 7(c) indicates that the cost saving of OnCMCCstoch against OnCMCCpred is higher for higher . Generally, OnCMCCstoch incurs very similar expected electricity cost to that of OnCMCCpred , this is the reason that OnCMCCstoch has a total cost almost equal to that of OnCMCCpred for the case where  = 0. With increasing the peak power cost the impact of prediction error on increasing the peak power cost of OnCMCCpred is worsen which can be mitigated using OnCMCCstoch . Finally, we set the carbon cap, , to the mean carbon footprint of MinCost and MinCarbon and run OnCMCCLypstoch

8000 6000

Execution Size of problem time (sec)

OnCMCCLyp, energy cost OnCMCCLyp, peak power cost OnCMCC, energy cost OnCMCC, peak power cost

9000 6000

References
number of variables number of constraints

Cost($)

3000
0 30

[1]

4000 2000 0

20 10 0
5 10 15 20 25 Number of scenarios 30

[2]

Opt

1

Number of scenarios

3

5

7

10

15

20

[3]

(a)

(b)
[4]

Fig. 8. Total cost of OnCMCCLypstoch and OnCMCCstoch versus OnCMCCopt (Opt) and number of scenarios of S2, and (b) Overhead of OnCMCCstoch .

with appropriate V value over different number of scenarios of S2. The results, as shown in Fig. 8(a), have a similar trend to those of the previous results (e.g., Fig. 6(c)) in the sense that the stochastic programming solutions (OnCMCCstoch and OnCMCCLypstoch ), significantly remove the impact of the prediction error, improving the cost of OnCMCCpred , and OnCMCCLyppred up to 30% by using ten scenarios (removing the impact of the prediction error by 66%). This cost saving comes at a slightly energy cost increase as shown in Fig. 8(a) and consequently a slightly carbon footprint increase. Overhead of the stochastic solution: The cost efficiency of the stochastic programming solutions comes at the expense of increasing the size of the optimization problems. As a result, the execution time of the solutions increases depending on the computing system's capability. Fig. 8(b) shows that the size of the optimization problem of OnCMCC stoch linearly increases with increasing the number of scenarios in terms of both the number of decision variables and the number of constraints. This translates into the exponential increase in the execution time of the solution in our testbed (Intel Quad core i7-3770 CPU 3.4GHz, and 8G memory). Therefore it is important to run the stochastic solutions with small number of scenarios and an efficient implementation.

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

VII.

Conclusions
[15]

We proposed a holistic global workload management solution, which jointly minimizes data centers operational cost (including peak power cost), while satisfying the carbon capping requirement of the geo-distributed data centers. Peak power cost management, energy buffering and carbon capping all introduce time coupling in the solution. We developed an online algorithm OnCMCCLyp which (i) leverages (daily) predictability of data center input parameters to efficiently manage energy storage dynamics and to smoothen the power draw from the grid, and (ii) uses T slot Lyapunov optimization to manage the cost carbon footprint tradeoff. Our trace based study shows that our T -slot Lyapunov based solution, OnCMCCLyp can achieve near one competitive ratio with respect to the optimal offline solution when the Lyapunov control parameter is appropriately adjusted, T is sufficiently large and data over T is accurately available. However, the prediction error of the parameters over T slots has a very harmful impact on the peak power shaving and consequently on the cost efficiency of the solution. Our proposed stochastic programming approach is shown to remove up to 66% of such negative impacts.

[16] [17]

[18] [19]

[20]

[21] [22]

K. Le, R. Bianchini, T. D. Nguyen, O. Bilgir, and M. Martonosi, "Capping the brown energy consumption of internet services at low cost," in Green Computing Conference, 2010 International. IEEE, 2010, pp. 314. S. Govindan, D. Wang, A. Sivasubramaniam, and B. Urgaonkar, "Aggressive datacenter power provisioning with batteries," ACM Transactions on Computer Systems (TOCS), vol. 31, no. 1, p. 2, 2013. A. Bar-Noy, M. P. Johnson, and O. Liu, "Peak shaving through resource buffering," in Approximation and Online Algorithms. Springer, 2009, pp. 147159. D. Wang, C. Ren, A. Sivasubramaniam, B. Urgaonkar, and H. Fathy, "Energy storage in datacenters: what, where, and how much?" ACM SIGMETRICS Perf. Eval. Rev., vol. 40, no. 1, pp. 187198, 2012. R. Urgaonkar, B. Urgaonkar, M. J. Neely, and A. Sivasubramanian, "Optimal power cost management using stored energy in data centers," in Proc. ACM SIGMETRICS, 2011, pp. 221232. S. Govindan, A. Sivasubramaniam, and B. Urgaonkar, "Benefits and limitations of tapping into stored energy for datacenters," in ISCA. IEEE, 2011, pp. 341351. A. Qureshi, R. Weber, H. Balakrishnan, J. Guttag, and B. Maggs, "Cutting the electric bill for Internet-scale systems," in Proc. ACM SIGCOMM, 2009, pp. 123134. V. Kontorinis, L. E. Zhang, B. Aksanli, J. Sampson, H. Homayoun, E. Pettis, D. M. Tullsen, and T. S. Rosing, "Managing distributed UPS energy for effective power capping in data centers," in ISCA. IEEE, 2012, pp. 488499. A. H. Mahmud and S. Ren, "Online capacity provisioning for carbonneutral data center with demand-responsive electricity prices," ACM SIGMETRICS Perf. Eval. Rev., vol. 41, no. 2, pp. 2637, 2013. M. Etinski, M. Martonosi, K. Le, R. Bianchini, and T. D. Nguyen, "Optimizing the use of request distribution and stored energy for cost reduction in multi-site internet services," in SustainIT. IEEE, 2012. N. Growe-Kuska, H. Heitsch, and W. Romisch, "Scenario reduction and scenario tree construction for power management problems," in Power Tech Conference Proceedings, vol. 3. IEEE, 2003, pp. 7pp. Z. Abbasi, M. Pore, and S. K. Gupta, "Online server and workload management for joint optimization of electricity cost and carbon footprint across data centers," in IPDPS. IEEE, May 2014. Z. Zhou, F. Liu, Y. Xu, R. Zou, H. Xu, J. C. Lui, and H. Jin, "Carbon-aware load balancing for geo-distributed cloud services," in IEEE MASCOTS, 2013. Z. Liu, A. Wierman, Y. Chen, B. Razon, and N. Chen, "Data center demand response: Avoiding the coincident peak via workload shifting and local generation," Performance Evaluation, vol. 70, no. 10, pp. 770791, 2013. V. S. Pappala, I. Erlich, K. Rohrig, and J. Dobschinski, "A stochastic model for the optimal operation of a wind-thermal power system," Power Systems, IEEE Transactions on, vol. 24, no. 2, pp. 940950, 2009. Z. Abbasi, "Sustainable cloud computing," Ph.D. dissertation, Arizona State University, 2014. M. J. Neely, "Energy optimal control for time-varying wireless networks," Information Theory, IEEE Transactions on, vol. 52, no. 7, pp. 29152934, 2006. A. Shapiro, D. Dentcheva et al., Lectures on stochastic programming: modeling and theory. SIAM, 2009, vol. 9. M. Kaut and S. W. Wallace, "Evaluation of scenario-generation methods for stochastic programming," Pacific Journal of Optimization, vol. 3, no. 2, pp. 257271, 2007. M. F. Arlitt and C. L. Williamson, "Web server workload characterization: The search for invariants," in ACM SIGMETRICS Perf. Eval. Rev., vol. 24, no. 1, 1996, pp. 126137. [Online]. Available: http://www.nrel.gov/midc/ M. G. De Giorgi, A. Ficarella, and M. Tarantino, "Error analysis of short term wind power prediction models," Applied Energy, vol. 88, no. 4, pp. 12981311, 2011.

Computational Methods for Dynamic Graphs
Corinna Cortes, Daryl Pregibon and Chris Volinsky 
AT&T Shannon Labs
November 24, 2003

 All

authors are members of research sta at AT&T Labs, 180 Park Avenue, Florham Park, New Jersey 07934.

1

Abstract
We consider problems that can be characterized by large dynamic graphs. Communication networks
provide the prototypical example of such problems where nodes in the graph are network IDs and the
edges represent communication between pairs of network IDs. In such graphs, nodes and edges appear
and disappear through time so that methods that apply to static graphs are not sucient. Our denition
of a dynamic graph is procedural. We introduce a data structure and an updating scheme that captures,
in an approximate sense, the graph and its evolution through time. The data structure arises from a
bottom-up representation of the large graph as the union of small subgraphs centered on every node.
These subgraphs are interesting in their own right and can be enhanced to form what we call Communities of Interest (COI). We discuss an application in the area of telecommunications fraud detection to
help motivate the ideas.

Keywords: transactional data streams, dynamic graphs, approximate subgraphs, exponential aver-

aging, fraud detection.

2

1 Introduction
Transactional data consists of records of interactions between pairs of entities occurring over time. For example, a sequence of credit card transactions consists of purchases of retail goods by individual consumers
from individual merchants. Transactional data can be represented by a graph where the nodes represent the
transactors and the edges represent the interactions between pairs of transactors. Viewed in this way, interesting new questions can be posed concerning the connectivity of nodes, the presence of atomic subgraphs,
or whether the graph structure leads to the identication and characterization of \interesting" nodes. For
example, Kleinberg[9] introduces the notion of \hubs" and \authorities" as interesting nodes on the Internet. The data used by Kleinberg dier signicantly from the data we consider in that he uses static links to
induce a graph over WWW pages. We use actual network trac, as captured by interactions between pairs
of transactors, to dene our graph. Since nodes and edges appear and disappear through time, the graph
we consider is dynamic.
There are many challenging issues that arise for dynamic graphs and we have used a specic application
to focus our research, namely the graph induced by calls carried on a large telecommunications network.
This application is interesting, both because of its size (i.e., hundreds of millions of nodes and edges) and
its rate of change (i.e., hundreds of thousands of new nodes and edges each day). Like all networks, it is
also diverse in the sense that some nodes are relatively inactive while others are superactive.
The paper is organized as follows. Section 2 illustrates characteristics of large dynamic graphs using
network trac from a specic telecommunications service. Section 3 introduces the denition of a dynamic
graph that we adopt and discusses the computational and statistical features of various alternatives. Section 4
describes an approximation to facilitate both interpretation and large scale computations, including updating
and maintaining the dynamic graph over time. Section 5 introduces an example that illustrates how these
subgraphs are used in practice. In Section 6 we expand on these notions and consider approximate subgraphs
centered on nodes that can be further enhanced or pruned to dene communities of interest. Section 7
discusses related work in and outside the data mining community. The nal section summarizes the ndings
and discusses future work.

2 Motivation
The large graphs that we are concerned with are dened from records of transactions on large telecommunications networks. We believe that characteristics of such graphs are shared by other large nancial and data
networks, including the Internet. These characteristics include a large number of nodes and edges, sparse
connectivity, and dynamics that have stable macro eects but substantial variation in micro eects. In this
section we illustrate these characteristics using telecommunications trac. Our data consists of hundreds of
millions of nodes, each of which represents an account. We observe several billion edges on this network in
an average week, presenting themselves in a continuous data stream. This section uses plots and tables to
3

present the salient features of this transaction stream.

2.1 Addition and attrition of nodes
Nodes in dynamic graphs appear and disappear through time. Figure 1 shows the addition and attrition of
nodes throughout the study period. By \node addition" we mean the number of new nodes that we see in
week i that we haven't yet seen through week i ? 1. By \node attrition" we mean the number of nodes that
we see for the last time in week i. The service we consider has hundreds of millions of nodes and billions of
edges. On any given day, tens of millions of these nodes are active and responsible for hundreds of millions
of transactions. The exact numbers are considered proprietary and we illustrate the volatility of the service
using relative change. Figure 1 attempts to capture this volatility by showing the addition and attrition
rates of nodes on the network. The gure illustrates that after a steady state is reached, roughly after 18
weeks of observation, new nodes are observed (for the rst time) at a rate of slightly less than 1% per week.
Similarly, discounting the end eects of the observation period, old nodes are observed (for the last time)
at the same rate. The parallel lines tted to the end/start of these sequences illustrate that the service is
stable despite there being considerable turnover of transactors each and every week.

100

Cumulative % of All Nodes Seen

80
o

o

o

o

+
o
o o o
o o
o o
o o
o
o o
o o
o o

o
o
60

o
+

o
+

40
+
+
20

0

+
+ +
+ +
+ +
+
+ +
+ +
+ +
+
o + +
0

5

10

15

+

+

+

+

20

25

Study Week Observed

Figure 1: Node addition and attrition through time. The data correspond to 25 successive weeks of network
activity. The upper curve shows the cumulative percent of unique nodes seen each study week that had not
been seen before. The lower curve shows the cumulative percent of nodes seen for the last time in each study
week. Steady state is reached after about 18 weeks and lines are tted to the remaining last and rst 7 weeks
respectively. The slopes correspond to a addition and attritrion rates of just under 1%. This means that
1% of the nodes we observe each week had not been observed previously, and another 1% will never be seen
again.
4

2.2 Addition and attrition of edges
We illustrate the addition and attrition of edges by following a random sample of 1000 residential accounts
over a 180 day period. This period was broken up into six 30-day slices, and the edges from the rst month
were tracked to see if they appeared in subsequent months.
The results in Table 1 show that edges seen in one month often do not show up again. Of all the edges
observed in the reference month, only 37.9% of them are seen the following month, and the number seen
steadily decreases for subsequent months. The cumulative results show aggregation of all the months. By
the end of the study period, only 53.7% of the edges from the rst month have been observed again, and the
leveling o of the cumulative numbers indicate that there will be a reasonably large percent of edges that
will never be seen again.
Month Old Edges Seen Percent Cumulative Percent
1
5995
100
{
2
2272
37.9
37.9
3
2001
33.4
46.5
4
1734
28.9
50.0
5
1585
26.4
52.3
6
1376
23.0
53.7
Table 1: Edge attrition for activity on 1000 residential accounts over a 6 month period. Month 1 is a
\reference month" for which we look for those edges in subsequent months. For each subsequent month
the table shows how many of the edges are observed again. The column marked Cumulative aggregates the
subsequent months to show how many of the edges we have seen overall.
Table 2 shows edge addition eects on the same sample over the same period. This time for each month,
we note how many edges are seen for the rst time. In addition, we show the total number of unique edges
seen up to and including that month. Note that after ve months, we have observed nearly 4 times more
edges than we observed in the rst month.

2.3 Connectivity
A fully connected graph with N nodes has M = N (N ? 1) directed edges. We explore connectivity using
the sample of 1000 residential accounts. Figure 2(a) displays the cumulative distribution of in-degree and
out-degree for the nodes in this sample. The gure shows that 90% of all nodes have in-degree of 22 or less,
and out-degree of 32 or less. This relative sparseness suggests a relationship of the form M / Nlog(N ),
or maybe even M / N , rather than M / N 2 . In Section 4 we exploit this sparsity with an approximate
representation for large network graphs.
5

Month New Edges Seen Percent New Cumulative Percent
1
5995
100
25.6
2
4461
66.3
44.7
3
4441
59.8
63.6
4
3130
50.8
77.0
5
3102
50.0
90.3
6
2274
44.4
100
Table 2: Edge addition for activity on 1000 residential accounts over a 6 month period. For each month we
show the percentage of the edges that we observed that we had not seen yet. The column marked Cumulative
Percent shows the unique edges we have seen through the entire study up until that point as a percentage of
the union of all edges.
Figure 2(b) shows that the distribution for the out-degrees roughly follows a power-law distribution (and
hence shows up as linear on a log-log scale). There is a growing literature (e.g. Barabasi[2]) showing that
power-law behavior exists in large networks, including the Internet, genetic networks, and social interactions
{ so it is not surprising to see evidence of this property in our data.

200

1.0

100
0.8

0.6
InDegree
OutDegree

# of Nodes

Cumulative % of Nodes

50

20

10

0.4

5
0.2

2

1

0.0
1

2

5

10

20

50

100

1

200

2

5

10

20

50

100

Out−Degree

Edge Degree

(a)

(b)

Figure 2: Plots of the (in-) out-degree of 1000 residential accounts over a 180 day period. Panel (a) shows
the cumulative percent of accounts having node degree less than or equal to k. Panel (b) shows the number
of nodes having a specic out-degree.
6

2.4 Activity
While nodes and edges arrive and depart in fairly large numbers, it is also interesting to consider how often
they were observed. Figure 3 is a histogram of the number of days that the given nodes were active during
the entire 180 day period. The median of this distribution is 29 days, indicating that a typical node was
only active on one out of six days during the study.

200

Frequency

150

100

50

0
0

50

100

150

Days Active (out of 180)

Figure 3: Histogram of the number of days with activity for 1000 residential accounts over a 180 day period.

2.5 Implications
The plots and tables in this section illustrate that large numbers of nodes and edges appear or fail to
appear on a daily basis. In some cases, their disappearance is temporary and in others it is permanent. Any
procedure that attempts to capture network behavior will have to deal with node/edge addition and attrition
in an automated fashion since there is little time to synchronize with databases of account information and
yet process the current network activity. The relatively sparse connectivity of the graph is the aspect of the
behavior that we attempt to exploit in devising a methodology to dene, build, evolve, and maintain an
approximate representation of the graph through time.

3 Denition of a Dynamic Graph
In this section we consider the denition of Gt , a dynamic graph G at time t. We consider discrete time
applications where new sets of nodes and edges corresponding to the transactions from time step t to t + 1
only become available at the end of the time step, for example once a day. Associated with every edge is a
weight, w()  0, that is derived from an aggregation function applied to all (directed) transactions between
7

a pair of nodes at time step t. For example, the aggregation function can be the \total duration of calls" or
the \number of calls" from one node to another.
We rst dene the sum of two graphs g and h
G = g  h

where  and  are non-negative scalars. The nodes and edges in G are obtained from the union of the nodes
and edges in g and h. The weight of an edge in G is
w(G) = w(g) +  w(h);
where the weight of an edge is set to zero if the edge is absent from the graph.
Let the graph corresponding to the transactions during time step t be gt . We can dene Gt from gi where
i = 1; : : : ; t in several ways, depending on the purpose to which the graph is intended to be used.

3.1 Summarizing historical behavior
The cumulative behavior of the graph through time can be dened as

Gt = g  g  : : :  gt =
1

2

t
M
i=1

gi = Gt?1  gt ;

(1)

This denition of Gt includes all historic transactions from the beginning of time. The last expression on the
right hand side illustrates that for computational purposes, the cumulative summary at time t is the sum
of the cumulative summary at time t ? 1 and the network activity at time step t. An alternative denition
that considers only recent network activity, say the last k time steps, is the moving window denition

Gt = gt?k  gt?k  : : :  gt =
+1

t
M

i=t?k

gi

(2)

This denition of Gt tracks the dynamics of the transactional data stream, and can be thought of as a
characterization of network behavior at time t ? k=2, the center of the moving window. In contrast to Eq.
(1), this denition requires storage and retrieval of graphs characterizing network activity at each time step.

3.2 Predicting future behavior
Gt can be thought of as a predictor of network activity at time t + 1. The simplest such prediction is Gt =

gt , the network graph corresponding to the transactions at time step t. The stability of this simple predictor

can be improved by blending in network activity in a way that discounts the past in favor of recent behavior:

P

Gt = ! g  ! g  : : :  !t gt =
1 1

2 2

t
M
i=1

!i gi

(3)

where the weights !i satisfy !i = 1 and are an increasing function of i. Eq. (2) can be expressed in this
form with !1 = : : : = !t?k?1 = 0 and !t?k = : : : = !t = 1=k. A particularly convenient form of the weights
8

Edge wt for a 60 minute call (in seconds)

800

θ = 0.75
θ = 0.8
θ = 0.85
θ = 0.9
θ = 0.95

600

400

200

ε = 0.1

0
1

2

5

10

20

50

100

Days

Figure 4: Contribution of a 60 minute call to edge weights as a function of time steps (days) in the recursive
expansion (3) of Gt . The horizontal line at  = 0:1 denotes an adjustable threshold whereby edges with weights
less than this value are deleted.
is !i = t?i (1 ? ) where 0    1 is a (scalar) parameter that allows more ( near 1) or less ( near 0)
history to inuence the current graph. Figure 4 displays this graphically. If processing occurs daily, then
with a value of  = 0:85, the edge weight associated with a 60 minute call will be eectively reduced to that
of a 6 second call in about 30 days. This form of weight function is convenient in the sense that Eq. (3) can
be expressed in recurrence form:
Gt = Gt?1  (1 ? )gt :
(4)
This form is well-known in statistics as exponential smoothing [17]. It provides a smooth dynamic evolution
of Gt without incurring the management and storage of graphs for many previous time periods. All that is
needed is the graph through time period t ? 1 and the new set of transactions dened by gt .
In the following we adopt Eq. (4) as the denition of a dynamic graph at time t. It's usefulness as a
smoothing operator and as a prediction make it suitable for a wide range of applications.

4 Approximating Gt
The graph dened by Eq. (4) is complete in the sense that it captures the entire graph, including edges
with innitesimally small weights. If such edges are maintained, the graph will eventually grow to be highly
connected. This connectivity is undesirable as it preserves dated relationships that may be misleading (e.g.,
behavior 6 months ago may not be representative of current activity) or invalid (e.g., when accounts are
closed their node labels are often reassigned, as is the case with telephone numbers and IP addresses). In
addition to these considerations, the entire graph of the size we consider can be unwieldy, especially if it is
9

too large to t into main memory. In this section we propose approximations to the entire graph that enable
sophisticated applications without compromising it's integrity.
Section 2 demonstrated that over a 6 month period, the vast majority of nodes exhibit a low degree
of connectivity. We propose an approximation to the entire graph that exploits this sparsity. The key to
our approximation is the introduction of new aggregator edges to the graph that summarize edges that are
eliminated for one of two reasons:

 Global costs. Either the weight associated with an edge is too small, in an absolute sense, to justify
the overhead of maintaining that edge, or

 Local costs. The weight associated with an edge is too small relative to other edges coming in or out
of a node to justify the overhead of maintaining that edge.

The new aggregator edges are introduced at the subgraph level. For each node in Gt , consider the subgraph
consisting of that node and the directed edges to its immediate neighbors. A new outbound aggregator edge
eectively replaces a subset of outbound edges of this subgraph such that it contains the same total weight
of the edge subset. The node label on the terminating side of this edge is simply called other. A new
inbound aggregator edge applies to a subset of inbound edges. The subsets of edges that are removed can
be parameterized by a pair of thresholding functions, one applying to global thresholding of edge weights,
and the other to local thresholding of edge weights.
We rst describe the global thresholding function. In practice, edges in Gt with extremely small weights
carry a disproportionate amount of overhead in maintenance and storage of the graphs relative to the
information that they contain. Such edges come about from new calls with unusually small weights (e.g.,
realizing that one dialed a fax number instead of a voice number) or from old calls that had meaningfully
large weights initially, but have decayed through time by exponential weighting. We apply a thresholding
function to each edge in Gt such that all edges with weights less than a constant  are eliminated prior to
storing the updated graph. In our applications we use  = 0:1. If edge weights reect call durations (in
seconds) then  = 0:1 coupled with  = 0:9 means that a one second call lasts one day in the updated graph
(since w(e)=0:9  0 + 0:1  1ses). Alternatively a 60 minute call persists in the graph for 78 days (since
w(e)=0:978  0:1  3600sec < 0:1).
The local thresholding function that we use applies indirectly to the value of the edge weights. For a
node n with outbound (inbound) edgeset fe : ei ; i = 1; :::N g, we retain only the top-k outbound (inbound)
edges where \top" is relative to the value of the weight associated with each edge:
top-kfeg = fe : !j (e) > ![N ?k] (e)g

(5)

where ![i] (e) is the ith order statistic of !(e). This type of thresholding function leads to possible asymmetry
in the sense that an edge eij : ni ! nj might be retained in the top-k outbound edgeset of ni but not in the
top-k inbound edgeset of nj . This would be the case for example if ni corresponds to a \normal" residential
10

1.0

% accounts with k slots

0.8

0.6

0.75
0.8
0.85
0.9
0.95

0.4

0.2

0

5

10

15

20

25

30

Number of Slots (k)

Figure 5: Cumulative proportion of accounts that had k distinct outbound edges at the end of a 90 day study
period.
account and nj a toll free number for a large retailer. For all nite k, the only invariance between the
complete graph and its top-k approximation is that the sum of the outbound edge weights equals the sum
of the inbound edge weights, where the sum includes all nodes labeled as other.
Considerations for selecting a value of k are discussed in the next subsection. In our experience we prefer
a relatively small value of k that balances computational complexity (e.g., as regards speed and storage)
with empirically determined accuracy (see below).

4.1 Tuning the approximation
The denition of Gt given by Eq. (4) requires a value of  that governs the degree to which new nodes and
edges are blended in with recent activity. For top-k approximations, interplay between  and k determines
both the size of G^t and the degree to which it captures the evolving network graph. In theory,  is also
a parameter which aects the graph, however we choose to hold  = 0:1 for its nice interpretation stated
above, that a one second call lasts about one day, and any edge weight less than one second will be below
threshold. In this section we explore the relationship between  and k in top-k approximations. In practice,
our recommended approach is to rst settle on a value of  that makes sense for the service being modeled,
and for that , choose k so that interesting detail on most of the nodes is adequately captured.
We explore values of  in the range 0.75 { 0.95. Reference to Figure 4 indicates that for this range of
values, a one hour call persists in the evolving network graph from several weeks to several months. In our
applications, where phone numbers can be reassigned 30 days after service is discontinued, a value of  > 0:95
would lead to contamination of the new accounts subgraph with activity from the previous account. Smaller
11

% accounts with 90% of weight in top k slots

1.0

0.8

0.6

0.75
0.8
0.85
0.9
0.95

0.4

5

10

15

20

Number of Slots (k)

Figure 6: Cumulative proportion of accounts that have at least 90% of their edge weights captured by k edges.
values of  force more dynamics, with the result being that for many accounts with infrequent or sporadic
usage, their subgraph is not adequately captured. We illustrate these points with several plots derived from
the sample of 1,000 accounts introduced in Section 2. The graphs for these accounts were evolved using a
range of values of , and the status of the accounts at the end of the period were used.
Figure 5 shows the cumulative proportion of accounts that had k distinct outbound edges at the end of the
90 day study period. The gure shows that for  = 0:90, 80% of all accounts have at most 15 distinct edges,
and 90% of all accounts have at most 25 distinct edges. If edge preservation was critical in an application,
choosing a value of k = 15(25) would lead to approximately 80% (90%) of all nodes having all their edges
preserved by the top-k approximation.
The curves displayed in Figure 5 relate to the presence or absence of edges without regard to the weights
on those edges. An edge that corresponds to daily one hour calls between a pair of accounts is treated
identically to an edge that resulted from a call lasting 1 second on the last day of the study period. The
only role that edge weight played was that an edge was deleted if its weight dropped below  = :1. Figure 6
addresses this issue by displaying the cumulative proportion of accounts that had at least 90% of their edge
weights captured by k edges. If the weight function represents the number of calls, then the curves represent
the cumulative proportion of accounts that had at least 90% of their calls captured by k distinct edges. If
the weight function represents the length of calls, then the curves represent the cumulative proportion of
accounts that had at least 90% of their \time on network" captured by k distinct edges. The gure shows
that for  = 0:90, 80% of all accounts have 90% of their edge weights captured in 5 distinct edges and 90% of
all accounts have 90% of their edge weights captured in 8 distinct edges. If weight preservation was critical
in an application, choosing a value of k = 5(8) would lead to approximately 80% (90%) of all nodes having
90% of their edge weights preserved by the top-k approximation.
12

0
BB
BB
BB
BB
B@

k

Old top-



X 5467
X 2656
X 4132
X 4231
X 3142
X 4212
X 1423
X 2312
X 4532
other

1
CC
CC
CC
CC
CA

edges:

node{labels

wts

:
5:0
4:5
2:3
1:9
1:8
0:8
0:5
0:2
0:1

5 2

+ (1

?

)

0
BB
BB
BB
BB
B@

Today's edges:
node{labels

X 5467
X 2656
X 4132
X 6547

wts

:
6:2
0:8
10:0
2 0

1
CC
CC
CC
CC
CA

k

New top-

=

X 2656
X 5467
X 4132
X 4231
X 3142
X 4212
X 6547
X 1423
X 2312
other

0 0

1
CC
CC
CC
CC
CA

edges:

node{labels

:

other

0
BB
BB
BB
BB
B@

wts

:
4:6
3:9
2:0
1:6
1:5
1:5
0:7
0:4
0:3

5 2

Figure 7: Computing a new top-k edge set from the old top-k edge set and today's edges. Note how a new
edge (X6547) enters the top-k edge set, forcing an old edge (X4532) to be added to other.

4.2 Implementing the approximation
We propose a constructive approach to implementing our approximation to a large time-varying graph.
Consider a node in the graph, its associated directed edges, and weights associated with each edge. A data
structure that consists of these weighted directed edge sets for each node is a representation of the complete
graph. This data structure is redundant since it is indexed by nodes so that edges must be stored twice,
once for the originating node and once for the terminating node. In contrast, a data structure that stores
each edge once must be doubly indexed by nodes. The cost of edge duplication is often mitigated by gains in
processing speed when subgraphs around nodes are expanded (see next subsection). For this reason we have
chosen to represent our graphs as a singly-indexed list of nodes, each with an associated array of weighted
directed edges.
The singly-indexed node list represents our approximation but the denition in Eq. (6) implies that new
activity must be blended into it at xed time steps. Let G^t?1 denote the top-k approximation to Gt?1 at
time t ? 1 and let gt denote the graph derived from the new transactions at time step t. The approximation
to Gt is formed from G^t?1 and gt , node by node, using a top-k approximation to Eq. (4):

G^t = top-kfG^t?  (1 ? )gt g
(6)
Thus, we rst calculate the edge weights for all the edges of G^t?  (1 ? )gt . Then for each node we sort
1

1

the edges according to their weight. (The overow node other is not given any special treatment in these
computations.) The top-k are preserved, and if there are more than k edges in the edge set for that node,
the weights of the remaining edges are added to the weight of the edge going from the node to node other.
These operations are displayed pictorially in Figure 7 using  = :85. Notice that a new call today with a
heavy edge weight (labeled X6547) replaces an old call with a low edge weight (labeled X4532).
Between updating steps, transactions need to be collected and temporarily stored. At the end of that
time period, the transactions are aggregated and the subgraph updated. The length of the time period
represents a trade-o in accuracy: the longer the time period, the better an estimate of the top-k edge set,
13

but the more outdated the resulting subgraph. In the application discussed in Section 5, we perform daily
updates, thereby maintaining reasonable accuracy while requiring temporary disk space for only one day of
data. For a related discussion see Cortes and Pregibon[5].
Prior to storing the updated graph, we remove all edges that fall below the  threshold. As argued
above, this reaping process results in both a smaller and a more interpretable graph. For our large network
graph this thresholding results in a 50% reduction in size, from 14gb (maximum size at saturation of top-9
approximation) to 7gb (stable size with average of 4.5 slots per indexed node).

4.3 Subgraph expansion
Our implementation of the subgraph consisting of the top-k inbound and the top-k outbound edges of a
node is ideal for fast extraction of larger subgraphs centered on the node. The data structure containing the
top-k approximation can be queried recursively for each node in the top-k edge sets of the center node.
We grow the subgraphs in a breadth-rst traversal of the data structure. For notational purposes, we
denote the top-k inbound and outbound nodes and edges of node n by R1 (n), the subgraph of radius 1
centered at node n. Similarly let R2 (n) denote the subgraph of radius 2 centered at node n. Note that
R2 (n) can be formed from the \union" of R1 (n) and the radius-1 subgraph centered on each node contained
in R1 (n). In general, we can dene subgraphs of any size using the recursion
Rj+1 (n) =

]

R1 (node)
node2Rj (n)
We use the quoted term \union" and the symbol ] instead of simply [ because of the aforementioned
potential asymmetry in the edge weights. The top-k approximation may force an edge from/to a high activity
node to be partially, or maybe even completely, absorbed by category other. However, a low activity node
that is connected to a high activity node is likely to preserve the edge in its top-k approximation. In cases
where both nodes preserve a common edge in their top-k list, the weights associated with that edge are
potentially dierent for the two nodes. In those cases we use the maximum edge weight in the denition of
Rj+1 (n), since the maximum represents the weight least aected by the top-k approximation.
The index structure of our representation is critical since we often need to compute and compare many
subgraphs on a daily basis. We have tuned our algorithms so that the average time for retrieving and
rendering R2 (n) subgraphs from our data structure of several hundred million nodes is just under one second
(on a single processor).
In our applications, we rarely explore edge sets greater than R2 (n), as the edge sets become unmanageably large and remarkably uninformative. Subgraph expansion also reinforces arguments suggesting that a
large value of k (in dening the top-k approximation) is not necessarily desirable when network graphs are
used to study relationships between nodes. Spurious edges (e.g., misdialed numbers) can have unintended
consequences upon subgraph expansion. We also discuss this further in Section 6 where we suggest additional
edge pruning in R2 (n) to further reduce \clutter" in a subgraph.

14

5 Application
In the telecommunications industry, there are many dierent types of fraudulent behavior. Subscription
fraud is a type of fraud that occurs when an account is set up by an individual who has no intention of
paying any bills. The enabler in such cases involves either awed processes for accepting and verifying
customer supplied information, or identity-theft where an individual impersonates another person. In either
case, if left undetected, the fraud is typically only discovered when the bill is returned to sender, often after
thousands of dollars have been lost. Since speed is critical in reducing losses due to fraud, it is essential to
essess inherent riskiness as each new account is activated on the network. In this section we explore this
possibility by dening a procedure that assesses risk on the basis of a node's connectivity to other nodes.

5.1 Subgraph-based account linkage
Consider the case where we have information on an account that was recently disconnected for fraud. If
identity theft was the root cause of the fraudulent account, we might expect this same individual to appear
with a new account bearing a dierent name and address. Basically the fraudster has now assumed the
identity of a new victim. We attack this problem with the intuition that while the subscription information
is not useful for matching network IDs to the same individual, the calling patterns of the new account, as
characterized by its R2 subgraph, should not change very much from the previous account. Figure 8 provides
an illustration where we overlay two R2 subgraphs apparently belonging to the same individual. The central
nodes of the two subgraphs are indicated by the rectangle with two ID numbers. One of these numbers
corresponds to a known fraudulent account, the other to a new account on our network. The amount of
overlap between the two subgraphs is strong evidence that these numbers are related and increases the fraud
risk of the new account.
Subgraph-based account linkage is a non-standard problem that involves dening a distance function
to quantify the closeness of two accounts based on subgraphs centered on the two accounts. The distance
between two subgraphs depends on both the quantity and the quality of the overlapping nodes. The quantity
of the overlap can be measured by the percentage of overlapping nodes. However all overlapping nodes are
not equally informative, so a measure of quality is needed as well. Many graphs will intersect at high-use
nodes, such as large telemarketing rms or widely advertised customer service numbers. An informative
overlapping node is one that has relatively low in- and out-degree, and in the best case, is shared only by
the nodes under consideration for a match. We now describe a measure that captures these notions.
Given a new account a and known fraudulent account b, let O = Rj (a) \ Rj (b) denote the set of all
overlapping nodes in the two radius j subgraphs. We dene
Overlap(Rj (a); Rj (b)) =

X wao wbo

1 1;

o2O wo dao dbo

where wo is the overall weight of node o (the sum of all edge weights in R1 (o)), wao is the weight of edges
15

XXX2549490

XXX6721477
XXX9749066
XXX9350055

XXX2320800
XXX3657504

XXX2768275

XXX4243460
XXX3190804XXX9724031

XXX6360824
XXX8534309

XXX9350316

XXX9241699

XXX3584547
XXX9687354

XXX6835622
XXX9375596
XXX2965398

XXX9724301
XXX4456363
XXX2057301

XXX9724032

XXX4762258

Figure 8: Subgraph-based account linkage. The two individual subgraphs are superimposed (in the doublylabeled rectangle) to emphasize their similarity. Solid lines indicate edges common to both subgraphs, while
dashed lines indicate edges belonging to only one subgraph.

100

% Correct matches

80

60

40

20

0
0.0

0.1

0.2

0.3
0.4
0.5
0.6
0.7
Predicted probabilities of a match

0.8

0.9

1.0

Figure 9: Success of subgraph matching. Observed percentage of matching node-pairs versus deciles of predicted matching probability.

16

between node a and node o in Rj (a), and dao is the minimal distance from node a to node o in Rj (a). The
terms wbo and dbo are dened similarly. [In the case where dao > 1, it is not clear what the weight wao should
be, since there is no direct edge between the two. For this application we elected to assign a small default
weight in order to minimize the eect of these overlaps]. Intuitively, the numerator measures the strength
of the connection from a and b to the overlapping node, while the denominator corrects for an overlap node
which is either common to many nodes or is further in the graph from a or b. This measure is large for
overlapping nodes that have strong links to the nodes of interest, but otherwise have low overall volume.
Even armed with this denition of distance, subgraph-based matching is computationally dicult because
of the dynamic nature of our network data as described in Section 2 { each day we see tens of thousands
of new accounts. For each of the new accounts, we need to compute it's subgraph, and then the distance
from it to the subgraph of all recently conrmed fraudulent accounts. Assuming for these purposes that we
maintain a library of the most recent 1000 fraudulent accounts, tens of millions of pairwise distances need
to be computed daily. We have harnessed the computations by maintaining R2 subgraphs for all accounts
in our \fraud library" and computing R1 subgraphs for all new accounts.
We obtained a training set of paired accounts where investigators were able to determine whether the
old and new accounts belonged to the same individual. We built a decision tree using the overlap score
dened above as a predictor along with several covariates obtained from the information provided by the
subscriber. The decision tree produces a \matching" probability for any set of node pairs. Figure 9 shows
the performance of the decision tree on an independent test set of paired accounts. For the sample of 1537
pairs that we validated, the gure shows the observed proportion of matching node-pairs for each decile
of predicted matching probability. As the plot shows, account pairs with a high predicted probability of
matching based on our methodology were indeed usually associated with the same individual.

6 Communities of Interest
As stated earlier, edge sets larger than R2 (n) can be unmanageably large and remarkably uninformative.
To help reduce this \clutter" we often apply a thresholding function to the edge weights in the expansion of
subgraphs, so that any edge whose weight is below the threshold need not be expanded.
This threshold function is the simplest operator in a series of functions one can apply to an edge set
to bring out what we call the Community of Interest, or COI, for a given node. In telecommunications, a
COI operationalizes the notion of a \calling circle", the group of accounts around a specied account, where
there is some unobservable relationship between them (i.e., personal/professional/familial interests) that
motivates accounts to communicate with each other. Intuition suggests that when such a relationship exists,
that nodes involved in the relationship will be linked and that the weights along these links will be larger than
weights along links to nodes not sharing in the relationship. There is also the notion of diameter of a calling
circle since one can discuss \immediate calling circles" as well as \extended calling circles". Our subgraphs
17

captures these notions in a primitive fashion through R1 (n) and Rj (n); j > 1 edge sets respectively, but in
applications, the raw edge sets are often treated as the starting point in deriving a COI.
There are several reasons why the raw edge sets are often not sucient for capturing COIs. They can be
summarized into two main categories:

 spurious edges in Rj (n)
 missing edges in Rj (n)
Spurious edges arise from the fact that while we posit an unobservable relationship between accounts
that encourages communication between them, additional calls are captured in the data that can be totally
outside the relationship. For example, misdialed calls are captured by the edge sets, as well as unwanted
telemarketing calls. The eect of such calls on the edge sets can be enormous, since expanding the edge
set to the next diameter, brings in all the accounts linked to this spurious node, and arguably, these are
conceptually far removed from an accounts calling circle.
Missing edges arise in several ways reecting realities associated with large graphs arising from transactinoal data. One important way that edges are missing relates to the fact that in many applications there
are numerous service providers so that any single network carries only a fraction of the transactions. A
related way that edges are missing concern the locations of the devices in the network topology that records
transactions. By this we mean that transactions are recorded when they cross certain network elements
where recording equipment is located, and the corresponding records are then sent to a central repository
for analysis. But many transactions could occur \below" the recording point and subsequent analysis is
blind to these transactions. An example is the seperation between local and long distance calls in telecommunications whereby the long distance carrier is blind to any calls on the local level. Similarly for internet
trac monitoring, data collection equipment at internet gateway routers is blind to TCP/IP trac between
computers behind that network gateway.
In our applications of COI for large dynamic graphs we address these deciencies by introducing aspects
of the problem not captured in the available data. For example, we would like to discount edges that might
have large weight due simply to a single long call, for example, to a customer support center. One way
we deal with such nuances is to build a large edge set, and then nd the strongly connected component
containing the node of interest. This creates a subgraph where every node can be reached from every other
node. Another way of removing spurious edges is to apply a high threshold , which will mitigate the eects
of one-time non-representative calls.
Application of a thresholding function and applying a strongly connected component algorithm are both
examples of operators we apply to prune edges and nodes from Rj (n). Alternatively, if we believe that edges
may be missing from the observed set of transactional records, we may want to insert pseudo edges between
certain nodes in Rj (n). For example, local phone calls between accounts would not appear in trac collected
from a long distance network. Similarly since most networks, telephony or otherwise, exist in competitive
18

markets, the observed edge set collected from a single network is blind to trac carried on a competitors
network. If the notion of COI is meant to capture the existence of implicit underlying relationships, adding
certain pseudo edges to competitor nodes is a reasonable approach to uncovering such relationships.
As is clear from this discussion, the transformation of an edge set into a COI is not a science. One
normally lacks a reference for calibrating the process so that feed-back from COI-based applications is often
the only guidance.

7 Related Work
The analysis of directed graphs goes by many names and has been studied in many elds, including sociology
[16], epidemiology [10], information retrieval [15], statistics [3], operations research [13], and software engineering [12]. Clustering is a common goal in these elds, and is similar in spirit to our approach of dening
communities of interest.
Arguably the oldest research eld in this area is the eld of social networks. Social networks model the
interdependencies between \actors" or \agents" in a data set by analyzing the relationships between them,
represented as edges in a graph. This type of analysis has grown to study such diverse topics as disease
transmission, international trade, and computer networking. Social network theory can incorporate complex
stochastic models, explanatory variables for both nodes and edges, and time dependent graphs. However,
the eld has always focused on the study of small graphs. A popular textbook in the eld, Wasserman and
Faust[16], contains ve datasets used to illustrate the methodology, the largest of which contains 32 nodes.
The mathematically complex and computationally intensive methods generally do not scale, and to date, we
have not used them in our research.
Flake, Lawrence, and Giles[7] provide a denition of communities of interest in terms of number of edges
connecting a set of nodes that has the nice property that the COI can be eciently enumerated by applying
a maximum ow algorithm. Citation analysis of scientic papers also aims at nding communities in large
graphs. A distance between two documents is dened using co-citation (the number of citations in common)
or bibliographic coupling (the number of times both works are cited in other papers). Using this distance
measure, clusters in the database can be found. A successful example of this work is the NEC Research
Index [11] (http://citeseer.nj.nec.com/cs), which currently documents and cross references 7 million
scientic works, and for each of those works, lists the most similar books by several dierent metrics.
The Internet is natural to treat as a massive graph, where web sites are nodes, and the links between them
are edges. Current research [8, 9] uses \hubs" (sites that link to many others) and \authorities" (sites that
are linked to by others) in order to identify clusters in the web that deal with a particular topic. Extensions
of this work use network ow algorithms [7] and these are quite eective in nding small subject clusters.
Marketing has also inspired analysis of large graphs. The active research topics of market basket analysis
[1], viral marketing [6], and collaborative ltering [14] all use graph algorithms to discover communities of
19

consumers with similar behavior. These popular methods have been used successfully at sites like Amazon.com, which suggests items to purchase based on purchases of other customers who recently purchased
the same item.
Despite the wealth of research into large network graphs, our research is unique in combining the following
attributes:

 Scale. Our network graphs contain hundreds of millions of nodes, and we are potentially interested
in retrieving local subgraphs for any one of them.

 Speed. Our data structure, accessed recursively, along with o-line processing, allows us to compute
subgraphs centered on any node of the graph in fractions of a second.

 Dynamic updating. The graph incorporates the continuous stream of incoming data, so that any
analysis is as recent as the most recent data. Time is a crucial element, since today's network may
contain tens of thousands of new nodes and edges than yesterday's graph did. Our exponential updating
creates a smoothed view of network behavior, with the largest weights on the most recent events and
the smallest weights on the oldest events.

 Condensed representation of the graph. Conceptually, we view a massive graph as the union of a

massive number of small graphs R1 (n). The approximation we employ that limits node degree to the
top-k is eective because large dynamic graphs seem to be sparse as well.

Another appealing aspect of this work is that our applications measure direct interaction between the
nodes. Accounts form a community by actually communicating, creating a richer basis on which to dene
clusters. In collaborative ltering or market basket analysis, the goal is to nd indirect links between people.
Two people are similar not because of a direct interaction, but because they both purchased similar items.
Similarly, large scale web mining explores static links between pages, but not user trac along those links.

8 Conclusions
In this paper we introduced the concept of a dynamic graph and motivated the concept with network
data from a sample of AT&T residential accounts. This led to our denition of a dynamic graph Gt (at
time t) as an exponentially weighted average of the previous graph (at time t ? 1) and a new network
activity. We introduced a data structure that can be used to capture the evolution of a graph through time
that was amenable to the exponential weighting scheme. This data structure allows the subgraph around
any particular node to be quickly and eciently expanded to an arbitrary diameter. An application was
introduced that capitalized on this feature.
We have concentrated on the computational aspects of building and evolving the data structure for real
applications. We have not explored the statistical aspects of treating our data structure and the associated
20

algorithm for traversal as an approximation G^t (k) to the true graph Gt where k denotes the size of the top-k
edge set maintained in the data structure. Similarly models and methods in social networks, while not
applicable to massive graphs, are applicable to R2 (n) edge sets. These models might provide the rigorous
justication for transforming edge sets into COI that we currently lack. We hope to explore these ideas in
the near future.
Another topic for further research is how to prune a subgraph so that only informative edges and nodes
are retained. A common approach from (static) graph theory is to extract the strongly connected component.
The strongly connected component algorithm has the advantage that it scales linearly in the order of nodes,
and we have used it in our computationally intensive applications. However, we feel that certain features
inherent to telecommunication networks such as asymmetric edges (due to some customers subscribing to a
competitor), sinks (toll-free calling) and sources (large corporations), makes strongly connected components
a less than ideal choice for pruning subgraphs Rj (n).
Initializing, storing, and updating the data structures that we employ are facilitated by the programming
language Hancock, [4]. Hancock is a domain-specic C-based language for ecient and reliable programming
with transactional data. Hancock is publicly available for non-commercial use at
http://www.research.att.com/~kfisher/hancock/

.

References
[1] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Jorge B.
Bocca, Matthias Jarke, and Carlo Zaniolo, editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB,
pages 487{499. Morgan Kaufmann, 1994.
[2] Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. Science, 286:509{
512, 1999.
[3] David Maxwell Chickering and David Heckerman. Ecient approximations for the marginal likelihood
of bayesian networks with hidden variables. Machine Learning, 29(2-3):181{212, 1997.
[4] C. Cortes, K. Fisher, D. Pregibon, A. Rogers, and F. Smith. The Hancock language for signature
processing. In Proceedings of the Sixth International Conference on Knowledge Discovery and Data
Mining, 2000.
[5] C. Cortes and D. Pregibon. An information-mining platform. In Proceedings of the Fifth International
Conference on Knowledge Discovery and Data Mining, 1999.

21

[6] Pedro Domingos and Matt Richardson. Mining the network value of customers. In Proceedings of the
Seventh International Conference on Knowledge Discovery and Data Mining, pages 57{66. ACM Press,
2001.
[7] Gary Flake, Steve Lawrence, and C. Lee Giles. Ecient identication of web communities. In Sixth
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 150{160,
2000.
[8] David Gibson, Jon M. Kleinberg, and Prabhakar Raghavan. Inferring web communities from link
topology. In UK Conference on Hypertext, pages 225{234, 1998.
[9] Jon M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604{
632, 1999.
[10] Alden S Klovdahl. Social networks and the spread of infectious diseases, the aids example. Social
Science and Medicine, 21:1203 { 1216, 1985.
[11] Steve Lawrence, Kurt Bollacker, and C. Lee Giles. Indexing and retrieval of scientic literature. In
Eighth International Conference on Information and Knowledge Management, CIKM 99, pages 139{146,
Kansas City, Missouri, November 1999.
[12] S. Mancoridis, B. S. Mitchell, C. Rorres, Y. Chen, and E. R. Gansner. Using automatic clustering to
produce high-level system organizations of source code. In IEEE Proceedings of the 1998 Int. Workshop
on Program Understanding (IWPC'98), 1998.
[13] A. Ravindran, D. Phillips, and J. Solberg. Operations Research-Principle and Practice. John Wiley and
Sons, New York, USA, 1987.
[14] P. Resnick, N. Iacovou, M. Suchak, P. Bergstorm, and J. Riedl. GroupLens: An Open Architecture for
Collaborative Filtering of Netnews. In Proceedings of ACM 1994 Conference on Computer Supported
Cooperative Work, pages 175{186, Chapel Hill, North Carolina, 1994. ACM.
[15] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, New York,
NY, 1983.
[16] Stanley Wasserman and Katherine Faust. Social Network Analysis. Cambridge University Press, 1994.
[17] P. R. Winters. Forecasting sales by exponentially weighted moving averages. Management Science,
6:324{342, 1960.

22

arXiv:1103.1109v4 [cs.DS] 2 Aug 2016

Fully dynamic maximal matching in O(log n) update time
Surender Baswana
Department of CSE,
I.I.T. Kanpur, India

Manoj Gupta
Department of CSE,
I.I.T. Delhi, India

sbaswana@cse.iitk.ac.in

gmanoj@cse.iitd.ernet.in

Sandeep Sen
Department of CSE,
I.I.T. Delhi, India
ssen@cse.iitd.ernet.in

August 3, 2016

Abstract
We present an algorithm for maintaining a maximal matching in a graph under addition and deletion
of edges. Our algorithm is randomized and it takes expected amortized O(log n) time for each edge
update where n is the number of vertices in the graph. Moreover, for any sequence of t edge updates,
the total time taken by the algorithm is O(t log n + n log2 n) with high probability.

Note: The previous version of this result appeared in SIAM J. Comp., 44(1): 88-113, 2015. However, the
analysis presented there for the algorithm was erroneous. This version rectifies this deficiency without any
changes in the algorithm while preserving the performance bounds of the original algorithm.

1

1

Introduction

Let G = (V, E) be an undirected graph on n = |V | vertices and m = |E| edges. A matching in G is a
set of edges M ⊆ E such that no two edges in M share any vertex. The study of matchings satisfying
various properties has remained the focus of graph theory for decades [21]. It is due to the elegant structure
of matching that it also appears in various combinatorial optimization problems [11, 20]. A few well studied
matching problems are maximum cardinality matching [10, 17, 22, 23], maximum weight matching [12,
18], minimum cost matching (chapter 7, [19]), stable matching [13], popular matching [2]. Among these
problems, the maximum cardinality matching problem has been studied most extensively. A matching M
is of maximum cardinality if the number of edges in M is maximum. A maximum cardinality matching
(MCM) is also referred to as a maximum matching. A matching is said to be a maximal matching if it
cannot be strictly contained in any other matching. It is well known that a maximal matching guarantees
a 2-approximation of the maximum matching. Though it is quite easy to compute a maximal matching in
O(m + n) time, designing an efficient algorithm for maximum matching has remained a very challenging
√
problem for researchers [10, 22]. The fastest algorithm, till date, for maximum matching runs in O(m n)
time and is due to Micali and Vazirani [22]. In this paper, we address the problem of maintaining a maximal
matching in a dynamic graph.
Most of the graph applications in real life deal with graphs that are not static, viz., the graph changes
over time caused by deletion and insertion of edges. This has motivated researchers to design efficient
algorithms for various graph problems in dynamic environment. An algorithmic graph problem is modeled
in the dynamic environment as follows. There is an online sequence of insertion and deletion of edges and
the goal is to update the solution of the graph problem after each edge update. A trivial way to achieve this
is to run the best static algorithm for this problem after each edge update; clearly this approach is wasteful.
The aim of a dynamic graph algorithm is to maintain some clever data structure for the underlying problem
such that the time taken to update the solution is much smaller than that of the best static algorithm. There
exist many efficient dynamic algorithms for various fundamental problems in graphs [6, 16, 27, 28, 30].
Baswana, Gupta and Sen [5] had presented a fully dynamic algorithm for maximal matching which
achieved O(log n) expected amortized time per edge insertion or deletion. Moreover, for any sequence of
t edge updates, the total time taken by the algorithm is O(t log n + n log2 n) with high probability. Their
algorithm improved an earlier result of Onak and Rubinfeld [25] who presented a randomized algorithm
for maintaining a c-approximate (for some unspecified large constant c) matching in a dynamic graph with
O(log2 n) expected amortized time for each edge update.
This algorithm also implied a similar result for maintaining a two approximate vertex cover. It is also
used as a basis for maintaining approximate maximum weight matching in a dynamic graph [3].
Unfortunately, the analysis given in [5] has a crucial flaw, viz., the statement of Lemma 4.10 [5] is
not true in general. Although the bound on the update time was critically dependent on Lemma 4.10, we
have presented an alternate proof of the claimed update time based on an interesting property of the original
algorithm that was not reported earlier. For completeness, we have presented the full details of the algorithm
of [5] and the new corrected analysis in this paper. This property is interesting in its own right and may have
other useful applications of the algorithm.

2 An overview
Let M denote a matching of the given graph at any moment. Every edge of M is called a matched edge and
an edge in E\M is called an unmatched edge. For an edge (u, v) ∈ M , we define u to be the mate of v and
vice versa. For a vertex x, if there is an edge incident to it in the matching M , then x is a matched vertex;
otherwise it is free or unmatched.

2

In order to maintain a maximal matching, it suffices to ensure that there is no edge (u, v) in the graph
such that both u and v are free with respect to the matching M . From this observation, an obvious approach
will be to maintain the information for each vertex whether it is matched or free at any stage. When an edge
(u, v) is inserted, add (u, v) to the matching if u and v are free. For a case when an unmatched edge (u, v) is
deleted, no action is required. Otherwise, for both u and v we search their neighborhoods for any free vertex
and update the matching accordingly. It follows that each update takes O(1) computation time except when
it involves deletion of a matched edge; in this case the computation time is of the order of the sum of the
degrees of the two vertices. So this trivial algorithm is quite efficient for small degree vertices, but could be
expensive for large degree vertices. An alternate approach to handling deletion of a matched edge is to use
a simple randomized technique - a vertex u is matched with a randomly chosen neighbor v. Following the
standard adversarial model, it can be observed that an expected deg(u)/2 edges incident to u will be deleted
before
 deleting the
 matched edge (u, v). So the expected amortized cost per edge deletion for u is roughly
deg(u)+deg(v)
O
. If deg(v) < deg(u), this cost is O(1); but if deg(v) ≫ deg(u), then it can be as bad
deg(u)/2
as the trivial algorithm. We combine the idea of choosing a random mate and the trivial algorithm suitably
√
as follows. We first present a fully dynamic algorithm which achieves expected amortized O( n) time per
update. We introduce a notion of ownership of edges in which we assign an edge to that endpoint which has
higher degree. We maintain a partition of the set of vertices into two levels : 0 and 1. Level 0 consists of
vertices which own fewer edges than an appropriate threshold and we handle the updates in level 0 using the
trivial algorithm. Level 1 consists of vertices (and their mates) which own larger number of edges and we
use the idea of random mate to handle their updates. In particular, a vertex chooses a random mate from its
set of owned edges which ensures that it selects a neighbor having a lower degree.
√
A careful analysis of the O( n) update time algorithm suggests that a finer partition of vertices may
help in achieving a better update time. This leads to our final algorithm which achieves expected amortized
O(log n) time per update. More specifically, our algorithm maintains an invariant that can be informally
summarized as follows.
Each vertex tries to rise to a level higher than its current level, if, upon reaching that level, there are
sufficiently large number of edges incident on it from lower levels. Once a vertex reaches a new level, it
selects a random edge from this set and makes it matched.
Note that we say that ”a vertex rises” to indicate that the vertex moves to a higher level and ”a vertex
falls” to indicate that the vertex moves to a lower level. A vertex may also fall to a lower level if the number
of edges incident to it decreases. But a vertex only uses its neighborhood information to decide whether to
move to higher or lower level. Overall, the vertices use their local information to reach a global equilibrium
state in which after each update each vertex is at the right level having no incentive to either move above or
below its current level.

2.1 Organization of the paper
For a gentle exposition of the ideas and techniques, we first describe a simple but less efficient fully dynamic
algorithm for maximal matching in Section 4. We present our final fully dynamic algorithm which achieves
expected amortized O(log n) time per update in Section 5. In Section 6, we illustrate an example of a
dynamic graph that establishes the tightness of the approximation factor guaranteed by our algorithm. In
the following section, we describe notations and elementary results from the probability theory that we shall
use.

3

3 Preliminaries
We shall use M to denote the matching maintained by our algorithm at any stage. Our algorithms maintain a
partition of the set of vertices among various levels. We shall use LEVEL (u) to denote the level of a vertex u.
We define LEVEL (u, v) for an edge (u, v) as max(LEVEL (u), LEVEL (v)). Our algorithms will ensure that
both the endpoints of each matched edge in M are present at the same level. So the matching M maintained
by our algorithms will be a set of tuples as follows.
M = {(u, v, ℓ) | u is matched with v at level ℓ}
The analysis of our algorithms will use a basic result about asymmetric random walk as follows.
Asymmetric random walk on a line
Consider a particle performing a discrete random walk on a line. In each step, it moves one unit to the right
with probability p or to the left with probability q = 1 − p. Each move is independent of the moves made in
the past. The following lemma holds if p > q.
Lemma 3.1 Suppose the random walk starts at location ℓ units to the right of the origin. Then the proba ℓ
bility that it ever reaches origin within L steps for any given L is less than pq .

The proof of Lemma 3.1 is sketched in Appendix. During the analysis of algorithms, we shall use the
terminology very high probability for those events whose probability is 1 − nc for some positive constant c.

√
4 Fully dynamic algorithm with expected amortized O( n) time per update
The algorithm maintains a partition of the set of vertices into two levels - 0 and 1. We now introduce the
concept of ownership of the edges. Each edge present in the graph will be owned by one or both of its
endpoints as follows. If both the endpoints of an edge are at level 0, then it is owned by both of them.
Otherwise it will be owned by exactly that endpoint which lies at higher level. If both the endpoints are
at level 1, the tie will be broken suitably by the algorithm. As the algorithm proceeds, the vertices will
make transition from one level to another and the ownership of edges will also change accordingly. Let Ou
denote the set of edges owned by u at any moment of time. Each vertex u ∈ V will keep the set Ou in
a dynamic hash table [26] so that each search or deletion operation on Ou can be performed in worst case
O(1) time and each insertion operation can be performed in expected O(1) time. This hash table is also
suitably augmented with a linked list storing Ou so that we can retrieve all edges of set Ou in O(|Ou |) time.
The algorithm maintains the following three invariants after each update.
1. Every vertex at level 1 is matched. Every free vertex at level 0 has all its neighbors matched.
√
2. Every vertex at level 0 owns less than n edges at any moment of time.
3. Both the endpoints of every matched edge are at the same level.
The first invariant implies that the matching M maintained is maximal at each stage. A vertex u is
said to be a dirty vertex at a moment if at least one of its invariants does not hold. In order to restore
the invariants, each dirty vertex might make transition to some new level and do some processing. This
processing involves owning or disowning some edges depending upon whether the level of the vertex has
risen or fallen. Thereafter, the vertex will execute RANDOM - SETTLE or NAIVE - SETTLE to settle down at
its new level. The pseudocode of our algorithms for handling insertion and deletion of an edge is given in
Figure 1 and Figure 2.
4

Handling insertion of an edge
Let (u, v) be the edge being inserted. If either u or v are at level 1, there is no violation of any invariant. So
the only processing that needs to be done is to assign (u, v) to Ou if LEVEL (u) = 1, and to Ov otherwise.
This takes O(1) time. However, if both u and v are at level 0, then we execute HANDLING - INSERTION
procedure which does the following (see Figure 1).
Procedure
1
2
3
4
5
6
7
8
9
10

H A N D L I N G - I N S E R T I O N (u, v)

Ou ← Ou ∪ {(u, v)};
Ov ← Ov ∪ {(u, v)};
if u and v are FREE then M ← M ∪ {(u, v)};
if |Ov | > |Ou | then swap(u, v);
√
if |Ou | = n then
foreach (u, w) ∈ Ou do
delete (u, w) from Ow ;

x ← RANDOM - SETTLE (u);
if x 6= NULL then NAIVE - SETTLE(x);
if w was previous mate of u then NAIVE - SETTLE(w);

Procedure R A N D O M - S E T T L E (u): Finds a random edge (u, y) from the edges owned by u and
returns the previous mate of y
1
2
3
4
5
6

7
8

Let (u, y) be a uniformly randomly selected edge from Ou ;
foreach (y, w) ∈ Oy do
delete (y, w) from Ow ;
if y is matched then
x ← MATE (y);
M ← M\{(x, y)}
else
x ← NULL ;

M ← M ∪ {(u, y)};
← 1; LEVEL (y) ← 1;
return x;

9 LEVEL (u)
10

Procedure
1
2
3
4

N A I V E - S E T T L E (u)

: Finds a free vertex adjacent to u deterministically

for each (u, x) ∈ Ou do
if x is free then
M ← M ∪ {(u, x)};
Break;
Figure 1: Procedure for handling insertion of an edge (u, v) where LEVEL (u) =

LEVEL (v)

= 0.

Both u and v become the owner of the edge (u, v). If u and v are free, then the insertion of (u, v) has
violated the first invariant for u as well as v. We restore it by adding (u, v) to M. Note that the insertion
of (u, v) also leads to increase of |Ou | and |Ov | by one. We process that vertex from {u, v} which owns
√
larger number of edges; let u be that vertex. If |Ou | = n, then Invariant 2 has got violated. We execute
RANDOM - SETTLE (u); as a result, u moves to level 1 and gets matched to some vertex, say y, selected
5

randomly uniformly from Ou . Vertex y moves to level 1 to satisfy Invariant 3. If w and x were respectively
the earlier mates of u and y at level 0, then the matching of u with y has rendered w and x free. So to restore
Invariant 1, we execute NAIVE - SETTLE(w) and NAIVE - SETTLE(x). This finishes the processing of insertion
of (u, v). Note that when u rises to level 1, |Ov | remains unchanged. Since all the invariants for v were
satisfied before the current edge update, it follows that the second invariant for v still remains valid.
Handling deletion of an edge
Let (u, v) be an edge that is deleted. If (u, v) ∈
/ M, all the invariants are still valid. So let us consider the
nontrivial case when (u, v) ∈ M. In this case, the deletion of (u, v) has made u and v free. Therefore,
potentially the first invariant might have got violated for u and v, making them dirty. We do the following
processing in this case.
If edge (u, v) was at level 0, then following the deletion of (u, v), vertex u executes NAIVE - SETTLE(u),
and then vertex v executes NAIVE - SETTLE(v). This restores the first invariant and the vertices u and v are
clean again. If edge (u, v) was at level 1, then u is processed using the procedure shown in Figure 2 which
does the following (v is processed similarly).
Procedure H A N D L I N G - D E L E T I O N (u,v)
1 foreach (u, w) ∈ Ou and LEVEL (w) = 1 do
2
move (u, w) from Ou to Ow ;
√
3 if |Ou | ≥
n then
4
x ← RANDOM - SETTLE (u);
5
if x 6= NULL then NAIVE - SETTLE(x);
else
6
LEVEL (u) ← 0;
7
foreach (u, w) ∈ Ou and LEVEL (w) = 0 do
8
add (u, w) to Ow ;
9
10
11
12
13

NAIVE - SETTLE(u);

foreach (u, w) ∈ Ou do
√
if |Ow | = n then
x ← RANDOM - SETTLE (w);
if x 6= NULL then NAIVE - SETTLE(x);

Figure 2: Procedure for processing u when (u, v) ∈ M is deleted and LEVEL (u)=LEVEL (v)=1.
First, u disowns all its edges whose other endpoint is at level 1. If |Ou | is still greater than or equal to
√
n, then u stays at level 1 and executes RANDOM - SETTLE(u). If |Ou | is less than n, u moves to level 0
and executes NAIVE - SETTLE(u). Note that the transition of u from level 1 to 0 leads to an increase in the
number of edges owned by each of its neighbors at level 0. The second invariant for each such neighbor, say
√
w, may get violated if |Ow | = n, making w dirty. So we scan each neighbor of u sequentially and for each
√
dirty neighbor w (that is, |Ow | = n), we execute RANDOM - SETTLE(w) to restore the second invariant.
This finishes the processing of deletion of (u, v).
It can be observed that, unlike insertion of an edge, the deletion of an edge may lead to creation of a
large number of dirty vertices. This may happen if the deleted edge is a matched edge at level 1 and at least
one of its endpoints move to level 0.

√

6

4.1 Analysis of the algorithm
While processing the sequence of insertions and deletions of edges, an edge may become matched or unmatched at different update steps. We analyze the algorithm using the concept of epochs, which we explain
as follows.
Definition 4.1 At any time t, let (u, v) be any edge in M. Then the epoch defined by (u, v) at time t is the
maximal continuous time period containing t during which it remains in M. An epoch is said to belong to
level 0 or 1 depending upon the level of the matched edge that defines the epoch.
The entire life span of an edge (u, v) consists of a sequence of epochs of (u, v) separated by the continuous periods when (u, v) is unmatched. It follows from the algorithm that any edge update that does
not change the matching is processed in O(1) time. An edge update that changes the matching results
in the start of new epoch(s) or the termination of some existing epoch(s). For the sake of analysis, we
will redistribute the computation performed at any update step t among the epochs that are created or terminated at step t. More specifically, let epoch of (u1 , v1 ), (u2 , v2 ), . . . , (ul , vl ) be created and epochs of
(w1 , x1 ), (w2 , x2 ), . . . , (wk , xk ) be terminated at step t. We will redistribute total computation performed at
step t in such a way that:
Pl
Total computation performed at step t =
computation associated with the start of epoch (ui , vi ) +
Pi=1
k
i=1 computation associated with the termination of epoch (wi , xi )
Now, we shall analyze the computation involved in each procedure of our algorithm and distribute it
suitably among various epochs.
1.

NAIVE - SETTLE (u)
Observe that whenever the procedure NAIVE - SETTLE(u) is carried out, u is present at level 0, and
√
hence |Ou | < n. The procedure NAIVE - SETTLE(u) searches for a free neighbor of u by scanning
√
Ou . Hence, the time complexity of NAIVE - SETTLE(u) is O(|Ou |) = O( n). Furthermore, this
procedure is called whenever u loses its mate, say v. So we can associate the computation cost of
NAIVE - SETTLE (u) with the termination of the previous epoch (u, v).

2.

RANDOM - SETTLE (u)

3.

√
Observe that whenever RANDOM - SETTLE(u) is invoked, u owns at least n edges incident from level
√
0, and hence |Ou | ≥ n. During RANDOM - SETTLE(u), u finds a random mate from level 0. This
is done by selecting a random number r ∈ [1, |Ou |], and then picking the r th edge, say (u, y), from
the linked list storing Ou . This takes O(|Ou |) time. Vertex u then pulls y to level 1 to satisfy the
third invariant. In this process, y becomes the sole owner of all those edges whose other endpoint is
√
at level 0 (line 2,3). Since y was the owner of at most n edges, the total computation time involved
√
in performing this step is O( n). Other steps in RANDOM - SETTLE can be executed in O(1) time.
√
√
Hence the total computation time is O(|Ou | + n) which is O(|Ou |) since |Ou | ≥ n. We associate
this computation time with the start of the epoch (u, y) that gets created at level 1.
HANDLING - INSERTION (u, v)

√
This procedure takes O(1) time unless one of the endpoints of (u, v) starts owning n edges. In
that case, the procedure invokes RANDOM - SETTLE (line 8) and NAIVE - SETTLE (line 9,10). We have
already distributed the time taken in these procedures to the respective epochs that get created. Excluding these tasks, the only computation performed in this procedure is in the FOR loop. The purpose
√
of this loop is to make u the sole owner of all its edges incident from level 0. Since u owns n edges
√
from level 0, the total computation time involved in performing this step is O( n). We associate this
computation time with the start of the epoch created by u at level 1.
7

4.

HANDLING - DELETION (u, v)
Procedure HANDLING - DELETION(u, v) is carried out when the matched edge (u, v) at level 1 gets
deleted. In addition to invoking RANDOM - SETTLE and NAIVE - SETTLE procedures whose computation cost is already assigned to respective epochs, this procedure scans the list Ou at most twice.
Notice that |Ou | can be Θ(n). We associate this computation time of O(n) with the termination of
the epoch (u, v).

Excluding the updates that cause the start and termination of an epoch of (u, v), every other edge update
on u and v during the epoch is handled in just O(1) time. Therefore, we shall focus only on the amount
of computation associated with the start and termination of an epoch. Let us now analyze the computation
time associated with the epoch at level 0 and level 1.
• Epoch at level 0
As discussed above, it is only the procedure NAIVE - SETTLE whose computation time is associated
√
with an epoch at level 0. This procedure takes O( n) time. Hence the computation time associated
√
with an epoch at level 0 is O( n).
• Epoch at level 1
Consider an epoch at level 1. There are two ways in which this epoch gets created at level 1:
– In HANDLING - INSERTION
An epoch of (u, v) can be created during the procedure HANDLING - INSERTION(u, v). In this
case, the computation time associated with the start of the epoch of (u, v) is the computation
time incurred in executing the procedure HANDLING - INSERTION and the procedure RANDOM SETTLE which it invokes. It follows from the above discussion that the computation cost associ√
√
√
ated with the epoch (u, v) is O( n + |Ou |) which is O( n) since |Ou | = n when we invoke
HANDLING - INSERTION (u, v).
– In HANDLING - DELETION
Procedure HANDLING - DELETION(u, v) invokes RANDOM - SETTLE at lines 4 and 12 to create
new epochs at level 1. The execution of RANDOM - SETTLE at line 4 creates a new epoch for u
and its computation time O(|Ou |), which can be Θ(n), gets associated with the start of the new
epoch created by u. The execution of RANDOM - SETTLE at line 12 creates a new epoch for some
√
vertex w which is some neighbor of u. Note that |Ow | = n. Its computation time, which is
√
O( n), is associated with the start of the epoch at level 1 created by w.
Now let us calculate the computation cost associated with an epoch, say of an edge (u, v), at level 1
when it terminates. It follows from the discussion above that the only computation time associated
with the termination of epoch (u, v) is the computation time of HANDLING - DELETION (excluding the
time spent in procedures RANDOM - SETTLE and NAIVE - SETTLE that are already associated with the
start of their respective epochs). This cost is at most O(n).
From our analysis given above, it follows that the amount of computation time associated with an epoch
√
at level 0 is O( n) and the computation time associated with an epoch at level 1 is O(n).
An epoch of (u, v) may either terminate (if (u, v) is removed from the matching) or remain alive, i.e.,
(u, v) remain in the matching after the end of all the updates. An epoch of (u, v), ends because of exactly
one of the following causes.
(i) if (u, v) is deleted from the graph.
8

epoch of (u, v)
epoch of (u, v)

LEVEL

1
natural epoch

:

epoch of (u, w)
:

induced epoch

epoch of (v, x)

Time

LEVEL

0

epoch of (u, w)
epoch of (v, x)

Figure 3: Epochs at level 0 and 1; the creation of an epoch at level 1 can terminate at most two epochs at
level 0.
(ii) u (or v) get matched to some other vertex leaving its current mate free.
An epoch will be called a natural epoch if it terminates due to cause (i); otherwise it will be called an
induced epoch. Induced epoch can terminate prematurely since, unlike natural epoch, the matched edge is
not actually deleted from the graph when an induced epoch terminates.
It follows from the algorithm described above that every epoch at level 1 is a natural epoch whereas
an epoch at level 0 can be natural or induced depending on the cause of its termination. Furthermore,
each induced epoch at level 0 can be associated with a natural epoch at level 1 whose creation led to the
termination of the former. In fact, there can be at most two induced epochs at level 0 which can be associated
with an epoch at level 1. It can be explained as follows (see Figure 3).
Consider an epoch at level 1 associated with an edge, say (u, v). Suppose it was created by vertex u. If
u was already matched at level 0, let w 6= v be its mate. Similarly, if v was also matched already, let x 6= u
be its current mate at level 0. So matching u to v terminates the epoch of (u, w) as well as the epoch of edge
(v, x) at level 0. We charge the overall cost of these two epochs to the epoch of (u, v). We have seen that
√
the computational cost associated with an epoch at level 0 is O( n). So the overall computation charged to
√
an epoch of (u, v) at level 1 is O(n + 2 n) which is O(n).
Lemma 4.1 The computation charged to a natural epoch at level 1 is O(n) and the computation charged
√
to a natural epoch at level 0 is O( n).
In order to analyze our algorithm, we just need to get a bound on the computation charged to all natural
epochs that get terminated during a sequence of updates or are alive at the end of the all the updates. Let
us first analyze the computation cost charged to all those epochs which are alive at the end of t updates.
Consider an epoch of edge (u, v) that is alive at the end of t updates. If this epoch is at level 0, the computation cost associated with the start of this epoch is O(1). If this epoch is at level 1, then the computation
√
time associated with the start of this epoch is O(|Ou |) and notice that |Ou | ≥ n. Note that there can be at
√
most two induced epochs at level 0 whose computation time, which is O( n), is also charged to the epoch
of (u, v). Hence the computation charged to the live epoch of (u, v) is O(|Ou |). Observe that, at any given
moment of time, Ou ∩ Ow = ∅ for any two vertices u, w presentP
at level 1. Hence the computation time
charged to all live epochs at the end of t updates is of the order of u |Ou | ≤ 2t = O(t). So all we need is
to analyse the computation charged to all natural epochs that get terminated during the sequence of updates.
Let t be the total number of updates. Each natural epoch at level 0 which gets terminated can be assigned
uniquely to the deletion of its matched edge. Hence it follows from Lemma 4.1 that the computation charged
√
to all natural epochs terminated at level 0 during t updates is O(t n). We shall now analyze the number of
epochs terminated at level 1. Our analysis will crucially exploit the following lemma.

9

Lemma 4.2 Suppose vertex v creates an epoch at level 1 during an update in the graph. and let Ovinit be
the set of edges that v owned at the time of the creation of this epoch. Then, for any arbitrary sequence D
of edge deletions of Ovinit , and for any (v, w) ∈ Ovinit
Pr[MATE (v) = w | D] =

1
|Ovinit |

We first carry out the analysis for the high probability bound on the total update time taken by our algorithm.
Thereafter we carry out the analysis for the expected value of the total update time.

4.2 High probability bound on the total update time
The key idea of randomization is that once a vretex v creates an epoch at level 1, there should be many edge
deletions from Ovinit before the matched edge of v is deleted. In order to quanitfy this key idea, we introduce
the following definition that categorizes an epoch as good or bad.
Definition 4.2 An epoch is said to be bad if it gets terminated naturally with in the deletion of the first 1/3
edges that it owned at the time of its creation. An epoch is said to be good if it is not bad.
It follows from Definition 4.2 that a good epoch undergoes many edge deletions before getting terminated.
So only the bad epochs are problematic. Now using Lemma 4.2, we establish an upper bound on the
probability of an epoch to be bad.
Lemma 4.3 Suppose vertex v creates an epoch at level 1 during the kth update for some k ≤ t. Then this
epoch is going to be bad with probability 1/3 irrespective of the future updates in the graph and the random
bits picked during their processing.
Proof: Consider any sequence of updates in the graph following the creation of this epoch. This sequence
defines the sequence D of edge deletions of Ovinit . The termination of this epoch is fully determined by the
mate that v picked and this sequence D. This epoch will be bad if the mate of v is among the endpoints of
the first 1/3 edges in this sequence. Then, Lemma 4.2 implies that the mate of v is equally likely to be the
endpoint of any edge in this sequence. So the probability of the epoch to be bad is 1/3 1 .

For the time complexity analysis, we will show that the number of bad epochs may exceed the number
of good epochs by at most O(log n) with very high probability. Notice that the number of epochs created
at level 1 is itself a random variable whose value may depend upon the updates in the graph as well as the
random bits picked during their processing. However, as shown by Lemma 4.3, each newly created epoch
at level 1 will be bad with probability 1/3 irrespective of the past epochs. The number of epochs created at
level 1 during any t updates is trivially O(nt). Therefore, the sequence of epochs at level 1 can be seen as an
instance of the asymmetric random walk as follows. The walk starts at location 2 log2 n to the right of the
origin. Each step of the walk is one unit to the right of the current location with probability 2/3 or one unit
to the left with probability 1/3 independent of the past moves. We need to find the probability that the walk
ever reaches the origin during any time in the algorithm. It follows from Lemma 3.1 that the probability of
this event is less than 1/n2 . So the following lemma holds immediately.
Lemma 4.4 During any sequence of t updates, the number of bad epoch at level 1 can exceed the number
of good epochs by 2 log2 n with probability at most 1/n2 .
1

The analysis assumed that each edge from Ovinit is going to be deleted sometime in future. If not, place all such edges
arbitrarily at the end of the sequence D. In this case, the probability of the epoch to be bad will be even less than 1/3.

10

As stated in Lemma 4.1, each epoch at level 1 has a computation cost of O(n) charged to it. Let t be the
total number of updates in the graph. For each epoch at level 1, the number of owned edges at the time of
√
√
its creation is at least n. As a result the number of good epochs during t updates is bounded by 4t/ n
√
deterministically. So the computation cost of good epochs at level 1 is bounded by O(t n). Lemma 4.4
implies that the computation cost of all bad epochs at level 1 can exceed the computation cost of all good
epochs at most by cn log n amount for some constant c with probability ≥ 1 − 1/n2 . So overall the cost of
√
all epochs at level 1 is bounded by O((t n + n log n) with high probability. The computation cost of all
√
epochs at level 0 is bounded deterministically by O(t n). Hence the total computation time taken by our
√
algorithm for any sequence of t updates is O(t n + n log n) with high probability.

4.3 Expected value of the total update time
Let Xv,i,k be a random variable which is 1 if v creates an epoch at level i at update step k, otherwise it is 0.
We denote this epoch as EPOCH(v, i, k). Let Zv,i,k denote the number of edges from Ovinit that are deleted
during the epoch. (If EPOCH(v, i, k) is not created, Zv,i,k is defined
P as 0). Since each edge deletion at level
1 is uniquely associated to the epoch that owned it. Therefore, v,k Zv,1,k ≤ t. Hence,
X
v,k

E[Zv,1,k ] ≤ t

(1)

We shall now derive a bound on the expected value of Zv,1,k in an alternate way.
√
Lemma 4.5 E[Zv,1,k ] ≥ n/2 · Pr[Xv,1,k = 1].
Proof: We shall first find the expectation of Zv,1,k conditioned on the event that v creates an epoch at
level 1 during kth update. That is, we shall find E[Zv,1,k |Xv,1,k = 1]. Let Ovinit be the set of edges
owned by v at the moment of creation of EPOCH(v, 1, k), and let D be the deletion sequence associated
with Ovinit . It follows from Lemma 4.2 that the matched egde of v is distributed uniformly over Ovinit . So
√
√
E[Zv,1,k |Xv,1,k = 1] = |Ovinit |/2 ≥ n/2 since |Ovinit | for an epoch at level 1 is at least n. Using
conditional expectation, we get
√
E[Zv,1,k ] = E[Zv,1,k |Xv,1,k = 1] · Pr[Xv,1,k = 1] ≥ n/2 · Pr[Xv,1,k = 1]

Notice that the computation cost of an epoch at level 1 is at most cn for some constant c. So the expected
value of the computation cost associated with all natural epochs that get terminated at level 1 during t
updates is
X
√ X√
n/2 · Pr[Xv,1,k = 1]
cn · Pr[Xv,1,k = 1] = 2c n
v,k

v,k

√ X
≤ 2c n
E[Zv,1,k ]
√
≤ 2c nt

using Lemma 4.5

v,k

using Equation 1

We can thus conclude with the following theorem.
Theorem 4.1 Starting with a graph on n vertices and no edges, we can maintain maximal matching for any
√
√
sequence of t updates in O(t n) time in expectation and O(t n + n log n) with high probability.

11

√
4.4 On improving the update time beyond O( n)
In order to extend our 2-LEVEL algorithm for getting a better update time, it is worth exploring the reason
√
underlying O( n) update time guaranteed by our 2-LEVEL algorithm. For this purpose, let us examine the
second invariant more carefully. Let α(n) be the threshold for the maximum number of edges that a vertex
at level 0 can own. Consider an epoch at level 1 associated with some edge, say (u, v). The computation
associated with this epoch is of the order of the number of edges u and v own which can be Θ(n) in the
worst case. However, the expected duration of the epoch is of the order of the minimum number of edges u
can own at the time of its creation, i.e., Θ(α(n)). Therefore, the expected amortized computation per edge
deletion for an epoch at level 1 is O(n/α(n)). Balancing this with the α(n) update time at level 0, yields
√
α(n) = n.
In order to improve the running time of our algorithm, we need to decrease the ratio between the maximum and the minimum number of edges a vertex can own during an epoch at any level. It is this ratio that
determines the expected amortized time of an epoch. This insight motivates us for having a finer partition of
vertices – the number of levels should be increased to O(log n) instead of just 2. When a vertex creates an
epoch at level i, it will own at least 2i edges, and during the epoch it will be allowed to own at most 2i+1 − 1
edges. As soon as it owns 2i+1 edges, it should migrate to higher level. Notice that the ratio of maximum to
√
minimum edges owned by a vertex during an epoch gets reduced from n to a constant.
We pursue the approach sketched above combined with some additional techniques in the following
section. This leads to a fully dynamic algorithm for maximal matching which achieves expected amortized
O(log n) update time per edge insertion or deletion.

5 Fully dynamic algorithm with expected amortized O(log n) time per update
This algorithm maintain a partition of vertices among various levels. We describe the difference in this
partition vis-a-vis 2-LEVEL algorithm.
1. The fully dynamic algorithm maintains a partition of vertices among ⌊log4 n⌋ + 2 levels. The levels
are numbered from −1 to L0 = ⌊log4 n⌋. During the algorithm, when a vertex moves to level i, it
owns at least 4i edges. So a vantage point is needed for a vertex that does not own any edge. As a
result, we introduce a level -1 that contains all the vertices that do not own any edge.
2. We use the notion of ownership of edges which is slightly different from the one used in the 2-LEVEL
algorithm. In the 2-LEVEL algorithm, at level 0, both the endpoints of the edge are the owner of the
edge. Here, at every level, each edge is owned by exactly one of its endpoints. If the endpoints of the
edge are at different levels, the edge is owned by the endpoint that lies at the higher level. If the two
endpoints are at the same level, then the tie is broken appropriately by the algorithm.
Like the 2-LEVEL algorithm, each vertex u will maintain a dynamic hash table storing the edges Ou
owned by it. In addition, the generalized fully dynamic algorithm will maintain the following data structure
for each vertex u. For each i ≥ LEVEL (u), let Eui be the set of all those edges incident on u from vertices at
level i that are not owned by u. The set Eui will be maintained in a dynamic hash table. However, the onus
of maintaining Eui will not be on u. For any edge (u, v) ∈ Eui , v will be responsible for the maintenance of
(u, v) in Eui since (u, v) ∈ Ov . For example, suppose vertex v moves to level j. If j > LEVEL (u), then v
will remove (u, v) from Eui and insert it to Euj . Otherwise ( (j ≤ LEVEL (u)), v will remove (u, v) from Eui
and insert it to Ou .

12

3
2
v

1
0

x

−

1

Figure 4: A snapshot of the algorithm on K9 : all vertices are matched( thick edges) except vertex x at level
−1. φv (2) = 4 < 42 and φv (3) = 6 < 43 , so v cannot rise to a higher level.

5.1 Invariants and a basic subroutine used by the algorithm
As can be seen from the 2-level algorithm, it is advantageous for a vertex to get settled at a higher level once
it owns a large number of edges. Pushing this idea still further, our fully dynamic algorithm will allow a
vertex to rise to a higher level if it can own sufficiently large number of edges after moving there. In order
to formally define this approach, we introduce an important notation here.
For a vertex v with LEVEL (v) = i,

P
|Ov | + i≤k<j |Evk | if j > i
φv (j) =
0
otherwise
In other words, for any vertex v at level i and any j > i, φv (j) denote the number of edges which v can
own if v rises to level j. Our algorithm will be based on the following key idea. If a vertex v has φv (j) ≥ 4j ,
then v would rise to the level j. In case, there are multiple levels to which v can rise, v will rise to the highest
such level. With this key idea, we now describe the three invariants which our algorithm will maintain.
1. Every vertex at level ≥ 0 is matched and every vertex at level −1 is free.
2. For each vertex v and for all j >

LEVEL (v),

φv (j) < 4j holds true.

3. Both the endpoints of a matched edge are at the same level.
It follows that the free vertices, if any, will be present at level −1 only. Any vertex v present at level −1 can
not have any neighbor at level −1. Otherwise, it would imply that φv (0) ≥ 1 = 40 , violating the second
invariant. Hence, every neighbor of a free vertex must be matched. This implies that the algorithm will
always maintain a maximal matching. Furthermore, the key idea of our algorithm is captured by the second
invariant – after processing every update there is no vertex which fulfills the criteria of rising. Figure 4
depicts a snapshot of the algorithm.
An edge update may lead to the violation of the invariants mentioned above and the algorithm basically
restores these invariants. This may involve rise or fall of vertices between levels. Notice that the second
invariant of a vertex is influenced by the rise and fall of its neighbors. We now state and prove two lemmas
which quantify this influence more precisely.
Lemma 5.1 The rise of a vertex v does not violate the second invariant for any of its neighbors.

13

Proof: Consider any neighbor u of v. Let LEVEL (u) = k. Since the second invariant holds true for u before
the rise of v, so φu (i) < 4i for all i > k. It suffices if we can show that φu (i) does not increase for any i
due to the rise of v. We show this as follows.
Let vertex v rise from level j to ℓ. If ℓ ≤ k, the edge (u, v) continues to be an element of Ou , and so
there is no change in φu (i) for any i. Let us consider the case when ℓ > k. The rise of v from j to ℓ causes
removal of (u, v) from Ou (or Euj if j ≥ k) and insertion to Euℓ . As a result φu (i) decreases by one for each
i in [max(j, k) + 1, ℓ], and remains unchanged for all other values of i.

Lemma 5.2 Suppose a vertex v falls from level j to j − 1. As a result, for any neighbor u of v, φu (i)
increases by at most 1 for i = j and remains unchanged for all other values of i.
Proof: Let LEVEL (u) = k. In case k ≥ j, there is no change in φu (i) for any i due to fall of v. So let us
consider the case j > k. In this case, the fall of v from level j to j − 1 leads to the insertion of (u, v) in
Euj−1 and deletion from Euj . Consequently, φu (i) increases by one only for i = j and remains unchanged for
all other values of i.

In order to detect any violation of the second invariant for a vertex v due to rise or fall of its neighbors,
we shall maintain {φv (i)|i ≤ L0 } in an array φv [] of size L0 + 2. The updates on this data structure during
the algorithm will involve the following two types of operations.
•

This operation decrements φv (i) by one for all i in interval I. This operation
will be executed when some neighbor of v rises. For example, suppose some neighbor of v rises from
level j to ℓ, then φv (i) decreases by one for all i in interval I = [max(j, LEVEL (v)) + 1, ℓ].

•

INCREMENT-φ(v, i): this operation increases φv (i) by one. This operation will be executed when
some neighbor of v falls from i to i − 1.

DECREMENT-φ(v, I):

It can be seen that a single DECREMENT-φ(v, I) operation takes O(|I|) time which is O(log n) in the worst
case. On the other hand any single INCREMENT-φ(v, i) operation takes O(1) time. However, since φv (i) is
0 initially and is non-negative always, we can conclude the following.
Lemma 5.3 The computation cost of all DECREMENT-φ() operations over all vertices is upper-bounded by
the computation cost of all INCREMENT-φ() operations over all vertices during the algorithm.
Observation 5.1 It follows from Lemma 5.3 that we just need to analyze the computation involving all
INCREMENT-φ() operations since the computation involved in DECREMENT-φ() operations is subsumed by
the former.
If any invariant of a vertex, say u, gets violated, it might rise or fall, though in some cases, it may still
remain at the same level. However, in all these cases, eventually the vertex u will execute the procedure,
GENERIC - RANDOM - SETTLE , shown in Figure 5. This procedure is essentially a generalized version of
RANDOM - SETTLE (u) which we used in the 2-level algorithm. GENERIC - RANDOM - SETTLE (u, i) starts with
moving u from its current level (LEVEL (u)) to level i. If level i is higher than the previous level of u, then
u performs the following tasks. For each edge (u, w) already owned by it, u informs w about its rise to
level i by updating Ewi . In addition u acquires the ownership of all the edges whose other endpoint lies at a
level ∈ [LEVEL (u), i − 1]. For each such edge (u, w) that is now owned by u, we perform DECREMENTφ(w, [LEVEL (w) + 1, i]) to reflect that the edge is now owned by vertex u which has moved to level i.
Henceforth, the procedure then resembles RANDOM - SETTLE. It finds a random edge (u, v) from Ou and
moves v to level i. The procedure returns the previous mate of v, if v was matched. We can thus state the
following lemma.
14

Lemma 5.4 Consider a vertex u that executes GENERIC - RANDOM - SETTLE(u, i) and selects a mate v.
Excluding the time spent in DECREMENT-φ operations, the computation time of this procedure is of the
order of |Ou | + |Ov | where Ou and Ov is the set of edges owned by u and v just at the end of the procedure.

Procedure G E N E R I C - R A N D O M - S E T T L E (u, i)
1 if LEVEL (u) < i then
2
for each (u, w) ∈ Ou do
LEVEL(u)
3
transfer (u, w) from Ew
to Ewi ;
4
DECREMENT-φ(w, [LEVEL (u) + 1, i]);
5
6
7
8
9
10
11
12
13
14
15

16
17
18
19
20
21
22
23
24
25
26
27
28

//u rises to level i
//u informs w about its rise

for each j = LEVEL (u) to i − 1 do
for each (u, w) ∈ Euj do
transfer (u, w) from Euj to Ewi ;
transfer (u, w) from Ow to Ou ;
DECREMENT-φ(w, [j + 1, i]);

//u gains ownership of some more edges

foreach j = LEVEL (u) + 1 to i do φu (j) ← 0;
← i;

LEVEL (u)

Let (u, v) be a uniformly randomly selected edge from Ou ;
if v is matched then
x ← MATE (v);
M ← M\{(v, x)};
else
x ← NULL ;
for each (v, w) ∈ Ov do
LEVEL(v)
transfer (v, w) from Ew
to Ewi ;
DECREMENT-φ(w, [LEVEL (v) + 1, i]);

//v informs w about its rise

for each j = LEVEL (v) to i − 1 do
for each (v, w) ∈ Evj do
transfer (v, w) from Evj to Ewi ;
transfer (v, w) from Ow to Ov ;
DECREMENT-φ(w, [j + 1, i]);

//v gains ownership of some more edges

M ← M ∪ {(u, v)};
foreach j = LEVEL (v) + 1 to i do φv (j) ← 0;
LEVEL (v) ← i ;
return x;

/*v rises to level i*/

Figure 5: Procedure used by a free vertex u to settle at LEVEL i.

5.2 Handling edge updates by the fully dynamic algorithm
Our fully dynamic algorithm will employ a generic procedure called PROCESS - FREE - VERTICES(). The
input to this procedure is a sequence S consisting of ordered pairs of the form (x, k) where x is a free
vertex at level k ≥ 0. Observe that the presence of free vertices at level ≥ 0 implies that matching M
15

is not necessarily maximal. In order to preserve maximality of matching, the procedure PROCESS - FREE VERTICES restores the invariants of each such free vertex till S becomes empty. We now describe our fully
dynamic algorithm.
Handling deletion of an edge
Consider deletion of an edge, say (u, v). For each j > max(LEVEL (u), LEVEL (v)), we decrement φu (j)
and φv (j) by one. If (u, v) is an unmatched edge, no invariant gets violated. So we only delete the edge
(u, v) from the data structures of u and v. Otherwise, let k = LEVEL (u) = LEVEL (v). We execute the
Procedure PROCESS - FREE - VERTICES(h(u, k), (v, k)i).
Handling insertion of an edge
Consider insertion of an edge, say (u, v). Without loss of generality, assume that initially u was at the same
LEVEL(u)
level as v or a higher level than v. So we add (u, v) to Ou and Ev
. For each j > max(LEVEL (u), LEVEL (v)),
we increment φu (j) and φv (j) by one. We check if the second invariant has got violated for either u or v.
This invariant may get violated for u (likewise for v) if there is any integer i > max(LEVEL (u), LEVEL (v)),
such that φu (i) has become 4i just after the insertion of edge (u, v). In case there are multiple such integers,
let imax be the largest such integer. To restore the invariant, u leaves its current mate, say w, and rises to level
imax . We execute GENERIC - RANDOM - SETTLE(u, imax ), and let x be the vertex returned by this procedure.
Let j and k be respectively the levels of w and x. Note that x and w are two free vertices now. We execute
PROCESS - FREE - VERTICES (h(x, k), (w, j)i).
If the insertion of edge (u, v) violates the second invariant for both u and v, we proceed as follows. Let
j be the highest level to which u can rise after the insertion of (u, v), that is, φu (j) = 4j . Similarly, let ℓ
be the highest level to which v may rise, that is, φv (ℓ) = 4ℓ . If j ≥ ℓ, we allow only u to rise to level j;
otherwise
edge (u, v) becomes an element of
P we allow only v to rise to ℓ. Note that after u moves to level j, P
Evj . So LEVEL(v)≤k<ℓ |Evk | decreases by 1. As a result, φv (ℓ) = |Ov | + LEVEL(v)≤k<ℓ |Evk | also decreases
by 1 and is now strictly less than 4ℓ ; thus the second invariant for v is also restored.
5.2.1

Description of Procedure PROCESS - FREE - VERTICES

The procedure receives a sequence S of ordered pairs (x, i) such that x is a free vertex at level i. It processes
the free vertices in a decreasing order of their levels starting from L0 . We give an overview of this processing
at level i. For a free vertex at level i, if it owns sufficiently large number of edges, then it settles at level i
and gets matched by selecting a random edge from the edges owned by it. Otherwise the vertex falls down
by one level. Notice that the fall of a vertex from level i to i − 1 may lead to rise of some of its neighbors
lying at level < i. However, as follows from Lemma 5.2, for each such vertex v, only φv (i) increases by
one and φv () value for all other levels remains unchanged. So the second invariant may get violated only for
φv (i). This implies that v will rise only to level i. After these rising vertices move to level i (by executing
GENERIC - RANDOM - SETTLE ), we move onto level i − 1 and proceed similarly. Overall, the entire process
can be seen as a wave of free vertices falling level by level. Eventually this wave of free vertices reaches
level −1 and fades away ensuring maximal matching. With this overview, we now describe the procedure
in more details and its complete pseudocode is given in Figure 6.
The procedure uses an array Q of size L0 + 2, where Q[i] is a pointer to a queue (initially empty)
corresponding to level i. For each ordered pair (x, k) ∈ S, it inserts x into queue Q[k]. The procedure
executes a FOR loop from L0 down to 0 where the ith iteration extracts and processes the vertices of queue
Q[i] one by one as follows. Let v be a vertex extracted from Q[i]. First we execute the function FALLING(v)
which does the following. v disowns all its edges whose other endpoint lies at level i. If v owns less than 4i
16

Procedure P R O C E S S - F R E E - V E R T I C E S (S)
1 for each (x, i) ∈ S do ENQUEUE(Q[i], x);
2 for i = L 0 to 0 do
3
while (Q[i] is not EMPTY) do
4
v ← DEQUEUE(Q[i]);
5
if FALLING(v) then
6
LEVEL (v) ← i − 1;
7
ENQUEUE(Q[i − 1], v);
9
8
for each (u, v) ∈ Ov do
10
transfer (u, v) from Eui to Eui−1 ;
11
INCREMENT-φ(u, i);
12
INCREMENT-φ(v, i);
13
if φu (i) ≥ 4i then
14
x ← GENERIC - RANDOM - SETTLE(u, i);
15
if x 6= NULL then
16
j ← LEVEL (x);
17
ENQUEUE(Q[j], x);

//v falls to i − 1

//u rises to i

18

19
20
21
22

else
x ← GENERIC - RANDOM - SETTLE(v, i);
if x 6= NULL then
j ← LEVEL (x);
ENQUEUE(Q[j], x);

Function F A L L I N G (v)
i ← LEVEL (v);
2 for each (u, v) ∈ Ov such that LEVEL(u) = i do
3
transfer (u, v) from Ov to Ou ;
4
transfer (u, v) from Eui to Evi ;

//v settles at level i

1

5

//v disowns all edges at level i

if |Ov | < 4i then return TRUE else return FALSE;

Figure 6: Procedure for processing free vertices given as a sequence S of ordered pairs (x, i) where x is a
free vertex at LEVEL i.

17

edges then v falls to level i − 1, otherwise v will continue to stay at level i. The processing of the free vertex
v for each of these two cases is done as follows.
1. v has to stay at level i.
v executes GENERIC - RANDOM - SETTLE and selects a random mate, say w, from level j < i (if w is
present in Q[j] then it is removed from it and is raised to level i). If x was the previous mate of w,
then x is a falling vertex. Vertex x gets added to Q[j]. This finishes the processing of v.
2. v has to fall.
In this case, v falls to level i − 1 and is inserted to Q[i − 1]. At this stage, Ov consists of neighbors
of v from level i − 1 or below. It follows from Lemma 5.2 that the fall of v from i to i − 1 leads to
increase in φu (i) by one for each neighbor u of v which is present at a level lower than i. Moreover,
φv (i), that was 0 initially, has to be set to |Ov |. So all the vertices of Ov are scanned, and for each
(u, v) ∈ Ov , we increment φu (i) and φv (i) by 1. In case φu (i) has become 4i , u has to rise to level i
and is processed as follows. u executes GENERIC - RANDOM - SETTLE(u, i) to selects a random mate,
say w, from level j < i. If w was in Q[j] then it is removed from it. If x was the previous mate of w,
then x is a falling vertex, and so it gets added to queue Q[j].
Remark 5.1 Notice a stark similarity between the above procedure for handling a free vertex and the procedure for handling a free vertex at level 1 in the 2-level algorithm.
In case 1, v remains at level i and w moves to the level i from some level j < i. This renders vertex x
(earlier mate of w) free and the first invariant of x is violated. So x is added to the queue at level j. The
processing of v does not change φu () for any neighbor u of v. Furthermore, the rise of w to level i does not
lead to violation of any invariant due to Lemma 5.1. In case 2, v falls to level i − 1 and as a result some
vertices may rise to level i. Each such rising vertex executes GENERIC - RANDOM - SETTLE. As in case 1,
the processing of these rising vertices may create some free vertices only at level < i. We can thus state the
following lemma.
Lemma 5.5 After ith iteration of the for loop of PROCESS - FREE - VERTICES, the free vertices are present
only in the queues at level < i, and for all vertices not belonging to these queues the three invariants holds.
Lemma 5.5 establishes that after termination of procedure PROCESS - FREE - VERTICES, there are no free
vertices at level ≥ 0 and all the invariants get restored globally.

5.3 Analysis of the algorithm
Processing the deletion or insertion of an edge (u, v) begins with decrementing or incrementing φu (i) and
φv (i) for each level j > max(LEVEL (u), LEVEL (v)). Since there are O(log n) levels, the computation
associated with this task over any sequence of t updates will be O(t log n). This task may be followed by
executing the procedure PROCESS - FREE - VERTICES that restores the invariants and updates the matching
accordingly. The updates in the matching can be seen as creation of new epochs and termination of some of
the existing epochs. Like 2-level algorithm, for the purpose of analysis, we visualize the entire algorithm as
a sequence of creation and termination of various epochs. Excluding the O(t log n) time for maintaining φ,
the total computation performed by the algorithm can be associated with all the epochs that get terminated
and those that remain alive at the end of the sequence of updates. Along exactly similar lines as in 2-level
algorithm, the computation associated with all the epochs that are alive at the end of t updates is O(t) only.
So we just need to focus on the epochs that get terminated and the computation associated with each of
them.

18

Let us first analyse the computation associated with an epoch of a matched edge (u, v). Suppose this
epoch got created by vertex v at level j. So v would have executed GENERIC - RANDOM - SETTLE and selected
u as a random mate from level < j. Note that v must be owning less than 4j+1 edges and u would be owning
at most 4j edges at that moment. This observation and Lemma 5.4 imply that the computation involved in the
creation of the epoch is O(4j ). Once the epoch is created, any update pertaining to u or v will be performed
in just O(1) time until the epoch gets terminated. Let us analyze the computation performed when the
epoch gets terminated. At this moment either one or both u and v become free vertices. If v becomes free, v
executes the following task (see procedure PROCESS - FREE - VERTICES in Figure 6): v scans all edges owned
by it, which is less than 4j+1 , and disowns those edges incident from vertices of level j. Thereafter, if v still
owns at least 4j edges, it settles at level j and creates a new epoch at level j. Otherwise, v keeps falling one
level at a time. For a single fall of v from level i to i − 1, the computation performed involves the following
tasks: scanning the edges owned by v, disowning those incident from vertices at level i, incrementing φw
values for each neighbor w of v lying at level less than i, and updating φv (i) to |Ov |. All this computation
is of the order of the number of edges v owns at level i which is less than 4i+1 . Eventually either v settles at
some level k ≥ 0 and becomes part
Pof a new epoch or it reaches level −1. The total computation performed
by v is, therefore, of the order of ji=k 4i+1 = O(4j ). This entire computation involving v (and u) in this
process is associated with the the epoch of (u, v). Hence we can state the following Lemma.
Lemma 5.6 For any i ≥ 0, the computation associated with an epoch at level i is O(4i ).
An epoch corresponding to edge (u, v) at level i could be terminated if the matched edge (u, v) gets
deleted. Such an epoch is called a natural epoch. However, this epoch could be terminated due to one of the
following reasons also.
• u (or v) get selected as a random mate by one of their neighbors present at LEVEL > i.
• u (or v) starts owning 4i+1 or more edges.
Each of the above factors render the epoch to be an induced epoch. For any level i > 0, the creation of an
epoch causes termination of at most two epochs at levels < i. It can be explained as follows: Consider an
epoch at level i associated with an edge, say (u, v). Suppose it was created by vertex u. If u was already
matched at some level j < i, let w 6= v be its mate. Similarly, if v was also matched already at some level
k < i, let x 6= u be its mate. So matching u to v terminates the epoch of (u, w) and (v, x) at level j and k
respectively. We can thus state the following lemma.
Lemma 5.7 Creation of an epoch at a level i may cause termination of at most 2 epochs at level < i.
5.3.1

Analysing an epoch

Consider an epoch created by a vertex v at level i. At the time of the creation of the epoch, let Ovinit be
the set of edges owned by v, and let w = MATE(v). This epoch may terminate much before the deletion
of (v, w). This happens when v or w moves to some level > i before the deletion of (v, w). In order to
analyse termination of an epoch, therefore, we associate an update sequence with it as follows. For each
edge (v, x) ∈ Ovinit , we consider the first time in future that x moves to some higher level 2 . The update
label associated with edge (v, x) is defined as
if x moves to a level > i before its deletion then it is classified as upward else it is deletion.
2

vertex x may move to level > i (and down) multiple times while the algorithm processes a sequence of updates. However, it
is only the first time (after the creation of the epoch) when x moves to a level > i that is relevant as far as the possibility of the
termination of the epoch by the upward movement of x is concerned.

19

Likewise, we also consider the first time in future that v moves to a level > i. If v never moves to any level
> i in future, we just append v at the end of all the updates associated with Ovinit . The update sequence U for
the epoch is the sequence of these updates on the edges of Ovinit and vertex v arranged in the chronological
order. Consider the following example. Suppose Ovinit has 10 edges and let the corresponding neighbors
of v be {w1 , . . . , w10 }. Let the updates in the chronological order be : the deletion of (v, w4 ), upward
movement of w1 , upward movement of w9 , the deletion of (v, w5 ), and so on. The corresponding update
sequence will be
•

↑

↑

•

•

•

↑

↑

•

↑

•

U : h w4 , w1 , w9 , w5 , w8 , w3 , w2 , v, w7 , w10 , w8 i
Observation 5.2 If the update associated with (the owner) v appears at ℓth location in U , then the epoch
will terminate on or before the ℓth update in U . Therefore, the updates at location > ℓ in U will have no
influence on the termination of the epoch.
Unlike the 2-level algorithm, the update sequence associated with an epoch is not uniquely defined by the
sequence of updates in the graph after the creation of the epoch. Rather, it also depends upon the current
matching as well as the random bits chosen by the algorithm while processing the updates. So there is
a probability distribution defined over all possible update sequences that depends upon these two factors.
Consequently, the analysis of an epoch in our final algorithm is more complex compared to the 2-level
algorithm. In particular, it is not obvious whether there is any dependence between the random mate picked
by a vertex while creating an epoch and the sequence of updates associated with the epoch. However, using
an interesting non-trivial property of our algorithm, we will establish that there is no dependence between
the two.
Lemma 5.8 Suppose a vertex v creates an epoch and let Ovinit be the set of its owned edges at the time of
the creation of this epoch. Then, for any update sequence U and for each (v, w) ∈ Ovinit ,
Pr[MATE (v) = w | U ] = Pr[MATE (v) = w] =

1
|Ovinit |

Lemma 5.8 can be seen as a generalization of Lemma 4.2 that we stated for our 2-level algorithm. Its proof
is given in Section 5.4. The analysis of our algorithm will be critically dependent on this lemma. Using this
lemma, we shall first establish a high probability bound on the total update time of the algorithm to process
a sequence of updates in the graph.
5.3.2

High probability bound on the total update time

Recall Definition 4.2 of a bad epoch. It can be observed from this definition that an induced epoch is always
a good epoch. Using Lemma 5.8, the lemma for the probability of a bad epoch extends seamlessly from
2-level algorithm to our final algorithm as follows.
Lemma 5.9 Suppose vertex v creates an epoch at level i while the algorithm processes kth update in the
graph. This epoch will be bad with probability at most 1/3 irrespective of the updates in the graph and the
random bits picked during their processing.
Proof: The termination of the epoch is completely determined by the mate that v picks and the update
sequence associated with this epoch. Consider any update sequence U associated with this epoch. It follows
from Lemma 5.8 that conditioned on U , the mate of v is equally likely to be the endpoint of any edge in
20

Ovinit . Now recall from Observation 5.2 that for the epoch to be terminated naturally, the mate of v must be
among the endpoints of the deleted edges that precede v in U . We distinguish between the following two
cases.
Case1. There are less than |Ovinit |/3 edge deletions preceding v in U .
The epoch will be bad only if the matched edge of v is one of these edge deletions preceding v in U .
Since the number of these edge deletions is less than |Ovinit |/3, so using Lemma 5.9 the probability
of the epoch to be bad is less than 1/3.
Case2. There are at least |Ovinit |/3 edge deletions preceding v in U .
The epoch will be bad if the matched edge of v is one of the first |Ovinit |/3 edge deletions in U . From
Lemma 5.8, the termination of the epoch is equiprobable for any of the Ovinit ’s, so the probability that
the epoch is bad in this case is exactly 1/3.
It follows that the epoch is going to be bad with probability 1/3 for each possible update sequence U associated with the epoch.

We will show that the number of bad epochs at a level i could exceed the number of good epochs at
level i by at most O(log n) with very high probability. Notice that the number of epochs created at level i
is itself a random variable. During any update, the number of epochs that will be created at level i depends
upon the past updates in the graph and the random bits picked during their processing. However, Lemma
5.9 implies that each newly created epoch at level i will be bad with probability at most 1/3 independent of
these events. Hence, the sequence of epochs at level i can be seen as an instance of the asymmetric random
walk as established in the analysis of the 2-level algorithm. So the bad epochs at any level i may exceed the
good epochs by 2 log 2 n with probability at most 1/n2 . There are O(log n) levels in the hierarchy. Hence
we get the following lemma using union bound.
Lemma 5.10 For every level i ≤ L0 , the number of bad epochs will not exceed the number of good epochs
by more than 2 log2 n with probability at least 1 − (log n)/n2 > 1 − 1/n.
Let us temporarily exclude the maximum surplus of O(log n) bad epochs at each level from our analysis.
Consequently, it follows from Lemma 5.10 that each bad epoch at a level can be mapped to a good epoch
at the same level in a unique manner - see Figure 7(i). Also the creation of each epoch at a level i + 1
can terminate at most two (induced) epochs at lower levels as stated in Lemma 5.7. Using this fact and
the mapping between the good and bad epochs at a level, we can construct a forest whose nodes will be
the epochs terminated across all levels during the algorithm. The intuition for defining this forest is that
eventually the computation cost of a bad epoch or an induced epoch will be charged to a good natural epoch.
Since a good natural epoch has sufficiently large number of edge deletions associated with it, these edge
deletions can be charged to pay for all the computation carried out by our algorithm.
With this intuition, we now provide the construction of the forest by defining parent of each epoch using
the following rules.
1. Parent of each induced epoch is the epoch at the higher level whose creation led to its termination.
2. Parent of a good epoch is itself (hence it is the root of its tree).
3. If a bad epoch is mapped to an induced epoch, then its parent is the same as the parent of the induced
epoch. Otherwise, it is the parent of itself (hence it is the root of its tree).

21

... ...

. . .a

...

b ...

...

c

g. . .

d. . .

g

i+1

a

i

b

A natural good epoch
An induced epoch
A bad epoch
c

d

(ii)

(i)

Figure 7: (i) Mapping between bad and good epochs at level i (ii) Assigning at most 4 epochs from lower
levels to an epoch.
It follows from rule 1 and 3 (the if part) that with an epoch at a level, at most 4 epochs from lower levels
can be associated. Hence each node in the forest will have at most four children. See Figure 7(ii). Moreover,
the root of each tree in the forest of epochs is either a bad epoch or a good natural epoch. Using Lemma
5.6, the computation cost C(i) associated with a tree of epochs whose root is at level i obeys the following
recurrence for some constant a.
C(i) = a4i + 4C(i − 1)

The solution of this recurrence is C(i) = O(i4i ). It follows from Lemma 5.10 and rule 3(Otherwise part)
that the trees rooted at good natural epochs at a level i are at least the number of trees rooted at bad epochs at
level i. Hence, it suffices to analyze the computation cost associated with all the tree rooted at good natural
epochs. Now for each good natural epoch at a level i, there are at least 4i /3 edge deletions associated
uniquely to it. This natural epoch will be charged for the computation cost C(i) = O(i4i ) associated with
the tree rooted at it. So if t is the total number of updates in the graph, then the computation cost associated
with all epochs
P in the forest is O(t log n). The computation cost associated with surplus bad epochs at all
levels is O( i i4i log n) = O(n log2 n). Hence with high probability the computation cost for processing
t edge updates by the algorithm is O(t log n + n log2 n). This also implies that the total expected update
time is O(t log n) for t = Ω(n log n). In the following subsection, we will establish O(t log n) bound on
the expected update time for all values of t.
5.3.3

Expected value of the total update time

During a sequence of t updates in the graph, various epochs get created by various vertices at various levels.
Let Xv,i,k be a random variable which is 1 if v creates an epoch at level i at update step k, otherwise it
is 0. We denote this epoch as EPOCH(v, i, k). Let Ovinit denote the edges that v owned at the time of the
creation of the epoch. Let Zv,i,k denote the number of edges from Ovinit that are deleted during the epoch.
(If EPOCH(v, i, k) is not created, Zv,i,k is defined as 0). The key role in bounding the expected running time
is played by a random variable Bv,i,k defined as follows:
(
(8Zv,i,k − 2 · 4i )Xv,i,k if EPOCH (v, i, k) is natural
(2)
Bv,i,k =
(4i+1 − 2 · 4i )Xv,i,k
if EPOCH (v, i, k) is induced
First observe that Bv,i,k = 0 if Xv,i,k = 0. Else (if Xv,i,k = 1), the random variable Bv,i,k can be seen
as credits associated with EPOCH(v, i, k) to be used for paying its computation cost. For a natural epoch, the
credits is defined in terms of the edges deleted during the epoch. So we define Bv,i,k to be 8Zv,i,k . However,
we need to discount for the two epochs at lower levels that may get terminated due to EPOCH(v, i, k). To
this end, from the term, we deduct 2 · 4i . Similarly, if EPOCH(v, i, k) is an induced epoch, then it gets 4i+1
credits from the epoch that destroyed it. But here again we need to discount for the two epochs at lower
22

i
i+1
levels that might be
P terminated by it. To this end, we again deduct 2 · 4 from 4 . The following lemma
gives a bound on Bv,i,k .

Lemma 5.11

P

v,i,k

Bv,i,k ≤ 8

P

v,i,k

Zv,i,k ≤ 8t, where t is the total number of updates in the graph.

Proof: We need to analyze the sum of Bv,i,k ’s for all those (i, v, k) values for which the EPOCH(i, v, k)
got created. If this epoch is an induced epoch, it can be associated with an epoch, say EPOCH(v ′ , i′ , k′ ), at
′
term in Bv′ ,i′ ,k′ cancels out
a higher level i′ > i whose creation destroyed it. Notice that the negative 4i P
the positive 4i+1 term in Bv,i,t . Hence, the contribution of induced epochs P
in v,i,k Bv,i,k is P
nullified and
all that remains is the sum of terms 8Zv,i,k for each natural epoch. Hence v,i,k Bv,i,k ≤ 8 v,i,k Zv,i,k .
An edge deletion
is associated with an epoch in a unique manner, so will contribute to exactly one Zv,i,k .
P
Therefore, v,i,k Zv,i,k is upper bounded by the total number of edges deleted.

P

Corollary 5.0.1

v,i,k

E[Bv,i,k ] ≤ 8t

Lemma 5.12 For all i, v, k, E[Bv,i,k ] ≥ Pr[Xv,i,k = 1] · 4i .
Proof: Since Xv,i,k is an indicator random variable, E[Bv,i,k ] = Pr[Xv,i,k = 1] E[Bv,i,k | Xv,i,k = 1]. We
will first estimate E[Bv,i,k | Xv,i,k = 1], that is, the expected value of Bv,i,k given that EPOCH(v, i, k) got
created.
Let (U , P ) be the probability space of all the update sequences associated with this epoch and let U ∈ U
be any update sequence. Suppose among the updates in U that precede the update associated with v, only
d are edge deletions. It follows from Lemma 5.8 that the matched edge of v is distributed uniformly over
Ovinit . So EPOCH(v, i, k) will be an induced epoch with probability (|Ovinit | − d)/|Ovinit | and in that case
B(v, i, k) will be 4i+1 − 2 · 4i . If the epoch is natural, it couldP
be due to any one of the d edge deletions
present in U . In that case the expected value of Bv,i,k will be 1/d dj=1 (8j −2·4i ) ≥ 4d−2·4i . Considering
the cases of induced and natural epoch together,
E[Bv,i,k | U ] =

d
4i+1 d − 4d2
|Ovinit | − d i+1
i
i
i
(4
−
2
·
4
)
+
(4d
−
2
·
4
)
=
2
·
4
−
|Ovinit |
|Ovinit |
|Ovinit |
4i · 4i
≥ 2 · 4i − init (for all values of d)
|Ov |

Therefore
E[Bv,i,k ] =

X

U ∈U

E[Bv,i,k | U ] · Pr[U ] ≥



4i · 4i
2 · 4 − init
|Ov |
i

Since |Ovinit | ≥ 4i , for level i, the result follows.

 X
4i · 4i
·
Pr[U ] = 2 · 4i − init
|Ov |
U ∈U



Let Wv,i,k be a random variable that corresponds to the value of the computation cost of EPOCH(v, i, k)
if the epoch is created and is 0 otherwise. Notice that the computation cost of an epoch at level i is c4i+1 for
some constant c. So, E[Wv,i,k ] = Pr[Xv,i,k = 1]c4i+1 . Therefore, using Lemma 5.12,
E[Wv,i,k ] ≤ 4cE[Bv,i,k ]

(3)

Using the above equation and Corollary 5.0.1, the total expected computation cost associated with all epochs
that get destroyed during the algorithm can be bounded by O(t) as follows.
X
X
E[Wv,i,k ] ≤
4cE[Bv,i,k ] ≤ 32ct = O(t)
v,i,k

v,i,k

23

Since for each update in the graph, we incur O(log n) time to update φ at various levels, there is an O(t log n)
overhead for t updates. We can thus conclude with the following theorem.
Theorem 5.1 Starting with a graph on n vertices and no edges, we can maintain a maximal matching for
any sequence of t updates in O(t log n) time in expectation and O(t log n + n log2 n) with high probability.

5.4 Proof of Lemma 5.8
Our algorithm use randomization to maintain maximal matching. After any given sequence of updates,
there is a set of possible maximal matchings that the algorithm may be maintaining and there is probability
distribution associated with these maximal matchings. So it is useful to think about the probability space of
these matchings as the algorithm proceeds while processing a sequence of updates.
We introduce some notations first. For any matching M maintained at any stage by our algorithm, let
Mi denote the matching at level i. Let M>i = ∪j>i Mj denote the matchings at all levels > i. Let Vi
denote the set of all the vertices belonging to levels in the range ∈ [−1, i]. We now extend the notations to
incorporate the updates in the graph. For any k ≥ 1, let G(k) denote the graph after a given sequence of k
updates and let M(k) denote the maximal matching of G(k) as maintained by our algorithm. Let M>i (k)
denote the matching at all levels > i after a given sequence of k updates.
After processing certain number of updates by the algorithm, suppose M and M ′ are any two matchings
′ . Consider any single update in the graph at this stage. In order to process it,
possible such that M>i = M>i
suppose we carry out two executions I and I ′ of our algorithm with the initial matching being M and M ′
respectively. That is, M(0) = M in the execution I and M(0) = M ′ in the execution I ′ . Our claim is
that the probability distribution of matching at levels > i will be identical at the end of both the executions.
More precisely, for any maximal matching µ(1) on a subset of vertices in graph G(1),
Pr[M>i (1) = µ(1)|M(0) = M ] = Pr[M>i (1) = µ(1)|M(0) = M ′ ]
In order to establish our claim, we shall crucially exploit the following lemma.
Lemma 5.13 For both the matchings M and M ′ , φv (j) is the same for each v ∈ V and j > i.
′ . This implies that for each level j > i the sets of vertices present are
Proof: It is given that M>i = M>i
identical in M and M ′ . Hence the set Vi of all the vertices present at levels ∈ [−1, i] is identical in M and
M ′ . Hence for any vertex v, and any level j > i, the set of all the neighbours of v at levels < j is identical;
notice that φv (j) is just the cardinality of this set. So it follows that φv (j) is the same for each vertex v and
each j > i.


We shall now establish our claim for the deletion of an edge e = (u, v). Establishing the claim for the
insertion of an edge is similar. Notice that our algorithm does not alter the matching if e is not a matched
edge. If e is a matched edge, a wave of free vertices originates from LEVEL (e) and propagates downward.
The following fact follows from our analysis in Section 5.2.1.
F 1. The algorithm won’t alter the matching at level >

LEVEL (e)

while processing the deletion of e.

F 2. The matching is updated in the decreasing order of levels, and once the updating of the matching at a
level is complete, the matching at that level will remain unchanged during the updates of the matching
at lower levels.

24

It follows from the description of M and M ′ that either LEVEL (e) is less than or equal to i in both the
matchings or LEVEL (e) is the same in M and M ′ . Let us first consider the (easier) case when LEVEL (e) ≤ i
in M as well as M ′ . It follows from Fact F 1 stated above that the only changes in matching M and M ′
will be at levels ≤ i. Hence the matching M>i (1) will be identical at the end of both the executions I
and I ′ . Let us now consider the more interesting case of LEVEL (e) > i. Both the executions I and I ′
invoke the procedure PROCESS - FREE - VERTICES(h(u, LEVEL (e)), (v, LEVEL (e))i) in this case. The reader
is recommended to revisit this procedure from Section 5.2.1 before proceeding further.
In order to establish our claim about I and I ′ , we shall establish the following. While the matching at
levels > i is being updated, for each step executed in I, the identical step can be executed in I ′ . Moreover,
if the step in I is executed with some probability, the step will be executed with the same probability in I ′
as well. In order to show this, let us analyse the first iteration of the procedure PROCESS - FREE - VERTICES.
Both I and I ′ will process u first. After disowning its edges from its present level, u owns the same set of
edges in both the executions. Thereafter, u will either stay at the same level or fall by one level. If u stays
at the same level, it chooses a random edge to get matched. The probability that any specific random edge
is picked by u is the same in both the executions. Let us consider the case that u falls by one level. For
each neighbour z of u, it follows from Lemma 5.13 that φℓ (z) is the same in the case of M and M ′ . Hence
the set of vertices rising to level ℓ are the same in both the executions. In addition, the set of edges that
each such vertex owns on rising to level j is also the same, hence, the probability that any specific random
mate is picked is the same in both the executions. So each update in M and M ′ is equally likely during the
processing of u. The reader may note that after each such identical update in M and M ′ , the matchings are
identical at each level > i. Hence, Lemma 5.13 holds again for the updated matchings.
Unlike the first iteration, a generic iteration of the procedure PROCESS - FREE - VERTICES may have free
vertices at levels ≤ LEVEL (e) that are kept in respective queues at these levels. Suppose in the beginning
of any such iteration of the procedure PROCESS - FREE - VERTICES there are two possible configurations such
that the matching as well as the queue storing the free vertices are identical at each level > i but differ at
levels ≤ i. Lemma 5.13 will hold for these configurations as well. Therefore, along exactly the same lines
as the first iteration analysed above, it can be shown that every update in the matching at level > i will be
carried out with the same probability during any generic iteration for any two configurations that match at
all levels > i.
Therefore, each sequence of updates in the matching is equally likely in both the executions I and I ′ till
the last free vertex at level i + 1 is processed. Henceforth, the two executions may differ. But as follows
from Fact F 2, it will affect only the matching at levels ≤ i and there won’t be any change in the matching
at higher levels.
This concludes our claim for a single update. This claim can be invoked appropriately for a sequence of
updates giving us the following theorem.
Theorem 5.2 Let M and M ′ be any two matchings possible by our algorithm at any time such that M>i =
′ . For any sequence of t update in the graph, suppose we carry out two executions I and I ′ of our
M>i
algorithm with the initial matching being M and M ′ respectively. The probability distribution of matching
at every level > i will be identical at the end of both the executions. That is,
Pr[M>i (t) = µ(t), . . . , M>i (1) = µ(1)|M(0) = M ] = Pr[M>i (t) = µ(t), . . . , M>i (1) = µ(1)|M(0) = M ′ ]
where µ(j), for 1 ≤ j ≤ t, is any maximal matching on a subset of vertices in the graph G(j).
For the proof of Theorem 5.2, we shall apply the argument for single update inductively and use the following lemma from elementary probability theory.
Lemma 5.14 Suppose A, B, C are three events defined over a probability space (Ω, P ). Then,
Pr[A ∩ B | C] = Pr[A | B ∩ C] · Pr[B | C]
25

Let us define events C as M(0) = M and C ′ as M(0) = M ′ . We have shown that Pr[M>i (1) =
µ(1) | C] = Pr[M>i (1) = µ(1) | C ′ ]. If we define event B as M>i (1) = µ(1) then by another application
of the arguments that we used for a single update,
Pr[M>i (2) = µ(2) | B, C] = Pr[M>i (2) = µ(2) | B, C ′ ]
Applying Lemma 5.14, we get
Pr[M>i (2) = µ(2), B | C] = Pr[M>i (2) = µ(2) | B, C] · Pr[B | C]
Since Pr[B | C] = Pr[B|C ′ ], it follows that
Pr[M>i (2) = µ(2), B | C] = Pr[M>i (2) = µ(2), B | C ′ ]
The above argument can be inductively applied for every subsequent update. This completes the proof of
Theorem 5.2
5.4.1

Connection to the analysis

We first state two lemmas from elementary probability theory that deal with the independence of events. For
the sake of completeness, the proof of these lemmas is given in Appendix.

The first lemma deals with conditional probability.
Lemma 5.15 Let A be an event and B1 , . . . , Bk be k mutually exclusive events defined over a probability
space (Ω, P ). If Pr[A | Bj ] = ρ for each 1 ≤ j ≤ k, then Pr[A | C] = ρ where event C = ∪j Bj .
The second lemma deals with independence of events. Let A and B be two events defined over a probability
space (Ω, P ). A is said to be independent of B if Pr[A | B] = Pr[A | B̄] = Pr[A]. Alternatively,
Pr[A ∩ B] = Pr[A] · Pr[B]. The notion of independence gets carried over from events to random variables
in a natural manner as follows.
Definition 5.1 An event A is said to be independent of a random variable X if for each x ∈ X, Pr[A | X =
x] = Pr[A].
Lemma 5.16 Suppose A is an event and X be a random variable defined over probability space (Ω, P ). If
A is independent of X, then for each x ∈ X,
Pr[X = x | A] = Pr[X = x]
Now we shall establish the connection of Theorem 5.2 to the anlysis of our algorithm. In particular, we shall
use this theorem to prove Lemma 5.8. Suppose a vertex v creates an epoch at level i while the algorithm
processes kth update in the graph for any k < t. We shall analyse the probability space of the future
matchings starting from the time just before the creation of this epoch.
While creating its epoch, v chooses its mate randomly uniformly out of Ovinit . Clearly, the change in the
matching at levels ≤ i will depend on the mate that v picks. Let M be the set of all possible matchings once
the algorithm completes the processing of the kth update. Now notice that all matchings from the set M are
identical at each level > i. So it follows from Theorem 5.2 that for any two matchings M, M ′ ∈ M,
26

Pr[M>i (t) = µ(t), . . . ,M>i (k + 1) = µ(k + 1) | M(k) = M ]
= Pr[M>i (t) = µ(t), . . . , M>i (k + 1) = µ(k + 1) | M(k) = M ′ ]
Let this conditional probability be ρ. For each (v, w) ∈ Ovinit , there may be several matchings in M in
which v is matched to w. By applying Lemma 5.15, the following equation holds for every (v, w) ∈ Ovinit ,
Pr[M>i (t) = µ(t), . . . , M>i (k + 1) = µ(k + 1) | MATE (v) = w] = ρ
Since this probability is the same for each (v, w) ∈ Ovinit , so using Definition 5.1, it follows that the
matchings at levels > i during any sequence of updates is independent of the mate that v picked during the
creation of its epoch. Now applying Lemma 5.16 we get the following lemma.
Lemma 5.17 Suppose a vertex v creates an epoch at level i while the algorithm processes kth update in the
graph. Consider any sequence of updates in the graph. The mate picked by v while creating the epoch is
independent of the sequence of matchings at levels > i computed by the algorithm while processing these
updates. That is, for any t > k, and any (v, w) ∈ Ovinit ,
Pr[MATE (v) = w|M>i (t) = µ(t), . . . , M>i (k + 1) = µ(k + 1)] = Pr[MATE (v) = w] =

1
|Ovinit |

Consider any given sequence of t updates in the graph. Subsequent to the time v creates an epoch at level
i during kth update, let µ = hµ(k + 1), . . . , µ(t)i be the sequence of matching at levels > i as computed by
the algorithm. Notice that the upward movement of v and each (v, w) ∈ Ovinit after the creation of epoch
is captured precisely by the corresponding update in the matching at level > i. Therefore, using µ we can
define the update sequence associated with the epoch as follows. Consider an edge (v, w) ∈ Ovinit and let ℓth
update in the graph be the deletion of (v, w). Let j < ℓ be the smallest integer such that w ∈ µj , that is, w
appears in the matching at level > i while processing of jth update in the graph, then the update associated
with (v, w) is the upward movement. If no such j exists, the update associated with (v, w) is its deletion.
Likewise, we define the update associated with v. The update sequence U for the epoch is the sequence of
these updates on v and the edges of Ovinit arranged in the chronological order.
For an update sequence U associated with an epoch, there may exist many sequences {µ1 , . . . , µq } such
that for each of them, the update sequence associated with the epoch is U . It follows from Lemma 5.17 that
the mate picked by v during its epoch is independent of each such sequence µr , 1 ≤ r ≤ q. Therefore, using
Lemma 5.15, the mate picked by v during its epoch is independent of U as well. Thus we have established
the validity of Lemma 5.8.

6 A tight example
We tested our algorithm on random graphs of various densities and found that the matching maintained is
very close to the maximum matching. This suggests that our algorithm might be able to maintain nearly
maximum matching for dynamic graphs appearing in various practical applications. However, it is not hard
to come up with an update sequence such that at the end of the sequence, the matching obtained by our
algorithm is strictly half the size of maximum matching. In other words, the approximation factor 2 for the
matching maintained by our algorithm is indeed tight. We present one such example as follows (see Figure
8).
Let G(V ∪ W, E) be a graph such that V = {v1 , v2 , . . . , vn } and W = {w1 , w2 , . . . , wn } for some
even number n. Consider the following update sequence. In the first phase, add edges between every pair
of vertices present in V . This results in a complete subgraph on vertices of V . The size of any maximal
27

Figure 8: An example where our algorithm gives a 2-approximation. The vertices on top are in V and form
a complete graph. The vertices at the bottom of the figure are in W .
matching on a complete graph of size n is n/2. After the first phase of updates ends, the size of matching
obtained by our algorithm is n/2. In the second phase, add edge (vi , wi ) for all i. Note that the degree of
each wi is one at the end of the updates. Let us now find the matching which our algorithm maintains. Let
(vi , vj ) be an edge in the matching after phase 1. Note that both these endpoints are at a level greater than
−1. A vertex in W is at level −1 as it does not have any adjacent edges after phase 1. When an edge (wi , vi )
is added, since vi is at a higher level than wi , vi becomes the owner of this edge. The second invariant of vi is
not violated after this edge insertion and nothing happens at this update step and wi still remains at level −1.
Using same reasoning, we can show that wj also remains at level −1 after the addition of edge (vj , wj ). So
matching maintained by the algorithm remains unchanged. It is easy to observe that the maximum matching
of the graph G now has size n which is twice the size of the matching maintained by our algorithm.

7 Postscript
We presented a fully dynamic randomized algorithm for maximal matching which achieves expected amortized O(log n) time per edge insertion or deletion. An interesting question is to explore how crucial randomization is for dynamic maximal matching.
Subsequent to an earlier version of this paper [5], Bhattacharya et al. [9] almost answered this question
in affirmative by designing a determinstic algorithm that maintains (2 + ǫ)-approximate matching in amortized O(poly(log n, 1/ǫ) update time. Another interesting question is to explore whether we can achieve
O(1) amortized update time. Very recently Solomon [29] answered this question in affirmative as well by
designing a randomized algorithm that takes O(t + n log n) update time with high probability to process
any sequence of t edge deletions. Though the basic building blocks of his algorithm are the same as ours,
the two algorithms are inherently different and so are their analysis.
In our algorithm, a vertex may rise to a higher level and create a new epoch even when its matched edge
is intact. But the algorithm of Solomon [29] of takes a lazy approach to maintain the hierarchy of vertices
wherein a vertex is processed only when it becomes absolutely necessary. Another crucial difference is the
following. Our algorithm maintains a function φv (j) for each vertex v and each level j. This function is used
to ensure an invariant that each vertex v is at the highest possible level ℓ such that the edges incident from
lower levels is at least 4ℓ . An important property guaranteed by this invariant is that the mate of a vertex
while creating an epoch is independent of the update sequence associated with the epoch. The analysis
of our algorithm crucially exploits this property. However, the explicit maintenance of φv (j) imposes an
overhead of Θ(log n) in the update time. In order to achieve O(1) update time, Solomon [29] gets rid of the

28

maintenance of φv (j) by taking a lazy approach and a couple of new ideas. As a result, unfortunately, the
property of our algorithm no longer holds for the algorithm of Solomon [29] - indeed there is dependence
between the update sequence associated with an epoch created by a vertex and the random mate picked by
it. Solomon [29] makes use of a new concept called uninterrupted duration of an epoch that bypasses the
need of our property for the analysis. His analysis can be adapted to our algorithm as well and can be viewed
as the correct counterpart of Lemma 4.10 in [5]. However, our new analysis has its own merits since it is
based on an insightful property of our algorithm which we believe is of its own independent interest and
importance.
Subsequent to the publication of the [5] there has been interesting progress in the area of dynamic
matching with approximation less than 2 [7, 8, 24, 14], and dynamic weighted matching [3, 4, 14].
One of the technical challenges in theoretical computer science is to prove lower bounds for algorithmic
problems. Recently there has been some progress on proving conditional lower bounds for dynamic graph
algorithms [1, 15]. In the light of the lower bound presented by Abboud and Williams [1] based on Ω(n2 )
hardness of the 3SUM problem, it would be an interesting and challenging problem to see if c-approximate
maximum matching for c < 2 can be maintained in o(n) update time.

8 Acknowledgment
The possibility of an error in Lemma 4.10 of [5] was pointed out by Sayan Bhattacharya and Divyarthi
Mohan. The second author would like to thank both of them for discussions on the proof of the expectation
bound and the definition of B(v, i, k). In [5], we used 2i as the threshold for raising a vertex to level i.
The possibility of increasing this threshold from 2i to bi for any constant b without any impact on the time
complexity was observed by Shay Solomon [29]. We are thankful to him for this observation.

References
[1] Amir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong lower bounds for
dynamic problems. CoRR, abs/1402.0054, 2014.
[2] David J. Abraham, Robert W. Irving, Telikepalli Kavitha, and Kurt Mehlhorn. Popular matchings.
SIAM J. Comput., 37(4):1030–1045, 2007.
[3] Abhash Anand, Surender Baswana, Manoj Gupta, and Sandeep Sen. Maintaining approximate maximum weighted matching in fully dynamic graphs. In FSTTCS, pages 257–266, 2012.
[4] Abhash Anand, Surender Baswana, Manoj Gupta, and Sandeep Sen. Maintaining approximate maximum weighted matching in fully dynamic graphs. CoRR, abs/1207.3976, 2012.
[5] Surender Baswana, Manoj Gupta, and Sandeep Sen. Fully dynamic maximal matching in O(log n)
update time. SIAM J. Comput. (preliminary version appeared in FOCS 2011), 44(1):88–113, 2015.
[6] Surender Baswana, Sumeet Khurana, and Soumojit Sarkar. Fully dynamic randomized algorithms for
graph spanners. ACM Transactions on Algorithms, 8(4):35, 2012.
[7] Aaron Bernstein and Cliff Stein. Fully dynamic matching in bipartite graphs. In Automata, Languages,
and Programming - 42nd International Colloquium, ICALP 2015, Kyoto, Japan, July 6-10, 2015,
Proceedings, Part I, pages 167–179, 2015.

29

[8] Aaron Bernstein and Cliff Stein. Faster fully dynamic matchings with small approximation ratios.
In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2016, Arlington, VA, USA, January 10-12, 2016, pages 692–711, 2016.
[9] Sayan Bhattacharya, Monika Henzinger, and Danupon Nanongkai. New deterministic approximation
algorithms for fully dynamic matching. In Proceedings of the 48th Annual ACM SIGACT Symposium
on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 398–411, 2016.
[10] J. Edmonds. Paths, trees, and flowers. Canadian Journal of Mathematics, 17:449467, 1965.
[11] J. Edmonds and E. L. Johnson. Matching, euler tours, and the chinese postman. Mathematical Programming, 5:88–124, 1973.
[12] Harold N. Gabow and Robert Endre Tarjan. Faster scaling algorithms for general graph-matching
problems. J. ACM, 38(4):815–853, 1991.
[13] D. Gale and L. S. Shapley. College admissions and the stability of marriage. American Mathematical
Monthly, 69:9–14, 1962.
[14] Manoj Gupta and Richard Peng. Fully dynamic $(1+)$-approximate matchings. In 54th Annual IEEE
Symposium on Foundations of Computer Science, 2013.
[15] Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak. Unifying
and strengthening hardness for dynamic problems via the online matrix-vector multiplication conjecture. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC
2015, Portland, OR, USA, June 14-17, 2015, pages 21–30, 2015.
[16] Jacob Holm, Kristian de Lichtenberg, and Mikkel Thorup. Poly-logarithmic deterministic fullydynamic algorithms for connectivity, minimum spanning tree, 2-edge, and biconnectivity. J. ACM,
48(4):723–760, 2001.
[17] John E. Hopcroft and Richard M. Karp. An n5/2 algorithm for maximum matchings in bipartite graphs.
SIAM J. Comput., 2(4):225–231, 1973.
[18] Chien-Chung Huang and Telikepalli Kavitha. Efficient algorithms for maximum weight matchings in
general graphs with small edge weights. In SODA, pages 1400–1412, 2012.
[19] Jon Kleinberg and Eva Tardos. Algorithm Design. Addison Wesley, 2005.
[20] E. Lawler. Combinatorial Optimization: Networks and Matroids. Holt, Rinehart & Winston, Newyork,
1976.
[21] L. Lovasz and M.D. Plummer. Matching Theory. AMS Chelsea Publishing, North-Holland, AmsterdamNew York, 1986.
p
[22] Silvio Micali and Vijay V. Vazirani. An O( (|V |)|E|) algorithm for finding maximum matching in
general graphs. In FOCS, pages 17–27, 1980.
[23] Marcin Mucha and Piotr Sankowski. Maximum matchings via gaussian elimination. In FOCS, pages
248–255, 2004.
[24] Ofer Neiman and Shay Solomon. Deterministic algorithms for fully dynamic maximal matching.
CoRR, abs/1207.1277, 2012.
30

[25] Krzysztof Onak and Ronitt Rubinfeld. Maintaining a large matching and a small vertex cover. In
STOC, pages 457–464, 2010.
[26] Rasmus Pagh and Flemming Friche Rodler. Cuckoo hashing. J. Algorithms, 51(2):122–144, 2004.
[27] Liam Roditty and Uri Zwick. Improved dynamic reachability algorithms for directed graphs. SIAM J.
Comput., 37(5):1455–1471, 2008.
[28] Liam Roditty and Uri Zwick. Dynamic approximate all-pairs shortest paths in undirected graphs. SIAM
J. Comput., 41(3):670–683, 2012.
[29] Shay Solomon. Fully dynamic maximal matching in constant update time. CoRR, abs/1604.08491,
2016.
[30] Mikkel Thorup. Fully-dynamic min-cut. Combinatorica, 27(1):91–127, 2007.

9 Appendix
Proof of Lemma 5.15
Proof:
Pr[A ∩ C] = Pr[A ∩ (∪i Bi )]
X
Pr[A ∩ Bi ]
=

since Bi ’s are mutually exclusive

i

=

X
i

= ρ·

Pr[A | Bi ] · Pr[Bi ]

X

using the definition of conditional probability

Pr[Bi ]

i

= ρ · Pr[∪i Bi ] = ρ · Pr[C]

since Bi ’s are mutually exclusive

Hence Pr[A | C] = Pr[A ∩ C]/Pr[C] = ρ.



Proof of Lemma 5.16
Proof: Since A is independent of X, so for each x ∈ X,
Pr[A ∩ X = x] = Pr[A] · Pr[X = x]

(4)

Hence
Pr[A ∩ X = x]
Pr[A]
Pr[A] · Pr[X = x]
=
Pr[A]
= Pr[X = x]

Pr[X = x|A] =

using Equation 4



31

Proof of Lemma 3.1
The asymmetric random walk problem can be seen as a special case of the famous Gambler’s ruin
problem described as follows.
Gambler’s ruin problem.
There are two players who play a game that goes in rounds. Initially Player 1 has a capital of c units
and Player 2 has a capital of c′ units. Player 1 wins a round with probability p and loses with probability
q = 1 − p independent of the previous rounds. The winner of a round takes away one unit of the capital
from the opponent. The game ends when the capital of one of the players becomes 0.
The following lemma is well-known in many text books on probability theory. A concise and self
contained proof is available at the link : examplehttp://faculty.washington.edu/fm1/394/Materials/Gambler.pdf.
Lemma 9.1 In the Gambler’s ruin problem with p > q, the probability that Player 1 gets ruined is
′

1 − (p/q)c
1 − (p/q)c+c′
Let us now put an additional restriction in the problem: the total number of rounds allowed in the game
is L for a given number L < c′ . Notice that with this restriction Player 2 will never be ruined. As a result
the game will be over when Player 1 gets ruined or when L rounds are over. The probability of Player 1
getting ruined in this restricted Gambler’s problem is strictly less than the probability of Player 1 getting
ruined in the original Gambler’s problem described above. This is because there is a non-zero probability
that Player 1 may be ruined after performing more than L steps, and the restricted Gambler’s problem rules
out this possibility. Hence, using Lemma 9.1, it follows that for the restricted version of the Gambler’s ruin
problem with p > q, the probability that Player 1 gets ruined is less than
′

1 − (p/q)c
<
1 − (p/q)c+c′

 c
q
p

The restricted version of the Gambler’s ruin problem can be formulated as an asymmetric random walk
problem: The walk starts at location c units to the right of the origin. In each step, the particle moves one
unit to the right with probability p or one unit to the left with probability q = 1 − p indepependent of the
past moves. The walk terminates upon reaching either the origin or when it has performed L step. This
completes the proof of Lemma 3.1.

32

MT-Diet : Automated Smartphone based Diet Assessment with Infrared Images
Junghyo Lee (Speaker), Ayan Banerjee, and Sandeep K. S. Gupta iMPACT Lab, CIDSE, Arizona Sate University, Tempe, AZ {jlee375, abanerj3, Sandeep.Gupta}@asu.edu

Diet
 American Medical Association in 2013
 Recognized obesity as a disease officially [1]

 What is the effective method to treat and prevent the obesity?  Existing Diet Monitoring and Problems
 Self-monitoring
 Low adherence, underreporting, and recall error

 Smartphone camera based monitoring (State-of-the-art )
 User inconvenience
 Segment multiple foods to each single food  Select a food location in the image (Rectangle)

 Fairly low food type identification accuracy (63%)
2

MT-Diet
 MT-Diet: Automated Smartphone based Diet Assessment System using Cyber-Physical Dynamics  Thermal Interaction of a food plate with the environment
 - 
 - 
 

= exp


-
 


,

 - 
 - 


= exp

-
 


If    , then    , for at least  >  
 - Temperature of food item at time ,  - initial food temperature,  - mass of food item.   - Temperature of food item at time ,  - initial food temperature,  - mass of plate.    - Ambient temperature,  - heat transfer coefficient, - area of food item,  - specific heat of food,  - specific heat of plate.


 Contributions: Fuse Thermal and Visual Spectra  Thermal and Color image correct each other  Results  Segment multiple foods to each single food automatically  High accuracy of identifying types of food (88.93%)
 State-of-art systems have accuracy at most 63%
3

Overview

4

System Model

5

Usability
 Balanced diet recommendation
 Assume the plate has uniform depth  Ratio of the surface area multiplied by the density of the food item

6

Diet Monitoring
 Food Segmentation Food Recognition  Food Identification  Intake Amount  Calorie Estimation  Dietary Feedback  Behavior Control

7

MT-Diet

8

Image Acquisition
 Prototype of MT-Diet
 Nexus 5  Seek thermal sensor  Micro USB wire

 Thermal Image
 Seek thermal sensor interfaced with the Nexus 5

 Color Image
 From Built-in Nexus 5 Camera

9

Food Segmentation

10

Challenges for Image Based Food Segmentation (1)
 Visual spectra may not have defined boundaries
 Thermal boundaries more significant
Better Contrast in Infrared Spectra DTT Output Food Item Location

4

8  connected component labeling [2]

 Thermal signatures are not stable
1
Dynamic Thermal Thresholding (DTT) 3
11

2

Challenges for Image Based Food Segmentation (2)
 Color (Gradient) and Texture are important features for identification  Food items of same color as plate may be lost in visual spectra  Fusion of visual and infrared spectra
Contour Detection Image Segmentation

Hierarchical Image Segmentation (HIS) [3]
12

Challenges for Image Based Food Segmentation (3)
 Food items not heated enough get deleted in DTT output
 Color image can recognize the food
Food item missing in DTT Exact food portions and locations along with labels for connected components Food item missing in HIS
13

Which portions are food?

Label Matching
 Identifies unique food items in both infrared and visual spectra
1 2

+
3 4 DTT output with food labels Contour and Image segments from visual spectra

 Remove plate portions that are connected to food items with Grabcut [4]
14

Food Identification

15

Feature Set
 Objective
 Classify the food type using SVM with kernels

 Feature Extraction
 Color feature
 RGB histogram: 32 histogram bins of each color channel

 Histogram of Oriented Gradient feature [4]
 16 windows and 36 bins of oriented gradients of the each windows

 Texture feature [5]
 Resized food image (400 X 400), 5 scales, 8 orientations, 4 by 4 down sampling

16

Feature Fusion Methods
 Feature Fusion
 Three fusion methods  Method of Dimensionality Reduction
 Kernel Principal Component Analysis  Principal Component Analysis

17

Experimental Results

18

Segmentation Result

20 Food Types in practice

19

Execution Time of Food Segmentation
 Execution time (s) of food segmentation using i7 processor
Min Median Max Sum Avg STD

HIS
DTT

89.08
2.83

92.89
3.13

97.50
3.9

7403.9
256.12

92.55
3.2

1.66
0.24

ROF
Grabcut Total

0.39
2.4 95.51

0.78
14.24 111.59

1.01
41.15 145

62.77
1138.97 8911.79

0.78
13.48 111.46

0.09
7.63 7.71
20

Food Identification Result
Feature Best Methods
(Reduction, Kernel, Fusion)

Accuracy(%) Execution(s)

Texture HOG RGB HOG & Texture HOG & RGB RGB & Texture All

KPCA, RBF PCA, RBF KPCA, Sigmoid KPCA, RBF, 3 KPCA, RBF, 3 KPCA, RBF, 3 KPCA, RBF, 3

45.08 63.11 88.11 59.43 88.93 88.93 87.7

5.13 0.35 0.57 5.13 0.7 5.62 5.43

Feature Size 100 100 100 200 200 200 300
21

Conclusion
 MT-Diet : Automatic diet monitoring system that interfaces thermal  High accuracy of automated food identification (88.93%)  User-friendly diet monitoring system application
 Only one task (Click a button)  Expected the promoted healthy eating habits

Even If your diet log is the empty.....

22

References
[1] [Online]. Available: http://www.ama-assn.org/ama/pub/news/news/2013/2013-06-18-new-amapolicies-annual-meeting.page [2] H. Samet and M. Tamminen (1988). "Efficient Component Labeling of Images of Arbitrary Dimension Represented by Linear Bintrees". IEEE Transactions on Pattern Analysis and Machine Intelligence (TIEEE Trans. Pattern Anal. Mach. Intell.) 10 (4): 579. doi:10.1109/34.3918. [3] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, "Contour detection and hierarchical image segmentation," Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 5, pp. 898916, 2011. [4] Rother, C., Kolmogorov, V., & Blake, A. (2004, August). Grabcut: Interactive foreground extraction using iterated graph cuts. In ACM transactions on graphics (TOG) (Vol. 23, No. 3, pp. 309-314). ACM. [5] N. Dalal and B. Triggs, "Histograms of oriented gradients for human detection," in Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 1. IEEE, 2005, pp. 886893. [6] M. Haghighat, S. Zonouz, and M. Abdel-Mottaleb, "Identification using encrypted biometrics," in Computer Analysis of Images and Patterns. Springer, 2013, pp. 440448.
23

Acknowledgment
Dr. Petteri Nurmi for shepherding out paper Dr. Meg Bruening for her insights on diet monitoring NSF IIS 1116385 NIBIB EB019202 GPSA for supporting the conference travel grant

24

25

arXiv:1103.1109v4 [cs.DS] 2 Aug 2016

Fully dynamic maximal matching in O(log n) update time
Surender Baswana
Department of CSE,
I.I.T. Kanpur, India

Manoj Gupta
Department of CSE,
I.I.T. Delhi, India

sbaswana@cse.iitk.ac.in

gmanoj@cse.iitd.ernet.in

Sandeep Sen
Department of CSE,
I.I.T. Delhi, India
ssen@cse.iitd.ernet.in

August 3, 2016

Abstract
We present an algorithm for maintaining a maximal matching in a graph under addition and deletion
of edges. Our algorithm is randomized and it takes expected amortized O(log n) time for each edge
update where n is the number of vertices in the graph. Moreover, for any sequence of t edge updates,
the total time taken by the algorithm is O(t log n + n log2 n) with high probability.

Note: The previous version of this result appeared in SIAM J. Comp., 44(1): 88-113, 2015. However, the
analysis presented there for the algorithm was erroneous. This version rectifies this deficiency without any
changes in the algorithm while preserving the performance bounds of the original algorithm.

1

1

Introduction

Let G = (V, E) be an undirected graph on n = |V | vertices and m = |E| edges. A matching in G is a
set of edges M ⊆ E such that no two edges in M share any vertex. The study of matchings satisfying
various properties has remained the focus of graph theory for decades [21]. It is due to the elegant structure
of matching that it also appears in various combinatorial optimization problems [11, 20]. A few well studied
matching problems are maximum cardinality matching [10, 17, 22, 23], maximum weight matching [12,
18], minimum cost matching (chapter 7, [19]), stable matching [13], popular matching [2]. Among these
problems, the maximum cardinality matching problem has been studied most extensively. A matching M
is of maximum cardinality if the number of edges in M is maximum. A maximum cardinality matching
(MCM) is also referred to as a maximum matching. A matching is said to be a maximal matching if it
cannot be strictly contained in any other matching. It is well known that a maximal matching guarantees
a 2-approximation of the maximum matching. Though it is quite easy to compute a maximal matching in
O(m + n) time, designing an efficient algorithm for maximum matching has remained a very challenging
√
problem for researchers [10, 22]. The fastest algorithm, till date, for maximum matching runs in O(m n)
time and is due to Micali and Vazirani [22]. In this paper, we address the problem of maintaining a maximal
matching in a dynamic graph.
Most of the graph applications in real life deal with graphs that are not static, viz., the graph changes
over time caused by deletion and insertion of edges. This has motivated researchers to design efficient
algorithms for various graph problems in dynamic environment. An algorithmic graph problem is modeled
in the dynamic environment as follows. There is an online sequence of insertion and deletion of edges and
the goal is to update the solution of the graph problem after each edge update. A trivial way to achieve this
is to run the best static algorithm for this problem after each edge update; clearly this approach is wasteful.
The aim of a dynamic graph algorithm is to maintain some clever data structure for the underlying problem
such that the time taken to update the solution is much smaller than that of the best static algorithm. There
exist many efficient dynamic algorithms for various fundamental problems in graphs [6, 16, 27, 28, 30].
Baswana, Gupta and Sen [5] had presented a fully dynamic algorithm for maximal matching which
achieved O(log n) expected amortized time per edge insertion or deletion. Moreover, for any sequence of
t edge updates, the total time taken by the algorithm is O(t log n + n log2 n) with high probability. Their
algorithm improved an earlier result of Onak and Rubinfeld [25] who presented a randomized algorithm
for maintaining a c-approximate (for some unspecified large constant c) matching in a dynamic graph with
O(log2 n) expected amortized time for each edge update.
This algorithm also implied a similar result for maintaining a two approximate vertex cover. It is also
used as a basis for maintaining approximate maximum weight matching in a dynamic graph [3].
Unfortunately, the analysis given in [5] has a crucial flaw, viz., the statement of Lemma 4.10 [5] is
not true in general. Although the bound on the update time was critically dependent on Lemma 4.10, we
have presented an alternate proof of the claimed update time based on an interesting property of the original
algorithm that was not reported earlier. For completeness, we have presented the full details of the algorithm
of [5] and the new corrected analysis in this paper. This property is interesting in its own right and may have
other useful applications of the algorithm.

2 An overview
Let M denote a matching of the given graph at any moment. Every edge of M is called a matched edge and
an edge in E\M is called an unmatched edge. For an edge (u, v) ∈ M , we define u to be the mate of v and
vice versa. For a vertex x, if there is an edge incident to it in the matching M , then x is a matched vertex;
otherwise it is free or unmatched.

2

In order to maintain a maximal matching, it suffices to ensure that there is no edge (u, v) in the graph
such that both u and v are free with respect to the matching M . From this observation, an obvious approach
will be to maintain the information for each vertex whether it is matched or free at any stage. When an edge
(u, v) is inserted, add (u, v) to the matching if u and v are free. For a case when an unmatched edge (u, v) is
deleted, no action is required. Otherwise, for both u and v we search their neighborhoods for any free vertex
and update the matching accordingly. It follows that each update takes O(1) computation time except when
it involves deletion of a matched edge; in this case the computation time is of the order of the sum of the
degrees of the two vertices. So this trivial algorithm is quite efficient for small degree vertices, but could be
expensive for large degree vertices. An alternate approach to handling deletion of a matched edge is to use
a simple randomized technique - a vertex u is matched with a randomly chosen neighbor v. Following the
standard adversarial model, it can be observed that an expected deg(u)/2 edges incident to u will be deleted
before
 deleting the
 matched edge (u, v). So the expected amortized cost per edge deletion for u is roughly
deg(u)+deg(v)
O
. If deg(v) < deg(u), this cost is O(1); but if deg(v) ≫ deg(u), then it can be as bad
deg(u)/2
as the trivial algorithm. We combine the idea of choosing a random mate and the trivial algorithm suitably
√
as follows. We first present a fully dynamic algorithm which achieves expected amortized O( n) time per
update. We introduce a notion of ownership of edges in which we assign an edge to that endpoint which has
higher degree. We maintain a partition of the set of vertices into two levels : 0 and 1. Level 0 consists of
vertices which own fewer edges than an appropriate threshold and we handle the updates in level 0 using the
trivial algorithm. Level 1 consists of vertices (and their mates) which own larger number of edges and we
use the idea of random mate to handle their updates. In particular, a vertex chooses a random mate from its
set of owned edges which ensures that it selects a neighbor having a lower degree.
√
A careful analysis of the O( n) update time algorithm suggests that a finer partition of vertices may
help in achieving a better update time. This leads to our final algorithm which achieves expected amortized
O(log n) time per update. More specifically, our algorithm maintains an invariant that can be informally
summarized as follows.
Each vertex tries to rise to a level higher than its current level, if, upon reaching that level, there are
sufficiently large number of edges incident on it from lower levels. Once a vertex reaches a new level, it
selects a random edge from this set and makes it matched.
Note that we say that ”a vertex rises” to indicate that the vertex moves to a higher level and ”a vertex
falls” to indicate that the vertex moves to a lower level. A vertex may also fall to a lower level if the number
of edges incident to it decreases. But a vertex only uses its neighborhood information to decide whether to
move to higher or lower level. Overall, the vertices use their local information to reach a global equilibrium
state in which after each update each vertex is at the right level having no incentive to either move above or
below its current level.

2.1 Organization of the paper
For a gentle exposition of the ideas and techniques, we first describe a simple but less efficient fully dynamic
algorithm for maximal matching in Section 4. We present our final fully dynamic algorithm which achieves
expected amortized O(log n) time per update in Section 5. In Section 6, we illustrate an example of a
dynamic graph that establishes the tightness of the approximation factor guaranteed by our algorithm. In
the following section, we describe notations and elementary results from the probability theory that we shall
use.

3

3 Preliminaries
We shall use M to denote the matching maintained by our algorithm at any stage. Our algorithms maintain a
partition of the set of vertices among various levels. We shall use LEVEL (u) to denote the level of a vertex u.
We define LEVEL (u, v) for an edge (u, v) as max(LEVEL (u), LEVEL (v)). Our algorithms will ensure that
both the endpoints of each matched edge in M are present at the same level. So the matching M maintained
by our algorithms will be a set of tuples as follows.
M = {(u, v, ℓ) | u is matched with v at level ℓ}
The analysis of our algorithms will use a basic result about asymmetric random walk as follows.
Asymmetric random walk on a line
Consider a particle performing a discrete random walk on a line. In each step, it moves one unit to the right
with probability p or to the left with probability q = 1 − p. Each move is independent of the moves made in
the past. The following lemma holds if p > q.
Lemma 3.1 Suppose the random walk starts at location ℓ units to the right of the origin. Then the proba ℓ
bility that it ever reaches origin within L steps for any given L is less than pq .

The proof of Lemma 3.1 is sketched in Appendix. During the analysis of algorithms, we shall use the
terminology very high probability for those events whose probability is 1 − nc for some positive constant c.

√
4 Fully dynamic algorithm with expected amortized O( n) time per update
The algorithm maintains a partition of the set of vertices into two levels - 0 and 1. We now introduce the
concept of ownership of the edges. Each edge present in the graph will be owned by one or both of its
endpoints as follows. If both the endpoints of an edge are at level 0, then it is owned by both of them.
Otherwise it will be owned by exactly that endpoint which lies at higher level. If both the endpoints are
at level 1, the tie will be broken suitably by the algorithm. As the algorithm proceeds, the vertices will
make transition from one level to another and the ownership of edges will also change accordingly. Let Ou
denote the set of edges owned by u at any moment of time. Each vertex u ∈ V will keep the set Ou in
a dynamic hash table [26] so that each search or deletion operation on Ou can be performed in worst case
O(1) time and each insertion operation can be performed in expected O(1) time. This hash table is also
suitably augmented with a linked list storing Ou so that we can retrieve all edges of set Ou in O(|Ou |) time.
The algorithm maintains the following three invariants after each update.
1. Every vertex at level 1 is matched. Every free vertex at level 0 has all its neighbors matched.
√
2. Every vertex at level 0 owns less than n edges at any moment of time.
3. Both the endpoints of every matched edge are at the same level.
The first invariant implies that the matching M maintained is maximal at each stage. A vertex u is
said to be a dirty vertex at a moment if at least one of its invariants does not hold. In order to restore
the invariants, each dirty vertex might make transition to some new level and do some processing. This
processing involves owning or disowning some edges depending upon whether the level of the vertex has
risen or fallen. Thereafter, the vertex will execute RANDOM - SETTLE or NAIVE - SETTLE to settle down at
its new level. The pseudocode of our algorithms for handling insertion and deletion of an edge is given in
Figure 1 and Figure 2.
4

Handling insertion of an edge
Let (u, v) be the edge being inserted. If either u or v are at level 1, there is no violation of any invariant. So
the only processing that needs to be done is to assign (u, v) to Ou if LEVEL (u) = 1, and to Ov otherwise.
This takes O(1) time. However, if both u and v are at level 0, then we execute HANDLING - INSERTION
procedure which does the following (see Figure 1).
Procedure
1
2
3
4
5
6
7
8
9
10

H A N D L I N G - I N S E R T I O N (u, v)

Ou ← Ou ∪ {(u, v)};
Ov ← Ov ∪ {(u, v)};
if u and v are FREE then M ← M ∪ {(u, v)};
if |Ov | > |Ou | then swap(u, v);
√
if |Ou | = n then
foreach (u, w) ∈ Ou do
delete (u, w) from Ow ;

x ← RANDOM - SETTLE (u);
if x 6= NULL then NAIVE - SETTLE(x);
if w was previous mate of u then NAIVE - SETTLE(w);

Procedure R A N D O M - S E T T L E (u): Finds a random edge (u, y) from the edges owned by u and
returns the previous mate of y
1
2
3
4
5
6

7
8

Let (u, y) be a uniformly randomly selected edge from Ou ;
foreach (y, w) ∈ Oy do
delete (y, w) from Ow ;
if y is matched then
x ← MATE (y);
M ← M\{(x, y)}
else
x ← NULL ;

M ← M ∪ {(u, y)};
← 1; LEVEL (y) ← 1;
return x;

9 LEVEL (u)
10

Procedure
1
2
3
4

N A I V E - S E T T L E (u)

: Finds a free vertex adjacent to u deterministically

for each (u, x) ∈ Ou do
if x is free then
M ← M ∪ {(u, x)};
Break;
Figure 1: Procedure for handling insertion of an edge (u, v) where LEVEL (u) =

LEVEL (v)

= 0.

Both u and v become the owner of the edge (u, v). If u and v are free, then the insertion of (u, v) has
violated the first invariant for u as well as v. We restore it by adding (u, v) to M. Note that the insertion
of (u, v) also leads to increase of |Ou | and |Ov | by one. We process that vertex from {u, v} which owns
√
larger number of edges; let u be that vertex. If |Ou | = n, then Invariant 2 has got violated. We execute
RANDOM - SETTLE (u); as a result, u moves to level 1 and gets matched to some vertex, say y, selected
5

randomly uniformly from Ou . Vertex y moves to level 1 to satisfy Invariant 3. If w and x were respectively
the earlier mates of u and y at level 0, then the matching of u with y has rendered w and x free. So to restore
Invariant 1, we execute NAIVE - SETTLE(w) and NAIVE - SETTLE(x). This finishes the processing of insertion
of (u, v). Note that when u rises to level 1, |Ov | remains unchanged. Since all the invariants for v were
satisfied before the current edge update, it follows that the second invariant for v still remains valid.
Handling deletion of an edge
Let (u, v) be an edge that is deleted. If (u, v) ∈
/ M, all the invariants are still valid. So let us consider the
nontrivial case when (u, v) ∈ M. In this case, the deletion of (u, v) has made u and v free. Therefore,
potentially the first invariant might have got violated for u and v, making them dirty. We do the following
processing in this case.
If edge (u, v) was at level 0, then following the deletion of (u, v), vertex u executes NAIVE - SETTLE(u),
and then vertex v executes NAIVE - SETTLE(v). This restores the first invariant and the vertices u and v are
clean again. If edge (u, v) was at level 1, then u is processed using the procedure shown in Figure 2 which
does the following (v is processed similarly).
Procedure H A N D L I N G - D E L E T I O N (u,v)
1 foreach (u, w) ∈ Ou and LEVEL (w) = 1 do
2
move (u, w) from Ou to Ow ;
√
3 if |Ou | ≥
n then
4
x ← RANDOM - SETTLE (u);
5
if x 6= NULL then NAIVE - SETTLE(x);
else
6
LEVEL (u) ← 0;
7
foreach (u, w) ∈ Ou and LEVEL (w) = 0 do
8
add (u, w) to Ow ;
9
10
11
12
13

NAIVE - SETTLE(u);

foreach (u, w) ∈ Ou do
√
if |Ow | = n then
x ← RANDOM - SETTLE (w);
if x 6= NULL then NAIVE - SETTLE(x);

Figure 2: Procedure for processing u when (u, v) ∈ M is deleted and LEVEL (u)=LEVEL (v)=1.
First, u disowns all its edges whose other endpoint is at level 1. If |Ou | is still greater than or equal to
√
n, then u stays at level 1 and executes RANDOM - SETTLE(u). If |Ou | is less than n, u moves to level 0
and executes NAIVE - SETTLE(u). Note that the transition of u from level 1 to 0 leads to an increase in the
number of edges owned by each of its neighbors at level 0. The second invariant for each such neighbor, say
√
w, may get violated if |Ow | = n, making w dirty. So we scan each neighbor of u sequentially and for each
√
dirty neighbor w (that is, |Ow | = n), we execute RANDOM - SETTLE(w) to restore the second invariant.
This finishes the processing of deletion of (u, v).
It can be observed that, unlike insertion of an edge, the deletion of an edge may lead to creation of a
large number of dirty vertices. This may happen if the deleted edge is a matched edge at level 1 and at least
one of its endpoints move to level 0.

√

6

4.1 Analysis of the algorithm
While processing the sequence of insertions and deletions of edges, an edge may become matched or unmatched at different update steps. We analyze the algorithm using the concept of epochs, which we explain
as follows.
Definition 4.1 At any time t, let (u, v) be any edge in M. Then the epoch defined by (u, v) at time t is the
maximal continuous time period containing t during which it remains in M. An epoch is said to belong to
level 0 or 1 depending upon the level of the matched edge that defines the epoch.
The entire life span of an edge (u, v) consists of a sequence of epochs of (u, v) separated by the continuous periods when (u, v) is unmatched. It follows from the algorithm that any edge update that does
not change the matching is processed in O(1) time. An edge update that changes the matching results
in the start of new epoch(s) or the termination of some existing epoch(s). For the sake of analysis, we
will redistribute the computation performed at any update step t among the epochs that are created or terminated at step t. More specifically, let epoch of (u1 , v1 ), (u2 , v2 ), . . . , (ul , vl ) be created and epochs of
(w1 , x1 ), (w2 , x2 ), . . . , (wk , xk ) be terminated at step t. We will redistribute total computation performed at
step t in such a way that:
Pl
Total computation performed at step t =
computation associated with the start of epoch (ui , vi ) +
Pi=1
k
i=1 computation associated with the termination of epoch (wi , xi )
Now, we shall analyze the computation involved in each procedure of our algorithm and distribute it
suitably among various epochs.
1.

NAIVE - SETTLE (u)
Observe that whenever the procedure NAIVE - SETTLE(u) is carried out, u is present at level 0, and
√
hence |Ou | < n. The procedure NAIVE - SETTLE(u) searches for a free neighbor of u by scanning
√
Ou . Hence, the time complexity of NAIVE - SETTLE(u) is O(|Ou |) = O( n). Furthermore, this
procedure is called whenever u loses its mate, say v. So we can associate the computation cost of
NAIVE - SETTLE (u) with the termination of the previous epoch (u, v).

2.

RANDOM - SETTLE (u)

3.

√
Observe that whenever RANDOM - SETTLE(u) is invoked, u owns at least n edges incident from level
√
0, and hence |Ou | ≥ n. During RANDOM - SETTLE(u), u finds a random mate from level 0. This
is done by selecting a random number r ∈ [1, |Ou |], and then picking the r th edge, say (u, y), from
the linked list storing Ou . This takes O(|Ou |) time. Vertex u then pulls y to level 1 to satisfy the
third invariant. In this process, y becomes the sole owner of all those edges whose other endpoint is
√
at level 0 (line 2,3). Since y was the owner of at most n edges, the total computation time involved
√
in performing this step is O( n). Other steps in RANDOM - SETTLE can be executed in O(1) time.
√
√
Hence the total computation time is O(|Ou | + n) which is O(|Ou |) since |Ou | ≥ n. We associate
this computation time with the start of the epoch (u, y) that gets created at level 1.
HANDLING - INSERTION (u, v)

√
This procedure takes O(1) time unless one of the endpoints of (u, v) starts owning n edges. In
that case, the procedure invokes RANDOM - SETTLE (line 8) and NAIVE - SETTLE (line 9,10). We have
already distributed the time taken in these procedures to the respective epochs that get created. Excluding these tasks, the only computation performed in this procedure is in the FOR loop. The purpose
√
of this loop is to make u the sole owner of all its edges incident from level 0. Since u owns n edges
√
from level 0, the total computation time involved in performing this step is O( n). We associate this
computation time with the start of the epoch created by u at level 1.
7

4.

HANDLING - DELETION (u, v)
Procedure HANDLING - DELETION(u, v) is carried out when the matched edge (u, v) at level 1 gets
deleted. In addition to invoking RANDOM - SETTLE and NAIVE - SETTLE procedures whose computation cost is already assigned to respective epochs, this procedure scans the list Ou at most twice.
Notice that |Ou | can be Θ(n). We associate this computation time of O(n) with the termination of
the epoch (u, v).

Excluding the updates that cause the start and termination of an epoch of (u, v), every other edge update
on u and v during the epoch is handled in just O(1) time. Therefore, we shall focus only on the amount
of computation associated with the start and termination of an epoch. Let us now analyze the computation
time associated with the epoch at level 0 and level 1.
• Epoch at level 0
As discussed above, it is only the procedure NAIVE - SETTLE whose computation time is associated
√
with an epoch at level 0. This procedure takes O( n) time. Hence the computation time associated
√
with an epoch at level 0 is O( n).
• Epoch at level 1
Consider an epoch at level 1. There are two ways in which this epoch gets created at level 1:
– In HANDLING - INSERTION
An epoch of (u, v) can be created during the procedure HANDLING - INSERTION(u, v). In this
case, the computation time associated with the start of the epoch of (u, v) is the computation
time incurred in executing the procedure HANDLING - INSERTION and the procedure RANDOM SETTLE which it invokes. It follows from the above discussion that the computation cost associ√
√
√
ated with the epoch (u, v) is O( n + |Ou |) which is O( n) since |Ou | = n when we invoke
HANDLING - INSERTION (u, v).
– In HANDLING - DELETION
Procedure HANDLING - DELETION(u, v) invokes RANDOM - SETTLE at lines 4 and 12 to create
new epochs at level 1. The execution of RANDOM - SETTLE at line 4 creates a new epoch for u
and its computation time O(|Ou |), which can be Θ(n), gets associated with the start of the new
epoch created by u. The execution of RANDOM - SETTLE at line 12 creates a new epoch for some
√
vertex w which is some neighbor of u. Note that |Ow | = n. Its computation time, which is
√
O( n), is associated with the start of the epoch at level 1 created by w.
Now let us calculate the computation cost associated with an epoch, say of an edge (u, v), at level 1
when it terminates. It follows from the discussion above that the only computation time associated
with the termination of epoch (u, v) is the computation time of HANDLING - DELETION (excluding the
time spent in procedures RANDOM - SETTLE and NAIVE - SETTLE that are already associated with the
start of their respective epochs). This cost is at most O(n).
From our analysis given above, it follows that the amount of computation time associated with an epoch
√
at level 0 is O( n) and the computation time associated with an epoch at level 1 is O(n).
An epoch of (u, v) may either terminate (if (u, v) is removed from the matching) or remain alive, i.e.,
(u, v) remain in the matching after the end of all the updates. An epoch of (u, v), ends because of exactly
one of the following causes.
(i) if (u, v) is deleted from the graph.
8

epoch of (u, v)
epoch of (u, v)

LEVEL

1
natural epoch

:

epoch of (u, w)
:

induced epoch

epoch of (v, x)

Time

LEVEL

0

epoch of (u, w)
epoch of (v, x)

Figure 3: Epochs at level 0 and 1; the creation of an epoch at level 1 can terminate at most two epochs at
level 0.
(ii) u (or v) get matched to some other vertex leaving its current mate free.
An epoch will be called a natural epoch if it terminates due to cause (i); otherwise it will be called an
induced epoch. Induced epoch can terminate prematurely since, unlike natural epoch, the matched edge is
not actually deleted from the graph when an induced epoch terminates.
It follows from the algorithm described above that every epoch at level 1 is a natural epoch whereas
an epoch at level 0 can be natural or induced depending on the cause of its termination. Furthermore,
each induced epoch at level 0 can be associated with a natural epoch at level 1 whose creation led to the
termination of the former. In fact, there can be at most two induced epochs at level 0 which can be associated
with an epoch at level 1. It can be explained as follows (see Figure 3).
Consider an epoch at level 1 associated with an edge, say (u, v). Suppose it was created by vertex u. If
u was already matched at level 0, let w 6= v be its mate. Similarly, if v was also matched already, let x 6= u
be its current mate at level 0. So matching u to v terminates the epoch of (u, w) as well as the epoch of edge
(v, x) at level 0. We charge the overall cost of these two epochs to the epoch of (u, v). We have seen that
√
the computational cost associated with an epoch at level 0 is O( n). So the overall computation charged to
√
an epoch of (u, v) at level 1 is O(n + 2 n) which is O(n).
Lemma 4.1 The computation charged to a natural epoch at level 1 is O(n) and the computation charged
√
to a natural epoch at level 0 is O( n).
In order to analyze our algorithm, we just need to get a bound on the computation charged to all natural
epochs that get terminated during a sequence of updates or are alive at the end of the all the updates. Let
us first analyze the computation cost charged to all those epochs which are alive at the end of t updates.
Consider an epoch of edge (u, v) that is alive at the end of t updates. If this epoch is at level 0, the computation cost associated with the start of this epoch is O(1). If this epoch is at level 1, then the computation
√
time associated with the start of this epoch is O(|Ou |) and notice that |Ou | ≥ n. Note that there can be at
√
most two induced epochs at level 0 whose computation time, which is O( n), is also charged to the epoch
of (u, v). Hence the computation charged to the live epoch of (u, v) is O(|Ou |). Observe that, at any given
moment of time, Ou ∩ Ow = ∅ for any two vertices u, w presentP
at level 1. Hence the computation time
charged to all live epochs at the end of t updates is of the order of u |Ou | ≤ 2t = O(t). So all we need is
to analyse the computation charged to all natural epochs that get terminated during the sequence of updates.
Let t be the total number of updates. Each natural epoch at level 0 which gets terminated can be assigned
uniquely to the deletion of its matched edge. Hence it follows from Lemma 4.1 that the computation charged
√
to all natural epochs terminated at level 0 during t updates is O(t n). We shall now analyze the number of
epochs terminated at level 1. Our analysis will crucially exploit the following lemma.

9

Lemma 4.2 Suppose vertex v creates an epoch at level 1 during an update in the graph. and let Ovinit be
the set of edges that v owned at the time of the creation of this epoch. Then, for any arbitrary sequence D
of edge deletions of Ovinit , and for any (v, w) ∈ Ovinit
Pr[MATE (v) = w | D] =

1
|Ovinit |

We first carry out the analysis for the high probability bound on the total update time taken by our algorithm.
Thereafter we carry out the analysis for the expected value of the total update time.

4.2 High probability bound on the total update time
The key idea of randomization is that once a vretex v creates an epoch at level 1, there should be many edge
deletions from Ovinit before the matched edge of v is deleted. In order to quanitfy this key idea, we introduce
the following definition that categorizes an epoch as good or bad.
Definition 4.2 An epoch is said to be bad if it gets terminated naturally with in the deletion of the first 1/3
edges that it owned at the time of its creation. An epoch is said to be good if it is not bad.
It follows from Definition 4.2 that a good epoch undergoes many edge deletions before getting terminated.
So only the bad epochs are problematic. Now using Lemma 4.2, we establish an upper bound on the
probability of an epoch to be bad.
Lemma 4.3 Suppose vertex v creates an epoch at level 1 during the kth update for some k ≤ t. Then this
epoch is going to be bad with probability 1/3 irrespective of the future updates in the graph and the random
bits picked during their processing.
Proof: Consider any sequence of updates in the graph following the creation of this epoch. This sequence
defines the sequence D of edge deletions of Ovinit . The termination of this epoch is fully determined by the
mate that v picked and this sequence D. This epoch will be bad if the mate of v is among the endpoints of
the first 1/3 edges in this sequence. Then, Lemma 4.2 implies that the mate of v is equally likely to be the
endpoint of any edge in this sequence. So the probability of the epoch to be bad is 1/3 1 .

For the time complexity analysis, we will show that the number of bad epochs may exceed the number
of good epochs by at most O(log n) with very high probability. Notice that the number of epochs created
at level 1 is itself a random variable whose value may depend upon the updates in the graph as well as the
random bits picked during their processing. However, as shown by Lemma 4.3, each newly created epoch
at level 1 will be bad with probability 1/3 irrespective of the past epochs. The number of epochs created at
level 1 during any t updates is trivially O(nt). Therefore, the sequence of epochs at level 1 can be seen as an
instance of the asymmetric random walk as follows. The walk starts at location 2 log2 n to the right of the
origin. Each step of the walk is one unit to the right of the current location with probability 2/3 or one unit
to the left with probability 1/3 independent of the past moves. We need to find the probability that the walk
ever reaches the origin during any time in the algorithm. It follows from Lemma 3.1 that the probability of
this event is less than 1/n2 . So the following lemma holds immediately.
Lemma 4.4 During any sequence of t updates, the number of bad epoch at level 1 can exceed the number
of good epochs by 2 log2 n with probability at most 1/n2 .
1

The analysis assumed that each edge from Ovinit is going to be deleted sometime in future. If not, place all such edges
arbitrarily at the end of the sequence D. In this case, the probability of the epoch to be bad will be even less than 1/3.

10

As stated in Lemma 4.1, each epoch at level 1 has a computation cost of O(n) charged to it. Let t be the
total number of updates in the graph. For each epoch at level 1, the number of owned edges at the time of
√
√
its creation is at least n. As a result the number of good epochs during t updates is bounded by 4t/ n
√
deterministically. So the computation cost of good epochs at level 1 is bounded by O(t n). Lemma 4.4
implies that the computation cost of all bad epochs at level 1 can exceed the computation cost of all good
epochs at most by cn log n amount for some constant c with probability ≥ 1 − 1/n2 . So overall the cost of
√
all epochs at level 1 is bounded by O((t n + n log n) with high probability. The computation cost of all
√
epochs at level 0 is bounded deterministically by O(t n). Hence the total computation time taken by our
√
algorithm for any sequence of t updates is O(t n + n log n) with high probability.

4.3 Expected value of the total update time
Let Xv,i,k be a random variable which is 1 if v creates an epoch at level i at update step k, otherwise it is 0.
We denote this epoch as EPOCH(v, i, k). Let Zv,i,k denote the number of edges from Ovinit that are deleted
during the epoch. (If EPOCH(v, i, k) is not created, Zv,i,k is defined
P as 0). Since each edge deletion at level
1 is uniquely associated to the epoch that owned it. Therefore, v,k Zv,1,k ≤ t. Hence,
X
v,k

E[Zv,1,k ] ≤ t

(1)

We shall now derive a bound on the expected value of Zv,1,k in an alternate way.
√
Lemma 4.5 E[Zv,1,k ] ≥ n/2 · Pr[Xv,1,k = 1].
Proof: We shall first find the expectation of Zv,1,k conditioned on the event that v creates an epoch at
level 1 during kth update. That is, we shall find E[Zv,1,k |Xv,1,k = 1]. Let Ovinit be the set of edges
owned by v at the moment of creation of EPOCH(v, 1, k), and let D be the deletion sequence associated
with Ovinit . It follows from Lemma 4.2 that the matched egde of v is distributed uniformly over Ovinit . So
√
√
E[Zv,1,k |Xv,1,k = 1] = |Ovinit |/2 ≥ n/2 since |Ovinit | for an epoch at level 1 is at least n. Using
conditional expectation, we get
√
E[Zv,1,k ] = E[Zv,1,k |Xv,1,k = 1] · Pr[Xv,1,k = 1] ≥ n/2 · Pr[Xv,1,k = 1]

Notice that the computation cost of an epoch at level 1 is at most cn for some constant c. So the expected
value of the computation cost associated with all natural epochs that get terminated at level 1 during t
updates is
X
√ X√
n/2 · Pr[Xv,1,k = 1]
cn · Pr[Xv,1,k = 1] = 2c n
v,k

v,k

√ X
≤ 2c n
E[Zv,1,k ]
√
≤ 2c nt

using Lemma 4.5

v,k

using Equation 1

We can thus conclude with the following theorem.
Theorem 4.1 Starting with a graph on n vertices and no edges, we can maintain maximal matching for any
√
√
sequence of t updates in O(t n) time in expectation and O(t n + n log n) with high probability.

11

√
4.4 On improving the update time beyond O( n)
In order to extend our 2-LEVEL algorithm for getting a better update time, it is worth exploring the reason
√
underlying O( n) update time guaranteed by our 2-LEVEL algorithm. For this purpose, let us examine the
second invariant more carefully. Let α(n) be the threshold for the maximum number of edges that a vertex
at level 0 can own. Consider an epoch at level 1 associated with some edge, say (u, v). The computation
associated with this epoch is of the order of the number of edges u and v own which can be Θ(n) in the
worst case. However, the expected duration of the epoch is of the order of the minimum number of edges u
can own at the time of its creation, i.e., Θ(α(n)). Therefore, the expected amortized computation per edge
deletion for an epoch at level 1 is O(n/α(n)). Balancing this with the α(n) update time at level 0, yields
√
α(n) = n.
In order to improve the running time of our algorithm, we need to decrease the ratio between the maximum and the minimum number of edges a vertex can own during an epoch at any level. It is this ratio that
determines the expected amortized time of an epoch. This insight motivates us for having a finer partition of
vertices – the number of levels should be increased to O(log n) instead of just 2. When a vertex creates an
epoch at level i, it will own at least 2i edges, and during the epoch it will be allowed to own at most 2i+1 − 1
edges. As soon as it owns 2i+1 edges, it should migrate to higher level. Notice that the ratio of maximum to
√
minimum edges owned by a vertex during an epoch gets reduced from n to a constant.
We pursue the approach sketched above combined with some additional techniques in the following
section. This leads to a fully dynamic algorithm for maximal matching which achieves expected amortized
O(log n) update time per edge insertion or deletion.

5 Fully dynamic algorithm with expected amortized O(log n) time per update
This algorithm maintain a partition of vertices among various levels. We describe the difference in this
partition vis-a-vis 2-LEVEL algorithm.
1. The fully dynamic algorithm maintains a partition of vertices among ⌊log4 n⌋ + 2 levels. The levels
are numbered from −1 to L0 = ⌊log4 n⌋. During the algorithm, when a vertex moves to level i, it
owns at least 4i edges. So a vantage point is needed for a vertex that does not own any edge. As a
result, we introduce a level -1 that contains all the vertices that do not own any edge.
2. We use the notion of ownership of edges which is slightly different from the one used in the 2-LEVEL
algorithm. In the 2-LEVEL algorithm, at level 0, both the endpoints of the edge are the owner of the
edge. Here, at every level, each edge is owned by exactly one of its endpoints. If the endpoints of the
edge are at different levels, the edge is owned by the endpoint that lies at the higher level. If the two
endpoints are at the same level, then the tie is broken appropriately by the algorithm.
Like the 2-LEVEL algorithm, each vertex u will maintain a dynamic hash table storing the edges Ou
owned by it. In addition, the generalized fully dynamic algorithm will maintain the following data structure
for each vertex u. For each i ≥ LEVEL (u), let Eui be the set of all those edges incident on u from vertices at
level i that are not owned by u. The set Eui will be maintained in a dynamic hash table. However, the onus
of maintaining Eui will not be on u. For any edge (u, v) ∈ Eui , v will be responsible for the maintenance of
(u, v) in Eui since (u, v) ∈ Ov . For example, suppose vertex v moves to level j. If j > LEVEL (u), then v
will remove (u, v) from Eui and insert it to Euj . Otherwise ( (j ≤ LEVEL (u)), v will remove (u, v) from Eui
and insert it to Ou .

12

3
2
v

1
0

x

−

1

Figure 4: A snapshot of the algorithm on K9 : all vertices are matched( thick edges) except vertex x at level
−1. φv (2) = 4 < 42 and φv (3) = 6 < 43 , so v cannot rise to a higher level.

5.1 Invariants and a basic subroutine used by the algorithm
As can be seen from the 2-level algorithm, it is advantageous for a vertex to get settled at a higher level once
it owns a large number of edges. Pushing this idea still further, our fully dynamic algorithm will allow a
vertex to rise to a higher level if it can own sufficiently large number of edges after moving there. In order
to formally define this approach, we introduce an important notation here.
For a vertex v with LEVEL (v) = i,

P
|Ov | + i≤k<j |Evk | if j > i
φv (j) =
0
otherwise
In other words, for any vertex v at level i and any j > i, φv (j) denote the number of edges which v can
own if v rises to level j. Our algorithm will be based on the following key idea. If a vertex v has φv (j) ≥ 4j ,
then v would rise to the level j. In case, there are multiple levels to which v can rise, v will rise to the highest
such level. With this key idea, we now describe the three invariants which our algorithm will maintain.
1. Every vertex at level ≥ 0 is matched and every vertex at level −1 is free.
2. For each vertex v and for all j >

LEVEL (v),

φv (j) < 4j holds true.

3. Both the endpoints of a matched edge are at the same level.
It follows that the free vertices, if any, will be present at level −1 only. Any vertex v present at level −1 can
not have any neighbor at level −1. Otherwise, it would imply that φv (0) ≥ 1 = 40 , violating the second
invariant. Hence, every neighbor of a free vertex must be matched. This implies that the algorithm will
always maintain a maximal matching. Furthermore, the key idea of our algorithm is captured by the second
invariant – after processing every update there is no vertex which fulfills the criteria of rising. Figure 4
depicts a snapshot of the algorithm.
An edge update may lead to the violation of the invariants mentioned above and the algorithm basically
restores these invariants. This may involve rise or fall of vertices between levels. Notice that the second
invariant of a vertex is influenced by the rise and fall of its neighbors. We now state and prove two lemmas
which quantify this influence more precisely.
Lemma 5.1 The rise of a vertex v does not violate the second invariant for any of its neighbors.

13

Proof: Consider any neighbor u of v. Let LEVEL (u) = k. Since the second invariant holds true for u before
the rise of v, so φu (i) < 4i for all i > k. It suffices if we can show that φu (i) does not increase for any i
due to the rise of v. We show this as follows.
Let vertex v rise from level j to ℓ. If ℓ ≤ k, the edge (u, v) continues to be an element of Ou , and so
there is no change in φu (i) for any i. Let us consider the case when ℓ > k. The rise of v from j to ℓ causes
removal of (u, v) from Ou (or Euj if j ≥ k) and insertion to Euℓ . As a result φu (i) decreases by one for each
i in [max(j, k) + 1, ℓ], and remains unchanged for all other values of i.

Lemma 5.2 Suppose a vertex v falls from level j to j − 1. As a result, for any neighbor u of v, φu (i)
increases by at most 1 for i = j and remains unchanged for all other values of i.
Proof: Let LEVEL (u) = k. In case k ≥ j, there is no change in φu (i) for any i due to fall of v. So let us
consider the case j > k. In this case, the fall of v from level j to j − 1 leads to the insertion of (u, v) in
Euj−1 and deletion from Euj . Consequently, φu (i) increases by one only for i = j and remains unchanged for
all other values of i.

In order to detect any violation of the second invariant for a vertex v due to rise or fall of its neighbors,
we shall maintain {φv (i)|i ≤ L0 } in an array φv [] of size L0 + 2. The updates on this data structure during
the algorithm will involve the following two types of operations.
•

This operation decrements φv (i) by one for all i in interval I. This operation
will be executed when some neighbor of v rises. For example, suppose some neighbor of v rises from
level j to ℓ, then φv (i) decreases by one for all i in interval I = [max(j, LEVEL (v)) + 1, ℓ].

•

INCREMENT-φ(v, i): this operation increases φv (i) by one. This operation will be executed when
some neighbor of v falls from i to i − 1.

DECREMENT-φ(v, I):

It can be seen that a single DECREMENT-φ(v, I) operation takes O(|I|) time which is O(log n) in the worst
case. On the other hand any single INCREMENT-φ(v, i) operation takes O(1) time. However, since φv (i) is
0 initially and is non-negative always, we can conclude the following.
Lemma 5.3 The computation cost of all DECREMENT-φ() operations over all vertices is upper-bounded by
the computation cost of all INCREMENT-φ() operations over all vertices during the algorithm.
Observation 5.1 It follows from Lemma 5.3 that we just need to analyze the computation involving all
INCREMENT-φ() operations since the computation involved in DECREMENT-φ() operations is subsumed by
the former.
If any invariant of a vertex, say u, gets violated, it might rise or fall, though in some cases, it may still
remain at the same level. However, in all these cases, eventually the vertex u will execute the procedure,
GENERIC - RANDOM - SETTLE , shown in Figure 5. This procedure is essentially a generalized version of
RANDOM - SETTLE (u) which we used in the 2-level algorithm. GENERIC - RANDOM - SETTLE (u, i) starts with
moving u from its current level (LEVEL (u)) to level i. If level i is higher than the previous level of u, then
u performs the following tasks. For each edge (u, w) already owned by it, u informs w about its rise to
level i by updating Ewi . In addition u acquires the ownership of all the edges whose other endpoint lies at a
level ∈ [LEVEL (u), i − 1]. For each such edge (u, w) that is now owned by u, we perform DECREMENTφ(w, [LEVEL (w) + 1, i]) to reflect that the edge is now owned by vertex u which has moved to level i.
Henceforth, the procedure then resembles RANDOM - SETTLE. It finds a random edge (u, v) from Ou and
moves v to level i. The procedure returns the previous mate of v, if v was matched. We can thus state the
following lemma.
14

Lemma 5.4 Consider a vertex u that executes GENERIC - RANDOM - SETTLE(u, i) and selects a mate v.
Excluding the time spent in DECREMENT-φ operations, the computation time of this procedure is of the
order of |Ou | + |Ov | where Ou and Ov is the set of edges owned by u and v just at the end of the procedure.

Procedure G E N E R I C - R A N D O M - S E T T L E (u, i)
1 if LEVEL (u) < i then
2
for each (u, w) ∈ Ou do
LEVEL(u)
3
transfer (u, w) from Ew
to Ewi ;
4
DECREMENT-φ(w, [LEVEL (u) + 1, i]);
5
6
7
8
9
10
11
12
13
14
15

16
17
18
19
20
21
22
23
24
25
26
27
28

//u rises to level i
//u informs w about its rise

for each j = LEVEL (u) to i − 1 do
for each (u, w) ∈ Euj do
transfer (u, w) from Euj to Ewi ;
transfer (u, w) from Ow to Ou ;
DECREMENT-φ(w, [j + 1, i]);

//u gains ownership of some more edges

foreach j = LEVEL (u) + 1 to i do φu (j) ← 0;
← i;

LEVEL (u)

Let (u, v) be a uniformly randomly selected edge from Ou ;
if v is matched then
x ← MATE (v);
M ← M\{(v, x)};
else
x ← NULL ;
for each (v, w) ∈ Ov do
LEVEL(v)
transfer (v, w) from Ew
to Ewi ;
DECREMENT-φ(w, [LEVEL (v) + 1, i]);

//v informs w about its rise

for each j = LEVEL (v) to i − 1 do
for each (v, w) ∈ Evj do
transfer (v, w) from Evj to Ewi ;
transfer (v, w) from Ow to Ov ;
DECREMENT-φ(w, [j + 1, i]);

//v gains ownership of some more edges

M ← M ∪ {(u, v)};
foreach j = LEVEL (v) + 1 to i do φv (j) ← 0;
LEVEL (v) ← i ;
return x;

/*v rises to level i*/

Figure 5: Procedure used by a free vertex u to settle at LEVEL i.

5.2 Handling edge updates by the fully dynamic algorithm
Our fully dynamic algorithm will employ a generic procedure called PROCESS - FREE - VERTICES(). The
input to this procedure is a sequence S consisting of ordered pairs of the form (x, k) where x is a free
vertex at level k ≥ 0. Observe that the presence of free vertices at level ≥ 0 implies that matching M
15

is not necessarily maximal. In order to preserve maximality of matching, the procedure PROCESS - FREE VERTICES restores the invariants of each such free vertex till S becomes empty. We now describe our fully
dynamic algorithm.
Handling deletion of an edge
Consider deletion of an edge, say (u, v). For each j > max(LEVEL (u), LEVEL (v)), we decrement φu (j)
and φv (j) by one. If (u, v) is an unmatched edge, no invariant gets violated. So we only delete the edge
(u, v) from the data structures of u and v. Otherwise, let k = LEVEL (u) = LEVEL (v). We execute the
Procedure PROCESS - FREE - VERTICES(h(u, k), (v, k)i).
Handling insertion of an edge
Consider insertion of an edge, say (u, v). Without loss of generality, assume that initially u was at the same
LEVEL(u)
level as v or a higher level than v. So we add (u, v) to Ou and Ev
. For each j > max(LEVEL (u), LEVEL (v)),
we increment φu (j) and φv (j) by one. We check if the second invariant has got violated for either u or v.
This invariant may get violated for u (likewise for v) if there is any integer i > max(LEVEL (u), LEVEL (v)),
such that φu (i) has become 4i just after the insertion of edge (u, v). In case there are multiple such integers,
let imax be the largest such integer. To restore the invariant, u leaves its current mate, say w, and rises to level
imax . We execute GENERIC - RANDOM - SETTLE(u, imax ), and let x be the vertex returned by this procedure.
Let j and k be respectively the levels of w and x. Note that x and w are two free vertices now. We execute
PROCESS - FREE - VERTICES (h(x, k), (w, j)i).
If the insertion of edge (u, v) violates the second invariant for both u and v, we proceed as follows. Let
j be the highest level to which u can rise after the insertion of (u, v), that is, φu (j) = 4j . Similarly, let ℓ
be the highest level to which v may rise, that is, φv (ℓ) = 4ℓ . If j ≥ ℓ, we allow only u to rise to level j;
otherwise
edge (u, v) becomes an element of
P we allow only v to rise to ℓ. Note that after u moves to level j, P
Evj . So LEVEL(v)≤k<ℓ |Evk | decreases by 1. As a result, φv (ℓ) = |Ov | + LEVEL(v)≤k<ℓ |Evk | also decreases
by 1 and is now strictly less than 4ℓ ; thus the second invariant for v is also restored.
5.2.1

Description of Procedure PROCESS - FREE - VERTICES

The procedure receives a sequence S of ordered pairs (x, i) such that x is a free vertex at level i. It processes
the free vertices in a decreasing order of their levels starting from L0 . We give an overview of this processing
at level i. For a free vertex at level i, if it owns sufficiently large number of edges, then it settles at level i
and gets matched by selecting a random edge from the edges owned by it. Otherwise the vertex falls down
by one level. Notice that the fall of a vertex from level i to i − 1 may lead to rise of some of its neighbors
lying at level < i. However, as follows from Lemma 5.2, for each such vertex v, only φv (i) increases by
one and φv () value for all other levels remains unchanged. So the second invariant may get violated only for
φv (i). This implies that v will rise only to level i. After these rising vertices move to level i (by executing
GENERIC - RANDOM - SETTLE ), we move onto level i − 1 and proceed similarly. Overall, the entire process
can be seen as a wave of free vertices falling level by level. Eventually this wave of free vertices reaches
level −1 and fades away ensuring maximal matching. With this overview, we now describe the procedure
in more details and its complete pseudocode is given in Figure 6.
The procedure uses an array Q of size L0 + 2, where Q[i] is a pointer to a queue (initially empty)
corresponding to level i. For each ordered pair (x, k) ∈ S, it inserts x into queue Q[k]. The procedure
executes a FOR loop from L0 down to 0 where the ith iteration extracts and processes the vertices of queue
Q[i] one by one as follows. Let v be a vertex extracted from Q[i]. First we execute the function FALLING(v)
which does the following. v disowns all its edges whose other endpoint lies at level i. If v owns less than 4i
16

Procedure P R O C E S S - F R E E - V E R T I C E S (S)
1 for each (x, i) ∈ S do ENQUEUE(Q[i], x);
2 for i = L 0 to 0 do
3
while (Q[i] is not EMPTY) do
4
v ← DEQUEUE(Q[i]);
5
if FALLING(v) then
6
LEVEL (v) ← i − 1;
7
ENQUEUE(Q[i − 1], v);
9
8
for each (u, v) ∈ Ov do
10
transfer (u, v) from Eui to Eui−1 ;
11
INCREMENT-φ(u, i);
12
INCREMENT-φ(v, i);
13
if φu (i) ≥ 4i then
14
x ← GENERIC - RANDOM - SETTLE(u, i);
15
if x 6= NULL then
16
j ← LEVEL (x);
17
ENQUEUE(Q[j], x);

//v falls to i − 1

//u rises to i

18

19
20
21
22

else
x ← GENERIC - RANDOM - SETTLE(v, i);
if x 6= NULL then
j ← LEVEL (x);
ENQUEUE(Q[j], x);

Function F A L L I N G (v)
i ← LEVEL (v);
2 for each (u, v) ∈ Ov such that LEVEL(u) = i do
3
transfer (u, v) from Ov to Ou ;
4
transfer (u, v) from Eui to Evi ;

//v settles at level i

1

5

//v disowns all edges at level i

if |Ov | < 4i then return TRUE else return FALSE;

Figure 6: Procedure for processing free vertices given as a sequence S of ordered pairs (x, i) where x is a
free vertex at LEVEL i.

17

edges then v falls to level i − 1, otherwise v will continue to stay at level i. The processing of the free vertex
v for each of these two cases is done as follows.
1. v has to stay at level i.
v executes GENERIC - RANDOM - SETTLE and selects a random mate, say w, from level j < i (if w is
present in Q[j] then it is removed from it and is raised to level i). If x was the previous mate of w,
then x is a falling vertex. Vertex x gets added to Q[j]. This finishes the processing of v.
2. v has to fall.
In this case, v falls to level i − 1 and is inserted to Q[i − 1]. At this stage, Ov consists of neighbors
of v from level i − 1 or below. It follows from Lemma 5.2 that the fall of v from i to i − 1 leads to
increase in φu (i) by one for each neighbor u of v which is present at a level lower than i. Moreover,
φv (i), that was 0 initially, has to be set to |Ov |. So all the vertices of Ov are scanned, and for each
(u, v) ∈ Ov , we increment φu (i) and φv (i) by 1. In case φu (i) has become 4i , u has to rise to level i
and is processed as follows. u executes GENERIC - RANDOM - SETTLE(u, i) to selects a random mate,
say w, from level j < i. If w was in Q[j] then it is removed from it. If x was the previous mate of w,
then x is a falling vertex, and so it gets added to queue Q[j].
Remark 5.1 Notice a stark similarity between the above procedure for handling a free vertex and the procedure for handling a free vertex at level 1 in the 2-level algorithm.
In case 1, v remains at level i and w moves to the level i from some level j < i. This renders vertex x
(earlier mate of w) free and the first invariant of x is violated. So x is added to the queue at level j. The
processing of v does not change φu () for any neighbor u of v. Furthermore, the rise of w to level i does not
lead to violation of any invariant due to Lemma 5.1. In case 2, v falls to level i − 1 and as a result some
vertices may rise to level i. Each such rising vertex executes GENERIC - RANDOM - SETTLE. As in case 1,
the processing of these rising vertices may create some free vertices only at level < i. We can thus state the
following lemma.
Lemma 5.5 After ith iteration of the for loop of PROCESS - FREE - VERTICES, the free vertices are present
only in the queues at level < i, and for all vertices not belonging to these queues the three invariants holds.
Lemma 5.5 establishes that after termination of procedure PROCESS - FREE - VERTICES, there are no free
vertices at level ≥ 0 and all the invariants get restored globally.

5.3 Analysis of the algorithm
Processing the deletion or insertion of an edge (u, v) begins with decrementing or incrementing φu (i) and
φv (i) for each level j > max(LEVEL (u), LEVEL (v)). Since there are O(log n) levels, the computation
associated with this task over any sequence of t updates will be O(t log n). This task may be followed by
executing the procedure PROCESS - FREE - VERTICES that restores the invariants and updates the matching
accordingly. The updates in the matching can be seen as creation of new epochs and termination of some of
the existing epochs. Like 2-level algorithm, for the purpose of analysis, we visualize the entire algorithm as
a sequence of creation and termination of various epochs. Excluding the O(t log n) time for maintaining φ,
the total computation performed by the algorithm can be associated with all the epochs that get terminated
and those that remain alive at the end of the sequence of updates. Along exactly similar lines as in 2-level
algorithm, the computation associated with all the epochs that are alive at the end of t updates is O(t) only.
So we just need to focus on the epochs that get terminated and the computation associated with each of
them.

18

Let us first analyse the computation associated with an epoch of a matched edge (u, v). Suppose this
epoch got created by vertex v at level j. So v would have executed GENERIC - RANDOM - SETTLE and selected
u as a random mate from level < j. Note that v must be owning less than 4j+1 edges and u would be owning
at most 4j edges at that moment. This observation and Lemma 5.4 imply that the computation involved in the
creation of the epoch is O(4j ). Once the epoch is created, any update pertaining to u or v will be performed
in just O(1) time until the epoch gets terminated. Let us analyze the computation performed when the
epoch gets terminated. At this moment either one or both u and v become free vertices. If v becomes free, v
executes the following task (see procedure PROCESS - FREE - VERTICES in Figure 6): v scans all edges owned
by it, which is less than 4j+1 , and disowns those edges incident from vertices of level j. Thereafter, if v still
owns at least 4j edges, it settles at level j and creates a new epoch at level j. Otherwise, v keeps falling one
level at a time. For a single fall of v from level i to i − 1, the computation performed involves the following
tasks: scanning the edges owned by v, disowning those incident from vertices at level i, incrementing φw
values for each neighbor w of v lying at level less than i, and updating φv (i) to |Ov |. All this computation
is of the order of the number of edges v owns at level i which is less than 4i+1 . Eventually either v settles at
some level k ≥ 0 and becomes part
Pof a new epoch or it reaches level −1. The total computation performed
by v is, therefore, of the order of ji=k 4i+1 = O(4j ). This entire computation involving v (and u) in this
process is associated with the the epoch of (u, v). Hence we can state the following Lemma.
Lemma 5.6 For any i ≥ 0, the computation associated with an epoch at level i is O(4i ).
An epoch corresponding to edge (u, v) at level i could be terminated if the matched edge (u, v) gets
deleted. Such an epoch is called a natural epoch. However, this epoch could be terminated due to one of the
following reasons also.
• u (or v) get selected as a random mate by one of their neighbors present at LEVEL > i.
• u (or v) starts owning 4i+1 or more edges.
Each of the above factors render the epoch to be an induced epoch. For any level i > 0, the creation of an
epoch causes termination of at most two epochs at levels < i. It can be explained as follows: Consider an
epoch at level i associated with an edge, say (u, v). Suppose it was created by vertex u. If u was already
matched at some level j < i, let w 6= v be its mate. Similarly, if v was also matched already at some level
k < i, let x 6= u be its mate. So matching u to v terminates the epoch of (u, w) and (v, x) at level j and k
respectively. We can thus state the following lemma.
Lemma 5.7 Creation of an epoch at a level i may cause termination of at most 2 epochs at level < i.
5.3.1

Analysing an epoch

Consider an epoch created by a vertex v at level i. At the time of the creation of the epoch, let Ovinit be
the set of edges owned by v, and let w = MATE(v). This epoch may terminate much before the deletion
of (v, w). This happens when v or w moves to some level > i before the deletion of (v, w). In order to
analyse termination of an epoch, therefore, we associate an update sequence with it as follows. For each
edge (v, x) ∈ Ovinit , we consider the first time in future that x moves to some higher level 2 . The update
label associated with edge (v, x) is defined as
if x moves to a level > i before its deletion then it is classified as upward else it is deletion.
2

vertex x may move to level > i (and down) multiple times while the algorithm processes a sequence of updates. However, it
is only the first time (after the creation of the epoch) when x moves to a level > i that is relevant as far as the possibility of the
termination of the epoch by the upward movement of x is concerned.

19

Likewise, we also consider the first time in future that v moves to a level > i. If v never moves to any level
> i in future, we just append v at the end of all the updates associated with Ovinit . The update sequence U for
the epoch is the sequence of these updates on the edges of Ovinit and vertex v arranged in the chronological
order. Consider the following example. Suppose Ovinit has 10 edges and let the corresponding neighbors
of v be {w1 , . . . , w10 }. Let the updates in the chronological order be : the deletion of (v, w4 ), upward
movement of w1 , upward movement of w9 , the deletion of (v, w5 ), and so on. The corresponding update
sequence will be
•

↑

↑

•

•

•

↑

↑

•

↑

•

U : h w4 , w1 , w9 , w5 , w8 , w3 , w2 , v, w7 , w10 , w8 i
Observation 5.2 If the update associated with (the owner) v appears at ℓth location in U , then the epoch
will terminate on or before the ℓth update in U . Therefore, the updates at location > ℓ in U will have no
influence on the termination of the epoch.
Unlike the 2-level algorithm, the update sequence associated with an epoch is not uniquely defined by the
sequence of updates in the graph after the creation of the epoch. Rather, it also depends upon the current
matching as well as the random bits chosen by the algorithm while processing the updates. So there is
a probability distribution defined over all possible update sequences that depends upon these two factors.
Consequently, the analysis of an epoch in our final algorithm is more complex compared to the 2-level
algorithm. In particular, it is not obvious whether there is any dependence between the random mate picked
by a vertex while creating an epoch and the sequence of updates associated with the epoch. However, using
an interesting non-trivial property of our algorithm, we will establish that there is no dependence between
the two.
Lemma 5.8 Suppose a vertex v creates an epoch and let Ovinit be the set of its owned edges at the time of
the creation of this epoch. Then, for any update sequence U and for each (v, w) ∈ Ovinit ,
Pr[MATE (v) = w | U ] = Pr[MATE (v) = w] =

1
|Ovinit |

Lemma 5.8 can be seen as a generalization of Lemma 4.2 that we stated for our 2-level algorithm. Its proof
is given in Section 5.4. The analysis of our algorithm will be critically dependent on this lemma. Using this
lemma, we shall first establish a high probability bound on the total update time of the algorithm to process
a sequence of updates in the graph.
5.3.2

High probability bound on the total update time

Recall Definition 4.2 of a bad epoch. It can be observed from this definition that an induced epoch is always
a good epoch. Using Lemma 5.8, the lemma for the probability of a bad epoch extends seamlessly from
2-level algorithm to our final algorithm as follows.
Lemma 5.9 Suppose vertex v creates an epoch at level i while the algorithm processes kth update in the
graph. This epoch will be bad with probability at most 1/3 irrespective of the updates in the graph and the
random bits picked during their processing.
Proof: The termination of the epoch is completely determined by the mate that v picks and the update
sequence associated with this epoch. Consider any update sequence U associated with this epoch. It follows
from Lemma 5.8 that conditioned on U , the mate of v is equally likely to be the endpoint of any edge in
20

Ovinit . Now recall from Observation 5.2 that for the epoch to be terminated naturally, the mate of v must be
among the endpoints of the deleted edges that precede v in U . We distinguish between the following two
cases.
Case1. There are less than |Ovinit |/3 edge deletions preceding v in U .
The epoch will be bad only if the matched edge of v is one of these edge deletions preceding v in U .
Since the number of these edge deletions is less than |Ovinit |/3, so using Lemma 5.9 the probability
of the epoch to be bad is less than 1/3.
Case2. There are at least |Ovinit |/3 edge deletions preceding v in U .
The epoch will be bad if the matched edge of v is one of the first |Ovinit |/3 edge deletions in U . From
Lemma 5.8, the termination of the epoch is equiprobable for any of the Ovinit ’s, so the probability that
the epoch is bad in this case is exactly 1/3.
It follows that the epoch is going to be bad with probability 1/3 for each possible update sequence U associated with the epoch.

We will show that the number of bad epochs at a level i could exceed the number of good epochs at
level i by at most O(log n) with very high probability. Notice that the number of epochs created at level i
is itself a random variable. During any update, the number of epochs that will be created at level i depends
upon the past updates in the graph and the random bits picked during their processing. However, Lemma
5.9 implies that each newly created epoch at level i will be bad with probability at most 1/3 independent of
these events. Hence, the sequence of epochs at level i can be seen as an instance of the asymmetric random
walk as established in the analysis of the 2-level algorithm. So the bad epochs at any level i may exceed the
good epochs by 2 log 2 n with probability at most 1/n2 . There are O(log n) levels in the hierarchy. Hence
we get the following lemma using union bound.
Lemma 5.10 For every level i ≤ L0 , the number of bad epochs will not exceed the number of good epochs
by more than 2 log2 n with probability at least 1 − (log n)/n2 > 1 − 1/n.
Let us temporarily exclude the maximum surplus of O(log n) bad epochs at each level from our analysis.
Consequently, it follows from Lemma 5.10 that each bad epoch at a level can be mapped to a good epoch
at the same level in a unique manner - see Figure 7(i). Also the creation of each epoch at a level i + 1
can terminate at most two (induced) epochs at lower levels as stated in Lemma 5.7. Using this fact and
the mapping between the good and bad epochs at a level, we can construct a forest whose nodes will be
the epochs terminated across all levels during the algorithm. The intuition for defining this forest is that
eventually the computation cost of a bad epoch or an induced epoch will be charged to a good natural epoch.
Since a good natural epoch has sufficiently large number of edge deletions associated with it, these edge
deletions can be charged to pay for all the computation carried out by our algorithm.
With this intuition, we now provide the construction of the forest by defining parent of each epoch using
the following rules.
1. Parent of each induced epoch is the epoch at the higher level whose creation led to its termination.
2. Parent of a good epoch is itself (hence it is the root of its tree).
3. If a bad epoch is mapped to an induced epoch, then its parent is the same as the parent of the induced
epoch. Otherwise, it is the parent of itself (hence it is the root of its tree).

21

... ...

. . .a

...

b ...

...

c

g. . .

d. . .

g

i+1

a

i

b

A natural good epoch
An induced epoch
A bad epoch
c

d

(ii)

(i)

Figure 7: (i) Mapping between bad and good epochs at level i (ii) Assigning at most 4 epochs from lower
levels to an epoch.
It follows from rule 1 and 3 (the if part) that with an epoch at a level, at most 4 epochs from lower levels
can be associated. Hence each node in the forest will have at most four children. See Figure 7(ii). Moreover,
the root of each tree in the forest of epochs is either a bad epoch or a good natural epoch. Using Lemma
5.6, the computation cost C(i) associated with a tree of epochs whose root is at level i obeys the following
recurrence for some constant a.
C(i) = a4i + 4C(i − 1)

The solution of this recurrence is C(i) = O(i4i ). It follows from Lemma 5.10 and rule 3(Otherwise part)
that the trees rooted at good natural epochs at a level i are at least the number of trees rooted at bad epochs at
level i. Hence, it suffices to analyze the computation cost associated with all the tree rooted at good natural
epochs. Now for each good natural epoch at a level i, there are at least 4i /3 edge deletions associated
uniquely to it. This natural epoch will be charged for the computation cost C(i) = O(i4i ) associated with
the tree rooted at it. So if t is the total number of updates in the graph, then the computation cost associated
with all epochs
P in the forest is O(t log n). The computation cost associated with surplus bad epochs at all
levels is O( i i4i log n) = O(n log2 n). Hence with high probability the computation cost for processing
t edge updates by the algorithm is O(t log n + n log2 n). This also implies that the total expected update
time is O(t log n) for t = Ω(n log n). In the following subsection, we will establish O(t log n) bound on
the expected update time for all values of t.
5.3.3

Expected value of the total update time

During a sequence of t updates in the graph, various epochs get created by various vertices at various levels.
Let Xv,i,k be a random variable which is 1 if v creates an epoch at level i at update step k, otherwise it
is 0. We denote this epoch as EPOCH(v, i, k). Let Ovinit denote the edges that v owned at the time of the
creation of the epoch. Let Zv,i,k denote the number of edges from Ovinit that are deleted during the epoch.
(If EPOCH(v, i, k) is not created, Zv,i,k is defined as 0). The key role in bounding the expected running time
is played by a random variable Bv,i,k defined as follows:
(
(8Zv,i,k − 2 · 4i )Xv,i,k if EPOCH (v, i, k) is natural
(2)
Bv,i,k =
(4i+1 − 2 · 4i )Xv,i,k
if EPOCH (v, i, k) is induced
First observe that Bv,i,k = 0 if Xv,i,k = 0. Else (if Xv,i,k = 1), the random variable Bv,i,k can be seen
as credits associated with EPOCH(v, i, k) to be used for paying its computation cost. For a natural epoch, the
credits is defined in terms of the edges deleted during the epoch. So we define Bv,i,k to be 8Zv,i,k . However,
we need to discount for the two epochs at lower levels that may get terminated due to EPOCH(v, i, k). To
this end, from the term, we deduct 2 · 4i . Similarly, if EPOCH(v, i, k) is an induced epoch, then it gets 4i+1
credits from the epoch that destroyed it. But here again we need to discount for the two epochs at lower
22

i
i+1
levels that might be
P terminated by it. To this end, we again deduct 2 · 4 from 4 . The following lemma
gives a bound on Bv,i,k .

Lemma 5.11

P

v,i,k

Bv,i,k ≤ 8

P

v,i,k

Zv,i,k ≤ 8t, where t is the total number of updates in the graph.

Proof: We need to analyze the sum of Bv,i,k ’s for all those (i, v, k) values for which the EPOCH(i, v, k)
got created. If this epoch is an induced epoch, it can be associated with an epoch, say EPOCH(v ′ , i′ , k′ ), at
′
term in Bv′ ,i′ ,k′ cancels out
a higher level i′ > i whose creation destroyed it. Notice that the negative 4i P
the positive 4i+1 term in Bv,i,t . Hence, the contribution of induced epochs P
in v,i,k Bv,i,k is P
nullified and
all that remains is the sum of terms 8Zv,i,k for each natural epoch. Hence v,i,k Bv,i,k ≤ 8 v,i,k Zv,i,k .
An edge deletion
is associated with an epoch in a unique manner, so will contribute to exactly one Zv,i,k .
P
Therefore, v,i,k Zv,i,k is upper bounded by the total number of edges deleted.

P

Corollary 5.0.1

v,i,k

E[Bv,i,k ] ≤ 8t

Lemma 5.12 For all i, v, k, E[Bv,i,k ] ≥ Pr[Xv,i,k = 1] · 4i .
Proof: Since Xv,i,k is an indicator random variable, E[Bv,i,k ] = Pr[Xv,i,k = 1] E[Bv,i,k | Xv,i,k = 1]. We
will first estimate E[Bv,i,k | Xv,i,k = 1], that is, the expected value of Bv,i,k given that EPOCH(v, i, k) got
created.
Let (U , P ) be the probability space of all the update sequences associated with this epoch and let U ∈ U
be any update sequence. Suppose among the updates in U that precede the update associated with v, only
d are edge deletions. It follows from Lemma 5.8 that the matched edge of v is distributed uniformly over
Ovinit . So EPOCH(v, i, k) will be an induced epoch with probability (|Ovinit | − d)/|Ovinit | and in that case
B(v, i, k) will be 4i+1 − 2 · 4i . If the epoch is natural, it couldP
be due to any one of the d edge deletions
present in U . In that case the expected value of Bv,i,k will be 1/d dj=1 (8j −2·4i ) ≥ 4d−2·4i . Considering
the cases of induced and natural epoch together,
E[Bv,i,k | U ] =

d
4i+1 d − 4d2
|Ovinit | − d i+1
i
i
i
(4
−
2
·
4
)
+
(4d
−
2
·
4
)
=
2
·
4
−
|Ovinit |
|Ovinit |
|Ovinit |
4i · 4i
≥ 2 · 4i − init (for all values of d)
|Ov |

Therefore
E[Bv,i,k ] =

X

U ∈U

E[Bv,i,k | U ] · Pr[U ] ≥



4i · 4i
2 · 4 − init
|Ov |
i

Since |Ovinit | ≥ 4i , for level i, the result follows.

 X
4i · 4i
·
Pr[U ] = 2 · 4i − init
|Ov |
U ∈U



Let Wv,i,k be a random variable that corresponds to the value of the computation cost of EPOCH(v, i, k)
if the epoch is created and is 0 otherwise. Notice that the computation cost of an epoch at level i is c4i+1 for
some constant c. So, E[Wv,i,k ] = Pr[Xv,i,k = 1]c4i+1 . Therefore, using Lemma 5.12,
E[Wv,i,k ] ≤ 4cE[Bv,i,k ]

(3)

Using the above equation and Corollary 5.0.1, the total expected computation cost associated with all epochs
that get destroyed during the algorithm can be bounded by O(t) as follows.
X
X
E[Wv,i,k ] ≤
4cE[Bv,i,k ] ≤ 32ct = O(t)
v,i,k

v,i,k

23

Since for each update in the graph, we incur O(log n) time to update φ at various levels, there is an O(t log n)
overhead for t updates. We can thus conclude with the following theorem.
Theorem 5.1 Starting with a graph on n vertices and no edges, we can maintain a maximal matching for
any sequence of t updates in O(t log n) time in expectation and O(t log n + n log2 n) with high probability.

5.4 Proof of Lemma 5.8
Our algorithm use randomization to maintain maximal matching. After any given sequence of updates,
there is a set of possible maximal matchings that the algorithm may be maintaining and there is probability
distribution associated with these maximal matchings. So it is useful to think about the probability space of
these matchings as the algorithm proceeds while processing a sequence of updates.
We introduce some notations first. For any matching M maintained at any stage by our algorithm, let
Mi denote the matching at level i. Let M>i = ∪j>i Mj denote the matchings at all levels > i. Let Vi
denote the set of all the vertices belonging to levels in the range ∈ [−1, i]. We now extend the notations to
incorporate the updates in the graph. For any k ≥ 1, let G(k) denote the graph after a given sequence of k
updates and let M(k) denote the maximal matching of G(k) as maintained by our algorithm. Let M>i (k)
denote the matching at all levels > i after a given sequence of k updates.
After processing certain number of updates by the algorithm, suppose M and M ′ are any two matchings
′ . Consider any single update in the graph at this stage. In order to process it,
possible such that M>i = M>i
suppose we carry out two executions I and I ′ of our algorithm with the initial matching being M and M ′
respectively. That is, M(0) = M in the execution I and M(0) = M ′ in the execution I ′ . Our claim is
that the probability distribution of matching at levels > i will be identical at the end of both the executions.
More precisely, for any maximal matching µ(1) on a subset of vertices in graph G(1),
Pr[M>i (1) = µ(1)|M(0) = M ] = Pr[M>i (1) = µ(1)|M(0) = M ′ ]
In order to establish our claim, we shall crucially exploit the following lemma.
Lemma 5.13 For both the matchings M and M ′ , φv (j) is the same for each v ∈ V and j > i.
′ . This implies that for each level j > i the sets of vertices present are
Proof: It is given that M>i = M>i
identical in M and M ′ . Hence the set Vi of all the vertices present at levels ∈ [−1, i] is identical in M and
M ′ . Hence for any vertex v, and any level j > i, the set of all the neighbours of v at levels < j is identical;
notice that φv (j) is just the cardinality of this set. So it follows that φv (j) is the same for each vertex v and
each j > i.


We shall now establish our claim for the deletion of an edge e = (u, v). Establishing the claim for the
insertion of an edge is similar. Notice that our algorithm does not alter the matching if e is not a matched
edge. If e is a matched edge, a wave of free vertices originates from LEVEL (e) and propagates downward.
The following fact follows from our analysis in Section 5.2.1.
F 1. The algorithm won’t alter the matching at level >

LEVEL (e)

while processing the deletion of e.

F 2. The matching is updated in the decreasing order of levels, and once the updating of the matching at a
level is complete, the matching at that level will remain unchanged during the updates of the matching
at lower levels.

24

It follows from the description of M and M ′ that either LEVEL (e) is less than or equal to i in both the
matchings or LEVEL (e) is the same in M and M ′ . Let us first consider the (easier) case when LEVEL (e) ≤ i
in M as well as M ′ . It follows from Fact F 1 stated above that the only changes in matching M and M ′
will be at levels ≤ i. Hence the matching M>i (1) will be identical at the end of both the executions I
and I ′ . Let us now consider the more interesting case of LEVEL (e) > i. Both the executions I and I ′
invoke the procedure PROCESS - FREE - VERTICES(h(u, LEVEL (e)), (v, LEVEL (e))i) in this case. The reader
is recommended to revisit this procedure from Section 5.2.1 before proceeding further.
In order to establish our claim about I and I ′ , we shall establish the following. While the matching at
levels > i is being updated, for each step executed in I, the identical step can be executed in I ′ . Moreover,
if the step in I is executed with some probability, the step will be executed with the same probability in I ′
as well. In order to show this, let us analyse the first iteration of the procedure PROCESS - FREE - VERTICES.
Both I and I ′ will process u first. After disowning its edges from its present level, u owns the same set of
edges in both the executions. Thereafter, u will either stay at the same level or fall by one level. If u stays
at the same level, it chooses a random edge to get matched. The probability that any specific random edge
is picked by u is the same in both the executions. Let us consider the case that u falls by one level. For
each neighbour z of u, it follows from Lemma 5.13 that φℓ (z) is the same in the case of M and M ′ . Hence
the set of vertices rising to level ℓ are the same in both the executions. In addition, the set of edges that
each such vertex owns on rising to level j is also the same, hence, the probability that any specific random
mate is picked is the same in both the executions. So each update in M and M ′ is equally likely during the
processing of u. The reader may note that after each such identical update in M and M ′ , the matchings are
identical at each level > i. Hence, Lemma 5.13 holds again for the updated matchings.
Unlike the first iteration, a generic iteration of the procedure PROCESS - FREE - VERTICES may have free
vertices at levels ≤ LEVEL (e) that are kept in respective queues at these levels. Suppose in the beginning
of any such iteration of the procedure PROCESS - FREE - VERTICES there are two possible configurations such
that the matching as well as the queue storing the free vertices are identical at each level > i but differ at
levels ≤ i. Lemma 5.13 will hold for these configurations as well. Therefore, along exactly the same lines
as the first iteration analysed above, it can be shown that every update in the matching at level > i will be
carried out with the same probability during any generic iteration for any two configurations that match at
all levels > i.
Therefore, each sequence of updates in the matching is equally likely in both the executions I and I ′ till
the last free vertex at level i + 1 is processed. Henceforth, the two executions may differ. But as follows
from Fact F 2, it will affect only the matching at levels ≤ i and there won’t be any change in the matching
at higher levels.
This concludes our claim for a single update. This claim can be invoked appropriately for a sequence of
updates giving us the following theorem.
Theorem 5.2 Let M and M ′ be any two matchings possible by our algorithm at any time such that M>i =
′ . For any sequence of t update in the graph, suppose we carry out two executions I and I ′ of our
M>i
algorithm with the initial matching being M and M ′ respectively. The probability distribution of matching
at every level > i will be identical at the end of both the executions. That is,
Pr[M>i (t) = µ(t), . . . , M>i (1) = µ(1)|M(0) = M ] = Pr[M>i (t) = µ(t), . . . , M>i (1) = µ(1)|M(0) = M ′ ]
where µ(j), for 1 ≤ j ≤ t, is any maximal matching on a subset of vertices in the graph G(j).
For the proof of Theorem 5.2, we shall apply the argument for single update inductively and use the following lemma from elementary probability theory.
Lemma 5.14 Suppose A, B, C are three events defined over a probability space (Ω, P ). Then,
Pr[A ∩ B | C] = Pr[A | B ∩ C] · Pr[B | C]
25

Let us define events C as M(0) = M and C ′ as M(0) = M ′ . We have shown that Pr[M>i (1) =
µ(1) | C] = Pr[M>i (1) = µ(1) | C ′ ]. If we define event B as M>i (1) = µ(1) then by another application
of the arguments that we used for a single update,
Pr[M>i (2) = µ(2) | B, C] = Pr[M>i (2) = µ(2) | B, C ′ ]
Applying Lemma 5.14, we get
Pr[M>i (2) = µ(2), B | C] = Pr[M>i (2) = µ(2) | B, C] · Pr[B | C]
Since Pr[B | C] = Pr[B|C ′ ], it follows that
Pr[M>i (2) = µ(2), B | C] = Pr[M>i (2) = µ(2), B | C ′ ]
The above argument can be inductively applied for every subsequent update. This completes the proof of
Theorem 5.2
5.4.1

Connection to the analysis

We first state two lemmas from elementary probability theory that deal with the independence of events. For
the sake of completeness, the proof of these lemmas is given in Appendix.

The first lemma deals with conditional probability.
Lemma 5.15 Let A be an event and B1 , . . . , Bk be k mutually exclusive events defined over a probability
space (Ω, P ). If Pr[A | Bj ] = ρ for each 1 ≤ j ≤ k, then Pr[A | C] = ρ where event C = ∪j Bj .
The second lemma deals with independence of events. Let A and B be two events defined over a probability
space (Ω, P ). A is said to be independent of B if Pr[A | B] = Pr[A | B̄] = Pr[A]. Alternatively,
Pr[A ∩ B] = Pr[A] · Pr[B]. The notion of independence gets carried over from events to random variables
in a natural manner as follows.
Definition 5.1 An event A is said to be independent of a random variable X if for each x ∈ X, Pr[A | X =
x] = Pr[A].
Lemma 5.16 Suppose A is an event and X be a random variable defined over probability space (Ω, P ). If
A is independent of X, then for each x ∈ X,
Pr[X = x | A] = Pr[X = x]
Now we shall establish the connection of Theorem 5.2 to the anlysis of our algorithm. In particular, we shall
use this theorem to prove Lemma 5.8. Suppose a vertex v creates an epoch at level i while the algorithm
processes kth update in the graph for any k < t. We shall analyse the probability space of the future
matchings starting from the time just before the creation of this epoch.
While creating its epoch, v chooses its mate randomly uniformly out of Ovinit . Clearly, the change in the
matching at levels ≤ i will depend on the mate that v picks. Let M be the set of all possible matchings once
the algorithm completes the processing of the kth update. Now notice that all matchings from the set M are
identical at each level > i. So it follows from Theorem 5.2 that for any two matchings M, M ′ ∈ M,
26

Pr[M>i (t) = µ(t), . . . ,M>i (k + 1) = µ(k + 1) | M(k) = M ]
= Pr[M>i (t) = µ(t), . . . , M>i (k + 1) = µ(k + 1) | M(k) = M ′ ]
Let this conditional probability be ρ. For each (v, w) ∈ Ovinit , there may be several matchings in M in
which v is matched to w. By applying Lemma 5.15, the following equation holds for every (v, w) ∈ Ovinit ,
Pr[M>i (t) = µ(t), . . . , M>i (k + 1) = µ(k + 1) | MATE (v) = w] = ρ
Since this probability is the same for each (v, w) ∈ Ovinit , so using Definition 5.1, it follows that the
matchings at levels > i during any sequence of updates is independent of the mate that v picked during the
creation of its epoch. Now applying Lemma 5.16 we get the following lemma.
Lemma 5.17 Suppose a vertex v creates an epoch at level i while the algorithm processes kth update in the
graph. Consider any sequence of updates in the graph. The mate picked by v while creating the epoch is
independent of the sequence of matchings at levels > i computed by the algorithm while processing these
updates. That is, for any t > k, and any (v, w) ∈ Ovinit ,
Pr[MATE (v) = w|M>i (t) = µ(t), . . . , M>i (k + 1) = µ(k + 1)] = Pr[MATE (v) = w] =

1
|Ovinit |

Consider any given sequence of t updates in the graph. Subsequent to the time v creates an epoch at level
i during kth update, let µ = hµ(k + 1), . . . , µ(t)i be the sequence of matching at levels > i as computed by
the algorithm. Notice that the upward movement of v and each (v, w) ∈ Ovinit after the creation of epoch
is captured precisely by the corresponding update in the matching at level > i. Therefore, using µ we can
define the update sequence associated with the epoch as follows. Consider an edge (v, w) ∈ Ovinit and let ℓth
update in the graph be the deletion of (v, w). Let j < ℓ be the smallest integer such that w ∈ µj , that is, w
appears in the matching at level > i while processing of jth update in the graph, then the update associated
with (v, w) is the upward movement. If no such j exists, the update associated with (v, w) is its deletion.
Likewise, we define the update associated with v. The update sequence U for the epoch is the sequence of
these updates on v and the edges of Ovinit arranged in the chronological order.
For an update sequence U associated with an epoch, there may exist many sequences {µ1 , . . . , µq } such
that for each of them, the update sequence associated with the epoch is U . It follows from Lemma 5.17 that
the mate picked by v during its epoch is independent of each such sequence µr , 1 ≤ r ≤ q. Therefore, using
Lemma 5.15, the mate picked by v during its epoch is independent of U as well. Thus we have established
the validity of Lemma 5.8.

6 A tight example
We tested our algorithm on random graphs of various densities and found that the matching maintained is
very close to the maximum matching. This suggests that our algorithm might be able to maintain nearly
maximum matching for dynamic graphs appearing in various practical applications. However, it is not hard
to come up with an update sequence such that at the end of the sequence, the matching obtained by our
algorithm is strictly half the size of maximum matching. In other words, the approximation factor 2 for the
matching maintained by our algorithm is indeed tight. We present one such example as follows (see Figure
8).
Let G(V ∪ W, E) be a graph such that V = {v1 , v2 , . . . , vn } and W = {w1 , w2 , . . . , wn } for some
even number n. Consider the following update sequence. In the first phase, add edges between every pair
of vertices present in V . This results in a complete subgraph on vertices of V . The size of any maximal
27

Figure 8: An example where our algorithm gives a 2-approximation. The vertices on top are in V and form
a complete graph. The vertices at the bottom of the figure are in W .
matching on a complete graph of size n is n/2. After the first phase of updates ends, the size of matching
obtained by our algorithm is n/2. In the second phase, add edge (vi , wi ) for all i. Note that the degree of
each wi is one at the end of the updates. Let us now find the matching which our algorithm maintains. Let
(vi , vj ) be an edge in the matching after phase 1. Note that both these endpoints are at a level greater than
−1. A vertex in W is at level −1 as it does not have any adjacent edges after phase 1. When an edge (wi , vi )
is added, since vi is at a higher level than wi , vi becomes the owner of this edge. The second invariant of vi is
not violated after this edge insertion and nothing happens at this update step and wi still remains at level −1.
Using same reasoning, we can show that wj also remains at level −1 after the addition of edge (vj , wj ). So
matching maintained by the algorithm remains unchanged. It is easy to observe that the maximum matching
of the graph G now has size n which is twice the size of the matching maintained by our algorithm.

7 Postscript
We presented a fully dynamic randomized algorithm for maximal matching which achieves expected amortized O(log n) time per edge insertion or deletion. An interesting question is to explore how crucial randomization is for dynamic maximal matching.
Subsequent to an earlier version of this paper [5], Bhattacharya et al. [9] almost answered this question
in affirmative by designing a determinstic algorithm that maintains (2 + ǫ)-approximate matching in amortized O(poly(log n, 1/ǫ) update time. Another interesting question is to explore whether we can achieve
O(1) amortized update time. Very recently Solomon [29] answered this question in affirmative as well by
designing a randomized algorithm that takes O(t + n log n) update time with high probability to process
any sequence of t edge deletions. Though the basic building blocks of his algorithm are the same as ours,
the two algorithms are inherently different and so are their analysis.
In our algorithm, a vertex may rise to a higher level and create a new epoch even when its matched edge
is intact. But the algorithm of Solomon [29] of takes a lazy approach to maintain the hierarchy of vertices
wherein a vertex is processed only when it becomes absolutely necessary. Another crucial difference is the
following. Our algorithm maintains a function φv (j) for each vertex v and each level j. This function is used
to ensure an invariant that each vertex v is at the highest possible level ℓ such that the edges incident from
lower levels is at least 4ℓ . An important property guaranteed by this invariant is that the mate of a vertex
while creating an epoch is independent of the update sequence associated with the epoch. The analysis
of our algorithm crucially exploits this property. However, the explicit maintenance of φv (j) imposes an
overhead of Θ(log n) in the update time. In order to achieve O(1) update time, Solomon [29] gets rid of the

28

maintenance of φv (j) by taking a lazy approach and a couple of new ideas. As a result, unfortunately, the
property of our algorithm no longer holds for the algorithm of Solomon [29] - indeed there is dependence
between the update sequence associated with an epoch created by a vertex and the random mate picked by
it. Solomon [29] makes use of a new concept called uninterrupted duration of an epoch that bypasses the
need of our property for the analysis. His analysis can be adapted to our algorithm as well and can be viewed
as the correct counterpart of Lemma 4.10 in [5]. However, our new analysis has its own merits since it is
based on an insightful property of our algorithm which we believe is of its own independent interest and
importance.
Subsequent to the publication of the [5] there has been interesting progress in the area of dynamic
matching with approximation less than 2 [7, 8, 24, 14], and dynamic weighted matching [3, 4, 14].
One of the technical challenges in theoretical computer science is to prove lower bounds for algorithmic
problems. Recently there has been some progress on proving conditional lower bounds for dynamic graph
algorithms [1, 15]. In the light of the lower bound presented by Abboud and Williams [1] based on Ω(n2 )
hardness of the 3SUM problem, it would be an interesting and challenging problem to see if c-approximate
maximum matching for c < 2 can be maintained in o(n) update time.

8 Acknowledgment
The possibility of an error in Lemma 4.10 of [5] was pointed out by Sayan Bhattacharya and Divyarthi
Mohan. The second author would like to thank both of them for discussions on the proof of the expectation
bound and the definition of B(v, i, k). In [5], we used 2i as the threshold for raising a vertex to level i.
The possibility of increasing this threshold from 2i to bi for any constant b without any impact on the time
complexity was observed by Shay Solomon [29]. We are thankful to him for this observation.

References
[1] Amir Abboud and Virginia Vassilevska Williams. Popular conjectures imply strong lower bounds for
dynamic problems. CoRR, abs/1402.0054, 2014.
[2] David J. Abraham, Robert W. Irving, Telikepalli Kavitha, and Kurt Mehlhorn. Popular matchings.
SIAM J. Comput., 37(4):1030–1045, 2007.
[3] Abhash Anand, Surender Baswana, Manoj Gupta, and Sandeep Sen. Maintaining approximate maximum weighted matching in fully dynamic graphs. In FSTTCS, pages 257–266, 2012.
[4] Abhash Anand, Surender Baswana, Manoj Gupta, and Sandeep Sen. Maintaining approximate maximum weighted matching in fully dynamic graphs. CoRR, abs/1207.3976, 2012.
[5] Surender Baswana, Manoj Gupta, and Sandeep Sen. Fully dynamic maximal matching in O(log n)
update time. SIAM J. Comput. (preliminary version appeared in FOCS 2011), 44(1):88–113, 2015.
[6] Surender Baswana, Sumeet Khurana, and Soumojit Sarkar. Fully dynamic randomized algorithms for
graph spanners. ACM Transactions on Algorithms, 8(4):35, 2012.
[7] Aaron Bernstein and Cliff Stein. Fully dynamic matching in bipartite graphs. In Automata, Languages,
and Programming - 42nd International Colloquium, ICALP 2015, Kyoto, Japan, July 6-10, 2015,
Proceedings, Part I, pages 167–179, 2015.

29

[8] Aaron Bernstein and Cliff Stein. Faster fully dynamic matchings with small approximation ratios.
In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2016, Arlington, VA, USA, January 10-12, 2016, pages 692–711, 2016.
[9] Sayan Bhattacharya, Monika Henzinger, and Danupon Nanongkai. New deterministic approximation
algorithms for fully dynamic matching. In Proceedings of the 48th Annual ACM SIGACT Symposium
on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 398–411, 2016.
[10] J. Edmonds. Paths, trees, and flowers. Canadian Journal of Mathematics, 17:449467, 1965.
[11] J. Edmonds and E. L. Johnson. Matching, euler tours, and the chinese postman. Mathematical Programming, 5:88–124, 1973.
[12] Harold N. Gabow and Robert Endre Tarjan. Faster scaling algorithms for general graph-matching
problems. J. ACM, 38(4):815–853, 1991.
[13] D. Gale and L. S. Shapley. College admissions and the stability of marriage. American Mathematical
Monthly, 69:9–14, 1962.
[14] Manoj Gupta and Richard Peng. Fully dynamic $(1+)$-approximate matchings. In 54th Annual IEEE
Symposium on Foundations of Computer Science, 2013.
[15] Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak. Unifying
and strengthening hardness for dynamic problems via the online matrix-vector multiplication conjecture. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC
2015, Portland, OR, USA, June 14-17, 2015, pages 21–30, 2015.
[16] Jacob Holm, Kristian de Lichtenberg, and Mikkel Thorup. Poly-logarithmic deterministic fullydynamic algorithms for connectivity, minimum spanning tree, 2-edge, and biconnectivity. J. ACM,
48(4):723–760, 2001.
[17] John E. Hopcroft and Richard M. Karp. An n5/2 algorithm for maximum matchings in bipartite graphs.
SIAM J. Comput., 2(4):225–231, 1973.
[18] Chien-Chung Huang and Telikepalli Kavitha. Efficient algorithms for maximum weight matchings in
general graphs with small edge weights. In SODA, pages 1400–1412, 2012.
[19] Jon Kleinberg and Eva Tardos. Algorithm Design. Addison Wesley, 2005.
[20] E. Lawler. Combinatorial Optimization: Networks and Matroids. Holt, Rinehart & Winston, Newyork,
1976.
[21] L. Lovasz and M.D. Plummer. Matching Theory. AMS Chelsea Publishing, North-Holland, AmsterdamNew York, 1986.
p
[22] Silvio Micali and Vijay V. Vazirani. An O( (|V |)|E|) algorithm for finding maximum matching in
general graphs. In FOCS, pages 17–27, 1980.
[23] Marcin Mucha and Piotr Sankowski. Maximum matchings via gaussian elimination. In FOCS, pages
248–255, 2004.
[24] Ofer Neiman and Shay Solomon. Deterministic algorithms for fully dynamic maximal matching.
CoRR, abs/1207.1277, 2012.
30

[25] Krzysztof Onak and Ronitt Rubinfeld. Maintaining a large matching and a small vertex cover. In
STOC, pages 457–464, 2010.
[26] Rasmus Pagh and Flemming Friche Rodler. Cuckoo hashing. J. Algorithms, 51(2):122–144, 2004.
[27] Liam Roditty and Uri Zwick. Improved dynamic reachability algorithms for directed graphs. SIAM J.
Comput., 37(5):1455–1471, 2008.
[28] Liam Roditty and Uri Zwick. Dynamic approximate all-pairs shortest paths in undirected graphs. SIAM
J. Comput., 41(3):670–683, 2012.
[29] Shay Solomon. Fully dynamic maximal matching in constant update time. CoRR, abs/1604.08491,
2016.
[30] Mikkel Thorup. Fully-dynamic min-cut. Combinatorica, 27(1):91–127, 2007.

9 Appendix
Proof of Lemma 5.15
Proof:
Pr[A ∩ C] = Pr[A ∩ (∪i Bi )]
X
Pr[A ∩ Bi ]
=

since Bi ’s are mutually exclusive

i

=

X
i

= ρ·

Pr[A | Bi ] · Pr[Bi ]

X

using the definition of conditional probability

Pr[Bi ]

i

= ρ · Pr[∪i Bi ] = ρ · Pr[C]

since Bi ’s are mutually exclusive

Hence Pr[A | C] = Pr[A ∩ C]/Pr[C] = ρ.



Proof of Lemma 5.16
Proof: Since A is independent of X, so for each x ∈ X,
Pr[A ∩ X = x] = Pr[A] · Pr[X = x]

(4)

Hence
Pr[A ∩ X = x]
Pr[A]
Pr[A] · Pr[X = x]
=
Pr[A]
= Pr[X = x]

Pr[X = x|A] =

using Equation 4



31

Proof of Lemma 3.1
The asymmetric random walk problem can be seen as a special case of the famous Gambler’s ruin
problem described as follows.
Gambler’s ruin problem.
There are two players who play a game that goes in rounds. Initially Player 1 has a capital of c units
and Player 2 has a capital of c′ units. Player 1 wins a round with probability p and loses with probability
q = 1 − p independent of the previous rounds. The winner of a round takes away one unit of the capital
from the opponent. The game ends when the capital of one of the players becomes 0.
The following lemma is well-known in many text books on probability theory. A concise and self
contained proof is available at the link : examplehttp://faculty.washington.edu/fm1/394/Materials/Gambler.pdf.
Lemma 9.1 In the Gambler’s ruin problem with p > q, the probability that Player 1 gets ruined is
′

1 − (p/q)c
1 − (p/q)c+c′
Let us now put an additional restriction in the problem: the total number of rounds allowed in the game
is L for a given number L < c′ . Notice that with this restriction Player 2 will never be ruined. As a result
the game will be over when Player 1 gets ruined or when L rounds are over. The probability of Player 1
getting ruined in this restricted Gambler’s problem is strictly less than the probability of Player 1 getting
ruined in the original Gambler’s problem described above. This is because there is a non-zero probability
that Player 1 may be ruined after performing more than L steps, and the restricted Gambler’s problem rules
out this possibility. Hence, using Lemma 9.1, it follows that for the restricted version of the Gambler’s ruin
problem with p > q, the probability that Player 1 gets ruined is less than
′

1 − (p/q)c
<
1 − (p/q)c+c′

 c
q
p

The restricted version of the Gambler’s ruin problem can be formulated as an asymmetric random walk
problem: The walk starts at location c units to the right of the origin. In each step, the particle moves one
unit to the right with probability p or one unit to the left with probability q = 1 − p indepependent of the
past moves. The walk terminates upon reaching either the origin or when it has performed L step. This
completes the proof of Lemma 3.1.

32

Collaborative Teaching in Large Classes of Computer Science Courses
Sanjay Goel, Suma Dawn, G. Dhanalekshmi, Hema N, Sandeep Kumar Singh, Sanchika Gupta, Taj Alam, Prashant Kaushik and Kashav Ajmera
Department of Computer Science Engineering and Information Technology Jaypee Institute of Information Technology Noida, India e-mail: {sanjay.goel, suma.dawn, dhanalekshmi.g, hema.n, sandeepk.singh, sanchika.gupta, taj.alam, prashant.kaushik, kashav.ajmera}@jiit.ac.in
Abstract--Collaborative teaching was applied by eight teachers for teaching nearly 700 students in four different sections of three different computer science courses with section strength varying from 120-240. Different forms of collaborative teaching were tried. Collaborative teaching at JIIT, Noida has turned out to be successful for large classes of the strength of 100 and above. Keywords--collaborative teaching; active teaching; computing education; information technology education; large class; computer science education; lecture

I.

INTRODUCTION

Worldwide there is a lot of emphasis on increasing the Gross Enrolment Ratio (GER) in higher education. Countries are making a lot of effort in this direction. In India, at the time of independence, the GER was only about 1%. It has increased from around 11% to approx. 20% in the last one decade. While there is a huge growth in the number of colleges and teachers, it has not kept pace with the growing number of students. The country-wide faculty-student ratio in the higher education at the time of independence was around 1:9. This has now reduced to around 1:23 [1]. Even at many IITs, this ratio varies between 1:15-1:20 and lecture class in a few courses at a few IITs can be as large as 250-400. With successive governments aiming to further increase the GER in higher education to around 50 % by 2030, the faculty-student ratio is further likely to deteriorate. While the countrywide student faculty ratio in higher education is deteriorating, the increasing emphasis on national/international university/college rankings, accreditation, and outcome based education are placing higher importance on quality and effectiveness of the educational process. The growth of engineering education has outpaced the general growth of higher education. It has gained a lot of popularity in the last few decades [2] and evermore so in India. Many reasons like societal mindset and financial stability have contributed to this. Further, an engineering education with its problem-solving approaches is also considered as a good program that prepares students for pursuing higher studies in other disciplines as well. The number of students enrolled in engineering courses has increased many folds in the last one

decade. This "massification of higher education" has occurred globally [3]. Tremendous growth of manpower requirement in IT industry has fuelled more admissions in popular branches of Computer Science, Information Technology and Electronics and Communication Engineering. This has raised the number of students being enrolled to such attractive courses leading to large class sizes. Hence, traditional models of engineering education are not sustainable anymore and innovative approaches have to be developed for all aspects of higher education. Some universities, academicians, and policy makers believe that Massive Open Online Courses (MOOCs) offer a solution to this problem. However, this paper is about an innovative approach for the conventional mode of face-to-face teaching in classroom. In India, regulatory bodies like UGC & AICTE prescribe norms for maintenance of standards for technical institutes. AICTE [4] prescribes a maximum class size of 60 for a section. However, this is not always feasible due to faculty, administrative or resource constraints. Hence, large classes are not an uncommon sight in engineering education. No doubt, learning in small group is good; however, having large classes may not be such a bane. Large classes have their pros and cons. Though there are varied outlooks regarding large class sizes in engineering education, nonetheless, it is also true that large classes are very challenging for ensuring a good quality of teaching-learning process. "Learn by listening and he may retain for a day or two, learn by watching and he may remember for some more time, but learn by doing and he will have gained knowledge for a lifetime" is a common saying. Such sentiments are now increasingly shaping education systems worldwide. The traditional way of delivering lectures is of limited educational value. Active and interactive lectures are found to be much more effective [5]. However, prodding many students into inclass learning activities is a herculean task. It is often a big challenge for many students to speak even before a class of 50 students. This wax museum-like aspect has, often, wreaked havoc even with otherwise wonderful courses. This paper discusses an experiment in collaborative teaching in large computer science classes. The experiment was conducted for teaching nearly 700 students in four

978-1-4673-7948-9/15/$31.00 2015 IEEE

different sections of three different computer science courses with section strength varying from 120-240. The next section of this paper gives an overview of the problems of large classes and also discusses the approaches of collaborative teaching as reported in the literature. Collaborative teaching was applied by eight teachers for teaching nearly seven hundred students in four different sections of three different computer science courses with section strength varying from 120 to 240. Different forms of collaborative teaching were tried. This is discussed in the third section. The fourth section briefly presents the results of this experiment and the papers concludes in the fifth section. II.
PROBLEM OF LARGE CLASS

surrounded by strangers and many prefer anonymity instead of chipping-in the discussion, doubt-clearing or even in a general question-answer session.  Appropriate pace and lecture content selection: In a large class, information dissemination itself may not be properly carried out; hence, engaging them in thinking becomes even more difficult. Manageable and reliable assessment: Assessment must be such that it arouses the students' higher-order thinking capabilities without overburdening the instructors with too much paper checking. Feedback coordination and management: Timely feedback can save the course from being a disaster.





Large classes can be seen as contrary to the objectives of university teaching "foster the growth of individuals, and encourage them in their individuality so that they become independent, creative, self-motivated, critical thinkers and learners" [6]. However, large classes have now gained larger acceptance to accommodate the burgeoning number of students who are able to enrol and complete their higher education. Nonetheless, class management and teaching time face significant negative impact, making it inept and dull for both the students and instructors [7]. Some problems faced in large classes, as reported in literature [7]-[10], [13], and [15] are as follows:  Large space and theatre-like setting: to physically accommodate a huge class, lecture rooms must be large and for visibility have a theatre-like projection facility. This reduces eye-to-eye contact between teachers and students. "Sage on the stage" problem: Many large classes tend to follow the "sage on the stage" style of teaching which is not considered an effective teaching style. Maintaining instructor enthusiasm: Consistent enthusiasm for the course of such large student-teacher ratio is a tasking job. Class management: Administrative tasks such as taking attendance, maintaining class decorum and discipline are at times difficult. Maintaining students' interest: Attention spans of students are difficult to maintain in large class where there may be many distractions. Poor discussion: The large strength of the class deters discussion that includes everyone. Declination in active learning: Active learning and class-participation by students and their engagement in the learning process is very important for deeper understanding of a subject. However, maintaining active engagement of students in a large class is difficult and demanding both for the instructors as well as the students. Student isolation and anonymity: With so many students of different socio-economic-academic backgrounds, students may feel as if they are

These and several other issues need to be sensitively addressed to promote a rewarding and fruitful teachinglearning experience in large classes. "The main goals for instructors teaching large classes, apart from delivering the course knowledge, are to make the class seem smaller than it is, encourage students to participate more, and make themselves accessible to the students" [7]. Many solutions to the numerous difficulties of teaching a large class have been reported in the literature. A. Changing Passive Learning to Active Learning For learning, instead of the size of the class, what should count is the quality of teaching-learning. Many problems, so encountered in large classes, can be reduced to manageable limits, if not erased altogether. In order to overcome the limitations of large classes the instructors and the students both need to be simultaneously active. Active teaching in large classes has been promoted through following approaches [10][13]:  Instructor competency and enthusiasm: In addition to the knowledge and experience with the subject, the instructor's enthusiasm in assisting the students and helping in their learning process is very important.  Organizational and teaching strategies: Proper communication of the subject matter is very difficult in a class size of, say, 100 or above. Hence, now using technology within the classroom has become a norm, not only in higher education but also at school level. This helps not only in delivering lecture using visual aids, but also encompasses using audience response system and communication [14].  Assignments: Graded and ungraded assignments may be supplemented with impromptu questions, surveys and participatory activities [12].  Personalizing teaching-learning space: The student should not feel isolated or anonymous within such large groups and this can be done by encouraging studentfaculty interaction, discussions and debates, encouraging questions, and sometimes giving personalized feedback too [28].  Encouraging more collaborative and cooperative learning environment: These not only involve learning









 



in cohesive groups but also being available for group discussions and giving feedback to the groups [18].  Collaborative Teaching: This involves having the assistance of teaching assistants and/or presenting the course through team-teaching. This will not only reduce the cognitive burden on an individual instructor but also enrich the learning in the course. B. Moving towards Collaborative Teaching As an idiom goes "Two heads are better than one", manya-times, two or even a small group of people working together will not only have more ideas but also implement these ideas in a better manner. Collaborative teaching involves collaboration by multiple teachers playing complementary roles [16]-[18]. It can also use a rotational model where collaborating teachers keep on changing their roles [19]. Collaborative teaching provides many pedagogical advantages over traditional teaching methodology. It helps in creating a dynamic and interactive learning environment [20]. The four predominant forms of collaborative teachings are supportive, parallel, complementary and team teaching [21]. In supportive form, the co-teacher(s) taking the supportive role provide one-to-one tutorial assistance when necessary, while the main teacher continues to lead the class. In parallel teaching, multiple teachers work with different groups of students in different subsections. It has eight variant  split classes, station teaching (learning centers), co-teachers rotate, each co-teacher teaching a different component of the lesson, cooperative group monitoring, experiment or lab monitoring, learning style focus, and supplementary instruction. In complementary teaching, the co-teacher does something to enhance the instruction through paraphrasing, model note taking, pre-teaching, etc. In the most sophisticated form of collaboration, team teaching, multiple teachers take responsibility for all of the students in the classroom with respect to planning the lesson as well as teaching and assessing the students. Instead of team teaching, we shall use the term synergetic teaching for this form of intensely collaborative teaching. There are many models to handle team taught courses which can fall in any of the four predominant forms of collaborative teaching at school as well as university level. 1) Collaborative Teaching at School Level In collaborative teaching at primary and secondary schools teachers have teamed up and integrated their approaches of teaching to make the teaching process more efficient and enjoyable for the students. Work in [22] has implemented the concept of collaborative teaching in Taiwan's primary and secondary schools for the course of Mathematics. In this model, co-teachers shared experiences and were involved in co-generative dialogue with each other. A review [23] accessed the effectiveness of interdisciplinary collaborative teaching in enhancing student achievement at Northwest Regional Educational Lab at Portland. It identified various areas which are positively affected by collaborative teaching - self-concept, happiness with school, attitude toward teachers, interest in subject matter,

sense of personal freedom, sense of influence on the school environment and self-reliance. Work in [24], [25] discussed an effective collaborative teaching using a special educator and a general educator to accommodate the needs of students with and without disabilities. 2) Collaborative Teaching at University Level Problems of large classes are usually handled using parallel teaching by dividing classes in several sections. Each section is handled by individual instructor. All instructors collaborate together from course design to final evaluation and grading [21]. In literature, most cases of collaborative teaching at University level have followed supportive [18], [20], [26]-[28] and synergetic forms [19], [26], [27], [29]-[33]. Complimentary teaching is evident in combination with other three forms [27], [33]. Authors in [28] experimented with supportive form of collaborative teaching model employed at a regional university of Australia for a large undergraduate marketing course. One instructor focused on lecture delivery and the other on assisting and monitoring students. Instructors swapped roles depending on the lecture activity. In another model, role of co-teacher was that of a careful observer and an exemplary student [18], [20]. As an exemplary student, co-teacher actively participated in discussions to create high level debates in class. Work in [29] showed synergetic form of collaborative teaching approach in interdisciplinary courses. Two instructors one from the Computer Science department and another from the Writing Program in the College of Humanities collaboratively taught senior software seminar course. An integrated framework with a structural paradigm was adopted in [31] wherein collaborative teaching was introduced as a language intervention in the course. Performance of students was evaluated before and after intervention. Course topics division based on individual preferences and lecture materials, was another model adopted in [32]. In "Rotational Model" discussed in [19], one instructor is always present every time and a series of instructors keep changing depending upon course topics that fall within their specialty. In another work [30], guest lecturers were used as part of collaborative teaching model to expose students' to a variety of topics from a different backgrounds and teaching styles. Another study was done in [26] on challenges faced in collaborative teaching activities and its effectiveness which identified six types of collaborative teaching structures like "One teach other observe, One teach other drift, parallel teaching, station teaching, alternative teaching and team teaching". Work in [27] adopted two different Mentor-mentee models. In first model, during first half, mentor taught the class and mentee observed, in the second half, mentee taught the class and mentor was observing. In second model, mentor taught the class up until the first examination with mentee observing. Subsequently, class was taught by the mentor or the mentee. C. Evaluation of Collaborative Teaching Models Collaborative teaching approaches have been evaluated both from student as well as teachers perspective.

1) In Terms of Student Perspective Collaborative teaching creates increased dialogue and participation of students in discussion of class. It leads to two or three level fair grading system for evaluation of students. Students have better understanding of the course and their connectivity with other related courses. Instructor's debate leads to understanding of different disciplines for students [19], [20]. Results from Stanford University [19] state that collaborative teaching increases student participation and improves student learning outcome. It also encourages students to have variety of perspective on a topic to make valuable contributions in class discussions. Collaborative teaching in interdisciplinary courses even enhances students' technical writing and communication skills with improvement in programming concepts [23], [29]. In case of physically challenged students, lowering the student-teacher ratio in co-taught classrooms offers more determined and individual instruction to students [24], [25]. Differences in lecturing style, quality of lecture contents, and the perceived lack of continuity and cohesiveness in lecture topics were negative aspects reported in collaborative teaching [30]. The critical success factor behind the team taught courses is the composition of a team [21], [28], [34] for better teaching and learning experience. 2) In Terms of Faculty Perspective Teamwork enhances the professional competence of the teachers and helps them to analyze things from different viewpoints [21]. Team teachers become expert learner and learn new approaches from their colleagues [27], [34]. Instructors develop compatible teaching style, spend more time in planning and coordinating various activities, e.g., selection of course topics, grading systems, presentation and delivery, and agreement on distribution of the course load [32]. Collaborative teaching gives instructors the opportunity "to teach in a different way, and to learn in a different way". It allows instructors to improve their academic skills and develop new topics for research [19]. The compatibility of the coteachers and the discrepancy between their approaches to teaching, their personal characteristics and effectiveness of collaborative teaching has overall effect on the team taught courses. The effective collaborative teaching in college can enhance the learning of new faculty [27], [33]. III.
COLLABORATIVE TEACHING AT JIIT

collaboration to teach approx. 700 students. The different collaboration styles of Supportive, Parallel, Complimentary and Synergetic teaching were applied in this collaborative teaching experiment. Number of lectures conducted in collaborative style in each section varied from 27 to 30. For each lecture, the collaborating teachers worked together before as well as after the class. It took around an hour for collaborative planning the lecture sequence and class activities and around 15 minutes after the class for review. A few examples of four forms of collaborative teaching in these three different computer science courses are given below: A. Supportive  In Data structure course, one teacher explained the concept of stacks and other one supported by providing the applications of stack in various problems. The applications included checking parenthesis matching, expression conversion like infix to postfix, infix to prefix, postfix to prefix and, postfix expression evaluation, etc.  In Microprocessor and microcontrollers course, one teacher gave practical demonstration of MASM I/O programming with examples. Based on this topic, problems were given to students. The other teachers interacted with students off the stage clearing their doubts and helped students in problem solving. In Microprocessor and microcontrollers course, 8086 addressing modes were presented by the main faculty. The supporting faculty intervened with questions that helped in clarifying the topic. Further, the topic was concluded by supporting faculty with mapping of logical address to physical address. In Microprocessor and microcontrollers course, the concept of Programmable Peripheral Interface (PPI) device 82C55 and its mode programming was taught by the main faculty, while supporting faculty provided real-life practical applications and demonstration of PPI modules with logic controllers to students.





This model was also applied for tasks such as system set-up and support, attendance record maintenance, maintaining the general class discipline and decorum B. Parallel  In Data structure course, the problems related to queues such as simulation of multiple queues and priority queues was given in class. The students were divided into two subsection and two teachers in parallel guided the subsections. The students in the different groups came up with different approaches for solving the same problem. Similarly, the Backtracking problems like Rat in a Maze and Eight Queens problems were discussed in parallel style of collaborative teaching.  In Microprocessor and microcontrollers course, delay generation using 8051 instructions for particular crystal frequency problem was given to two subsections of

The department of CSE & IT at JIIT has been using collaborative teaching in the form of multi-section parallel teaching and topic sharing in single section for the last several years. However, in the spring semester 2015, for the first time, after two weeks of the start of the semester, multiple teachers collaborated to conduct almost all lecture class of a few selected courses. The experiment was conducted in three different courses of B.Tech programmes. We have experimented in two out of seven sections of Data structure course with class strength of approximately 120 students each in the first year, Microprocessor and microcontrollers course with class strength of 240 students in the second year and Computer Games with class strength of 211 students in the final year. Eight teachers participated in this form of

students. One teacher supported each subsection. The first group to arrive at the correct solution was declared as the winner.  In Microprocessor and microcontrollers course, a simple problem of checking odd or even was given for different subsections with different logic like using LSB check, using shift or rotate instruction and division to check a remainder. One teacher supported each subsection. At the end of problem solving, it was shown that using division method to check odd or even is a bad approach, as it consumes many registers and complexity of problem also increases. Whereas using LSB check is a simpler approach as assembly language is well equipped for bit manipulations. This kind of comparative study helped the students to better understand and compare the programming concepts. In Computer games, the topic of testing and debugging required five lectures. For each of these lectures, mostly parallel and sometimes complementary and supportive teaching styles were adopted. While teaching in parallel mode, the class was divided into two subsections. Latter, multiple groups were formed for discussions which were assisted by both the instructors. At the start of the topic, testing and debugging methods were shown to the students. While one faculty demonstrated using the computer, the other faculty (on stage) explained the manner in which testing and debugging was needed to be implemented. After demonstration, both the instructors adopted complementary teaching style for enhancing the learning by encouraging more discussions amongst students. The topic of game optimization was discussed for over 3 classes and the content included theoretical concepts as well as the practical example of the memory and CPU usage while running the game. For this topic, parallel and synergetic lecturing styles were interleaved. While one instructor explained the generalized best practices for optimizations, the other was involved in showing how these may be implemented in the game modules to small groups of students. The students also pointed out the required optimization in the games that they usually play and also critiqued other students' games. Further, the CPU and memory-usage snapshots of some popular as well as student-developed games were shown and profiled in Unity game engine. The students could appreciate and pin-point their modules wherein major optimization was required. In total 5 games were profiled and optimized. The topic of game design pattern was conducted mostly as a hands-on experience in the class for making new game plays. For this, parallel teaching style was used as the class was divided into two subsections which were further divided to two groups. After explaining the design patterns in the class, the students were asked to modify the patterns to create new game plays. Each subsection was given a theme

within which the two groups were to follow two different game design patterns and both the instructors moved in the class to see how students are making new game plays. Hence, many spin off game plays were developed in the class itself. C. Complementary  In Data structure course, one teacher explained the concept of Graphs and its representation. To enhance this explanation, the other faculty explained the same concept in a different perspective. Both instructors supported their discussion in terms of different practical application such as representing friend's network as graph, transport system as graph etc. This gave more insight to the students.  In Microprocessor and microcontrollers course, first attempt to make students understand the operation of XLAT instruction is generally difficult. 8086 XLAT instruction was taught by one instructor. This was followed by a comparison with 8051 MOVC instruction (taught in an earlier class) by another faculty. This helped in developing a deeper and quicker understanding of it as they were already familiar with 8051. In Computer games, the topics like game development life cycles and game play design were taught in the complementary style as one instructor explained storyboarding while the other stressed on game play. In Computer games, 3D character animation was discussed for over 3 lectures, the complementary style of teaching was followed. The theoretical aspects including the 3D designing, transformations, interrelationship with 2D and rendering pipeline were introduced by one instructor, and subtopics such as kinematics and inverse kinematics and their implementation including animation designing were explained by the other instructor. While one instructor was teaching or demonstrating, the other was involved in minor problem solving with small groups of students.











D. Synergetic  In Computer games the topic of game development was discussed following the synergetic model. The discussions included Unity3D game engine and various best practices for the development after the designing stage. These lectures were delivered in synergetic mode as both the instructors took the stage, and explained and demonstrated the game in a step-by-step manner. Both instructors also explained different aspects of the development tools.  Often, instead of "teaching" a topic, such as critiquing a game, and finding flaws within them, the instructors adopted a debate-rebuttal style which followed the synergetic format of teaching.

IV.

RESULTS

The impact of this experiment in collaborative teaching has been analyzed through reflections by 8 participating teachers as well as feedback by students through informal discussions and a structured survey at the end of the semester. A. Student's Perspective Initial reaction from the students was that of bewilderment as they had never experienced such form of teaching before. Having two or more teachers teaching them simultaneously was not only exciting but also confusing at times. This required explanation on the part of instructors. As the semester progressed, the students also accepted this form of teaching to a large extent, if not whole heartedly. Discussions with students revealed that they felt benefited from the multiple perspectives of more than one teacher on the same topic. Discussions between the teachers and amongst the students themselves enriched the learning to a greater extent. It also improved the faculty accessibility, resulting in enhanced doubt clearing. Further, at the end of the semester, a questionnaire was administered to get student's reaction to the collaborative teaching. The questionnaire asked them to compare the coteaching class to typical non-co-taught class of approximately same student strength with respective to various parameters such as - their subject-specific and generic learning, problemsolving ability, soft skills, level of in-class and out-of-class learning, participation in collaborative activities and interaction with faculty. These questions were answered using 5 options. A large percentage of students reported a very positive feedback to the collaborative teaching style on various parameters. The results are shown in Table I. From Table I, it can be inferred that, there was a greater appreciation of collaborative teaching in Computer games and Data structures. 92% of the students in Computer games and 71% in Data structure course felt an increase in the learning from the collaborative teaching. In all the three courses only a small fraction of students, 5%, 20% and 0% in Data structures, and Microprocessor and microcontroller, and Computer Games respectively felt that collaborative teaching had a negative impact on learning. The students found the integration of diverse learning from two different teachers useful. The benefits reported by students are as follows:    More opportunities for clarification of doubts, Enhanced learning and understanding due to different approaches used for problem solving. If any part of the topic-content was not highlighted by one instructor, the other helped in identification of such minor lapses and rectified it. Classes were conducted even if a particular faculty was absent or busy as the others could easily take-over for that particular duration. Decreased monotony of classes as students interacted from two or more different instructors simultaneously.

also showed enthusiasm towards having collaborative teaching classes in future.
TABLE I.

more

such

STUDENT FEEDBACK ON COLLABORATIVE TEACHING ON VARIOUS PARAMETERS (IN %) Parameters Students (%) who selected the options in: 1. Data Structures 2. [Microcontroller & Microprocessor] 3. (Computer Games) Definitely Almost Definitely Increased/ same Decreased/ Increased Decreased 52 45 3 [31] [35] [35] (88) (14) (0) 45 48 7 [22] [58] [20] (74) (23) (3) 7 33 60 [27] [62] [11] (19) (3) (78) 48 45 5 [27] [47] [25] (72) (22) (6) 52 38 10 [11] [53] [36] (73) (22) (5) 50 40 10 [27] [40] [33] (72) (23) (5) 62 32 6 [36] [44] [20] (76) (20) (4) 24 71 5 [49] [31] [20] (8) (92) (0)

Subject specific technical learning Generic technical learning Generic Problem Solving learning Generic soft skills learning Level of student participation in class In-class student collaborative activities Students faculty interaction in & out of class Overall assessment of the co-teaching experience

B. Faculty Reaction Initially the collaborative teachers themselves were little uncomfortable, which might have contributed to the confusion with students. However, not only this gave an opportunity to build a rapport among them but also helped them in understanding a different point-of-view for a particular topic and gain more insight into it. Also, there was an increase in the learning activity experienced by the co-teachers that helped them to grow, reflect and deepen content understanding and improve its subsequent delivery, while also providing students with a variety of effective instructional methods. The collaborative teaching model though has several advantages over single teacher taught courses, it also has many challenges and pitfalls that need to be taken care of during the design of the course and lecture session. In large classes, the form of interactivity has to be planned before, so that students are not distracted. Even the supporting teacher has to correlate to topic being taught and the terminology being used by the other faculty, so that the students are not confused with the use of dual terminology. The students sometimes faced problem in terms of switching of teacher. This may result in break of flow of delivering lecture by the instructors. Thus certain challenges that need more attention include pre-synchronization of the content delivery and smooth transition between instructors and topics.





However, the overall feedback showed an increase in active participation and enriched learning experience. The students

Since the experiment was being performed for the first time, it was a great learning experience in terms of subject learning as well interpersonal understanding. Though, sometimes smooth and well-coordinated transition during lecture delivery was not always without initial discontinuities, the collaborative teachers could mutually assist, harmonize and enrich their teaching and learning. V.
CONCULSIONS AND FUTURE WORK

[15] [16] [17] [18]

Despite of some initial hesitation shown by the collaborative teachers and the students, this experiment has turned out to be successful for large classes of the strength of 100 and above. From the survey it is inferred that students have felt greatly benefited from this collaborative teaching activity and want it to be repeated in other courses as well. As a future work, the department has planned to repeat this experiment in 7 different courses undergraduate as well as graduate level courses involving more than 25 faculty members and over 1,500 students in the coming fall semester. REFERENCES
[1] [2] "All India Survey on Higher Education", Dept. of Higher Education, Ministry of Human Resource Development, Government of India, 2013. Benbow, J., Mizrachi, A., Oliver, D., Said-Moshiro, L., "Large Class Size in the Developing World: What Do We Know and What Can We Do?" American Institute for Research: Educational Quality Improvement Program, 2009. http://www.equip123.net/docs/E1-Large Classrooms.pdf Mohamedbhai, G., "The effect of massification on higher education in Africa". http://www2.aau.org/whge/scm/meetings/mai08/adea/ study_massification.pdf AICTE Approval Process Handbook, (2013-2014).All India Council of Technical Education, New Delhi, India. http://www.aicteindia.org/downloads/ performance.pdf Goel, S., "Do Engineering Faculty Know What's Broken?", The National Teaching & Learning Forum, Vol. 15 No. 2, pp 1-5, James Rhem & Associates, USA,2006. Gedalog, A. J., "Teaching Large Classes", Focus on University Teaching and Learning, Vol 8, No. 3, May 1999. "Large Classes: A Teaching Guide: Personalizing the Large Class", Centre for Teaching Excellence, University of Maryland, 2008. http://www/cte.umd.edu/library/teachinhLargeClass/guide/ch4.html "Teaching and Assessment in Large Classes", Teaching and Educational Development Institute, The University of Queensland Australia, 2001. Aagard, H., Bowen, K., Olesova, L., "Hotseat: Opening the Backchannel in Large Lectures", Educause Qarterly, 2010. Markwell, D., "Improving Teaching and Learning in Universities", Business/ Higher Education Round Table, Vol. 18, pp: 1-5, Nov 2003. CADQ guide: Teaching large groups. http://www.ntu.ac.uk/adq/document_uploads/teaching/137815.pdf "Strategies for Teaching Large Undergraduate Classes", Hanover Research, 2010. Ives, S.M., "A Survival Handbook for Teaching Large Classes", Faculty Center for Teaching, UNC Charlotte. Oliver, R., "Using mobile technologies to support learning in large on campus university classes", ICT: Providing choices for learners and

[19] [20] [21]

[22]

[23]

[24]

[25]

[3]

[26] [27]

[4]

[5]

[28]

[6] [7]

[29]

[30]

[8] [9] [10] [11] [12] [13] [14]

[31]

[32]

[33]

[34]

learning: Proceedings `ascilite' Singapore, 2007. http://www.ascilite.org.au/conferences/singapore07/procs/oliver.pdf Burnett, L., Krause, Kerri-Lee., "GIHE Good Practice Guide on Teaching Large Classes"Griffith University. www.griffith.edu.au/gihe Sharpe Co-Teaching Manual MSDE Grant, Towson University, pp 1-26, 2012. Shumway, Larry K., Gallo, G., Dickson, S., Gibbs, J., "Co-Teaching Handbook", Utah State Office of Education, pp 1-39, 2011. Anderson, Rebecca S., Speck, Bruce W., "On what a difference a team makes: why team teaching makes a difference", Teaching and Teacher Education,Vol. 14, No. 7, pp. 671-686, 1998. "Speaking of teaching: The centre for teaching and learning", Stanford University, Newsletter, Vol.16, No.1, pp 1-4, 2006. Leavitt, Melissa C., "Team Teaching: Benefits and challenges", Stanford Fall 2006 Newsletter, Vol. 16, No. 1, pp 1-3, 2006. Thousand, Jacqueline S., Villa, Richard A, Nevin, Ann I., "Many Faces of Collaborative Planning and Teaching" Theory into Practice, Vol. 4, No. 3, pp 239248, 2006. Jang, Syh-Jong, "Research on the effects of team teaching Upon two secondary school teachers",Chung-Yuan Christian University, Taiwan, Educational Research, Vol. 48, No. 2, pp. 177  194, 2006. Kathlee, C., "Effects of Interdisciplinary Team Teaching. Research Synthesis", Northwest Regional Educational Lab., Portland, Oreg, pp 116, 1982. Magiera, K., Simmons, R., "A Co-Teaching Model: A Response to Students with Disabilities and Their Performance on NYS State University of New York", SAANYS Journal, Vol. 34, No. 2, pp 1-5, 2005. Malian, I., McRae, E., "Co-Teaching Beliefs to Support Inclusive Education: Survey of Relationships between General and Special Educators in Inclusive Classes", Electronic Journal for Inclusive Education Vol. 2, No. 6, pp 1-19, 2010. "The Effectiveness of the Co-Teaching Model", Hanover Research, Washingtom DC, pp.1-20, 2012. Carpenter, J., Meng, D., Ponder, N., Schroder, B., "Team teaching merged sections as a way of mentoring faculty", Frontiers in Education Conference, pp F3F-1-6, 2000. Yanamandram, V., Noble, G., "Student experiences and perceptions of team-teaching in a large undergraduate class" Journal of University Teaching and Learning Practice, Vol. 3, No. 1, pp: 4966, 2006. Louise, R., Hollaar, L., "Co-Teaching Engineering and Writing: Learning about Programming, Teamwork, and Communication." Issues in Integrative Studies Vol. 1, No. 15, pp 125-147, 1997. Hanusch, F., Obijiofor, L., Volcic, Z., "Theoretical and Practical Issues in Team Teaching: Large Undergraduate Class", International Journal of Teaching and Learning in Higher Education, Volume 21, Number 1, pp 66-74, 2009. Kamai, R., Badaki, J. V., "Structuring Team Teaching to Enhance Teaching and Learning of Literature-in English and English Language in Secondary Schools", Journal of Education and Practice , Vol 3, No 13, pp 127-133, 2012 Azemi, A., D'Imperio, N., "New approach to teaching an introductory computer science course," Frontiers in Education Conference , pp S2G1-6, 2011. Riegner, Dawn E., " Team Teaching in College", Center for Teaching Excellence, United States Military Academy, West Point, NY, pp 1-6, 2007. Conderman, G., "Middle school co-teaching: Effective Practices and Student reflection", Middle school Journal, pp 25-31, 201

SafeDrive: An Autonomous Driver Safety Application in Aware Cities
Koosha Sadeghi , Ayan Banerjee , Javad Sohankar , and Sandeep K.S. Gupta
iMPACT Lab, CIDSE, Arizona State University Tempe, Arizona, USA Email: { ssadegh4,  abanerj3,  j.sohankar,  sandeep.gupta}@asu.edu

Abstract--Commercially available wearables and apps that convert mobile devices into data collection hubs can be used to implement smart applications in aware cities. In this paper, we consider wearable devices on various human users as a networked cluster of computing power and information source in an Internet-of-People architecture. Applications can be developed to perform computation on this data and gain group level aggregate inferences and provide feedback. We propose "SafeDrive", an autonomous transportation application, that estimates mental fatigue of a driver using brain sensors, predicts collision probability by fusing car parameters with driver mental state, and issues feedback just in time to avoid accidents. However, significant challenges exist with respect to ensuring safety, accurate context computation and real-time operation, and sustainability, resource efficiency. In this regard, we present the "HumaNet" framework that consists of a middleware installed in mobile devices for developing aware cities applications. HumaNet applies modelbased requirements checking approach for off-line analysis and optimization of a given application. Further, it applies contextbased requirements checking to decide how to use different types of computation resources to satisfy safety and sustainability requirements in run-time.

I. I NTRODUCTION Advances in the Internet of Things (IoT) domain is making city infrastructure smarter, by incorporating context awareness in different domains including transportation, health, education, and energy management. Context awareness in aware cities is obtained through collection of data from the environment including its inhabitants using sensors, inferring knowledge from the data, and using that knowledge to provide automated services [1], [2]. To this effect, deployment of Internet enabled wearable devices on human body such as the Apple watch, the Google glass, and smartphone has converted the human user to a human server, a rich repository of environmental, and physiological data [3][5]. The IoT concept has morphed into the Internet-of-People (IoP) that enables sharing of the knowledge through community networking of human servers for collaborative applications such as traffic condition reporting or crowd driven emergency response [6]. In this paper, we propose SafeDrive, a context aware transportation safety application, that uses community monitoring to determine mental fatigue levels of drivers and provides them feedback on impending collisions. SafeDrive assumes that each driver on the road is equipped with a brain sensor that connects to their smartphone. The smartphone is also a
iMPACT Lab URL: https://impact.asu.edu/

hub for other car sensors such as the wheel speed, the rear view, or blind spot cameras. The smartphone sends the mental fatigue level along with car speed and neighboring car details to a central server. The server computes impending collision based on the mental fatigue levels of other drivers on the road as well as their current driving status and alerts individuals using alarms or warning messages. SafeDrive makes city transportation context aware by combining knowledge from different domains such as mental states of drivers and mechanical states of a vehicle, transferring them to different computational and decision making hubs in the IoP infrastructure, and processing them in a collaborative environment to derive community based inferences, which influence decision making. For SafeDrive to be useful, it needs to satisfy three main requirements: a) the mental state detection system has to be accurate, b) the feedback on collision has to be real-time, and c) it needs to be energy efficient on smartphone. Previous works on driver drowsiness detection systems are based on information from an individual (i.e. driver). In [7], facial expression and photoplethysmograph (PPG) data are combined to measure driver alertness. They reach 83% and 94% true detection rates using two features (i.e. percentage of eye closure and PPG power spectrum density) and two additional features (i.e. average eye closer speed and heart rate variability), respectively. In [8], set of medical grade sensors are used to record electroencephalogram (EEG), electrooculogram (EOG), and electrocardiogram (ECG) signals to detect drowsiness with 95-97% accuracy. In [9], skin conductance and oximetry pulse are monitored to detect mental fatigue and they reach around 93% accuracy using neural network approach. In [10], a four channel wireless EEG sensor is used to detect drowsiness and an accuracy of 74.6% is obtained. At last, in [11], they use Neurosky [12] EEG sensor and support vector machine algorithm for drowsiness detection and reach 88.8% accuracy in the best case. These works suffer from at least one of these four limitations: 1) usage of medical grade sensors with tedious setup procedure (reducing usability), 2) combination of several types of sensors and features (reducing usability), 3) computationally intensive processing (hampering real-time operation and energy efficiency), and 4) low performance (potentially undermining safety). These shortcomings reduce usage feasibility in on-line mobile settings. In SafeDrive, information from all drivers on a route is collected and processed to improve the performance. Applying

Context Information Decisions
Car Conditions

Transportation Control System (Cloud Server)

Mental State

Warning Message

Fig. 1: Operation of SafeDrive application using HumaNet.

IoP approach in developing SafeDrive makes it feasible to: 1) exploit easy-to-use and commercially available sensors such as Neurosky or Emotiv [13] (limited number of channels and low signal-to-noise ratio of these headsets are challenges toward reaching high accuracy), 2) use just one type of signal features (i.e. variations in the frequency domain), 3) deploy a light weight mental fatigue detection algorithm (based on a Markov chain model), and finally, 4) obtain an average of 91% accuracy to detect mental fatigue (in five trials of one subject). These points make SafeDrive usable as well as accurate. It is noteworthy that evaluation on a larger pool of subjects is required for real-world usages. To enable safety and sustainability assured IoP based implementation of SafeDrive, we propose HumaNet. It is a framework that applies model-based [14] and context-based [15] requirements checking to ensure safe and sustainable operation of aware cities apps such as SafeDrive. In model-based checking, HumaNet runs models of the app to determine its specifications (e.g. response time and power consumption) off-line. Model-based checking also indicates optimized configuration of the app to be executed with high accuracy, real-time, and energy efficient way. Off-line monitoring may not be sufficient by its own to guarantee safety and sustainability for the running app. So, context-based checking monitors execution of the app on run-time and according to the context decides whether to offload the computations on the central server to satisfy requirements. To implement SafeDrive app based on HumaNet, an application specific middleware is downloaded into each participating smartphone. In model-based checking, temporal logic is used to optimize the execution of SafeDrive. Afterward, in context-based checking, SafeDrive is monitored in run-time to check if safety and sustainability requirements are met. Otherwise, the computation is offloaded on a central cloud server. II. S AFE D RIVE : D RIVER S AFETY A PPLICATION As shown in Figure 1, two types of information from each driver are sent to the transportation control system. The first

type is driver's mental state (i.e. alert or mentally fatigue) and the second type includes car conditions (i.e. its speed and location). The control system server processes the collected information, and based on the analysis results, an appropriate warning feedback will be sent to driver. In our system, for triggering an alarm tone, we consider two scenarios, 1) an individual mental fatigue level is lower than a threshold, and 2) a fatigue driver is nearby or probable collision is predicted. In this sense, as seen in Figure 2, a wearable device on a human user has to provide three interfaces: a) an interface to connect to available sensors and create databases for each of them, b) a platform to execute application executables, and c) a webserver that can be accessed through the Internet and hosts the raw data or the output of the applications executed in the platform. SafeDrive app executables (Figure 2) consist of two parts: 1) client side that executes a mental fatigue detection algorithm (for the first scenario), and 2) server side that receives mental states from client side, runs collision detection algorithm, and sends back feedback messages to client side (for the second scenario). On the client side, the EEG sensors capture signals and send them to the smartphone. The collected data is processed and the alertness of the driver is determined according to the preset threshold. Recognition of fatigue state will trigger an alarm tone. Furthermore, her mental state is sent to the central control system (i.e. server side app) along with her car conditions. In the server side, the transportation control system sends a warning message to a driver in case of the presence of a fatigue driver nearby or prediction of a probable collision in her vicinity. In sections IV-A1 and IV-B, we describe the details of measuring mental fatigue level and detecting probable collision, respectively. To measure the mental fatigue level of a driver, we need to train our algorithm to determine a threshold before using the app during driving

Central Cloud Server

SafeDrive Server Database

Data Aggregator with Dynamic Domain Name Service

HumaNet
Web Server

Web Server
"Device 2" SafeDrive Client Database

Web Server
"Device n" SafeDrive Client Database

Wearable Smart Devices

"Device 1" SafeDrive Client Database

Sensors
Emotiv Neurosky

Speed & Location

Fig. 2: The autonomous driver safety system model.

App
Code

HumaNet

Context-based Requirements Checking
Context and Resource Manager (CRM)

Execution Manager

Data

Model-based Requirements Checking

Data Collection and Update
Finding the best Configuration

Offload Manager

Resource Communication Manager

Context

Fig. 3: Components of HumaNet architecture.

(Section IV-A2). The threshold will be used later to indicate whether the individual is mentally fatigue while driving. III. H UMA N ET A RCHITECTURE HumaNet provides an IoP framework that facilitates the communication and data aggregation among different nodes (apps and devices) for developing safe and sustainable apps. In this sense, every participating smartphone installs the middleware that runs in background. Aggregated data from group of people can be exploited to develop apps based on community inferences. On the other hand, to ensure safety and sustainability requirements, off-line model-based and run-time context-based checking are provided as seen in Figure 3. A. Model-Based Requirements Checking In model-based checking, models of a given app are developed and tested to analyze and optimize performance considering accuracy, response time, and energy efficiency requirements. Behavioral models are formed and studied to avoid unpredicted states of the app. Alongside, for optimization, the test results are assessed to improve the app configurations (e.g. sensors, algorithms, and platforms) through iterations. B. Context-Based Requirements Checking In this component, the IoP including sensors, apps, smartphones, and servers are monitored on-the-fly. The monitored data is analyzed to make system level decisions toward ensuring the requirements. Thus, HumaNet applies computational offloading (to a central server or smartphones in vicinity), controlling sampling rate of sensing modules, prioritizing execution of various apps, or adjusting trade-offs among accuracy, latency, and energy. For instance, the specifications (e.g. memory, workload, and battery level) of the candidate devices are checked to choose the best match for offloading. The functionalities for offloading are described below: 1) Context and Resource Manager (CRM): The CRM module runs periodically within the device to ensure that the application meets specific context and resource requirements by sensing the context as well as the resources. The mobility of users, other running apps, and user's state can change the context and resource values of the mobile device. Based on the sensor information, the device context values are evaluated against the context and resource requirement of the app.

2) Execution Manager: This module manages the execution of the code. It downloads the code and data blocks assigned to it. The code is then run which takes input as the downloaded data and results are generated. 3) Offload Manager: Offload manager takes the inputs from the CRM to assess if the device currently meets the context specifications. In case the required context does not exist, the task is offloaded. Context and requirement specifications of the application tasks are considered to determine the type of task to be offloaded to the other devices. 4) Communication Manager: This module potentially offloads tasks to nearby available devices if needed. IV. S AFE D RIVE I MPLEMENTATION As mentioned in Section II, smartphone has a foreground application (SafeDrive client side) that computes mental fatigue levels and the collision detection algorithm (SafeDrive server side) is executed on the central sever. The HumaNet on the background is equipped with a timed automata model of the entire execution of SafeDrive. On estimation of a possible collision the time automata model is used to verify whether sending a warning message will lead to a safe condition. Using the timed automata model ensure that the driver reaches the desired mental alertness state much before the reaction time to the impending accident. A. EEG-Based Mental Fatigue Detector Application Electroencephalography (EEG) is the most commonly used technique for real-time analysis of brain signals to detect mental fatigue. The process of falling asleep is divided into three states, known as Awake, Non-Rapid Eye Movement sleep (NREM), and Rapid Eye Movement sleep (REM) [16], [17]. In each stage, EEG signals divulge specific characteristics. Table I lists main frequency bands extracted from EEG signals and their corresponding cognitive state. Mental fatigue can be recognized in the earliest stage of NREM. This stage coincides with diminution in power changes in alpha frequency band and simultaneously power raise in alpha and theta bands [16], [18]. 1) Mental Fatigue Detection Algorithm: There are many algorithms to detect mental fatigue from EEG datasets including Mean Comparison Test (MCT) and Thresholding [19], Principal Component Analysis and Linear Regression Model [10], F-measure (the harmonic mean of precision and recall) [20], and support vector machine [11]. However, these algorithms work best on a desktop setting and require compute intensive operations. In our application, we use the Markov chain decision process, which according to our test results is computationally less expensive and has comparable accuracy. TABLE I: EEG signals frequency bands and cognitive states.
Frequency Band Delta (0.5-4 Hz) Theta (4-8 Hz) Alpha (8-13 Hz) Beta (13-30 Hz) Gamma (30-100 Hz) Cognitive State [16] Sleep Activity Attention Level Relaxation at decreased Attention Levels Active Concentration and Alertness State Perception

Research on mental fatigue detection shows that decrease in P /P and increase in (P + P )/P parameters expose mental fatigue clearly, where P , P , and P indicate power level in Alpha, Beta, and Theta bands, respectively [18]. In our work, the Fast Fourier Transform (FFT) is performed on raw EEG signals to transform them from time to frequency domain. The signal amplitudes in the frequency domain are used to compute the average EEG powers P , P , and P over 14 channels for Emotiv and one channel for MindWave. FFT is applied on every second of signal to update P /P and (P + P )/P . Two thresholds (Tl , Th ) are determined which are proportional to the average value of P /P and (P + P )/P in alert state, respectively. The threshold calculation is described in Section IV-A2. We then employ a two-state Markov chain model (Figure 4) to compute the steady state probabilities (0 , 1 ) for alert and fatigue states. The model parameters are defined as follow:
  d0,0   d 0,1 d1,1    d1,0 : : : : P /P P /P P /P P /P >   > Tl (P + P )/P < Th in State 0 Tl &&(P + P )/P  Th in State 0 Tl &&(P + P )/P  Th in State 1 Tl (P + P )/P < Th in State 1 (1)

Alert

Fatigue

Fig. 4: The two-state Markov chain model. D0 and D1 represent the sums d0,0 + d0,1 and d1,0 + d1,1 , respectively. algorithm (Section IV-A1) can determine mental state (i.e. alert or fatigue) of the subject. B. Vehicle Collision Detection Algorithm In this algorithm (i.e. our previous work [24]), we assume that the transportation control system (Figure 1) can monitor the car parameters of a large number of cars on a highway or at least the speed of the vehicle precede and follow the vehicle whose mental fatigue is being monitored. To predict the motion of the concerned vehicle, lateral and longitudinal control algorithms [25] can be used. The lateral control algorithm generates steering angle (i.e lateral control angle) based on the current position and the next destination over a short amount of time as given by the following equation:
 = arctan[2l(3y - xtan )/x ],
2

0 and 1 are updated every second, according to the parameters derived from the last 30s of data recording. As soon as 1 exceeds beyond 50%, the condition of the user is interpreted as a mental fatigue state. When the fatigue state is detected, the application will make an alarm tone. 2) Threshold Setting and Ground Truth: We need to set thresholds accurately to detect mental fatigue in a timely manner and avoid false alarms as much as possible. According to sleep studies, human mental fatigue generally appears late at night and impairs performance [21]. Subsequently, in fatigue detection research, the most common way to determine thresholds on alpha, beta, and theta is through experiments in different times of a day [22], [23]. In these studies, they assume driver is alert in the morning and mentally fatigue late at night as their ground truth. We also take a similar approach. Our experiment has two phases: training and testing. In the training phase, we need brain signal samples of a subject once she is completely alert and the other time when she feels drowsy. So, we asked the subject to wear the headset (e.g. Neurosky MindWave or Emotiv) for two 30-minute sessions; one in the morning and the other one late at night (signals are recorded with 512 Hz sampling rate). Signals from the morning session, are considered as alert state or baseline signals and vice versa. We applied MCT on the mean values of ratio indices P /P and (P + P )/P in each alert and mental fatigue state to set the thresholds. As a result, at least about 25% reduction in P /P value and about 10% growth in (P + P )/P value (compared to these values in alert state) indicates mental fatigue state. In the testing phase, the subject puts on the headset and run the mobile application in five trials on different days. Each trial has two 30-minute sessions, one in the morning and the other one late at night. The headset captures EEG signals and the smartphone monitors, and processes them real-time. According to the threshold from the training phase, mental fatigue detection

(2)

where  is the steering angle, l is the wheel base of the vehicle, (x, y ) is the next way point, and  is the heading angle with respect to the road. The longitudinal control algorithm generates speed based on the preceding and following vehicle's speed as per the equation given below:
vr = vp + k1(vp - vf ) + k2(Lr - Lm ) (3)

where vr is the speed of the AV, vp and vf are the speed of the preceding vehicle and following vehicle respectively, Lr is the minimum longitudinal distance between two vehicles, Lm is the measured inter vehicular distance, k 1 = m1 Lr - Lm / Lr , k 2 = m2k 1, and m1 and m2 are control gains. A collision can be detected whenever either a) vr - vp > (xr - xp )2 + (yr - yp )2 / , where (xr , yr ) and (xp , yp ) are the positions of the vehicle and the preceding one, respectively and  is a small amount of time that is less than the typical reaction time of human drivers, road conditions, and speed limits. b) vf - vr > (xf - xr )2 + (yf - yr )2 / , where (xr , yr ) is the current position of the following vehicle, or c) (x - xp )2 + (y - yp )2  or (x - xf )2 + (y - yf )2  , where is a parameter determined by the road conditions and highway speed limits. C. Modeling Real-Time Safety Properties In the SafeDrive application, temporal constraints can limit the choices of mental fatigue detection algorithm that can be used in the wearable devices. The computation discussed in Section IV-B can accurately predict imminent collision 6s in

the future. Within this time interval the smartphone has to compute mental fatigue levels and based on the thresholds, the smartphone will decide to set the alarm. The alarm has to be set at a time such that the driver has enough time to maneuver out of the dangerous situation. The reaction time of a driver in highway with a speed limit of 65 mph is around 2-4s [26], [27]. This leaves the mental fatigue application only 2s to make a decision. Figure 5 shows a Finite State Automata (FSA) representation of the SafeDrive application. The FSA has states and transitions between them. Each transition operates on some input x = xi and assigns an output y := yi and the transition condition is denoted by x = xi /y := yi . The FSA for SafeDrive app has five states, four inputs (X , Z , T , and P ), and two outputs (Y and C ) as described in Figure 5. The FSA in each vehicle starts in the state E. When a car enters the danger zone X = 1, it sets Y = 0 and sends baseline EEG activity to the server and the FSA transits to state L. In the state L if there is an threshold update request then T = 1 and the FSA goes to state U and after the update comes back to L. If the collision detection algorithm detects collision then Z = 1 and the FSA goes to the state D to invoke the driver drowsiness alertness algorithm. If the driver is alert then P = 0 and Y = 1 since the driver sends the drowsiness levels to the server. If the driver is drowsy then the FSA goes to alarm (A) state and sets Y = 1. In the alarm state if there is a collision then C = 1 and updated to the server. Else when the driver gets out of the danger zone, it updates the server with the values for Y and C and exits. Given this FSA specification real-time properties can be specified using a combination of temporal logic and timer clock in the FSA. The temporal properties include: a) update requirement: after every new update threshold message from the client the brain server has to immediately update its software, and b) collision avoidance window: since the collision predictor can only predict 6s in future, and driver reaction time in the worst case is 4s, the entire process including collision data availability, mental fatigue detection, and alarm setting has to be done within a window of 2s. In Linear Temporal Logic (LTL), a proposition x = y denotes the logical function x  (x  y ). Some of the key operators used in this paper are , , and  . The operator  applied on a proposition M denotes that the proposition is true for any sequence or subsequence of states in the finite state automata. This effectively means that it is true for each state of the FSA. The operator  applied on a proposition M denotes that the proposition eventually be true for some time t > 0. The operator  applied on a proposition M denotes that the proposition is true in the next state. The proposition L, D, U , E , or A are true if the FSA is in the corresponding states. The update requirement can be expressed using the formulation (L  T = 1 = U ). The collision avoidance window can be expressed by introducing another time variable in the FSA. In each state of the FSA, we include the equation t = t +  , where  is the time resolution of the FSA. In the state L, we consider that whenever Z = 1

Input X  location input X = 1 when vehicle enters danger zone, = 0 when vehicle exits danger zone E L U Input Z  collision prediction input Z = 1 when server predicts a collision  =0/ = 0 when server predicts no collision  1, Z= 1/  = 0/  1 Input T  threshold update input T = 1 when threshold is updated A D = 0 when threshold is not updated Input P  drowsiness detection  =1/  1 P = 1 when drowsiness is detected = 0 when not drowsy E  Initial state when the vehicle enters the danger zone Output Y - communicate data to client L  Listening state when waiting for server to Y = 0, when sending baseline active respond with collision prediction alpha, beta and theta activity D  State to invoke execution of drowsiness detection application A  State to invoke drowsiness related alarms U  State to update threshold = 1, when sending drowsiness levels, Output C - collision status update to client C = 0, when no collision = 1, when collision

 = 1/ 0

=1/

Fig. 5: Finite state automata representation of the SafeDrive. we set t = 0.2s, which is the execution time of the prediction algorithm on the client side and the communication delay. Now the requirement is that there has to be a mental fatigue related alarm if P = 1 within t = 2s can be expressed as follows (L  Z = 1 = (XD  (P = 1 = (A  (t < 2.0s)))))

V. E VALUATION OF S AFE D RIVE We evaluate SafeDrive application implementation using HumaNet with respect to two metrics: a) real-time safety requirements, and b) energy and power consumption. A. Real-Time Safety Requirements For verifying the safety requirement, we need to consider accuracy and response time of our algorithm. 1) Accuracy: The True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) were calcuT P +T N lated for each trial, to obtain: Accuracy = T P +T N +F P +F N , TP TN Sensitivity = T P +F N , and Specificity = F P +T N as listed in Table II. In these formulas, true positive represents the number of alarms in fatigue state, and false negative is equal to number of alarm misses in fatigue state. On the other hand, true negative is the number of alertness detection in alert state, and false positive represents the number of alarms in alert state. Test results in Table II show that the application can detect mental fatigue with 91% accuracy using Emotiv before the user fall asleep and completely lose control of the vehicle. MindWave has just one electrode placed on the user's forehead. So, compared with to Emotiv, which uses 14 channels, more test sessions are required using MindWave to set up thresholds and reach the accuracy of Emotiv. 2) Response Time: We measured the end-to-end latency of the SafeDrive app. For Emotiv, the average end-to-end latency of the system is 2.031s with variance ( 2 ) = 0.142 and standard deviation ( ) = 0.377. For Neurosky, the average end-to-end latency of the system is 0.995s with  2 = 0.223 and  = 0.472. According to collision studies in highways (Section III-A), with these response times, the system can avoid accidents caused by mental fatigue of the driver and satisfy real-time expectations of the system.

TABLE II: The experimental results in five test trials.
Test Trial Accuracy Sensitivity Specificity 1 93% 86% 100% 2 92% 85% 100% 3 88% 76% 100% 4 91% 81% 99% 5 95% 90% 99% Average 91% 83% 99%

B. Energy and Power Consumption One of the key concerns in designing any practical, efficient, and sustainable systems is to keep the amount of energy and power consumption as low as possible. By using Emotiv, a rough estimation of battery usage shows that when the application is running on the phone, we have 2% more battery discharge. Power monitoring on the phone indicates that the running application consumes about 130 mW on average. Monitoring battery usage using MindWave for EEG recording shows a 1% increase when the application runs on the phone and consumes about 109 mW power on average. There is an inverse relation between power and latency. We can decrease power usage by reducing frequency and increasing response time. So, the trade off between power and time should be considered for real-time processes. VI. C ONCLUSIONS In this paper, we introduce HumaNet architecture for developing real-time driving assistant mobile applications for aware cities. HumaNet enables apps using domain specific knowledge, to rapidly be implemented in a community networking framework that is optimized for accuracy, response time, and energy efficiency. HumaNet allows apps to share data and hence reduces cost of physiological sensing. We implement a prototype version of HumaNet and show its usage for developing the SafeDrive application. SafeDrive application requires sharing of physiological data among individuals and provides accurate physiological feedback through auditory stimulation in real-time. In this sense, HumaNet platform applies modelbased and context-based optimization techniques to satisfy safety and energy efficiency requirements in SafeDrive. ACKNOWLEDGMENTS This work has been partly funded by CNS grant #1218505, IIS grant #1116385, and NIH grant #EB019202. R EFERENCES
[1] M. Naphade, G. Banavar, C. Harrison, J. Paraszczak, and R. Morris, "Smarter cities and their innovation challenges," Computer, vol. 44, no. 6, pp. 3239, 2011. [2] J. M. Schleicher, M. V ogler, C. Inzinger, and S. Dustdar, "Towards the internet of cities: A research roadmap for next-generation smart cities," in Proceedings of the ACM First International Workshop on Understanding the City with Urban Informatics. ACM, 2015, pp. 36. [3] A. Mostashari, F. Arnold, M. Maurer, and J. Wade, "Citizens as sensors: The cognitive city paradigm," in Emerging Technologies for a Smarter World, 8th International Conference & Expo on. IEEE, 2011, pp. 15. [4] K. S. Oskooyee, A. Banerjee, and S. K. S. Gupta, "Neuro movie theatre: A real-time internet-of-people based mobile application," in The 16th ACM International Workshop on Mobile Computing Systems and Applications. ACM, 2015. [5] J. Sohankar, K. Sadeghi, A. Banerjee, and S. K. S. Gupta, "E-bias: A pervasive eeg-based identification and authentication system," in Proceedings of the 11th ACM Symposium on QoS and Security for Wireless and Mobile Networks. ACM, 2015, pp. 165172.

[6] J. Yin, A. Lampert, M. Cameron, B. Robinson, and R. Power, "Using social media to enhance emergency situation awareness," IEEE Intelligent Systems, vol. 27, no. 6, pp. 5259, 2012. [7] B.-G. Lee and W.-Y. Chung, "Driver alertness monitoring using fusion of facial features and bio-signals," Sensors Journal, IEEE, vol. 12, no. 7, pp. 24162422, 2012. [8] R. N. Khushaba, S. Kodagoda, S. Lal, and G. Dissanayake, "Driver drowsiness classification using fuzzy wavelet-packet-based featureextraction algorithm," Biomedical Engineering, IEEE Transactions on, vol. 58, no. 1, pp. 121131, 2011. [9] M. M. Bundele and R. Banerjee, "Detection of fatigue of vehicular driver using skin conductance and oximetry pulse: a neural network approach," in Proceedings of the 11th International Conference on Information Integration and Web-based Applications & Services. ACM, 2009, pp. 739744. [10] C.-T. Lin and et. al, "Development of wireless brain computer interface with embedded multitask scheduling and its application on real-time driver's drowsiness detection and warning," Biomedical Engineering, IEEE Transactions on, vol. 55, no. 5, pp. 15821591, 2008. [11] I. Shin and et. al, "Development of drowsiness detection system with analyzing attention and meditation wave using support vector machine method," ISICO 2013, 2013. [12] "Neurosky body and mind quantified. neurosky.com." [13] A. Stopczynski, C. Stahlhut, J. E. Larsen, M. K. Petersen, and L. K. Hansen, "The smartphone brain scanner: a portable real-time neuroimaging system," PloS one, vol. 9, no. 2, 2014. [14] A. Banerjee, K. K. Venkatasubramanian, T. Mukherjee, and S. K. S. Gupta, "Ensuring safety, security, and sustainability of mission-critical cyberphysical systems," Proceedings of the IEEE, vol. 100, no. 1, pp. 283299, 2012. [15] M. Pore, K. Sadeghi, V. Chakati, A. Banerjee, and S. K. S. Gupta, "Enabling real-time collaborative brain-mobile interactive applications on volunteer mobile devices," in Proceedings of the 2nd International Workshop on Hot Topics in Wireless. ACM, 2015, pp. 4650. [16] A. Sahayadhas, K. Sundaraj, and M. Murugappan, "Detecting driver drowsiness based on sensors: a review," Sensors, vol. 12, no. 12, pp. 16 93716 953, 2012. [17] P. A. Bryant, J. Trinder, and N. Curtis, "Sick and tired: does sleep have a vital role in the immune system?" Nature Reviews Immunology, vol. 4, no. 6, pp. 457467, 2004. [18] H. J. Eoh, M. K. Chung, and S.-H. Kim, "Electroencephalographic study of drowsiness in simulated driving with sleep deprivation," International Journal of Industrial Ergonomics, vol. 35, no. 4, pp. 307320, 2005. [19] J. Park, L. Xu, V. Sridhar, M. Chi, and G. Cauwenberghs, "Wireless dry eeg for drowsiness detection," in Engineering in Medicine and Biology Society, EMBC, 2011 Annual International Conference of the IEEE. IEEE, 2011, pp. 32983301. [20] C.-T. Lin and et. al, "A real-time wireless braincomputer interface system for drowsiness detection," Biomedical Circuits and Systems, IEEE Transactions on, vol. 4, no. 4, pp. 214222, 2010. [21] C.-T. Lin, R.-C. Wu, T.-P. Jung, S.-F. Liang, and T.-Y. Huang, "Estimating driving performance based on eeg spectrum analysis," EURASIP Journal on Applied Signal Processing, vol. 2005, pp. 31653174, 2005. [22] T. Kurita, N. Otsu, and N. Abdelmalek, "Maximum likelihood thresholding based on population mixture models," Pattern Recognition, vol. 25, no. 10, pp. 1231  1240, 1992. [23] S. H. Kwon, "Threshold selection based on cluster analysis," Pattern Recognition Letters, vol. 25, no. 9, pp. 1045  1050, 2004. [24] S. Kandula, T. Mukherjee, and S. K. S. Gupta, "Toward autonomous vehicle safety verification from mobile cyber-physical systems perspective," SIGBED Rev., vol. 8, no. 2, pp. 1922, jun 2011. [25] S. Kato, S. Tsugawa, K. Tokuda, T. Matsui, and H. Fujii, "Vehicle control algorithms for cooperative driving with automated vehicles and intervehicle communications," Intelligent Transportation Systems, IEEE Transactions on, vol. 3, no. 3, pp. 155  161, sep. 2002. [26] A. Mehmood and S. M. Easa, "Modeling reaction time in car-following behaviour based on human factors," International Journal of Applied Science, Engineering and Technology, 2009. [27] J. W. Muttart, "Quantifying driver response times based upon research and real life data," in 3rd International Driving Symposium on Human Factors in Driver Assessment, Training, and Vehicle Design, vol. 3, 2005, pp. 829.

SCEPTRE: a Pervasive, Non-Invasive, and Programmable
Gesture Recognition Technology
Prajwal Paudyal, Ayan Banerjee, and Sandeep K.S. Gupta
IMPACT Lab, http://impact.asu.edu
Arizona State University, Tempe, Arizona
{Prajwal.Paudyal, abanerj3, sandeep.gupta}@asu.edu
ABSTRACT

Communication and collaboration between deaf people and
hearing people is hindered by lack of a common language.
Although there has been a lot of research in this domain,
there is room for work towards a system that is ubiquitous,
non-invasive, works in real-time and can be trained interactively by the user. Such a system will be powerful enough
to translate gestures performed in real-time, while also being flexible enough to be fully personalized to be used as
a platform for gesture based HCI. We propose SCEPTRE
which utilizes two non-invasive wrist-worn devices to decipher gesture-based communication. The system uses a multitiered template based comparison system for classification on
input data from accelerometer, gyroscope and electromyography (EMG) sensors. This work demonstrates that the system
is very easily trained using just one to three training instances
each for twenty randomly chosen signs from the American
Sign Language(ASL) dictionary and also for user-generated
custom gestures. The system is able to achieve an accuracy
of 97.72 % for ASL gestures.
Author Keywords

Sign language processing; gesture-based interfaces; assistive
technology; wearable and pervasive computing.
ACM Classification Keywords

K.4.2 Social Issues: Assistive technologies for persons with
disabilities; H.5.2 User Interfaces: User-centered design
INTRODUCTION

Non-verbal communication is a big part of day-to-day interactions. Body movements can be a powerful medium for
non-verbal communication, which is done most effectively
through gestures [6]. However the human computer interfaces today are dominated by text based inputs and are increasingly moving towards voice based control [9]. Although
speech is a very natural way to communicate with other people and computers, it can be inappropriate in certain circumstances that require silence, or impossible in the case of deaf
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IUI 2016, March 7–10, 2016, Sonoma, CA, USA.
Copyright c 2016 ACM 978-1-4503-4137-0/16/03 ...$15.00.
http://dx.doi.org/10.1145/2856767.2856794

people [26]. Expressibility is also lost because a lot more
could be communicated if machines were trained to recognize gestures on top of the traditional User Interface(UI) elements like text input and speech. These factors have inevitably nudged us towards gesture based communication.
There are several challenges that need to be addressed and
solved before we can fully implement such methods. The first
one is the lack of a common protocol for gesture based communication, and the second one is the lack of a framework
that can successfully translate such communication gestures
to meaningful information in real-time. Another important
aspect of designing gesture based communication methods is
that they need to be pervasive and non-invasive. A survey of
ASL users that was taken as part of this research to better assess such requirements, which is summarized in Table 2. The
participants of this survey were university students between
ages of 19-38, who had completed or were working towards
an ASL course at ASU. Everybody in the survey was comfortable conveying simple ideas using ASL.
The system that is proposed as part of this work, SCEPTRE,
fulfills these requirements. The entire system is comprised of
an Android smartphone or a Bluetooth enabled computer and
one to two commercial grade Myo devices that are worn on
the wrist to monitor accelerometer, electromyogram(EMG)
and orientation. Data is aggregated and preprocessed in
the smartphones and is either processed locally or sent to a
server. This flexible approach makes it very pervasive as the
users can walk around anywhere with the ready-to-use system [4],[2]. Furthermore the devices can be worn underneath
a shirt inconspicuously. In the future smart-watches equipped
with EMG sensors can be used instead. The main challenge
of this research was to develop a method to store, retrieve and
most importantly match gestures effectively, conclusively and
in real-time.
There are various known methods for gesture recognition and
time-series analysis, such as Support Vector Machines, Linear Correlation, Hidden Markov Models, and Dynamic Time
Warping [24], [14], [7], [10]. Two classes of methods are usually used for time-series analysis and data-matching: One is
the learning based, in which models are developed and stored
and the other one is the template based, in which the new
time-series data is compared to a saved template. Although
learning based models were tried, the template based models were preferred to allow user-driven training with the least
number of repetitions and to make sure the system is usable

in real-time. A very compelling advantage of utilizing a usertrained approach is that the gesture space can start small and
can gradually grow as it is being used. This will also give
the users an ability to discover a pattern of expression, train
their own system and use that to communicate seamlessly
even with users of other systems or languages. Due to the
flexibility of the system, a user who is well versed in a particular gesture based system such as ASL, can choose to train
the system according to the specifications of that language.
She can then add custom gestures or shortcuts to add to that
system if desired.

can be converted to a regular language for communication
purposes just as one would use an audio input to form sentences, or they can be used to trigger specific actions according to pre-configured preferences. For example a user may
use a certain gesture to signal she wants to control a certain
device. The smartphone hub can then trigger a connection to
that device, say a garage door, and the user can give a specific
gesture command to open the door, and then another one to
disconnect as seen in Figure 1.

To demonstrate this, the system was first trained by randomly
selecting ten control-oriented signs that are intuitive to use
such as drawing a square or a letter in the English alphabet in
the air with one or both hands. Other more intuitive gestures
such as ’twisting one hand’ or ’waving right’ etc. were also
used. The recognition rate on these ’custom’ gestures can be
kept very high since the system is designed in such a way
that while using non-language based signs, the system is able
to guide a user to select only those signs that are sufficiently
’different’ from the signs that the system is already trained
in; this is achieved in a similar way as if a test gesture was
being recognized. The system accepts a new gesture only if
recognizing the gesture causes no loss in overall accuracy.
This is called the ’guided’ mode in the system.

Most existing work on sign language recognition utilize image/video based recognition techniques while some other utilize data gloves [21], [19], [12], [17], [29]. Although a high
accuracy is achieved in the aforementioned data-glove based
approaches, one major drawback is that the system is wired
and invasive as it interferes with day-to-day activities of the
user. According to Fang et al. [12], commercially existing
data-gloves are also too expensive for large scale deployment.
The system that we propose, utilizes Bluetooth enabled sensors that appear in wearables such as smart-watches, and are
thus pervasive and non-invasive. They can also be easily worn
underneath a shirt to make them less conspicuous.

To demonstrate the extendibility of the system, it was then
trained for 20 ASL signs by turning the ’guided mode’ off.
Experiments were done both when the system was trained
in these signs by the user, and when the system was used
with training data only from the other ’pool’ of users. Due to
the vast difference in EMG signals between users, the latter
experiments did not yield very favorable results. However,
recognition rates of 97.72% was achieved when all three of
EMG, accelerometer and orientation sensors were used and
the database consisted of three instances of the gesture trained
by the same user. Using any two of the three sensors, the
highest recognition accuracy of 95.45% was achieved when
using EMG sensors paired with orientation sensors. With
only one of the sensors the highest accuracy that was achieved
was 93% for orientation sensor. Data was collected from 10
healthy adults between the ages of 22 and 35, each of whom
performed a total of 20 ASL gestures.
Usage

The system is envisioned to be used in two primary use cases:
1. User-to-user communication where at least one user uses a
sign language: In this scenario, the user wishes to communicate with a person using a gesture based communication
medium, but the other person cannot understand that mode
of communication. The first user, then begins the translation
mode and performs a gesture. The hub receives the data, processes it and sends the recognized output to the user via a text
message or audio message as seen in Figure 2 or converted to
animated models as facilitated by systems like that described
by Marshall et al. [31].
2. User-to-computer Interactions: In this scenario, the user of
system uses ASL or some form of custom gestures to communicate with a computer system. The gestures transmitted

RELATED WORK

The image and video based recognition systems [33, 23, 16,
18, 5] use either annotated fingers, color bands etc, or just
plain raw images and video from users to translate gesture
based communication. Some studies also utilized multiple
cameras [37] to enhance the accuracy by some margin. However, image and video based systems are dependent on presence of cameras in the right positions and are affected by
background colors. Another drawback to these systems is
that image and video processing is computationally expensive
which makes the recognition system’s response time slower.
Also, heavy dependence on Hidden Markov Model (HMM)
based methods makes training the system cumbersome and
data intensive. Unlike these approaches, using portable wristwatch like sensors to detect gestures is far less invasive and a
template based recognition approach does not require many
training data-sets. The computational complexity also decreases as there is no need to deal with data-intensive images
and videos, this allows the system to be real-time as well as
environment independent.
Li et al. [20] has done a study on combining accelerometer
and EMG sensors to recognize sub-word level gestures for
Chinese Sign Language and Chen et al. [8, 39] show that
combination of multiple sensors helps to increase the recognition accuracy. Following this path, we add the EMG measurements from eight built-in pods in the Myo device to get information that can be leveraged to detect subtle finger movements and configurations which are essential for detecting
certain signs and distinguishing them from others. The orientation sensors help to distinguish between signs that have a
similar movement and muscle patterns but different rotational
primitives. Accelerometer sensors detect movement and position of the hands. The combination of all these sensors into
one commercial grade, wireless sensor has given the ability
to deploy gesture and sign-language recognition abilities in a
far more practical and user-friendly way. Bluetooth technol-

Figure 2. Usage for Person-to-Person Communication.

Figure 1. Usage for HCI/ Control system.

Existing
Gesture
Recognition
Algorithms
[21]
[19]
[12]
[33]
[8]
Sceptre(Proposed)

Mobile

Table 1. Summary of existing Gesture Recognition tools.

⇥
NA
⇥
⇥
⇥
X

User
Extendable

Requires
Video

Real Time

User Independent

Invasive

⇥
⇥
X
⇥
⇥
X

⇥
⇥
⇥
X
⇥
⇥

X
⇥
NA
X
X
X

⇥
X
X
⇥
⇥
⇥

X
X
X
X
X
⇥

Language
Independent
⇥
⇥
⇥
⇥
⇥
X

ogy for transfer of data makes the system wireless and easy
to use. The overall accuracy is also increases due to the use
of all three types of signals.

is utilized in recognizing orientation. Data from EMG sensors
is utilized in detecting muscle tensions to distinguish handshapes, which is another very important feature of the signs.

Thus by coupling pervasive technologies with simple algorithms we achieve high accuracy rates and great mobility.
Starner et al. propose a solution that might be user and language independent as possibilities, however an implementation is not provided [33]. Moving this system towards complete user-independence is part of future work. The visual
nature of sign language communication is however not limited to just hand gestures; expressions and body gait also play
important parts in the overall communication. However analyzing facial expressions and body gaits will be part of future
work.

Location, or tab, refers to specific places that the hands occupy as they are used to form words. ASL is known to use
12 locations excluding the hands [36]. They are concentrated around different parts of the body. The use of a passive
hand and different shapes of the passive hand is also considered valid locations. Movement or sig, refers to some form
of hand action to form words. ASL is known to use about
twenty movements, which include lateral, twist, flex, opening or closing [34]. A combination of accelerometer and gyroscope data is instrumental in distinguishing signs based on
movement and signing space.

PRELIMINARIES
American Sign Language Specifics

Signs in ASL are composed of a number of components: the
shape the hands assumes, the orientation of the hands, the
movement they perform and their location relative to other
parts of the body. One or both hands may be used to perform
a sign, and facial expressions and body tilts also contribute
to the meaning. Changing any one of these may change the
meaning of a sign [15], [3], [35], [11]. This is what gives
sign-language the immense power of expressibility, however
this also means that multi-modal information has to be processed for sign-language recognition.
Orientation is the degree of rotation of a hand when signing.
This can appear in a sign with or without movement. The
gyroscope sensors provide rotation data in three axes, which

Sign language is a very visual medium of communication, the
system that this paper proposes however tries to capture this
visual information, in a non-visual manner to ensure pervasiveness. Thus, multi-modal signals are considered to get as
close of an estimate as possible.
Problem Description

The primary task of the system is to match gestures. Since
multi-modal signals are being considered, they are compared
individually to the signals stored in the database and the results are combined. The form of the input data is described in
detail Section Methodology, but in short it consists of an array
of time-series data. The gesture database has pre-processed
iterations of the same gesture along with other gesture data.
The problem that the system is solving is comparing this test
gesture data to each of the existing sample data using DTW

Algorithm 1 Problem Description.
1: procedure M ATCH G ESTURES(test)
2:
for (i in 1 to database size) do
3:
template
next template
4:
for (j in 1 to number of sensors) do
5:
distance[j]
dist(test[j], template[j])
6:
end for
7:
Overall dist = combine(distance)
8:
end for
9:
Output = min(Overall dist)
10: end procedure

Figure 3. Myo Devices and Android Smartphone with the Sceptre App
showing.

and Energy based techniques as appropriate and then combining the results at the end to give a distance score. Then, at
the very end, the task is to recognize the gesture based on the
least distance. Algorithm 1 provides a high level overview of
this approach.
REQUIREMENTS

The need/demand for sign language recognition software for
accessibility has been previously established, but accessing
if there is a need or demand for systems that can utilize an
established natural language as a human computer interface
has not been studied (to our knowledge). As part of this research a survey was taken where 13 users of ASL expressed
their opinions on these topics that included their willingness
to use a gesture based system, the perceived practicality of
using ASL for HCI, the acceptable time constraints for user
to user and user to computer communications etc. The results
of the survey are summarized in Table 2.
Extendability

With more and more systems with voice command interfaces,
it only seems plausible that a system that understands sign
language will be desirable. To the question 2 in the survey,
more than 75% of the ASL users agreed that they were very
likely to use ASL for HCI, and the rest would be somewhat
likely to use such a system. More than 75% people also responded that it was very important that the system be easily
trainable, and another 15% thought it was somewhat important. This follows from the knowledge [15] that ASL (and
sign language in general) varies a lot over geographical, racial
and cultural bounds. Thus, the ability to add unrecognized
signs or new signs easily is very important. When ASL users
were asked if they were likely to use the system if it could be

trained using 3 training instances, more than 76% said very
likely and the rest responded they were somewhat likely to
spend the time training. This requirement puts the constraint
on the system that it has to be relatively easy to train. This
system can be trained using only three instances of training
per sign. The relationship between training instances and accuracy is summarized in SectionResults and Evaluation.
Time Constraints

Communication is a real-time activity and the need for fast response times in both person-to-person communication as well
as HCI is well established. A timing delay of 0.1 s is considered ’instantaneous’ while a delay of 0.2 s to 1 s is noticeable
to the user, but generally acceptable. If a system takes more
than 1 s, then some indication needs to be given to user about
the ’processing’ aspect [22]. The survey question regarding
timing backs this up: while more than 76% of ASL users said
they were very likely and an additional 15% said they were
somewhat likely to use the system if the response time was
under 3 s, this number falls to 53% and 0% very likely responses when the response times were 3-7 s, and more than 7
s respectively. This establishes a strict constraint for recognition time for the system. To meet these standards the system
has to be light-weight and should not rely on computationally expensive methods. Also, if the processing is delayed
due to unpredictable issues like client network sluggishness,
a proper indication has to be given to the user of the wait.
Non-Invasiveness

Historically there has been a certain stigma associated with
signing. It was only in 2013 that the White House published a statement in support of using sign language as an official medium of instruction for children, although it has been
known to be a developmentally important for deaf children
for years [1]. Given this stigma, and due to other more obvious reasons like usability, an interpretation system should
be as non-invasive as possible. The survey results confirm
this, as more than 90% thought that it is important or very
important for the system to be non-invasive.
Pervasiveness

The other disadvantage of using more invasive technology
like data-gloves or multiple-cameras is that it makes the system either tethered to a certain lab based environment settings
or it requires bulky equipment that discourages its use in an
everyday setting [21, 19, 12]. This makes the use of smartwatch like wireless equipment that can work on the streets as
well as it works in a lab more desirable.
To make sure that the proposed system meets these requirements some performance metrics are decided upon which are
discussed in detail in Subsection Performance Metrics and
subsequently the system is tested w.r.t to these metrics as discussed in Subsection Design of experiments.
SYSTEM ARCHITECTURE

The system consists of two Myo Devices connected via Bluetooth to an Android device which acts as a hub as shown
in Figures 3 and 4. For the EMG part a Bluetooth enabled
computer is used to obtain the data since the Myo framework

Table 2. Summary of ASL users’ survey on a continuum from very unlikely (0) to very likely (5).

Question
Interest in trying solutions for gesture based HCI
Interest in using ASL for gesture based HCI
Likeliness to use a system that requires 3 trainings instances
Importance of non-invasivenss
Likeliness to use if wait time 0-3 s
Likeliness to use if wait time 3-7 s
Likeliness to use if wait time 7-10 s
Likeliness to try a prototype
Likeliness of the use of this system by a native speaker for communication

1
0
0
0
0
0
1
4
0
0

2
0
0
0
1
1
1
4
1
1

3
1
3
3
3
2
4
5
3
1

4
3
4
1
2
4
5
0
4
5

5
9
6
9
7
6
2
0
5
6

specific accuracy, new signs trained etc. to the server. This
information can be used to make future predictive methods
more efficient which is not part of this work.

Figure 4. Deployment: A user with a wrist-band devices on each hand
performing a gesture.

does not yet facilitate streaming EMG data to smart phones.
For tests that do not use EMG data, Android device collects
and does some pre-processing on the data of the gesture performed and stores this as time-series data. All this is sent to
the server for processing. Three to four instances for each
gesture data are collected, processed and stored. A high level
overview of the pre-processing that is done to the data before
it is stored in the database is summarized in Figure 5.
At test time when a gesture is performed, the system processes the gesture-data in a similar way as described above. In
the Architecture Figure 6, this is encapsulated in the ’Preprocessing’ block. After this is done, the system compares this
data with the other gesture data stored in the database using
a specialized comparison method. The comparison method
comprises of testing accelerometer data, orientation data and
EMG data by different methods and later combining the results before calculating a ’nearness’ score for each pair. After
this step a ranking algorithm is employed in the server to compute a list of stored gestures that are closest to the test gesture
and best one is returned. Details on these techniques is given
in Section Methodology. The system using the same methods
of comparison is able to help the user be selective in the signs
that the system is trained on, this is called the ’guided mode’
as explained in Subsection Training. A smart ranking architecture is used when the size of the database grows to ensure
that the results come back in acceptable time to be a real-time
application. This is discussed in more details in Subsection
Ranking and Optimization. The system also periodically uploads all recognized gesture data and other meta-data like user

Figure 5. Pre-processing: Data is collected from the Myo devices, processed and then stored in Database.

Training

Training the system requires two Myo devices, a hub for data
collection, and server for processing. The hub and the server
may be one and the same. A typical training sessions consists
of the following steps: 1. The user selects either the ’guided
mode’ or ’ASL’ mode. (in the smartphone interface) 2. She
selects a sign from a dropdown list, or creates a new sign. 3.
She performs the sign. 4. The system annotates the input data
with the name of the sign and stores the data. 5. If guided
mode was selected, the system performs a scan of the sign
database specific to the user and determines if there are any
clashes. If not, the user is asked to repeat the sign two more
times after which the sign is ready to use. If however there
was a clash, then the user is suggested to choose another sign
instead. 6. If the ’ASL’ mode was selected, the system does
not give such feedback, and simply asks the user to train the
system two more times. 7. The gesture is ready to use.
METHODOLOGY

The overall methodology of gesture recognition consists of
the following steps as shown in Figure 6.
Pre-processing

Data is collected from two Myo devices while the gesture is
being performed (Figure 5). As soon as the end is detected or

Figure 7. Variation in EMG signals between two users.
Figure 6. Gesture Comparison and Ranking: New data is collected, processed and compared with existing gesture data to get a ranked-list of
similarities.

signaled, the pre-processing begins. The data collected from
two hands is aggregated into one data-table then stored into a
file as an array of time-series data. At 50 Hz. of sampling rate
a 5 s. gesture data will consist of: 6 accelerometer Vectors of
length 250 each, 6 gyroscope Vectors of Length 250 each,
6 orientation Vectors of Length 250 each, 16 EMG Vectors,
Length 250 each. We combine all this for simplicity into a
34 ⇥ 250 matrix. Each time-series is transformed to make
sure the initial value is 0, by subtracting this value from all
values in the time-series. This will help prevent errors when
the user performs the sign in a different starting positions.
Normalization is then done by representing all values as floats
between 0 and 1 by min-max method.
Orientation values are received in the form of three timeseries in terms of unit quaternions. The pitch, yaw and roll
values are obtained from the quaternion values w, x, y and z
by using the following equations:
roll = tan
pitch = sin

1

1 2(wx+yz)
( x2 y 2 )

(max( 1, min(1, 2(wy

yaw = tan

(1)
zx))))

1 2(wz+xy)
( y2 z2 ).

EMG Energy

After correctly identifying the location of each of the individual pods of the two Myo devices, data is stored and shuffled
in such a way that the final stored data is aligned from EMG
pod-1 to EMG pod-8. This gives flexibility to the end-user as
she no longer has to remember ’how’ to put on the devices,
and can thus randomly put either Myo device on either hand
and expect good results. This is a great leap towards the pervasiveness and ease of use of this technology.
While we found that the DTW based testing worked very
good for accelerometer and orientation comparisons, the
EMG comparison numbers were not satisfactory. This is because, EMG activations seem to be much more stochastic and
a ’shape’ based comparison did not yield good results. The

shapes formed by performing the same gesture for two different people are drastically different as seen in Figure 7, but
they also differ in shape significantly enough between two
repetitions by the same person to not give a small DTW distance. However, it was found by experimentation that gestures tend to activate the same EMG pods when repeated.
Thus an energy based comparison was tried:
EMG energy ’E’ on each pod is calculated as the sum of
squares of x[n] , the value of the time-series at point ’n’ .
E = sum(x[n]2 ).

(2)

Dynamic Time Warping

We used four different approaches for comparing accelerometer and orientation data: a. Euclidian distance b. Regression
c. Principal Component Analysis (PCA) and d. DTW. Euclidian distance simply compares the two time-series using
mean-squared error. Regression analysis fits a model to the
time-series and uses this model to compare best fit for the test
gesture. Given a set of features from the time-series for a gesture, PCA derives the optimal set of features for comparison.
According to [25], DTW is a well-known technique to find an
optimal alignment between two given (time-dependent) sequences under certain restrictions. It was found that DTW
based methods gave best results. DTW based approached
proved ideal in our use-case since it could gracefully handle the situations when the sign was performed slower or
faster than the stored training data, or performed with a small
lag. Traditionally, DTW has been used extensively for speech
recognition, and it is finding increasing use in the fields of
gesture recognition as well [30], specially when combined
with Hidden Markov Models [38]. SCEPTRE takes a simpler approach by randomly re-sampling the training and test
datasets based on the least number of points in either one,
then doing a DTW based distance analysis on them. First a
DTW analysis of Accelerometer Data is run and a ranked list
of ’probable’ signs is passed on for DTW based analysis of
orientation Data which in turn creates an even shorter ranked
list to be processed by the final EMG algorithm.
On another approach, the normalized distances from each of
the outputs is taken and the sum of squares of the final out-

Algorithm 2 Overall Gesture recognition method.

Algorithm 3 Optimization to meet time constraints.

1: procedure G ESTURE R ECOGNITION
2:
t
elapsed time from start of the experiment
3:
test gesture
get gesture data from the user
4:
SG
query list of recognized gestures from database
5:
normalized accl
accel values f irst accl value
6:
normalized orient
orientation f irst orientation
7:
normalized emg
emg values f irst emg value
8:
for each trained gesture TG 2 SG do
9:
dtw accl
dtw(training accl, normalized accl)
10:
dtw orient
dtw(training orient, normalized orient)
E
11:
TG
calculated energy of each pod for the training gesture
E
12:
emg energy dif f
TG
- test emg energy
13:
scal lim
{0, 1}
14:
scaled emg dif f
scale(emg energy diff,scal lim)
15:
scaled accl dist
scale(dtw accl, scal lim)
16:
scaled orient dist
scale(dtw orient, scal lim)
17:
combined nearness
compute nearness from Eq. 3
18:
end for
19:
Sort TG with respect to combined nearness.
20:
recognized gesture
top TG in the sorted list
21: end procedure

1:
2:
3:
4:
5:
6:
7:
8:

databasesize
sizeof gesturedatabasef oruser
time to compute
estimatedtimef rompreviousiterations
if time to compute < time contraint then
Compute Similarity Score with one template per gesture
top gestures
list(top gestures, time to compute, time constrant)
Utilize Algorithm 2
END
end if

put is taken as an indication of ’closeness’ of a test sign to a
training sign. This is the approach that was chosen due to its
computational simplicity and speed.
Combination

The overall ’nearness’ of two signs is computed to be the total
distance between those signs which is obtained by adding up
the scaled distances for accelerometer, orientation and EMG
as discussed above. An extra step of scaling the distance values between (0,1) is performed so as to give equal weight to
each of the features. Also, since we have 8 EMG pods and
only 3 each of accelerometer and orientation sensors, we use
the following formula for the combination. The formula is for
combining accelerometer sum of distances and EMG sum of
distances. Similar techniques were applied for the other combinations. An algorithmic summary can be found in Equation
3.
dist = (8cs(sc accl comb) + 3cs(sc emg comb))/24.

(3)

where cs() is a function that returns the sum of columns,
sc accl comb is a data frame that holds the combined accelerometer DTW distances for both hands for all trained
signs, and sc emg comb is a data frame that holds the combined EMG energy distances for both hands for all trained
signs. The overall algorithm that is employed for recognition
is summarized in Algorithm 2.
Ranking and Optimization

Timing constraints due to the real-time nature of the application requires us to optimize the recognition algorithm to
be efficient, specially as the gesture space increases. As the
number of gestures in the database increases to beyond 60,
as we can see in Figure 8, the recognition time for identifying one gesture goes beyond the .5 s mark, which we have
chosen to represent a real-time timing constraint. Thus, to
deal with this situation, we modify our comparison algorithm
to first compare to one stored instance of each gesture, then
choose the top ’n’ number of gestures which when compared
to ’k’ of each, still allowing the time-constraint to be fulfilled.
We then, proceed with the normal gesture comparison routine
on only these gesture instances and thus keep the recognition

Figure 8. Database Size vs. Recognition Time.

time within our constraints. All the variables for this method
viz. the ’n’, ’k’ are calculated dynamically by what is allowed
by the timing constraint ’tc’, thus making this approach fluid
and adaptable to more vigorous time constraints if required.
The overall optimization algorithm is summarized in Algorithm 3.
RESULTS AND EVALUATION
Design of experiments

Experiments were designed to test the application with scalability in mind. The system begins on an ”empty slate” and
the gestures are trained interactively. Since a template based
comparison is being utilized, we store the entire data set plus
some features. After all the data is collected, the system is
ready to be trained for another gesture. A sample application
screen can be found in Figure 3. It is advised to provide 3-4
training instances of the same gesture to improve accuracy of
recognition but that is not required. We test the system with
1, 2, 3, and 4 instances of each gesture to determine the correlation between number of instances saved and the recognition
accuracy rate. That is all that is needed at the training phase.
During testing phase we evaluate our system based on time
it takes for recognition and on combination of features that
produce the best results. Then we evaluate how recognition
time increases with the increase in the number of gestures.
The way gestures are performed by the same person are generally similar but they have a tendency to vary slightly over
time. Although, considering signals generated by a portable
device that the user can put on and off by herself, provides the

system a lot of portability and pervasiveness, it comes with a
drawback that the signals we receive might not always be the
same. To account for this experiments were performed that
allowed users to put on the devices themselves, they were allowed to face any direction, and perform the gesture as long
as they stayed in the Bluetooth range of the hub. Also, samples were collected over multiple sessions to account for variability in signing over time. The test subjects were 10 University students between the ages of 20 and 32 who performed
the gestures in multiple sessions while also varying direction
they were facing and if they were standing up/sitting down.
The subjects viewed a video recording of the gestures being
performed by ASL instructors before performing them with
the system.

Figure 9. Training Data vs. Accuracy.

Prototyping and Choice of Gestures

20 ASL signs were chosen to prototype the system. They
were carefully chosen such that a. A good mix of the various
ASL components as discussed in Section Preliminaries and b.
To include signs that are very close in some components but
different in others. The choice of signs and the break-down
of components for 10 of the 20 chosen signs is summarized in
Table 3. The other 10 signs that were chosen are hurt, horse,
pizza, large, happy, home, home, mom, goodnight, wash. Detailed analysis of system performance is give in Section Results and Evaluation.
Performance Metrics

The system is evaluated on each of the requirements discussed in Section Requirements. Pervasiveness of the system
is justified since it is wireless and can work in Bluetooth range
of the hub device which can either do the processing itself or
offload it to a cloud server. Invasiveness is a harder metric to
evaluate, as this seems to be more subjective. However, with
wearable devices like smartwatches, wristbands such as FitBit, Myo becoming increasingly compact, wireless and even
stylish [13], the proposed system is much less invasive then
any of the alternatives. (see Related Work). Accuracy is undoubtedly one of the fundamental performance metrics for
any recognition system. This basic function of the system is
to facilitate real-life conversations, thus the system has to be
able to function in real-time. Another aspect of the system
is that it is extendable, thus the ease in scalability to a larger
number of signs is very important. With the increase in the
gesture space, the recognition time will also increase, the system should be able to scale up with a reasonable recognition
time. Although there is a slight dip in recognition rate, the potential for scalability showcases the system’s flexibility. Thus
the experiments focus on these three metrics to evaluate the
system:

on both hands. The desired frequency of each channel is 50
Hz. b. Wireless interface to transmit raw data from each of
these sensors. c. A smartphone that acts as a hub to receive,
pre-process, communicate the data to server, and later to display the results. d. A server that stores previous data in an
individualized database per user, compares gestures, finds a
match, and sends information back to the smartphone.
All experiments were performed with the setup described in
Section System Architecture with devices with the following
configurations:
Two Myo Sensors: Medical Grade Stainless Steel EMG sensors, Highly sensitive nine-axis IMU containing three-axis
gyroscope, three-axis accelerometer, three-axis magnetometer. Expandable between 7.5 - 13 inches, wieghing 93 grams
with a thickness of 0.45 inches. Communication channel:
Bluetooth. Processor:ARM Cortex M4 Processor Hub Specifications(Android): OS: Android OS, v5.0 Chipset: Qualcomm MSM8974 Snapdragon 800 CPU: Quad-core 2.3 GHz
Krait 400 Memory: 2 GB RAM Mac Specifications(Server)
Processor:3.06GHz Intel Core i3 processor with 4MB level 3
cache; supports Hyper-Threading Momory: 8 GB DDR
Results

Device Requirements

It was determined through experiments that three is a good
choice for the number of training instances for each gesture.
This formed a good compromise between usability and results as shown in Figure 9. Thus, each gesture was performed
three times at varying times in the day. Then all data was aggregated, and annotated according to the gesture performed.
The testing comprised of taking one dataset and comparing it
with all other data sets and then estimating the correct gesture. With the ’guided mode’ turned on, a recognition rate of
100% was achieved as expected. Then 20 randomly selected
ASL Signs were used. The best accuracy was obtained by a
tiered-combination method of all three features. The relative
accuracy of other methods can be seen in Figure 11. This
helps confirm that the combination of all three features produces the highest results.

Sensors: a. EMG, accelerometer and orientation (or Gyroscope) sensors that go between the wrist and elbow of a user

Figure 8 shows how the recognition time increases with the
increase in the number of gestures that are stored in the system. With 65 gestures in the database, a recognition time of

1. Recognition time
2. Extensibility
3. Recognition rate (Accuracy)
The device requirements for the experiments are:

Number Name
of
Sign
1
Blue
2
Cat
3
Cost

Location(Tab)

4

Table 3. Component breakdown of 10 of the chosen signs.

Movement

Orientation

Around the Face
Around the face
Trunk

Wrist-Twist
Outward and closing
Wrist-Flex

Dollar

Trunk

Two Wrist-Flexes

5

Gold

Side of the head

6
7
8
9
10

Orange
Bird
Shirt
Large
Please

Mouth
Mouth
Shoulder
Trunk
Trunk/Chest

Away from center, change handshape twist
Open and close twice
Pinch with Two fingers
Twice Wrist Flex
Both Hands Away from Center
Form a circle

Away from signer
Towards center
One towards signer one towards
center
One towards signer one towards
center
Towards the signer
Towards user
Away from signer
Facing down
Facing away from signer
Both facing towards signer

Number
of
Hands
1
2
2
2
1
1
1
2
2
1

Figure 10. Gesture for ’Mom’ Vs. ’Eat’.
Figure 12. User Trained Gestures.

Figure 11. Features vs. Accuracy.

0.552 s was achieved. (processed on a 3.06 GHz Intel Core
i3 8 GB RAM Mac). This fast response time means that the
system qualifies for use for real-time communications. This
time can be further improved upon by utilizing a more powerful server and optimizing the data queries, which will be part
of future work.
Figure 13 shows the performance of the system based on
recognition results for a combination of the features. The gestures tested were ’day’, ’enjoy’, ’eat’, ’happy’, ’home’, ’hot’,
’large’, ’mom’, ’pizza’, ’shirt’, ’still’, ’wash’ and ’wave’. The
different sub-figures show that although comparing solely
based on accelerometer performs good for some gestures like

’happy’, it is ineffective in distinguishing between others like
’day’. This can be explained because the nature of performing
this gesture is very similar to other gestures in overall hand
movements, but are different when it comes to finger configurations. Thus the performance gets better as shown in Figure
13c when EMG sensors are brought into the equation. Like
we discussed earlier, the least number of recognition errors
overall occur when all three of the sensor data are fused as
seen in Figure 13d. This gives an insight on where the system
is error-prone and thus can be optimized. Server level Optimization based on such input will be part of future work. We
envision a server that continuously monitors success-failure
data and becomes better by implementing machine learning
algorithms to give weights to the different features.
These results can be understood better in context with the
breakdown information given for each gesture in Table 3. For
instance, Figure 10 shows screenshots of a person performing
the gesture ’mom’ and another one performing ’eat’. These
two gestures are very similar in ’Location’ and ’Movement’
but are distinctive when it comes to ’HandShape’ and ’Orientation of Hand’. Thus, while the system isn’t able to distinguish between these two gestures based on Accelerometer
information only as seen in Figure 13a, the system does well
when EMG information is included as seen in Figure 13c.
The guided mode as discussed in Usage was trained using 3
iterations each of 10 custom signs shown in Figure 12. An

Framework Limitations: Currently due to limitations of the
Myo framework for Android, final tests involving EMG data
signals had to be done using laptop computers. Two Mac
computers were used to gather the data simultaneously via
Bluetooth channels and then combine them and store them
to a cloud based data storage system. A separate script was
triggered to process the stored data at test time. With the
anticipated release of new framework for Myo Devices, this
computation can be done at the mobile phone level and data
can be sent to the server only at designated intervals.
User Independence: EMG data fluctuates a lot between people. This is apparent from Figure 7. More research can be
done on data gathering and feature selection of the EMG data
pods to come up with a ’unifying’ framework that works for
everyone. This will be a definitive direction in attaining userindependence while using EMG signals.
Search Algorithm: The search algorithm that is implemented can be improved upon to decrease the recognition
time. Hierarchal searching can be done, in which training
gestures are clustered according to the energies in each timeseries data, and only those gestures are compared which have
comparable energies.
Figure 13. Gesture Recognition vs. Features used.

accuracy of 100% is achieved for this mode as expected, because clashes are detected at training time and avoided.
The results in Figure 11 show an accuracy of 97.72% for 20
ASL signs when all accelerometer, orientation and EMG are
used. The database consisted of signs from 10 different people, and each person performed each sign 3 times. This table
also lists the accuracy of other variations in the combination
of these three input signals. The accuracy varied when individual databases were separated, however it stayed in the
(97-100)% bracket when all signals were used on a dataset
consisting of template gestures from the test user.
DISCUSSION

Continuous Processing: The gestures that were recognized
in this test were isolated in nature. A start and stop button, or
a timer was utilized to gather data. This was done to test the
algorithms in isolation first. However, when the system is actually deployed to the public, a continuous monitoring algorithm has to be utilized. This can be done by using windowing
methods and optimizing based on best results. However, we
expect that the system will require a lot more training.
Dictionary Size: Another thing that can be improved upon
is the dictionary size and the support for Signed Alphabet.
Signed alphabet give an ASL user the ability to use English
Language Alphabet to visually spell out a word. This is an
important aspect of sign language communication since not
all english words or ideas have sign language counterparts.
The dictionary size used is currently 20 ASL words and 10
user invented gestures. The video based sign language dictionary website signasl.org currently has over 30,000 videos for
sign languages in use, so there is a lot of room to grow into.

SCEPTRE in collaboration with other HCI techniques such
as brain-driven control [27, 28, 32] can revolutionize the way
people interact with computers. In addition to the uses of HCI
or Sign Language Communication, this technology can also
be extended to uses in the domains of activity recognition,
food intake estimation or even physiotherapy.
CONCLUSIONS

DTW and energy based comparison methods are fast and
highly effective. Template based comparison techniques have
a great advantage of lossless information storing. Comparing accelerometer and orientation data between gestures is
best done by using Dynamic Time Warping methods which
muscle movement (EMG) data is best compared by comparing total energies in each pods. The structure present in Sign
Languages like ASL can be divided into components which
explain the success of certain signals in recognizing them.
The overall accuracy of the system is increased by a smart
combination of these signals without compromising on speed,
recognition rate, or usability. The sensors that are used are
non-invasive and versatile thus allowing people to effectively
utilize them in day-to-day situations without drawing much
attention. The future direction of this research is in incorporating continuous sign processing, user independence in the
system and increasing the dictionary size.
ACKNOWLEDGEMENTS

This project was partly funded by NIH NIBIB (Grant
#EB019202) and NSF IIS (Grant #1116385). We thank Julie
Stylinski, Lecturer at ASU for her invaluable ASL domain
knowledge and for facilitating the survey. We thank all
the survey participants as well as volunteers that helped to
train/test the system. We thank Subhasish Das of IMPACT
lab, ASU for his technical contribution to get the project going.

REFERENCES

1. Daphne Bavelier Aaron J. Newman. A critical period for
right hemisphere recruitment in american sign language
processing. In Multimodal Signals: Cognitive and
Algorithmic Issues, pages 76–80, 2002.
2. Frank Adelstein, Sandeep KS Gupta, Golden Richard,
and Loren Schwiebert. Fundamentals of mobile and
pervasive computing, volume 1. McGraw-Hill New
York, 2005.
3. B.J. Bahan. Non-manual Realization of Agreement in
American Sign Language. UMI Dissertation Services.
UMI, 1996.
4. Ayan Banerjee and Sandeep KS Gupta. Analysis of
smart mobile applications for healthcare under dynamic
context changes. Mobile Computing, IEEE Transactions
on, 14(5):904–919, 2015.
5. Sara Bilal, Rini Akmeliawati, Amir A Shafie, and
Momoh Jimoh E Salami. Hidden markov model for
human to computer interaction: a study on human hand
gesture recognition. Artificial Intelligence Review,
40(4):495–516, 2013.
6. Baptiste Caramiaux, Marco Donnarumma, and Atau
Tanaka. Understanding gesture expressivity through
muscle sensing. ACM Transactions on
Computer-Human Interaction (TOCHI), 21(6):31, 2015.
7. Feng-Sheng Chen, Chih-Ming Fu, and Chung-Lin
Huang. Hand gesture recognition using a real-time
tracking method and hidden markov models. Image and
vision computing, 21(8):745–758, 2003.
8. Xiang Chen, Xu Zhang, Zhang-Yan Zhao, Ji-Hai Yang,
Vuokko Lantz, and Kong-Qiao Wang. Hand gesture
recognition research based on surface emg sensors and
2d-accelerometers. In Wearable Computers, 2007 11th
IEEE International Symposium on, pages 11–14. IEEE,
2007.
9. Philip R Cohen and Sharon L Oviatt. The role of voice
input for human-machine communication. proceedings
of the National Academy of Sciences,
92(22):9921–9927, 1995.

10. Andrea Corradini. Dynamic time warping for off-line
recognition of a small gesture vocabulary. In
Recognition, Analysis, and Tracking of Faces and
Gestures in Real-Time Systems, 2001. Proceedings.
IEEE ICCV Workshop on, pages 82–89. IEEE, 2001.
11. E. Costello. Random House Webster’s American Sign
Language Dictionary. Random House Reference, 2008.
12. Gaolin Fang, Wen Gao, and Debin Zhao. Large
vocabulary sign language recognition based on fuzzy
decision trees. Systems, Man and Cybernetics, Part A:
Systems and Humans, IEEE Transactions on,
34(3):305–314, 2004.
13. Sandeep KS Gupta, Tridib Mukherjee, and
Krishna Kumar Venkatasubramanian. Body area
networks: Safety, security, and sustainability.
Cambridge University Press, 2013.

14. Deng-Yuan Huang, Wu-Chih Hu, and Sung-Hsiang
Chang. Vision-based hand gesture recognition using
pca+ gabor filters and svm. In Intelligent Information
Hiding and Multimedia Signal Processing, 2009.
IIH-MSP’09. Fifth International Conference on, pages
1–4. IEEE, 2009.
15. T. Johnston and A. Schembri. Australian Sign Language
(Auslan): An introduction to sign language linguistics.
Cambridge University Press, 2007.
16. Daniel Kelly, Jane Reilly Delannoy, John Mc Donald,
and Charles Markham. A framework for continuous
multimodal sign language recognition. In Proceedings of
the 2009 international conference on Multimodal
interfaces, pages 351–358. ACM, 2009.
17. Kyung-Won Kim, Mi-So Lee, Bo-Ram Soon, Mun-Ho
Ryu, and Je-Nam Kim. Recognition of sign language
with an inertial sensor-based data glove. Technology and
Health Care, 24(s1):S223–S230, 2015.
18. Oscar Koller, Jens Forster, and Hermann Ney.
Continuous sign language recognition: Towards large
vocabulary statistical recognition systems handling
multiple signers. Computer Vision and Image
Understanding, 141:108–125, 2015.
19. T Kuroda, Y Tabata, A Goto, H Ikuta, M Murakami,
et al. Consumer price data-glove for sign language
recognition. In Proc. of 5th Intl Conf. Disability, Virtual
Reality Assoc. Tech., Oxford, UK, pages 253–258, 2004.
20. Yun Li, Xiang Chen, Jianxun Tian, Xu Zhang, Kongqiao
Wang, and Jihai Yang. Automatic recognition of sign
language subwords based on portable accelerometer and
emg sensors. In International Conference on
Multimodal Interfaces and the Workshop on Machine
Learning for Multimodal Interaction, ICMI-MLMI ’10,
pages 17:1–17:7, New York, NY, USA, 2010. ACM.
21. Rung-Huei Liang and Ming Ouhyoung. A real-time
continuous gesture recognition system for sign
language. In Automatic Face and Gesture Recognition,
1998. Proceedings. Third IEEE International
Conference on, pages 558–567. IEEE, 1998.
22. Robert B Miller. Response time in man-computer
conversational transactions. In Proceedings of the
December 9-11, 1968, fall joint computer conference,
part I, pages 267–277. ACM, 1968.
23. Sushmita Mitra and Tinku Acharya. Gesture
recognition: A survey. Systems, Man, and Cybernetics,
Part C: Applications and Reviews, IEEE Transactions
on, 37(3):311–324, 2007.
24. Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. Latent-dynamic discriminative models for
continuous gesture recognition. In Computer Vision and
Pattern Recognition, 2007. CVPR’07. IEEE Conference
on, pages 1–8. IEEE, 2007.

25. Meinard Müller. Dynamic time warping. Information
retrieval for music and motion, pages 69–84, 2007.
26. Yuji Nagashima. Present stage and issues of sign
linguistic engineering. HCI, 2001, 2001.
27. Koosha Sadeghi Oskooyee, Ayan Banerjee, and
Sandeep KS Gupta. Neuro movie theatre: A real-time
internet-of-people based mobile application. 2015.
28. Madhurima Pore, Koosha Sadeghi, Vinaya Chakati,
Ayan Banerjee, and Sandeep KS Gupta. Enabling
real-time collaborative brain-mobile interactive
applications on volunteer mobile devices. In
Proceedings of the 2nd International Workshop on Hot
Topics in Wireless, pages 46–50. ACM, 2015.
29. Nikhita Praveen, Naveen Karanth, and MS Megha. Sign
language interpreter using a smart glove. In Advances in
Electronics, Computers and Communications
(ICAECC), 2014 International Conference on, pages
1–5. IEEE, 2014.
30. Lawrence R Rabiner. A tutorial on hidden markov
models and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257–286, 1989.
31. Éva Sáfár and Ian Marshall. The architecture of an
english-text-to-sign-languages translation system. In
Recent Advances in Natural Language Processing
(RANLP), pages 223–228. Tzigov Chark Bulgaria, 2001.
32. Javad Sohankar, Koosha Sadeghi, Ayan Banerjee, and
Sandeep KS Gupta. E-bias: A pervasive eeg-based
identification and authentication system. In Proceedings
of the 11th ACM Symposium on QoS and Security for

Wireless and Mobile Networks, pages 165–172. ACM,
2015.
33. Thad Starner, Joshua Weaver, and Alex Pentland.
Real-time american sign language recognition using
desk and wearable computer based video. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 20(12):1371–1375, 1998.
34. W.C. Stokoe, D.C. Casterline, and C.G. Croneberg. A
Dictionary of American Sign Language on Linguistic
Principles. Linstok Press, 1976.
35. W.C. Stokoe, ERIC Clearinghouse for Linguistics, and
Center for Applied Linguistics. The Study of Sign
Language. ERIC Clearinghouse for Linguistics, Center
for Applied Linguistics, 1970.
36. R.A. Tennant and M.G. Brown. The American Sign
Language Handshape Dictionary. Clerc Books, 1998.
37. Christian Vogler and Dimitris Metaxas. Asl recognition
based on a coupling between hmms and 3d motion
analysis. In Computer Vision, 1998. Sixth International
Conference on, pages 363–369. IEEE, 1998.
38. Andrew D Wilson and Aaron F Bobick. Parametric
hidden markov models for gesture recognition. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 21(9):884–900, 1999.
39. Xu Zhang, Xiang Chen, Yun Li, Vuokko Lantz,
Kongqiao Wang, and Jihai Yang. A framework for hand
gesture recognition based on accelerometer and emg
sensors. Systems, Man and Cybernetics, Part A: Systems
and Humans, IEEE Transactions on, 41(6):1064–1076,
2011.

Collaborative Teaching in Large Classes of Computer
Science Courses
Sanjay Goel, Suma Dawn, G. Dhanalekshmi, Hema N, Sandeep Kumar Singh, Sanchika Gupta, Taj Alam, Prashant
Kaushik and Kashav Ajmera
Department of Computer Science Engineering and Information Technology
Jaypee Institute of Information Technology
Noida, India
e-mail: {sanjay.goel, suma.dawn, dhanalekshmi.g, hema.n, sandeepk.singh, sanchika.gupta, taj.alam, prashant.kaushik,
kashav.ajmera}@jiit.ac.in
Abstract—Collaborative teaching was applied by
eight
teachers for teaching nearly 700 students in four different
sections of three different computer science courses with section
strength varying from 120-240. Different forms of collaborative
teaching were tried. Collaborative teaching at JIIT, Noida has
turned out to be successful for large classes of the strength of 100
and above.
Keywords—collaborative teaching; active teaching; computing
education; information technology education; large class;
computer science education; lecture

I.

INTRODUCTION

Worldwide there is a lot of emphasis on increasing the
Gross Enrolment Ratio (GER) in higher education. Countries
are making a lot of effort in this direction. In India, at the time
of independence, the GER was only about 1%. It has increased
from around 11% to approx. 20% in the last one decade. While
there is a huge growth in the number of colleges and teachers,
it has not kept pace with the growing number of students. The
country-wide faculty-student ratio in the higher education at
the time of independence was around 1:9. This has now
reduced to around 1:23 [1]. Even at many IITs, this ratio varies
between 1:15-1:20 and lecture class in a few courses at a few
IITs can be as large as 250-400. With successive governments
aiming to further increase the GER in higher education to
around 50 % by 2030, the faculty-student ratio is further likely
to deteriorate. While the countrywide student faculty ratio in
higher education is deteriorating, the increasing emphasis on
national/international
university/college
rankings,
accreditation, and outcome based education are placing higher
importance on quality and effectiveness of the educational
process.
The growth of engineering education has outpaced the
general growth of higher education. It has gained a lot of
popularity in the last few decades [2] and evermore so in India.
Many reasons like societal mindset and financial stability have
contributed to this. Further, an engineering education with its
problem-solving approaches is also considered as a good
program that prepares students for pursuing higher studies in
other disciplines as well. The number of students enrolled in
engineering courses has increased many folds in the last one

978-1-4673-7948-9/15/$31.00 ©2015 IEEE

decade. This “massification of higher education” has occurred
globally [3]. Tremendous growth of manpower requirement in
IT industry has fuelled more admissions in popular branches of
Computer Science, Information Technology and Electronics
and Communication Engineering. This has raised the number
of students being enrolled to such attractive courses leading to
large class sizes. Hence, traditional models of engineering
education are not sustainable anymore and innovative
approaches have to be developed for all aspects of higher
education. Some universities, academicians, and policy makers
believe that Massive Open Online Courses (MOOCs) offer a
solution to this problem. However, this paper is about an
innovative approach for the conventional mode of face-to-face
teaching in classroom.
In India, regulatory bodies like UGC & AICTE prescribe
norms for maintenance of standards for technical institutes.
AICTE [4] prescribes a maximum class size of 60 for a
section. However, this is not always feasible due to faculty,
administrative or resource constraints. Hence, large classes are
not an uncommon sight in engineering education. No doubt,
learning in small group is good; however, having large classes
may not be such a bane. Large classes have their pros and
cons. Though there are varied outlooks regarding large class
sizes in engineering education, nonetheless, it is also true that
large classes are very challenging for ensuring a good quality
of teaching-learning process.
“Learn by listening and he may retain for a day or two,
learn by watching and he may remember for some more time,
but learn by doing and he will have gained knowledge for a
lifetime” is a common saying. Such sentiments are now
increasingly shaping education systems worldwide. The
traditional way of delivering lectures is of limited educational
value. Active and interactive lectures are found to be much
more effective [5]. However, prodding many students into inclass learning activities is a herculean task. It is often a big
challenge for many students to speak even before a class of 50
students. This wax museum-like aspect has, often, wreaked
havoc even with otherwise wonderful courses.
This paper discusses an experiment in collaborative
teaching in large computer science classes. The experiment
was conducted for teaching nearly 700 students in four

different sections of three different computer science courses
with section strength varying from 120-240. The next section
of this paper gives an overview of the problems of large classes
and also discusses the approaches of collaborative teaching as
reported in the literature. Collaborative teaching was applied by
eight teachers for teaching nearly seven hundred students in
four different sections of three different computer science
courses with section strength varying from 120 to 240.
Different forms of collaborative teaching were tried. This is
discussed in the third section. The fourth section briefly
presents the results of this experiment and the papers concludes
in the fifth section.
II.

PROBLEM OF LARGE CLASS

Large classes can be seen as contrary to the objectives of
university teaching “foster the growth of individuals, and
encourage them in their individuality so that they become
independent, creative, self-motivated, critical thinkers and
learners” [6]. However, large classes have now gained larger
acceptance to accommodate the burgeoning number of students
who are able to enrol and complete their higher education.
Nonetheless, class management and teaching time face
significant negative impact, making it inept and dull for both
the students and instructors [7]. Some problems faced in large
classes, as reported in literature [7]-[10], [13], and [15] are as
follows:
¾

Large space and theatre-like setting: to physically
accommodate a huge class, lecture rooms must be
large and for visibility have a theatre-like projection
facility. This reduces eye-to-eye contact between
teachers and students.

¾

“Sage on the stage” problem: Many large classes tend
to follow the “sage on the stage” style of teaching
which is not considered an effective teaching style.

¾

Maintaining instructor enthusiasm: Consistent
enthusiasm for the course of such large student-teacher
ratio is a tasking job.

¾

Class management: Administrative tasks such as taking
attendance, maintaining class decorum and discipline
are at times difficult.

¾

Maintaining students’ interest: Attention spans of
students are difficult to maintain in large class where
there may be many distractions.

¾

Poor discussion: The large strength of the class deters
discussion that includes everyone.

¾

Declination in active learning: Active learning and
class-participation by students and their engagement in
the learning process is very important for deeper
understanding of a subject. However, maintaining
active engagement of students in a large class is
difficult and demanding both for the instructors as well
as the students.

¾

Student isolation and anonymity: With so many
students of different socio-economic-academic
backgrounds, students may feel as if they are

surrounded by strangers and many prefer anonymity
instead of chipping-in the discussion, doubt-clearing or
even in a general question-answer session.
¾

Appropriate pace and lecture content selection: In a
large class, information dissemination itself may not be
properly carried out; hence, engaging them in thinking
becomes even more difficult.

¾

Manageable and reliable assessment: Assessment must
be such that it arouses the students’ higher-order
thinking capabilities without overburdening the
instructors with too much paper checking.

¾

Feedback coordination and management: Timely
feedback can save the course from being a disaster.

These and several other issues need to be sensitively
addressed to promote a rewarding and fruitful teachinglearning experience in large classes. “The main goals for
instructors teaching large classes, apart from delivering the
course knowledge, are to make the class seem smaller than it
is, encourage students to participate more, and make
themselves accessible to the students” [7]. Many solutions to
the numerous difficulties of teaching a large class have been
reported in the literature.
A. Changing Passive Learning to Active Learning
For learning, instead of the size of the class, what should
count is the quality of teaching-learning. Many problems, so
encountered in large classes, can be reduced to manageable
limits, if not erased altogether. In order to overcome the
limitations of large classes the instructors and the students
both need to be simultaneously active. Active teaching in large
classes has been promoted through following approaches [10][13]:
¾ Instructor competency and enthusiasm: In addition to
the knowledge and experience with the subject, the
instructor’s enthusiasm in assisting the students and
helping in their learning process is very important.
¾ Organizational and teaching strategies: Proper
communication of the subject matter is very difficult in
a class size of, say, 100 or above. Hence, now using
technology within the classroom has become a norm,
not only in higher education but also at school level.
This helps not only in delivering lecture using visual
aids, but also encompasses using audience response
system and communication [14].
¾ Assignments: Graded and ungraded assignments may
be supplemented with impromptu questions, surveys
and participatory activities [12].
¾ Personalizing teaching-learning space: The student
should not feel isolated or anonymous within such large
groups and this can be done by encouraging studentfaculty interaction, discussions and debates,
encouraging questions, and sometimes giving
personalized feedback too [28].
¾ Encouraging more collaborative and cooperative
learning environment: These not only involve learning

in cohesive groups but also being available for group
discussions and giving feedback to the groups [18].
¾ Collaborative Teaching: This involves having the
assistance of teaching assistants and/or presenting the
course through team-teaching. This will not only reduce
the cognitive burden on an individual instructor but also
enrich the learning in the course.
B. Moving towards Collaborative Teaching
As an idiom goes “Two heads are better than one”, manya-times, two or even a small group of people working together
will not only have more ideas but also implement these ideas in
a better manner. Collaborative teaching involves collaboration
by multiple teachers playing complementary roles [16]-[18]. It
can also use a rotational model where collaborating teachers
keep on changing their roles [19]. Collaborative teaching
provides many pedagogical advantages over traditional
teaching methodology. It helps in creating a dynamic and
interactive learning environment [20].
The four predominant forms of collaborative teachings are
supportive, parallel, complementary and team teaching [21]. In
supportive form, the co-teacher(s) taking the supportive role
provide one-to-one tutorial assistance when necessary, while
the main teacher continues to lead the class. In parallel
teaching, multiple teachers work with different groups of
students in different subsections. It has eight variant – split
classes, station teaching (learning centers), co-teachers rotate,
each co-teacher teaching a different component of the lesson,
cooperative group monitoring, experiment or lab monitoring,
learning style focus, and supplementary instruction. In
complementary teaching, the co-teacher does something to
enhance the instruction through paraphrasing, model note
taking, pre-teaching, etc. In the most sophisticated form of
collaboration, team teaching, multiple teachers take
responsibility for all of the students in the classroom with
respect to planning the lesson as well as teaching and assessing
the students. Instead of team teaching, we shall use the term
synergetic teaching for this form of intensely collaborative
teaching.
There are many models to handle team taught courses
which can fall in any of the four predominant forms of
collaborative teaching at school as well as university level.
1) Collaborative Teaching at School Level
In collaborative teaching at primary and secondary schools
teachers have teamed up and integrated their approaches of
teaching to make the teaching process more efficient and
enjoyable for the students.
Work in [22] has implemented the concept of collaborative
teaching in Taiwan’s primary and secondary schools for the
course of Mathematics. In this model, co-teachers shared
experiences and were involved in co-generative dialogue with
each other. A review [23] accessed the effectiveness of
interdisciplinary collaborative teaching in enhancing student
achievement at Northwest Regional Educational Lab at
Portland. It identified various areas which are positively
affected by collaborative teaching - self-concept, happiness
with school, attitude toward teachers, interest in subject matter,

sense of personal freedom, sense of influence on the school
environment and self-reliance. Work in [24], [25] discussed an
effective collaborative teaching using a special educator and a
general educator to accommodate the needs of students with
and without disabilities.
2) Collaborative Teaching at University Level
Problems of large classes are usually handled using parallel
teaching by dividing classes in several sections. Each section is
handled by individual instructor. All instructors collaborate
together from course design to final evaluation and grading
[21]. In literature, most cases of collaborative teaching at
University level have followed supportive [18], [20], [26]-[28]
and synergetic forms [19], [26], [27], [29]-[33].
Complimentary teaching is evident in combination with other
three forms [27], [33].
Authors in [28] experimented with supportive form of
collaborative teaching model employed at a regional university
of Australia for a large undergraduate marketing course. One
instructor focused on lecture delivery and the other on assisting
and monitoring students. Instructors swapped roles depending
on the lecture activity. In another model, role of co-teacher was
that of a careful observer and an exemplary student [18], [20].
As an exemplary student, co-teacher actively participated in
discussions to create high level debates in class.
Work in [29] showed synergetic form of collaborative
teaching approach in interdisciplinary courses. Two instructors
one from the Computer Science department and another from
the Writing Program in the College of Humanities
collaboratively taught senior software seminar course. An
integrated framework with a structural paradigm was adopted
in [31] wherein collaborative teaching was introduced as a
language intervention in the course. Performance of students
was evaluated before and after intervention. Course topics
division based on individual preferences and lecture materials,
was another model adopted in [32]. In “Rotational
Model” discussed in [19], one instructor is always present
every time and a series of instructors keep changing depending
upon course topics that fall within their specialty. In another
work [30], guest lecturers were used as part of collaborative
teaching model to expose students’ to a variety of topics from a
different backgrounds and teaching styles.
Another study was done in [26] on challenges faced in
collaborative teaching activities and its effectiveness which
identified six types of collaborative teaching structures like
“One teach other observe, One teach other drift, parallel
teaching, station teaching, alternative teaching and team
teaching”. Work in [27] adopted two different Mentor-mentee
models. In first model, during first half, mentor taught the class
and mentee observed, in the second half, mentee taught the
class and mentor was observing. In second model, mentor
taught the class up until the first examination with mentee
observing. Subsequently, class was taught by the mentor or the
mentee.
C. Evaluation of Collaborative Teaching Models
Collaborative teaching approaches have been evaluated
both from student as well as teachers perspective.

1) In Terms of Student Perspective
Collaborative teaching creates increased dialogue and
participation of students in discussion of class. It leads to two
or three level fair grading system for evaluation of students.
Students have better understanding of the course and their
connectivity with other related courses. Instructor’s debate
leads to understanding of different disciplines for students [19],
[20]. Results from Stanford University [19] state that
collaborative teaching increases student participation and
improves student learning outcome. It also encourages students
to have variety of perspective on a topic to make valuable
contributions in class discussions. Collaborative teaching in
interdisciplinary courses even enhances students’ technical
writing and communication skills with improvement in
programming concepts [23], [29].
In case of physically challenged students, lowering the
student-teacher ratio in co-taught classrooms offers more
determined and individual instruction to students [24], [25].
Differences in lecturing style, quality of lecture contents,
and the perceived lack of continuity and cohesiveness in lecture
topics were negative aspects reported in collaborative teaching
[30]. The critical success factor behind the team taught courses
is the composition of a team [21], [28], [34] for better teaching
and learning experience.
2) In Terms of Faculty Perspective
Teamwork enhances the professional competence of the
teachers and helps them to analyze things from different
viewpoints [21]. Team teachers become expert learner and
learn new approaches from their colleagues [27], [34].
Instructors develop compatible teaching style, spend more time
in planning and coordinating various activities, e.g., selection
of course topics, grading systems, presentation and delivery,
and agreement on distribution of the course load [32].
Collaborative teaching gives instructors the opportunity “to
teach in a different way, and to learn in a different way”. It
allows instructors to improve their academic skills and develop
new topics for research [19]. The compatibility of the coteachers and the discrepancy between their approaches to
teaching, their personal characteristics and effectiveness of
collaborative teaching has overall effect on the team taught
courses. The effective collaborative teaching in college can
enhance the learning of new faculty [27], [33].
III.

COLLABORATIVE TEACHING AT JIIT

The department of CSE & IT at JIIT has been using
collaborative teaching in the form of multi-section parallel
teaching and topic sharing in single section for the last several
years. However, in the spring semester 2015, for the first time,
after two weeks of the start of the semester, multiple teachers
collaborated to conduct almost all lecture class of a few
selected courses. The experiment was conducted in three
different courses of B.Tech programmes. We have
experimented in two out of seven sections of Data structure
course with class strength of approximately 120 students each
in the first year, Microprocessor and microcontrollers course
with class strength of 240 students in the second year and
Computer Games with class strength of 211 students in the
final year. Eight teachers participated in this form of

collaboration to teach approx. 700 students. The different
collaboration styles of Supportive, Parallel, Complimentary
and Synergetic teaching were applied in this collaborative
teaching experiment. Number of lectures conducted in
collaborative style in each section varied from 27 to 30. For
each lecture, the collaborating teachers worked together before
as well as after the class. It took around an hour for
collaborative planning the lecture sequence and class activities
and around 15 minutes after the class for review.
A few examples of four forms of collaborative teaching in
these three different computer science courses are given below:
A. Supportive
¾ In Data structure course, one teacher explained the
concept of stacks and other one supported by providing
the applications of stack in various problems. The
applications included checking parenthesis matching,
expression conversion like infix to postfix, infix to
prefix, postfix to prefix and, postfix expression
evaluation, etc.
¾

In Microprocessor and microcontrollers course, one
teacher gave practical demonstration of MASM I/O
programming with examples. Based on this topic,
problems were given to students. The other teachers
interacted with students off the stage clearing their
doubts and helped students in problem solving.

¾

In Microprocessor and microcontrollers course, 8086
addressing modes were presented by the main faculty.
The supporting faculty intervened with questions that
helped in clarifying the topic. Further, the topic was
concluded by supporting faculty with mapping of
logical address to physical address.

¾

In Microprocessor and microcontrollers course, the
concept of Programmable Peripheral Interface (PPI)
device 82C55 and its mode programming was taught
by the main faculty, while supporting faculty provided
real-life practical applications and demonstration of
PPI modules with logic controllers to students.

This model was also applied for tasks such as system set-up
and support, attendance record maintenance, maintaining the
general class discipline and decorum
B. Parallel
¾ In Data structure course, the problems related to
queues such as simulation of multiple queues and
priority queues was given in class. The students were
divided into two subsection and two teachers in
parallel guided the subsections. The students in the
different groups came up with different approaches for
solving the same problem. Similarly, the Backtracking
problems like Rat in a Maze and Eight Queens
problems were discussed in parallel style of
collaborative teaching.
¾

In Microprocessor and microcontrollers course, delay
generation using 8051 instructions for particular crystal
frequency problem was given to two subsections of

students. One teacher supported each subsection. The
first group to arrive at the correct solution was declared
as the winner.
¾

¾

¾

¾

In Microprocessor and microcontrollers course, a
simple problem of checking odd or even was given for
different subsections with different logic like using
LSB check, using shift or rotate instruction and
division to check a remainder. One teacher supported
each subsection. At the end of problem solving, it was
shown that using division method to check odd or even
is a bad approach, as it consumes many registers and
complexity of problem also increases. Whereas using
LSB check is a simpler approach as assembly language
is well equipped for bit manipulations. This kind of
comparative study helped the students to better
understand and compare the programming concepts.
In Computer games, the topic of testing and debugging
required five lectures. For each of these lectures,
mostly parallel and sometimes complementary and
supportive teaching styles were adopted. While
teaching in parallel mode, the class was divided into
two subsections. Latter, multiple groups were formed
for discussions which were assisted by both the
instructors. At the start of the topic, testing and
debugging methods were shown to the students. While
one faculty demonstrated using the computer, the other
faculty (on stage) explained the manner in which
testing and debugging was needed to be implemented.
After demonstration, both the instructors adopted
complementary teaching style for enhancing the
learning by encouraging more discussions amongst
students.
The topic of game optimization was discussed for over
3 classes and the content included theoretical concepts
as well as the practical example of the memory and
CPU usage while running the game. For this topic,
parallel and synergetic lecturing styles were
interleaved. While one instructor explained the
generalized best practices for optimizations, the other
was involved in showing how these may be
implemented in the game modules to small groups of
students. The students also pointed out the required
optimization in the games that they usually play and
also critiqued other students’ games. Further, the CPU
and memory-usage snapshots of some popular as well
as student-developed games were shown and profiled
in Unity game engine. The students could appreciate
and pin-point their modules wherein major
optimization was required. In total 5 games were
profiled and optimized.
The topic of game design pattern was conducted
mostly as a hands-on experience in the class for
making new game plays. For this, parallel teaching
style was used as the class was divided into two subsections which were further divided to two groups.
After explaining the design patterns in the class, the
students were asked to modify the patterns to create
new game plays. Each subsection was given a theme

within which the two groups were to follow two
different game design patterns and both the instructors
moved in the class to see how students are making new
game plays. Hence, many spin off game plays were
developed in the class itself.
C. Complementary
¾ In Data structure course, one teacher explained the
concept of Graphs and its representation. To enhance
this explanation, the other faculty explained the same
concept in a different perspective. Both instructors
supported their discussion in terms of different
practical application such as representing friend’s
network as graph, transport system as graph etc. This
gave more insight to the students.
¾

In Microprocessor and microcontrollers course, first
attempt to make students understand the operation of
XLAT instruction is generally difficult. 8086 XLAT
instruction was taught by one instructor. This was
followed by a comparison with 8051 MOVC
instruction (taught in an earlier class) by another
faculty. This helped in developing a deeper and
quicker understanding of it as they were already
familiar with 8051.

¾

In Computer games, the topics like game development
life cycles and game play design were taught in the
complementary style as one instructor explained
storyboarding while the other stressed on game play.

¾

In Computer games, 3D character animation was
discussed for over 3 lectures, the complementary style
of teaching was followed. The theoretical aspects
including the 3D designing, transformations, interrelationship with 2D and rendering pipeline were
introduced by one instructor, and subtopics such as
kinematics and inverse kinematics and their
implementation including animation designing were
explained by the other instructor. While one instructor
was teaching or demonstrating, the other was involved
in minor problem solving with small groups of
students.

D. Synergetic
¾ In Computer games the topic of game development
was discussed following the synergetic model. The
discussions included Unity3D game engine and various
best practices for the development after the designing
stage. These lectures were delivered in synergetic
mode as both the instructors took the stage, and
explained and demonstrated the game in a step-by-step
manner. Both instructors also explained different
aspects of the development tools.
¾

Often, instead of “teaching” a topic, such as critiquing
a game, and finding flaws within them, the instructors
adopted a debate-rebuttal style which followed the
synergetic format of teaching.

IV.

RESULTS

The impact of this experiment in collaborative teaching has
been analyzed through reflections by 8 participating teachers as
well as feedback by students through informal discussions and
a structured survey at the end of the semester.
A. Student’s Perspective
Initial reaction from the students was that of bewilderment
as they had never experienced such form of teaching before.
Having two or more teachers teaching them simultaneously
was not only exciting but also confusing at times. This required
explanation on the part of instructors. As the semester
progressed, the students also accepted this form of teaching to a
large extent, if not whole heartedly. Discussions with students
revealed that they felt benefited from the multiple perspectives
of more than one teacher on the same topic. Discussions
between the teachers and amongst the students themselves
enriched the learning to a greater extent. It also improved the
faculty accessibility, resulting in enhanced doubt clearing.
Further, at the end of the semester, a questionnaire was
administered to get student’s reaction to the collaborative
teaching. The questionnaire asked them to compare the coteaching class to typical non-co-taught class of approximately
same student strength with respective to various parameters
such as - their subject-specific and generic learning, problemsolving ability, soft skills, level of in-class and out-of-class
learning, participation in collaborative activities and interaction
with faculty. These questions were answered using 5 options.
A large percentage of students reported a very positive
feedback to the collaborative teaching style on various
parameters. The results are shown in Table I.
From Table I, it can be inferred that, there was a greater
appreciation of collaborative teaching in Computer games and
Data structures. 92% of the students in Computer games and
71% in Data structure course felt an increase in the learning
from the collaborative teaching. In all the three courses only a
small fraction of students, 5%, 20% and 0% in Data structures,
and Microprocessor and microcontroller, and Computer
Games respectively felt that collaborative teaching had a
negative impact on learning. The students found the integration
of diverse learning from two different teachers useful. The
benefits reported by students are as follows:
¾

More opportunities for clarification of doubts,

¾

Enhanced learning and understanding due to different
approaches used for problem solving.

¾

If any part of the topic-content was not highlighted by
one instructor, the other helped in identification of such
minor lapses and rectified it.

¾

Classes were conducted even if a particular faculty was
absent or busy as the others could easily take-over for
that particular duration.

¾

Decreased monotony of classes as students interacted
from two or more different instructors simultaneously.

However, the overall feedback showed an increase in active
participation and enriched learning experience. The students

also showed enthusiasm towards having
collaborative teaching classes in future.

more

such

TABLE I.

STUDENT FEEDBACK ON COLLABORATIVE TEACHING ON
VARIOUS PARAMETERS (IN %)
Parameters

Subject specific
technical learning
Generic technical
learning
Generic Problem
Solving learning
Generic soft skills
learning
Level of student
participation in
class
In-class student
collaborative
activities
Students faculty
interaction in & out
of class
Overall assessment
of the co-teaching
experience

Students (%) who selected the options in:
1. Data Structures
2. [Microcontroller & Microprocessor]
3. (Computer Games)
Definitely
Almost
Definitely
Increased/
same
Decreased/
Increased
Decreased
52
45
3
[31]
[35]
[35]
(88)
(14)
(0)
45
48
7
[22]
[58]
[20]
(74)
(23)
(3)
7
33
60
[27]
[62]
[11]
(19)
(3)
(78)
48
45
5
[27]
[47]
[25]
(72)
(22)
(6)
52
38
10
[11]
[53]
[36]
(73)
(22)
(5)
50
40
10
[27]
[40]
[33]
(72)
(23)
(5)
62
32
6
[36]
[44]
[20]
(76)
(20)
(4)
24
71
5
[49]
[31]
[20]
(8)
(92)
(0)

B. Faculty Reaction
Initially the collaborative teachers themselves were little
uncomfortable, which might have contributed to the confusion
with students. However, not only this gave an opportunity to
build a rapport among them but also helped them in
understanding a different point-of-view for a particular topic
and gain more insight into it. Also, there was an increase in the
learning activity experienced by the co-teachers that helped
them to grow, reflect and deepen content understanding and
improve its subsequent delivery, while also providing students
with a variety of effective instructional methods.
The collaborative teaching model though has several
advantages over single teacher taught courses, it also has many
challenges and pitfalls that need to be taken care of during the
design of the course and lecture session. In large classes, the
form of interactivity has to be planned before, so that students
are not distracted. Even the supporting teacher has to correlate
to topic being taught and the terminology being used by the
other faculty, so that the students are not confused with the use
of dual terminology. The students sometimes faced problem in
terms of switching of teacher. This may result in break of flow
of delivering lecture by the instructors. Thus certain challenges
that need more attention include pre-synchronization of the
content delivery and smooth transition between instructors and
topics.

Since the experiment was being performed for the first
time, it was a great learning experience in terms of subject
learning as well interpersonal understanding. Though,
sometimes smooth and well-coordinated transition during
lecture delivery was not always without initial discontinuities,
the collaborative teachers could mutually assist, harmonize and
enrich their teaching and learning.
V.

CONCULSIONS AND FUTURE WORK

Despite of some initial hesitation shown by the
collaborative teachers and the students, this experiment has
turned out to be successful for large classes of the strength of
100 and above. From the survey it is inferred that students have
felt greatly benefited from this collaborative teaching activity
and want it to be repeated in other courses as well. As a future
work, the department has planned to repeat this experiment in 7
different courses undergraduate as well as graduate level
courses involving more than 25 faculty members and over
1,500 students in the coming fall semester.

[15]
[16]
[17]
[18]

[19]
[20]
[21]

[22]

[23]

REFERENCES
[1]
[2]

[3]

[4]

[5]

[6]
[7]

[8]
[9]
[10]
[11]
[12]
[13]
[14]

“All India Survey on Higher Education”, Dept. of Higher Education,
Ministry of Human Resource Development, Government of India, 2013.
Benbow, J., Mizrachi, A., Oliver, D., Said-Moshiro, L., “Large Class
Size in the Developing World: What Do We Know and What Can We
Do?” American Institute for Research: Educational Quality
Improvement Program, 2009. http://www.equip123.net/docs/E1-Large
Classrooms.pdf
Mohamedbhai, G., “The effect of massification on higher education in
Africa”. http://www2.aau.org/whge/scm/meetings/mai08/adea/
study_massification.pdf
AICTE Approval Process Handbook, (2013-2014).All India Council
of
Technical Education, New Delhi, India. http://www.aicteindia.org/downloads/ performance.pdf
Goel, S., “Do Engineering Faculty Know What’s Broken?”, The
National Teaching & Learning Forum, Vol. 15 No. 2, pp 1-5, James
Rhem & Associates, USA,2006.
Gedalog, A. J., “Teaching Large Classes”, Focus on University
Teaching and Learning, Vol 8, No. 3, May 1999.
“Large Classes: A Teaching Guide: Personalizing the Large Class”,
Centre for Teaching Excellence, University of Maryland, 2008.
http://www/cte.umd.edu/library/teachinhLargeClass/guide/ch4.html
“Teaching and Assessment in Large Classes”, Teaching and Educational
Development Institute, The University of Queensland Australia, 2001.
Aagard, H., Bowen, K., Olesova, L., “Hotseat: Opening the Backchannel
in Large Lectures”, Educause Qarterly, 2010.
Markwell, D., “Improving Teaching and Learning in Universities”,
Business/ Higher Education Round Table, Vol. 18, pp: 1-5, Nov 2003.
CADQ
guide:
Teaching
large
groups.
http://www.ntu.ac.uk/adq/document_uploads/teaching/137815.pdf
“Strategies for Teaching Large Undergraduate Classes”, Hanover
Research, 2010.
Ives, S.M., “A Survival Handbook for Teaching Large Classes”, Faculty
Center for Teaching, UNC Charlotte.
Oliver, R., “Using mobile technologies to support learning in large on
campus university classes”, ICT: Providing choices for learners and

[24]

[25]

[26]
[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

learning:
Proceedings
‘ascilite’
Singapore,
2007.
http://www.ascilite.org.au/conferences/singapore07/procs/oliver.pdf
Burnett, L., Krause, Kerri-Lee., “GIHE Good Practice Guide on
Teaching Large Classes”Griffith University. www.griffith.edu.au/gihe
Sharpe Co-Teaching Manual MSDE Grant, Towson University, pp 1-26,
2012.
Shumway, Larry K., Gallo, G., Dickson, S., Gibbs, J., “Co-Teaching
Handbook”, Utah State Office of Education, pp 1-39, 2011.
Anderson, Rebecca S., Speck, Bruce W., “On what a difference a team
makes: why team teaching makes a difference”, Teaching and Teacher
Education,Vol. 14, No. 7, pp. 671-686, 1998.
“Speaking of teaching: The centre for teaching and learning”, Stanford
University, Newsletter, Vol.16, No.1, pp 1-4, 2006.
Leavitt, Melissa C., “Team Teaching: Benefits and challenges”,
Stanford Fall 2006 Newsletter, Vol. 16, No. 1, pp 1-3, 2006.
Thousand, Jacqueline S., Villa, Richard A, Nevin, Ann I., “Many Faces
of Collaborative Planning and Teaching” Theory into Practice, Vol. 4,
No. 3, pp 239–248, 2006.
Jang, Syh-Jong, “Research on the effects of team teaching Upon two
secondary school teachers”,Chung-Yuan Christian University, Taiwan,
Educational Research, Vol. 48, No. 2, pp. 177 – 194, 2006.
Kathlee, C., “Effects of Interdisciplinary Team Teaching. Research
Synthesis”, Northwest Regional Educational Lab., Portland, Oreg, pp 116, 1982.
Magiera, K., Simmons, R., “A Co-Teaching Model: A Response to
Students with Disabilities and Their Performance on NYS State
University of New York”, SAANYS Journal, Vol. 34, No. 2, pp 1-5,
2005.
Malian, I., McRae, E., “Co-Teaching Beliefs to Support Inclusive
Education: Survey of Relationships between General and Special
Educators in Inclusive Classes”, Electronic Journal for Inclusive
Education Vol. 2, No. 6, pp 1-19, 2010.
“The Effectiveness of the Co-Teaching Model”, Hanover Research,
Washingtom DC, pp.1-20, 2012.
Carpenter, J., Meng, D., Ponder, N., Schroder, B., "Team teaching
merged sections as a way of mentoring faculty", Frontiers in Education
Conference, pp F3F-1-6, 2000.
Yanamandram, V., Noble, G., “Student experiences and perceptions of
team-teaching in a large undergraduate class” Journal of University
Teaching and Learning Practice, Vol. 3, No. 1, pp: 49–66, 2006.
Louise, R., Hollaar, L., "Co-Teaching Engineering and Writing:
Learning about Programming, Teamwork, and Communication." Issues
in Integrative Studies Vol. 1, No. 15, pp 125-147, 1997.
Hanusch, F., Obijiofor, L., Volcic, Z., “Theoretical and Practical Issues
in Team Teaching: Large Undergraduate Class”, International Journal of
Teaching and Learning in Higher Education, Volume 21, Number 1, pp
66-74, 2009.
Kamai, R., Badaki, J. V., “Structuring Team Teaching to Enhance
Teaching and Learning of Literature-in English and English Language in
Secondary Schools”, Journal of Education and Practice , Vol 3, No 13,
pp 127-133, 2012
Azemi, A., D'Imperio, N., "New approach to teaching an introductory
computer science course," Frontiers in Education Conference , pp S2G1-6, 2011.
Riegner, Dawn E., “ Team Teaching in College”, Center for Teaching
Excellence, United States Military Academy, West Point, NY, pp 1-6,
2007.
Conderman, G., “Middle school co-teaching: Effective Practices and
Student reflection”, Middle school Journal, pp 25-31, 201

The update complexity of selection and related problems

arXiv:1108.5525v1 [cs.DS] 29 Aug 2011

Manoj Gupta
Deptt. of Comp. Sc.,
IIT Delhi, New Delhi.

Yogish Sabharwal
IBM Research - India,
New Delhi.

Sandeep Sen
Deptt. of Comp. Sc.,
IIT Delhi, New Delhi.

October 8, 2013
Abstract
We present a framework for computing with input data specified by intervals, representing uncertainty
in the values of the input parameters. To compute a solution, the algorithm can query the input
parameters that yield more refined estimates in form of sub-intervals and the objective is to minimize
the number of queries. The previous approaches address the scenario where every query returns an exact
value. Our framework is more general as it can deal with a wider variety of inputs and query responses and
we establish interesting relationships between them that have not been investigated previously. Although
some of the approaches of the previous restricted models can be adapted to the more general model, we
require more sophisticated techniques for the analysis and we also obtain improved algorithms for the
previous model.
We address selection problems in the generalized model and show that there exist 2-update competitive algorithms that do not depend on the lengths or distribution of the sub-intervals and hold against
the worst case adversary. We also obtain similar bounds on the competitive ratio for the MST problem
in graphs.

1

Introduction

A common scenario in many computational problems is uncertainty about the precise values of one or
more parameters. Many different models have been considered in the database community for dealing with
uncertain data. In one of the commonly used models, the uncertain parameters are represented by probability
distributions (for a comprehensive survey, see[AY09]). In another model, the uncertain parameters are
represented by interval ranges, wherein the parameter may take on any value within the specified interval
(see [KT01]). In this paper, we focus on the latter model. More formally, we consider the model wherein
we want to compute a function f (x1 , x2 . . . xn ) where some (or all) xi ’s are not fully known. The xi ’s are
typically known to lie in some range (interval). Any assignment of xi = x′i consistent with the known range
of xi is a feasible realization. The algorithm can make queries about xi . This problem has been studied
before [KT01, HEK+ 08]. A common assumption made in the existing literature is that the exact value of xi
is returned by a single query. However, in many applications, a query about xi may only yield a more refined
estimate of the xi . As a matter of fact, in many such applications, it is not even possible to obtain the exact
value of the parameter. As an example, consider the case of handling satellite data such as maps. Due to
the large amount of data involved, the data is often stored hierarchically at different scales of resolutions.
Typically the data is presented at the highest level of resolution. Depending on the area of interest, data
may be retrieved for the next level of resolution for a smaller area (zoom in) by performing a query. Now
consider a query to find the closest hospital. Based on the highest scale of resolution, the distances to the
hospitals can be determined within a certain range of uncertainty. If the closest hospital cannot be resolved
at this level, then further queries are required for certain hospitals to determine which amongst them is the
closest. These queries proceed down the hierarchical scales of resolution until it is resolved which is the
closest hospital.

1

Let us illustrate this model using the problem of finding minimum when the exact values are not known
but each element is associated with a real interval [ℓi , ri ]. Consider the three elements x1 = [3, 17], x2 =
[14, 19], x3 = [15, 20]. Clearly any of these can be the minimum element as these are mutually overlapping
intervals. Suppose a query returns the exact value, then with three queries, we obtain the complete information and the problem is trivially solved. But the interesting question is - are three queries necessary ?
Suppose our first query yields that x1 = 10, then clearly we do not need to make any further queries. On the
other hand, the query may yield x1 = 16, so that we are forced to make further queries. In a more general
situation, where a query may return a sub-interval, we may obtain x1 = [8, 16] that doesn’t yield any useful
information about the identity of the minimum element. On the other hand, if the query returns [8, 10],
then we can conclude x1 to be the minimum even though we do not know the exact value of x1 .
It is natural to compare the number of queries made by the algorithm w.r.t. a hypothetical OP T
which can be thought of as a non-deterministic strategy that makes the minimum queries for any feasible
realization of the input. Moreover, the algorithm must contain a certificate of correctness of the final answer,
viz., that no more queries are necessary regardless of the number of unresolved parameters. This also brings
up the related verification problem, i.e., given an incompletely specified problem, does it contain sufficient
information for a solution to be computed (without further queries).

1.1

Related Previous Work

Kahan [Kah91] described a technique for maintaining data structures for online problems like flight-path
collisions using predictive estimates to obtain higher efficiency. The estimates could be used to prune objects
that couldn’t provably affect the solution and only those critical objects were updated that could affect the
answer. Kahan’s work laid the foundations for later work on kinetic data structures but in his paper,
he focussed on describing a framework for minimizing updates of critical objects. Kahan compared the
efficiency of his data structures with respect to a non-deterministic optimal algorithm, or more specifically,
the competitive ratio in the online setting. If our algorithm makes qS (n) queries for an input S of size n,
then it has competitive ratio c 1 iff for some constant α > 0,
qS (n) ≤ c · OP T (S) + α
where OP T may be thought of as a non-deterministic algorithm (coined as lucky in [Kah91]) Note that OP T
has an unfair advantage in being able to guess the optimal sequence of queries and ensure that it can be
verified in collusion with an adversary controlling the output of the queries.
For instance, if the given intervals are x1 = [2, 6], x2 = [2, 6], x3 = [2, 6], i.e., all of them are identical, OP T
may guess the answer to be x3 and if the query yields x3 = 2, then it is verified. On the other hand, an
algorithm has no means of distinguishing between the xi ’s. Even use of randomization does not appear to
provide any significant advantage in this scenario. Kahan [Kah91] tackled this issue (without acknowledging
as much) by changing the problem definition to that of reporting all values that are equal to the minimum.
Khanna and Tan [KT01] also used the competitive ratio as a measure of efficiency of their algorithms
but their parametrization didn’t yield O(1) bounds. Their algorithms for selection was related to the clique
number (maximum clique size) of the input. They compare with Non-deterministic optimal and show that,
no on-line algorithm can achieve a better competitive ratio than the clique number.
A somewhat different model was used by Erlebach et al.[HEK+ 08], who showed how to compute an exact
minimum spanning tree for graph with interval data using minimal number of queries. The final answer is
a combinatorial description (in this case a spanning tree) and not necessarily the weight of the spanning
tree. Erlebach et al.[HEK+ 08] proved that their algorithm has competitive ratio 2 when the edge weights
are initially specified as open intervals. One limitation of their result is the critical use of the property of
open intervals which is used to weaken the advantage of OP T in guessing and verifying the answer. Their
results on constant competitive ratio do not hold for closed or semi-closed intervals.
A recent motivation for this line of work came from caching problems in distributed databases, (Olston
and Widom [OW00]), where local cached copies are used for faster query processing where the cached values
are intervals that are guaranteed to contain the actual value called the master value. Their work showed
1 So

strictly speaking, the algorithm could take exponential time but may have a bounded competitive ratio.

2

O
C
OC
P
OP
CP
OCP

O
Category-1
(Note α)
(Note α)
trivial
Category-2
(Note α)
(Note α)

C
(Note α)
Category-1
(Note α)
(Note α)
Category-2
(Note α)

OC
(Note α)
(Note α)
Category-1
(Note α)
(Note α)
Category-2

P
(Note α)
(Note α)
(Note α)
OP-P
Category-3
Category-3

OP
(Note α)
(Note α)
(Note α)
OP-OP
(Note α)
(Note α)

CP
(Note α)
(Note α)
(Note α)
(Note α)
Category-3
(Note α)

OCP
(Note α)
(Note α)
(Note α)
(Note α)
(Note α)
Category-3

Figure 1: Models for studying uncertain data problems (see note for α below). The allowed input types
listed along the rows and the query return types listed along the columns. (The pure input point model is
trivial as no queries are required).
trade-off between the number of queries and the precision ∆ of the actual answer. This model was further
explored in the work of [FMP+ 03, FMO+ 03] that tackled fundamental problems like median-finding and
shortest-paths. They distinguished between the offline (oblivious) and online (adaptive) queries including
weighted versions where queries could have varying costs for different intervals. Unlike the previous work,
they compared their efficiency with respect to a worst case optimal rather than a non-deterministic inputspecific optimal. Therefore their results cannot be compared effectively with the previous work. Other
approaches like [AH04, KZ06] minimize the worst case deviation from actual values or minimizing queries
to get improved estimates of the expected solution when the distribution is known [GGM06, GM07].

2

Our contributions

In this paper, we generalize the query model in several directions. We classify models based on the types
of the inputs allowed and the return type of the queries. The input may specify a combination of points
(P), open intervals (I) and/or closed intervals (C). This leads to 7 variations , namely, O, C, P, OC, OP,
CP and OCP. Similarly queries on intervals (open/closed) may yield points (P), open intervals (I) and/or
closed intervals (C)2 . This also leads to seven variations. These models are specified in Figure 1. We denote
the models by X-Y where X denotes the type of the input allowed in the input instance and Y denotes the
query return types where X and Y can take values from O, C, P, OC, OP, CP and OCP (here the literals
O, C and P correspond to open intervals, closed intervals and points respectively). Thus for instance OP-P
denotes the model wherein the input can consist of open intervals as well as points and the queries can only
return points.
(Note α): Although there are 49 models possible, many of them are unnatural as they can lead to a
change of the input type after some initial queries. The framework of such models can be covered under the
framework of another suitable model. For instance, a problem under the O-P model would convert to OP-P
model after a single query and is thus better studied under the OP-P model. Similarly, the OC-C model can
be covered under the OC-OC model.
We categorize the valid models into 5 different categories (See Figure 1). The competitive ratios are
based on this categorization of the models. Category-1 corresponds to the models where the input and query
return types are only intervals (O-O, C-C, OC-OC models). Category-2 corresponds to the models where the
input may contain points by the queries only return intervals (OP-O, CP-C, OCP-OC models). Category-3
corresponds to the models where the input may contain closed intervals and the query may return points.
The other two categories correspond to the OP-P and OP-OP models themselves.
Our main results can be summarized as follows
1. We first generalize the models to practical scenarios wherein queries may return sub-intervals as answers
rather than exact values. The sub-intervals need not have any properties with respect to lengths or
distributions. In other words, with further queries, we obtain increasingly refined estimates of the
2 We can also handle semi-closed intervals but we have avoided further classification as they don’t lead to any interesting
results.

3

values until sufficient information has been obtained, i.e., the verification problem can be solved. We
show that the witness based approach used in the previous models can be adapted to the models
considered in this paper. More specifically, we establish interesting relationships between the various
models (see Figure 2).
2. We study the selection problem of finding the k th smallest value and present update competitive
algorithms with different guarantees for the different models for this problem. We also study the update
complexity of minimum spanning tree problem under the different models that is closely related to the
extremal selection problem (finding the heaviest edge in a cycle – also called the Red rule).
3. We also show that by deviating from the witness based approach studied in prior literature, we can
actually obtain improved bounds for the selection problem. These algorithms attain an additive overhead from optimal, that is similar to a competitive ratio of unity for some cases and are interesting in
their own right.
4. Given that closed intervals have not been successfully handled in prior literature[HEK+ 08] leading to
unbounded competitive ratios, is it possible to characterize the problem more precisely? For instance,
do we run into the same issues if we allow queries to return intervals? One approach for addressing
issues with closed intervals is to output all the optimal solutions[Kah91]. It can be quite expensive to
output all the solutions. Is there an alternate framework that addresses the issues with closed intervals
without determining all the solutions.
We show that this problem is a characteristic of models that allow closed intervals in the input and
points to be returned in the queries. We extend our models to handle closed intervals by using the
notion of lexicographically smallest solution (in case multiple solutions exist). This is a natural version
in many problems where the initial ordering is important and we will show later that this has the
desired effect of limiting non-deterministic guessing powers of OP T .
Another interesting variation could be assigning cost to a query depending on the the precision of the
answer given but we have not addressed this version in this paper. There is a growing body of work that
addresses the problem of computing exact answer with minimal queries [BEE+ 06, BHKR05] and coping with
more generalized queries is an important and fundamental direction of algorithmic research.
Problem

Extremal
selection

K-selection

MST

Competitive
ratio
OP T + 1
OP T + 1
2 · OP T
2 · OP T
OP T + 1
t · OP T
OP T + k
2 · OP T
2 · (OP T + k)
2 · OP T
2 · OP T
OP T + C
2 · OP T
2 · OP T

Models

Comment

Source

OCP-P
OP-P
Category-1,2 & OP-OP
Category-3
OCP-P
CP-P
OP-P
Category-1
OP-OP
Category-3
OP-P
OP-P
Category-1,2 & OP-OP
Category-3

Report all solutions
Value

Kahan [Kah91]
this paper
this paper
this paper
Kahan [Kah91]
Khanna-Tan [KT01]
this paper
this paper
this paper
this paper
Erlebach et al.[HEK+ 08]
this paper
this paper
this paper

lex first
Report all solutions
t = clique no.
Value, ≤ k · OP T
element
Value, lex first
C ≤ OP T C = no. of red rule
lex first

Figure 2: Known results in prior literature and our new results

3

Problem Definition

We consider a problem P where we are given an instance P = (C, A) that consists of
• an ordered set of data C = {c1 , c2 , . . . , cn } called a configuration; and
4

• an ordered set of data A = {a1 , a2 , . . . an } called areas of uncertainty such that ci ∈ ai ∀i.
The configuration C is not known to us – only the areas of uncertainty, A, are known. As an example
consider the problem, P, of finding the index of the minimum element. An example instance is given by
Pex = (C, A) where C is the ordered set of points C = {3, 7, 10} and A is the ordered set of intervals (areas
of uncertainties) A = {(2, 6), (5, 8), (9, 11)}.
We focus our discussion to problems where the input is Real data. Thus, the configuration consists of
points on the Real line ℜ, and the areas of uncertainty may be intervals on the Real line. The concepts can
be extended to higher-dimensional problems.
Verifier: We are also given a verifier V for the problem P, that takes as input the areas of uncertainty,
A and returns whether a solution of the problem P can be determined from A or not. For the example
instance, Pex , described above, the verifier would return false as it cannot determine a solution from the
given areas of uncertainty. However, if the intervals were A = {(2, 5), (6, 8), (9, 11)}, then the verifier would
return true as clearly the first interval has to contain the minimum.
Order-Invariance: An important characteristic of the problems we study is that the result of the
verifier is only dependent on the ordering of the areas of uncertainty. More formally, consider two instances
P = (C, A) and P ′ = (C ′ , A′ ) where A = {a1 , a2 , . . . , an } and A′ = {a′1 , a′2 , . . . , a′n } for the same problem
P. We say that P and P ′ are order-equivalent if for every pair of indices i, j ∈ {1, 2, . . . , n}, it can be
determined that ai ≤ aj iff it can be determined that a′i ≤ a′j . We say that a problem P is order-invariant if
the verifier returns the same value for any two order-equivalent configuration instances. It is easy to verify
that the problems such as selection (finding minimum, finding k th -minimum) and minimum spanning tree
are order-invariant.
Update operations: We are allowed to perform update operations on the areas. Performing an update
operation on area ai results in knowledge of the area to a greater degree of accuracy. More precisely, performing an update operation on ai in the instance P = (C, A), where A = {a1 , a2 , . . . , ai−1 , ai , ai+1 , . . . , an } results
in another instance P ′ = (C, A′ ), where A′ = {a1 , a2 , . . . , ai−1 , a′i , ai+1 , . . . , an } such that a′i is completely
contained in ai . An important characteristic of the models that we consider is that the results of updates
on an area are independent of updates on any other area. That is, given a multi-set S = {i1 , i2 , . . . , ik } of
indices of the areas, applying updates on the corresponding areas results in the same instance, irrespective
of the sequence in which these updates are applied. We refer to this as the update independence property.
Solution: Our goal is to solve the problem P by performing minimum number of updates, i.e., perform
the minimum number of updates that result in an instance for which the verifier returns true. For a problem
instance P = (C, A), a solution, S, is defined to be a multi-set of indices {i1 , i2 , . . . , ik } such that performing
updates on the areas ai1 , ai2 , . . . , aik results in a problem instance P ′ = (C, A′ ) for which V (A′ ) returns
true, i.e., a solution of the problem can be determined from A without performing any more updates. In
this case, we say that S solves the problem instance P . Let S(P ) denote the set of all such solutions. An
optimal solution is a solution, S ∈ S(P ) such that any other solution in S(P ) has at least as many indices,
i.e., |S| ≤ |S ′ | for all solutions, S ′ ∈ S(P ). Therefore, an optimal solution corresponds to a smallest set of
indices that need to be updated in order to solve the problem.
As mentioned before, the OP-P and the CP-P models have been studied before. We shall show now
show that the algorithms for the OP-P model can be generalized for the many other models for problems
that are order-invariant. These update competitive algorithms are based on the concept of witness sets. We
discuss these concepts in Section 4; these concepts are borrowed from [BHKR05] and presented here with
modifications suitable to discuss all our models. Then we discuss how to extend these algorithms to other
models.

4

The Witness Set Framework

For a problem instance P = (C, A), a set W is said to be a witness set of P if for every solution S ∈ S(P ),
W ∩ S 6= φ. Thus, no algorithm can solve P without querying any area from W .
Suppose that we have an algorithm, WALG, that given any instance P = (V, A) of the problem, finds a
witness-set of size at most k. Then there exists a k-update competitive algorithm for the problem. The

5

algorithm is presented in Figure 3. It simply keeps applying algorithm WALG to find a witness set of size at
most k and updates all the areas in the witness set. It keeps doing this until the problem is solved.
Algorithm SOLVE( Problem Instance P , Verifier V , Witness Algorithm WALG )
Input: - problem instance P = (C, A),
- a verifier algorithm V for the given problem,
- a witness algorithm WALG for the given problem.
Output: k-update competitive solution to problem instance P
Initialize solution S = {};
If ( V (A) returns false ) /* problem instance is not yet solved */
W = WALG(P);
Update the areas in W to reduce the problem instance P to P ′ ;
S = S ∪ SOLVE(P ′ , V, WALG);
Endif;
Output S;

Figure 3: Algorithm to determine k-update competitive solution given witness algorithm
We now formally show that the solution returned by this algorithm is k-update competitive. Note that
this result is independent of the model under consideration. The witness algorithm and verifier however are
dependent on the underlying model.
Theorem 4.1. The solution returned by the algorithm in Figure 3 is k-update competitive for the problem
instance P .
Proof. See Appendix.
Witness Algorithms For Different Models. Witness algorithms have been proposed for several problems
under the OP-P model. We now show that the same witness algorithms can be used for various other models
as well.
Theorem 4.2. A witness algorithm for a problem under the OP-P model is also a witness algorithm for the
same problem under the category-1, category-2 and OP-OP models (i.e., O-O, C-C, OC-OC, OP-O, CP-C,
OCP-OC and OP-OP models).
Proof. See Appendix.
Corollary 4.3. Algorithm 3 is k-update competitive under the category-1, category-2 and OP-OP models
with the same witness algorithms as that for the OP-P model.
Proof. See Appendix.
We make an important observation here. While the reduction might seem straightforward, it is important
to note many of these reductions are only one-way reduction. For instance, we can reuse the witness algorithm
for the OP-P model for the OP-O model but not vice-versa. We demonstrate this later for the k-min selection
problem, where we show that while it is possible to design a 2-update competitive algorithm under the OP-P
model, it is not possible to design an algorithm that is better than k-update competitive under the OP-O
model using witness sets.
Another important observation we make is that prior literature has shown that no algorithm can give
bounded update complexity guarantees for the selection problem under the CP-P models. However, we
have derived constant factor update-competitive algorithms for models involving closed intervals (i.e., the
CP-C, C-C, OC-OC and OCP-OC models). This highlights the fact that the problem is not in dealing with
closed intervals but rather with the combination of allowing closed intervals in the input and simultaneously
allowing queries to return points for such closed intervals.

5

The selection problem

In an instance P = (C, A) of the k-Min problem, C = {p1 , p2 , · · · , pn } is an ordered set of points in ℜ, and
A = {a1 , a2 , · · · , an } is an ordered set of intervals on ℜ. The nature of the intervals is determined by the
6

model under consideration. The goal is to find the index of the k th smallest element in C.
We denote by lj and uj , the lower and upper ends of the interval aj respectively. To avoid overloading
of notations, we will assume that lj and uj always refer to the latest known values for the interval ranges,
considering all the updates that have already been performed.

5.1

1-Min

In this section we look at the special case when k = 1, i.e., we are interested in finding the index of the
smallest value interval.
Witness Algorithm And Verifier. We first present the witness algorithm for the OP-P model. Consider
an instance P = (C, A). The witness algorithm chooses the interval with the “smallest l-value” and the along
with the interval with the next “smallest l-value” and returns them as the witness set. The verifier simply
determines if some interval can be determined to be smaller than all the other intervals. Let S = {1..n}
denote the set of indices of the intervals. For any subset S ′ ⊆ S, we define orderl (S ′ ) to be a permutation
of indices in S ′ in increasing order of the lower values of the corresponding intervals, i.e., orderl (S ′ ) =<
j1 , j2 , · · · , jm >, such that lj1 ≤ lj2 ≤ · · · ≤ ljm . Similarly define orderu (S ′ ) =< j1 , j2 , · · · , jm >, such that
uj1 ≤ uj2 ≤ · · · ≤ ujm .
The witness algorithm and the verifier are formally presented in Figure 4.
Witness Algorithm:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. Return ap1 and ap2 as the witness set

Verifier:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. If x ≤ y for all x ∈ ap1 and y ∈ apj , j 6= 1,
return the interval with index p1 as the solution
Else return false

Figure 4: Witness Algorithm and Verifier for 1-Min under the OP-P model
Note that an interval is declared to be the smallest interval only when no other interval can contain a
smaller value. Therefore the algorithm always outputs the correct interval.
Competitiveness. We now show that the algorithm is 2-update competitive under the OP-P model.
Lemma 5.1. The set W = {p1 , p2 } returned by the algorithm of Figure 4 is a witness set for the 1-Min
problem under the OP-P model.
Proof. See Appendix.
It follows from Theorem 4.2 and Corollary 4.3 that we can derive 2-update competitive algorithms for
the category-1, category-2 and OP-OP models.
Tight Example. We now show that the update-competitive bound of 2 is tight for all the models that allow
the queries to return intervals, i.e., for the category-1, category-2 and OP-OP models (but not the OP-P
model). This is demonstrated by the following example. We are given intervals A = {a0 , a1 , a2 , . . . , an }
where a0 = (1, 5) and aj = (3, 7) for all 1 ≤ j ≤ n. We argue that any algorithm can be forced to perform
2n queries while the OPT can determine the interval containing the minimum with only n queries. Let S
represent the set of intervals A \ {a0 }, i.e., S = {a1 , a2 , . . . , an }.
Suppose that the algorithm has already performed 2n − 1 queries. The adversary behaves as follows.
For the first n − 1 queries on a0 it returns the interval (1 + iε, 5) in the ith query, where ε is a small value
< 1/(2n). For the first n − 1 queries on intervals from the set S it returns the interval (6, 7). The remaining
actions of the adversary are based on whether the algorithm performs n queries on a0 or whether it queries
n intervals from S. Note that in performing 2n − 1 queries, the algorithm must encounter one of these cases.
These are considered in the following 2 cases:
• Case 1: The algorithm makes n queries to a0 .
In this case the adversary continues to return the interval (1+iε, 5) for the ith query on a0 where i ≤ 2n−1
and it returns the interval (6, 7) for each subsequent interval queried from S. Note that in this case, on
7

performing 2n − 1 queries, the algorithm could not have queried all the intervals from S. Therefore at
the end of 2n − 1 queries, as there is overlap between interval a0 and the unqueried intervals from S, the
algorithm is forced to make 2n queries. The OPT on the other hand can just query all the intervals in
S. The adversary will return the interval (6, 7) for OPT on the remaining intervals. Thus, OPT is able
to determine that a0 contains the minimum element by just performing n queries.
• Case 2: The algorithm makes n queries to intervals in S.
In this case, the adversary returns (3, 4) for the last (nth ) interval queried in S. For any subsequent
queries to a0 , the adversary continues to return (1 + iε, 5) for the ith query. Note that in this case, the
adversary performs less than n queries on a0 . Therefore at the end of 2n − 1 queries, as there is overlap
between interval a0 and the last queried intervals from S, the algorithm is forced to make 2n queries.
The OPT on the other hand can just query all the intervals in a0 . The adversary will return the value
(2, 3) for OPT on its nth query to a0 (recall that in this case the algorithm did not perform n queries
on a0 ). Thus, OPT is able to determine that a0 contains the minimum element by just performing n
queries.
It is surprising that though this tight example demonstrates that we cannot obtain better than 2-update
competitive algorithms for these models, it is possible to obtain a 1-update competitive algorithm for the
OP-P model; however, this is obtained by an approach different from the Witness Set framework. This is
discussed in more detail in Section 6.

5.2

K-Min

We now generalize the 1-min algorithm presented above to the k th -min problem, but under the O-O model.
We later discuss issues related to handling points under the OP-P model.
Witness Algorithm And Verifier. We now present a witness algorithm and verifier for this problem
under the O-O model.
Witness Algorithm:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk−1 }
3. If x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ S \ S ′
return the witness set of 1-Min algorithm
4. Else
let < q1 , q2 , · · · , q|S ′ | > = orderu (S ′ )
return apk and aq1 as the witness set

Verifier:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk−1 }
3. If (x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ apk ) and
(x ≥ y ∀ x ∈ ai , i ∈ S \ (S ′ ∪ apk ) and ∀ y ∈ apk )
return apk
else return false

Figure 5: Witness and Verifier Algorithm for K-Min under the O-O model
We say intervals ai and aj are disjoint if ∀x ∈ ai , y ∈ aj , x ≤ y or vice-verse. The witness algorithm
checks if the first k − 1 interval are disjoint with the last n − k + 1 interval. If that is the case, it returns
the witness set of the 1-Min algorithm. Else it chooses apk and an interval from S ′ with largest u value(aq1 )
as the witness set.
The verif ier takes the first k − 1 intervals(S ′ ) depending on their l values. The verif ier checks if these
k − 1 intervals are disjoint from the apk . Then it takes the last n − k intervals(S \ (S ′ ∪ apk )) and checks if
all of them disjoint with apk . If both the condition holds, it returns apk else it returns false.
Competitiveness. We now show that the algorithm is 2-update competitive for the O-O model. It follows
using proofs similar to Theorem 4.2 and Corollary 4.3 that we can derive 2-update competitive algorithms
for the other category-1 models.
Lemma 5.2. The witness set W returned by the algorithm of Figure 5 is a witness set for the k-Min problem
under the O-O model.
Proof. See Appendix.

8

Tight Example. It is not difficult to construct examples similar to that discussed for the 1-Min algorithm
to show that the update-competitive bound of 2 is tight under the category-1 models.
It is interesting to note here that while a 2-update competitive algorithm can be designed for the k-min
problem under the category-1 models, no algorithm can be better than k-update competitive for this problem
under models that allow points, i.e., the category-2 and OP-P models. This is illustrated by the following
example3 . Suppose we have 2k areas of which k are open intervals of the form (0, 5) and k are fixed points
of the value 3. For the first k − 1 intervals queried by any algorithm, the adversary returns 1 and for the
k th interval, the adversary returns 4 (or interval (3.5,4.5) as the case may be), thereby forcing k queries.
However, OPT only needs to update the interval with value 4 and can thereafter return any of the k fixed
points of value 3 as the k th smallest.
However, in the next section we show that it is possible to design algorithms for the k-Min problem
under these models that allow for points, obtaining update competitive bounds with additive factor k (i.e.,
the algorithm performs k more updates than OPT). This however is achieved by bypassing the Witness set
framework.

6

Bypassing the Witness Set framework

While the witness set framework, studied in prior literature, provides a general method for solving problems
with data uncertainty under the update complexity models, it has its limitations. We demonstrate this
by presenting algorithms that require to perform only k more queries than OPT for the k th -Min selection
problem. Note that, for the 1-Min problem this implies a 1-update competitive algorithm, as only one query
more than OPT is required to be performed.

6.1

1-Min

Consider the following algorithm. We note here that the set of intervals returned by the “witness” algorithm
‘‘Witness’’ Algorithm:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. Let A = {ap1 } and B = {p2 , · · · , p|S | }
3. Return interval in A.

Verifier:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. If x ≤ y for all x ∈ ap1 and y ∈ apj , j 6= 1,
return the interval with index p1 as
the solution
Else return false

Figure 6: “Witness” Algorithm and Verifier for 1-Min under the OP-P model
is not a true witness set. However, we stick to the terminology for the sake of consistency. The algorithm
remains the same, it updates the intervals returned by the “witness” algorithm until we obtain a solution.
Lemma 6.1. Let cOP T be the total number of queries made by OPT to find 1-Min, then total number of
queries made by algorithm in Figure 6 is at most cOP T + 1 in the OP-P model.
Proof. See Appendix.
Note that this simple algorithm for 1-Min in OP-P model fails for the OP-O model. Consider the following
example. Let there be two intervals I1 = (2,20) and I2 = (19,21) Suppose at the ith query of I1 , we get a
new interval (di , 20), where di < 19, so I1 and I2 will always intersect if we just query I1 . The algorithm
in Figure 6 always queries I1 , so it takes huge number of queries to find 1-Min. But if we just query I2 , it
returns a subinterval (20.5,21). This is what OPT does and uses just one query to find the answer.

6.2

k-Min

Consider the algorithm in Figure 7 for k-selection in the OP-P model which generalizes the result of the
algorithm in Figure 6.
3 This

was pointed out by an anonymous reviewer of a previous version

9

‘‘Witness’’ Algorithm:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk }
3. let < q1 , q2 , · · · , qk > = orderu (S ′ )
′
′
.
Let Smax
= aqk . Query Smax
4. If x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ S \ S ′
return the “witness set” of the
1-Max algorithm of S ′ (of Figure 4).

Verifier:
1. Let < p1 , p2 , · · · , pn > = orderl (S )
2. Let S ′ = {p1 , .., pk−1 }
3. If (x ≤ y ∀ x ∈ ai , i ∈ S ′ and ∀ y ∈ apk ) and
(x ≥ y ∀ x ∈ ai , i ∈ S \ (S ′ ∪ apk ) and ∀ y ∈ apk )
return apk
else return false

Figure 7: Witness and Verifier Algorithm for K-Min under the OP-P model
Lemma 6.2. The algorithm of Figure 7 uses atmost cOP T + min{k, n − k} queries where cOP T is the
minimum number of queries required by the OPT.
Proof. See Appendix.
Now let us consider the OP-OP model. Note that since we have 2 · OP T algorithms for the OP-O model
and an OP T +k algorithm for the OP-P model, we can derive a 2·(OP T +k) algorithm for the OP-OP model
by combining these 2 algorithms. This is done by alternating the witness algorithms of the two models. This
ensures that we only need to perform at most twice the number of queries performed by the algorithms of
either of the two models.

7

Closed intervals with point returning queries

As discussed above, the competitive ratio is unbounded for the special cases where the input allows for closed
intervals and queries may return points (i.e., the category-3 models). For instance consider the problem of
finding the index of the minimum element. Further, consider the problem instance P = (C, A) where
ai = [1, 3] for all 1 ≤ i ≤ n. The adversary in this case acts as follows; for each of our queries except the last,
it returns 2. Finally, for our last query, say on interval ak , it returns 1. On the other hand, OPT directly
queries interval ak and obtains the optimal solution. This results in an unbounded competitive ratio.
The primary reason for this anomaly is the possibility of existence of multiple optimal solutions. In such
cases, the adversary is able to get away with few queries by just querying the necessary intervals that reveal
one of the optimal solutions. For any algorithm on the other hand, it is not able to distinguish from the areas
of uncertainty (as shown above) which are the necessary intervals to query to reveal the optimal solution.
One of the ways that has been suggested in prior literature to deal with this special case is to require all
the optimal solutions to be output. However, it can be quite expensive to output all these solutions. This
raises the question of whether other reasonable conditions can be laid on the structure of the required output
that are not so expensive but reasonable. We now consider such a condition, which we call the lexicographic
condition, for which we show that this special can be handled. Recall that the sets C and A that define
a problem instance are ordered sets. Thus, the set of indices that define a solution can be considered as a
string (called solution string) defined as follows: the length of the string is n and the ith element of the string
is set to 1 if it defines the solution and 0 otherwise. In the lexicographic setting, amongst all the optimal
solutions, we are interested in finding the solution for which the solution string has the smallest lexicographic
ordering.
Now consider again the example above. Note that, even though OPT queries ak and determines a solution
with optimal solution value, it cannot terminate without making further queries as it cannot decide whether
or not there exists another solution with the same value but a smaller lexicographic ordering.
We note that new witness algorithms may require to be developed for the lexicographic variants of the
problems. However, we show by case of examples that these are not very different from the corresponding
witness algorithms for the original problems.
It can be shown that once a witness algorithm is developed for a lexicographic variant of the problem
under the CP-P model, the same witness algorithm can be extended to other models along the same lines
as discussed in Section 4.

10

Now let us consider the lexicographic variant of the 1-Min problem. In order to obtain the witness algorithm for the lexicographic variant for the category-3 models, the notion of ordering of intervals, orderl (.),
needs to be extended to incorporate lexicographic ordering and closed intervals. As before, for any subset
S ′ ⊆ S, we define orderl (S ′ ) to be a permutation of indices in S ′ in increasing order of the lower values
of the corresponding intervals, i.e., orderl (S ′ ) =< j1 , j2 , · · · , jm >, such that lj1 ≤ lj2 ≤ · · · ≤ ljm . When
comparing two intervals with the same l-values, say lj and lj ′ , ties are resolved as follows: If aj contains a
point x such that x < y for all y ∈ aj ′ , then j precedes j ′ in the ordering; similarly if aj ′ contains such a
point, then j ′ precedes j; and if neither can be established, then the lexicographically smaller index precedes
the larger one in the ordering. Thus, if one of the intervals, say aj , is open from the left and another interval,
say aj ′ , is either closed from the left or a point, then j ′ precedes j in the ordering; in all other cases, the
lexicographic smaller of j and j ′ precedes the other in the ordering.
The witness algorithm and verifier are formally presented in Figure 8. Note that the verifier is also
modified so that it can check that the minimum interval can be determined or not based on the lexicographic
ordering.
Witness Algorithm:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. Return ap1 and ap2 as the witness set

Verifier:
1. Let < p1 , p2 , · · · , p|S | > = orderl (S )
2. If (x ≤ y ∀ x ∈ ap1 and y ∈ apj , pj > p1 ) and
(x < y ∀ x ∈ ap1 and y ∈ apj , pj < p1 ),
return the interval with index p1 as the solution
Else return false

Figure 8: Witness Algorithm for 1-Min under the CP-P model
The proof of update competitiveness is similar to the case for the original problem.
Lemma 7.1. The set W = {p1 , p2 } returned by the algorithm of Figure 8 is a witness set for the lexicographic
1-Min problem under the CP-P model.
Proof. See Appendix.
The fact that no algorithm can be better than 2-update competitive for the 1-Min problem under the
CP-P model follows from the same reasoning as for the OP-P model.
We can extend this 2-update competitive algorithm for the other category-3 models using techniques
similar to that in Section 4.
Finally, we can design 2-update competitive algorithms for the k-min version as well under these models
by using similar techniques.

8

Minimum Spanning Tree

In the Lexicographic MST problem, we are given a graph G = (V, E). The edge lengths are specified with
uncertainty. Let E = {e1 , e2 , . . . , en } be the ordered set of edges. Then the ordered set C = {v1 , v2 , · · · , vn }
denotes the values of the edge lengths and the ordered set A = {a1 , a2 , · · · , an } denotes the intervals within
which the edge lengths are known to lie. The goal is to find the lexicographically smallest MST under the
category-3 models.
A 2-update competitive algorithm for the MST problem was given by [HEK+ 08] under the OP-P model.
By applying Theorems 4.2 and Corollary 4.3, we conclude that it is 2-update competitive for the Category-1,2
and OP-OP models as well. The Lexicographic MST problem can be solved under the Category-3 models
with few changes to the algorithm described in [HEK+ 08] (these changes are outlined in Appendix A). This
gives us the following result.
Theorem 8.1. There exists a 2-update competitive algorithm for the Lexicographic MST problem under the
Category-3 models.

11

Remark: It may be noted that the algorithm described in [HEK+ 08] in conjunction with Lemma 6.1 can be
used to derive an OP T + C update competitive algorithm for the MST problem under the OP-OP model
where C is the number of red-rules applied by the optimal algorithm. Note that C can be much less than
OP T .

9

Conclusion

We extended the one-shot query model to the more general situation where a query can return arbitrary subintervals as answers and established strong relationships between these models. Many of the previous results
in the restricted model can be generalized based on this relationship that simplifies the task of designing
algorithms for the more general model. This is far from obvious as the sub-interval query model presents
some obvious challenges because the uncertainty (in the values of any parameter) can take an arbitrary
number of steps to be resolved and can be controlled by an adversary. One drawback of this approach is
that the actual algorithmic complexity is overlooked and we only focus on the competitive ratio which is
justified on the basis of very high cost of a query. For future work, the algorithmic complexity needs to be
incorporated in a meaningful way.

References
[AH04]

Ionut D. Aron and Pascal Van Hentenryck. On the complexity of the robust spanning tree problem with
interval data. Oper. Res. Lett., 32(1):36–40, 2004.

[AY09]

Charu C. Aggarwal and Philip S. Yu. A survey of uncertain data algorithms and applications. IEEE
Trans. Knowl. Data Eng., 21(5):609–623, 2009.

[BEE+ 06] Zuzana Beerliova, Felix Eberhard, Thomas Erlebach, Alexander Hall, Michael Hoffmann 0002, Matús
Mihalák, and L. Shankar Ram. Network discovery and verification. IEEE Journal on Selected Areas in
Communications, 24(12):2168–2181, 2006.
[BHKR05] Richard Bruce, Michael Hoffmann, Danny Krizanc, and Rajeev Raman. Efficient update strategies for
geometric computing with uncertainty. Theory Comput. Syst., 38(4):411–423, 2005.
[FMO+ 03] Tomás Feder, Rajeev Motwani, Liadan O’Callaghan, Chris Olston, and Rina Panigrahy. Computing
shortest paths with uncertainty. In STACS, pages 367–378, 2003.
[FMP+ 03] Tomás Feder, Rajeev Motwani, Rina Panigrahy, Chris Olston, and Jennifer Widom. Computing the
median with uncertainty. SIAM J. Comput., 32(2):538–547, 2003.
[GGM06]

Ashish Goel, Sudipto Guha, and Kamesh Munagala. Asking the right questions: model-driven optimization using probes. In PODS, pages 203–212, 2006.

[GM07]

Sudipto Guha and Kamesh Munagala. Model-driven optimization using adaptive probes. In SODA, pages
308–317, 2007.

[HEK+ 08] Michael Hoffmann, Thomas Erlebach, Danny Krizanc, Matús Mihalák, and Rajeev Raman. Computing
minimum spanning trees with uncertainty. In STACS, pages 277–288, 2008.
[Kah91]

Simon Kahan. A model for data in motion. In STOC, pages 267–277, 1991.

[KT01]

Sanjeev Khanna and Wang Chiew Tan. On computing functions with uncertainty. In PODS, pages
171–182, 2001.

[KZ06]

A. Kasperski and P. Zielenski. An approximation algorithm for interval data minmax regret combinatorial
optimization problem. Information Processing Letters, 97(5):177–180, 2006.

[OW00]

Chris Olston and Jennifer Widom. Offering a precision-performance tradeoff for aggregation queries over
replicated data. In VLDB, pages 144–155, 2000.

Appendix A. Sketch of changes for Lexicographic MST
The following changes are required to the algorithm of [HEK+ 08]. Here we use the notation Ux for ux and
Lx for lx to remain consistent with [HEK+ 08].
12

1. The main change involves modifying the comparison operator. We modify the comparison operator
defined on the intervals as follows: Let x be the l-value or u-value of some interval, i.e., x = le or
x = ue for some interval e. Similarly, let y be the l-value or u-value of some interval, i.e., y = lf or
y = uf for some interval f 6= e. We say that x ≺ y if x < y or x = y and e is lexicographically smaller
than f .
2. An edge e of a cycle C is said to be always maximal if Uc ≺ Le for all c ∈ C − {e}. Note that the only
change introduced in this definition is in replacing the comparison operator.
3. We similarly modify the notion of comparing two edges e and f based on the comparison operator
as follows. We say that e ≺ f if Le ≺ Lf . While indexing the edges in the algorithm, the edges are
considered in the order defined by ≺ above.
4. The witness set is determined as follows. Once a cycle C is detected, if it contains an always maximal
edge, that edge is deleted. Otherwise let f ∈ C such that Uf = max{Uc |c ∈ C} where max is based on
the new ≺ operator. Further let g ∈ C − {f } such that Lf ≺ Ug . Then f and g form the witness set.
Theorem 8.1 can be proved with these changes along the same lines as presented in [HEK+ 08].

Appendix B. Proofs for the Witness Set Framework
9.1

Proof of Theorem 4.1

We first prove a claim that will be required in the proof of the above result.
Claim 9.1. Suppose that we are given a problem instance P = (C, A). Further, suppose that we know that
an optimal solution, So for P contains an index i, i.e., So queries the area ai . Let P ′ = (C, A′ ) be the
problem instance reduced from P on querying area ai . Then So′ = So \ {i} is an optimal solution for P ′ .
Here the operation \ on the multiset So removes only one instance of i from it in case there are multiple
instances.
Proof. See Appendix.
Proof. Recall that the update independence property implies that irrespective of the order in which the
updates are applied, applying all the updates in So solves the problem P . Therefore, clearly So′ solves the
problem instance P ′ . In order to argue that this is an optimal solution, all we need to show is that there
does not exist a solution of smaller size. Suppose otherwise. Then there exists a solution S of size smaller
than So′ that solves P ′ . But then, S ′ = S ∪ {i} solves P which contradicts the fact that So is an optimal
solution of P .
We now present the proof of Theorem 4.1.
Proof. The proof is by induction on the size of an optimal solution on instance P . For the base case, consider
a problem instance P for which any optimal solution has size 1. Let W be a witness set returned by algorithm
WALG. Clearly, W is k-update competitive by definition.
Now suppose that the claim holds for any problem instance P having optimum solution of size i or less.
Consider a problem instance P for which any optimum solution has size i + 1. Let W be a witness set of size
≤ k returned by WALG. Let the instance P be reduced to instance P ′ on applying updates on the areas in W .
By Claim 9.1, any optimal solution on P ′ has size ≤ i. By induction, the algorithm determines a k-update
competitive solution S ′ for P ′ . Hence |S ′ ∪ W | ≤ k(i + 1), and thus S ′ ∪ W is a k-update competitive solution
for P .

13

9.2

Proof of Theorem 4.2

Proof. We formally prove this for the CP-C model. The proofs for the other models follow similarly; we
point out the changes required.
Let P be any instance of the given problem under the CP-C model. Let P ′ be obtained from P by
modifying the configuration and areas of uncertainty as follows; (i) All the closed intervals are replaced with
open intervals; and (ii) The configuration is suitably modified in order to ensure that the configuration points
are always contained in the corresponding areas of uncertainty – this is explained in more detail later. Let
W be any witness set for P ′ under the OP-P model. We need to show that W is also a witness set for P
under the CP-C model. Suppose this is not so, i.e., W is not a witness set for P under the CP-C model.
We will then argue that there exists a set of queries excluding W that when applied to P ′ under the OP-P
model can result in an instance for which the verifier returns true; this implies that W is not a witness set
for P ′ under the OP-P model leading to a contradiction. Hence our supposition is incorrect and W must be
a witness set for P as well.
It remains to find a possible set of queries excluding W and query outcomes that when applied to
P ′ under the OP-P model results in an instance for which the verifier returns true under the assumption
that W is not a witness set for P under the CP-C model. Considering this assumption, there exists a
solution S = {i1 , i2 , . . . , ik } for P under the CP-C model that does not contain the index for any area
in W . Let P1 , P 2, . . . , Pk be the sequence of instances obtained on applying the updates in S where Pt
is obtained from Pt−1 on applying the update on ait for 1 ≤ t ≤ k. For any interval (not a point) aj
in Pk (the final configuration in the sequence above) let lj , uj denote the interval end points. Let ε =
min{uj − lj |aj aj is an interval in Pk }, i.e., ε is the minimum length of any interval in Pk .
As mentioned earlier, the configuration points in P ′ are also suitably modified in order to ensure that
they are always contained in the corresponding areas of uncertainty. This is done by setting a configuration
point cj to cj + ε/10 if cj = lj in Pk , and setting it to cj − ε/10 if cj = uj in Pk . This ensures that no
configuration point coincides with the interval end-points; this will allow us to replace closed intervals with
open intervals. Moreover, the modified configuration points are consistent with all the query outputs.
Now, consider the case where the same sequence of updates in S is applied to P ′ under the OP-P model.
A possible sequence of outcomes is P1′ , P2′ , . . . , Pk′ wherein Pt′ is the same as Pt with all closed intervals
′
on applying the update on a′it for 1 ≤ t < k). Now
replaced by open intervals (Pt′ is obtained from Pt−1
note that since S solves P , the verifier returns true for Pk . However, Pk and Pk′ are order-equivalent and
since the problem is order-invariant, the verifier must return true for Pk′ under the OP-P model as well. This
implies that W is not a witness set for P ′ under the OP-P model leading to the required contradiction.
We can similarly show that the witness set for the OP-P model can be reused for a variety of other
models, thereby resulting in comparable update-competitive algorithms. The proofs are similar to that of
Theorem 4.2 above; the only difference is in the way the the instances P1′ , . . . , Pk′ for the OP-P model are
constructed from the instances P1 , . . . , Pk for the new model.
For the C-C model, OC-OC model and the OCP-OC model, the instance Pt′ is obtained from instance
Pt by replacing the closed intervals in Pt with corresponding open intervals and modifying the configuration
points as described in the proof above.
For the O-O model, and the OP-O model, the instance Pt′ is obtained from instance Pt by replacing the
intervals in Pt corresponding to the areas of uncertainty having indices in the set {i1 , i2 , . . . , it } with the
corresponding configuration points ci1 , ci2 , . . . , cit . Note that in this case, it does not make sense to query
the interval on the same index more than once, therefore the number of queries can be reduced.
This completes the proof of the Theorem.

Proof of Corollary 4.3
Proof. Consider the CP-C model. By Theorem 4.2, we know that the witness algorithm for the OP-P model
is also a witness algorithm for the CP-C model. Moreover, the verifier for the OP-P model is also a verifier
for the CP-C model as the problem considered is order-invariant. The proof for the other models follows
similarly.

14

Appendix C. Proofs for the Selection Problem
Proof of Lemma 5.1
Proof. The proof is by contradiction. Suppose OPT updates neither ap1 nor ap2 . Let the index of the interval
returned by OPT as the answer be aq . We consider the 2 cases:
• aq = ap1 : As the witness algorithm is invoked only when the verifier returns false, by examining the
condition in Step 2 of the verifier (which must have failed for the current instance), we conclude that ∃
x ∈ ap1 and y ∈ apj , j 6= 1 such that y < x. Thus OPT has not fully demonstrated that ap1 contains the
point which is minimum as ap2 could be made to contain the minimum point.
• aq 6= ap1 : By the definition of orderl (.) applied in Step 1 of the witness algorithm and by examining the
condition in Step 2 of the verifier, we conclude that lp1 ≤ lpq . Thus, OPT has not demonstrated that apq
contains the point which is minimum as ap1 could be made to contain the minimum point.

Proof of Lemma 5.2
Proof. The proof is again by contradiction. There are two cases:
• The witness set returned is the witness set of the 1-Min Algorithm, W = {apk , apk+1 }:
Since the first k − 1 intervals are disjoint with the rest of the intervals, the problem of finding the k th
minimum interval becomes the problem of finding 1-Min in S \ S ′ . Using Lemma 6.1, W is a valid witness
set.
• W = {apk , aq1 }:
Suppose OPT updates neither apk nor aq1 . Let the index returned by OPT be aj . So aj has to be disjoint
with all the other intervals. Since the witness algorithm was called only because the verifier returned
false, so by examining the condition of step 3 of the verifier, we infer that ∃api with 1 ≤ i ≤ k − 1 such
that ∃x ∈ api , y ∈ apk for which x > y. So api and apk are not disjoint. If such api exists, then by the
definition of aq1 , we see that aq1 and apk are also not disjoint. So the solution returned by OPT cannot
be apk and aq1 as they both are not disjoint. As aj must be disjoint, we consider following cases:
– uj ≤ lpk and uj ≤ lq1 : Initially there were less than k − 2 intervals with l values ≤ lq1 . Since aq1 is
not updated, any update of other intervals cannot increase the number of intervals with l values ≤
lq1 . Since uj ≤ lpk , the number of intervals with l values ≤ lj is less than k − 3. So aj cannot be
the k th minimum interval.
– lj ≥ upk and lj ≥ uq1 : Initially there are k − 2 intervals with u values ≤ uq1 . Since aq1 is not
updated, any update of other intervals is not going to decrease the number of such intervals. These
intervals together with q1 and pk have u values ≤ lj . So there are k intervals with u values ≤ lj . So
aj cannot be the k th minimum interval.

Appendix D. Proofs for Bypassing the Witness set Framework
Proof of Lemma 6.1
Proof. Assume for contradiction that we have queried cOP T + j intervals where j ≥ 2. Let a1 and a2 be
any two intervals that algorithm in Figure 6 has queried but OPT has not queried such that l1 ≤ l2 . Since
OPT did not query a1 , we conclude that a1 is the interval which contains the minimum. Also since the
algorithm in Figure 6 queried a2 , ∃ x ∈ a1 and y ∈ a2 such that y < x. But we have assumed that OPT
does not query a2 , so OPT cannot demonstrate that a1 contains the point which is minimum. So we get a
contradiction.
15

Proof of Lemma 6.2
Proof. Let 1 < k ≤ n − k - the other case can be argued similarly and k = 1 is addressed by the algorithm in
′
′
′
Figure 6. If Smax
is not queried by OPT then Smax
has rank ≤ k. Smax
cannot have rank > k by definition
′
′
′
of S . Indeed, if Smax has rank > k, then there must be at least k points to the left of Smax
that violates
′
′
the definition of S . If Smax has rank ≤ k then atmost k − 1 such intervals can remain unqueried, otherwise
′
the rank of the element returned cannot provably be k. (If Smax
has rank = k, then the OPT must query
all except one, which is ≤ k − 1 for k > 1). For the second phase, to find out the maximum among S ′ , the
algorithm of Figure 6 needs at most cmax
OP T + 1 queries. So, overall, our algorithm makes at most k − 1 + 1 = k
queries more than the OPT.

Appendix E. Proofs for Closed intervals with point returning queries
Proof of Lemma 7.1
Proof. The proof is again by contradiction. Suppose OPT updates neither ap1 nor ap2 . Let the index of the
interval returned by OPT as the answer be aq . We consider the 2 cases:
• aq = ap1 : As the witness algorithm is invoked only when the verifier returns false, by examining the
condition in Step 2 of the verifier (which must have failed for the current instance), we conclude that
either (i) ∃ x ∈ ap1 and y ∈ apj , j 6= 1 such that y < x; or (ii) ∃ x ∈ ap1 and y ∈ apj such that y = x
and p2 < p1 . In either case, we observe that OPT has not fully demonstrated that ap1 contains the point
which is minimum as ap2 could be made to contain the minimum point.
• aq 6= ap1 : By the definition of orderl (.) applied in Step 1 of the witness algorithm and examining the
condition in Step 2 of the verifier, we conclude that lp1 ≤ lpq . Thus, OPT has not demonstrated that apq
contains the point which is minimum as ap1 could be made to contain the minimum point.

16

Holistic Management of Sustainable Geo-Distributed
Data Centers
Zahra Abbasi∗

Sandeep K.S. Gupta

Ericsson Research, San Jose, CA
zahra.abbasi@ericsson.com

IMPACT Lab, Arizona St. Univ., Tempe, AZ
sandeep.gupta@asu.edu

Abstract—This paper designs a holistic global workload
management solution which explores diversities of a set of
geo-distributed data centers and energy buffering in order to
minimize the electricity cost, reduce the peak power drawn from
utilities while maintaining the carbon capping requirement of
the data centers. The prior work often designed solutions to
address each of the aforementioned energy and cost optimization
separately, disregarding the possible conflicts between the solutions’ objectives. We propose a holistic solution to concurrently
optimize the aforementioned potentially competing objectives.
The proposed solution combines the techniques from Lyapunov
optimization and predictive solution in order to manage the
tradeoffs of electricity cost and carbon footprint reduction, and
electricity cost and peak power cost reduction, respectively. The
predicted data center parameters, being a significant aid to
near optimally manage energy buffering and smoothing data
centers’ peak power draw, adversely affect the peak power cost
due to the parameters’ prediction error. The proposed holistic
solution adapts stochastic programing to take the predicted
parameters’ randomness into consideration for minimizing the
harmful impact of the prediction error. Our trace-based study
confirms our analytical result that our holistic solution balances
all the tradeoffs towards achieving energy and cost sustainability.
Also our solution removes up to 66% of the prediction error
impact in increasing the cost.
Keywords—data centers, cloud computing, peak power, prediction error, carbon capping, electricity cost.

I.

Introduction

Internet data centers, typically distributed across the world
in order to provide timely and reliable Internet service, have
been increasingly pressurized to reduce their carbon footprint
and electricity usage. Particularly, data centers will soon be
required to abide by carbon capping polices which limit
their maximum carbon footprint emission to encourage brown
energy conservation [1]. Huge monthly electricity bill and
costly power infrastructure of these data centers are other
big concerns to the operators. Data centers spend 10 to 25
dollar per watt in provisioning their power infrastructure,
regardless of the Watts actually consumed [2]. Since peak
power needs arise rarely, provisioning power infrastructure
for them can be expensive. Further, some utilities penalize
data centers for their peak power consumption in addition to
charging for the energy consumed (i.e., $/W). Energy buffering management using energy storage devices (ESDs), e.g.,
existing UPSes, has been shown to be promising to shave the
power demand, allowing aggressive under-provisioning of the
This research has been funded by NSF CNS grant #1218505.
∗ The work was done when this author was with Arizona State University.

power infrastructure [2]–[4]. Global workload management,
i.e., intelligently distributing the workload across data centers
according to their electricity price ($/J) and carbon footprint
(CO2 /J) at a given time, can also be of significant aid to shave
the peak power drawn without requiring large-scale ESDs.
Although energy buffering and global workload management
have been considerably studied in the literature [5]–[9], the
solutions designed so far are piecemeal in the sense that each
of which addresses only some aspects of the problem. In particular, prior work has independently considered (i) electricity
cost minimization and carbon footprint capping through an
intelligent global workload management, and (ii) peak power
cost reduction via energy buffering. We argue that there is a
need for a holistic approach, which combines all the available
leverages and concurrently optimizes the potentially conflicting
objectives. Accordingly, we propose a new holistic global
workload management for large-scale Internet services running
in geo-distributed data centers. Such a holistic management,
however, introduces new challenges.
First, the optimal solutions for the peak power cost minimization, energy buffering management and carbon capping
can be only found offline. Prior online algorithms are designed
to manage each (or two) of the aforementioned objectives separately, disregarding their implications on each other. Particularly, a window based predictive scheme, efficient for online
management of peak power shaving [3], fails to competitively
manage carbon capping with respect to the offline solution.
This is because adjusting the carbon cap for each prediction
window is difficult considering the intermittent nature of the
available renewable energy. We use a combination of window
based predictive scheme and T-slot Lyapunov optimization to
jointly manage the energy cost (electricity and peak power
cost) and the carbon footprint. The idea is to leverage the variability of data center parameters within the time frame T (e.g.,
a day) in order to smoothen the peak power drawn, and utilize
Lyapunov technique to adjust the desired carbon footprint for
each time frame over the entire budgeting period (e.g., a year).
We hypothesize that such a solution achieves near optimal cost
saving given the limited capacity of energy storage devices,
and the daily variability of data center parameters.
Nevertheless, the efficiency of the previously discussed
solution depends on the prediction accuracy of data center
parameters over T , e.g., workload, electricity prices, and the
available renewable energy. In particular, the prediction error
has a very harmful impact on the peak power cost. The
reason is that the optimal approach is to utilize the data
centers with low electricity cost as much as possible without

Holistic global workload management solution
Prediction
model

Historical data

Prediction models

Prediction error Stochastic
scenarios

Data center
model

Workload
model

Predicted data over future
time window

Power consumption
model

Power supply
model

Optimization model of global workload management
Electricity
cost

Peak
power cost

Workload and data center parameters
(e.g., electricity price)

Lyapunov
control

Carbon
footprint

Workload distribution and
server management policies

Front-end
Data center
Data
Control data

Geo-distributed data centers

Fig. 1.

footprint across data centers (Section IV-C). OnCMCCLyp is
proven to operate near offline optimal solution when T is
sufficiently large and the predicted data are accurately available
(Theorem 1). Next, we present our concluding holistic solution
which adapts stochastic programming to model and solve the
online solutions, in the presence of the parameters’ prediction
error (Section V). Finally, we perform a real-world trace based
study to complement our analysis (Section VI).

Holistic global workload management solution.

increasing their peak power. Under any under prediction of
those data centers’ workload, for instance, their peak power
most likely increases, resulting in an unexpected increase in
the peak power cost. Previous prediction based schemes of
peak power management are performed without considering
the impact of the prediction error [4], [10]. We propose to
make use of stochastic programming approach in order to
mitigate the sensitivity of the solution to the prediction error.
Accordingly, we formalize the cost minimization problem
taking into account the stochastic scenarios, each of which
represents the future realization of the cloud random parameters (e.g., input workload). A huge number of scenarios
are required to completely describe the stochastic nature of
the uncertainties associated with the cloud input parameters.
Solving the stochastic cost minimization problem with those
huge set of scenarios is computationally too expensive. So
an appropriate scenario reduction technique must be used to
limit the number of scenarios. We make use of the previously
proposed scenario reduction algorithms [11] along with some
problem-specific heuristics to reduce the number of scenarios.
We hypothesize that a stochastic approach using a small set
of stochastic scenarios, achieves a comparable performance
against the solution with accurate data over T .
In summary we make the following contributions. We
design a holistic global workload management solution which
combines the potential benefits from a set of geo-distributed
data centers to concurrently optimize the electricity cost, the
carbon footprint and the peak power cost of data centers. As
shown in Fig. 1, the holistic solution arranges and employs
diverse set of models and techniques including predictive
solution, stochastic programming and Lyapunov optimization
to tackle energy management tradeoffs and enable coordination
management of the energy cost and the carbon footprint.
Throughout the paper, we incrementally enhance the holistic
solution. We first frame the holistic global workload management problem as a linear programming (Section III), and
develop a predictive solution, Online Cost minimization and
Carbon Capping (OnCMCC), which utilizes T future slots’
information. Next, we design a predictive Lyapunov based
solution (OnCMCCLyp), which uses T -slot Lyapunov optimization technique to jointly minimize the cost and the carbon

II.

Related work

There have been related efforts on reducing electricity
bill and carbon footprint of data centers through workload
management for a single [4]–[6], [8], [9] and a set of geodistributed data centers [1], [7], [12]. In particular, related
work proposed includes [1] and Lyapunov based optimization
[9], [12], [13] for joint optimization of the electricity cost
and the carbon capping. The efficiency of Lyapunov based
solutions heavily depends on the value of the Lyapunov control
parameter. There are also some recent works which explored
the use of existing UPSes or any ESDs to reduce both the
energy cost and the peak power cost for a single [2], [4]–
[6], [8] and a set of geo-distributed data centers [10] without
considering the carbon capping requirements of data centers.
The related work, thereby, lacks a holistic solution to jointly
manage the energy cost, the peak power cost and the carbon
capping, a key solution for today’s data centers to operate
under carbon capping policies. This is important since such
a holistic solution introduces new challenges which need to be
addressed. Existing studies mainly used offline and predictive
solutions for energy buffering management in data centers
[4], [6], [8]. However, Lyapunov technique is used to exploit
batteries in data centers for energy cost minimization [5]. The
performance of the solution in [5] is based on restricting the
maximum value of the Lyapunov control parameter, and the
minimum required ESD capacity (which is relatively a large
value). However, first, we seek a practical solution without
requiring large scale ESDs to avoid their space and financial
overhead. Second, the proposed solution only accounts for the
energy cost. However ESDs can be best utilized to shave the
peak power drawn, where its online management is shown to
be effective when using window based predictive approach [3].
Third, using Lyapunov optimization for online management of
both the carbon footprint and the ESD dynamics becomes a
tedious task (if possible at all) since it requires a Lyapunov
control parameter adjustment that optimally manages the two.
Further, the existing solutions on data center peak power
shaving rely on the predictability of data centers’ parameters
over a window of time [2]–[4], [10], lacking analysis/solution
to overcome the harmful impact of the prediction error on the
peak power shaving. [14] designed an algorithm for single data
centers to utilize Diesel Generators in order to compensate
the impact of the prediction error in increasing the peak
power cost. We use stochastic programming approach, to
incorporate the randomness of the predicted parameters into
the decision making process. Stochastic programming has been
successfully applied in many applications, particularly, in grid
power management and renewable energy optimization [11],
[15]. However, we are the first (to our knowledge) to apply it
for data center energy and power cost optimization.

INPUTS FROM
FRONT-ENDS:

Control data

Workload arrival
rate (Λ)
OUTPUT TO
FRONT-ENDS:

Front-ends

Workload
distribution
policies (λ)

INPUTS FROM DATA CENTERS:

Workload

Online global
workload
management
Data centers

1

λ11

2

1

2
3

Fig. 2.

OUTPUT TO DATA CENTERS:
# of active servers (y)
Charging/discharging of
batteries (R/D)

System model.
TABLE I.

Symbols and definitions.

Sym. Definition
t
S
T
j
i
N
Y
λ
Λ
p
g
r
X
S0
p0
ζ
W

Electricity cost (α)
Peak power cost ()
Available renew. energy (r)
Power efficiency (p)
Cooling efficiency (PUE)
Carbon emission factor (ε)
Data center capacity (Y)

slot index
total # of slots
time frame (T << S )
frontend index
data center index
# of data centers
total # of servers
data centers’ workload
frontends’ workload
per server power cons
grid power
renewable power
virtual queue
peak power billing period
stipulated peak power
prediction error rand. var.
set of stochastic scenarios

III.

Sym. Definition
εg
εr
Ψ
ψ= Ψ
S
α
β
E
d
c
D
C
η
V
bmax
φ
R

grid carbon emission
renew. carbon emission
carbon cap
time-avg carbon cap
electricity price ($/J)
peak power cost ($/W)
ESD capacity
ESD discharge rate
ESD charge rate
ESD max discharge rate
ESD max charging rate
ESD energy inefficiency
Lyap. control parameter
per-slot max carbon
ESD cost per
charge/discharge
feasible set of y and λ

System model and problem formulation

We consider a cloud, which consists of N geographically
distributed data centers (see Fig. 2), where data center i has
Yi servers. We assume servers have only two states: active
and inactive. Data centers get their required power from a
mix of grid, on-site solar and wind renewable energy sources,
and Energy Storage Devices (ESDs). We assume ESDs can be
charged either from the grid or the available renewables.
We assume Internet workload for data centers, which
typically exhibit daily and weekly seasonality and require
timely services. End users’ requests arrive from M geographically distributed front-ends (i.e., the sources), as shown in
Fig. 2. The geographical front-ends may be network prefixes,
or geographic groupings (states and cities). The workload
management system operates in slotted time i.e., t = 0 . . . S − 1
for the budgeting period of S slots where the time slot length
matches the timescale at which the server provisioning and
energy storage charging/discharging cycle can be updated. In
this paper we aim to design an online holistic global workload
management which decides on the workload distribution and
power management polices over every T slots, where T << S .
In the following sections, we incrementally design and
enhance our holistic global workload management solution to
conclude our final solution as shown in Fig. 1. Given models
to describe the workload, the power demand and the power
supply, we first frame the global workload management as
an optimization problem. We argue that the joint management
of the electricity cost, the peak power cost and the carbon
footprint requires a combined technique of predictive solution
and Lyapuonv optimization (Section IV-C). The resulting so-

lution, however, is only effective when the parameters can be
accurately predicted (zero prediction error). Given non-zero
prediction error of the parameters, we design our final solution
by taking into consideration the randomness of the predicted
data (Section V), following the roadmap of the holistic solution
given in Fig. 1.
IV.

Optimization problem formulation

Our optimization problem uses models characterizing the
power demand, derived from the workload distribution model,
and the power consumption model of servers, and the power
supply model which consists of models to describe the power
drawn from the grid, batteries and the on-site renewables. The
formulation and models build on the models used by the related
work e.g., [4], [12]. The key change we make to [12] is to
combine models of energy storage devices, peak power cost,
and carbon capping in order to design a holistic solution for
cost management and carbon capping of data centers.
Data Center Power Demand: Let Λ j (t) denote the
average workload arrival rate at front-end j, our algorithm
decides on the workload distribution of front-end j to data
center i, denoted by λi, j (t), and the number of active servers at
each data center i, denoted by yi (t), subject to a set of workload
requirements (e.g., delay requirement, availability of computation data) and data centers capacity. To model these requirements, we assume that λi (t) = (λi,1 (t), . . . λi,j (t), . . . , λi,M (t)),
and the corresponding yi (t) must be drawn from a feasible set,
(λi (t), yi (t)) ∈ RM+1
(t). Our analysis works for P
any convex set
i
of RiM+1 (t), which contains the constraints that i λi, j (t)=Λ j (t)
(workload processing requirement), and yi (t) ≤ Yi (upper
bound of number of servers). RiM+1 (t), for instance, can also
contain the constraint that λi, j (t)=0 to represent the constraint
that workload arriving at front-ends j cannot be processed at
data center i due to the network latency or data availability. To
solve our problem in Section VI, we use models in [1] which
account for finding λi , and yi based on M/M/n queuing model
of data centers and an additive slack for number of severs to
deal with workload spikes.
Given RiM+1 (t), the average one-slot energy consumption of
an active server, denoted by pi , can be obtained by profiling.
tot
Then ptot
i , where, pi =yi,t pi estimates the total one-slot energy
consumed by active servers in data center i.
Data Center Power Supply: Data centers get their
primary power from the grid. We perform our study under the
dynamic pricing managed by the wholesale electricity market
(e.g., north America). Under this model, the electricity pricing
is dynamic, significantly varies over time and has seasonal
daily, and monthly pattern. In addition to the energy actually
consumed, some utility providers penalize the excess power
draw: imposing additional fee if the peak power draw over a
certain time window e.g., average power every 15 minutes [4],
seen in a billing period (denoted by S 0 ) exceeds the stipulated
power (denoted by p0 ). Hence, we consider that the electricity
price from the grid includes αi (t), the usage price, and βi , the
surcharge per excess power draw over S 0 from p0 .
To model energy storage, we denote the energy storage
level at time t by ei (t), and the charge/discharge energy during
time slot t by ci (t) and di (t), respectively. There is a limit
on the maximum charging and discharging rate denoted by

C and D, respectively. An ESD has limited capacity, further
it is associated with a cycle-life i.e., the average number of
charging/discharging cycles in the lifetime of the device for a
given depth of discharge. Furthermore, data centers reserver
some of ESDs’ capacity for use during the power outages.
Therefore, we denote E the capacity of the ESD which can
be used to manage the energy cost and the renewable energy
utilization without affecting the data center availability and
without violating the given depth of discharge. We assume
that the efficiencies of ESD charging and discharging are the
same, denoted by η ∈ [0, 1], e.g., η = 0.8 means that only 80%
of the charged or discharged energy is useful. Energy level of
an ESD over time satisfies the following:

the integer constraint of number of active servers (yi ) and
round the resulting solution with minimal increase in cost.
Also observe that P1 disregards the non-convex and non-linear
constraint (2), however the following lemma asserts that the
optimal solution to P1 never chooses to simultaneously charge
and discharge from ESDs. This is intuitively clear, because
charging and discharging the ESD in the same slot incurs
additional battery cost and energy cost due to the battery
inefficiency. It is, thereby, beneficial to instead satisfy the
demand from the grid or do either charging or discharging.

∀i, t : ei (t + 1) = ei (t) + ηi ci (t) − 1η di (t)[ESD energy level],
i
∀i, t : 0 ≤ ei (t + 1) ≤ Ei , 0 ≤ ei (0) ≤ Ei ,
∀i, t : 0 ≤ ci ≤ Ci , 0 ≤ di ≤ Di .
(1)
ESDs have some other physical limitations such as self discharge rate, which are ignored for notation brevity. Finally, in
any slot, one can either recharge or discharge the battery or
do neither, but not both. Hence, for all t and i we have:

Lemma 1 can be proved by construction, which is deleted
due to the space limitation (see [16, Lemma 7.1.1]).

∀i, t :

ci (t)di (t) = 0.

(2)

Consistent with today’s data centers, we assume data
centers get their power partially from the available on-site
renewable energy (wind and solar) denoted by ri (t) ≤ Ri . For
every data center i and all time t the energy demand and supply
should be balanced as follows:
∀i, t : gi (t) + ri (t) + di (t) = ptot
i (t) + ci (t),
(3)
∀i, t : gi (t) ≥ 0.
Cloud’s Carbon Cap: To incorporate the carbon capping, we consider that each data center is associated with
carbon emission intensities for the power source from utility
denoted by εgi (t) and its on-site renewable denoted by εri (t) in
unit of CO2 g/J. The total carbon footprint of the cloud, within
slot t can be written as follows: bi (t) = εgi (t)gi (t) + εri (t)ri (t).
The cloud desires to follow the long-term carbon capping
target, denoted by Ψ, which is typically expressed for a year
of operation of a data center.:
S −1
1 XX
Ψ
bi (t) ≤
= ψ,
(4)
S t=0 i
S
where ψ denotes the time-averaged carbon cap.
A. Operational Cost Minimization and Carbon Capping
We set renewable energy operational cost to zero to
maximize their utilization. In addition to the energy cost
and the peak power cost, we consider that the data centers’
operational cost accounts for the cost per maximum charging
and discharging denoted by φi,C , and φi,D , respectively which
depends on the ESD characteristics (e.g., cycle-life). The timeaveraged operational cost of data centers over S slots, can be
written as the following optimization problem, namely P1:
P P
ci (t)
di (t)
S
minimize S1
t=1 i gi (t)αi (t) + C φi,C + D φi,D

PS /S 0 −1
+ t0 =0 max(t0 −1)S 0 ≤τ≤t0 S 0 −1 (gi (τ) − p0 )+ βi ,
subject to (λi (t), yi (t)) ∈ RM+1
(t) (1), (3), and(4).
i
(5)
To simplify the problem P1, note that Internet data centers
typically contain thousands of active servers. So, we can relax

Lemma 1. The optimal solution to P1 for every data center i
and time t always chooses ci (t)di (t) = 0.

The problem P1 as described above is a linear programing
(given a linear model for Rm+1 ) which can be optimally solved
using the existing linear programming solvers. However, the
solutions of P1 over time are dependent due to the several sources of coupling factors: (i) the peak power cost is
calculated over every S 0 ≥1 slots (5), as a result it couples
the solutions over S 0 , (ii) the ESDs’ dynamics (1) and the
carbon capping constraint (4) couples the solutions over time.
In practice, the billing period (S 0 ) is typically a month, and
the carbon cap is typically given over a year of operation of
the data centers. This means that S is typically equals to the
number of slots for a year. Therefore, in practice, it becomes
impractical to solve P1 due to the unavailability of data as
well as “curse of dimensionality”. In this paper, we study and
propose online solutions to solve P1. The performance of the
online solutions are based on (i) the feasibility assumption
which ensures that P1 has non-zero feasible solutions, (ii)
the bounded assumption which ensures that the total one-slot
cloud’s carbon footprint is bounded by bmax , i.e., b(t)≤bmax ∀t,
and (iii) the predictability assumption which ensures that
the data center parameters are predictable over T slots with
reasonable accuracy, and their most variabilities fall within T
slots. Observe that, the assumptions are not constraining in
practice, and that the last assumption is consistent with the
daily variability of the data center parameters.
B. OnCMCC: predictive online solution
We design the online solution, namely OnCMCC, as a
reference solution to solve the problem P1 over T ≤S 0 , where
T consists of slots for one or more days (e.g., T =24 or T =48
for hourly basis slots). In this solution we also use β0 for the
peak power cost where β0 = ST0 β. OnCMCC is inspired by the
observation that the variation of the data center parameters
across days is usually lower than their variation across slots
within days. Given the limited ESDs’ sizes, the ESDs are
most likely to be best utilized to leverage the daily variation
of the data center parameters. The availability of renewable
energy, however, not only significantly varies during days
(solar energy is available only when sufficient sunshine is
there), but also significantly varies during a days and even
months in a year depending on the weather conditions and
geographical locations. However, due to the limited size of
ESDs and their physical limitations (e.g., self-discharge), it
is impractical to migrate renewable energy across such long
periods, making the cost optimality distance of OnCMCC

negligible when carbon capping requirement is relaxed. Note,
OnCMCC can only satisfy the carbon cap in a best-effort
manner. Due to the intermittent nature of the renewable power,
therefore, OnCMCC may significantly violate the carbon cap,
making it inefficient particularly when cloud needs to perform
under a relatively tight carbon capping requirement (i.e., ψ is
comparable to that of the minimum carbon footprint possible).
To avoid this problem, we extend OnCMCC to leverage the
T -slot Lyapunov optimization in order to account for the
dynamics of the carbon footprint.
C. OnCMCCLyp: T -slot Lyapunov based solution
In accordance with Lyapunov optimization, we define a
virtual queue [17] with occupancy X(t) equal to the maximum
excess carbon footprint beyond the average carbon footprint
over every T -slot. Using X(0)=0, we propagate the X(t) values
over every T -slot as follows:
t0X
+T −1 X
X(t0 + T ) = max[X(t0 ) − T ψ, 0] +
bi (τ).
(6)
τ=t0

i

Building upon Lyapunov optimization technique we design
OnCMCCLyp as given in Algorithm 1. The parameter V in
Algorithm 1 is the Lyapunov control parameter which manages
the electricity cost versus the carbon footprint reduction tradeoff. OnCMCCLyp requires only T slots ahead information.
The algorithm removes the coupling property of P1 by (i)
removing the constraint (4)), and (ii) managing the energy
storage dynamics over window (t, t+T −1) rather than S and
managing peak power reduction over (t, t+T −1), rather than S 0 .
OnCMCCLyp uses T future slots information to manage the
operational cost according to the variation of the parameters
within the frame T and the Lyapunov technique to stabilize
the carbon footprint dynamics across T -slots. In order to evaluate OnCMCCLyp, we theoretically compare its performance
against the offline optimal solution of problem P1 for the
case of (i) S 0 =T , and (ii) the energy storage dynamics only
depends on the window of T . In other words, we consider
that the operational cost and energy storage can be optimally
managed using T future slots information, and evaluate how
OnCMCCLyp can manage the carbon cap (i.e., Ψ) without
excessively increasing the operational cost.
Theorem 1. (Performance Bound Analysis of OnCMCCLyp):
Suppose X(0)=0, and that the maximum carbon footprint of the
cloud over every T slot is upper bounded by T bmax . Also define
cost∗T as the optimal solution to the special case of problem P1,
where S 0 =T , and for every t0 the beginning slot in every frame
T, we have ei (t0 + T )+ = ei (t0 ). Further, suppose data center
parameters are i.i.d. over every T -slots, and let cost(τ) and
b(τ) denote the OnCMCCLyp cost and the carbon footprint,
respectively for slot τ. Then for V > 0, and the integer variable
k = 0, 1, . . . K where S = KT we have the following:
PK−1 PkT +T −1
costT = lim supK→∞ K1 k=0
E{ τ=kT cost(τ)}
(7)
B
∗
≤ costT + V ,
v
u
t
S −1 X
K−1 kTX
+T −1
X
X
√
bi (t) ≤ Ψ + 2 KB+V(KcostT∗ −
cost(τ)),
t=0

i

where B = 12 (T 2 b2max + T 2 ψ2 ).

k=0

τ=kT

(8)

Algorithm 1 OnCMCCLyp Algorithm
1: Initialize the virtual queue X
2: for every k=1 . . . KT =S , where t=kT do
3:
Predict the system parameters over the window t+T −1
4:
Minimize:

V
+

di (t)
ci (t)
i gi (t)αi (t) + C φi,C + D φi,D

PT +T −1
+
i maxt≤τ≤t+T −1 (gi (τ) − pi,0 ) βi − X(t) τ=t

 P
t+T −1 P
1
PT

τ=t

P

i bi (τ)

(9)

(t), (1), and (3).
Subject to: (λi (t), yi (t)) ∈ RM+1
i
5:
Update the virtual queue X using (6).
6: end for

Similar steps of [12, Theroem 1] can be taken to prove the
Theorem.
From (8) and (7) it can be concluded that OnCMCCLyp
achieves near optimal performance for sufficiently large value
of S . According to Theorem 1, the OnCMCCLyp achieves
average cost no more than a factor of O(1/V) above the
optimal average cost of P1 under the Theorem’s conditions.
The large value of V comes at the expense of an O(V) tradeoff
in achieving the carbon cap.
V.

Stochastic Programming Approach

The performance of the online solutions depends on the
predictability of the parameters over T . We use stochastic
programming to take into consideration the randomness of the
predicted input parameters. The major issue in developing the
stochastic problem formulation is modeling of the uncertainties. We characterize and model uncertainties in the form of
scenarios (possible outcomes of the data), a typical scheme in
stochastic programming approach [18]. The goal is to find a
policy that is feasible for all the possible parameter realizations
(scenarios), and optimize the expectation of the objective
functions given the probability associated with each scenario.
Stochastic programming has many variants including stochastic dynamic programming. Stochastic dynamic programming
in our problem requires discretization of the ESD states for
every data centers in the cloud. This causes the number of
states at each stage of the stochastic dynamic programming to
dramatically increase. We, instead, choose to incorporate the
stochastic scenarios in the original optimization problem and
design the “deterministic equivalent” of the stochastic problem
which is a typical stochastic programming approach [18].
Consider a deterministic optimization problem of the objective
function f , the constraint function of h, and the decision
variable of x, i.e., minimize f (x), subject to h(x). Its stochastic
programming counterpart over set of stochastic scenarios of W
can be written as follows:
P
Minimize
w∈W Pr(w) f (x, w),
(10)
Subject to: h(x, w) ∀w ∈ W,
where W denotes the set of scenarios, w denotes a scenario in
W, and Pr denotes the probability function.
To characterize the scenarios, we model the prediction error
of the input parameters, i.e., workload of each front-end, Λ j ,
the available renewable power at each data center i, ri (t), the
electricity price at data center location i, αi , and the carbon
intensity of the grid power at each data center i, εgi . We denote
ri (t) the actual renewable energy available to data center i at
time t and use r̂i (t) for the predicted generation. We denote

Stage 1

Z=z1

Z=z2

Z=z3

One scenario

Stage 2

Z=z1 Z=z2 Z=z3

Z=z1 Z=z2 Z=z3

(a)

Z=z1 Z=z2 Z=z3

Fuel
coal
PL
NG
NE
wind
solar

CO2 / kWh
986
890
(b)
440
15
22.5
18

Fig. 3. (a) A sample scenario tree for the random variable Z over two stages,
(b) carbon emission of electricity fuels (CO2 / kWh).

ri (t)=(1+ζi,r )r̂i (t) , where ζi,r is the prediction error. We assume
unbiased prediction error, i.e., E(ζr )=0, and denote the variance
by σ2r which can be obtained from historic data. These are
standard assumptions in statistics. We use similar assumptions
for the prediction error of the other input parameters. We also
consider that the random variables (e.g., ζ j,λ ) are independent
random processes. As a result, the evolution of these stochastic
processes is modeled as a multivariate random process. The
marginal distribution for each of these random processes at any
time step is assumed to be a normal distribution in accordance
with the nature of unbiased prediction error. To define the
scenarios, we approximate the marginal distribution of the
random parameters (i.e., ζ) into discrete samples. The multivariate random process has therefore, LλM LrN LαN LεN samples at
each time step, where Lλ , Lr , Lα , and Lε denote the number
of discretization levels used for the workload demand, the
renewable power, the electricity price, and the grid carbon
intensity, respectively. The evolution of the random process
for the entire T slots is a huge set of scenarios. This type
of uncertainty modeling results in a multistage “scenario tree”
with T branching stages and LλM LrN LαN LεN samples at each node
of the tree (see Fig. 3(a)). Each scenario (i.e., a path from root
to a leaf of the tree) represents a possible future realization of
the random process.
Observe that the scenario tree for our problem is huge.
For instance for T =24, N=5, M=10, and Lλ =Lr =Lα =Lε =5,
the number of scenarios is 5600 . To solve the stochastic model,
the multivariate random process with huge set of scenarios
has to be approximated to a simple random process with
small set of scenarios and should be as close as possible to
the original scenario tree. There have been several scenario
reduction algorithms in the literature which typically make use
of probability metrics to choose a subset of scenarios [11],
[19]. The scenario to be deleted is selected by comparing
each scenario with the rest of the scenarios. Accordingly, the
process of one to one comparisons of the scenarios needs to
be repeated several times, which may not be feasible for a
huge initial scenario tree. For instance, the scenario reduction
algorithms in [11] make use of algorithms very similar to “kmeans” and “k-medoids” where the probabilistic measures are
used to evaluate the distance between the scenarios. Similar to
k-means, these solutions can be implemented efficiently using
parallel programming to run on a huge set of initial scenarios.
As a general case, where running scenario reduction algorithms
on the complete scenario tree may not be feasible, we can
use problem-speccific strategies to generate a scenario tree
with reasonable size as follows. Staregy one, use stochastic
aggregation rules to reduce the number of initial input

random processes (e.g., workload). Consider the random
processes X and Y with normal distribution, X ∼ N(µ1 , σ21 ),
Y ∼ N(µ2 , σ22 ), then aX+bY, where a and b are constant
numbers, also has a normal distribution as follows, aX+bY ∼
N(aµ1 +bµ2 , a2 σ21 +b2 σ22 ). Data centers may use a combination
of wind and solar energy where an aggregated random process
of the two can capture their randomness. Also, in practice,
number of front-ends (i.e., M) is very large. Suppose, every
front-end can get service from all the available data centers
in the cloud. Then, the entire input workload of all frontends can be aggregated into one single random process. In
practice, however, there are always some restrictions such as
network latency (proximity of front-ends to the data centers)
and data availability, where every front-ends can get service
from a subset of data centers. In this case we can group frontends depending on their feasible destination data centers and
aggregate the workload of each group. Strategy two, ignore
random processes which have small standard deviation.
By removing such processes we significantly reduce the initial
scenario tree size with negligible impact in the solution.
Following the model of (10), and given the set of scenarios
W and the associated probability to each scenario w ∈ W,
denoted by Pr(w), we formulate the stochastic counterpart
of the problem P1, namely P2. Next, we design our final
holistic solution OnCMCCLypstoch , i.e., the stochastic counterpart of the online solution OnCMCCLyp based on P2.
Following our road-map, given in Fig 1, OnCMCCLypstoch
combines leverages form data center and workload prediction
models, stochastic programing, data center power consumption
and power supply models, and Lyapunov optimization to
concurrently optimize electricity cost, peak power cost and
carbon footprint. Similar to OnCMCCLypstoch the stochastic
counterpart of OnCMCC, namely OnCMCCstoch is designed.
VI.

Evaluation

We simulate a cloud consisting of six data centers located
at CA, TX, GA, IA, NC and VA, most of which correspond
to Google’s data centers’ locations, namely DC1, DC2, DC3,
DC4, DC5, and DC6, respectively. The data centers are assumed to be homogeneous in terms of power consumption
and computing characteristics, such that all the electricity cost
savings and the carbon footprint reduction only comes from
spatio-temporal variation of the electricity cost and the carbon
footprint. Servers in each of the data centers are assumed
to consume 300 W at peak utilization, and average response
time and server slacks is set such that active servers have
average utilization of 75% (active servers consume 250 W
at this utilization). We set the slot length to one hour, S
and S 0 to one month, and use realistic hourly traces of the
electricity price (see Fig. 4(a) where data is taken from Locational Marginal Prices available at the corresponding RTO/ISO
websites 1 , and carbon intensity from Fig. 3(b) and fuel mix
of data center locations. We also use the renewable traces of
http://www.nrel.gov/midc/ for three sites located in
the data center locations of CA, TX and GA. To ensure data
consistency, all traces are chosen from the month of July and
August (see Fig. 5(a)). According to [4] a typical peak power
cost is 12 $/KW per averaged power over 15-minutes slots.
1 negative prices happen on the power wholesale market when a high power
generation plant meets low demand.

CA

20

CO2/kWh

$/MWh

TX

400

0
GA

50

IA

300

0

0
0

40

600
500

1000

50

700

Workload arrival rate
(request/ms)

100
0
-100

200
NC

VA

100
200

400

Time (every hour)

600

0
0

CA

TX

200

GA

IA

NC

400
Time (every hour))

(a)

(b)

VA

600

1

0
40
20
0
40

2

3

10
0
40
10
0
0

4

100

200

300
400
500
Time index (hour)

600

700

(c)

Fig. 4. Hourly traces for one month : (a) electricity price (data taken from corresponding RTO/ISO website), (b) carbon emission, (c) workload (data taken
from [20]).

Given our hourly basis slots we amortize β to 30 $/KW. In
most of the experiments we set p0 as 80% of data centers
maximum power consumption, unless stated.
We consider four front-ends, corresponding to four timezones in the U.S., and use two months (July and August) of
NASA workload Internet trace [20]. The workload of each
front-end is scaled proportionally to the number of Internet
users and shifted according to the time zone for each frontend in the corresponding area, as shown in Fig. 4(c). Each
data center has 280 servers, and the intensity of the workload
is such that at peak, 95% of servers in the entire cloud are
required to be activated. We assume that a data center can
receive workload from any of the front-ends.
We also consider a relatively large capacity for ESDs which
can sustain the data centers for an hour. The results therefore,
report a pessimistic performance of the online solutions, as
large ESDs leverage the variabilities of prices and workload
both within T slots and across T slots to minimize the cost. The
physical characteristics of ESDs are set according to data sheet
of Flood Lead Acidic (FLA) batteries used in data centers.
We use GNU Linear Programming Kit (GLPK) to solve
problems P1, P2 and the online algorithms.
Prediction results:: We use one month of training data
(July traces) and build weekly and daily Seasonal Auto Regressive Integrated and Moving Average (SARIMA) prediction
model (using “forecast” library of “R” package) to predict
workload, and electricity prices and solar energy, respectively.
Further, we use ARMA prediction model for wind energy. The
lag one (one hour-ahead) prediction error is 14%, 20% and
25% for workload, electricity prices, solar and wind energy
respectively. The error goes up to 20%, 40% and 54% for
24 lag (24 hour ahead) prediction of workload, solar and
wind energy, respectively. Observe that the prediction error
of both the solar and the wind energy in our data set is very
high which can be typically improved using sufficient training
data (using historical data of about 2-3 years [22]). Since,
sufficient training data is not always available, we perform
a pessimistic analysis on the impact of high prediction error
on our solution, and the way that stochastic programming
can remove its harmful impact. The prediction results of the
electricity prices are very different across data centers. In
particular, the electricity prices of DC4, DC5, and DC6 are
predicted with relatively high accuracy, exhibiting error of 5%

for lag one and 15% for lag 24. The electricity prices of DC1,
DC2, and DC3, however, are predicted with low accuracy,
exhibiting the error of 25% for lag one and 36% for lag 24.
Due to the data insufficiency we do not build prediction model
for carbon intensities and use accurate data.
Experiments performed: We perform experiments under different configurations: the length of T , the magnitude
of the stipulated power, p0 , the magnitude of the carbon
cap Ψ, and the prediction error. To evaluate OnCMCC and
OnCMCCLyp, we use three reference solutions namely Optimal (optimal offline solution to P1), MinCost and MinCarbon. MinCost performs global workload management over
the cloud to first minimize the cost and then the carbon
footprint. MinCarbon, on the contrary, first minimizes the
carbon footprint across the cloud and then the cost. MinCost
and MinCarbon can be viewed as representative of the previous schemes which solely focus on either cost minimization
(e.g., [7]) or carbon footprint minimization. We perform
experiments to evaluate the incremental solutions of the global
workload management scheme, which altogether framed the
holistic solution. We first, evaluate OnCMCC to assess the
efficiency of predictive solution for joint optimization of the
electricity cost, and the peak power cost. Next, we evaluate
OnCMCCLyp and compare it against OnCMCC to assess
the combined predictive and Lyapunov based technique for
coordinated management of the electricity cost, the peak power
cost and the carbon footprint. Finally, we evaluate our final
holistic solution OnCMCCLypstoch to assess its performance
for coordinated management of the electricity cost, the peak
power cost and the carbon footprint in the presence of realistic
predicted parameters and the prediction error.
A. Joint optimization of cost and carbon capping
We first relax the carbon capping constraint, and evaluate
OnCMCC versus T and the magnitude of the stipulated power,
p0 (as percentage of data centers’ maximum power). In order
to run Optimal in a reasonable time, we run the experiments
of this section using only three data centers (DC1, DC2,
and DC3). The results, shown in Fig. 5(b), depicts that the
larger the value of T , the closer the performance of OnCMCC
becomes to that of Optimal. A daily basis T (T =24) can
competitively manage the cost compared to Optimal even for
large ESDs as long as p0 is reasonably large. The magnitude
of p0 is typically such that such a power consumption arises

10

Total Cost($)

0
20
10
0
20

5000

Optimal
OnCMCC, T=6

4000

OnCMCC, T=12
OnCMCC, T=24

Time averaged
carbon (CO2 g)

6000

wind

3000
2000

10

1000

0
0

200

400
Time index (hour)

95% 90%

80% 70% 65%

50%

40%

Total cost ($)

Power
(KW), CA
Power
(KW), TX
Power
(KW), GA

solar

20

Stipulated power (p0 ) given as percentage
of the data centers` maximum power

600

(a)

x 10

4

2.5
2
MinCarbon
Optimal
OnCMCCLyp

1600

OnCMCC
MinCost

1300
1000

0

1
V value

(b)

2

3
11
x 10

(c)

4

3

2.5

2
1600

MinCarbon
Optimal
OnCMCCLyp

OnCMCC
MinCost

1300
1000

0

2

V value

4

6
11
x 10

x 105
6 Workload scenarios of S1
4
2
x 10 5
6 Workload scenarios of S2
4
2
4
x 10
Renew scens
Predicted
Actual
2
of S2
Scenario
0

5

10
15
Time (hours)

(a)

8000

6000

Total Cost($)

x 10

Request Request
per sec. per sec.

3.5

Renew
power (W)

Total cost ($)

Time averaged
carbon (CO2 g)

Fig. 5. (a) Hourly traces of solar and wind power (data taken from [21]), (b) total cost of OnCMCC versus stipulated peak power (p0 ), and (c) performance
of OnCMCCLyp versus Optimal and OnCMCC for various V values with tight Ψ.
energy cost
peak power cost
4%

11% 11%
23% 24%

4000

2000

20

(b)

0

Opt 1 3 5 7 10 15 20 25 30 35 40
Number of scenarios

(c)

Fig. 6. (a) Performance of OnCMCCLyp for various V values and for Ψ is equal to the mean carbon footprint of MinCost and MinCarbon, (b) scenario tree of
stochastic workload and renewable generation for a sample time frame T = 24, and (c) total cost of OnCMCCstoch versus OnCMCCopt (Opt) (the cost savings
are calculated with respect to the one scenario case i.e., OnCMCCpred ) and number of S1 scenarios (zero renewable energy).

rarely, and a stipulated power which is 60% of the data center
maximum power is an unrealistic value which is used to
evaluate the worst-case performance of the solution.
Next, we run MinCost and MinCarbon and perform some
experiments to evaluate OnCMCC and OnCMCCLyp for T =
24, two values of the cap, Ψ, and various values of V, the Lyapunov control parameter. First, we set Ψ to a value very close
to the carbon footprint achieved by MinCarbon. This is an
example of the case where the cloud is associated with a tight
cap. Results, shown in Fig. 5(c), depict that OnCMCC fails
to meet the cap, whereas OnCMCCLyp meets the cap for V
values less than 1.5×1011 (see Fig. 5(c)). Interestingly, Fig 5(c)
shows that for V values in the range [0.5×1011 1.5×1011 ],
OnCMCCLyp yields lower carbon footprint and achieves lower
energy cost (up to 7.5% lower cost) than that of OnCMCC.
In particular, for a V value around 1.3×1011 , OnCMCCLyp
performs very close to Optimal in terms of minimizing cost
(sum of the electricity cost, the peak power cost and ESD
cost) while satisfying the cap. Since OnCMCC independently
manages the carbon footprint across T frames, it cannot
opportunistically leverage the ups and downs of the cloud
carbon footprint and the energy cost to optimally manage the
two. OnCMCCLyp, however, takes the dynamics of the cloud
carbon footprint into account and achieves a performance near
to Optimal when V is appropriately adjusted.
Second, we set Ψ to the mean carbon footprint of MinCost
and MinCarbon, example of the case where carbon cap is
loose. Results, shown in Fig. 6(a), indicate that OnCMCC, in

this case, achieves a lower carbon than that of Optimal, albeit
at the expense of increasing the cost by 10%. OnCMCCLyp,
however, for V values less than 4.5×1011 meets the cap.
Similar to the previous case, OnCMCCLyp, when run with
appropriate V value, outperforms than OnCMCC and achieves
near Optimal performance in terms of minimizing the cost (see
Fig. 6(a) for V values in the range [2.5×1011 4.5×1011 ]). In
practice, OnCMCCLyp is expected to yield higher performance
against OnCMCC when performed for more than one month,
since the carbon intensity variations over months are huge.
Although the results of Theorem 1 is based on the assumption of T =S 0 (in the experiment S 0 =S =168 i.e., one
month), the experimental results running for T =24<<S 0 show
that OnCMCCLyp achieves near one competitive ratio against
Optimal for an appropriate V value. From the above results we
conclude that T -slot Lyapunov based solution, OnCMCCLyp,
is indeed effective for using as a holistic solution to manage the
electricity cost, the peak power cost and the carbon capping.
Note, the appropriate V value depends on the cloud parameters
e.g., the carbon footprint. The parameter B in Theorem 1, gives
a clue for adjusting V. The results so far, however, are given
for the case where the T slot future information are accurately
available. Next section evaluates the solutions when using
predicted data over T slots.
B. Stochastic optimization
We characterize ζr,i , ζλ and ζα,i through the prediction
results. Then the marginal distribution of each of them, cover-

Total Cost($)

10000
8000
6000

4000

15000

OnCMCC opt, energy cost
OnCMCC pred , energy cost
OnCMCC pred , peak power cost
OnCMCC stoch , energy cost
OnCMCC
, peak power cost

10000

stoch

19%

23% 26%

2000

30%

0% 7% 15% 26% 41% 57%
% of renew. energy when using OnCMCC
opt

(a)

10000
6%

29%

30% 27%

23% 16%

8000

OnCMCCopt energy cost
OnCMCCpred, energy cost
OnCMCCpred, peak power cost
OnCMCCstoch, energy cost
OnCMCCstoch, peak power cost

6000
4000

32%
0

0

12000

7%

5000

30%

OnCMCCopt energy cost
OnCMCCopt peak power cost
OnCMCCpred, energy cost
OnCMCCpred, peak power cost
OnCMCCstoch,energy cost
OnCMCCstoch, peak power cost

Cost($)

12000

5%

39% 42%
12% 27%

2000
95% 90% 80% 70% 65% 50% 40%

Stipulated power (p 0 ) as percentage of the
data centers` peak power

(b)

0
0
6
12
30
50
60
Peak power cost over a month (β ), $/KW

(c)

Fig. 7. Total cost of OnCMCCstoch versus OnCMCCopt (Opt) using 15 scenarios of S2: (a) various renewable energy utilization, (b) various stipulated power
(p0 ), and (c) various peak power cost (β).

ing 90% confidence interval, is approximated to five samples
each with equal probabilities. Due to their large differences,
parameters’ samples are normalized between zero and one.
We use ζr,i to represent the aggregated prediction error of both
the wind and the solar energy at each data center and ζλ to
represent the aggregated prediction error of the workload for
all front-ends (see Section V). Given a random process at one
stage, we construct the scenario tree over T and apply [11,
Algorithm 2] to construct two reduced scenario sets: (i) S1
solely from the discrete marginal distribution of ζλ , and (ii) S2
from the discrete marginal distribution of ζλ , ζr,i , and ζα,i . In
order to run [11, Algorithm 2] in a reasonable time, we evolve
the scenario trees of S1 over eight stages, and S2 over two
stages. Fig. 6(b), shows that S1 and S2 capture the randomness
of the predicted workload more accurately than that of the
predicted renewable energy due to its high prediction error. We
evaluate OnCMCC and OnCMCCLyp when using predicted
data over T =24 (namely OnCMCCpred and OnCMCCLyppred )
versus when using stochastic programming approach (namely
OnCMCCstoch and OnCMCCLypstoch ) and when using accurate data (namely OnCMCCopt and OnCMCCLypopt ). We
run stochastic solutions for different number of scenarios
(OnCMCCstoch of one scenario is identical to OnCMCCpred ).
In the figures we show the sum of the electricity cost and the
battery cost as energy cost.
Number and type of scenarios: First, we set the renewable energy of all data centers to zero and use S1. From the
results of Fig. 6(c), it can be seen that the prediction error has
a harmful effect on the peak power cost. In particular, while
OnCMCCopt can manage grid power draw to avoid the peak
power cost, OnCMCCpred with one scenario incurs $2400 for
the peak power, increasing the total cost by 66% compared
to OnCMCCopt . The total cost of OnCMCCstoch is decreased
from 6% for 3 scenarios up to 24% for 15 scenarios compared
to the total cost of OnCMCCpred (i.e., OnCMCCstoch of one
scenario). This means that OnCMCCstoch yielding $900 more
cost than OnCMCCopt (as opposed to $2400 for OnCMCCpred ),
can remove 62.5% of the harmful prediction error impact in
increasing the cost. Hence, the results agree with our initial
hypothesis that stochastic programming with small number of
scenarios can significantly mitigate the harmful impact of the
prediction error. Fig. 6(c) also shows that the peak power cost
saving of OnCMCCstoch with multiple scenarios, compared
to its deterministic counterpart (OnCMCCpred ), comes at the
expense of a slight increase in the energy cost. Further,

the performance of OnCMCCstoch does not improve when
number of scenarios increases beyond 15. Note that stochastic
programming does not guarantee an optimal performance, and
its performance heavily depends on the problem, the predicted
error magnitude, and the scenarios.
Next, we fix the number of scenarios of S2 to 15,
and scale the renewable energy of DC1, DC2, and DC3
such that the total renewable energy utilization of the cloud
varies from 0% to 57% when using OnCMCCopt . Results,
as shown in Fig. 7(a), similar to that of Fig. 6(c), indicates
that OnCMCCstoch when using S2 significantly removes the
impact of the prediction error of the workload, the electricity
prices, and the renewables (removing 66% and 89% of the
prediction error impact for 15% and 57% renewable energy
utilization cases, respectively). The less scenario coverage
of S2 for the predicted workload compared to that of S1,
causes the performance of OnCMCCstoch to downgrade by 5%
(compare 24% cost saving of OnCMCCstoch in Fig. 6(c) with
19% in Fig. 7(a) for the case of 0% renewable utilization).
The cost saving of OnCMCCstoch increases compared to its
deterministic counterpart (OnCMCCpred ) with increasing the
availability of the renewable energy. This is because taking
the randomness of the renewable and workload prediction error
into consideration results in higher utilization of the renewable
energy and consequently decreasing the cost. The impact of
such a management is higher for the higher availability of the
renewable energy.
We also evaluate the performance of OnCMCCstoch (when
using S2 with 15 scenarios and 15% renewable energy utilization case) for various stipulated peak power (p0 ) and
peak power cost (β). Fig. 7(b) shows that the cost saving
of OnCMCCstoch against OnCMCCpred is higher for higher
stipulated power where stochastic scenarios can significantly
affect the decisions. Fig. 7(c) indicates that the cost saving
of OnCMCCstoch against OnCMCCpred is higher for higher
β. Generally, OnCMCCstoch incurs very similar expected
electricity cost to that of OnCMCCpred , this is the reason
that OnCMCCstoch has a total cost almost equal to that of
OnCMCCpred for the case where β = 0. With increasing the
peak power cost the impact of prediction error on increasing
the peak power cost of OnCMCCpred is worsen which can be
mitigated using OnCMCCstoch .
Finally, we set the carbon cap, Ψ, to the mean carbon footprint of MinCost and MinCarbon and run OnCMCCLypstoch

6000

References

9000
6000

Cost($)

3000

4000
2000
0

Execution Size of problem
time (sec)

OnCMCCLyp, energy cost
OnCMCCLyp, peak power cost
OnCMCC, energy cost
OnCMCC, peak power cost

8000

30

20

1

3

5

7

10

Number of scenarios

15

[2]

10
0

Opt

number of variables
number of constraints

0

[1]

20

(a)

5

10
15
20
25
Number of scenarios

30

[3]

(b)

Fig. 8. Total cost of OnCMCCLypstoch and OnCMCCstoch versus OnCMCCopt
(Opt) and number of scenarios of S2, and (b) Overhead of OnCMCCstoch .

[4]

[5]

with appropriate V value over different number of scenarios
of S2. The results, as shown in Fig. 8(a), have a similar
trend to those of the previous results (e.g., Fig. 6(c)) in the
sense that the stochastic programming solutions (OnCMCCstoch
and OnCMCCLypstoch ), significantly remove the impact of
the prediction error, improving the cost of OnCMCCpred , and
OnCMCCLyppred up to 30% by using ten scenarios (removing
the impact of the prediction error by 66%). This cost saving
comes at a slightly energy cost increase as shown in Fig. 8(a)
and consequently a slightly carbon footprint increase.
Overhead of the stochastic solution: The cost efficiency
of the stochastic programming solutions comes at the expense
of increasing the size of the optimization problems. As a result,
the execution time of the solutions increases depending on the
computing system’s capability. Fig. 8(b) shows that the size of
the optimization problem of OnCMCC stoch linearly increases
with increasing the number of scenarios in terms of both the
number of decision variables and the number of constraints.
This translates into the exponential increase in the execution
time of the solution in our testbed (Intel Quad core i7-3770
CPU 3.4GHz, and 8G memory). Therefore it is important to
run the stochastic solutions with small number of scenarios
and an efficient implementation.

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

VII.

Conclusions

We proposed a holistic global workload management solution, which jointly minimizes data centers operational cost
(including peak power cost), while satisfying the carbon capping requirement of the geo-distributed data centers. Peak
power cost management, energy buffering and carbon capping
all introduce time coupling in the solution. We developed
an online algorithm OnCMCCLyp which (i) leverages (daily)
predictability of data center input parameters to efficiently
manage energy storage dynamics and to smoothen the power
draw from the grid, and (ii) uses T slot Lyapunov optimization
to manage the cost carbon footprint tradeoff. Our trace based
study shows that our T -slot Lyapunov based solution, OnCMCCLyp can achieve near one competitive ratio with respect to the
optimal offline solution when the Lyapunov control parameter
is appropriately adjusted, T is sufficiently large and data over
T is accurately available. However, the prediction error of the
parameters over T slots has a very harmful impact on the peak
power shaving and consequently on the cost efficiency of the
solution. Our proposed stochastic programming approach is
shown to remove up to 66% of such negative impacts.

[15]

[16]
[17]

[18]
[19]

[20]

[21]
[22]

K. Le, R. Bianchini, T. D. Nguyen, O. Bilgir, and M. Martonosi,
“Capping the brown energy consumption of internet services at low
cost,” in Green Computing Conference, 2010 International. IEEE,
2010, pp. 3–14.
S. Govindan, D. Wang, A. Sivasubramaniam, and B. Urgaonkar, “Aggressive datacenter power provisioning with batteries,” ACM Transactions on Computer Systems (TOCS), vol. 31, no. 1, p. 2, 2013.
A. Bar-Noy, M. P. Johnson, and O. Liu, “Peak shaving through resource
buffering,” in Approximation and Online Algorithms. Springer, 2009,
pp. 147–159.
D. Wang, C. Ren, A. Sivasubramaniam, B. Urgaonkar, and H. Fathy,
“Energy storage in datacenters: what, where, and how much?” ACM
SIGMETRICS Perf. Eval. Rev., vol. 40, no. 1, pp. 187–198, 2012.
R. Urgaonkar, B. Urgaonkar, M. J. Neely, and A. Sivasubramanian,
“Optimal power cost management using stored energy in data centers,”
in Proc. ACM SIGMETRICS, 2011, pp. 221–232.
S. Govindan, A. Sivasubramaniam, and B. Urgaonkar, “Benefits and
limitations of tapping into stored energy for datacenters,” in ISCA.
IEEE, 2011, pp. 341–351.
A. Qureshi, R. Weber, H. Balakrishnan, J. Guttag, and B. Maggs,
“Cutting the electric bill for Internet-scale systems,” in Proc. ACM
SIGCOMM, 2009, pp. 123–134.
V. Kontorinis, L. E. Zhang, B. Aksanli, J. Sampson, H. Homayoun,
E. Pettis, D. M. Tullsen, and T. S. Rosing, “Managing distributed UPS
energy for effective power capping in data centers,” in ISCA. IEEE,
2012, pp. 488–499.
A. H. Mahmud and S. Ren, “Online capacity provisioning for carbonneutral data center with demand-responsive electricity prices,” ACM
SIGMETRICS Perf. Eval. Rev., vol. 41, no. 2, pp. 26–37, 2013.
M. Etinski, M. Martonosi, K. Le, R. Bianchini, and T. D. Nguyen,
“Optimizing the use of request distribution and stored energy for cost
reduction in multi-site internet services,” in SustainIT. IEEE, 2012.
N. Growe-Kuska, H. Heitsch, and W. Romisch, “Scenario reduction and
scenario tree construction for power management problems,” in Power
Tech Conference Proceedings, vol. 3. IEEE, 2003, pp. 7–pp.
Z. Abbasi, M. Pore, and S. K. Gupta, “Online server and workload management for joint optimization of electricity cost and carbon footprint
across data centers,” in IPDPS. IEEE, May 2014.
Z. Zhou, F. Liu, Y. Xu, R. Zou, H. Xu, J. C. Lui, and H. Jin,
“Carbon-aware load balancing for geo-distributed cloud services,” in
IEEE MASCOTS, 2013.
Z. Liu, A. Wierman, Y. Chen, B. Razon, and N. Chen, “Data center
demand response: Avoiding the coincident peak via workload shifting
and local generation,” Performance Evaluation, vol. 70, no. 10, pp.
770–791, 2013.
V. S. Pappala, I. Erlich, K. Rohrig, and J. Dobschinski, “A stochastic
model for the optimal operation of a wind-thermal power system,”
Power Systems, IEEE Transactions on, vol. 24, no. 2, pp. 940–950,
2009.
Z. Abbasi, “Sustainable cloud computing,” Ph.D. dissertation, Arizona
State University, 2014.
M. J. Neely, “Energy optimal control for time-varying wireless networks,” Information Theory, IEEE Transactions on, vol. 52, no. 7, pp.
2915–2934, 2006.
A. Shapiro, D. Dentcheva et al., Lectures on stochastic programming:
modeling and theory. SIAM, 2009, vol. 9.
M. Kaut and S. W. Wallace, “Evaluation of scenario-generation methods
for stochastic programming,” Pacific Journal of Optimization, vol. 3,
no. 2, pp. 257–271, 2007.
M. F. Arlitt and C. L. Williamson, “Web server workload characterization: The search for invariants,” in ACM SIGMETRICS Perf. Eval. Rev.,
vol. 24, no. 1, 1996, pp. 126–137.
[Online]. Available: http://www.nrel.gov/midc/
M. G. De Giorgi, A. Ficarella, and M. Tarantino, “Error analysis of
short term wind power prediction models,” Applied Energy, vol. 88,
no. 4, pp. 1298–1311, 2011.

Rapid Evidence-based Development of Mobile Medical IoT Apps


Priyanka Bagade
Intel Corporation, Portland, Oregon Email: pbagade@asu.edu

Ayan Banerjee and Sandeep K.S. Gupta
IMPACT Lab, CIDSE, Arizona State University, Tempe, Arizona Email: {abanerj3,sandeep.gupta}@asu.edu

Abstract--Mobile medical apps (MMAs) work in close loop with human physiology through sensing and control. As such it is essential for them to achieve intended functionality, without having harmful effects on human physiology, affecting the availability of the service and compromising the privacy of health data. However for a mobile app manufacturer, generating evidences regarding safety, sustainability and security (S3) of MMAs can be time consuming. To accelerate the development of S3 assured MMAs, we propose Health-Dev  tool that takes high level description of MMAs and automatically generates validated code and evidences of safety, security, and sustainability. Using the mobile artificial pancreas medical control application we show that Health-Dev  tool can generate code that satisfies requirements and reduce development time by a factor of 1.8.

Glucosemeter Sensed blood glucose level

Blood glucose level data

Controller Decision

Upload

Insulin Infusion

Actuation Information (Drug Concentration) Infusion Pump

Fig. 1. System model for mobile medical applications. Artificial pancreas mobile application example.

I. I NTRODUCTION The Internet of Things (IoT) is a network of systems connected to the Internet in which various embedded systems (wearable sensors/actuators) communicate with each other to exchange information. One of the example of IoT is Mobile Medical Apps (MMAs) used to provide pervasive health. According to industry surveys, by 2018, more than 1.7 billion smartphone and tablet users will have downloaded an MMA [1]. Such widespread adoption of smartphone based medical apps is opening new avenues for innovation, bringing MMAs to the forefront of low cost healthcare delivery. These apps often control physiology and work on sensitive data, thus it is necessary to have evidences of their correct functioning before actual deployment. The key challenges in ensuring correct working of MMAs are maintaining privacy of health data, long term operation of wearable sensors and ensuring no physical harm to the user [2]. Providing such evidences for safety, security and sustainability (S3) properties can increase development time and may require higher skills set for the developer. Hence, although ensuring S3 is essential, it often acts as a hindrance to innovation. In this paper, we propose Health-Dev  tool which takes high level models of MMA and rapidly generates S3 satisfied code as well as evidences. We also show examples where using Health-Dev  reduces development time by almost 1.8 times. In an MMA (Figure 1), a wearable sensor can collect physiological data, and communicate it to the smartphone through the wireless channel. The smartphone in turn can communicate the data to a cloud server, where it can be stored
 We are thankful to Yi Zhang and Paul Jones at FDA for their insights in regulatory challenges of MMAs. The work is partly funded by NSF IIS 1116385 and NIH NIBIB EB019202.  This work was done when Priyanka was at ASU.

in an electronic health record (EHR). The smartphone can also control safety critical devices such as drug infusion pump, thus forming a cyber-physical system. The control inputs computed by the MMA directly affects the human physiology. In such cases, the control algorithms have to be verified for patient safety. Due to lack of in depth knowledge about human physiology, smartphone app developers might fail to consider these safety concerns. Further, since the smartphone is not a dedicated healthcare platform, non-medical apps such as games, can affect patient safety by compromising the availability of the medical app (e.g. via rapid battery depletion). These apps often obtain physiological data from the wearable sensors which work on low power. Thus to have the sensor availability for long time, sustainable sensor design is needed. All communication between sensors, smartphone and cloud takes place through the wireless channel which is prone to security attacks. Thus the wirelessly communicated data should be encrypted. However, current app development and certification methods for MMAs do not adequately address these safety, sustainability and security issues. Recently there has been efforts taken for data security evidence generation in MMAs with companies such as Happtique [3], however most of these approaches are subjective. Moreover, the proposed regulation system had failed to effectively detect serious security flaws in apps such as password stored in plain text. This gives rise to a need of objectively evaluating S3 requirements of MMAs before market approval. In this paper, we hypothesize that a model based app development approach will enable us to objectively evaluate S3 requirement of MMAs and possibly generate trustworthy (S3 assured) code for sensitive operations such as actuator

App 1

App 2

App3

Traditional Method
Artificial Pancreas Broadcast Receiver GUI

Proposed Method
Artificial Pancreas

Trustworthy Data Manager Broadcast Receiver
Content Provider

TDM
Runtime Requirements Validator Safety Security Sustainability

GUI Execute Controller Algorithm
Content Resolver

Execute Controller Algorithm

Content Provider WiFi

Sensors

Actuator Cloud

Glucomete r, Infusion Pump Interface WiFi

Fig. 2. Conceptual operating model for MMAs. The Trustworthy Data Manager monitors interfacing calls from different applications and verifies them against requirements.

Fig. 3. Change in MMAs data sharing methodology. For access to sensing, actuation, and communication services the developer should issue interprocess calls to the TDM.

control, sensor interface and operation, and cloud communication. If the application design conforms to a standard model then automated tools such as hybrid automata based model checking techniques can be used for evaluating patient safety due to interaction between human physiology and app software as shown in our previous work [4]. Sensor design optimizer can be used to obtain sustainable design for long term availability. Moreover, security enabled software for interfacing the smartphone with sensors, actuators, and the cloud services can be generated automatically. To enable use of automated verification tools, we propose an App model, an operating model for the MMAs. It isolates the MMA sensor or actuator interface, data communication and data storage from the core mobile device graphical user interface (GUI) and processing algorithms. An app model should not limit the functionalities of the smartphone that can be exploited by the developers, nor it can cause a performance degradation of the app in terms of response time or memory and battery usage. The main contributions of the paper are as follows: App Model: The proposed app model can enable safe and secure interactions of MMAs with sensors, cloud and other apps through a trustworthy entity. It can contain multiple apps working individually or sharing data with each other under accurate permissions. Trustworthy Framework for development of MMAs: We propose Health-Dev  tool, which can be used by a developer to either generate evidences for trustworthiness of MMAs or critical parts of the code. In our manifestation, the safety verification of smartphone controller algorithm is done using formal modeling with spatio-temporal hybrid automata (STHA) [5], sustainable sensor design for a defined time, hardware and software constraints is obtained by using an optimizer algorithm, and the security enabled data communication over wireless channel is done by generating the interfacing components with automated code generator, Health-Dev  . Finally, we evaluate improvements in development time with Health-dev  as opposed to manually coding using software development effort estimation tools, COCOMO [6]. II. R ELATED W ORKS The existing frameworks for mobile health app development can be classified into three categories as: a) API support, b) programming abstractions, and c) automated code generation. The mobile app development with APIs is popular due its ease of programming [7][9], however, these do not verify

S3 requirements. UPHIAC [10] and PRISM [11] frameworks provide health data security with APIs to interface with smartphone sensors and cloud for data storage. However these frameworks do not check for safety and sustainability requirements of health apps. Programming abstractions for sensors are being widely used to allow developers to write minimal sensor code to include wearables in health apps [12] [16]. Considering the limited energy availability on wearables, energy efficiency techniques are supported by Reflex [14], LittleRock [15] and Turducken [16]. However these methods require sensor programming from the developer. Thus to reduce health apps development time and to get bug-free code, automated sensor code generators were proposed [17][20]. Still they require developers to write code for smartphone apps and its vulnerable components such as interfacing with sensors, actuators and cloud. This led to fully automated code generators for sensors and smartphone apps [21], [22]. However they only consider system safety and do not consider sensor sustainability and data security. This paper discusses an automated code generator for health apps which can ensure their trustworthiness while reducing developer burden. III. A PP M ODEL We hypothesize that S3 assured MMAs should have an operating model, an app model as shown in Figure 2. In this new model of applications, the smartphone software itself is in charge of only the graphical and algorithmic aspects of the application. Every instance of data communication to the sensor, cloud, data storage in the smartphone, control inputs to the actuator, interaction with the user should be through a Trustworthy Data Manager (TDM). The app model enables the development of MMAs in the form of a suite, where participating apps are certified by the regulatory agencies against safety, sustainability and security, defined as follows: Safety: Any form of control input from MMA is safety assured by checking it with hybrid automata based model. Sustainability: Any sensor communicating with TDM supports long term availability with the sustainable design. Security: Any form of data communication is privacy ensured by TDM. Also, data collected by the different applications are kept in secured databases (similar in principal to application sandboxing) shared with certified apps only. Change in Development Paradigm: We consider a smartphone based wearable closed loop blood glucose level control

AP MMA App

App request
Broadcast Receiver

Message parser (ID, Data) Select algorithm for ID ID Data Model Simulator (Data) System parameters Match (system parameters, Requirements) Yes Create new system call Send request No

Load requirements for ID from database
Requirements

Set off Alarm

Fig. 4. Flowchart of the TDM runtime environment for validating the Apps operation against requirements.

application to explain the app model. The artificial pancreas application takes glucose levels from a continuous glucose monitor (CGM), computes the next infusion level according to a control algorithm, and sends actuation information to an infusion pump through the wireless channel. Figure 3 shows change in app development method with the app model as compared to the traditional method. Using the traditional method, to develop Artificial pancreas app, developer needs to implement GUI, the control algorithm, CGM sensor code, security algorithms at both smartphone as well as sensor end. Considering the low power working and low level implementation language requirements, developing sensor code can be a challenging task. Developer is also responsible for writing methods to access and share data with other apps using content provider and resolver respectively, receiving broadcast messages, accessing WiFi and interfacing with cloud, CGM sensor and infusion pump. On the other hand, to develop artificial pancreas (AP) app following the proposed app model, developer only develops the smartphone software to display and execute the control algorithm. The Health-Dev  tool takes care of developing sensor code and generating TDM app. TDM interfaces an app with sensor, other apps and cloud in S3 assured manner. Thus, the broadcast receiver, WiFi access, sensor interface required for data sharing and accessing are implemented at TDM end. Developer needs to implement only one content resolver through which it can access data from TDM. The AP app queries TDM to get data from CGM and sends data to the infusion pump instead of directly communicating with them. IV. T RUSTWORTHY DATA M ANAGER The TDM runtime process monitoring and validation functionality is not only cognizant of user preferences but also has enough embedded intelligence so that it can detect anomalous behavior of an app as shown in Figure 4. An app can register with a TDM by first establishing a Binder IPC channel with the TDM. Through the Binder channel it passes the UUID to

TDM. The app can have its Multi-Tier Model encrypted using the UUID so that no other app except the TDM can access its models. TDM has a broadcast listener, which continuously listens for app requests. Once it receives a request from MMA, it will invoke a message parser which separates the requests into two parts: ID and Data. Corresponding to every ID there is an algorithm for runtime requirement checking. The algorithm will load requirements for the specific ID from the database. These requirements can be the safety regulations of mobile medical apps, the sensor sustainability requirements and the HIPAA security requirements. Simultaneously a model simulator will take the data as input and simulate a model for certain time to generate system parameter. Models may include Finite State Automata based representation of secure data management schemes, hybrid automata models of safety assessment of control algorithms and optimization algorithm to check sustainable sensor configuration. The TDM will be equipped with simulators that can execute the models and estimate the expected operating condition or state of the app for current phone context (CPC). These simulators can be reachability analysis on hybrid automata for MMA safety verification [5], sustainable sensor configuration using optimization algorithm [23]. If the output of the simulator matches the requirements, the TDM will create new system call to issue the initial app request. If the requirement is not matched, the TDM will set off an alarm. The runtime validation functionality of the TDM can be expressed using functions fsaf ety , fsustainability and fsecurity as shown in Equations 1, 2 and 3.
fsaf ety = < M1 (CP C, Vh ), T h > fsustainability = < M2 (CP C, P ), Bmin > fsecurity = < M3 (CP C, M3 ), T rue or F alse > (1) (2) (3)

Here, M1 , M2 and M3 are models used by TDM to validate the app request against safety, sustainability and security. Vh is the physiological parameter controlled by the app and T h is the safety threshold value. P is the power required for the sensor for specified operation in the system call whereas Bmin is the minimum threshold value for the sensor battery level. Data access request M3 , is checked against M3 to see if the request is valid or not for a given CPC. A more accurate estimation of the actual state of the app can be obtained by tracing the system calls made by the app. The app makes different types of system calls such as Broadcast Provider for radio transmission or Content Provider for accessing the sensors and actuators. All these system calls goes through the TDM app in our proposed method. and hence the TDM can trace the state of the app from these system calls. These two states can be compared in runtime. If there is a match then the app behaves as the proposed model and hence its system call is trustworthy and the TDM allows the requested operation. If the app operation does not match the model then a default operation is performed. This default operation has to be specified by the App and is kind of a fail safe mode of the application. If this fail safe mode is ever reached an exception will be raised by the TDM.

AP MMA App Intent intBroad = new Intent(); intBroad.putExtra("InfusionPumpInput", insulinAmount); sendBroadcast(intBroad); \\ App request Broadcast Receiver onReceive(Context arg0, Intent arg1) Message parser arg.getExtras(); InfusionPumpInput, insulinAmount \\ (ID, Data) Select algorithm for InfusionPumpInput i.e. safety checking algorithm ID Load requirements for ID from database Data Hybrid Automata simulator (insulinAmount)

Health App Development
App Software
Control Graphical + Algorithm User Interface
Model (STHA)

Interfaces Sensor/ + Communication Actuator Specs Interface

Health-Dev  Safety
STHA Static Analysis

Sustainability
Multi object Optimization

Security
Encryption Algorithms
S3 Tool Set

S3 enables App Configuration

Glucose level Glucose level after t time - Range 70 to 105 mg/dl (System parameters ) (Requirements) Match (system No parameters, Set off Alarm Requirements) Yes Create new system call Send request

Code Generator
Developer Written Code Safety and Static Analysis Reports

Sensor Code

Interface Code

Trustworthy Health App Software

Fig. 5. Example flowchart for TDM runtime environment for artificial pancreas application.

Fig. 6. Architecture for development of S3 assured MMAs. The developer writes the graphical UI and algorithms and also provides models for safety, security, and sustainability. For all interfaces it sends IPC calls to TDM. The TDM then uses the S3 assurance toolset to verify requirements.

Artificial Pancreas Example TDM: Figure 5 shows the MMA request processing by TDM for artificial pancreas app.Artificial Pancreas MMA sends the message of change in insulin concentration to TDM using Intent message. Broadcast listener implemented in TDM receives this message and invokes a message parser which separates the requests into two parts, ID and Data. For artificial pancreas, the ID is infusion pump input and data is amount of insulin to be infused. As for this example the request is for infusion pump to change drug concentration, the algorithm for checking if the requested insulin amount is safe or not is called. The algorithm loads requirements for the specific ID from the database i.e. glucose safe range. Simultaneously a model simulator takes the data as input and simulate a model for certain time to generate system parameter. For safety verification hybrid automata model for artificial pancreas is used. The simulator takes amount of insulin to be infused as input, simulated hybrid automata for that input and outputs blood sugar level after certain amount of time. Now this output matches the requirements i.e. the sugar level within required range, the TDM creates new system call to issue the initial app request. If the requirement is not matched, the TDM will set off an alarm. V. F RAMEWORK FOR S3 ASSURED MMA D EVELOPMENT We propose a framework, Health-Dev  , to allow the developer to implement MMAs following the app model in Figure 2 and automatically ensuring safety from interactions, sustainability in sensor and security of data. It also generates TDM for interfacing with sensors, actuators, and cloud. We envision that the Health-Dev  tool for trustworthy development of MMAs should have the architecture as shown in Figure 6. A developer can use such an architecture to either verify whether the developed MMA satisfies trustworthiness requirements or automatically generate critical parts of the code that can impose S3 vulnerabilities. The developer can provide a high level design of his app as input to the architecture. The high level specification considered in this paper

is Architectural Analysis and Description Language (AADL) since it is industry standard and is generic enough to describe computational methods of smartphone apps as well as physiological aspects of human body [24], [25]. The high level design can be further optimized for sensor sustainability constraints using the optimizer algorithm. The algorithm gives designs, which are sustainable under certain time constraints [23]. For the controller app, control algorithm on smartphone end can be theoretically verified [5]. In this case, the developer should specify a hybrid automata representation of the application design in the Health-Dev  tool input. In our previous work, we have shown how AADL can be effectively used to represent hybrid automata [24]. The hybrid automata represented by the developer can then be extracted from the model and can be analyzed using the reachability analysis methods to determine patient safety. The rest of the design can be used by the automatic code generator to generate sensor interface and communication code for the MMA. In our previous work, we have developed Health-Dev [26], which can convert an AADL design into sensor and smartphone implementations. In addition to using the standard software primitives, Health-Dev can be extended to have security plug-in, the TDM app, sensor design optimizer and app safety verifier which can be added on top of interfacing code to ensure S3 properties. The STHA reachability analysis [5], sustainable optimized sensor design [23] and the security primitives generate a certification report stating their findings. The code and the certification report can then be reviewed by an expert personnel in the field to certify the application. If the MMA fails certification, then the developer can redesign and again use the same architecture. A. Sustianable Design for Wearable Sensors Wearable sensors typically scavenge energy from light or human body heat. The availability of scavenging sources is unpredictable. Thus, to have sustainable working of the sensors, we form an optimization problem to achieve energy neutrality. A system is energy neutral when the storage device

level remains same before and after the computation i.e. energy needed for the computation is solely obtained from a scavenging source. In this problem formulation, we optimize the data sending frequency of the sensor depending on the availability of scavenging source. Let us consider that sending frequency is fc when scavenging source is available and fd when scavenging source is unavailable. The sensor battery will be charged only when the scavenging source is available. Thus, to achieve energy neutrality, we need to find values of fc and fd such that the execution should require only the energy obtained from scavenged source. Further, even the scavenging source is available, the sensor battery is unable to store energy beyond the battery capacity. Given these constraints, our aim is to minimize fc and fd . Equation 4 shows the simplified optimization formulation used to find optimal sensor design [23].
Find fc and fd that optimizes (Power consumption should not go above scavenged power) such that Solar Energy: obtained when the source is available, Energy Neutrality: battery level = initial battery level, Storage Constraint: battery level  0, Battery Capacity Constraint: Stored Energy  Battery Capacity.

(4)

The optimizer algorithm gives a set of time constrained sustainable designs. Developer can chose any one of the designs depending on availability and configuration settings. B. Safety Verification in Smartphone Controller Design The safety verification ensures that the side-effects of interactions between human physiology and sensors are within accepted limit. These interactions are typically governed by smartphone control algorithm when sensor acts as an actuator. In such a scenario, developer provides the hybrid automata which embeds the control algorithm while specifying high level design. Reachability analysis is then performed on the specified hybrid automata while considering optimized time constrained designs as initial set. One of the safety verification methods is using spatiotemporal hybrid automata (STHA) [5] and related reachability analysis algorithm. STHA considers effect of interactions over time as well as over space to give more accurate results as compared to only time-based analysis. It considers the interactions of the form,
Ai V 2V = Bi + Ci V + ui , t x2 (5)

supports the use of physiological signal processing algorithms to process health data. The current Health-Dev input model allows to specify various components such as sensing, computation and communication. In sensing, type of sensors, motes, sampling frequency, sending frequency can be specified. The computation component includes processing of data on both sensors and smartphone which uses various physiological algorithms from the database such as Fast Fourier Transform (FFT), peak detection, mean etc. Developer can add new algorithm as well as modify the inputs. Further, communication protocol between sensors and smartphone, as well as smartphone and cloud can be specified. It also includes specification of energy management techniques such as duty cycle, radio power level. We have extended the specification of communication component to include security protocol from a security algorithm database which can also be extended with new protocols. TDM app generation: The code generator also generates a TDM app which ensures the secure wireless communication. It includes the security algorithm specified by the user in the input. The code generator maintains a code template for generating TDM app. On getting the specification of the security algorithm, it pulls the algorithm from database and inserts in the code. It contains encryption-decryption algorithms such as PEES [27], Advanced Encryption Algorithm (AES) to secure the physiological data over vulnerable wireless channel. In TDM code generation, code generator uses the Application Programming Interfaces (APIs) to allow communication between external wireless mote and an Android smartphone via Bluetooth. It consists of two components, Bluetooth API and sensor handler. Bluetooth API ensures connection establishment and data communication between mote and smartphone while sensor handler acts as a manager and registers all user assigned sensor, different algorithms associated with each sensor and handling of data received from the particular sensor. For each of the wireless communication calls, the generator appends the security protocol. VI. E VALUATION OF H EALTH -D EV  T OOL In this section, we evaluate Health-Dev to determine the overhead during runtime and the development effort reduction. A. Overhead analysis of TDM runtime application To check the overhead of runtime validation in TDM app, we have implemented the safety and sustainability models to get their execution time. Runtime security validation uses FSM which checks for the program flow with no complex computation involved thus it is assumed to no time. For safety analysis, pharmacokinetic model [28] for the artificial pancreas example discussed in Section III is implemented as Android service. It took 10.894 seconds to check for the given drug concentration value if the blood glucose level will remain within safety thresholds. For sustainability, the objective function in the optimization problem formulation (Equation 4) is implemented as Android service to check for the given sensor configuration change if the sensor will satisfy energy neutrality constraint. The execution time for the sustainability model was 0.03 seconds.

This technique can be easily integrated with Health-Dev tool which can provide evidence for safety of the medical app. C. Automated Implementation of Security-enabled MMAs We have developed an automated model-based code generator, Health-Dev [26]. It takes requirements of smartphone and physiological sensors in the form of models in AADL and generates downloadable code for them. The automation in code generation helps to reduce manual implementation errors in developing wireless health monitoring apps. Health-Dev also provides graphical user interface to input requirements for the user who does not have any programming knowledge. It

B. Effort Reduction in Developing MMAs Generating S3 evidences for MMAs might increase the burden on development end. Thus to validate that Health-Dev  tool doesn't hinder the development process but accelerates it by automated code generation, we estimated and compared efforts required with manual implementation. For that, we used Constructive Cost Model (COCOMO) [6], a cost-effort estimation tool. As an example, we considered development of PetPeeves [29] app which requires developing user interface, ECG sensor code and communication interface between smartphone and sensor. Manual implementation of PetPeeves app required 2553 lines of code (LOC) without any S3 assured evidences. With Health-Dev  , the communication and sensor code is automatically generated with TDM app, leaving developer to write only 1678 LOC including sensor specifications. After executing the model, we got effort required with Health-Dev  tool as 6.9 developer-month, whereas for manual implementation it came out as 12.1 developer-month which is almost 1.8 times more. The developer cost is also increased by 1.8 times for manual implementation. This shows that the proposed tool not only saves the development efforts but guarantees correct working of the software by generating evidences of S3 requirements. VII. C ONCLUSIONS In this paper, we focus on developing evidence-based medical apps for smartphones. The paper proposes an automated code generator, Health-Dev  for medical apps which generates evidences of safety, sustainability and security of the apps without hampering the development efforts. The COCOMO [6] effort estimation tool shows that the time required for developing MMAs with Health-Dev  is nearly 1.8 times less than manual implementation. R EFERENCES
[1] FDA. Mobile medical applications. http://www.fda.gov/medicaldevices/productsandmedicalprocedures /connectedhealth/mobilemedicalapplications/default.htm. [2] S. K. S. Gupta, T. Mukherjee, and K. K. Venkatasubramanian, Body Area Networks: Safety, Security, and Sustainability. New York, NY, USA: Cambridge University Press, 2013. [3] P. L. Dolan. http://exclusive.multibriefs.com/content/health-appcertification-program-halted. [4] P. Bagade, A. Banerjee, and S. Gupta, "Evidence-based development approach for safe, sustainable and secure mobile medical app," in Wearable Electronics Sensors, ser. Smart Sensors, Measurement and Instrumentation, S. C. Mukhopadhyay, Ed. Springer International Publishing, 2015, vol. 15, pp. 135174. [5] A. Banerjee and S. K. S. Gupta, "Spatio-temporal hybrid automata for safe cyber-physical systems: A medical case study," in Cyber-Physical Systems (ICCPS), 2013 ACM/IEEE International Conference on, April 2013, pp. 7180. [6] "Cocomo model," http://csse.usc.edu/csse/research/COCOMOII/ cocomo main.html. [7] X. Chen, A. Waluyo, I. Pek, and W.-S. Yeoh, "Mobile middleware for wireless body area network," in Engineering in Medicine and Biology Society (EMBC), 2010 Annual International Conference of the IEEE, 31 2010-sept. 4 2010, pp. 5504 5507. [8] B. Kaufmann and L. Buechley, "Amarino: a toolkit for the rapid prototyping of mobile ubiquitous computing," in Proceedings of the 12th international conference on Human computer interaction with mobile devices and services. ACM, 2010, pp. 291298. [9] I. for Android, "Android development tools," https://www.sparkfun.com/ products/retired/10748.

[10] T. Laakko, J. Lepp anen, J. L ahteenm aki, A. Nummiaho et al., "Mobile health and wellness application framework," Methods Inf Med, vol. 47, no. 3, pp. 217222, 2008. [11] T. Das, P. Mohan, V. N. Padmanabhan, R. Ramjee, and A. Sharma, "Prism: platform for remote sensing using smartphones," in Proceedings of the 8th international conference on Mobile systems, applications, and services. ACM, 2010, pp. 6376. [12] W. Brunette, R. Sodt, R. Chaudhri, M. Goel, M. Falcone, J. Van Orden, and G. Borriello, "Open data kit sensors: a sensor integration framework for android at the application-level," in Proceedings of the 10th international conference on Mobile systems, applications, and services. ACM, 2012, pp. 351364. [13] F. X. Lin, A. Rahmati, and L. Zhong, "Dandelion: a framework for transparently programming phone-centered wireless body sensor applications for health," in Wireless Health 2010. ACM, 2010, pp. 7483. [14] F. X. Lin, Z. Wang, R. LiKamWa, and L. Zhong, "Reflex: using low-power processors in smartphones without knowing them," ACM SIGARCH Computer Architecture News, vol. 40, no. 1, pp. 1324, 2012. [15] B. Priyantha, D. Lymberopoulos, and J. Liu, "Enabling energy efficient continuous sensing on mobile phones with littlerock," in Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks. ACM, 2010, pp. 420421. [16] J. Sorber, N. Banerjee, M. D. Corner, and S. Rollins, "Turducken: hierarchical power management for mobile devices," in Proceedings of the 3rd international conference on Mobile systems, applications, and services. ACM, 2005, pp. 261274. [17] J. B. Lim, B. Jang, S. Yoon, M. L. Sichitiu, and A. G. Dean, "Raptex: Rapid prototyping tool for embedded communication systems," ACM Trans. Sen. Netw., vol. 7, pp. 7:17:40, August 2010. [18] E. Cheong, E. A. Lee, and Y. Zhao, "Viptos: a graphical development and simulation environment for tinyos-based wireless sensor networks," in Proceedings of the 3rd international conference on Embedded networked sensor systems, ser. SenSys '05. New York, NY, USA: ACM, 2005, pp. 302302. [19] M. Mozumdar, F. Gregoretti, L. Lavagno, L. Vanzago, and S. Olivieri, "A framework for modeling, simulation and automatic code generation of sensor network application," in Sensor, Mesh and Ad Hoc Communications and Networks, 2008. SECON '08. 5th Annual IEEE Communications Society Conference on, june 2008, pp. 515 522. [20] B. Kim, L. T. Phan, O. Sokolsky, and I. Lee, "Platform-dependent code generation for embedded real-time software," in Compilers, Architecture and Synthesis for Embedded Systems (CASES), International Conference on. IEEE, 2013, pp. 110. [21] M. Paschou, E. Sakkopoulos, and A. Tsakalidis, "easyhealthapps: ehealth apps dynamic generation for smartphones & tablets," Journal of medical systems, vol. 37, no. 3, pp. 112, 2013. [22] S. Procter and J. Hatcliff, "An architecturally-integrated, systems-based hazard analysis for medical applications," in Formal Methods and Models for Codesign (MEMOCODE), 2014 Twelfth ACM/IEEE International Conference on. IEEE, 2014, pp. 124133. [23] P. Bagade, A. Banerjee, and S. K. S. Gupta, "Optimal design for symbiotic wearable wireless sensors," in Wearable and Implantable Body Sensor Networks (BSN), 2014 11th International Conference on. IEEE, 2014, pp. 132137. [24] A. Banerjee and S. K. S. Gupta, "Your mobility can be injurious to your health: Analyzing pervasive health monitoring systems under dynamic context changes," in Pervasive Computing and Communications (PerCom), 2012 IEEE International Conference on. IEEE, 2012, pp. 3947. [25] ----, "Analysis of smart mobile applications for healthcare under dynamic context changes," Mobile Computing, IEEE Transactions on, vol. 14, no. 5, pp. 904919, May 2015. [26] A. Banerjee, S. Verma, P. Bagade, and S. K. S. Gupta, "Health-dev: Model based development pervasive health monitoring systems," in Wearable and Implantable Body Sensor Networks (BSN), 2012 Nineth International Conference on, may 2012, pp. 85 90. [27] A. Banerjee, S. K. S. Gupta, and K. K. Venkatasubramanian, "Pees: physiology-based end-to-end security for mhealth." in Wireless Health. Citeseer, 2013, p. 2. [28] D. Wada and D. Ward, "The hybrid model: a new pharmacokinetic model for computer-controlled infusion pumps," Biomedical Engineering, IEEE Transactions on, vol. 41, no. 2, pp. 134 142, feb. 1994. [29] J. Milazzo, P. Bagade, A. Banerjee, and S. K. S. Gupta, "bhealthy: A physiological feedback-based mobile wellness application suite," in Proceedings of the conference on Wireless Health. ACM, 2013.

1

MT-Diet Demo: Demonstration of Automated
Smartphone Based Diet Assessment System
Junghyo Lee, Ayan Banerjee, and Sandeep K. S. Gupta
IMPACT Lab, CIDSE, Arizona State University, Tempe, AZ
Email: {jlee375,abanerj3,sandeep.gupta}@asu.edu

Abstract—Background: According to several recent research
results [1]–[4], obesity can increase the risk of many diseases
such as diabetes, chronic kidney disease, metabolic disease,
cardiovascular disease, etc. To prevent and treat the obesity
efficiently and effectively, diet monitoring is an important factor.
Purpose: Manual self-monitoring techniques for diet suffer from
drawbacks such as low adherence, underreporting, and recall
error [5]–[7]. Camera based applications that automatically
extract type and quantity of food from an image of the food plate
can potentially improve adherence and accuracy. However, stateof-the-art systems [8] have fairly low accuracy for identifying
cooked food (only 63%) and are not fully automatic. To overcome
these drawbacks such as low adherence, underreporting, recall
error, low accuracy, and semi-automatedness, we introduce MTDiet, a fully automated diet assessment system. It can identify
cooked food with an accuracy of 88.93%. This is a significant
improvement (over 20%) from the current state-of-the art system.
Method: MT-Diet is a smartphone-based system that interfaces a
thermal sensor with a smartphone. Using this system a user can
take both thermal and visual images of her food plate with just
one click. We used a database of 80 frozen meals which contain
several different types of foods so that the actual total number
of our food database 244 and the database has 33 different
types of foods. By using the database, we demonstrate two core
components: a) food segmentation, separating food items from
the plate and recognizing multiple food items as a single food
item, and b) food identification, determining the type of foods.
Result: MT-Diet food segmentation methodology is fully automatic and requires no user input as opposed to recent works,
the accuracy of separating food parts from the plate was 97.5%.
The accuracy of food identification using Support Vector Machine
with Radial Basis Function kernel based on color, texture, and
histogram of oriented gradients features is 88.5%.
Conclusion: We suggest a new and novel approach for diet
assessment, MT-Diet. Our approach can potentially be an inexpensive, real time for the feedback on calorie intake, easy-to-use,
privacy preservation, personalization based on eating habits of
individuals, and fully automated diet monitoring system. The tool
can also be used to conduct clinical studies to develop models of
meal patterns that can be incorporated to design better artificial
pancreas.

systems easy to use and more usable than text based records
[9]. Another study on a medium size cohort of adults aged 18
to 24 years has reported that nearly 87 % of users feel that they
will not be opposed using a food image based diet monitoring
mobile application for long term [10]. However, feedback from
a study on the usability of MyFitnessPal [11], an image based
diet monitoring application indicate that users would prefer
more information related to type of food and calorie intake
from just an image upload. According to recent surveys [12]–
[14], image based applications that automatically extract type
and quantity of food from an image of the food plate have
good accuracy for identifying fresh food. However, a larger
share of daily calorie intake comes from hot cooked food for
which these systems have fairly low identification accuracy
(nearly 63% accuracy at best). This is because hot/warm
foods also tend to be mixed dishes (e.g., lasagna), which are
difficult to assess using color images. We demonstrate MTDiet, a smartphone based automated cooked food identification
system that can determine the type of cooked food on plate
with nearly 90% accuracy.
MT-Diet interfaces a thermal sensor with a smartphone,
so that a user can take both thermal and visual images
of his/her food plate with just one click. The system uses
thermal maps of a food plate to increase accuracy of extraction
and segmentation of food parts, combines thermal and visual
images to improve accuracy in the detection of cooked food.
Preliminary testing results show that MT-Diet can determine
the type of food consumed with an average accuracy of 90%,
which is a significant improvement from the current stateof-the-art. MT-Diet is implemented as a part of the bHealthy
application suite for behavioral healthy monitoring [15].
II. D EMONSTRATION S ETUP AND P LAN
MT-Diet application
Requirements: a) cooked foods, b) a smartphone built in a camera, c) a
seek thermal sensor [16], and d) two small caps filled with cold water.

I. I NTRODUCTION
Accurate assessment of dietary intake is important for: i)
analyzing the relationship between caloric intake and health
outcomes such as obesity, ii) evaluating outcomes of dietary
change interventions, iii) ensuring compliance with dietary
recommendations, iv) self-motivated diet control, or v) identifying factors that induce change in dietary intake so that they
can be used in targeted interventions. Recent studies have reported that smartphone users find image based diet monitoring
This project is partially funded by NSF IIS 1116385 and NIBIB EB019202.

Inputs: a) a plate full of hot food, b) color image from smartphone
camera, and c) thermal image from infrared camera.
Outputs: a) Segmented food images, b) Food type in plate, and c)
Nutrition information into USDA website.
Platform: a) a smartphone interfaced with thermal camera and b)
reliable connection of the smartphone with a cloud server.
Assumptions:
a) Food temperature  Plate temperature;
b) Plate temperature > Background temperature;
c) The plate is not overflowing with food; and
d) A database of food items is prepared offline and available to MT-Diet.

2

Fig. 1. Actual requirements: Left are Nexus 5, Seek thermal sensor, and two
small caps. Right is a cooked frozen food.

Fig. 3. Flow Chart of MT-Diet by Socket.

Fig. 2. MT-Diet application steps for the demonstration.

Before launching the application, we need to prepare cooked
foods, a smartphone with built-in a camera, and a Seek
thermal sensor [16] interfaced with the smartphone such as
Fig. 1. Frozen foods are used since it is affordable and easy
to get. These foods are defrosted for 15 minutes using a
microwave. Then, we take the image of cooked food using
the smartphone camera and the thermal camera. The Seek
camera is connected to the Nexus 5 phone using a microUSB chord, due to which, the resulting images have different
angles and distances. Hence, we put two small caps filled with
cold water at two diagonally opposite ends of the plate for the
calibration purpose. After taking these two images by these
cameras, we are ready to launch the MT-Diet application. The
application needs both a color image and a thermal image of
the food as inputs. The application then provides the user with
(a) segmented food images (removed plate and background),
(b) food type in plate, and (c) each food’s nutrition information
into USDA (United States Department of Agriculture) website.
The demonstration consists of following steps: (a) The user
selects from color image and thermal image by clicking
two buttons: Color Image and Thermal Image such as
Step 3-1 and 3-2 in Fig. 2 . After clicking these buttons,
the application opens the gallery and the users can find the

images that they took. The application displays the images as
well as their absolute paths, the user can now visually verify
whether the selected images are correct. (b) The user clicks
the start button to send the images to the cloud server. The
cloud server connects the application using the socket communication. Fig. 3 illustrates the connection between the server
and the application. As seen in the figure, the application
sends the two images to the server. (c) The server processes
the food recognition included the food segmentation and
food identification. The application produces accurate food
segmentation without needing to to ask the user by combining
three algorithms: Dynamic Thermal Threshold (DTT), Hierarchical Image Segmentation (HIS) [17], and Grabcut [18]. The
approach and algorithms in the detail are provided in a paper
[19] Also, the execution time of the food identification is fast
since the server already has the trained parameters for SVM
classification. (d) The server sends the segmented images
and food types to the application. After receiving the food
types, the application can find the USDA database links of the
food type by matching with the USDA database.
The food types are displayed as a spinner button (Step 4-1 in
Fig. 2) and the segmented images clicking the Segmentation
button (Step 4-2 in Fig. 2). After looking at the segmented
images, the user can judge each segmented image quality,
which is critical for the identification accuracy. Moreover,
the user can access the USDA database about the food by
clicking the food type spinner button. Therefore, by taking
two images, the user can obtain the accurate food information
automatically. The application is envisioned to be inexpensive,
easy-to-use, and privacy preserving. In addition the application
can provide real-time feedback on calorie intake and can be
personalized based on eating habits of individuals.
A video of the demonstration is available in youtube [20].

III. S PECIAL REQUIREMENTS
For the demo, we need (a) Microwave with power outlet to
cook the frozen foods and (b) WiFi Network to connect to the
cloud server.

3

$392 Billion cost. In this project we evaluate the accuracy
and usability of MT-Diet, a cost effective smartphone based
automated diet monitoring application that uses the image of
a food plate in both the thermal and visual spectra to identify
food type. Such an easy-to-use and cost effective solution
to real time diet monitoring can potentially achieve higher
adherence to interventions which may in turn lead to beneficial
health impacts such as effective weight reduction.
R EFERENCES
Fig. 4. System Architecture of the extended MT-Diet food monitoring system
including the measuring food consumption.

IV. D ISCUSSION AND F UTURE W ORK
Since this demo is in an initial phase, there are some
insufficient implementations so in this section, we will discuss
the future phases. The first extension we propose is a technique
such that the two bottle caps that were used for calibration are
no longer needed. Using the bottle caps limits the usage of the
application so we suggest an extended implementation. In the
initial implementation, Nexus 5 needs the micro USB wire
because the direction of the thermal camera lens is opposite
to the direction of a built-in Nexus 5 camera. The micro USB
wire is an obstacle for calibration because it generates different
angles and distance between the thermal image and color
image. So we will change the smartphone to LG G2 which
does not need the micro USB wire to connect the thermal
camera. Then, we find the fixed different angle and distance
the thermal image and color image by executing only one
calibration task. Therefore, the small caps are not required for
the calibration process.
In addition, the the application requires high computation
time (almost 100 seconds for the segmentation) even when the
cloud server is employed. So, we consider parallel computation
as a potential solution of the issue. Moreover, if the user
is in a situation where there is no network connection, the
application cannot work, thus we pursuit an approach so that
the application will work in a non-network environment. To
deal with both issues, we consider changing the programming
language from Matlab to C++ because C++ not only supports
cross compiling with Android but also can be extended to
OpenCL for smartphone GPU implementation.
Food identification is a important task for diet monitoring,
but estimating food consumption is also a critical issue. Our
idea for the measuring the user’s food consumption is to
recognize the user hand movements using the wrist sensors
as seen in Step 2 in Fig. 4. To measure the food consumption
using the sensor, we need to consider three problems: a)
identifying utensils such as fork, spoon, chopstick, and knife
b) mapping food image location and actual food location and
c) identifying and counting user’s eating motion.
V. C ONCLUSION
Automated diet monitoring and caloric intake prediction is
an effective intervention for chronic diseases such as cardiac
problems, obesity and diabetes that affect more than onethird of US adults with a combined estimated economic

[1] C. Zhang, K. M. Rexrode, R. M. van Dam, T. Y. Li, and F. B. Hu,
“Abdominal obesity and the risk of all-cause, cardiovascular, and cancer
mortality sixteen years of follow-up in us women,” Circulation, vol. 117,
no. 13, pp. 1658–1667, 2008.
[2] M. T. Hamilton, D. G. Hamilton, and T. W. Zderic, “Role of low energy
expenditure and sitting in obesity, metabolic syndrome, type 2 diabetes,
and cardiovascular disease,” Diabetes, vol. 56, no. 11, pp. 2655–2667,
2007.
[3] H. Bays and C. Ballantyne, “Adiposopathy: why do adiposity and obesity
cause metabolic disease?” 2006.
[4] J. E. Hall, J. R. Henegar, T. M. Dwyer, J. Liu, A. A. da Silva, J. J. Kuo,
and L. Tallam, “Is obesity a major cause of chronic kidney disease?”
Advances in renal replacement therapy, vol. 11, no. 1, pp. 41–54, 2004.
[5] T. L. Burrows, R. J. Martin, and C. E. Collins, “A systematic review of
the validity of dietary assessment methods in children when compared
with the method of doubly labeled water,” Journal of the American
Dietetic Association, vol. 110, no. 10, pp. 1501–1510, 2010.
[6] C. M. Champagne, G. A. Bray, A. A. Kurtz, J. B. R. Monteiro, E. Tucker,
J. Volaufova, and J. P. Delany, “Energy intake and energy expenditure: a
controlled study comparing dietitians and non-dietitians,” Journal of the
American Dietetic Association, vol. 102, no. 10, pp. 1428–1432, 2002.
[7] J. R. Hebert, C. B. Ebbeling, C. E. Matthews, T. G. Hurley, M. Yunsheng,
S. Druker, and L. Clemow, “Systematic errors in middle-aged women’s
estimates of energy intake: comparing three self-report measures to total
energy expenditure from doubly labeled water,” Annals of epidemiology,
vol. 12, no. 8, pp. 577–586, 2002.
[8] M.-Y. Chen, Y.-H. Yang, C.-J. Ho, S.-H. Wang, S.-M. Liu, E. Chang,
C.-H. Yeh, and M. Ouhyoung, “Automatic chinese food identification
and quantity estimation,” in SIGGRAPH Asia. ACM, 2012, p. 29.
[9] K. Aizawa, K. Maeda, M. Ogawa, Y. Sato, M. Kasamatsu, K. Waki,
and H. Takimoto, “Comparative study of the routine daily usability
of foodlog a smartphone-based food recording tool assisted by image
retrieval,” Journal of diabetes science and technology, vol. 8, no. 2, pp.
203–208, 2014.
[10] S. Sharma and S. Fulton, “Diet-induced obesity promotes depressivelike behaviour that is associated with neural adaptations in brain reward
circuitry,” International journal of obesity, vol. 37, no. 3, pp. 382–389,
2013.
[11] [Online]. Available: https://www.myfitnesspal.com/.
[12] P. Pouladzadeh, P. Kuhad, S. V. B. Peddi, A. Yassine, and S. Shirmohammadi, “Mobile cloud based food calorie measurement,” in Multimedia
and Expo Workshops (ICMEW). IEEE, 2014, pp. 1–6.
[13] P. Pouladzadeh, S. Shirmohammadi, A. Bakirov, A. Bulut, and A. Yassine, “Cloud-based svm for food categorization,” Multimedia Tools and
Applications, pp. 1–18, 2014.
[14] P. Pouladzadeh, S. Shirmohammadi, and A. Yassine, “Using graph cut
segmentation for food calorie measurement,” in Medical Measurements
and Applications (MeMeA). IEEE, 2014, pp. 1–6.
[15] J. Milazzo, P. Bagade, A. Banerjee, and S. K. S. Gupta, “bhealthy:
A physiological feedback-based mobile wellness application suite,” in
Proceedings of the 4th Conference on Wireless Health, ser. WH ’13.
New York, NY, USA: ACM, 2013, pp. 14:1–14:2.
[16] Accessed: 2015-07-10. [Online]. Available: http://www.thermal.com/.
[17] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection
and hierarchical image segmentation,” Pattern Analysis and Machine
Intelligence, IEEE Transactions on, vol. 33, no. 5, pp. 898–916, 2011.
[18] C. Rother, V. Kolmogorov, and A. Blake, “Grabcut: Interactive foreground extraction using iterated graph cuts,” ACM Transactions on
Graphics (TOG), vol. 23, no. 3, pp. 309–314, 2004.
[19] J. Lee, A. Banerjee, and S. K. S. Gupta, “Mt-diet: Automated smartphone based diet assessment with infrared images,” in Pervasive Computing and Communications (PerCom). IEEE, 2016.
[20] [Online]. Available: https://www.youtube.com/watch?v=En8iyJ5JSI4.

Automated Evaluation of Non-Native English Pronunciation Quality:
Combining Knowledge- and Data-Driven Features at Multiple Time Scales
Matthew P. Black1,2,3 , Daniel Bone1 , Zisis I. Skordilis1 , Rahul Gupta1 , Wei Xia1 ,
Pavlos Papadopoulos1 , Sandeep Nallan Chakravarthula1 , Bo Xiao1 , Maarten Van Segbroeck1 ,
Jangwon Kim1 , Panayiotis G. Georgiou1 , and Shrikanth S. Narayanan1,2,3
1

Signal Analysis & Interpretation Laboratory, Univ. of Southern California, Los Angeles, CA, USA
2
Information Sciences Institute, Univ. of Southern California, Marina del Rey, CA, USA
3
Behavioral Informatix, LLC, Los Angeles, CA, USA
1

http://sail.usc.edu,

2

www.isi.edu,

Abstract
Automatically evaluating pronunciation quality of non-native
speech has seen tremendous success in both research and commercial settings, with applications in L2 learning. In this paper,
submitted for the INTERSPEECH 2015 Degree of Nativeness
Sub-Challenge, this problem is posed under a challenging crosscorpora setting using speech data drawn from multiple speakers
from a variety of language backgrounds (L1) reading different
English sentences. Since the perception of non-nativeness is realized at the segmental and suprasegmental linguistic levels, we
explore a number of acoustic cues at multiple time scales. We
experiment with both data-driven and knowledge-inspired features that capture degree of nativeness from pauses in speech,
speaking rate, rhythm/stress, and goodness of phone pronunciation. One promising finding is that highly accurate automated
assessment can be attained using a small diverse set of intuitive
and interpretable features. Performance is further boosted by
smoothing scores across utterances from the same speaker; our
best system significantly outperforms the challenge baseline.
Index Terms: Behavioral Signal Processing (BSP), computational paralinguistics, Goodness of Pronunciation (GOP),
speech assessment, non-native speech, prosody, challenge

1. Introduction
Speech production is an intricate process involving multiple levels of planning and motor coordination in order to interweave
segmental articulations and encode suprasegmental linguistic
and paralinguistic information. Moreover, individuals differ
in all facets of the speech production pipeline due to a variety of reasons (e.g., physical, environmental), leading to several
sources of variability, including language background.
With the advancement of speech technologies, engineers
have focused on creating assistive tools for language learning:
from automatic literacy tutors [1], to the focus of this challenge, speech nativeness [2, 3]. Computer-Assisted Pronunciation Training (CAPT) is an invaluable resource for secondlanguage (L2) learners that offers flexibility in scheduling at
reduced costs. Languages differ in their phonemic, prosodic,
and grammatical structures; quite often, L2 learners will retain
certain speech attributes of their native language, leading to perceived abnormality, or “non-nativeness,” by L1 listeners. Our
approach to this challenge is grounded in the automatic creation
of informative pronunciation and prosodic features.
Prosody is at the core of effective human-human communication. It serves grammatical functions, such as segmenting
utterances into phrases, pragmatic functions like differentiating
statements from questions, and communicates attitude and af-

3

www.behavioralinformatix.com

fect. Since languages themselves have different rhythms and
intonations, non-native speakers often use those prosodic characteristics of their first language, particularly when learning a
stress-timed L2 given a syllable-timed L1, or vice-versa. Several recent studies have investigated L2 pronunciation through
prosodic features. Levow developed a pitch accent recognition
algorithm for labeling non-native speech which uses local and
co-articulatory context [4]. Hansen and Arslan also proposed
using source-generator based prosodic features to classify foreign accents of American English [5]; they showed that energy,
duration, and spectral related features could play an important
role in accent detection. Much research has been done on automatically separating native vs. non-native speakers [3, 6–8].
Phonemic identity and pronunciation quality are also important cues for automatically scoring degree of nativeness.
Phoneme confusion between languages makes both comprehension and production difficult for L2 learners. For instance,
German speakers of English cannot pronounce /z/ as clearly
since there is no /z/ sound in the German language, while for
Japanese, the lack of /r/ usually causes speakers to pronounce /r/
as /l/. Witt and Young presented a Goodness of Pronunciation
(GOP) measure to quantify phonetic pronunciation quality [9].
They also improved their model by including the expected pronunciation errors in the recognition network, as did Black et al.
in the context of children’s literacy assessment [10].
Our approach in this work is centered on combining
knowledge-based and data-driven feature sets extracted at multiple time scales (segmental, suprasegmental). Much of these
knowledge-inspired features are prosodic (suprasegmental) and
pronunciation/articulatory (segmental) cues that were motivated
and discussed in this section. Data-driven features have the advantage of fewer dependencies, with the idea being to discover
trends through supervised learning methods; the downside is
that they are naı̈ve and suffer from high dimensionality. Conversely, knowledge-inspired features rely on other information
that may be unreliable or noisy, but they are more intuitive and
interpretable and have a lower dimensionality. Finally, we also
consider unsupervised speaker clustering and smoothing methods, under the assumption that speakers will be perceived with
consistent levels of nativeness across utterances.

2. Corpora & Baseline System
Four different corpora were analyzed: two make up the train
set, and the other two are the development (“dev”) and test sets.
Each corpus is comprised of multiple non-native speakers of
English from a variety of language backgrounds (German, Italian, Chinese, Japanese, French, Spanish, Hindi, other), although

Train (N=3890, m=3.2, s=2.1)
Dev (N=999, m=5.7, s=3.5)
Test (N=594, m=6.8, s=2.9)

1

2

3

4

5

6

7

Time (s)

8

9

10

11

12

Figure 1: Interleaved train/dev/test set utterance durations.
language ID was not provided. Each speaker read multiple sentences (disjoint across corpora). Fig. 1 shows that the distribution of utterance durations varies across corpora. The degree
of nativeness (DN ) was labeled for each utterance, with higher
scores representing higher degrees of non-nativeness. As shown
in Fig. 2, the train and dev sets were rated on different scales;
this was the primary reason for Spearman’s correlation as the
performance metric. DN scores and speaker IDs for the test set
were not provided. Word-level transcriptions were available for
each utterance in all data sets and included enriched markers for
expected positions of major (B3) and minor (B2) phrase boundaries. The challenge organizers have requested that readers refer
to the challenge paper for more details [11].
The DN Challenge baseline system consists of a purely
data-driven approach: training a linear support vector regression
(SVR) model on 6373 utterance-level static features by computing functionals of low-level descriptors (LLDs) that include
prosodic (e.g., f0 , energy), voice quality (e.g., jitter, shimmer),
and spectral (e.g., MFCCs, RASTA coefficients) cues. Please
see [11, 12] for more details. We will compare the performance
of this baseline system to our proposed methodology in Sec. 6.

3. Data Pre-processing
3.1. Forced Alignment & Voice Activity Detection
We exploited the available transcriptions by performing speechtext (“forced”) alignment using freely available HTK [13]
acoustic models (AMs) trained on out-of-domain native English
speech [14]; we will leverage this “mismatch” in native vs. nonnative speakers in Sec. 4.2. We used the CMU dictionary [15],
with its multiple acceptable phonetic pronunciations for each
word, and appended entries for any out-of-vocabulary words.
From the output of the forced alignment, we extracted word,
syllable, and phone boundaries (and the corresponding acoustic log-likelihoods). By using a grammar that allowed for the
optional detection of inter-word pauses, this alignment process
also acted as an accurate voice activity detector (VAD).
3.2. Speaker Clustering
As illustrated in Fig. 2, DN scores are correlated across utterances from the same speaker. Therefore, the identity of the
speaker for each utterance can be utilized to smooth and improve DN score predictions. However, as described in Sec. 2,
the speaker ID is unknown for the test utterances, so an unsupervised speaker clustering approach is needed. We used a bottomup agglomerative hierarchical clustering (AHC) method [16]
with k-means post-refinement [17]. Each cluster is modeled
by a single Gaussian, and the generalized likelihood ratio [18]
is used as the cluster distance metric [17]. Using the VAD
(Sec. 3.1), we removed all periods of silence from the utterances before clustering. Since the number of speakers in the
test set was known a priori (54, with 11 sentences per speaker),
we stopped clustering once 54 distinct clusters were created.

Degree of Nativeness

Normalized Histogram

0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0

3.5
3
2.5
2
1.5
1
0.5
0
0

Train: mean +/− S.D.

10

20

Dev: mean +/− S.D.

30

40

50

Speaker (Ordered by Mean)

60

Figure 2: Nativeness scores for speakers in the train/dev sets.

4. Feature Extraction
4.1. Data-Driven Features
Our proposed data-driven features begin with the baseline LLDs
extracted every 10ms with openSMILE [19]. Non-speech
frames according to VAD (Sec. 3.1) are removed. In addition to computing utterance-level functionals as in the baseline
(Sec. 2), we also computed “functionals-of-functionals,” as proposed in [20] and used successfully in [21, 22]. LLD contours
are first split into short disjoint windows of equal length L.
Functionals are then computed within each window, and finally,
functionals of these functionals are computed across all windows in the utterance. This process is meant to better capture
moment-to-moment changes that occur at shorter time scales.
In this work, we chose two different window lengths, L ∈{0.1s,
0.5s}, based on the fact that more than 95% of the utterances
in the corpora are more than one second in duration (Fig. 1),
which means the vast majority of utterances have a sufficient
number of windows to compute reliable statistics. Because of
the compounding nature of these calculations, there are 11323
data-driven functionals-of-functionals features in total.
4.2. Knowledge-Inspired Features
Prosody is the rhythm, stress, and intonation of speech. As
such, we computed features that targeted these qualitative constructs. Immediately from the forced alignment (Sec. 3.1), we
extracted important prosodic cues that captured pausing behaviors/strategies and speaking rate. Since the transcriptions had
markers indicating phrase boundaries (Sec. 2), we first categorized each inter-word pause as one of the following: expected
major (B3), expected minor (B2), or unexpected. Utterancelevel pausing features were calculated for each combination of
the 3 pausing categories in two ways: 1) percentage of the utterance duration due to pauses, and 2) mean duration of pauses.
We extracted two types of speaking rate features: rates
(token / time) and durations (time / token). The former was calculated by creating a vector of rates (1/duration) for each token
in the utterance and computing functionals (e.g., mean, S.D.)
across it. We chose 3 different tokens at varying time scales:
word, syllable, and phone. Similarly, the duration features were
calculated by computing functionals of durations for different
linguistic classes: syllables, phones, vowels, and consonants.
The next set of knowledge-driven features are related to lexical stress and speech rhythm. We compared the stress of each
aligned phone to the stress labels in the CMU dictionary [15]
that are provided for each vowel to signal data. For each aligned
phone, we compute the mean f0 , energy, and intensity and then
quantize these values into three quantiles for each utterance.
Since the stress labels are also 3-valued, we define a distance
measure based on exact-matching. For each signal (3), we obtain a measure that is the summation of mis-matches between
the labeled and signal-derived stress.
We also quantify speech rhythm using pairwise variability
indices (PVI, [23]) and global interval proportions (GIP, [24]).
PVIs measure local changes in duration. We compute six PVI

Type

Nsig /N

Pausing
7/8
Rate
18/47
Rhythm 5/14
Template 6/13
GOP
3/6

Best Feature: Spearman’s Correlation
Description of Best Feature

Train

Fraction unexpected pauses
Mean rate: phones/sec
Consonant PVI
Phone duration mean |diff|
Utterance: phone+sil loop

0.39 0.59
-0.43 -0.56
0.25 0.37
0.37 0.49
-0.48 -0.55

Dev

Table 1: Number of significantly correlated knowledge-inspired
features (p < 0.05), along with the best performing feature.
measures: normalized and unnormalized measures on consonant, vowel, and syllable durations. GIPs compute gross statistics on segmental durations; in particular, we included the percentage of vowel speech and S.D. of vowel and consonant durations within an utterance. Similar speech rhythm measures were
previously considered for intoxicated speech detection [25].
Next, we implicitly model the stress and intonation of
speech through template-based exemplar features, initially proposed for modeling children’s prosody vs. an adult exemplar [26] and also successfully used in [27]. First, a single
prosodic functional for each token (phone or word) is calculated per feature: duration, median f0 , and median intensity.
This is a time-aligned feature representation which we can compare with other readings. Template-based features are advantageous in that all computations are performed on the continuousscale signal contours. However, since we do not have an exemplar production for each sentence, we must infer one from
the other speakers. We take the mean feature contour from all
other productions of the same sentence, assuming that this average will provide a suitable exemplar. In reference to this exemplar, we then computed 3 measures (Pearson’s correlation,
mean absolute difference, S.D.) for each of the following 4 contours: phone duration/pitch/intensity and word duration. Only
sentences repeated at least 5 times were considered; otherwise,
imputation with the mean value was used.
The last proposed knowledge-inspired feature is Goodness
of Pronunciation (GOP) scoring, first introduced in [9] to detect
phone-level pronunciation errors but also applied to L2 learning [28] and children’s literacy assessment [10, 29]. The main
idea behind GOP scoring is leveraging acoustic models (AMs)
trained on native speakers to quantify pronunciation quality:


P (O|Transcript)
1
GOP =
log
(1)
N
P (O|AM loop)
, where N is the number of frames and O are the acoustic features (MFCCs in this case). The numerator is the likelihood of
the acoustics, given the transcription and the native AMs, which
is equivalent to forced alignment (Sec. 3.1). The denominator is
estimated via automatic speech recognition with the same set of
AMs and an “AM loop” grammar. Higher GOP scores indicate
higher pronunciation quality, whereas lower GOP scores suggest the speech is a poor match to the native AMs. In this work,
we experimented with 3 different AM loops (phones+silence,
phones only, silence only) and computed GOP scores over different temporal regions: full utterance (including inter-word
pauses), speech regions only, vowels only (based on [30]).
Table 1 shows that almost half of the knowledge-based features are significantly correlated with the DN scores in the train
and dev sets (p < 0.05). We restrict our analysis to the best
performing feature for each proposed type due to space constraints. As shown in Table 1, speakers sound more non-native
when they: pause more unexpectedly; speak at a slower average
rate; have higher local variability in consonant durations; differ

Figure 3: Automatically evaluating degree of nativeness using
support vector regression (SVR), trained on data-driven and
knowledge-inspired features, with speaker-level smoothing.
more than other productions of the same sentence; and have
lower GOP scores. All of these findings agree with intuition.

5. Predicting Degree of Nativeness
We experimented with several supervised machine learning
techniques to map the various features to the DN scores, including regression and ranking algorithms [31], ensemble methods
(e.g., bagging, boosting [32], stacking [33]), and early vs. late
fusion. Due to space constraints, we only describe our best performing system, shown in Fig. 3. For parameter-tuning purposes and to prevent overfitting, the train set was split into 6 approximately equal-sized speaker-disjoint cross-validation folds.
5.1. Support Vector Regression & Fusion
For the data-driven functionals-of-functionals (“FoF”), since dimensionality is so high, we first employed unsupervised feature
reduction. A feature was dropped unless the following two criteria were both met for all cross-validation folds and datasets:
1) the feature must have a coefficient of variation ≥ 0.01 (to
eliminate irrelevant features), and 2) the feature must not have a
Pearson’s or Spearman’s correlation ≥ 0.98 with any other feature (to eliminate redundant features). When two features were
highly correlated, we dropped the one that was less normally
distributed, based on the Kolmogorov-Smirnov test. This process reduced the dimensionality by 34% (from 11323 to 7478).
The feature matrix was then normalized/scaled four different ways: Z-normalization, i.e., subtracting the mean and dividing by the S.D., 1) across all data; 2) for each fold and dataset
separately; and scaling the feature matrix to be bounded between 0 and 1, i.e., subtracting the minimum and dividing by
the range, 3) across all data; 4) for each fold and dataset separately. Finally, we trained l1 -loss and l2 -loss l2 -regularized
linear support vector regression (SVR) models, as implemented
in LIBLINEAR [34], with the loss functions defined as:
!
 l
 
X


T
T
min w w+C
max 0,DNu −w xu − , l ∈ {1,2} (2)
w

u

, where xu is the feature vector for utterance u, w is the linear
weight vector, l determines the type of loss function (l1 or l2 ), 
is the loss sensitivity parameter, and C is the regularization parameter. A grid search was used to tune  and C. The “optimal”
parameter values, normalization technique, and SVR loss type,
i.e., the combination that attained the highest mean Spearman’s
correlation across the 6 train folds, was then applied to the dev
set after retraining the SVR model on the full train set.
Feature-level (“early”) fusion of the proposed knowledgeinspired features was possible because of their low dimensionalities. Fusion between the data-driven and knowledge-inspired
features was attained by treating the output prediction scores
of the data-driven FoF SVR model as one additional “feature”
(Fig. 3). After removing irrelevant/redundant features and normalizing/scaling the feature matrix as before, we trained similar
l1 -loss and l2 -loss linear SVR models on these fused features.

Category

Feature

Train

N

Dev

Baseline [11] Utterance

6373

mean S.D.
0.40
—

Time Scale
Time Scale
Time Scale

530
3100
3848

0.37
0.48
0.47

0.12
0.08
0.06

0.29
0.44
0.38

All Proposed All Proposed 7478

0.48

0.06

0.45

Utterance
FoF (0.1s)
FoF (0.5s)

Feature/Classifier

0.42

Baseline [11]

6373

Func-of-Func (FoF)

7478

Table 2: Dimensionality and performance (Spearman’s correlation) of the support vector regression model for various subsets
of data-driven features on the train set (6 folds) and dev set.
5.2. Speaker-level Smoothing
As motivated in Sec. 3.2 and shown in Fig. 2, DN scores
are correlated across a speaker’s utterances, suggesting that directly modeling this dependency may improve prediction performance. We experimented with the following smoothing techdus is the predicted DN score of utterance
nique in (3), where DN
u from speaker s, and wus is the number of syllables in u:
X s
gus = (1−α)DN
dus + Pα
dus , 0 ≤ α ≤ 1 (3)
DN
wu DN
wus u∈s
u∈s

s

gu is a smoothed linear combination of the utTherefore, DN
dus with the speaker’s weighted avterance’s predicted score DN
erage score (with longer utterances containing more syllables
carrying more weight when computing this average). The decision to use this weighted average was made since there is more
acoustic evidence in longer utterances, giving human raters and
computational methods alike more of a chance to make a reliable decision. In (3), tuning parameter α determines the level of
smoothing; α = 0 means no smoothing, i.e., the utterance’s predicted score is applied, while α = 1 means the speaker’s average
score is applied to all of the speaker’s utterances.

6. Results & Discussion
In this section, we analyze the performance of our various datadriven and knowledge-inspired sub-systems. First, we examine
performance of SVR using data-driven functional-of-functional
(FoF) features on the train and development sets (Table 2). Our
reduced set of utterance-level functionals (i.e., 530 vs. 6373)
show a drop in performance relative to the baseline. However,
through modeling these features at various time scales, FoF features exceed baseline performance on the train and dev sets; for
both, FoF at the 0.1s time-scale is the top performing individual
feature set. We note that as the number of features increases,
the S.D. of model performance across training folds decreases,
an indication of a robust linear-kernel SVR model. Fusion of all
proposed FoF features exceeds baseline performance, showing
the benefits of the FoF modeling framework.
Knowledge-inspired feature performance is shown in Table 3, including score-level fusion with data-driven FoF fea-

Spearman Correlation

Tr

0.75
0.73
0.71
0.69
0.67
0.65
0.63
0.61
0.59
0.57

0

Tr: Smooth

0.1

0.2

Dev

0.3

α

Dev: Smooth

0.4

0.5

sel

0.6

α (Smoothing Parameter)

0.7

N

Test: Smooth

0.8

0.9

Figure 4: Improving utterance-level score prediction by smoothing with the speaker’s average score across all utterances.

Train
mean
0.403

S.D.
—

Dev

Test

0.415 0.425

0.483 0.065

0.454

—

Pausing
Speaking Rate
Rhythm
Template
GOP

8
47
14
13
6

0.405
0.552
0.262
0.370
0.430

0.107
0.047
0.048
0.104
0.066

0.613
0.627
0.411
0.455
0.444

—
—
—
—
—

LOO: Func-of-Func

88

0.585 0.055

0.690

—

0.700
0.701
0.703
0.688
0.699

—
—
—
—
—

LOO: Pausing
LOO: Speaking Rate
LOO: Rhythm
LOO: Template
LOO: GOP

80+1
41+1
74+1
75+1
82+1

0.603
0.584
0.603
0.604
0.596

All Proposed

88+1

0.605 0.050

0.707 0.710

Spkr Smooth; α=0.44 88+1

0.653 0.061

0.744 0.745

0.051
0.047
0.048
0.052
0.053

Table 3: Dimensionality and performance (Spearman’s correlation) for each of the proposed features on the train set (6 folds)
and dev/test sets. “+1” represents score-level fusion with the
Func-of-Func (FoF) classifier. “LOO” means Leave-One-Out.
tures. Speaking rate features perform better than any other
feature group on both train and dev, followed closely by pausing features; this reflects the critical importance of timing to
perceived nativeness. GOP features alone (6 features) exceed
baseline performance, indicating that segmental cues can identify non-native speech. Rhythm and template features also have
significant performance, albeit below the other feature groups.
Fusion of all knowledge-inspired features leads to performances
well above the baseline (listed as LOO: Func-of-Func). Leaveone-out experiments do not suggest that any feature group
greatly degrades performance; on the contrary, fusion of all proposed features achieves peak performance of 0.605, 0.707, and
0.710 on the train, dev, and test sets, respectively.
Lastly, we perform speaker-level smoothing of our best
SVR model, utilizing the assumption that non-native speakers
are consistently non-native. Optimization is shown in Fig. 4,
where optimal performance is found for α = 0.44. Smoothing
provides an improvement to 0.744 on the dev set; test set performance with this system is consistent at 0.745, significantly
outperforming the baseline correlation of 0.425 (p < 0.0001).

7. Conclusions & Future Work
In this paper, we showed that a combination of low-dimensional
knowledge-inspired segmental and suprasegmental features
(pausing, speaking rate, stress/rhythm, and Goodness of Pronunciation) were able to predict subjective degree of nativeness
ratings. In combination with data-driven features extracted at
multiple time scales and speaker-level smoothing, we achieved
accurate evaluation of pronunciation quality.
Future work will include experimentation with additional
fusion techniques, as well as novel ways to incorporate a
speaker’s L1 (e.g., exploiting specific non-native trends that are
more prone to occur). Given that English is a stress-timed language, it may be beneficial to utilize metrics that measure variability between stressed syllables, rather than all adjacent syllables as we have currently done (i.e., isochrony features [35]).

8. Acknowledgements
This research was supported in part by NSF and NIH. Special
thanks to the INTERSPEECH 2015 Challenge organizers.

9. References
[1] A. Hagen, B. Pellom, and R. Cole, “Children’s speech recognition
with application to interactive books and tutors,” in IEEE Workshop on Automatic Speech Recognition and Understanding, 2003,
2003, pp. 186–191.
[2] F. Hönig, A. Batliner, and E. Nöth, “Automatic assessment of nonnative prosody – annotation, modelling and evaluation,” in International Symposium on Automatic Detection of Errors in Pronunciation Training (IS ADEPT), 2012, pp. 21–30.
[3] F. Hönig, A. Batliner, K. Weilhammer, and E. Nöth, “Automatic assessment of non-native prosody for English as L2,” Proc.
Speech Prosody, Chicago, 2010.
[4] G.-A. Levow, “Investigating pitch accent recognition in nonnative speech,” in Proceedings of the ACL-IJCNLP Conference
Short Papers, 2009, pp. 269–272.
[5] J. H. L. Hansen and L. M. Arslan, “Foreign accent classification
using source generator based prosodic features,” in IEEE ICASSP,
1995, pp. 836–839.
[6] M. Piat, D. Fohr, and I. Illina, “Foreign accent identification based
on prosodic parameters,” in INTERSPEECH, 2008.
[7] F. Hnig, A. Batliner, K. Weilhammer, and E. Nth, “Islands of failure: Employing word accent information for pronunciation quality assessment of English L2 learners,” in Proceedings of SLATE,
Wroxall Abbey, 2009.
[8] J. Lopes, I. Trancoso, and A. Abad, “A nativeness classifier for
TED talks,” in IEEE ICASSP, 2011, pp. 5672–5675.

[20] B. Schuller, M. Wimmer, L. Mösenlechner, C. Kern, D. Arsic, and
G. Rigoll, “Brute-forcing hierarchical functionals for paralinguistics: A waste of feature space?” in IEEE ICASSP, Las Vegas, NV,
USA, 2008, pp. 4501–4504.
[21] M. P. Black, A. Katsamanis, B. Baucom, C.-C. Lee, A. Lammert,
A. Christensen, P. G. Georgiou, and S. S. Narayanan, “Toward automating a human behavioral coding system for married couples’
interactions using speech acoustic features ” Speech Communication, vol. 55, no. 1, pp. 1–21, 2013.
[22] D. Bone, M. Li, M. P. Black, and S. S. Narayanan, “Intoxicated speech detection: A fusion framework with speakernormalized hierarchical functionals and GMM supervectors ”
Computer Speech & Language, vol. 28, no. 2, pp. 375–391, 2014.
[23] E. Grabe and E. L. Low, “Durational variability in speech and the
rhythm class hypothesis,” Papers in laboratory phonology, vol. 7,
pp. 515–546, 2002.
[24] F. Ramus, “Acoustic correlates of linguistic rhythm: Perspectives,” Proceedings of Speech Prosody, 2002.
[25] F. Hönig, A. Batliner, and E. Nöth, “Does it groove or does it
stumble-automatic classification of alcoholic intoxication using
prosodic features.” in INTERSPEECH, 2011, pp. 3225–3228.
[26] M. Duong, J. Mostow, and S. Sitaram, “Two methods for assessing oral reading prosody,” ACM Transactions on Speech and Language Processing (TSLP), vol. 7, no. 4, pp. 14:1–14:22, 2011.

[9] S. M. Witt and S. J. Young, “Phone-level pronunciation scoring
and assessment for interactive language learning,” Speech Communication, vol. 30, no. 2, pp. 95–108, 2000.

[27] D. Bone, T. Chaspari, K. Audhkhasi, J. Gibson, A. Tsiartas,
M. Van Segbroeck, M. Li, S. Lee, and S. S. Narayanan, “Classifying language-related developmental disorders from speech cues:
the promise and the potential confounds.” in INTERSPEECH,
2013, pp. 182–186.

[10] M. P. Black, J. Tepperman, and S. S. Narayanan, “Automatic prediction of children’s reading ability for high-level literacy assessment,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 1015–1028, 2011.

[28] A. Neri, C. Cucchiarini, and H. Strik, “The effectiveness of
computer-based speech corrective feedback for improving segmental quality in L2 Dutch,” ReCALL, vol. 20, no. 2, pp. 225–243,
May 2008.

[11] B. Schuller, S. Steidl, A. Batliner, S. Hantke, F. Hönig, J. R.
Orozco-Arroyave, E. Nöth, Y. Zhang, and F. Weninger, “The
INTERSPEECH 2015 Computational Paralinguistics Challenge:
Nativeness, Parkinson’s & Eating Condition,” in INTERSPEECH,
2015.

[29] J. Tepperman, M. P. Black, P. Price, S. Lee, A. Kazemzadeh,
M. Gerosa, M. Heritage, A. Alwan, and S. S. Narayanan, “A
bayesian network classifier for word-level reading assessment,” in
INTERSPEECH, Antwerp, Belgium, Aug. 2007, pp. 2185–2188.

[12] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer,
F. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi,
H. Salamin, A. Polychroniou, F. Valente, and S. Kim, “The INTERSPEECH 2013 Computational Paralinguistics Challenge: social signals, conflict, emotion, autism,” in INTERSPEECH, 2013.

[30] L. Chen, K. Evanini, and X. Sun, “Assessment of non-native
speech using vowel space characteristics,” in Spoken Language
Technology Workshop, 2010, pp. 139–144.
[31] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer, “An efficient
boosting algorithm for combining preferences,” The Journal of
machine learning research, vol. 4, pp. 933–969, 2003.

[13] S. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Kershaw,
X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and
P. Woodland, “The HTK book (for HTK version 3.4),” Cambridge
University Engineering Department, Tech. Rep., Dec. 2006.

[32] Y. Freund, R. Schapire, and N. Abe, “A short introduction to
boosting,” Journal of Japanese Society for Artificial Intelligence,
vol. 14, no. 5, pp. 771–780, 1999.

[14] K. Vertanen, “Baseline WSJ acoustic models for HTK
and Sphinx: Training recipes and recognition experiments,”
Cavendish Laboratory, Tech. Rep., 2006.

[33] S. Džeroski and B. Ženko, “Is combining classifiers with stacking
better than selecting the best one?” Machine learning, vol. 54,
no. 3, pp. 255–273, 2004.

[15] R. L. Weide, “CMU pronouncing dictionary,” Carnegie Mellon
University, 1994. [Online]. Available: http://www.speech.cs.cmu.
edu/cgi-bin/cmudict/

[34] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin, “LIBLINEAR:
A library for large linear classification,” The Journal of Machine
Learning Research, vol. 9, pp. 1871–1874, 2008.

[16] W. Wang, P. Lv, and Y. H. Yan, “An improved hierarchical speaker
clustering,” Acta Acustica, vol. 33, no. 1, 2008.

[35] I. Lehiste, “Isochrony reconsidered.” Journal of Phonetics, vol. 5,
no. 3, pp. 253–263, 1977.

[17] K. J. Han, S. Kim, and S. S. Narayanan, “Strategies to improve
the robustness of agglomerative hierarchical clustering under data
source variation for speaker diarization,” IEEE Transactions on
Audio, Speech, and Language Processing, vol. 16, no. 8, pp.
1590–1601, Nov 2008.
[18] A. S. Willsky and H. L. Jones, “A generalized likelihood ratio
approach to the detection and estimation of jumps in linear systems,” IEEE Transactions on Automatic Control, vol. 21, no. 1,
pp. 108–112, 1976.
[19] F. Eyben, M. Wöllmer, and B. Schuller, “OpenSMILE - The Munich versatile and fast open-source audio feature extractor,” in
ACM Multimedia, Firenze, Italy, 2010, pp. 1459–1462.

MT-Diet : Automated Smartphone based
Diet Assessment with Infrared Images
Junghyo Lee (Speaker), Ayan Banerjee, and Sandeep K. S. Gupta
iMPACT Lab, CIDSE, Arizona Sate University, Tempe, AZ
{jlee375, abanerj3, Sandeep.Gupta}@asu.edu

Diet
• American Medical Association in 2013
• Recognized obesity as a disease officially [1]

• What is the effective method to treat and prevent the obesity?
• Existing Diet Monitoring and Problems
• Self-monitoring
• Low adherence, underreporting, and recall error

• Smartphone camera based monitoring (State-of-the-art )
• User inconvenience
• Segment multiple foods to each single food
• Select a food location in the image (Rectangle)

• Fairly low food type identification accuracy (63%)
2

MT-Diet
• MT-Diet: Automated Smartphone based Diet Assessment System using Cyber-Physical
Dynamics
• Thermal Interaction of a food plate with the environment
𝑇𝑓 − 𝑇𝐴
𝑓

𝑇𝑖 − 𝑇𝐴
𝒇

= exp

−ℎ𝐴𝑡
𝑓

𝑚𝑓 𝐶𝑝

,

𝑇𝑝 − 𝑇𝐴
𝑝

𝑇𝑖 − 𝑇𝐴

= exp

−ℎ𝐴𝑡
𝑝

𝑚𝑝 𝐶𝑝

𝒑

If 𝑻𝒊 ≫ 𝑻𝒊 , then 𝑻𝒇 ≫ 𝑻𝒑 , for at least 𝒕 > 𝟏𝟎 𝒎𝒊𝒏𝒔
𝑓

𝑇𝑓 - Temperature of food item at time 𝑡, 𝑇𝑖 - initial food temperature, 𝑚𝑓 - mass of food item.
𝑝
𝑇𝑝 - Temperature of food item at time 𝑡, 𝑇𝑖 - initial food temperature, 𝑚𝑝 - mass of plate.
𝑓
𝑝
𝑇𝐴 - Ambient temperature, ℎ - heat transfer coefficient, 𝐴- area of food item, 𝐶𝑝 - specific heat of food, 𝐶𝑝 - specific heat of plate.

• Contributions: Fuse Thermal and Visual Spectra
• Thermal and Color image correct each other
• Results
• Segment multiple foods to each single food automatically
• High accuracy of identifying types of food (88.93%)
• State-of-art systems have accuracy at most 63%

3

Overview

4

System Model

5

Usability
• Balanced diet recommendation
• Assume the plate has uniform depth
• Ratio of the surface area multiplied by the density of the food item

6

Diet Monitoring
• Food Segmentation
Food Recognition
• Food Identification
• Intake Amount
• Calorie Estimation
• Dietary Feedback
• Behavior Control

7

MT-Diet

8

Image Acquisition
• Prototype of MT-Diet
• Nexus 5
• Seek thermal sensor
• Micro USB wire

• Thermal Image
• Seek thermal sensor interfaced
with the Nexus 5

• Color Image
• From Built-in Nexus 5 Camera

9

Food Segmentation

10

Challenges for Image Based
Food Segmentation (1)
• Visual spectra may not have defined boundaries
• Thermal boundaries more significant
Better Contrast
in Infrared
Spectra

Food Item Location
DTT Output

4

8 – connected
component labeling [2]

• Thermal signatures are not stable
1
Dynamic Thermal Thresholding (DTT)

3

2
11

Challenges for Image Based
Food Segmentation (2)
• Color (Gradient) and Texture are important features for identification
• Food items of same color as plate may be lost in visual spectra
• Fusion of visual and infrared spectra
Contour Detection
Image Segmentation

Hierarchical Image Segmentation (HIS) [3]
12

Challenges for Image Based
Food Segmentation (3)
• Food items not heated enough get deleted in DTT output
• Color image can recognize the food

Which portions
are food?
Food item
missing in DTT
Exact food portions and
locations along with
labels for connected
components
Food item
missing in HIS
13

Label Matching
• Identifies unique food items in both infrared and visual spectra
2

1

+
3

DTT output with
food labels

4

Contour and Image segments from
visual spectra

• Remove plate portions that are connected to food items with Grabcut
[4]
14

Food Identification

15

Feature Set
• Objective
• Classify the food type using SVM with kernels

• Feature Extraction
• Color feature
• RGB histogram: 32 histogram bins of each color channel

• Histogram of Oriented Gradient feature [4]
• 16 windows and 36 bins of oriented gradients of the each windows

• Texture feature [5]
• Resized food image (400 X 400), 5 scales, 8 orientations, 4 by 4 down sampling

16

Feature Fusion Methods
• Feature Fusion
• Three fusion methods
• Method of Dimensionality Reduction
• Kernel Principal Component Analysis
• Principal Component Analysis

17

Experimental Results

18

Segmentation Result

20 Food Types
in practice

19

Execution Time of Food Segmentation
• Execution time (s) of food segmentation using i7 processor
Min

Median

Max

Sum

Avg

STD

HIS

89.08

92.89

97.50

7403.9

92.55

1.66

DTT

2.83

3.13

3.9

256.12

3.2

0.24

ROF

0.39

0.78

1.01

62.77

0.78

0.09

Grabcut

2.4

14.24

41.15

1138.97

13.48

7.63

Total

95.51

111.59

145

8911.79

111.46

7.71
20

Food Identification Result
Feature

Texture
HOG
RGB
HOG & Texture
HOG & RGB
RGB & Texture
All

Best Methods
(Reduction, Kernel, Fusion)

KPCA, RBF
PCA, RBF
KPCA, Sigmoid
KPCA, RBF, 3
KPCA, RBF, 3
KPCA, RBF, 3
KPCA, RBF, 3

Accuracy(%) Execution(s)

45.08
63.11
88.11
59.43
88.93
88.93
87.7

5.13
0.35
0.57
5.13
0.7
5.62
5.43

Feature
Size
100
100
100
200
200
200
300
21

Conclusion
• MT-Diet : Automatic diet monitoring system that interfaces thermal
• High accuracy of automated food identification (88.93%)
• User-friendly diet monitoring system application
• Only one task (Click a button)
• Expected the promoted healthy eating habits

Even If your diet log is the empty…..

22

References
[1] [Online]. Available: http://www.ama-assn.org/ama/pub/news/news/2013/2013-06-18-new-amapolicies-annual-meeting.page
[2] H. Samet and M. Tamminen (1988). "Efficient Component Labeling of Images of Arbitrary
Dimension Represented by Linear Bintrees". IEEE Transactions on Pattern Analysis and Machine
Intelligence (TIEEE Trans. Pattern Anal. Mach. Intell.) 10 (4): 579. doi:10.1109/34.3918.
[3] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and hierarchical image
segmentation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 5, pp.
898–916, 2011.
[4] Rother, C., Kolmogorov, V., & Blake, A. (2004, August). Grabcut: Interactive foreground
extraction using iterated graph cuts. In ACM transactions on graphics (TOG) (Vol. 23, No. 3, pp.
309-314). ACM.
[5] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,” in Computer
Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 1.
IEEE, 2005, pp. 886–893.
[6] M. Haghighat, S. Zonouz, and M. Abdel-Mottaleb, “Identification using encrypted biometrics,”
in Computer Analysis of Images and Patterns. Springer, 2013, pp. 440–448.
23

Acknowledgment
Dr. Petteri Nurmi for shepherding out paper
Dr. Meg Bruening for her insights on diet monitoring
NSF IIS 1116385
NIBIB EB019202
GPSA for supporting the conference travel grant

24

25

SCEPTRE: a Pervasive, Non-Invasive, and Programmable
Gesture Recognition Technology
Prajwal Paudyal, Ayan Banerjee, and Sandeep K.S. Gupta
IMPACT Lab, http://impact.asu.edu
Arizona State University, Tempe, Arizona
{Prajwal.Paudyal, abanerj3, sandeep.gupta}@asu.edu
ABSTRACT

Communication and collaboration between deaf people and
hearing people is hindered by lack of a common language.
Although there has been a lot of research in this domain,
there is room for work towards a system that is ubiquitous,
non-invasive, works in real-time and can be trained interactively by the user. Such a system will be powerful enough
to translate gestures performed in real-time, while also being flexible enough to be fully personalized to be used as
a platform for gesture based HCI. We propose SCEPTRE
which utilizes two non-invasive wrist-worn devices to decipher gesture-based communication. The system uses a multitiered template based comparison system for classification on
input data from accelerometer, gyroscope and electromyography (EMG) sensors. This work demonstrates that the system
is very easily trained using just one to three training instances
each for twenty randomly chosen signs from the American
Sign Language(ASL) dictionary and also for user-generated
custom gestures. The system is able to achieve an accuracy
of 97.72 % for ASL gestures.
Author Keywords

Sign language processing; gesture-based interfaces; assistive
technology; wearable and pervasive computing.
ACM Classification Keywords

K.4.2 Social Issues: Assistive technologies for persons with
disabilities; H.5.2 User Interfaces: User-centered design
INTRODUCTION

Non-verbal communication is a big part of day-to-day interactions. Body movements can be a powerful medium for
non-verbal communication, which is done most effectively
through gestures [6]. However the human computer interfaces today are dominated by text based inputs and are increasingly moving towards voice based control [9]. Although
speech is a very natural way to communicate with other people and computers, it can be inappropriate in certain circumstances that require silence, or impossible in the case of deaf
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IUI 2016, March 7–10, 2016, Sonoma, CA, USA.
Copyright c 2016 ACM 978-1-4503-4137-0/16/03 ...$15.00.
http://dx.doi.org/10.1145/2856767.2856794

people [26]. Expressibility is also lost because a lot more
could be communicated if machines were trained to recognize gestures on top of the traditional User Interface(UI) elements like text input and speech. These factors have inevitably nudged us towards gesture based communication.
There are several challenges that need to be addressed and
solved before we can fully implement such methods. The first
one is the lack of a common protocol for gesture based communication, and the second one is the lack of a framework
that can successfully translate such communication gestures
to meaningful information in real-time. Another important
aspect of designing gesture based communication methods is
that they need to be pervasive and non-invasive. A survey of
ASL users that was taken as part of this research to better assess such requirements, which is summarized in Table 2. The
participants of this survey were university students between
ages of 19-38, who had completed or were working towards
an ASL course at ASU. Everybody in the survey was comfortable conveying simple ideas using ASL.
The system that is proposed as part of this work, SCEPTRE,
fulfills these requirements. The entire system is comprised of
an Android smartphone or a Bluetooth enabled computer and
one to two commercial grade Myo devices that are worn on
the wrist to monitor accelerometer, electromyogram(EMG)
and orientation. Data is aggregated and preprocessed in
the smartphones and is either processed locally or sent to a
server. This flexible approach makes it very pervasive as the
users can walk around anywhere with the ready-to-use system [4],[2]. Furthermore the devices can be worn underneath
a shirt inconspicuously. In the future smart-watches equipped
with EMG sensors can be used instead. The main challenge
of this research was to develop a method to store, retrieve and
most importantly match gestures effectively, conclusively and
in real-time.
There are various known methods for gesture recognition and
time-series analysis, such as Support Vector Machines, Linear Correlation, Hidden Markov Models, and Dynamic Time
Warping [24], [14], [7], [10]. Two classes of methods are usually used for time-series analysis and data-matching: One is
the learning based, in which models are developed and stored
and the other one is the template based, in which the new
time-series data is compared to a saved template. Although
learning based models were tried, the template based models were preferred to allow user-driven training with the least
number of repetitions and to make sure the system is usable

in real-time. A very compelling advantage of utilizing a usertrained approach is that the gesture space can start small and
can gradually grow as it is being used. This will also give
the users an ability to discover a pattern of expression, train
their own system and use that to communicate seamlessly
even with users of other systems or languages. Due to the
flexibility of the system, a user who is well versed in a particular gesture based system such as ASL, can choose to train
the system according to the specifications of that language.
She can then add custom gestures or shortcuts to add to that
system if desired.

can be converted to a regular language for communication
purposes just as one would use an audio input to form sentences, or they can be used to trigger specific actions according to pre-configured preferences. For example a user may
use a certain gesture to signal she wants to control a certain
device. The smartphone hub can then trigger a connection to
that device, say a garage door, and the user can give a specific
gesture command to open the door, and then another one to
disconnect as seen in Figure 1.

To demonstrate this, the system was first trained by randomly
selecting ten control-oriented signs that are intuitive to use
such as drawing a square or a letter in the English alphabet in
the air with one or both hands. Other more intuitive gestures
such as ’twisting one hand’ or ’waving right’ etc. were also
used. The recognition rate on these ’custom’ gestures can be
kept very high since the system is designed in such a way
that while using non-language based signs, the system is able
to guide a user to select only those signs that are sufficiently
’different’ from the signs that the system is already trained
in; this is achieved in a similar way as if a test gesture was
being recognized. The system accepts a new gesture only if
recognizing the gesture causes no loss in overall accuracy.
This is called the ’guided’ mode in the system.

Most existing work on sign language recognition utilize image/video based recognition techniques while some other utilize data gloves [21], [19], [12], [17], [29]. Although a high
accuracy is achieved in the aforementioned data-glove based
approaches, one major drawback is that the system is wired
and invasive as it interferes with day-to-day activities of the
user. According to Fang et al. [12], commercially existing
data-gloves are also too expensive for large scale deployment.
The system that we propose, utilizes Bluetooth enabled sensors that appear in wearables such as smart-watches, and are
thus pervasive and non-invasive. They can also be easily worn
underneath a shirt to make them less conspicuous.

To demonstrate the extendibility of the system, it was then
trained for 20 ASL signs by turning the ’guided mode’ off.
Experiments were done both when the system was trained
in these signs by the user, and when the system was used
with training data only from the other ’pool’ of users. Due to
the vast difference in EMG signals between users, the latter
experiments did not yield very favorable results. However,
recognition rates of 97.72% was achieved when all three of
EMG, accelerometer and orientation sensors were used and
the database consisted of three instances of the gesture trained
by the same user. Using any two of the three sensors, the
highest recognition accuracy of 95.45% was achieved when
using EMG sensors paired with orientation sensors. With
only one of the sensors the highest accuracy that was achieved
was 93% for orientation sensor. Data was collected from 10
healthy adults between the ages of 22 and 35, each of whom
performed a total of 20 ASL gestures.
Usage

The system is envisioned to be used in two primary use cases:
1. User-to-user communication where at least one user uses a
sign language: In this scenario, the user wishes to communicate with a person using a gesture based communication
medium, but the other person cannot understand that mode
of communication. The first user, then begins the translation
mode and performs a gesture. The hub receives the data, processes it and sends the recognized output to the user via a text
message or audio message as seen in Figure 2 or converted to
animated models as facilitated by systems like that described
by Marshall et al. [31].
2. User-to-computer Interactions: In this scenario, the user of
system uses ASL or some form of custom gestures to communicate with a computer system. The gestures transmitted

RELATED WORK

The image and video based recognition systems [33, 23, 16,
18, 5] use either annotated fingers, color bands etc, or just
plain raw images and video from users to translate gesture
based communication. Some studies also utilized multiple
cameras [37] to enhance the accuracy by some margin. However, image and video based systems are dependent on presence of cameras in the right positions and are affected by
background colors. Another drawback to these systems is
that image and video processing is computationally expensive
which makes the recognition system’s response time slower.
Also, heavy dependence on Hidden Markov Model (HMM)
based methods makes training the system cumbersome and
data intensive. Unlike these approaches, using portable wristwatch like sensors to detect gestures is far less invasive and a
template based recognition approach does not require many
training data-sets. The computational complexity also decreases as there is no need to deal with data-intensive images
and videos, this allows the system to be real-time as well as
environment independent.
Li et al. [20] has done a study on combining accelerometer
and EMG sensors to recognize sub-word level gestures for
Chinese Sign Language and Chen et al. [8, 39] show that
combination of multiple sensors helps to increase the recognition accuracy. Following this path, we add the EMG measurements from eight built-in pods in the Myo device to get information that can be leveraged to detect subtle finger movements and configurations which are essential for detecting
certain signs and distinguishing them from others. The orientation sensors help to distinguish between signs that have a
similar movement and muscle patterns but different rotational
primitives. Accelerometer sensors detect movement and position of the hands. The combination of all these sensors into
one commercial grade, wireless sensor has given the ability
to deploy gesture and sign-language recognition abilities in a
far more practical and user-friendly way. Bluetooth technol-

Figure 2. Usage for Person-to-Person Communication.

Figure 1. Usage for HCI/ Control system.

Existing
Gesture
Recognition
Algorithms
[21]
[19]
[12]
[33]
[8]
Sceptre(Proposed)

Mobile

Table 1. Summary of existing Gesture Recognition tools.

⇥
NA
⇥
⇥
⇥
X

User
Extendable

Requires
Video

Real Time

User Independent

Invasive

⇥
⇥
X
⇥
⇥
X

⇥
⇥
⇥
X
⇥
⇥

X
⇥
NA
X
X
X

⇥
X
X
⇥
⇥
⇥

X
X
X
X
X
⇥

Language
Independent
⇥
⇥
⇥
⇥
⇥
X

ogy for transfer of data makes the system wireless and easy
to use. The overall accuracy is also increases due to the use
of all three types of signals.

is utilized in recognizing orientation. Data from EMG sensors
is utilized in detecting muscle tensions to distinguish handshapes, which is another very important feature of the signs.

Thus by coupling pervasive technologies with simple algorithms we achieve high accuracy rates and great mobility.
Starner et al. propose a solution that might be user and language independent as possibilities, however an implementation is not provided [33]. Moving this system towards complete user-independence is part of future work. The visual
nature of sign language communication is however not limited to just hand gestures; expressions and body gait also play
important parts in the overall communication. However analyzing facial expressions and body gaits will be part of future
work.

Location, or tab, refers to specific places that the hands occupy as they are used to form words. ASL is known to use
12 locations excluding the hands [36]. They are concentrated around different parts of the body. The use of a passive
hand and different shapes of the passive hand is also considered valid locations. Movement or sig, refers to some form
of hand action to form words. ASL is known to use about
twenty movements, which include lateral, twist, flex, opening or closing [34]. A combination of accelerometer and gyroscope data is instrumental in distinguishing signs based on
movement and signing space.

PRELIMINARIES
American Sign Language Specifics

Signs in ASL are composed of a number of components: the
shape the hands assumes, the orientation of the hands, the
movement they perform and their location relative to other
parts of the body. One or both hands may be used to perform
a sign, and facial expressions and body tilts also contribute
to the meaning. Changing any one of these may change the
meaning of a sign [15], [3], [35], [11]. This is what gives
sign-language the immense power of expressibility, however
this also means that multi-modal information has to be processed for sign-language recognition.
Orientation is the degree of rotation of a hand when signing.
This can appear in a sign with or without movement. The
gyroscope sensors provide rotation data in three axes, which

Sign language is a very visual medium of communication, the
system that this paper proposes however tries to capture this
visual information, in a non-visual manner to ensure pervasiveness. Thus, multi-modal signals are considered to get as
close of an estimate as possible.
Problem Description

The primary task of the system is to match gestures. Since
multi-modal signals are being considered, they are compared
individually to the signals stored in the database and the results are combined. The form of the input data is described in
detail Section Methodology, but in short it consists of an array
of time-series data. The gesture database has pre-processed
iterations of the same gesture along with other gesture data.
The problem that the system is solving is comparing this test
gesture data to each of the existing sample data using DTW

Algorithm 1 Problem Description.
1: procedure M ATCH G ESTURES(test)
2:
for (i in 1 to database size) do
3:
template
next template
4:
for (j in 1 to number of sensors) do
5:
distance[j]
dist(test[j], template[j])
6:
end for
7:
Overall dist = combine(distance)
8:
end for
9:
Output = min(Overall dist)
10: end procedure

Figure 3. Myo Devices and Android Smartphone with the Sceptre App
showing.

and Energy based techniques as appropriate and then combining the results at the end to give a distance score. Then, at
the very end, the task is to recognize the gesture based on the
least distance. Algorithm 1 provides a high level overview of
this approach.
REQUIREMENTS

The need/demand for sign language recognition software for
accessibility has been previously established, but accessing
if there is a need or demand for systems that can utilize an
established natural language as a human computer interface
has not been studied (to our knowledge). As part of this research a survey was taken where 13 users of ASL expressed
their opinions on these topics that included their willingness
to use a gesture based system, the perceived practicality of
using ASL for HCI, the acceptable time constraints for user
to user and user to computer communications etc. The results
of the survey are summarized in Table 2.
Extendability

With more and more systems with voice command interfaces,
it only seems plausible that a system that understands sign
language will be desirable. To the question 2 in the survey,
more than 75% of the ASL users agreed that they were very
likely to use ASL for HCI, and the rest would be somewhat
likely to use such a system. More than 75% people also responded that it was very important that the system be easily
trainable, and another 15% thought it was somewhat important. This follows from the knowledge [15] that ASL (and
sign language in general) varies a lot over geographical, racial
and cultural bounds. Thus, the ability to add unrecognized
signs or new signs easily is very important. When ASL users
were asked if they were likely to use the system if it could be

trained using 3 training instances, more than 76% said very
likely and the rest responded they were somewhat likely to
spend the time training. This requirement puts the constraint
on the system that it has to be relatively easy to train. This
system can be trained using only three instances of training
per sign. The relationship between training instances and accuracy is summarized in SectionResults and Evaluation.
Time Constraints

Communication is a real-time activity and the need for fast response times in both person-to-person communication as well
as HCI is well established. A timing delay of 0.1 s is considered ’instantaneous’ while a delay of 0.2 s to 1 s is noticeable
to the user, but generally acceptable. If a system takes more
than 1 s, then some indication needs to be given to user about
the ’processing’ aspect [22]. The survey question regarding
timing backs this up: while more than 76% of ASL users said
they were very likely and an additional 15% said they were
somewhat likely to use the system if the response time was
under 3 s, this number falls to 53% and 0% very likely responses when the response times were 3-7 s, and more than 7
s respectively. This establishes a strict constraint for recognition time for the system. To meet these standards the system
has to be light-weight and should not rely on computationally expensive methods. Also, if the processing is delayed
due to unpredictable issues like client network sluggishness,
a proper indication has to be given to the user of the wait.
Non-Invasiveness

Historically there has been a certain stigma associated with
signing. It was only in 2013 that the White House published a statement in support of using sign language as an official medium of instruction for children, although it has been
known to be a developmentally important for deaf children
for years [1]. Given this stigma, and due to other more obvious reasons like usability, an interpretation system should
be as non-invasive as possible. The survey results confirm
this, as more than 90% thought that it is important or very
important for the system to be non-invasive.
Pervasiveness

The other disadvantage of using more invasive technology
like data-gloves or multiple-cameras is that it makes the system either tethered to a certain lab based environment settings
or it requires bulky equipment that discourages its use in an
everyday setting [21, 19, 12]. This makes the use of smartwatch like wireless equipment that can work on the streets as
well as it works in a lab more desirable.
To make sure that the proposed system meets these requirements some performance metrics are decided upon which are
discussed in detail in Subsection Performance Metrics and
subsequently the system is tested w.r.t to these metrics as discussed in Subsection Design of experiments.
SYSTEM ARCHITECTURE

The system consists of two Myo Devices connected via Bluetooth to an Android device which acts as a hub as shown
in Figures 3 and 4. For the EMG part a Bluetooth enabled
computer is used to obtain the data since the Myo framework

Table 2. Summary of ASL users’ survey on a continuum from very unlikely (0) to very likely (5).

Question
Interest in trying solutions for gesture based HCI
Interest in using ASL for gesture based HCI
Likeliness to use a system that requires 3 trainings instances
Importance of non-invasivenss
Likeliness to use if wait time 0-3 s
Likeliness to use if wait time 3-7 s
Likeliness to use if wait time 7-10 s
Likeliness to try a prototype
Likeliness of the use of this system by a native speaker for communication

1
0
0
0
0
0
1
4
0
0

2
0
0
0
1
1
1
4
1
1

3
1
3
3
3
2
4
5
3
1

4
3
4
1
2
4
5
0
4
5

5
9
6
9
7
6
2
0
5
6

specific accuracy, new signs trained etc. to the server. This
information can be used to make future predictive methods
more efficient which is not part of this work.

Figure 4. Deployment: A user with a wrist-band devices on each hand
performing a gesture.

does not yet facilitate streaming EMG data to smart phones.
For tests that do not use EMG data, Android device collects
and does some pre-processing on the data of the gesture performed and stores this as time-series data. All this is sent to
the server for processing. Three to four instances for each
gesture data are collected, processed and stored. A high level
overview of the pre-processing that is done to the data before
it is stored in the database is summarized in Figure 5.
At test time when a gesture is performed, the system processes the gesture-data in a similar way as described above. In
the Architecture Figure 6, this is encapsulated in the ’Preprocessing’ block. After this is done, the system compares this
data with the other gesture data stored in the database using
a specialized comparison method. The comparison method
comprises of testing accelerometer data, orientation data and
EMG data by different methods and later combining the results before calculating a ’nearness’ score for each pair. After
this step a ranking algorithm is employed in the server to compute a list of stored gestures that are closest to the test gesture
and best one is returned. Details on these techniques is given
in Section Methodology. The system using the same methods
of comparison is able to help the user be selective in the signs
that the system is trained on, this is called the ’guided mode’
as explained in Subsection Training. A smart ranking architecture is used when the size of the database grows to ensure
that the results come back in acceptable time to be a real-time
application. This is discussed in more details in Subsection
Ranking and Optimization. The system also periodically uploads all recognized gesture data and other meta-data like user

Figure 5. Pre-processing: Data is collected from the Myo devices, processed and then stored in Database.

Training

Training the system requires two Myo devices, a hub for data
collection, and server for processing. The hub and the server
may be one and the same. A typical training sessions consists
of the following steps: 1. The user selects either the ’guided
mode’ or ’ASL’ mode. (in the smartphone interface) 2. She
selects a sign from a dropdown list, or creates a new sign. 3.
She performs the sign. 4. The system annotates the input data
with the name of the sign and stores the data. 5. If guided
mode was selected, the system performs a scan of the sign
database specific to the user and determines if there are any
clashes. If not, the user is asked to repeat the sign two more
times after which the sign is ready to use. If however there
was a clash, then the user is suggested to choose another sign
instead. 6. If the ’ASL’ mode was selected, the system does
not give such feedback, and simply asks the user to train the
system two more times. 7. The gesture is ready to use.
METHODOLOGY

The overall methodology of gesture recognition consists of
the following steps as shown in Figure 6.
Pre-processing

Data is collected from two Myo devices while the gesture is
being performed (Figure 5). As soon as the end is detected or

Figure 7. Variation in EMG signals between two users.
Figure 6. Gesture Comparison and Ranking: New data is collected, processed and compared with existing gesture data to get a ranked-list of
similarities.

signaled, the pre-processing begins. The data collected from
two hands is aggregated into one data-table then stored into a
file as an array of time-series data. At 50 Hz. of sampling rate
a 5 s. gesture data will consist of: 6 accelerometer Vectors of
length 250 each, 6 gyroscope Vectors of Length 250 each,
6 orientation Vectors of Length 250 each, 16 EMG Vectors,
Length 250 each. We combine all this for simplicity into a
34 ⇥ 250 matrix. Each time-series is transformed to make
sure the initial value is 0, by subtracting this value from all
values in the time-series. This will help prevent errors when
the user performs the sign in a different starting positions.
Normalization is then done by representing all values as floats
between 0 and 1 by min-max method.
Orientation values are received in the form of three timeseries in terms of unit quaternions. The pitch, yaw and roll
values are obtained from the quaternion values w, x, y and z
by using the following equations:
roll = tan
pitch = sin

1

1 2(wx+yz)
( x2 y 2 )

(max( 1, min(1, 2(wy

yaw = tan

(1)
zx))))

1 2(wz+xy)
( y2 z2 ).

EMG Energy

After correctly identifying the location of each of the individual pods of the two Myo devices, data is stored and shuffled
in such a way that the final stored data is aligned from EMG
pod-1 to EMG pod-8. This gives flexibility to the end-user as
she no longer has to remember ’how’ to put on the devices,
and can thus randomly put either Myo device on either hand
and expect good results. This is a great leap towards the pervasiveness and ease of use of this technology.
While we found that the DTW based testing worked very
good for accelerometer and orientation comparisons, the
EMG comparison numbers were not satisfactory. This is because, EMG activations seem to be much more stochastic and
a ’shape’ based comparison did not yield good results. The

shapes formed by performing the same gesture for two different people are drastically different as seen in Figure 7, but
they also differ in shape significantly enough between two
repetitions by the same person to not give a small DTW distance. However, it was found by experimentation that gestures tend to activate the same EMG pods when repeated.
Thus an energy based comparison was tried:
EMG energy ’E’ on each pod is calculated as the sum of
squares of x[n] , the value of the time-series at point ’n’ .
E = sum(x[n]2 ).

(2)

Dynamic Time Warping

We used four different approaches for comparing accelerometer and orientation data: a. Euclidian distance b. Regression
c. Principal Component Analysis (PCA) and d. DTW. Euclidian distance simply compares the two time-series using
mean-squared error. Regression analysis fits a model to the
time-series and uses this model to compare best fit for the test
gesture. Given a set of features from the time-series for a gesture, PCA derives the optimal set of features for comparison.
According to [25], DTW is a well-known technique to find an
optimal alignment between two given (time-dependent) sequences under certain restrictions. It was found that DTW
based methods gave best results. DTW based approached
proved ideal in our use-case since it could gracefully handle the situations when the sign was performed slower or
faster than the stored training data, or performed with a small
lag. Traditionally, DTW has been used extensively for speech
recognition, and it is finding increasing use in the fields of
gesture recognition as well [30], specially when combined
with Hidden Markov Models [38]. SCEPTRE takes a simpler approach by randomly re-sampling the training and test
datasets based on the least number of points in either one,
then doing a DTW based distance analysis on them. First a
DTW analysis of Accelerometer Data is run and a ranked list
of ’probable’ signs is passed on for DTW based analysis of
orientation Data which in turn creates an even shorter ranked
list to be processed by the final EMG algorithm.
On another approach, the normalized distances from each of
the outputs is taken and the sum of squares of the final out-

Algorithm 2 Overall Gesture recognition method.

Algorithm 3 Optimization to meet time constraints.

1: procedure G ESTURE R ECOGNITION
2:
t
elapsed time from start of the experiment
3:
test gesture
get gesture data from the user
4:
SG
query list of recognized gestures from database
5:
normalized accl
accel values f irst accl value
6:
normalized orient
orientation f irst orientation
7:
normalized emg
emg values f irst emg value
8:
for each trained gesture TG 2 SG do
9:
dtw accl
dtw(training accl, normalized accl)
10:
dtw orient
dtw(training orient, normalized orient)
E
11:
TG
calculated energy of each pod for the training gesture
E
12:
emg energy dif f
TG
- test emg energy
13:
scal lim
{0, 1}
14:
scaled emg dif f
scale(emg energy diff,scal lim)
15:
scaled accl dist
scale(dtw accl, scal lim)
16:
scaled orient dist
scale(dtw orient, scal lim)
17:
combined nearness
compute nearness from Eq. 3
18:
end for
19:
Sort TG with respect to combined nearness.
20:
recognized gesture
top TG in the sorted list
21: end procedure

1:
2:
3:
4:
5:
6:
7:
8:

databasesize
sizeof gesturedatabasef oruser
time to compute
estimatedtimef rompreviousiterations
if time to compute < time contraint then
Compute Similarity Score with one template per gesture
top gestures
list(top gestures, time to compute, time constrant)
Utilize Algorithm 2
END
end if

put is taken as an indication of ’closeness’ of a test sign to a
training sign. This is the approach that was chosen due to its
computational simplicity and speed.
Combination

The overall ’nearness’ of two signs is computed to be the total
distance between those signs which is obtained by adding up
the scaled distances for accelerometer, orientation and EMG
as discussed above. An extra step of scaling the distance values between (0,1) is performed so as to give equal weight to
each of the features. Also, since we have 8 EMG pods and
only 3 each of accelerometer and orientation sensors, we use
the following formula for the combination. The formula is for
combining accelerometer sum of distances and EMG sum of
distances. Similar techniques were applied for the other combinations. An algorithmic summary can be found in Equation
3.
dist = (8cs(sc accl comb) + 3cs(sc emg comb))/24.

(3)

where cs() is a function that returns the sum of columns,
sc accl comb is a data frame that holds the combined accelerometer DTW distances for both hands for all trained
signs, and sc emg comb is a data frame that holds the combined EMG energy distances for both hands for all trained
signs. The overall algorithm that is employed for recognition
is summarized in Algorithm 2.
Ranking and Optimization

Timing constraints due to the real-time nature of the application requires us to optimize the recognition algorithm to
be efficient, specially as the gesture space increases. As the
number of gestures in the database increases to beyond 60,
as we can see in Figure 8, the recognition time for identifying one gesture goes beyond the .5 s mark, which we have
chosen to represent a real-time timing constraint. Thus, to
deal with this situation, we modify our comparison algorithm
to first compare to one stored instance of each gesture, then
choose the top ’n’ number of gestures which when compared
to ’k’ of each, still allowing the time-constraint to be fulfilled.
We then, proceed with the normal gesture comparison routine
on only these gesture instances and thus keep the recognition

Figure 8. Database Size vs. Recognition Time.

time within our constraints. All the variables for this method
viz. the ’n’, ’k’ are calculated dynamically by what is allowed
by the timing constraint ’tc’, thus making this approach fluid
and adaptable to more vigorous time constraints if required.
The overall optimization algorithm is summarized in Algorithm 3.
RESULTS AND EVALUATION
Design of experiments

Experiments were designed to test the application with scalability in mind. The system begins on an ”empty slate” and
the gestures are trained interactively. Since a template based
comparison is being utilized, we store the entire data set plus
some features. After all the data is collected, the system is
ready to be trained for another gesture. A sample application
screen can be found in Figure 3. It is advised to provide 3-4
training instances of the same gesture to improve accuracy of
recognition but that is not required. We test the system with
1, 2, 3, and 4 instances of each gesture to determine the correlation between number of instances saved and the recognition
accuracy rate. That is all that is needed at the training phase.
During testing phase we evaluate our system based on time
it takes for recognition and on combination of features that
produce the best results. Then we evaluate how recognition
time increases with the increase in the number of gestures.
The way gestures are performed by the same person are generally similar but they have a tendency to vary slightly over
time. Although, considering signals generated by a portable
device that the user can put on and off by herself, provides the

system a lot of portability and pervasiveness, it comes with a
drawback that the signals we receive might not always be the
same. To account for this experiments were performed that
allowed users to put on the devices themselves, they were allowed to face any direction, and perform the gesture as long
as they stayed in the Bluetooth range of the hub. Also, samples were collected over multiple sessions to account for variability in signing over time. The test subjects were 10 University students between the ages of 20 and 32 who performed
the gestures in multiple sessions while also varying direction
they were facing and if they were standing up/sitting down.
The subjects viewed a video recording of the gestures being
performed by ASL instructors before performing them with
the system.

Figure 9. Training Data vs. Accuracy.

Prototyping and Choice of Gestures

20 ASL signs were chosen to prototype the system. They
were carefully chosen such that a. A good mix of the various
ASL components as discussed in Section Preliminaries and b.
To include signs that are very close in some components but
different in others. The choice of signs and the break-down
of components for 10 of the 20 chosen signs is summarized in
Table 3. The other 10 signs that were chosen are hurt, horse,
pizza, large, happy, home, home, mom, goodnight, wash. Detailed analysis of system performance is give in Section Results and Evaluation.
Performance Metrics

The system is evaluated on each of the requirements discussed in Section Requirements. Pervasiveness of the system
is justified since it is wireless and can work in Bluetooth range
of the hub device which can either do the processing itself or
offload it to a cloud server. Invasiveness is a harder metric to
evaluate, as this seems to be more subjective. However, with
wearable devices like smartwatches, wristbands such as FitBit, Myo becoming increasingly compact, wireless and even
stylish [13], the proposed system is much less invasive then
any of the alternatives. (see Related Work). Accuracy is undoubtedly one of the fundamental performance metrics for
any recognition system. This basic function of the system is
to facilitate real-life conversations, thus the system has to be
able to function in real-time. Another aspect of the system
is that it is extendable, thus the ease in scalability to a larger
number of signs is very important. With the increase in the
gesture space, the recognition time will also increase, the system should be able to scale up with a reasonable recognition
time. Although there is a slight dip in recognition rate, the potential for scalability showcases the system’s flexibility. Thus
the experiments focus on these three metrics to evaluate the
system:

on both hands. The desired frequency of each channel is 50
Hz. b. Wireless interface to transmit raw data from each of
these sensors. c. A smartphone that acts as a hub to receive,
pre-process, communicate the data to server, and later to display the results. d. A server that stores previous data in an
individualized database per user, compares gestures, finds a
match, and sends information back to the smartphone.
All experiments were performed with the setup described in
Section System Architecture with devices with the following
configurations:
Two Myo Sensors: Medical Grade Stainless Steel EMG sensors, Highly sensitive nine-axis IMU containing three-axis
gyroscope, three-axis accelerometer, three-axis magnetometer. Expandable between 7.5 - 13 inches, wieghing 93 grams
with a thickness of 0.45 inches. Communication channel:
Bluetooth. Processor:ARM Cortex M4 Processor Hub Specifications(Android): OS: Android OS, v5.0 Chipset: Qualcomm MSM8974 Snapdragon 800 CPU: Quad-core 2.3 GHz
Krait 400 Memory: 2 GB RAM Mac Specifications(Server)
Processor:3.06GHz Intel Core i3 processor with 4MB level 3
cache; supports Hyper-Threading Momory: 8 GB DDR
Results

Device Requirements

It was determined through experiments that three is a good
choice for the number of training instances for each gesture.
This formed a good compromise between usability and results as shown in Figure 9. Thus, each gesture was performed
three times at varying times in the day. Then all data was aggregated, and annotated according to the gesture performed.
The testing comprised of taking one dataset and comparing it
with all other data sets and then estimating the correct gesture. With the ’guided mode’ turned on, a recognition rate of
100% was achieved as expected. Then 20 randomly selected
ASL Signs were used. The best accuracy was obtained by a
tiered-combination method of all three features. The relative
accuracy of other methods can be seen in Figure 11. This
helps confirm that the combination of all three features produces the highest results.

Sensors: a. EMG, accelerometer and orientation (or Gyroscope) sensors that go between the wrist and elbow of a user

Figure 8 shows how the recognition time increases with the
increase in the number of gestures that are stored in the system. With 65 gestures in the database, a recognition time of

1. Recognition time
2. Extensibility
3. Recognition rate (Accuracy)
The device requirements for the experiments are:

Number Name
of
Sign
1
Blue
2
Cat
3
Cost

Location(Tab)

4

Table 3. Component breakdown of 10 of the chosen signs.

Movement

Orientation

Around the Face
Around the face
Trunk

Wrist-Twist
Outward and closing
Wrist-Flex

Dollar

Trunk

Two Wrist-Flexes

5

Gold

Side of the head

6
7
8
9
10

Orange
Bird
Shirt
Large
Please

Mouth
Mouth
Shoulder
Trunk
Trunk/Chest

Away from center, change handshape twist
Open and close twice
Pinch with Two fingers
Twice Wrist Flex
Both Hands Away from Center
Form a circle

Away from signer
Towards center
One towards signer one towards
center
One towards signer one towards
center
Towards the signer
Towards user
Away from signer
Facing down
Facing away from signer
Both facing towards signer

Number
of
Hands
1
2
2
2
1
1
1
2
2
1

Figure 10. Gesture for ’Mom’ Vs. ’Eat’.
Figure 12. User Trained Gestures.

Figure 11. Features vs. Accuracy.

0.552 s was achieved. (processed on a 3.06 GHz Intel Core
i3 8 GB RAM Mac). This fast response time means that the
system qualifies for use for real-time communications. This
time can be further improved upon by utilizing a more powerful server and optimizing the data queries, which will be part
of future work.
Figure 13 shows the performance of the system based on
recognition results for a combination of the features. The gestures tested were ’day’, ’enjoy’, ’eat’, ’happy’, ’home’, ’hot’,
’large’, ’mom’, ’pizza’, ’shirt’, ’still’, ’wash’ and ’wave’. The
different sub-figures show that although comparing solely
based on accelerometer performs good for some gestures like

’happy’, it is ineffective in distinguishing between others like
’day’. This can be explained because the nature of performing
this gesture is very similar to other gestures in overall hand
movements, but are different when it comes to finger configurations. Thus the performance gets better as shown in Figure
13c when EMG sensors are brought into the equation. Like
we discussed earlier, the least number of recognition errors
overall occur when all three of the sensor data are fused as
seen in Figure 13d. This gives an insight on where the system
is error-prone and thus can be optimized. Server level Optimization based on such input will be part of future work. We
envision a server that continuously monitors success-failure
data and becomes better by implementing machine learning
algorithms to give weights to the different features.
These results can be understood better in context with the
breakdown information given for each gesture in Table 3. For
instance, Figure 10 shows screenshots of a person performing
the gesture ’mom’ and another one performing ’eat’. These
two gestures are very similar in ’Location’ and ’Movement’
but are distinctive when it comes to ’HandShape’ and ’Orientation of Hand’. Thus, while the system isn’t able to distinguish between these two gestures based on Accelerometer
information only as seen in Figure 13a, the system does well
when EMG information is included as seen in Figure 13c.
The guided mode as discussed in Usage was trained using 3
iterations each of 10 custom signs shown in Figure 12. An

Framework Limitations: Currently due to limitations of the
Myo framework for Android, final tests involving EMG data
signals had to be done using laptop computers. Two Mac
computers were used to gather the data simultaneously via
Bluetooth channels and then combine them and store them
to a cloud based data storage system. A separate script was
triggered to process the stored data at test time. With the
anticipated release of new framework for Myo Devices, this
computation can be done at the mobile phone level and data
can be sent to the server only at designated intervals.
User Independence: EMG data fluctuates a lot between people. This is apparent from Figure 7. More research can be
done on data gathering and feature selection of the EMG data
pods to come up with a ’unifying’ framework that works for
everyone. This will be a definitive direction in attaining userindependence while using EMG signals.
Search Algorithm: The search algorithm that is implemented can be improved upon to decrease the recognition
time. Hierarchal searching can be done, in which training
gestures are clustered according to the energies in each timeseries data, and only those gestures are compared which have
comparable energies.
Figure 13. Gesture Recognition vs. Features used.

accuracy of 100% is achieved for this mode as expected, because clashes are detected at training time and avoided.
The results in Figure 11 show an accuracy of 97.72% for 20
ASL signs when all accelerometer, orientation and EMG are
used. The database consisted of signs from 10 different people, and each person performed each sign 3 times. This table
also lists the accuracy of other variations in the combination
of these three input signals. The accuracy varied when individual databases were separated, however it stayed in the
(97-100)% bracket when all signals were used on a dataset
consisting of template gestures from the test user.
DISCUSSION

Continuous Processing: The gestures that were recognized
in this test were isolated in nature. A start and stop button, or
a timer was utilized to gather data. This was done to test the
algorithms in isolation first. However, when the system is actually deployed to the public, a continuous monitoring algorithm has to be utilized. This can be done by using windowing
methods and optimizing based on best results. However, we
expect that the system will require a lot more training.
Dictionary Size: Another thing that can be improved upon
is the dictionary size and the support for Signed Alphabet.
Signed alphabet give an ASL user the ability to use English
Language Alphabet to visually spell out a word. This is an
important aspect of sign language communication since not
all english words or ideas have sign language counterparts.
The dictionary size used is currently 20 ASL words and 10
user invented gestures. The video based sign language dictionary website signasl.org currently has over 30,000 videos for
sign languages in use, so there is a lot of room to grow into.

SCEPTRE in collaboration with other HCI techniques such
as brain-driven control [27, 28, 32] can revolutionize the way
people interact with computers. In addition to the uses of HCI
or Sign Language Communication, this technology can also
be extended to uses in the domains of activity recognition,
food intake estimation or even physiotherapy.
CONCLUSIONS

DTW and energy based comparison methods are fast and
highly effective. Template based comparison techniques have
a great advantage of lossless information storing. Comparing accelerometer and orientation data between gestures is
best done by using Dynamic Time Warping methods which
muscle movement (EMG) data is best compared by comparing total energies in each pods. The structure present in Sign
Languages like ASL can be divided into components which
explain the success of certain signals in recognizing them.
The overall accuracy of the system is increased by a smart
combination of these signals without compromising on speed,
recognition rate, or usability. The sensors that are used are
non-invasive and versatile thus allowing people to effectively
utilize them in day-to-day situations without drawing much
attention. The future direction of this research is in incorporating continuous sign processing, user independence in the
system and increasing the dictionary size.
ACKNOWLEDGEMENTS

This project was partly funded by NIH NIBIB (Grant
#EB019202) and NSF IIS (Grant #1116385). We thank Julie
Stylinski, Lecturer at ASU for her invaluable ASL domain
knowledge and for facilitating the survey. We thank all
the survey participants as well as volunteers that helped to
train/test the system. We thank Subhasish Das of IMPACT
lab, ASU for his technical contribution to get the project going.

REFERENCES

1. Daphne Bavelier Aaron J. Newman. A critical period for
right hemisphere recruitment in american sign language
processing. In Multimodal Signals: Cognitive and
Algorithmic Issues, pages 76–80, 2002.
2. Frank Adelstein, Sandeep KS Gupta, Golden Richard,
and Loren Schwiebert. Fundamentals of mobile and
pervasive computing, volume 1. McGraw-Hill New
York, 2005.
3. B.J. Bahan. Non-manual Realization of Agreement in
American Sign Language. UMI Dissertation Services.
UMI, 1996.
4. Ayan Banerjee and Sandeep KS Gupta. Analysis of
smart mobile applications for healthcare under dynamic
context changes. Mobile Computing, IEEE Transactions
on, 14(5):904–919, 2015.
5. Sara Bilal, Rini Akmeliawati, Amir A Shafie, and
Momoh Jimoh E Salami. Hidden markov model for
human to computer interaction: a study on human hand
gesture recognition. Artificial Intelligence Review,
40(4):495–516, 2013.
6. Baptiste Caramiaux, Marco Donnarumma, and Atau
Tanaka. Understanding gesture expressivity through
muscle sensing. ACM Transactions on
Computer-Human Interaction (TOCHI), 21(6):31, 2015.
7. Feng-Sheng Chen, Chih-Ming Fu, and Chung-Lin
Huang. Hand gesture recognition using a real-time
tracking method and hidden markov models. Image and
vision computing, 21(8):745–758, 2003.
8. Xiang Chen, Xu Zhang, Zhang-Yan Zhao, Ji-Hai Yang,
Vuokko Lantz, and Kong-Qiao Wang. Hand gesture
recognition research based on surface emg sensors and
2d-accelerometers. In Wearable Computers, 2007 11th
IEEE International Symposium on, pages 11–14. IEEE,
2007.
9. Philip R Cohen and Sharon L Oviatt. The role of voice
input for human-machine communication. proceedings
of the National Academy of Sciences,
92(22):9921–9927, 1995.

10. Andrea Corradini. Dynamic time warping for off-line
recognition of a small gesture vocabulary. In
Recognition, Analysis, and Tracking of Faces and
Gestures in Real-Time Systems, 2001. Proceedings.
IEEE ICCV Workshop on, pages 82–89. IEEE, 2001.
11. E. Costello. Random House Webster’s American Sign
Language Dictionary. Random House Reference, 2008.
12. Gaolin Fang, Wen Gao, and Debin Zhao. Large
vocabulary sign language recognition based on fuzzy
decision trees. Systems, Man and Cybernetics, Part A:
Systems and Humans, IEEE Transactions on,
34(3):305–314, 2004.
13. Sandeep KS Gupta, Tridib Mukherjee, and
Krishna Kumar Venkatasubramanian. Body area
networks: Safety, security, and sustainability.
Cambridge University Press, 2013.

14. Deng-Yuan Huang, Wu-Chih Hu, and Sung-Hsiang
Chang. Vision-based hand gesture recognition using
pca+ gabor filters and svm. In Intelligent Information
Hiding and Multimedia Signal Processing, 2009.
IIH-MSP’09. Fifth International Conference on, pages
1–4. IEEE, 2009.
15. T. Johnston and A. Schembri. Australian Sign Language
(Auslan): An introduction to sign language linguistics.
Cambridge University Press, 2007.
16. Daniel Kelly, Jane Reilly Delannoy, John Mc Donald,
and Charles Markham. A framework for continuous
multimodal sign language recognition. In Proceedings of
the 2009 international conference on Multimodal
interfaces, pages 351–358. ACM, 2009.
17. Kyung-Won Kim, Mi-So Lee, Bo-Ram Soon, Mun-Ho
Ryu, and Je-Nam Kim. Recognition of sign language
with an inertial sensor-based data glove. Technology and
Health Care, 24(s1):S223–S230, 2015.
18. Oscar Koller, Jens Forster, and Hermann Ney.
Continuous sign language recognition: Towards large
vocabulary statistical recognition systems handling
multiple signers. Computer Vision and Image
Understanding, 141:108–125, 2015.
19. T Kuroda, Y Tabata, A Goto, H Ikuta, M Murakami,
et al. Consumer price data-glove for sign language
recognition. In Proc. of 5th Intl Conf. Disability, Virtual
Reality Assoc. Tech., Oxford, UK, pages 253–258, 2004.
20. Yun Li, Xiang Chen, Jianxun Tian, Xu Zhang, Kongqiao
Wang, and Jihai Yang. Automatic recognition of sign
language subwords based on portable accelerometer and
emg sensors. In International Conference on
Multimodal Interfaces and the Workshop on Machine
Learning for Multimodal Interaction, ICMI-MLMI ’10,
pages 17:1–17:7, New York, NY, USA, 2010. ACM.
21. Rung-Huei Liang and Ming Ouhyoung. A real-time
continuous gesture recognition system for sign
language. In Automatic Face and Gesture Recognition,
1998. Proceedings. Third IEEE International
Conference on, pages 558–567. IEEE, 1998.
22. Robert B Miller. Response time in man-computer
conversational transactions. In Proceedings of the
December 9-11, 1968, fall joint computer conference,
part I, pages 267–277. ACM, 1968.
23. Sushmita Mitra and Tinku Acharya. Gesture
recognition: A survey. Systems, Man, and Cybernetics,
Part C: Applications and Reviews, IEEE Transactions
on, 37(3):311–324, 2007.
24. Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. Latent-dynamic discriminative models for
continuous gesture recognition. In Computer Vision and
Pattern Recognition, 2007. CVPR’07. IEEE Conference
on, pages 1–8. IEEE, 2007.

25. Meinard Müller. Dynamic time warping. Information
retrieval for music and motion, pages 69–84, 2007.
26. Yuji Nagashima. Present stage and issues of sign
linguistic engineering. HCI, 2001, 2001.
27. Koosha Sadeghi Oskooyee, Ayan Banerjee, and
Sandeep KS Gupta. Neuro movie theatre: A real-time
internet-of-people based mobile application. 2015.
28. Madhurima Pore, Koosha Sadeghi, Vinaya Chakati,
Ayan Banerjee, and Sandeep KS Gupta. Enabling
real-time collaborative brain-mobile interactive
applications on volunteer mobile devices. In
Proceedings of the 2nd International Workshop on Hot
Topics in Wireless, pages 46–50. ACM, 2015.
29. Nikhita Praveen, Naveen Karanth, and MS Megha. Sign
language interpreter using a smart glove. In Advances in
Electronics, Computers and Communications
(ICAECC), 2014 International Conference on, pages
1–5. IEEE, 2014.
30. Lawrence R Rabiner. A tutorial on hidden markov
models and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257–286, 1989.
31. Éva Sáfár and Ian Marshall. The architecture of an
english-text-to-sign-languages translation system. In
Recent Advances in Natural Language Processing
(RANLP), pages 223–228. Tzigov Chark Bulgaria, 2001.
32. Javad Sohankar, Koosha Sadeghi, Ayan Banerjee, and
Sandeep KS Gupta. E-bias: A pervasive eeg-based
identification and authentication system. In Proceedings
of the 11th ACM Symposium on QoS and Security for

Wireless and Mobile Networks, pages 165–172. ACM,
2015.
33. Thad Starner, Joshua Weaver, and Alex Pentland.
Real-time american sign language recognition using
desk and wearable computer based video. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 20(12):1371–1375, 1998.
34. W.C. Stokoe, D.C. Casterline, and C.G. Croneberg. A
Dictionary of American Sign Language on Linguistic
Principles. Linstok Press, 1976.
35. W.C. Stokoe, ERIC Clearinghouse for Linguistics, and
Center for Applied Linguistics. The Study of Sign
Language. ERIC Clearinghouse for Linguistics, Center
for Applied Linguistics, 1970.
36. R.A. Tennant and M.G. Brown. The American Sign
Language Handshape Dictionary. Clerc Books, 1998.
37. Christian Vogler and Dimitris Metaxas. Asl recognition
based on a coupling between hmms and 3d motion
analysis. In Computer Vision, 1998. Sixth International
Conference on, pages 363–369. IEEE, 1998.
38. Andrew D Wilson and Aaron F Bobick. Parametric
hidden markov models for gesture recognition. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 21(9):884–900, 1999.
39. Xu Zhang, Xiang Chen, Yun Li, Vuokko Lantz,
Kongqiao Wang, and Jihai Yang. A framework for hand
gesture recognition based on accelerometer and emg
sensors. Systems, Man and Cybernetics, Part A: Systems
and Humans, IEEE Transactions on, 41(6):1064–1076,
2011.

Rapid Evidence-based Development of Mobile
Medical IoT Apps
†

Priyanka Bagade∗

Ayan Banerjee and Sandeep K.S. Gupta

Intel Corporation, Portland, Oregon
Email: pbagade@asu.edu

IMPACT Lab, CIDSE, Arizona State University, Tempe, Arizona
Email: {abanerj3,sandeep.gupta}@asu.edu

Abstract—Mobile medical apps (MMAs) work in close loop
with human physiology through sensing and control. As such
it is essential for them to achieve intended functionality, without having harmful effects on human physiology, affecting the
availability of the service and compromising the privacy of
health data. However for a mobile app manufacturer, generating
evidences regarding safety, sustainability and security (S3) of
MMAs can be time consuming. To accelerate the development of
S3 assured MMAs, we propose Health-Dev β tool that takes high
level description of MMAs and automatically generates validated
code and evidences of safety, security, and sustainability. Using
the mobile artificial pancreas medical control application we
show that Health-Dev β tool can generate code that satisfies
requirements and reduce development time by a factor of 1.8.

I. I NTRODUCTION
The Internet of Things (IoT) is a network of systems
connected to the Internet in which various embedded systems
(wearable sensors/actuators) communicate with each other to
exchange information. One of the example of IoT is Mobile
Medical Apps (MMAs) used to provide pervasive health.
According to industry surveys, by 2018, more than 1.7 billion smartphone and tablet users will have downloaded an
MMA [1]. Such widespread adoption of smartphone based
medical apps is opening new avenues for innovation, bringing
MMAs to the forefront of low cost healthcare delivery. These
apps often control physiology and work on sensitive data, thus
it is necessary to have evidences of their correct functioning
before actual deployment. The key challenges in ensuring
correct working of MMAs are maintaining privacy of health
data, long term operation of wearable sensors and ensuring no
physical harm to the user [2]. Providing such evidences for
safety, security and sustainability (S3) properties can increase
development time and may require higher skills set for the
developer. Hence, although ensuring S3 is essential, it often
acts as a hindrance to innovation. In this paper, we propose
Health-Dev β tool which takes high level models of MMA
and rapidly generates S3 satisfied code as well as evidences.
We also show examples where using Health-Dev β reduces
development time by almost 1.8 times.
In an MMA (Figure 1), a wearable sensor can collect
physiological data, and communicate it to the smartphone
through the wireless channel. The smartphone in turn can
communicate the data to a cloud server, where it can be stored
† We are thankful to Yi Zhang and Paul Jones at FDA for their insights
in regulatory challenges of MMAs. The work is partly funded by NSF IIS
1116385 and NIH NIBIB EB019202.
∗ This work was done when Priyanka was at ASU.

Glucosemeter
Sensed blood glucose
level

Blood glucose level data

Controller
Decision

Insulin
Infusion

Upload

Actuation
Information (Drug
Concentration)
Infusion Pump

Fig. 1. System model for mobile medical applications. Artificial pancreas
mobile application example.

in an electronic health record (EHR). The smartphone can also
control safety critical devices such as drug infusion pump,
thus forming a cyber-physical system. The control inputs
computed by the MMA directly affects the human physiology.
In such cases, the control algorithms have to be verified
for patient safety. Due to lack of in depth knowledge about
human physiology, smartphone app developers might fail to
consider these safety concerns. Further, since the smartphone
is not a dedicated healthcare platform, non-medical apps
such as games, can affect patient safety by compromising
the availability of the medical app (e.g. via rapid battery
depletion). These apps often obtain physiological data from
the wearable sensors which work on low power. Thus to have
the sensor availability for long time, sustainable sensor design
is needed. All communication between sensors, smartphone
and cloud takes place through the wireless channel which is
prone to security attacks. Thus the wirelessly communicated
data should be encrypted. However, current app development
and certification methods for MMAs do not adequately address
these safety, sustainability and security issues.
Recently there has been efforts taken for data security
evidence generation in MMAs with companies such as Happtique [3], however most of these approaches are subjective.
Moreover, the proposed regulation system had failed to effectively detect serious security flaws in apps such as password
stored in plain text. This gives rise to a need of objectively
evaluating S3 requirements of MMAs before market approval.
In this paper, we hypothesize that a model based app
development approach will enable us to objectively evaluate
S3 requirement of MMAs and possibly generate trustworthy
(S3 assured) code for sensitive operations such as actuator

App 1

App 2

App3

Traditional Method
Artificial Pancreas
Broadcast
Receiver
GUI

TDM
Runtime Requirements Validator
Safety

Sensors

Security

Sustainability

Execute
Controller
Algorithm

Content
Provider
WiFi

Proposed Method
Artificial Pancreas

Trustworthy
Data Manager
Broadcast
Receiver

GUI
Execute
Controller
Algorithm

Content
Resolver

Actuator

Content
Provider

Glucomete
r, Infusion
Pump
Interface
WiFi

Cloud

Fig. 2. Conceptual operating model for MMAs. The Trustworthy Data
Manager monitors interfacing calls from different applications and verifies
them against requirements.

Fig. 3. Change in MMAs data sharing methodology. For access to sensing,
actuation, and communication services the developer should issue interprocess calls to the TDM.

control, sensor interface and operation, and cloud communication. If the application design conforms to a standard
model then automated tools such as hybrid automata based
model checking techniques can be used for evaluating patient
safety due to interaction between human physiology and app
software as shown in our previous work [4]. Sensor design
optimizer can be used to obtain sustainable design for long
term availability. Moreover, security enabled software for
interfacing the smartphone with sensors, actuators, and the
cloud services can be generated automatically. To enable use
of automated verification tools, we propose an App model, an
operating model for the MMAs. It isolates the MMA sensor
or actuator interface, data communication and data storage
from the core mobile device graphical user interface (GUI)
and processing algorithms. An app model should not limit the
functionalities of the smartphone that can be exploited by the
developers, nor it can cause a performance degradation of the
app in terms of response time or memory and battery usage.
The main contributions of the paper are as follows:
App Model: The proposed app model can enable safe and
secure interactions of MMAs with sensors, cloud and other
apps through a trustworthy entity. It can contain multiple apps
working individually or sharing data with each other under
accurate permissions.
Trustworthy Framework for development of MMAs: We
propose Health-Dev β tool, which can be used by a developer
to either generate evidences for trustworthiness of MMAs or
critical parts of the code. In our manifestation, the safety
verification of smartphone controller algorithm is done using formal modeling with spatio-temporal hybrid automata
(STHA) [5], sustainable sensor design for a defined time,
hardware and software constraints is obtained by using an optimizer algorithm, and the security enabled data communication
over wireless channel is done by generating the interfacing
components with automated code generator, Health-Dev β.
Finally, we evaluate improvements in development time
with Health-dev β as opposed to manually coding using
software development effort estimation tools, COCOMO [6].

S3 requirements. UPHIAC [10] and PRISM [11] frameworks
provide health data security with APIs to interface with
smartphone sensors and cloud for data storage. However
these frameworks do not check for safety and sustainability
requirements of health apps. Programming abstractions for
sensors are being widely used to allow developers to write
minimal sensor code to include wearables in health apps [12]–
[16]. Considering the limited energy availability on wearables,
energy efficiency techniques are supported by Reflex [14],
LittleRock [15] and Turducken [16]. However these methods
require sensor programming from the developer. Thus to
reduce health apps development time and to get bug-free code,
automated sensor code generators were proposed [17]–[20].
Still they require developers to write code for smartphone
apps and its vulnerable components such as interfacing with
sensors, actuators and cloud. This led to fully automated
code generators for sensors and smartphone apps [21], [22].
However they only consider system safety and do not consider
sensor sustainability and data security. This paper discusses an
automated code generator for health apps which can ensure
their trustworthiness while reducing developer burden.

II. R ELATED W ORKS
The existing frameworks for mobile health app development
can be classified into three categories as: a) API support, b)
programming abstractions, and c) automated code generation.
The mobile app development with APIs is popular due its
ease of programming [7]–[9], however, these do not verify

III. A PP M ODEL
We hypothesize that S3 assured MMAs should have an
operating model, an app model as shown in Figure 2. In this
new model of applications, the smartphone software itself is
in charge of only the graphical and algorithmic aspects of
the application. Every instance of data communication to the
sensor, cloud, data storage in the smartphone, control inputs
to the actuator, interaction with the user should be through a
Trustworthy Data Manager (TDM). The app model enables
the development of MMAs in the form of a suite, where
participating apps are certified by the regulatory agencies
against safety, sustainability and security, defined as follows:
Safety: Any form of control input from MMA is safety assured
by checking it with hybrid automata based model.
Sustainability: Any sensor communicating with TDM supports
long term availability with the sustainable design.
Security: Any form of data communication is privacy ensured
by TDM. Also, data collected by the different applications are
kept in secured databases (similar in principal to application
sandboxing) shared with certified apps only.
Change in Development Paradigm: We consider a smartphone based wearable closed loop blood glucose level control

AP MMA App

App request
Broadcast Receiver

Message parser
(ID, Data)
Select algorithm for ID
Data

ID

Load requirements
for ID from
database

Model Simulator (Data)

Requirements

System parameters
Match (system
parameters,
Requirements)

No

Set off Alarm

Yes
Create new system
call
Send request

Fig. 4. Flowchart of the TDM runtime environment for validating the Apps
operation against requirements.

application to explain the app model. The artificial pancreas
application takes glucose levels from a continuous glucose
monitor (CGM), computes the next infusion level according
to a control algorithm, and sends actuation information to an
infusion pump through the wireless channel.
Figure 3 shows change in app development method with
the app model as compared to the traditional method. Using
the traditional method, to develop Artificial pancreas app,
developer needs to implement GUI, the control algorithm,
CGM sensor code, security algorithms at both smartphone as
well as sensor end. Considering the low power working and
low level implementation language requirements, developing
sensor code can be a challenging task. Developer is also
responsible for writing methods to access and share data with
other apps using content provider and resolver respectively,
receiving broadcast messages, accessing WiFi and interfacing
with cloud, CGM sensor and infusion pump.
On the other hand, to develop artificial pancreas (AP) app
following the proposed app model, developer only develops
the smartphone software to display and execute the control
algorithm. The Health-Dev β tool takes care of developing
sensor code and generating TDM app. TDM interfaces an
app with sensor, other apps and cloud in S3 assured manner.
Thus, the broadcast receiver, WiFi access, sensor interface
required for data sharing and accessing are implemented at
TDM end. Developer needs to implement only one content
resolver through which it can access data from TDM. The AP
app queries TDM to get data from CGM and sends data to the
infusion pump instead of directly communicating with them.
IV. T RUSTWORTHY DATA M ANAGER
The TDM runtime process monitoring and validation functionality is not only cognizant of user preferences but also has
enough embedded intelligence so that it can detect anomalous
behavior of an app as shown in Figure 4. An app can register
with a TDM by first establishing a Binder IPC channel with
the TDM. Through the Binder channel it passes the UUID to

TDM. The app can have its Multi-Tier Model encrypted using
the UUID so that no other app except the TDM can access its
models. TDM has a broadcast listener, which continuously
listens for app requests. Once it receives a request from
MMA, it will invoke a message parser which separates the
requests into two parts: ID and Data. Corresponding to every
ID there is an algorithm for runtime requirement checking.
The algorithm will load requirements for the specific ID from
the database. These requirements can be the safety regulations
of mobile medical apps, the sensor sustainability requirements
and the HIPAA security requirements. Simultaneously a model
simulator will take the data as input and simulate a model
for certain time to generate system parameter. Models may
include Finite State Automata based representation of secure
data management schemes, hybrid automata models of safety
assessment of control algorithms and optimization algorithm
to check sustainable sensor configuration. The TDM will
be equipped with simulators that can execute the models
and estimate the expected operating condition or state of
the app for current phone context (CPC). These simulators
can be reachability analysis on hybrid automata for MMA
safety verification [5], sustainable sensor configuration using
optimization algorithm [23]. If the output of the simulator
matches the requirements, the TDM will create new system
call to issue the initial app request. If the requirement is not
matched, the TDM will set off an alarm.
The runtime validation functionality of the TDM can be
expressed using functions fsaf ety , fsustainability and fsecurity
as shown in Equations 1, 2 and 3.
fsaf ety = < M1 (CP C, Vh ), T h >

(1)

fsustainability = < M2 (CP C, P ), Bmin >

(2)

fsecurity = < M3 (CP C, τM3 ), T rue or F alse >

(3)

Here, M1 , M2 and M3 are models used by TDM to validate
the app request against safety, sustainability and security. Vh
is the physiological parameter controlled by the app and T h
is the safety threshold value. P is the power required for the
sensor for specified operation in the system call whereas Bmin
is the minimum threshold value for the sensor battery level.
Data access request τM3 , is checked against M3 to see if the
request is valid or not for a given CPC.
A more accurate estimation of the actual state of the app
can be obtained by tracing the system calls made by the
app. The app makes different types of system calls such as
Broadcast Provider for radio transmission or Content Provider
for accessing the sensors and actuators. All these system calls
goes through the TDM app in our proposed method. and hence
the TDM can trace the state of the app from these system
calls. These two states can be compared in runtime. If there
is a match then the app behaves as the proposed model and
hence its system call is trustworthy and the TDM allows the
requested operation. If the app operation does not match the
model then a default operation is performed. This default
operation has to be specified by the App and is kind of a
fail safe mode of the application. If this fail safe mode is ever
reached an exception will be raised by the TDM.

AP MMA App
Intent intBroad = new Intent();
intBroad.putExtra(“InfusionPumpInput",
insulinAmount);
sendBroadcast(intBroad); \\ App request
Broadcast Receiver
onReceive(Context arg0, Intent arg1)
Message parser
arg.getExtras();
InfusionPumpInput, insulinAmount \\ (ID, Data)
Select algorithm for
InfusionPumpInput
i.e. safety checking algorithm

Health App Development
App Software

Interfaces

Control
Graphical
+ Algorithm
User Interface

Sensor/
+ Communication
Actuator Specs
Interface

Model (STHA)

Health-Dev ẞ

Data

ID
Load requirements
for ID from
database

Yes
Create new system
call
Send request

Fig. 5. Example flowchart for TDM runtime environment for artificial
pancreas application.

Artificial Pancreas Example TDM: Figure 5 shows the
MMA request processing by TDM for artificial pancreas
app.Artificial Pancreas MMA sends the message of change
in insulin concentration to TDM using Intent message. Broadcast listener implemented in TDM receives this message and
invokes a message parser which separates the requests into two
parts, ID and Data. For artificial pancreas, the ID is infusion
pump input and data is amount of insulin to be infused. As
for this example the request is for infusion pump to change
drug concentration, the algorithm for checking if the requested
insulin amount is safe or not is called. The algorithm loads
requirements for the specific ID from the database i.e. glucose
safe range. Simultaneously a model simulator takes the data as
input and simulate a model for certain time to generate system
parameter. For safety verification hybrid automata model for
artificial pancreas is used. The simulator takes amount of
insulin to be infused as input, simulated hybrid automata for
that input and outputs blood sugar level after certain amount
of time. Now this output matches the requirements i.e. the
sugar level within required range, the TDM creates new system
call to issue the initial app request. If the requirement is not
matched, the TDM will set off an alarm.
V. F RAMEWORK FOR S3 ASSURED MMA D EVELOPMENT
We propose a framework, Health-Dev β, to allow the
developer to implement MMAs following the app model in
Figure 2 and automatically ensuring safety from interactions,
sustainability in sensor and security of data. It also generates
TDM for interfacing with sensors, actuators, and cloud.
We envision that the Health-Dev β tool for trustworthy
development of MMAs should have the architecture as shown
in Figure 6. A developer can use such an architecture to either
verify whether the developed MMA satisfies trustworthiness
requirements or automatically generate critical parts of the
code that can impose S3 vulnerabilities. The developer can
provide a high level design of his app as input to the architecture. The high level specification considered in this paper

Sustainability

Security

STHA

Multi object
Optimization

Encryption
Algorithms

Static Analysis

Hybrid Automata
simulator
(insulinAmount)

Glucose level
Glucose level after t time
- Range 70 to 105 mg/dl
(System parameters )
(Requirements)
Match (system
No
parameters,
Set off Alarm
Requirements)

Safety

S3 Tool Set

S3 enables App Configuration

Code Generator
Developer
Written Code

Safety and Static
Analysis Reports

Sensor Code

Interface Code

Trustworthy Health App Software

Fig. 6. Architecture for development of S3 assured MMAs. The developer
writes the graphical UI and algorithms and also provides models for safety,
security, and sustainability. For all interfaces it sends IPC calls to TDM. The
TDM then uses the S3 assurance toolset to verify requirements.

is Architectural Analysis and Description Language (AADL)
since it is industry standard and is generic enough to describe
computational methods of smartphone apps as well as physiological aspects of human body [24], [25]. The high level design
can be further optimized for sensor sustainability constraints
using the optimizer algorithm. The algorithm gives designs,
which are sustainable under certain time constraints [23]. For
the controller app, control algorithm on smartphone end can
be theoretically verified [5]. In this case, the developer should
specify a hybrid automata representation of the application
design in the Health-Dev β tool input. In our previous work,
we have shown how AADL can be effectively used to represent
hybrid automata [24]. The hybrid automata represented by the
developer can then be extracted from the model and can be
analyzed using the reachability analysis methods to determine
patient safety.
The rest of the design can be used by the automatic code
generator to generate sensor interface and communication code
for the MMA. In our previous work, we have developed
Health-Dev [26], which can convert an AADL design into
sensor and smartphone implementations. In addition to using
the standard software primitives, Health-Dev can be extended
to have security plug-in, the TDM app, sensor design optimizer and app safety verifier which can be added on top of
interfacing code to ensure S3 properties.
The STHA reachability analysis [5], sustainable optimized
sensor design [23] and the security primitives generate a
certification report stating their findings. The code and the
certification report can then be reviewed by an expert personnel in the field to certify the application. If the MMA fails
certification, then the developer can redesign and again use
the same architecture.
A. Sustianable Design for Wearable Sensors
Wearable sensors typically scavenge energy from light or
human body heat. The availability of scavenging sources
is unpredictable. Thus, to have sustainable working of the
sensors, we form an optimization problem to achieve energy
neutrality. A system is energy neutral when the storage device

level remains same before and after the computation i.e.
energy needed for the computation is solely obtained from
a scavenging source.
In this problem formulation, we optimize the data sending
frequency of the sensor depending on the availability of
scavenging source. Let us consider that sending frequency is fc
when scavenging source is available and fd when scavenging
source is unavailable. The sensor battery will be charged only
when the scavenging source is available. Thus, to achieve
energy neutrality, we need to find values of fc and fd such
that the execution should require only the energy obtained
from scavenged source. Further, even the scavenging source
is available, the sensor battery is unable to store energy beyond
the battery capacity. Given these constraints, our aim is to minimize fc and fd . Equation 4 shows the simplified optimization
formulation used to find optimal sensor design [23].
Find fc and fd that optimizes
(Power consumption should not go above scavenged power)
such that
Solar Energy: obtained when the source is available,
Energy Neutrality: battery level = initial battery level,
Storage Constraint: battery level ≥ 0,
Battery Capacity Constraint: Stored Energy ≤ Battery Capacity.

(4)

The optimizer algorithm gives a set of time constrained
sustainable designs. Developer can chose any one of the
designs depending on availability and configuration settings.
B. Safety Verification in Smartphone Controller Design
The safety verification ensures that the side-effects of interactions between human physiology and sensors are within
accepted limit. These interactions are typically governed by
smartphone control algorithm when sensor acts as an actuator.
In such a scenario, developer provides the hybrid automata
which embeds the control algorithm while specifying high
level design. Reachability analysis is then performed on the
specified hybrid automata while considering optimized time
constrained designs as initial set.
One of the safety verification methods is using spatiotemporal hybrid automata (STHA) [5] and related reachability
analysis algorithm. STHA considers effect of interactions over
time as well as over space to give more accurate results
as compared to only time-based analysis. It considers the
interactions of the form,
Ai

∂V
∂2V
= Bi
+ Ci V + ui ,
∂t
∂x2

(5)

This technique can be easily integrated with Health-Dev tool
which can provide evidence for safety of the medical app.
C. Automated Implementation of Security-enabled MMAs
We have developed an automated model-based code generator, Health-Dev [26]. It takes requirements of smartphone
and physiological sensors in the form of models in AADL
and generates downloadable code for them. The automation in
code generation helps to reduce manual implementation errors
in developing wireless health monitoring apps. Health-Dev
also provides graphical user interface to input requirements for
the user who does not have any programming knowledge. It

supports the use of physiological signal processing algorithms
to process health data.
The current Health-Dev input model allows to specify
various components such as sensing, computation and communication. In sensing, type of sensors, motes, sampling frequency, sending frequency can be specified. The computation
component includes processing of data on both sensors and
smartphone which uses various physiological algorithms from
the database such as Fast Fourier Transform (FFT), peak
detection, mean etc. Developer can add new algorithm as
well as modify the inputs. Further, communication protocol
between sensors and smartphone, as well as smartphone and
cloud can be specified. It also includes specification of energy
management techniques such as duty cycle, radio power level.
We have extended the specification of communication component to include security protocol from a security algorithm
database which can also be extended with new protocols.
TDM app generation: The code generator also generates a
TDM app which ensures the secure wireless communication.
It includes the security algorithm specified by the user in the
input. The code generator maintains a code template for generating TDM app. On getting the specification of the security
algorithm, it pulls the algorithm from database and inserts in
the code. It contains encryption-decryption algorithms such as
PEES [27], Advanced Encryption Algorithm (AES) to secure
the physiological data over vulnerable wireless channel. In
TDM code generation, code generator uses the Application
Programming Interfaces (APIs) to allow communication between external wireless mote and an Android smartphone via
Bluetooth. It consists of two components, Bluetooth API and
sensor handler. Bluetooth API ensures connection establishment and data communication between mote and smartphone
while sensor handler acts as a manager and registers all
user assigned sensor, different algorithms associated with each
sensor and handling of data received from the particular sensor.
For each of the wireless communication calls, the generator
appends the security protocol.
VI. E VALUATION OF H EALTH -D EV β T OOL
In this section, we evaluate Health-Dev to determine the
overhead during runtime and the development effort reduction.
A. Overhead analysis of TDM runtime application
To check the overhead of runtime validation in TDM app,
we have implemented the safety and sustainability models
to get their execution time. Runtime security validation uses
FSM which checks for the program flow with no complex
computation involved thus it is assumed to no time. For
safety analysis, pharmacokinetic model [28] for the artificial
pancreas example discussed in Section III is implemented as
Android service. It took 10.894 seconds to check for the
given drug concentration value if the blood glucose level
will remain within safety thresholds. For sustainability, the
objective function in the optimization problem formulation
(Equation 4) is implemented as Android service to check
for the given sensor configuration change if the sensor will
satisfy energy neutrality constraint. The execution time for the
sustainability model was 0.03 seconds.

B. Effort Reduction in Developing MMAs
Generating S3 evidences for MMAs might increase the
burden on development end. Thus to validate that Health-Dev
β tool doesn’t hinder the development process but accelerates
it by automated code generation, we estimated and compared
efforts required with manual implementation. For that, we
used Constructive Cost Model (COCOMO) [6], a cost-effort
estimation tool. As an example, we considered development
of PetPeeves [29] app which requires developing user interface, ECG sensor code and communication interface between
smartphone and sensor. Manual implementation of PetPeeves
app required 2553 lines of code (LOC) without any S3
assured evidences. With Health-Dev β, the communication and
sensor code is automatically generated with TDM app, leaving
developer to write only 1678 LOC including sensor specifications. After executing the model, we got effort required
with Health-Dev β tool as 6.9 developer-month, whereas for
manual implementation it came out as 12.1 developer-month
which is almost 1.8 times more. The developer cost is also
increased by 1.8 times for manual implementation. This shows
that the proposed tool not only saves the development efforts
but guarantees correct working of the software by generating
evidences of S3 requirements.
VII. C ONCLUSIONS
In this paper, we focus on developing evidence-based medical apps for smartphones. The paper proposes an automated
code generator, Health-Dev β for medical apps which generates evidences of safety, sustainability and security of the apps
without hampering the development efforts. The COCOMO
[6] effort estimation tool shows that the time required for
developing MMAs with Health-Dev β is nearly 1.8 times less
than manual implementation.
R EFERENCES
[1] FDA.
Mobile
medical
applications.
http://www.fda.gov/medicaldevices/productsandmedicalprocedures
/connectedhealth/mobilemedicalapplications/default.htm.
[2] S. K. S. Gupta, T. Mukherjee, and K. K. Venkatasubramanian, Body
Area Networks: Safety, Security, and Sustainability. New York, NY,
USA: Cambridge University Press, 2013.
[3] P. L. Dolan. http://exclusive.multibriefs.com/content/health-appcertification-program-halted.
[4] P. Bagade, A. Banerjee, and S. Gupta, “Evidence-based development
approach for safe, sustainable and secure mobile medical app,” in
Wearable Electronics Sensors, ser. Smart Sensors, Measurement and
Instrumentation, S. C. Mukhopadhyay, Ed.
Springer International
Publishing, 2015, vol. 15, pp. 135–174.
[5] A. Banerjee and S. K. S. Gupta, “Spatio-temporal hybrid automata for
safe cyber-physical systems: A medical case study,” in Cyber-Physical
Systems (ICCPS), 2013 ACM/IEEE International Conference on, April
2013, pp. 71–80.
[6] “Cocomo
model,”
http://csse.usc.edu/csse/research/COCOMOII/
cocomo main.html.
[7] X. Chen, A. Waluyo, I. Pek, and W.-S. Yeoh, “Mobile middleware for
wireless body area network,” in Engineering in Medicine and Biology
Society (EMBC), 2010 Annual International Conference of the IEEE, 31
2010-sept. 4 2010, pp. 5504 –5507.
[8] B. Kaufmann and L. Buechley, “Amarino: a toolkit for the rapid
prototyping of mobile ubiquitous computing,” in Proceedings of the 12th
international conference on Human computer interaction with mobile
devices and services. ACM, 2010, pp. 291–298.
[9] I. for Android, “Android development tools,” https://www.sparkfun.com/
products/retired/10748.

[10] T. Laakko, J. Leppänen, J. Lähteenmäki, A. Nummiaho et al., “Mobile
health and wellness application framework,” Methods Inf Med, vol. 47,
no. 3, pp. 217–222, 2008.
[11] T. Das, P. Mohan, V. N. Padmanabhan, R. Ramjee, and A. Sharma,
“Prism: platform for remote sensing using smartphones,” in Proceedings
of the 8th international conference on Mobile systems, applications, and
services. ACM, 2010, pp. 63–76.
[12] W. Brunette, R. Sodt, R. Chaudhri, M. Goel, M. Falcone, J. Van Orden,
and G. Borriello, “Open data kit sensors: a sensor integration framework for android at the application-level,” in Proceedings of the 10th
international conference on Mobile systems, applications, and services.
ACM, 2012, pp. 351–364.
[13] F. X. Lin, A. Rahmati, and L. Zhong, “Dandelion: a framework for transparently programming phone-centered wireless body sensor applications
for health,” in Wireless Health 2010. ACM, 2010, pp. 74–83.
[14] F. X. Lin, Z. Wang, R. LiKamWa, and L. Zhong, “Reflex: using
low-power processors in smartphones without knowing them,” ACM
SIGARCH Computer Architecture News, vol. 40, no. 1, pp. 13–24, 2012.
[15] B. Priyantha, D. Lymberopoulos, and J. Liu, “Enabling energy efficient
continuous sensing on mobile phones with littlerock,” in Proceedings of
the 9th ACM/IEEE International Conference on Information Processing
in Sensor Networks. ACM, 2010, pp. 420–421.
[16] J. Sorber, N. Banerjee, M. D. Corner, and S. Rollins, “Turducken:
hierarchical power management for mobile devices,” in Proceedings of
the 3rd international conference on Mobile systems, applications, and
services. ACM, 2005, pp. 261–274.
[17] J. B. Lim, B. Jang, S. Yoon, M. L. Sichitiu, and A. G. Dean, “Raptex:
Rapid prototyping tool for embedded communication systems,” ACM
Trans. Sen. Netw., vol. 7, pp. 7:1–7:40, August 2010.
[18] E. Cheong, E. A. Lee, and Y. Zhao, “Viptos: a graphical development
and simulation environment for tinyos-based wireless sensor networks,”
in Proceedings of the 3rd international conference on Embedded networked sensor systems, ser. SenSys ’05. New York, NY, USA: ACM,
2005, pp. 302–302.
[19] M. Mozumdar, F. Gregoretti, L. Lavagno, L. Vanzago, and S. Olivieri,
“A framework for modeling, simulation and automatic code generation of sensor network application,” in Sensor, Mesh and Ad Hoc
Communications and Networks, 2008. SECON ’08. 5th Annual IEEE
Communications Society Conference on, june 2008, pp. 515 –522.
[20] B. Kim, L. T. Phan, O. Sokolsky, and I. Lee, “Platform-dependent code
generation for embedded real-time software,” in Compilers, Architecture
and Synthesis for Embedded Systems (CASES), International Conference
on. IEEE, 2013, pp. 1–10.
[21] M. Paschou, E. Sakkopoulos, and A. Tsakalidis, “easyhealthapps: ehealth apps dynamic generation for smartphones & tablets,” Journal of
medical systems, vol. 37, no. 3, pp. 1–12, 2013.
[22] S. Procter and J. Hatcliff, “An architecturally-integrated, systems-based
hazard analysis for medical applications,” in Formal Methods and Models for Codesign (MEMOCODE), 2014 Twelfth ACM/IEEE International
Conference on. IEEE, 2014, pp. 124–133.
[23] P. Bagade, A. Banerjee, and S. K. S. Gupta, “Optimal design for
symbiotic wearable wireless sensors,” in Wearable and Implantable Body
Sensor Networks (BSN), 2014 11th International Conference on. IEEE,
2014, pp. 132–137.
[24] A. Banerjee and S. K. S. Gupta, “Your mobility can be injurious
to your health: Analyzing pervasive health monitoring systems under
dynamic context changes,” in Pervasive Computing and Communications
(PerCom), 2012 IEEE International Conference on. IEEE, 2012, pp.
39–47.
[25] ——, “Analysis of smart mobile applications for healthcare under
dynamic context changes,” Mobile Computing, IEEE Transactions on,
vol. 14, no. 5, pp. 904–919, May 2015.
[26] A. Banerjee, S. Verma, P. Bagade, and S. K. S. Gupta, “Health-dev:
Model based development pervasive health monitoring systems,” in
Wearable and Implantable Body Sensor Networks (BSN), 2012 Nineth
International Conference on, may 2012, pp. 85 –90.
[27] A. Banerjee, S. K. S. Gupta, and K. K. Venkatasubramanian, “Pees:
physiology-based end-to-end security for mhealth.” in Wireless Health.
Citeseer, 2013, p. 2.
[28] D. Wada and D. Ward, “The hybrid model: a new pharmacokinetic model
for computer-controlled infusion pumps,” Biomedical Engineering, IEEE
Transactions on, vol. 41, no. 2, pp. 134 –142, feb. 1994.
[29] J. Milazzo, P. Bagade, A. Banerjee, and S. K. S. Gupta, “bhealthy:
A physiological feedback-based mobile wellness application suite,” in
Proceedings of the conference on Wireless Health. ACM, 2013.

Automated Evaluation of Non-Native English Pronunciation Quality:
Combining Knowledge- and Data-Driven Features at Multiple Time Scales
Matthew P. Black1,2,3 , Daniel Bone1 , Zisis I. Skordilis1 , Rahul Gupta1 , Wei Xia1 ,
Pavlos Papadopoulos1 , Sandeep Nallan Chakravarthula1 , Bo Xiao1 , Maarten Van Segbroeck1 ,
Jangwon Kim1 , Panayiotis G. Georgiou1 , and Shrikanth S. Narayanan1,2,3
1

Signal Analysis & Interpretation Laboratory, Univ. of Southern California, Los Angeles, CA, USA
2
Information Sciences Institute, Univ. of Southern California, Marina del Rey, CA, USA
3
Behavioral Informatix, LLC, Los Angeles, CA, USA
1

http://sail.usc.edu,

2

www.isi.edu,

Abstract
Automatically evaluating pronunciation quality of non-native
speech has seen tremendous success in both research and commercial settings, with applications in L2 learning. In this paper,
submitted for the INTERSPEECH 2015 Degree of Nativeness
Sub-Challenge, this problem is posed under a challenging crosscorpora setting using speech data drawn from multiple speakers
from a variety of language backgrounds (L1) reading different
English sentences. Since the perception of non-nativeness is realized at the segmental and suprasegmental linguistic levels, we
explore a number of acoustic cues at multiple time scales. We
experiment with both data-driven and knowledge-inspired features that capture degree of nativeness from pauses in speech,
speaking rate, rhythm/stress, and goodness of phone pronunciation. One promising finding is that highly accurate automated
assessment can be attained using a small diverse set of intuitive
and interpretable features. Performance is further boosted by
smoothing scores across utterances from the same speaker; our
best system significantly outperforms the challenge baseline.
Index Terms: Behavioral Signal Processing (BSP), computational paralinguistics, Goodness of Pronunciation (GOP),
speech assessment, non-native speech, prosody, challenge

1. Introduction
Speech production is an intricate process involving multiple levels of planning and motor coordination in order to interweave
segmental articulations and encode suprasegmental linguistic
and paralinguistic information. Moreover, individuals differ
in all facets of the speech production pipeline due to a variety of reasons (e.g., physical, environmental), leading to several
sources of variability, including language background.
With the advancement of speech technologies, engineers
have focused on creating assistive tools for language learning:
from automatic literacy tutors [1], to the focus of this challenge, speech nativeness [2, 3]. Computer-Assisted Pronunciation Training (CAPT) is an invaluable resource for secondlanguage (L2) learners that offers flexibility in scheduling at
reduced costs. Languages differ in their phonemic, prosodic,
and grammatical structures; quite often, L2 learners will retain
certain speech attributes of their native language, leading to perceived abnormality, or “non-nativeness,” by L1 listeners. Our
approach to this challenge is grounded in the automatic creation
of informative pronunciation and prosodic features.
Prosody is at the core of effective human-human communication. It serves grammatical functions, such as segmenting
utterances into phrases, pragmatic functions like differentiating
statements from questions, and communicates attitude and af-

3

www.behavioralinformatix.com

fect. Since languages themselves have different rhythms and
intonations, non-native speakers often use those prosodic characteristics of their first language, particularly when learning a
stress-timed L2 given a syllable-timed L1, or vice-versa. Several recent studies have investigated L2 pronunciation through
prosodic features. Levow developed a pitch accent recognition
algorithm for labeling non-native speech which uses local and
co-articulatory context [4]. Hansen and Arslan also proposed
using source-generator based prosodic features to classify foreign accents of American English [5]; they showed that energy,
duration, and spectral related features could play an important
role in accent detection. Much research has been done on automatically separating native vs. non-native speakers [3, 6–8].
Phonemic identity and pronunciation quality are also important cues for automatically scoring degree of nativeness.
Phoneme confusion between languages makes both comprehension and production difficult for L2 learners. For instance,
German speakers of English cannot pronounce /z/ as clearly
since there is no /z/ sound in the German language, while for
Japanese, the lack of /r/ usually causes speakers to pronounce /r/
as /l/. Witt and Young presented a Goodness of Pronunciation
(GOP) measure to quantify phonetic pronunciation quality [9].
They also improved their model by including the expected pronunciation errors in the recognition network, as did Black et al.
in the context of children’s literacy assessment [10].
Our approach in this work is centered on combining
knowledge-based and data-driven feature sets extracted at multiple time scales (segmental, suprasegmental). Much of these
knowledge-inspired features are prosodic (suprasegmental) and
pronunciation/articulatory (segmental) cues that were motivated
and discussed in this section. Data-driven features have the advantage of fewer dependencies, with the idea being to discover
trends through supervised learning methods; the downside is
that they are naı̈ve and suffer from high dimensionality. Conversely, knowledge-inspired features rely on other information
that may be unreliable or noisy, but they are more intuitive and
interpretable and have a lower dimensionality. Finally, we also
consider unsupervised speaker clustering and smoothing methods, under the assumption that speakers will be perceived with
consistent levels of nativeness across utterances.

2. Corpora & Baseline System
Four different corpora were analyzed: two make up the train
set, and the other two are the development (“dev”) and test sets.
Each corpus is comprised of multiple non-native speakers of
English from a variety of language backgrounds (German, Italian, Chinese, Japanese, French, Spanish, Hindi, other), although

Train (N=3890, m=3.2, s=2.1)
Dev (N=999, m=5.7, s=3.5)
Test (N=594, m=6.8, s=2.9)

1

2

3

4

5

6

7

Time (s)

8

9

10

11

12

Figure 1: Interleaved train/dev/test set utterance durations.
language ID was not provided. Each speaker read multiple sentences (disjoint across corpora). Fig. 1 shows that the distribution of utterance durations varies across corpora. The degree
of nativeness (DN ) was labeled for each utterance, with higher
scores representing higher degrees of non-nativeness. As shown
in Fig. 2, the train and dev sets were rated on different scales;
this was the primary reason for Spearman’s correlation as the
performance metric. DN scores and speaker IDs for the test set
were not provided. Word-level transcriptions were available for
each utterance in all data sets and included enriched markers for
expected positions of major (B3) and minor (B2) phrase boundaries. The challenge organizers have requested that readers refer
to the challenge paper for more details [11].
The DN Challenge baseline system consists of a purely
data-driven approach: training a linear support vector regression
(SVR) model on 6373 utterance-level static features by computing functionals of low-level descriptors (LLDs) that include
prosodic (e.g., f0 , energy), voice quality (e.g., jitter, shimmer),
and spectral (e.g., MFCCs, RASTA coefficients) cues. Please
see [11, 12] for more details. We will compare the performance
of this baseline system to our proposed methodology in Sec. 6.

3. Data Pre-processing
3.1. Forced Alignment & Voice Activity Detection
We exploited the available transcriptions by performing speechtext (“forced”) alignment using freely available HTK [13]
acoustic models (AMs) trained on out-of-domain native English
speech [14]; we will leverage this “mismatch” in native vs. nonnative speakers in Sec. 4.2. We used the CMU dictionary [15],
with its multiple acceptable phonetic pronunciations for each
word, and appended entries for any out-of-vocabulary words.
From the output of the forced alignment, we extracted word,
syllable, and phone boundaries (and the corresponding acoustic log-likelihoods). By using a grammar that allowed for the
optional detection of inter-word pauses, this alignment process
also acted as an accurate voice activity detector (VAD).
3.2. Speaker Clustering
As illustrated in Fig. 2, DN scores are correlated across utterances from the same speaker. Therefore, the identity of the
speaker for each utterance can be utilized to smooth and improve DN score predictions. However, as described in Sec. 2,
the speaker ID is unknown for the test utterances, so an unsupervised speaker clustering approach is needed. We used a bottomup agglomerative hierarchical clustering (AHC) method [16]
with k-means post-refinement [17]. Each cluster is modeled
by a single Gaussian, and the generalized likelihood ratio [18]
is used as the cluster distance metric [17]. Using the VAD
(Sec. 3.1), we removed all periods of silence from the utterances before clustering. Since the number of speakers in the
test set was known a priori (54, with 11 sentences per speaker),
we stopped clustering once 54 distinct clusters were created.

Degree of Nativeness

Normalized Histogram

0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0

3.5
3
2.5
2
1.5
1
0.5
0
0

Train: mean +/− S.D.

10

20

Dev: mean +/− S.D.

30

40

50

Speaker (Ordered by Mean)

60

Figure 2: Nativeness scores for speakers in the train/dev sets.

4. Feature Extraction
4.1. Data-Driven Features
Our proposed data-driven features begin with the baseline LLDs
extracted every 10ms with openSMILE [19]. Non-speech
frames according to VAD (Sec. 3.1) are removed. In addition to computing utterance-level functionals as in the baseline
(Sec. 2), we also computed “functionals-of-functionals,” as proposed in [20] and used successfully in [21, 22]. LLD contours
are first split into short disjoint windows of equal length L.
Functionals are then computed within each window, and finally,
functionals of these functionals are computed across all windows in the utterance. This process is meant to better capture
moment-to-moment changes that occur at shorter time scales.
In this work, we chose two different window lengths, L ∈{0.1s,
0.5s}, based on the fact that more than 95% of the utterances
in the corpora are more than one second in duration (Fig. 1),
which means the vast majority of utterances have a sufficient
number of windows to compute reliable statistics. Because of
the compounding nature of these calculations, there are 11323
data-driven functionals-of-functionals features in total.
4.2. Knowledge-Inspired Features
Prosody is the rhythm, stress, and intonation of speech. As
such, we computed features that targeted these qualitative constructs. Immediately from the forced alignment (Sec. 3.1), we
extracted important prosodic cues that captured pausing behaviors/strategies and speaking rate. Since the transcriptions had
markers indicating phrase boundaries (Sec. 2), we first categorized each inter-word pause as one of the following: expected
major (B3), expected minor (B2), or unexpected. Utterancelevel pausing features were calculated for each combination of
the 3 pausing categories in two ways: 1) percentage of the utterance duration due to pauses, and 2) mean duration of pauses.
We extracted two types of speaking rate features: rates
(token / time) and durations (time / token). The former was calculated by creating a vector of rates (1/duration) for each token
in the utterance and computing functionals (e.g., mean, S.D.)
across it. We chose 3 different tokens at varying time scales:
word, syllable, and phone. Similarly, the duration features were
calculated by computing functionals of durations for different
linguistic classes: syllables, phones, vowels, and consonants.
The next set of knowledge-driven features are related to lexical stress and speech rhythm. We compared the stress of each
aligned phone to the stress labels in the CMU dictionary [15]
that are provided for each vowel to signal data. For each aligned
phone, we compute the mean f0 , energy, and intensity and then
quantize these values into three quantiles for each utterance.
Since the stress labels are also 3-valued, we define a distance
measure based on exact-matching. For each signal (3), we obtain a measure that is the summation of mis-matches between
the labeled and signal-derived stress.
We also quantify speech rhythm using pairwise variability
indices (PVI, [23]) and global interval proportions (GIP, [24]).
PVIs measure local changes in duration. We compute six PVI

Type

Nsig /N

Pausing
7/8
Rate
18/47
Rhythm 5/14
Template 6/13
GOP
3/6

Best Feature: Spearman’s Correlation
Description of Best Feature

Train

Fraction unexpected pauses
Mean rate: phones/sec
Consonant PVI
Phone duration mean |diff|
Utterance: phone+sil loop

0.39 0.59
-0.43 -0.56
0.25 0.37
0.37 0.49
-0.48 -0.55

Dev

Table 1: Number of significantly correlated knowledge-inspired
features (p < 0.05), along with the best performing feature.
measures: normalized and unnormalized measures on consonant, vowel, and syllable durations. GIPs compute gross statistics on segmental durations; in particular, we included the percentage of vowel speech and S.D. of vowel and consonant durations within an utterance. Similar speech rhythm measures were
previously considered for intoxicated speech detection [25].
Next, we implicitly model the stress and intonation of
speech through template-based exemplar features, initially proposed for modeling children’s prosody vs. an adult exemplar [26] and also successfully used in [27]. First, a single
prosodic functional for each token (phone or word) is calculated per feature: duration, median f0 , and median intensity.
This is a time-aligned feature representation which we can compare with other readings. Template-based features are advantageous in that all computations are performed on the continuousscale signal contours. However, since we do not have an exemplar production for each sentence, we must infer one from
the other speakers. We take the mean feature contour from all
other productions of the same sentence, assuming that this average will provide a suitable exemplar. In reference to this exemplar, we then computed 3 measures (Pearson’s correlation,
mean absolute difference, S.D.) for each of the following 4 contours: phone duration/pitch/intensity and word duration. Only
sentences repeated at least 5 times were considered; otherwise,
imputation with the mean value was used.
The last proposed knowledge-inspired feature is Goodness
of Pronunciation (GOP) scoring, first introduced in [9] to detect
phone-level pronunciation errors but also applied to L2 learning [28] and children’s literacy assessment [10, 29]. The main
idea behind GOP scoring is leveraging acoustic models (AMs)
trained on native speakers to quantify pronunciation quality:


P (O|Transcript)
1
GOP =
log
(1)
N
P (O|AM loop)
, where N is the number of frames and O are the acoustic features (MFCCs in this case). The numerator is the likelihood of
the acoustics, given the transcription and the native AMs, which
is equivalent to forced alignment (Sec. 3.1). The denominator is
estimated via automatic speech recognition with the same set of
AMs and an “AM loop” grammar. Higher GOP scores indicate
higher pronunciation quality, whereas lower GOP scores suggest the speech is a poor match to the native AMs. In this work,
we experimented with 3 different AM loops (phones+silence,
phones only, silence only) and computed GOP scores over different temporal regions: full utterance (including inter-word
pauses), speech regions only, vowels only (based on [30]).
Table 1 shows that almost half of the knowledge-based features are significantly correlated with the DN scores in the train
and dev sets (p < 0.05). We restrict our analysis to the best
performing feature for each proposed type due to space constraints. As shown in Table 1, speakers sound more non-native
when they: pause more unexpectedly; speak at a slower average
rate; have higher local variability in consonant durations; differ

Figure 3: Automatically evaluating degree of nativeness using
support vector regression (SVR), trained on data-driven and
knowledge-inspired features, with speaker-level smoothing.
more than other productions of the same sentence; and have
lower GOP scores. All of these findings agree with intuition.

5. Predicting Degree of Nativeness
We experimented with several supervised machine learning
techniques to map the various features to the DN scores, including regression and ranking algorithms [31], ensemble methods
(e.g., bagging, boosting [32], stacking [33]), and early vs. late
fusion. Due to space constraints, we only describe our best performing system, shown in Fig. 3. For parameter-tuning purposes and to prevent overfitting, the train set was split into 6 approximately equal-sized speaker-disjoint cross-validation folds.
5.1. Support Vector Regression & Fusion
For the data-driven functionals-of-functionals (“FoF”), since dimensionality is so high, we first employed unsupervised feature
reduction. A feature was dropped unless the following two criteria were both met for all cross-validation folds and datasets:
1) the feature must have a coefficient of variation ≥ 0.01 (to
eliminate irrelevant features), and 2) the feature must not have a
Pearson’s or Spearman’s correlation ≥ 0.98 with any other feature (to eliminate redundant features). When two features were
highly correlated, we dropped the one that was less normally
distributed, based on the Kolmogorov-Smirnov test. This process reduced the dimensionality by 34% (from 11323 to 7478).
The feature matrix was then normalized/scaled four different ways: Z-normalization, i.e., subtracting the mean and dividing by the S.D., 1) across all data; 2) for each fold and dataset
separately; and scaling the feature matrix to be bounded between 0 and 1, i.e., subtracting the minimum and dividing by
the range, 3) across all data; 4) for each fold and dataset separately. Finally, we trained l1 -loss and l2 -loss l2 -regularized
linear support vector regression (SVR) models, as implemented
in LIBLINEAR [34], with the loss functions defined as:
!
 l
 
X


T
T
min w w+C
max 0,DNu −w xu − , l ∈ {1,2} (2)
w

u

, where xu is the feature vector for utterance u, w is the linear
weight vector, l determines the type of loss function (l1 or l2 ), 
is the loss sensitivity parameter, and C is the regularization parameter. A grid search was used to tune  and C. The “optimal”
parameter values, normalization technique, and SVR loss type,
i.e., the combination that attained the highest mean Spearman’s
correlation across the 6 train folds, was then applied to the dev
set after retraining the SVR model on the full train set.
Feature-level (“early”) fusion of the proposed knowledgeinspired features was possible because of their low dimensionalities. Fusion between the data-driven and knowledge-inspired
features was attained by treating the output prediction scores
of the data-driven FoF SVR model as one additional “feature”
(Fig. 3). After removing irrelevant/redundant features and normalizing/scaling the feature matrix as before, we trained similar
l1 -loss and l2 -loss linear SVR models on these fused features.

Category

Feature

Train

N

Dev

Baseline [11] Utterance

6373

mean S.D.
0.40
—

Time Scale
Time Scale
Time Scale

530
3100
3848

0.37
0.48
0.47

0.12
0.08
0.06

0.29
0.44
0.38

All Proposed All Proposed 7478

0.48

0.06

0.45

Utterance
FoF (0.1s)
FoF (0.5s)

Feature/Classifier

0.42

Baseline [11]

6373

Func-of-Func (FoF)

7478

Table 2: Dimensionality and performance (Spearman’s correlation) of the support vector regression model for various subsets
of data-driven features on the train set (6 folds) and dev set.
5.2. Speaker-level Smoothing
As motivated in Sec. 3.2 and shown in Fig. 2, DN scores
are correlated across a speaker’s utterances, suggesting that directly modeling this dependency may improve prediction performance. We experimented with the following smoothing techdus is the predicted DN score of utterance
nique in (3), where DN
u from speaker s, and wus is the number of syllables in u:
X s
gus = (1−α)DN
dus + Pα
dus , 0 ≤ α ≤ 1 (3)
DN
wu DN
wus u∈s
u∈s

s

gu is a smoothed linear combination of the utTherefore, DN
dus with the speaker’s weighted avterance’s predicted score DN
erage score (with longer utterances containing more syllables
carrying more weight when computing this average). The decision to use this weighted average was made since there is more
acoustic evidence in longer utterances, giving human raters and
computational methods alike more of a chance to make a reliable decision. In (3), tuning parameter α determines the level of
smoothing; α = 0 means no smoothing, i.e., the utterance’s predicted score is applied, while α = 1 means the speaker’s average
score is applied to all of the speaker’s utterances.

6. Results & Discussion
In this section, we analyze the performance of our various datadriven and knowledge-inspired sub-systems. First, we examine
performance of SVR using data-driven functional-of-functional
(FoF) features on the train and development sets (Table 2). Our
reduced set of utterance-level functionals (i.e., 530 vs. 6373)
show a drop in performance relative to the baseline. However,
through modeling these features at various time scales, FoF features exceed baseline performance on the train and dev sets; for
both, FoF at the 0.1s time-scale is the top performing individual
feature set. We note that as the number of features increases,
the S.D. of model performance across training folds decreases,
an indication of a robust linear-kernel SVR model. Fusion of all
proposed FoF features exceeds baseline performance, showing
the benefits of the FoF modeling framework.
Knowledge-inspired feature performance is shown in Table 3, including score-level fusion with data-driven FoF fea-

Spearman Correlation

Tr

0.75
0.73
0.71
0.69
0.67
0.65
0.63
0.61
0.59
0.57

0

Tr: Smooth

0.1

0.2

Dev

0.3

α

Dev: Smooth

0.4

0.5

sel

0.6

α (Smoothing Parameter)

0.7

N

Test: Smooth

0.8

0.9

Figure 4: Improving utterance-level score prediction by smoothing with the speaker’s average score across all utterances.

Train
mean
0.403

S.D.
—

Dev

Test

0.415 0.425

0.483 0.065

0.454

—

Pausing
Speaking Rate
Rhythm
Template
GOP

8
47
14
13
6

0.405
0.552
0.262
0.370
0.430

0.107
0.047
0.048
0.104
0.066

0.613
0.627
0.411
0.455
0.444

—
—
—
—
—

LOO: Func-of-Func

88

0.585 0.055

0.690

—

0.700
0.701
0.703
0.688
0.699

—
—
—
—
—

LOO: Pausing
LOO: Speaking Rate
LOO: Rhythm
LOO: Template
LOO: GOP

80+1
41+1
74+1
75+1
82+1

0.603
0.584
0.603
0.604
0.596

All Proposed

88+1

0.605 0.050

0.707 0.710

Spkr Smooth; α=0.44 88+1

0.653 0.061

0.744 0.745

0.051
0.047
0.048
0.052
0.053

Table 3: Dimensionality and performance (Spearman’s correlation) for each of the proposed features on the train set (6 folds)
and dev/test sets. “+1” represents score-level fusion with the
Func-of-Func (FoF) classifier. “LOO” means Leave-One-Out.
tures. Speaking rate features perform better than any other
feature group on both train and dev, followed closely by pausing features; this reflects the critical importance of timing to
perceived nativeness. GOP features alone (6 features) exceed
baseline performance, indicating that segmental cues can identify non-native speech. Rhythm and template features also have
significant performance, albeit below the other feature groups.
Fusion of all knowledge-inspired features leads to performances
well above the baseline (listed as LOO: Func-of-Func). Leaveone-out experiments do not suggest that any feature group
greatly degrades performance; on the contrary, fusion of all proposed features achieves peak performance of 0.605, 0.707, and
0.710 on the train, dev, and test sets, respectively.
Lastly, we perform speaker-level smoothing of our best
SVR model, utilizing the assumption that non-native speakers
are consistently non-native. Optimization is shown in Fig. 4,
where optimal performance is found for α = 0.44. Smoothing
provides an improvement to 0.744 on the dev set; test set performance with this system is consistent at 0.745, significantly
outperforming the baseline correlation of 0.425 (p < 0.0001).

7. Conclusions & Future Work
In this paper, we showed that a combination of low-dimensional
knowledge-inspired segmental and suprasegmental features
(pausing, speaking rate, stress/rhythm, and Goodness of Pronunciation) were able to predict subjective degree of nativeness
ratings. In combination with data-driven features extracted at
multiple time scales and speaker-level smoothing, we achieved
accurate evaluation of pronunciation quality.
Future work will include experimentation with additional
fusion techniques, as well as novel ways to incorporate a
speaker’s L1 (e.g., exploiting specific non-native trends that are
more prone to occur). Given that English is a stress-timed language, it may be beneficial to utilize metrics that measure variability between stressed syllables, rather than all adjacent syllables as we have currently done (i.e., isochrony features [35]).

8. Acknowledgements
This research was supported in part by NSF and NIH. Special
thanks to the INTERSPEECH 2015 Challenge organizers.

9. References
[1] A. Hagen, B. Pellom, and R. Cole, “Children’s speech recognition
with application to interactive books and tutors,” in IEEE Workshop on Automatic Speech Recognition and Understanding, 2003,
2003, pp. 186–191.
[2] F. Hönig, A. Batliner, and E. Nöth, “Automatic assessment of nonnative prosody – annotation, modelling and evaluation,” in International Symposium on Automatic Detection of Errors in Pronunciation Training (IS ADEPT), 2012, pp. 21–30.
[3] F. Hönig, A. Batliner, K. Weilhammer, and E. Nöth, “Automatic assessment of non-native prosody for English as L2,” Proc.
Speech Prosody, Chicago, 2010.
[4] G.-A. Levow, “Investigating pitch accent recognition in nonnative speech,” in Proceedings of the ACL-IJCNLP Conference
Short Papers, 2009, pp. 269–272.
[5] J. H. L. Hansen and L. M. Arslan, “Foreign accent classification
using source generator based prosodic features,” in IEEE ICASSP,
1995, pp. 836–839.
[6] M. Piat, D. Fohr, and I. Illina, “Foreign accent identification based
on prosodic parameters,” in INTERSPEECH, 2008.
[7] F. Hnig, A. Batliner, K. Weilhammer, and E. Nth, “Islands of failure: Employing word accent information for pronunciation quality assessment of English L2 learners,” in Proceedings of SLATE,
Wroxall Abbey, 2009.
[8] J. Lopes, I. Trancoso, and A. Abad, “A nativeness classifier for
TED talks,” in IEEE ICASSP, 2011, pp. 5672–5675.

[20] B. Schuller, M. Wimmer, L. Mösenlechner, C. Kern, D. Arsic, and
G. Rigoll, “Brute-forcing hierarchical functionals for paralinguistics: A waste of feature space?” in IEEE ICASSP, Las Vegas, NV,
USA, 2008, pp. 4501–4504.
[21] M. P. Black, A. Katsamanis, B. Baucom, C.-C. Lee, A. Lammert,
A. Christensen, P. G. Georgiou, and S. S. Narayanan, “Toward automating a human behavioral coding system for married couples’
interactions using speech acoustic features ” Speech Communication, vol. 55, no. 1, pp. 1–21, 2013.
[22] D. Bone, M. Li, M. P. Black, and S. S. Narayanan, “Intoxicated speech detection: A fusion framework with speakernormalized hierarchical functionals and GMM supervectors ”
Computer Speech & Language, vol. 28, no. 2, pp. 375–391, 2014.
[23] E. Grabe and E. L. Low, “Durational variability in speech and the
rhythm class hypothesis,” Papers in laboratory phonology, vol. 7,
pp. 515–546, 2002.
[24] F. Ramus, “Acoustic correlates of linguistic rhythm: Perspectives,” Proceedings of Speech Prosody, 2002.
[25] F. Hönig, A. Batliner, and E. Nöth, “Does it groove or does it
stumble-automatic classification of alcoholic intoxication using
prosodic features.” in INTERSPEECH, 2011, pp. 3225–3228.
[26] M. Duong, J. Mostow, and S. Sitaram, “Two methods for assessing oral reading prosody,” ACM Transactions on Speech and Language Processing (TSLP), vol. 7, no. 4, pp. 14:1–14:22, 2011.

[9] S. M. Witt and S. J. Young, “Phone-level pronunciation scoring
and assessment for interactive language learning,” Speech Communication, vol. 30, no. 2, pp. 95–108, 2000.

[27] D. Bone, T. Chaspari, K. Audhkhasi, J. Gibson, A. Tsiartas,
M. Van Segbroeck, M. Li, S. Lee, and S. S. Narayanan, “Classifying language-related developmental disorders from speech cues:
the promise and the potential confounds.” in INTERSPEECH,
2013, pp. 182–186.

[10] M. P. Black, J. Tepperman, and S. S. Narayanan, “Automatic prediction of children’s reading ability for high-level literacy assessment,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 1015–1028, 2011.

[28] A. Neri, C. Cucchiarini, and H. Strik, “The effectiveness of
computer-based speech corrective feedback for improving segmental quality in L2 Dutch,” ReCALL, vol. 20, no. 2, pp. 225–243,
May 2008.

[11] B. Schuller, S. Steidl, A. Batliner, S. Hantke, F. Hönig, J. R.
Orozco-Arroyave, E. Nöth, Y. Zhang, and F. Weninger, “The
INTERSPEECH 2015 Computational Paralinguistics Challenge:
Nativeness, Parkinson’s & Eating Condition,” in INTERSPEECH,
2015.

[29] J. Tepperman, M. P. Black, P. Price, S. Lee, A. Kazemzadeh,
M. Gerosa, M. Heritage, A. Alwan, and S. S. Narayanan, “A
bayesian network classifier for word-level reading assessment,” in
INTERSPEECH, Antwerp, Belgium, Aug. 2007, pp. 2185–2188.

[12] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer,
F. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi,
H. Salamin, A. Polychroniou, F. Valente, and S. Kim, “The INTERSPEECH 2013 Computational Paralinguistics Challenge: social signals, conflict, emotion, autism,” in INTERSPEECH, 2013.

[30] L. Chen, K. Evanini, and X. Sun, “Assessment of non-native
speech using vowel space characteristics,” in Spoken Language
Technology Workshop, 2010, pp. 139–144.
[31] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer, “An efficient
boosting algorithm for combining preferences,” The Journal of
machine learning research, vol. 4, pp. 933–969, 2003.

[13] S. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Kershaw,
X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and
P. Woodland, “The HTK book (for HTK version 3.4),” Cambridge
University Engineering Department, Tech. Rep., Dec. 2006.

[32] Y. Freund, R. Schapire, and N. Abe, “A short introduction to
boosting,” Journal of Japanese Society for Artificial Intelligence,
vol. 14, no. 5, pp. 771–780, 1999.

[14] K. Vertanen, “Baseline WSJ acoustic models for HTK
and Sphinx: Training recipes and recognition experiments,”
Cavendish Laboratory, Tech. Rep., 2006.

[33] S. Džeroski and B. Ženko, “Is combining classifiers with stacking
better than selecting the best one?” Machine learning, vol. 54,
no. 3, pp. 255–273, 2004.

[15] R. L. Weide, “CMU pronouncing dictionary,” Carnegie Mellon
University, 1994. [Online]. Available: http://www.speech.cs.cmu.
edu/cgi-bin/cmudict/

[34] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin, “LIBLINEAR:
A library for large linear classification,” The Journal of Machine
Learning Research, vol. 9, pp. 1871–1874, 2008.

[16] W. Wang, P. Lv, and Y. H. Yan, “An improved hierarchical speaker
clustering,” Acta Acustica, vol. 33, no. 1, 2008.

[35] I. Lehiste, “Isochrony reconsidered.” Journal of Phonetics, vol. 5,
no. 3, pp. 253–263, 1977.

[17] K. J. Han, S. Kim, and S. S. Narayanan, “Strategies to improve
the robustness of agglomerative hierarchical clustering under data
source variation for speaker diarization,” IEEE Transactions on
Audio, Speech, and Language Processing, vol. 16, no. 8, pp.
1590–1601, Nov 2008.
[18] A. S. Willsky and H. L. Jones, “A generalized likelihood ratio
approach to the detection and estimation of jumps in linear systems,” IEEE Transactions on Automatic Control, vol. 21, no. 1,
pp. 108–112, 1976.
[19] F. Eyben, M. Wöllmer, and B. Schuller, “OpenSMILE - The Munich versatile and fast open-source audio feature extractor,” in
ACM Multimedia, Firenze, Italy, 2010, pp. 1459–1462.

SafeDrive: An Autonomous Driver Safety
Application in Aware Cities
Koosha Sadeghi∗ , Ayan Banerjee† , Javad Sohankar‡ , and Sandeep K.S. Gupta§
iMPACT Lab, CIDSE, Arizona State University
Tempe, Arizona, USA
Email: {∗ ssadegh4, † abanerj3, ‡ j.sohankar, § sandeep.gupta}@asu.edu

Abstract—Commercially available wearables and apps that
convert mobile devices into data collection hubs can be used
to implement smart applications in aware cities. In this paper,
we consider wearable devices on various human users as a networked cluster of computing power and information source in an
Internet-of-People architecture. Applications can be developed to
perform computation on this data and gain group level aggregate
inferences and provide feedback. We propose “SafeDrive”, an
autonomous transportation application, that estimates mental fatigue of a driver using brain sensors, predicts collision probability
by fusing car parameters with driver mental state, and issues
feedback just in time to avoid accidents. However, significant
challenges exist with respect to ensuring safety, accurate context
computation and real-time operation, and sustainability, resource
efficiency. In this regard, we present the “HumaNet” framework
that consists of a middleware installed in mobile devices for
developing aware cities applications. HumaNet applies modelbased requirements checking approach for off-line analysis and
optimization of a given application. Further, it applies contextbased requirements checking to decide how to use different
types of computation resources to satisfy safety and sustainability
requirements in run-time.

I. I NTRODUCTION
Advances in the Internet of Things (IoT) domain is making
city infrastructure smarter, by incorporating context awareness in different domains including transportation, health,
education, and energy management. Context awareness in
aware cities is obtained through collection of data from the
environment including its inhabitants using sensors, inferring
knowledge from the data, and using that knowledge to provide
automated services [1], [2]. To this effect, deployment of
Internet enabled wearable devices on human body such as the
Apple watch, the Google glass, and smartphone has converted
the human user to a human server, a rich repository of environmental, and physiological data [3]–[5]. The IoT concept has
morphed into the Internet-of-People (IoP) that enables sharing
of the knowledge through community networking of human
servers for collaborative applications such as traffic condition
reporting or crowd driven emergency response [6].
In this paper, we propose SafeDrive, a context aware transportation safety application, that uses community monitoring
to determine mental fatigue levels of drivers and provides
them feedback on impending collisions. SafeDrive assumes
that each driver on the road is equipped with a brain sensor
that connects to their smartphone. The smartphone is also a
iMPACT Lab URL: https://impact.asu.edu/

hub for other car sensors such as the wheel speed, the rear
view, or blind spot cameras. The smartphone sends the mental
fatigue level along with car speed and neighboring car details
to a central server. The server computes impending collision
based on the mental fatigue levels of other drivers on the
road as well as their current driving status and alerts individuals using alarms or warning messages. SafeDrive makes
city transportation context aware by combining knowledge
from different domains such as mental states of drivers and
mechanical states of a vehicle, transferring them to different
computational and decision making hubs in the IoP infrastructure, and processing them in a collaborative environment to
derive community based inferences, which influence decision
making. For SafeDrive to be useful, it needs to satisfy three
main requirements: a) the mental state detection system has to
be accurate, b) the feedback on collision has to be real-time,
and c) it needs to be energy efficient on smartphone.
Previous works on driver drowsiness detection systems are
based on information from an individual (i.e. driver). In [7],
facial expression and photoplethysmograph (PPG) data are
combined to measure driver alertness. They reach 83% and
94% true detection rates using two features (i.e. percentage
of eye closure and PPG power spectrum density) and two
additional features (i.e. average eye closer speed and heart rate
variability), respectively. In [8], set of medical grade sensors
are used to record electroencephalogram (EEG), electrooculogram (EOG), and electrocardiogram (ECG) signals to detect
drowsiness with 95-97% accuracy. In [9], skin conductance
and oximetry pulse are monitored to detect mental fatigue
and they reach around 93% accuracy using neural network
approach. In [10], a four channel wireless EEG sensor is used
to detect drowsiness and an accuracy of 74.6% is obtained. At
last, in [11], they use Neurosky [12] EEG sensor and support
vector machine algorithm for drowsiness detection and reach
88.8% accuracy in the best case. These works suffer from at
least one of these four limitations: 1) usage of medical grade
sensors with tedious setup procedure (reducing usability), 2)
combination of several types of sensors and features (reducing
usability), 3) computationally intensive processing (hampering
real-time operation and energy efficiency), and 4) low performance (potentially undermining safety). These shortcomings
reduce usage feasibility in on-line mobile settings.
In SafeDrive, information from all drivers on a route is
collected and processed to improve the performance. Applying

Context
Information

Transportation
Control System
(Cloud Server)

Decisions

Mental
State

Warning
Message

Car
Conditions

Fig. 1: Operation of SafeDrive application using HumaNet.

IoP approach in developing SafeDrive makes it feasible to: 1)
exploit easy-to-use and commercially available sensors such
as Neurosky or Emotiv [13] (limited number of channels
and low signal-to-noise ratio of these headsets are challenges
toward reaching high accuracy), 2) use just one type of signal
features (i.e. variations in the frequency domain), 3) deploy
a light weight mental fatigue detection algorithm (based on a
Markov chain model), and finally, 4) obtain an average of 91%
accuracy to detect mental fatigue (in five trials of one subject).
These points make SafeDrive usable as well as accurate. It
is noteworthy that evaluation on a larger pool of subjects is
required for real-world usages.
To enable safety and sustainability assured IoP based implementation of SafeDrive, we propose HumaNet. It is a framework that applies model-based [14] and context-based [15]
requirements checking to ensure safe and sustainable operation
of aware cities apps such as SafeDrive. In model-based checking, HumaNet runs models of the app to determine its specifications (e.g. response time and power consumption) off-line.
Model-based checking also indicates optimized configuration
of the app to be executed with high accuracy, real-time, and
energy efficient way. Off-line monitoring may not be sufficient
by its own to guarantee safety and sustainability for the
running app. So, context-based checking monitors execution
of the app on run-time and according to the context decides
whether to offload the computations on the central server to
satisfy requirements. To implement SafeDrive app based on
HumaNet, an application specific middleware is downloaded
into each participating smartphone. In model-based checking,
temporal logic is used to optimize the execution of SafeDrive.
Afterward, in context-based checking, SafeDrive is monitored
in run-time to check if safety and sustainability requirements
are met. Otherwise, the computation is offloaded on a central
cloud server.

type is driver’s mental state (i.e. alert or mentally fatigue)
and the second type includes car conditions (i.e. its speed and
location). The control system server processes the collected
information, and based on the analysis results, an appropriate
warning feedback will be sent to driver. In our system, for
triggering an alarm tone, we consider two scenarios, 1) an
individual mental fatigue level is lower than a threshold, and
2) a fatigue driver is nearby or probable collision is predicted.
In this sense, as seen in Figure 2, a wearable device on a
human user has to provide three interfaces: a) an interface to
connect to available sensors and create databases for each of
them, b) a platform to execute application executables, and c) a
webserver that can be accessed through the Internet and hosts
the raw data or the output of the applications executed in the
platform. SafeDrive app executables (Figure 2) consist of two
parts: 1) client side that executes a mental fatigue detection
algorithm (for the first scenario), and 2) server side that
receives mental states from client side, runs collision detection
algorithm, and sends back feedback messages to client side
(for the second scenario). On the client side, the EEG sensors
capture signals and send them to the smartphone. The collected
data is processed and the alertness of the driver is determined
according to the preset threshold. Recognition of fatigue state
will trigger an alarm tone. Furthermore, her mental state is
sent to the central control system (i.e. server side app) along
with her car conditions. In the server side, the transportation
control system sends a warning message to a driver in case
of the presence of a fatigue driver nearby or prediction of a
probable collision in her vicinity. In sections IV-A1 and IV-B,
we describe the details of measuring mental fatigue level
and detecting probable collision, respectively. To measure the
mental fatigue level of a driver, we need to train our algorithm
to determine a threshold before using the app during driving

SafeDrive Server

Central
Cloud
Server

Database

Data Aggregator with Dynamic Domain Name Service

HumaNet
Wearable
Smart
Devices

Web Server

Web Server

Web Server

“Device 1”
SafeDrive
Client

“Device 2”
SafeDrive
Client

“Device n”
SafeDrive
Client

Database

Database

Database

Sensors

II. S AFE D RIVE : D RIVER S AFETY A PPLICATION
As shown in Figure 1, two types of information from each
driver are sent to the transportation control system. The first

Emotiv

Neurosky

Speed &
Location

Fig. 2: The autonomous driver safety system model.

App

HumaNet

Context-based
Requirements
Checking

Code

Data

Model-based
Requirements
Checking

Execution
Manager

Context and Resource
Manager (CRM)

Data Collection
and Update

Offload
Manager

Resource

Context

Finding the best
Configuration

Communication
Manager

Fig. 3: Components of HumaNet architecture.

(Section IV-A2). The threshold will be used later to indicate
whether the individual is mentally fatigue while driving.
III. H UMA N ET A RCHITECTURE

2) Execution Manager: This module manages the execution of the code. It downloads the code and data blocks
assigned to it. The code is then run which takes input as the
downloaded data and results are generated.
3) Offload Manager: Offload manager takes the inputs from
the CRM to assess if the device currently meets the context
specifications. In case the required context does not exist, the
task is offloaded. Context and requirement specifications of
the application tasks are considered to determine the type of
task to be offloaded to the other devices.
4) Communication Manager: This module potentially offloads tasks to nearby available devices if needed.
IV. S AFE D RIVE I MPLEMENTATION

HumaNet provides an IoP framework that facilitates the
communication and data aggregation among different nodes
(apps and devices) for developing safe and sustainable apps.
In this sense, every participating smartphone installs the
middleware that runs in background. Aggregated data from
group of people can be exploited to develop apps based on
community inferences. On the other hand, to ensure safety and
sustainability requirements, off-line model-based and run-time
context-based checking are provided as seen in Figure 3.

As mentioned in Section II, smartphone has a foreground
application (SafeDrive client side) that computes mental fatigue levels and the collision detection algorithm (SafeDrive
server side) is executed on the central sever. The HumaNet on
the background is equipped with a timed automata model of
the entire execution of SafeDrive. On estimation of a possible
collision the time automata model is used to verify whether
sending a warning message will lead to a safe condition. Using
the timed automata model ensure that the driver reaches the
desired mental alertness state much before the reaction time
to the impending accident.

A. Model-Based Requirements Checking

A. EEG-Based Mental Fatigue Detector Application

In model-based checking, models of a given app are
developed and tested to analyze and optimize performance
considering accuracy, response time, and energy efficiency requirements. Behavioral models are formed and studied to avoid
unpredicted states of the app. Alongside, for optimization, the
test results are assessed to improve the app configurations (e.g.
sensors, algorithms, and platforms) through iterations.

Electroencephalography (EEG) is the most commonly used
technique for real-time analysis of brain signals to detect
mental fatigue. The process of falling asleep is divided into
three states, known as Awake, Non-Rapid Eye Movement sleep
(NREM), and Rapid Eye Movement sleep (REM) [16], [17].
In each stage, EEG signals divulge specific characteristics.
Table I lists main frequency bands extracted from EEG signals
and their corresponding cognitive state. Mental fatigue can be
recognized in the earliest stage of NREM. This stage coincides
with diminution in power changes in alpha frequency band and
simultaneously power raise in alpha and theta bands [16], [18].
1) Mental Fatigue Detection Algorithm: There are many
algorithms to detect mental fatigue from EEG datasets including Mean Comparison Test (MCT) and Thresholding
[19], Principal Component Analysis and Linear Regression
Model [10], F-measure (the harmonic mean of precision and
recall) [20], and support vector machine [11]. However, these
algorithms work best on a desktop setting and require compute
intensive operations. In our application, we use the Markov
chain decision process, which according to our test results is
computationally less expensive and has comparable accuracy.

B. Context-Based Requirements Checking
In this component, the IoP including sensors, apps, smartphones, and servers are monitored on-the-fly. The monitored
data is analyzed to make system level decisions toward ensuring the requirements. Thus, HumaNet applies computational
offloading (to a central server or smartphones in vicinity),
controlling sampling rate of sensing modules, prioritizing
execution of various apps, or adjusting trade-offs among
accuracy, latency, and energy. For instance, the specifications
(e.g. memory, workload, and battery level) of the candidate
devices are checked to choose the best match for offloading.
The functionalities for offloading are described below:
1) Context and Resource Manager (CRM): The CRM
module runs periodically within the device to ensure that the
application meets specific context and resource requirements
by sensing the context as well as the resources. The mobility
of users, other running apps, and user’s state can change the
context and resource values of the mobile device. Based on
the sensor information, the device context values are evaluated
against the context and resource requirement of the app.

TABLE I: EEG signals frequency bands and cognitive states.
Frequency Band
Delta (0.5-4 Hz)
Theta (4-8 Hz)
Alpha (8-13 Hz)
Beta (13-30 Hz)
Gamma (30-100 Hz)

Cognitive State [16]
Sleep Activity
Attention Level
Relaxation at decreased Attention Levels
Active Concentration and Alertness State
Perception

Research on mental fatigue detection shows that decrease
in Pβ /Pα and increase in (Pα + Pθ )/Pβ parameters expose
mental fatigue clearly, where Pα , Pβ , and Pθ indicate power
level in Alpha, Beta, and Theta bands, respectively [18]. In
our work, the Fast Fourier Transform (FFT) is performed on
raw EEG signals to transform them from time to frequency
domain. The signal amplitudes in the frequency domain are
used to compute the average EEG powers Pα , Pβ , and Pθ
over 14 channels for Emotiv and one channel for MindWave.
FFT is applied on every second of signal to update Pβ /Pα and
(Pα + Pθ )/Pβ . Two thresholds (Tl , Th ) are determined which
are proportional to the average value of Pβ /Pα and (Pα +
Pθ )/Pβ in alert state, respectively. The threshold calculation
is described in Section IV-A2. We then employ a two-state
Markov chain model (Figure 4) to compute the steady state
probabilities (π0 , π1 ) for alert and fatigue states. The model
parameters are defined as follow:


d0,0


d
0,1
d1,1



d1,0

:
:
:
:

Pβ /Pα
Pβ /Pα
Pβ /Pα
Pβ /Pα

>
≤
≤
>

Tl k (Pα + Pθ )/Pβ < Th in State 0
Tl &&(Pα + Pθ )/Pβ ≥ Th in State 0
Tl &&(Pα + Pθ )/Pβ ≥ Th in State 1
Tl k (Pα + Pθ )/Pβ < Th in State 1

(1)

π0 and π1 are updated every second, according to the parameters derived from the last 30s of data recording. As soon as π1
exceeds beyond 50%, the condition of the user is interpreted
as a mental fatigue state. When the fatigue state is detected,
the application will make an alarm tone.
2) Threshold Setting and Ground Truth: We need to set
thresholds accurately to detect mental fatigue in a timely
manner and avoid false alarms as much as possible. According
to sleep studies, human mental fatigue generally appears
late at night and impairs performance [21]. Subsequently, in
fatigue detection research, the most common way to determine
thresholds on alpha, beta, and theta is through experiments
in different times of a day [22], [23]. In these studies, they
assume driver is alert in the morning and mentally fatigue late
at night as their ground truth. We also take a similar approach.
Our experiment has two phases: training and testing. In
the training phase, we need brain signal samples of a subject
once she is completely alert and the other time when she feels
drowsy. So, we asked the subject to wear the headset (e.g.
Neurosky MindWave or Emotiv) for two 30-minute sessions;
one in the morning and the other one late at night (signals
are recorded with 512 Hz sampling rate). Signals from the
morning session, are considered as alert state or baseline
signals and vice versa. We applied MCT on the mean values
of ratio indices Pβ /Pα and (Pα + Pθ )/Pβ in each alert and
mental fatigue state to set the thresholds. As a result, at
least about 25% reduction in Pβ /Pα value and about 10%
growth in (Pα + Pθ )/Pβ value (compared to these values
in alert state) indicates mental fatigue state. In the testing
phase, the subject puts on the headset and run the mobile
application in five trials on different days. Each trial has two
30-minute sessions, one in the morning and the other one late
at night. The headset captures EEG signals and the smartphone monitors, and processes them real-time. According to
the threshold from the training phase, mental fatigue detection

Alert

Fatigue

Fig. 4: The two-state Markov chain model. D0 and D1
represent the sums d0,0 + d0,1 and d1,0 + d1,1 , respectively.
algorithm (Section IV-A1) can determine mental state (i.e. alert
or fatigue) of the subject.
B. Vehicle Collision Detection Algorithm
In this algorithm (i.e. our previous work [24]), we assume
that the transportation control system (Figure 1) can monitor
the car parameters of a large number of cars on a highway
or at least the speed of the vehicle precede and follow the
vehicle whose mental fatigue is being monitored. To predict
the motion of the concerned vehicle, lateral and longitudinal
control algorithms [25] can be used. The lateral control
algorithm generates steering angle (i.e lateral control angle)
based on the current position and the next destination over a
short amount of time as given by the following equation:
2

δ = arctan[2l(3y − xtanθ)/x ],

(2)

where δ is the steering angle, l is the wheel base of the
vehicle, (x, y) is the next way point, and θ is the heading angle
with respect to the road. The longitudinal control algorithm
generates speed based on the preceding and following vehicle’s
speed as per the equation given below:
vr = vp + k1(vp − vf ) + k2(Lr − Lm )

(3)

where vr is the speed of the AV, vp and vf are the speed of
the preceding vehicle and following vehicle respectively, Lr
is the minimum longitudinal distance between two vehicles,
Lm is the measured inter vehicular distance, k1 = m1kLr −
Lm k/kLr k, k2 = m2k1, and m1 and m2 are control gains.
A collision can be detected whenever either p
a) vr − vp > (xr − xp )2 + (yr − yp )2 /τ , where (xr , yr ) and
(xp , yp ) are the positions of the vehicle and the preceding one,
respectively and τ is a small amount of time that is less than
the typical reaction time of human drivers, road conditions,
and speed limits.
p
b) vf − vr > (xf − xr )2 + (yf − yr )2 /τ , where (xr , yr ) is the
current position of the following vehicle, or
p
p
c) (x − xp )2 + (y − yp )2 ≤  or (x − xf )2 + (y − yf )2 ≤ ,
where  is a parameter determined by the road conditions and
highway speed limits.
C. Modeling Real-Time Safety Properties
In the SafeDrive application, temporal constraints can limit
the choices of mental fatigue detection algorithm that can be
used in the wearable devices. The computation discussed in
Section IV-B can accurately predict imminent collision 6s in

the future. Within this time interval the smartphone has to
compute mental fatigue levels and based on the thresholds,
the smartphone will decide to set the alarm. The alarm has
to be set at a time such that the driver has enough time to
maneuver out of the dangerous situation. The reaction time of
a driver in highway with a speed limit of 65 mph is around
2-4s [26], [27]. This leaves the mental fatigue application only
2s to make a decision.
Figure 5 shows a Finite State Automata (FSA) representation of the SafeDrive application. The FSA has states and
transitions between them. Each transition operates on some
input x = xi and assigns an output y := yi and the transition
condition is denoted by x = xi /y := yi . The FSA for
SafeDrive app has five states, four inputs (X, Z, T , and P ),
and two outputs (Y and C) as described in Figure 5. The
FSA in each vehicle starts in the state E. When a car enters
the danger zone X = 1, it sets Y = 0 and sends baseline
EEG activity to the server and the FSA transits to state L.
In the state L if there is an threshold update request then
T = 1 and the FSA goes to state U and after the update
comes back to L. If the collision detection algorithm detects
collision then Z = 1 and the FSA goes to the state D to
invoke the driver drowsiness alertness algorithm. If the driver
is alert then P = 0 and Y = 1 since the driver sends the
drowsiness levels to the server. If the driver is drowsy then
the FSA goes to alarm (A) state and sets Y = 1. In the alarm
state if there is a collision then C = 1 and updated to the
server. Else when the driver gets out of the danger zone, it
updates the server with the values for Y and C and exits. Given
this FSA specification real-time properties can be specified
using a combination of temporal logic and timer clock in the
FSA. The temporal properties include: a) update requirement:
after every new update threshold message from the client
the brain server has to immediately update its software, and
b) collision avoidance window: since the collision predictor
can only predict 6s in future, and driver reaction time in the
worst case is 4s, the entire process including collision data
availability, mental fatigue detection, and alarm setting has to
be done within a window of 2s.
In Linear Temporal Logic (LTL), a proposition x =⇒ y
denotes the logical function ¬x ∨ (x ∧ y). Some of the key
operators used in this paper are Γ, Υ, and ξ. The operator
Γ applied on a proposition M denotes that the proposition is
true for any sequence or subsequence of states in the finite
state automata. This effectively means that it is true for each
state of the FSA. The operator Υ applied on a proposition M
denotes that the proposition eventually be true for some time
t > 0. The operator ξ applied on a proposition M denotes that
the proposition is true in the next state.
The proposition L, D, U , E, or A are true if the FSA is
in the corresponding states. The update requirement can be
expressed using the formulation Γ(L ∧ T = 1 =⇒ ξU ). The
collision avoidance window can be expressed by introducing
another time variable in the FSA. In each state of the FSA, we
include the equation t = t + τ , where τ is the time resolution
of the FSA. In the state L, we consider that whenever Z = 1

𝑋 = 1/𝑌 ≔0

𝑇=1/

Input X – location input
X = 1 when vehicle enters danger zone,
= 0 when vehicle exits danger zone
E
L
U
Input Z – collision prediction input
Z = 1 when server predicts a collision
𝑋 =0/
= 0 when server predicts no collision
𝑌 ≔1,𝐶
Z= 1/
𝑃 = 0/𝑌 ≔ 1 Input T – threshold update input
T = 1 when threshold is updated
A
D
= 0 when threshold is not updated
Input P – drowsiness detection
𝑃 =1/𝑌 ≔ 1
P = 1 when drowsiness is detected
= 0 when not drowsy
E – Initial state when the vehicle enters the
danger zone
Output Y - communicate data to client
L – Listening state when waiting for server to
Y = 0, when sending baseline active
respond with collision prediction
alpha, beta and theta activity
D – State to invoke execution of drowsiness
detection application
A – State to invoke drowsiness related alarms
U – State to update threshold

= 1, when sending drowsiness levels,
Output C - collision status update to
client
C = 0, when no collision
= 1, when collision

Fig. 5: Finite state automata representation of the SafeDrive.
we set t = 0.2s, which is the execution time of the prediction
algorithm on the client side and the communication delay. Now
the requirement is that there has to be a mental fatigue related
alarm if P = 1 within t = 2s can be expressed as follows Γ(L ∧ Z = 1 =⇒ (XD ∧ Υ(P = 1 =⇒ (ΥA ∧ (t < 2.0s)))))

V. E VALUATION OF S AFE D RIVE
We evaluate SafeDrive application implementation using
HumaNet with respect to two metrics: a) real-time safety
requirements, and b) energy and power consumption.
A. Real-Time Safety Requirements
For verifying the safety requirement, we need to consider
accuracy and response time of our algorithm.
1) Accuracy: The True Positive (TP), True Negative (TN),
False Positive (FP), and False Negative (FN) were calcuP +T N
lated for each trial, to obtain: Accuracy = T P +TTN
+F P +F N ,
TP
TN
Sensitivity = T P +F N , and Specificity = F P +T N as listed
in Table II. In these formulas, true positive represents the
number of alarms in fatigue state, and false negative is equal
to number of alarm misses in fatigue state. On the other hand,
true negative is the number of alertness detection in alert
state, and false positive represents the number of alarms in
alert state. Test results in Table II show that the application
can detect mental fatigue with 91% accuracy using Emotiv
before the user fall asleep and completely lose control of
the vehicle. MindWave has just one electrode placed on the
user’s forehead. So, compared with to Emotiv, which uses 14
channels, more test sessions are required using MindWave to
set up thresholds and reach the accuracy of Emotiv.
2) Response Time: We measured the end-to-end latency of
the SafeDrive app. For Emotiv, the average end-to-end latency
of the system is 2.031s with variance (σ 2 ) = 0.142 and standard
deviation (σ) = 0.377. For Neurosky, the average end-to-end
latency of the system is 0.995s with σ 2 = 0.223 and σ = 0.472.
According to collision studies in highways (Section III-A),
with these response times, the system can avoid accidents
caused by mental fatigue of the driver and satisfy real-time
expectations of the system.

TABLE II: The experimental results in five test trials.
Test Trial
Accuracy
Sensitivity
Specificity

1
93%
86%
100%

2
92%
85%
100%

3
88%
76%
100%

4
91%
81%
99%

5
95%
90%
99%

Average
91%
83%
99%

B. Energy and Power Consumption
One of the key concerns in designing any practical, efficient,
and sustainable systems is to keep the amount of energy and
power consumption as low as possible. By using Emotiv,
a rough estimation of battery usage shows that when the
application is running on the phone, we have 2% more battery
discharge. Power monitoring on the phone indicates that the
running application consumes about 130 mW on average.
Monitoring battery usage using MindWave for EEG recording
shows a 1% increase when the application runs on the phone
and consumes about 109 mW power on average. There is an
inverse relation between power and latency. We can decrease
power usage by reducing frequency and increasing response
time. So, the trade off between power and time should be
considered for real-time processes.
VI. C ONCLUSIONS
In this paper, we introduce HumaNet architecture for developing real-time driving assistant mobile applications for aware
cities. HumaNet enables apps using domain specific knowledge, to rapidly be implemented in a community networking
framework that is optimized for accuracy, response time, and
energy efficiency. HumaNet allows apps to share data and
hence reduces cost of physiological sensing. We implement a
prototype version of HumaNet and show its usage for developing the SafeDrive application. SafeDrive application requires
sharing of physiological data among individuals and provides
accurate physiological feedback through auditory stimulation
in real-time. In this sense, HumaNet platform applies modelbased and context-based optimization techniques to satisfy
safety and energy efficiency requirements in SafeDrive.
ACKNOWLEDGMENTS
This work has been partly funded by CNS grant #1218505,
IIS grant #1116385, and NIH grant #EB019202.
R EFERENCES
[1] M. Naphade, G. Banavar, C. Harrison, J. Paraszczak, and R. Morris,
“Smarter cities and their innovation challenges,” Computer, vol. 44,
no. 6, pp. 32–39, 2011.
[2] J. M. Schleicher, M. Vögler, C. Inzinger, and S. Dustdar, “Towards
the internet of cities: A research roadmap for next-generation smart
cities,” in Proceedings of the ACM First International Workshop on
Understanding the City with Urban Informatics. ACM, 2015, pp. 3–6.
[3] A. Mostashari, F. Arnold, M. Maurer, and J. Wade, “Citizens as sensors:
The cognitive city paradigm,” in Emerging Technologies for a Smarter
World, 8th International Conference & Expo on. IEEE, 2011, pp. 1–5.
[4] K. S. Oskooyee, A. Banerjee, and S. K. S. Gupta, “Neuro movie
theatre: A real-time internet-of-people based mobile application,” in The
16th ACM International Workshop on Mobile Computing Systems and
Applications. ACM, 2015.
[5] J. Sohankar, K. Sadeghi, A. Banerjee, and S. K. S. Gupta, “E-bias:
A pervasive eeg-based identification and authentication system,” in
Proceedings of the 11th ACM Symposium on QoS and Security for
Wireless and Mobile Networks. ACM, 2015, pp. 165–172.

[6] J. Yin, A. Lampert, M. Cameron, B. Robinson, and R. Power, “Using social media to enhance emergency situation awareness,” IEEE Intelligent
Systems, vol. 27, no. 6, pp. 52–59, 2012.
[7] B.-G. Lee and W.-Y. Chung, “Driver alertness monitoring using fusion
of facial features and bio-signals,” Sensors Journal, IEEE, vol. 12, no. 7,
pp. 2416–2422, 2012.
[8] R. N. Khushaba, S. Kodagoda, S. Lal, and G. Dissanayake, “Driver
drowsiness classification using fuzzy wavelet-packet-based featureextraction algorithm,” Biomedical Engineering, IEEE Transactions on,
vol. 58, no. 1, pp. 121–131, 2011.
[9] M. M. Bundele and R. Banerjee, “Detection of fatigue of vehicular driver
using skin conductance and oximetry pulse: a neural network approach,”
in Proceedings of the 11th International Conference on Information
Integration and Web-based Applications & Services. ACM, 2009, pp.
739–744.
[10] C.-T. Lin and et. al, “Development of wireless brain computer interface
with embedded multitask scheduling and its application on real-time
driver’s drowsiness detection and warning,” Biomedical Engineering,
IEEE Transactions on, vol. 55, no. 5, pp. 1582–1591, 2008.
[11] I. Shin and et. al, “Development of drowsiness detection system with
analyzing attention and meditation wave using support vector machine
method,” ISICO 2013, 2013.
[12] “Neurosky body and mind quantified. neurosky.com.”
[13] A. Stopczynski, C. Stahlhut, J. E. Larsen, M. K. Petersen, and L. K.
Hansen, “The smartphone brain scanner: a portable real-time neuroimaging system,” PloS one, vol. 9, no. 2, 2014.
[14] A. Banerjee, K. K. Venkatasubramanian, T. Mukherjee, and S. K. S.
Gupta, “Ensuring safety, security, and sustainability of mission-critical
cyber–physical systems,” Proceedings of the IEEE, vol. 100, no. 1, pp.
283–299, 2012.
[15] M. Pore, K. Sadeghi, V. Chakati, A. Banerjee, and S. K. S. Gupta,
“Enabling real-time collaborative brain-mobile interactive applications
on volunteer mobile devices,” in Proceedings of the 2nd International
Workshop on Hot Topics in Wireless. ACM, 2015, pp. 46–50.
[16] A. Sahayadhas, K. Sundaraj, and M. Murugappan, “Detecting driver
drowsiness based on sensors: a review,” Sensors, vol. 12, no. 12, pp.
16 937–16 953, 2012.
[17] P. A. Bryant, J. Trinder, and N. Curtis, “Sick and tired: does sleep have
a vital role in the immune system?” Nature Reviews Immunology, vol. 4,
no. 6, pp. 457–467, 2004.
[18] H. J. Eoh, M. K. Chung, and S.-H. Kim, “Electroencephalographic study
of drowsiness in simulated driving with sleep deprivation,” International
Journal of Industrial Ergonomics, vol. 35, no. 4, pp. 307–320, 2005.
[19] J. Park, L. Xu, V. Sridhar, M. Chi, and G. Cauwenberghs, “Wireless dry
eeg for drowsiness detection,” in Engineering in Medicine and Biology
Society, EMBC, 2011 Annual International Conference of the IEEE.
IEEE, 2011, pp. 3298–3301.
[20] C.-T. Lin and et. al, “A real-time wireless brain–computer interface
system for drowsiness detection,” Biomedical Circuits and Systems,
IEEE Transactions on, vol. 4, no. 4, pp. 214–222, 2010.
[21] C.-T. Lin, R.-C. Wu, T.-P. Jung, S.-F. Liang, and T.-Y. Huang, “Estimating driving performance based on eeg spectrum analysis,” EURASIP
Journal on Applied Signal Processing, vol. 2005, pp. 3165–3174, 2005.
[22] T. Kurita, N. Otsu, and N. Abdelmalek, “Maximum likelihood thresholding based on population mixture models,” Pattern Recognition, vol. 25,
no. 10, pp. 1231 – 1240, 1992.
[23] S. H. Kwon, “Threshold selection based on cluster analysis,” Pattern
Recognition Letters, vol. 25, no. 9, pp. 1045 – 1050, 2004.
[24] S. Kandula, T. Mukherjee, and S. K. S. Gupta, “Toward autonomous
vehicle safety verification from mobile cyber-physical systems perspective,” SIGBED Rev., vol. 8, no. 2, pp. 19–22, jun 2011.
[25] S. Kato, S. Tsugawa, K. Tokuda, T. Matsui, and H. Fujii, “Vehicle
control algorithms for cooperative driving with automated vehicles and
intervehicle communications,” Intelligent Transportation Systems, IEEE
Transactions on, vol. 3, no. 3, pp. 155 – 161, sep. 2002.
[26] A. Mehmood and S. M. Easa, “Modeling reaction time in car-following
behaviour based on human factors,” International Journal of Applied
Science, Engineering and Technology, 2009.
[27] J. W. Muttart, “Quantifying driver response times based upon research
and real life data,” in 3rd International Driving Symposium on Human
Factors in Driver Assessment, Training, and Vehicle Design, vol. 3,
2005, pp. 8–29.

A LANGUAGE-BASED GENERATIVE MODEL FRAMEWORK
FOR BEHAVIORAL ANALYSIS OF COUPLES’ THERAPY
Sandeep Nallan Chakravarthula1 , Rahul Gupta1 , Brian Baucom2 , Panayiotis Georgiou1
1

2

University of Southern California, Los Angeles, CA, USA
The University of Utah, Department of Psychology, UT, USA

ABSTRACT
Observational studies for psychological evaluations rely on careful
assessment of multiple behavioral cues. Recent studies have made
good progress in automating the psychological evaluation, which often involved tedious manual annotation of a set of behavioral codes.
However, the current methods impose strict and often unnatural assumptions for evaluation. In this work, we specifically investigate
two goals: (1) Human behavior changes throughout an interaction
and better models of this evolution can improve automated behavioral annotation and (2) Human perception of this evolution can be
quite complex and non-linear and better techniques than averaging
need to be investigated. For this purpose, we propose a Dynamic
Behavior Modeling (DBM) scheme, which models a spouse as undergoing changes in behavioral state within a session, and contrast it
against a Static Behavior Model (SBM) which allows only a constant
session-long behavioral state. We use Negativity in a couples therapy task as our case study. We present results and analysis on both
models for capturing the local behavior information and predicting
the session level negativity label.
Index Terms— Behavioral Signal Processing, Static Behavioral
Models (SBM), Dynamic Behavioral Model (DBM), Hard Expectation Maximization algorithms.
1. INTRODUCTION
Observational studies are the basis of most psychological evaluations
and they rely on careful observation and assessment of social, affective, and communicative behavioral cues. In family studies research
and practice, our topic of interest, psychologists rely on a variety of
established coding standards [1–3], with the aim of producing accurate, consistent ratings of human behavior by human annotators.
This manual coding is a costly and time consuming process. First, a
detailed coding manual must be created, which often requires several
design iterations. Then, multiple coders, each of whom has his/her
own biases and limitations, must be trained in a consistent manner.
The process is mentally straining and the resulting human agreement
is often quite low [4].
Great strides have been made over the last few years on establishing the ability of Signal Processing and Natural Language Processing methods as viable in estimating the behavioral state of the interlocutors. Overview papers [5,6] describe some of those advances.
In couples therapy, example work includes [7–16], while similar
work exists in other domains such as motivational interview therapy for addiction and autism [17–21]. These methods however suffer from the simplistic assumption of estimating a single behavioral
state from the whole interaction: the inherent underlying assumption
is that the human is a behavioral state generator and he/she generates
from the same state for the entire duration. On the other hand, previous work has also focused on dynamically modeling the behavioral
constructs such as emotions [22, 23] and engagement [24]. Whereas
these capture the evolution of behavioral states, they fail to provide
a link to an overall perception.
This work was supported by the National Science Foundation (NSF).

978-1-4673-6997-8/15/$31.00 ©2015 IEEE

This work is the first step towards the creation of a more realistic
model of human interaction that will try to address the following:
(1) Human annotators employ a range of information to reach an
integrated conclusion as to the behavior of interlocutors. While in
past work we assumed that all observations (say talk-turns) belonged
to the same class as the overall rating, we will expand our models to
allow for the human subject to move through a range of behavioral
states. We hypothesize that this more realistic model can both (a)
provide more accurate overall judgment and (b) higher-resolution
information about what happens in the interaction.
(2) Humans do not integrate information in a linear manner. The
saying “first impression counts” is an indicator of this non-linear process. This phenomenon is also studied extensively in the psychology
literature. For example [25] investigates recency (thing observed last
counts more) and primacy (thing observed first counts more). Integrating information requires an in-depth investigation of the algorithmic metrics of behavior (say log likelihood of a negative statement)
versus the impact this has on the coding process. In our recent work
we briefly touched on this [26] in investigating how human annotators employ isolated saliency or causal integration to make their decisions. Through the higher-resolution information achieved through
(1) we can evaluate non-linear techniques in fusing local decisions
for reaching a global (session) decision.
This paper provides a proof of concept of the above by employing a 2-state model described below as Dynamic Behavioral Model
(DBM) and contrasting it with the Static Behavior Model (SBM).
The SBM works as our baseline assuming a constant behavioral state
throughout interaction. A DBM allows for transitions between behavioral states within a session, and learns behavior representations
from these transitions. Comparison of these models helps provide
evaluation results for the goal (1a) in this work while we are working with our psychology partners to evaluate (1b). In order to evaluate fusion of turn-level decisions as stated in goal (2), we evaluate
methods such as model log-likelihood comparison and comparing
distribution of behavioral state occupancies.
This paper is organized as follows: Section 2 provides a description of the database. We describe the SBM and DBMs in detail in
Section 3, followed by the training methodology in section 4. The
evaluation of our behavioral models and their results are detailed in
Section 5, after which we discuss the results in Section 6. Finally,
we present our conclusions in Section 7.
2. DATABASE
We use the data of 134 couples from the UCLA/UW Couple Therapy Research Project [27]. The corpus consists of audio and video,
recorded during sessions of real couples interacting, and the session
transcripts. During a session, the husband and wife converse about
a pre-decided topic (e.g. “why can’t you leave my stuff alone ?”)
for a certain duration of time. Based on the Couples Interaction Rating System [1] and Social Support Interaction Rating System [2],
each participant is later rated by annotators for a set of 33 behavioral
codes including “Acceptance”, “Blame”, “Positivity” and “Negativity”. Annotators provide subjective ratings on a Likert scale of 1-9,

2090

ICASSP 2015

Utterance

Utterance

Utterance

Utterance

Utterance

Utterance

Utterance

Ci

=

P (U |Cj )P (Cj )
P (U )

(1)

arg max P (U |Cj )P (Cj )

(2)

arg max
Cj

Utterance

Utterance
Static Behavior Model

For class Ci = {C0 , C1 }:

=

Utterance

Cj

Dynamic Behavior Model

Fig. 1: Conceptualizing the proposed graphical model on the right
versus the baseline on the left

Since our dataset is balanced (70 files per class), the class prior
probabilities are equal, i.e. P (C0 )=P (C1 ). Therefore, we re-write
the decision as:
Ci = arg max P (U |Cj )
(3)
Cj

where 1 indicates absence of the behavior and 9 implies a strong
presence. The sessions are rated by 2-12 annotators with majority of
the sessions (∼ 90%) rated by 3-4 annotators.
For our task, we analyze the ‘Negativity’ behavioral code. In
order to simplify the code learning, we only use sessions with mean
annotator ratings in the top 20% (‘High Negativity’) and bottom 20%
(‘Low Negativity’) of the code range and we binarize into C1 and
C0 respectively. In this manner, we pose the learning problem as a
binary classification one, as was also done in our earlier work [16].
Table 1 lists the statistics on the chosen set of data and we describe
our modeling schemes in the next section.
For more information, the reader can refer to [1, 2, 10]

Husband
Class Label
Code Range
No. of Files

C0
1.00-1.75
70

C1
5.33-9.00
70

Wife
C0
1.00-2.00
70

C1
6.33-9.00
70

Equation 3 represents the final decision scheme for the SBM.
The training and testing methodologies for this model are discussed
in Sections 4.1 and 5.1 respectively.
3.2. Dynamic Behavior Models
Dynamic Behavior Models (DBMs) allow for a person’s behavior
to change over time. This is modeled in the form of transitions
between different behavioral states throughout a session, shown in
Fig. 1 (right). In our work we will simplify the model to have 2
states and we assume that behavior remains short-term stationary
(i.e. behavior does not change within an utterance, but only from
one utterance to another).
We will denote utterance states as Si = {S0 , S1 }. The behavioral state of the interlocutor, labeled by the human annotators as
Low/High Negativity or C0 /C1 , does not provide a one-to-one correspondence any more. A person that is very negative (C1 ) can generate from both states (S0 , S1 ). The definition of the states S0 , S1
will be described in detail in Sec. 4.2.
Given only one turn, similar to the formulation of the SBM, a
ML model will result in:

Table 1: Data Demographics on 20% least/most negativity sessions

3. BEHAVIORAL MODELING
Existing work [12] has assumed that an interlocutor is constantly in
the same behavioral state. This is equivalent to modeling each interlocutor in the interaction as a single state generative model as shown
in Fig. 1 (left). This is clearly limiting and does not reflect human
behavior, that dynamically adapts based on the various stimuli, internal and external. Below we present two models: the Static Behavior
Model in Sec. 3.1 as our baseline and similar to the one in [12], and
the Dynamic Behavior Model in Sec. 3.2 that allows for transition
between two behavioral states throughout the interaction and thus
makes turn-level decisions instead of session level decisions.
3.1. Static Behavior Models
In the Static Behavior Modeling (SBM) framework, a person’s behavior is assumed to remain the same throughout the interaction,
irrespective of external stimuli such as spouse’s utterances, topic being discussed, etc. This is the same model we had proposed in [12]
and in effect corresponds to behavioral averaging. Thus, all the utterances observed in that session are generated from the same behavioral state as in Fig. 1(left).
Identifying the behavioral state of the interlocutor in this case
is equivalent to identifying the class label Ci = {C0 , C1 } from
the whole transcript. In this work we employ a Maximum Likelihood (ML) formulation for the binary classification. For a set
of observed utterances, corresponding to the whole transcript,
U ={U (1), . . . , U (M )}, we want to find:
P (Code Low-Negatity or High-Negativity|U ) = P (C0 or C1 |U )

P (Si |U (m))

=

P (U (m)|Si )P (Si )

(4)

This method estimates turn-level state probabilities. Human
coders integrate behavioral information and give a summative opinion on the session level. In order to reach inference from the DBM
for session level behavioral descriptors, we need to also integrate
such turn-level behavioral information. Below we provide two
fusion methods for behavioral integration.
3.3. Fusion Methods for DBMs
There are many perception inspired methods for behavioral integration. For instance in our work [26] we evaluated whether we could
judge global behavior based on a locally isolated, yet highly informative event or whether integrating information over time was more
effective. The premise of such work is that the human perception
process is capable of integrating local events to generate an overall
impression at the global level, but this process is not transparent and
as such is difficult to replicate. What we can instead do is use local
information to derive the same global decisions.
In this work we will employ two fusion methods for the DBM:
Activation-based and Likelihood-based.
3.3.1. Activation-based
In the Activation-based DBM (ADBM), shown in Fig. 2 (left), local,
state-label decisions are made for Si as shown below, based on (4)
Si = arg max P (U (m)|Sj )P (Sj )

(5)

Sj

This fusion method uses a majority vote assumption, where the
behavioral state which generates the largest proportion of utterances
determines the session label decision. From the training data we can
learn the mapping of Si → Cj , ∀ i, j ∈ {0, 1} as explained in
Sec.4.3.1

2091

Activation-Based Dynamic Behavior Model

S0

Likelihood-based Dynamic Behavior Model

Utterance

S0

S1

Utterance Utterance

Utterance
S1

Utterance Utterance

S1

Utterance

Utterance Utterance

S0

Utterance Utterance

Utterance

Utterance Utterance

Utterance

Thus C1 selected due to association

C1
Best path from decoding from C0 model therefore C0 behavior selected

learned on training data

(even if more S1 states in selected path)

Turn level decisions. Majority voting S1.

C0

Fig. 2: Activation-based DBM (left) versus the Likelihood-based
DBM (right)

3.3.2. Likelihood-based
In the Likelihood-based DBM (LDBM) we employ a Hidden Markov
Model (HMM) representation of the behavioral classes C0 and C1 .
This model assumes that an interaction is comprised of utterances
U ={U (1), ..., U (M )} which are observations emitted by the hidden
state sequence S={S(1), ..., S(M )}, where S(i) ∈ {S0 , S1 }, i ∈
{1, ..., M }. It is also assumed that both classes C0 and C1 can produce both states, albeit with different likelihood, as shown in Fig. 2
(right). In human terms, negative persons are likely to remain mostly
in negative states and express themselves negatively, although it is
sometimes likely that they will express a positive attitude.
In order to identify the most likely behavioral class, we decode
U based on the HMMs of C0 and C1 and chose the model that is
more likely to have generated it from the state sequence S.
Ci = arg max P (S j |U , αj , π)

(6)

αj

Where
αj is the HMM state transition matrix of Cj
π is the initial state probability vector
S j is state sequence decoded by HMM of Cj for U
4. BEHAVIOR MODEL TRAINING
In this section, we describe the training procedure for the SBM and
the two DBMs discussed in the previous section. For our implementation, we use language models to represent probabilistic models of lexical content, and build them using the SRILM toolkit [28].
We replace maximum likelihood schemes with minimum perplexity
schemes wherever applicable. Perplexity is a measure of how well
a probability model predicts an observation; lower the value, better
the model. For an utterance U (m), it is calculated as:
PP(U (m)) = P (w1 w2 ...wN )−1/N

(7)

Where
P (w1 w2 ...wN ) is joint probability of occurrence of words
Algorithm 1 Hard-Assignment EM algorithm for state convergence
in activation-based DBM
Initialize utterances in C0 session ∈ {S0 }, C1 ∈ {S1 }
Build language model L0 from utterances ∈ S0 , L1 from utterances ∈ S1
while training perplexity does not converge do
E-step: Classify every utterance U (m) in C0 ,C1 classes
Get perplexities PP0 , PP1 of U (m) computed by L0 ,L1
PP0 {U (m)}

state=S1

≷

wi is the ith word of utterance U (m), i ∈ {1, 2, ..., N }
From (7) we see that minimizing perplexity is equivalent to maximizing probability
4.1. SBM
From the SBM assumption, we know that the classes and states are
equivalent. Therefore, for building a model of a particular behavioral state C0 /C1 , we collect all utterances from the corresponding
sessions and train the language models L0 /L1 respectively. While
doing so, we combine our trained LMs with a Universal Background
Model (UBM), in order to smooth the language models. We use
a parameter λ=0.1 as the interpolation weight for the UBM. Thus,
the SBM consists of two LMs for each speaker that model his/her
language structure corresponding to the least and the most negative
behavior.
4.2. DBM
As seen in Section 3.2, DBMs allow multiple transitions within an
interaction and as a result, each session consists of utterances of multiple types of behavior. However, we have only the global session
ratings C0 and C1 and not the utterance-level state labels. Therefore, we use two iterative semi-supervised methods to label turnlevel information into the two clusters corresponding to S0 and S1 ,
as described in Section 4.3. Due to the non-availability of utterancelevel labels, the convergence of these methods is verified indirectly
through the overall training perplexity and the testing accuracy.
4.3. Fusion Methods for DBMs
Fusion methods tie local utterance-level decisions to the global
session-level behavior. Therefore, the type of fusion used defines
the relation between behavioral states S0 /S1 and behavioral classes
C0 /C1 , as explained below:
4.3.1. ADBM
Just like in the SBM, we initialize the language models L0 /L1 for
states S0 /S1 from utterances in C0 /C1 respectively, smoothed with
a UBM. We then classify each utterance as S0 or S1 and re-train language models until the training perplexity converges. Since perplexities are not direct measures of log-likelihoods, we classify each ut-

Algorithm 2 Viterbi-EM algorithm for state and class parameter
convergence in likelihood-based DBM
Initialize utterances in C0 session ∈ {S0 }, C1 ∈ {S1 }
Build language model L0 from utterances ∈ S0 , L1 from utterances ∈ S1
Initialize π,α0 ,α1
while training perplexity does not converge do
E-step: Decode C0 utterances using α0 , L0 , L1 , π
for every session utterance U (m) do
Get probabilities P0 ,P1 of utterance U (m) from L0 ,L1
Find probability that U (m) was generated by state Sk
if m=1 (start of session) then
γk (m) = πk *Pk {U (m)}; k ∈ {0, 1}
else
γk (m) = arg maxj [γj (m-1) * α0 (j,k)] * Pk {U (m)};
j,k ∈ {0, 1}
end if

PP1 {U (m)}; m ∈ {0, 1, ..., M }

state=S0

M-step: Build L0 from S0 utterances,& L1 from S1
end while

2092

γ0 (m)

state = S0

≷

γ1 (m)

state = S1

end for
Repeat E-step for C1 utterances; replace α0 with α1
M-step: Re-estimate states, class parameters
Build Lk from all U (m) whose state = Sk ; k ∈ {0, 1}
state pairs in class Cn )
; i,j,k ∈
Update αn (i,j) = Pcount(<i,j>
k count(<i,k> state pairs in class Cn )
{0, 1}, n ∈ {0, 1}
end while

terance independently of the rest, for which a Hard-Assignment Expectation Maximization (EM) scheme is better suited, as described
in Algorithm 1. At the completion of this re-assignment, each class
now contains both states. We, therefore, compute the proportions
of state occupancies for C0 and C1 sessions as decoded using the
models L0 and L1 .
4.3.2. LDBM
We initialize our model parameters as in Section 4.3.1 and obtain
converged estimates of behavioral states and the class parameters,
using the Viterbi-EM algorithm, as described in Algorithm 2. In this
model, the Hidden Markov Model (HMM) parameters need to be reestimated over iterations. Finally, each behavioral state is described
by the initial vector π, which contains the probability of the spouse
starting in each state, and its language model. Each class is now associated with a matrix that governs state transitions in that class; α0
for the C0 sessions, and α1 for C1 . α0 (i, j) represents the probability of a C0 -rated spouse transitioning to state j, given that he/she
was previously in state i.
5. BEHAVIORAL MODEL EVALUATION
In this section, we explain our evaluation procedure for the two different models and the two different fusion techniques used in DBMs.
5.1. SBM
For a given test session, we assign C1 /C0 label to a speaker after
comparing the perplexities from his/her LMs. Given a set of M
utterances U ={U (1), ..., U (M )} from a speaker during the test session, we compute the LM perplexities based on L0 and L1 , and the
class label assignment is shown in equation 8.
X
P Pj {U (m)}
(8)
Ci = arg min
j

m

Where
P Pi {} represents perplexity score of utterance computed by LM Li
The results for SBM, obtained using a 1-gram LM, are shown in
Table 2.
5.2. DBM
Although both ADBM and LDBM allow state transitions, the scoring mechanism is quite different in each case, as explained below.
5.2.1. ADBM
For a given test session, we perform hard-assignment of state labels
to utterances using L0 and L1 and compute the state distribution.
For a set of utterances U ={U (1), ..., U (M )} where the most commonly occurring state is found to be Sk , the ADBM chooses the
behavioral class that maximizes
Ci = arg max P (Sk |Cj )

(9)

Cj

The performance of ADBM improves, on average, by around 1%
relative to the SBM, and the results are shown in Table 2.
5.2.2. LDBM
For a set of test utterances U , we decode based on HMMs of both
C0 and C1 . The label of the class whose prediction is associated
with the highest likelihood is assigned to the test file. This decision
scheme is shown in equation (10)
Ci = arg max P (S j |U , αj , π)

(10)

Model

Husband

Wife

Average

SBM
Activation-DBM
Likelihood-DBM

79.29%
85.00%
83.57%

83.57%
79.29%
88.57%

81.43%
82.15%
86.07%

Table 2: Classification Accuracy of Behavioral Models using 1grams

6. DISCUSSION
The likelihood-DBM has the best average performance across both
spouses, at 86.07%, while the SBM has the lowest, with 81.43%.
This matches our expectations since the likelihood-DBM is less
rigid in its assumptions about behavioral changes. Thus, it is better equipped to capture information about dynamic behavior, as
compared to the other two models.
While the SBM helps in identifying which state a person’s behavior more likely corresponded to, it is limited in its ability to
model changes in behavior within a session. The Activation-DBM
can model behavioral changes within the same session, and performs
well, but it allows rapid changes in behavioral states across successive utterances. We feel that such fast changes in behavior do not
realistically mirror the usual changes in human behavioral expression. In addition, there is no dependence on previous utterances,
meaning that context is ignored. The Likelihood-DBM avoids the
pitfalls associated with the first two models, and while more complex, it is more accurate in predicting behavior from language. Note
that both dynamic implementations can likely be improved through
supervised learning and it is notable that the achieved performance
is through only loose semi-supervised clustering.
7. CONCLUSION
Automating the psychological evaluations can have a significant
impact by providing less expensive and more accurate methods
of behavioral coding. In this work, we proposed a Dynamic Behavior Model based scheme which models a spouse in couples’
therapy as transitioning through multiple behavioral states to obtain an overall perception of negativity. We tested two models for
dynamically modeling the behavior: Activation-based DBM and
Likelihood-based DBM which outperformed the Static Behavioral
Model. Furthermore, the LDBM performed slightly better than
ADBM as it provided more freedom to the way behavior could be
expressed.
In this work, we addressed only two models of local information
integration towards deriving global behavioral descriptions. We plan
to extend this model to further incorporate dependencies amongst
spouses to model their behavioral influences as well as dependencies among other codes apart from negativity. So far, we have only
considered two state models to predict binary behavior levels. We
will develop models with a finer behavior stratification and observe
its relation with the number of states. Finally, this model can be
employed in a range of application domains involving behavioral
evaluations in multi-party interactions.
8. REFERENCES
[1] C. Heavey, D. Gill, and A. Christensen, Couples interaction
rating system 2 (CIRS2), University of California, Los Angeles, 2002.

λj

Where
U is set of test utterances, {U (1), ..., U (M )}
S n is the test state sequence predicted by class Cn , {S(1), ..., S(M )}
λn is set of HMM parameters of class Cn , {αn ,L0 ,L1 ,π}
The performance of LDBM improves, on average, by around 5.7%
relative to the SBM, and the results are shown in Table 2.

2093

[2] J. Jones and A. Christensen, “Couples interaction study: Social
support interaction rating system,” Technical manual, University of California, Los Angeles, 1998.
[3] RE Heyman and D. Vivian, “Rmics: Rapid marital interaction
coding system: Training manual for coders, state university of
new york, stony brook, ny (1993),” Unpublished manuscript.
Available at, 1993.

[4] G. Margolin, P.H. Oliver, E.B. Gordis, H.G. O’Hearn, A.M.
Medina, C.M. Ghosh, and L. Morland, “The nuts and bolts
of behavioral observation of marital and family interaction,”
Clinical Child and Family Psychology Review, vol. 1, no. 4,
pp. 195–213, 1998.
[5] Shrikanth S. Narayanan and Panayiotis G. Georgiou, “Behavioral signal processing: Deriving human behavioral informatics from speech and language,” Proceeding of the IEEE, 2014.
[6] Panayiotis G. Georgiou, Matthew P. Black, and Shrikanth S.
Narayanan, “Behavioral signal processing for understanding
(distressed) dyadic interactions: some recent developments,”
in Proceedings of the 2011 joint ACM workshop on Human
gesture and behavior understanding, New York, NY, 2011, JHGBU ’11, pp. 7–12, ACM.
[7] Bo Xiao, Panayiotis Georgiou, and S. Narayanan, “Data driven
modeling of head motion towards analysis of behaviors in
couple interactions,” in International Conference on Audio,
Speech and Signal Processing, 2013.
[8] James Gibson, Bo Xiao, Panayiotis Georgiou, and
S. Narayanan,
“An audio-visual approach to learning
salient behaviors in couples’ problem solving discussions,”
in International Conference on Audio, Speech and Signal
Processing, 2013.
[9] Chi-Chun Lee, Athanasios Katsamanis, Matthew P. Black,
Brian Baucom, Andrew Christensen, Panayiotis G. Georgiou,
and Shrikanth S. Narayanan, “Computing vocal entrainment:
A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions,” Computer, Speech, and Language, 2012.
[10] M.P. Black, A. Katsamanis, B.R. Baucom, C.C. Lee, A.C.
Lammert, A. Christensen, P.G. Georgiou, and S.S. Narayanan,
“Toward automating a human behavioral coding system for
married couples interactions using speech acoustic features,”
Speech Communication, 2012.
[11] Matthew Black, Panayiotis Georgiou, Athanasios Katsamanis,
Brian Baucom, and Shrikanth Narayanan, ““You made me
do it”: Classification of blame in married couples’ interaction
by fusing automatically derived speech and language information,” in Proceedings of InterSpeech, Florence, Italy, August
2011.
[12] P. G. Georgiou, M. P. Black, A. Lammert, B. Baucom, and
S. S. Narayanan, ““That’s aggravating, very aggravating”: Is
it possible to classify behaviors in couple interactions using
automatically derived lexical features?,” in Proceedings of Affective Computing and Intelligent Interaction, Memphis, TN,
USA, 2011.
[13] C.C. Lee, A. Katsamanis, M. Black, B. Baucom, P. Georgiou,
and S. Narayanan, “Affective state recognition in married couples interactions using PCA-based vocal entrainment measures
with multiple instance learning,” Affective Computing and Intelligent Interaction, pp. 31–41, 2011.
[14] C.-C. Lee, A. Katsamanis, M. P. Black, B. R. Baucom, P. G.
Georgiou, and S. S. Narayanan, “An analysis of PCA-based vocal entrainment measures in married couples’ affective spoken
interactions,” in Proceedings of InterSpeech, Florence, Italy,
2011.

[16] M. P. Black, A. Katsamanis, C.-C. Lee, A. Lammert, B. R.
Baucom, A. Christensen, P. G. Georgiou, and S. S. Narayanan,
“Automatic classification of married couples’ behavior using
audio features,” in Proceedings of InterSpeech, 2010.
[17] Bo Xiao, Daniel Bone, Maarten Van Segbroeck, Zac E. Imel,
David Atkins, Panayiotis Georgiou, and Shrikanth Narayanan,
“Modeling therapist empathy through prosody in drug addiction counseling,” in In Proceedings of Interspeech, 2014.
[18] Daniel Bone, Chi-Chun Lee, Theodora Chaspari, Matthew P.
Black, Marian Williams, Sungbok Lee, Pat Levitt, and
Shrikanth S. Narayanan, “Acoustic-prosodic, turn-taking, and
language cues in child-psychologist interactions for varying
social demand,” in Proceedings of InterSpeech, Aug. 2013.
[19] Theodora Chaspari, Chi-Chun Lee, and Shrikanth Narayanan,
“Interplay between verbal response latency and physiology of
children with autism during ECA interactions,” in Proceedings
of InterSpeech, 2012.
[20] Theodora Chaspari, Emily Mower Provost, Athanasios Katsamanis, and Shrikanth Narayanan, “An acoustic analysis of
shared enjoyment in ECA interactions of children with autism,”
in Proceedings of the International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), Kyoto, Japan, 2012.
[21] E. Mower, C.C. Lee, J. Gibson, T. Chaspari, M.E. Williams,
and S. Narayanan, “Analyzing the nature of ECA interactions
in children with autism,” in Twelfth Annual Conference of the
International Speech Communication Association, 2011.
[22] Rahul Gupta, Nikolaos Malandrakis, Bo Xiao, Tanaya Guha,
Maarten Van Segbroeck, Matthew P Black, Alexandros
Potamianos, and Shrikanth S Narayanan, “Multimodal prediction of affective dimensions and depression in humancomputer interactions,” .
[23] Björn Schuller, Gerhard Rigoll, and Manfred Lang, “Hidden markov model-based speech emotion recognition,” in
Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03). 2003 IEEE International Conference on.
IEEE, 2003, vol. 2, pp. II–1.
[24] Gian-Marco Baschera, Alberto Giovanni Busetto, Severin
Klingler, Joachim M Buhmann, and Markus Gross, “Modeling engagement dynamics in spelling learning,” in Artificial
Intelligence in Education. Springer, 2011, pp. 31–38.
[25] Dirk D Steiner and Jeffrey S Rain, “Immediate and delayed primacy and recency effects in performance evaluation.,” Journal
of Applied Psychology, vol. 74, no. 1, pp. 136, 1989.
[26] Chi-Chun Lee, Athanasios Katsamanis, Panayiotis G. Georgiou, and Shrikanth S. Narayanan, “Based on isolated saliency
or causal integration? toward a better understanding of human
annotation process using multiple instance learning and sequential probability ratio test,” in Proceedings of InterSpeech,
Sept. 2012.
[27] A. Christensen, D.C. Atkins, S. Berns, J. Wheeler, D.H. Baucom, and L.E. Simpson, “Traditional versus integrative behavioral couple therapy for significantly and chronically distressed
married couples,” Journal of Consulting and Clinical Psychology, vol. 72, no. 2, pp. 176–191, 2004.
[28] Andreas Stolcke et al., “Srilm-an extensible language modeling toolkit.,” in INTERSPEECH, 2002.

[15] C.-C. Lee, M. P. Black, A. Katsamanis, A. Lammert, B. R.
Baucom, A. Christensen, P. G. Georgiou, and S. Narayanan,
“Quantification of prosodic entrainment in affective spontaneous spoken interactions of married couples,” in Proceedings
of InterSpeech, 2010.

2094

