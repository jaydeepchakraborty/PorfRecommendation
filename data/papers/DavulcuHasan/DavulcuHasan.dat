arXiv:1608.01771v1 [cs.SI] 5 Aug 2016

Community Detection in Political Twitter Networks
using Nonnegative Matrix Factorization Methods
Mert Ozer

Nyunsu Kim

Hasan Davulcu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
mozer@asu.edu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
nkim30@asu.edu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
hdavulcu@asu.edu

Abstract—Community detection is a fundamental task in social
network analysis. In this paper, first we develop an endorsement
filtered user connectivity network by utilizing Heider’s structural
balance theory and certain Twitter triad patterns. Next, we
develop three Nonnegative Matrix Factorization frameworks to
investigate the contributions of different types of user connectivity
and content information in community detection. We show that
user content and endorsement filtered connectivity information
are complementary to each other in clustering politically motivated users into pure political communities. Word usage is
the strongest indicator of users’ political orientation among all
content categories. Incorporating user-word matrix and word
similarity regularizer provides the missing link in connectivityonly methods which suffer from detection of artificially large
number of clusters for Twitter networks.

I. I NTRODUCTION
Twitter has become one of the main stages of political
activity both among politicians and partisan crowds. We have
seen huge political mobilizations over Twitter in recent uprisings such as the Arab Spring and the Gezi protests [1].
Since then, politicians have been engaging in using Twitter to
attract supporters and people have been using it to express their
political views and opinions on various leaders and issues.
Community detection is a fundamental task in social network analysis [2]. A community [3] can be defined as a group
of users that (1) interact with each other more frequently than
with those outside the group and (2) are more similar to each
other than to those outside the group. Utilizing community detection algorithms to detect online political camps has attracted
many researchers [4], [5], [6]. In this work, we propose three
nonnegative matrix factorization frameworks to exploit both
user connectivity and content information in Twitter to find
ideologically pure communities in terms of their members’
political orientations.
Twitter presents three types of connectivity information
between users: follow, retweet and user mention. In this paper,
we do not use follow information since follow relationships
correspond to longer-term structural bonds [10] and it remains
challenging to determine if a follow relationship between a
pair of users indicate political support or opposition. Furthermore, it has been observed that neither user retweets nor user
mentions always indicate endorsement in Twitter [7]. However

in the political sub-domain of Twitter, it has been shown that
retweets tend to happen between like-minded users rather than
between members of opposing camps [8].
Using both connectivity and content information for community detection in social networks has been a popular approach among many researchers’ prior works [4], [5], [6], [3].
In [4], Tang et al. propose a general framework for integrating
multiple heterogenous data sources for community detection.
Tang’s work does not pay attention to identifying the endorsement subgraph of the connectivity graph. In [5] Sachan et al.
propose an LDA-like social interaction model by representing
user connectivity as a document alongside message content.
This approach also does not discriminate between positive
or negative user engagement. In [6], Ruan et al. propose to
use a filtered graph to eliminate ambiguous interactions by
checking content similarity in the user’s neighborhood. In
this formulation, only local content patterns are taken into
consideration whereas in our formulations we incorporate the
global content patterns into our optimization framework.
The contributions of this paper can be summarized as
follows:
•

•

We start with retweets without edits as indicators of
positive endorsements between users and utilize Heider’s
P-O-X triad balance theory [11] to incorporate selected
”structurally balanced” edited retweets and user mentions
into a weighted undirected connectivity graph as additional indicators of positive endorsements.
We develop algorithms which incorporate users’ content
information in our community detection frameworks to
overcome the sparse nature of Twitter connectivity networks. We break down Twitter message content into
three categories; words, hashtags and urls, and design
experiments to measure the performance contributions
of each category. Proposed Nonnegative Matrix Factorization (NMF) algorithms use user-word, user-hashtag
and user-domain frequency matrices to be factorized into
lower rank user vector representations while regularizing
over user connectivity and content similarity to map users
into their respective communities.

Pei et al. in [3] also model the problem as nonnegative

matrix tri-factorization problem which factorizes user-word,
tweet-word and user-user matrices into lower rank representations of users and tweets while regularizing it with user interaction and message similarity matrices. They build user-user
connectivity matrix by utilizing the structural follow relationships which do not capture dynamic political context-sensitive
engagement. They treat all user mentions and retweets identically and without any discrimination for endorsement. Their
framework also lacks word similarity regularization.
We develop and experiment with three nonnegative matrix factorization frameworks: MultiNMF, TriNMF, DualNMF,
which incorporate connectivity alongside different types of
content information as regularizers. After experimenting with
different dimensions of user content and different types of
induced connectivity networks we discovered that incorporating more information does not necessarily yield higher
clustering performance. Highest quality clustering is achieved
through endorsement filtered connectivity based on methods
we develop in Section III alongside user-word matrix based
content regularization. Our DualNMF framework gives purity
scores around 88%, adjusted rand index around 75% and NMI
around 67%. It improves all of the other baseline methods
significantly as presented in Section V and it also improves
over the NMTF framework developed recently by Pei et al.
[3] by 8% in purity, 47% in ARI and by up to 60% in
NMI metrics. Proposed endorsement filtered sub-graph of user
mentions and retweets also improves all baseline methods in
almost all of the experimental setups by up to 109% in NMI,
71% in ARI and 17% in purity.
The rest of the paper is organized as follows. Section
II briefly surveys related work. In Section III, we present
Heider’s theory of P-O-X structural balance of triads and
its application to retweet and mention graphs to identify
endorsement filtered user connectivity networks. In Section IV,
we introduce our three nonnegative matrix factorization frameworks for community detection. In Section V, we present our
experimental design, evaluation metrics and results. Section
VI concludes the paper and discusses future work.
II. R ELATED W ORK
A. Community Detection
Since the introduction of the modularity metric by Newman
in [16], plenty of modularity based community detection
methods have been proposed in the literature [17], [18], [19],
[20]. We employ Blondel et al. [18] and Clauset et al. [19]
works as baseline algorithms to compare with ours due to their
wide popularity among practitioners. A general drawback of
these algorithms, when they are applied to Twitter networks,
is that due to the sparse nature of the connectivity they end
up with an artificially large number of communities.

incorporate Laplacian graph regularization to the standard
NMF algorithm which assumes data points are sampled from
a Euclidian space which is not the case usually for real-world
applications. Gu et al. [31] further incorporate local learning regularization to NMF which assumes that geometrically
neighboring data points are similar to each other, and should be
in the same cluster. For co-clustering purposes Ding et al. [28]
propose nonnegative matrix tri-factorization with orthogonality
constraints. Shang et al. introduce graph dual regularized NMF
algorithm in [27] by claiming that not only observed data but
also features lie on a manifold. Yao et al. [24] apply the same
logic for collaborative filtering domain and propose a dual
regularized one-class collaborative filtering method.
III. T HE S TRUCTURAL BALANCE OF R ETWEET AND
M ENTION G RAPH
Since P-O-X triad balance theory proposed by Heider in
[11], structural balance of signed networks has been studied
extensively. Heider proposed that in a signed triad, only two
combinations of eight possible sign configurations are possible
for a triad to be structurally balanced. Those are the following
cases;
1) three positive edges,
2) one positive and a pair of negative edges.
In other words, there cannot be any structurally balanced triad
having only one negative edge. We adopt this social theory
for the Twitter user connectivity networks, by assuming that
”retweets without edits” imply political endorsement or an
unambiguous positive edge [12]. However, when a retweet is
edited, it has already been shown that [13], it does not necessarily mean endorsement anymore. Moreover, user mentions
do not imply endorsements either. For these reasons, we only
consider retweets without edits as positive edges. For the rest
of the user actions, corresponding to retweets with edits and
users mentions, it is hard to detect positivity or negativity of
the edges.
In certain triad configurations, retweets with edits and user
mentions can be identified as positive edges with the help
of Heider’s triad structural balance (TSB) rules. Since we
do not have unambiguous negative edges, the second case
is not applicable. However, since we have some positive
edges to begin with, we can employ Heider’s first case (i.e.
three positive edges), to infer that in the presence of a triad
with a pair of positive edges, the third edge can also be
labeled as positive. An example configuration with a pair of
positive edges is shown in Figure 1. In this case, TSB rule is
applicable and would allow us to infer that any user mention
or retweet with an edit edge connecting the lower pair of
users in the triad is indeed a positive edge. By employing
this inference mechanism we identify the endorsement filtered
user connectivity network.

B. Nonnegative Matrix Factorization
Nonnegative Matrix Factorization(NMF) algorithms by Lee
et al. [22] and Lin et al. [23] have been extensively used
and extended for different variations of community detection
problems. Cai et al. [29] introduced GNMF algorithm to

IV. P ROPOSED M ETHODS
We propose three methods for clustering politically motivated users in Twitter namely; MultiNMF, TriNMF and

•

•

•

Fig. 1.

An example application of TSB Rule

DualNMF. For MultiNMF method we use document term
representation of user-word, user-hashtag and user-domain matrices to be factorized and regularize the factorization problem
with the user connectivity graph, cosine similarity matrices of
words, domains and hashtag co-occurence matrix. For TriNMF
method we use only user-word and one of user-hashtag or
user-domain matrices and regularize over user connectivity
and cosine domain similarity or hashtag co-occurence matrix.
For DualNMF method we factorize user-word matrix into two
nonnegative lower rank matrices while regularizing it with user
connectivity and cosine word similarity. Before going into the
details of the three algorithms we present notation in Table I. In
TABLE I
N OTATION
Xuw
Xuh

user x word
user x hashtag

Xud

user x domain

R

user x user

M

user x user

∆M

user x user

∆Mw

user x user

C

user x user

Hsim
Dsim
Wsim
α
γ
θ
β
U
H
D
W

hashtag x hashtag
domain x domain
word x word
number
number
number
number
user x cluster
hashtag x cluster
domain x cluster
word x cluster

frequencies of words used by users
frequencies of hashtags used by users
frequencies of distinct domains used
by users
adjacency matrix of
retweet without edit graph
adjacency matrix of
mention and retweet with edit graph
adjacency matrix of mentions
and retweet with edits completing
retweet without edit triads
adjacency matrix of mentions
and retweet with edits completing
retweet without edit triads
weighted by retweet without edit edges
any combination of user connectivity
graphs
hashtag co-occurence matrix
domain similarity matrix
word similarity matrix
user connectivity regularizer parameter
hashtag similarity regularizer parameter
domain similarity regularizer parameter
word similarity regularizer parameter
cluster assignment matrix of users
cluster assignment matrix of hashtags
cluster assignment matrix of domains
cluster assignment matrix of words

this work, instead of using only full user retweet and mention
network we offer three types of user connectivity regularizers
as follows;

R + M: It is the adjacency matrix of the full retweet and
mention graph. If there exists both retweet and mention
edges between two users, weights are summed up.
R + ∆M: It is the adjacency matrix of the union of
retweet and mention graphs in which mention edges and
retweet with edits either complete a missing link in a
triad of retweet without edit or already correspond to a
retweet without edit edge. ∆M can be formally defined
as;
PN
∆M = {(i, j, Mij ) | Rij > 0 ∨ k=1 Rik Rkj > 0}
R + ∆Mw : It is the adjacency matrix of the union of
retweet and mention graphs in which mention edges and
retweet with edits either complete a missing link in a triad
of retweet without edit or already correspond to a retweet
without edit edge. The ones that complete a missing link
in a triad of retweet without edit are weighted by the
multiplication of the weights of two retweet without edit
edges in the triad. ∆Mw can be defined formally as;
∆Mw = {(i, j, Mij (Rij +

PN

k=1

Rik Rkj ))}

For word similarity and domain similarity regularizers
we make use of cosine similarity. It can be formally defined as;
cos(θ) =

vi · vj
k v i k ∗ k vj k

where vi is the user usage frequency vector of ith word or
domain. For hashtag similarity we build similarity matrix by
making use of co-occurences of hashtags in tweets. If two
hashtags occur in the same tweet, we assume that those two
hashtags are similar.
A. MultiNMF with multi regularizers
To incorporate usage of both hashtags and domains of
shared url links by users, we propose an NMF framework
which has the following objective function;
JU,H,D,W =k Xuw − UWT k2F + k Xuh − UHT k2F
+ k Xud − UDT k2F +αT r(UT LC U)
+ γT r(HT LHsim H) + θT r(DT LDsim D)

(1)

T

+ βT r(W LWsim W)
s.t. U ≥ 0, H ≥ 0, D ≥ 0, W ≥ 0
where LC is the Laplacian matrix of adjacency matrix of user
connectivity graph defined as DC − C and DC is the matrix
which contains the degree of each user node in its diagonals.
LHsim , LDsim and LWsim follow the same definition for
hashtags and words. Due to the very fuzzy multi-class nature
of words, hashtags and domain names, we do not include
orthogonality constraints for matrices U, H, D, W, which
usually result in more precise clusters for co-clustering tasks.
It is easy to see that the proposed objective function is not
convex for U, H, D and W, hence we develop an iterative
algorithm which tries to find a local minima by updating each
matrix iteratively as follows;
s
Xuw W + Xuh H + Xud D + αL−
CU
U←U
(2)
UWT W + UHT H + UDT D + αL+
CU

v
u T
u Xuh H + γL−
Hsim H
H←Ht
T
HU U + γL+
Hsim H
v
u T
u Xud D + θL−
Dsim D
D←Dt
T
DU U + θL+
Dsim D
v
u T
u Xuw U + βL−
Wsim W
W ←Wt
T
WU U + βL+
Wsim W
L+
ij

(4)

(5)

L−
ij

B. TriNMF with three regularizers
To incorporate usage of hashtags or domains of shared url
links solely, we propose a new NMF framework which has
the following objective function.
JU,H,W =k Xuw − UWT k2F + k Xuh − UHT k2F
+ αT r(UT LC U) + γT r(HT LHsim H)
+ βT r(WT LWsim W)

To use only user word matrix as user content and regularize
factorization with user connectivity and keyword similarity,
inspired by [24], we present DualNMF objective function as;
JU,W =k Xuw − UWT k2F +αT r(UT LC U)
+ βT r(WT LWsim W)

= (|Lij | − Lij )/2.
[·]
 represents element-wise multiplication and
represents
[·]
element-wise division. Derivation of update rules can be seen
in Appendix A. Complexity of the method can be inferred
as O(i(uwk + uhk + udk + u2 k + h2 k + d2 k + w2 k))
when complexity of multiplying any X matrix with any of
U, H, D, W is considered to be O(uwk), O(uhk), O(udk)
and multiplying any of Laplacian matrices L with any of
U, H, D, W is taken as O(u2 k), O(h2 k), O(d2 k) or O(w2 k)
where i is the number of iterations, u is number of users, h is
the number of hashtags, d is the number of domains, w is the
number of words and k is the number of clusters. The general
algorithmic framework is given at the end of methodology in
Algorithm 1.
where

= (|Lij | + Lij )/2 and

C. DualNMF
(3)

(6)

(10)

s.t. U ≥ 0, W ≥ 0
After following the same procedure introduced in Section
IV-A, we can get the update rules for U and W as;
s
Xuw W + αL−
CU
(11)
U←U
T
UW W + αL+
CU
v
u T
u Xuw U + βL−
Wsim W
(12)
W ←Wt
T
WU U + βL+
Wsim W
Complexity of the method can be inferred as O(i(uwk+u2 k+
w2 k)) after omitting the extra operations done over matrices
Xuh , H and DHsim in the previous method. The general
Algorithm 1 NMF Algorithms
Input:{Xuw , Xuh , Xud , C, Hsim , Dsim , Wsim , α, β, θ, γ}
Output: U
1: Initialize U, H, D, W > 0
2: while ∆residual > threshold do
3:
Update U by using one of Equations 2, 7, 11
4:
Update H by using one of Equations 3, 8
5:
Update D by using Equation 4
6:
Update W by using one of Equations 5, 9, 12
7: end while
8: Assign user i to community j where j = argmaxj Uij .

s.t. U ≥ 0, H ≥ 0, W ≥ 0
where LC is the Laplacian matrix of user connectivity defined
as DC − C and DC is a diagonal matrix which contains the
degree of each user in its diagonals. LHsim and LWsim follows
the same definition for hashtags and words. After applying the
same procedure followed in Section IV-A, we get updating
rules as follows.
s
Xuw W + Xuh H + αL−
CU
(7)
U←U
T
T
UW W + UH H + αL+
CU
v
u T
u Xuh U + γL−
Hsim H
H←Ht
(8)
T
HU U + γL+
Hsim H
v
u T
u Xuw U + βL−
Wsim W
(9)
W ←Wt
T
WU U + βL+
Wsim W
Note that this update rules can be obtained by setting D, Dsim
and θ equal to 0 in Equations 2, 3, 5. Complexity of the method
can be calculated by omitting the costs of operations done over
matrices Xud , D and LDsim . The complexity of the method
is O(i(uwk + uhk + u2 k + h2 k + w2 k)).

algorithm can be summarized as the application of the related
update rules to the matrices U, H, D, W. For MultiNMF with
multi regularizers method, equations 2, 3, 4, 5 are applied.
For TriNMF with three regularizers method, equations 7, 8,
9 are applied and D matrix is not included in calculations.
For DualNMF method, equations 11 and 12 are applied and
H and D matrices are not included in calculations.
V. E XPERIMENTS AND R ESULTS
A. Data Description
We make use of a pair of publicly available1 political Twitter
datasets to evaluate our methods. These datasets are user lists
of 419 British political figures from four major political parties
in the UK, namely; Conservative and Unionist Party, Labour
Party, Scottish National Party, Liberal Democrats and others,
and 349 major Irish political figures from seven political
parties; Fianna Fil, Fine Gael, Green Party, Sinn Fin, United
Left Alliance, Independents. Several statistics for the datasets
are shown in Table II.
1 Users’
Twitter
id
lists
http://mlg.ucd.ie/aggregation/index.html

can

be

obtained

from

TABLE II
DATA ATTRIBUTES

#
#
#
#
#
#
#
#

of
of
of
of
of
of
of
of

Tweets
Retweets
Mentions
Words
Hashtags
URL Domains
Users
Baseline Communities

UK

Ireland

19,947
1,566
4,956
10,766
945
946
233
5

14,656
7,088
22,072
7,973
986
634
258
7

For the UK and Ireland data, we crawl all of the tweets sent
from the accounts of given user id lists. In order not to be
heavily influenced by the extremely polarized election season,
we only used tweets dated after May, 7 2015, which was the
election day in the UK. To balance the share of number of
tweets from each user we limit the number of tweets to 200
per user.
For each dataset, same preprocessing method is followed.
First, words occurring less than 20 times and stop words
are eliminated. After eliminating word features, users and
tweets that lack content are also eliminated. Hashtags and
domains that appear only once are not taken into consideration
either. Statistics shown in Table II show the numbers after
preprocessing.
B. Evaluation Metrics
To evaluate the methods, we make use of three well known
clustering quality metrics, namely; purity, adjusted rand index
and normalized mutual information.
Purity can be formally defined as;
k

P urity =

1X
maxj |Ci ∩ lj |
n i=1

where k is the number of communities found, n is the number
of instances, lj is the set of instances which belong to the class
j, and Ci is the set of instances that are members of community
i.
Adjusted Rand Index [14] can be formally defined as;
ARI =

RI − E[RI]
max(RI) − E[RI]

where
RI =

s + s0

n
2

s is the number of pairs which belong to both same groundtruth class and identified community. s0 is the number of
pairs which belong to both different ground-truth classes and
identified communities. It evaluates the similarity of groundtruth class labels and clustering result.
Normalized Mutual Information can be formally defined
as;
 P (j, i) 
P|l| P|C|
P
(j,
i)log
j=1
i=1
P (i)P (j)
p
NMI =
H(l)H(C)

where, H(l) and H(C) are the entropy of class and community
assignments of l and C. P (j, i) is the probability that randomly
picked user has class label j and belongs to the community i
while P (j) gives the probability of randomly picked user to
be in class j and similarly P (i) to be in community i.
C. Baseline Algorithms
As a baseline to evaluate the performance of using both
connectivity and content information, we design experiments
with connectivity-only and content-only clustering methods.
For connectivity-only method, we use Louvain [18] and
CNM [19] algorithms utilizing modularity optimization over
user adjacency matrix. Modularity is defined as:
ki kj
1 X
(Aij −
)δ(ci , cj )
(13)
Q=
2m ij
2m
where δ(ci , cj ) is the Kronecker delta symbol, ci is the label
of the community to which node i is assigned, and ki is the
degree of node i.
For content-only approach, we experiment with kmeans[21] and conventional non-negative matrix factorization
algorithm [22].
For approaches employing both connectivity and content
information of users, we test GNMF [29] and NMTF [3]
algorithms besides proposed methods. GNMF algorithm is
introduced by Cai et al. to incorporate intrinsic geometric
similarity of users. We feed previously defined three types
of user connectivity graphs’ adjacency matrices as graph
regularization terms to the GNMF algorithm.
Pei et al. work in [3] applies nonnegative matrix trifactorization with regularization to Twitter data. It makes use
of user similarity, [tweet x word] and [user x word] matrices
and regularize the objective function with tweet similarity and
user connectivity matrices. Complexity of the algorithm is
O(rk(mn + mw + nw + m3 + n2 )) where r is the iteration
times. m, n, k, and w denote the number of users, messages,
features and communities.
D. Experimental Design
First set of experiments test the performance of using
connectivity-only information for community detection, labeled as the Experiment Set 1. We test Louvain and CNM
algorithms on three different types of connectivity graphs.
Second set of experiments test the performance of contentonly methods, labeled as Experiment Set 2. We test k-means
and NMF methods. Third set of experiments test the performance of methods utilizing both connectivity and content
information, labeled as Experiment Set 3. We test GNMF and
NMTF frameworks proposed by [3] as baseline algorithms,
alongside our proposed MultiNMF, TriNMF and DualNMF
methods. In user content dimension, we use DualNMF method
to test the experiment design that only uses user-word content.
We use TriNMF method to test the experiment design that
uses user-hashtag or user-domain information in combination
with the user-word information. We use MultiNMF method

TABLE V
I RELAND E XPERIMENT S ET 1 R ESULTS

to test the experiment design that uses all of user-word, userhashtag and user-domain contents. We label these experiments
as Experiment Set 3.1, 3.2 and 3.3 respectively.
E. Experimental Results
First, we present statistics of retweets without edits and user
mentions on the full and endorsement filtered user connectivity
graphs. Table III shows that retweeting without edits indeed
occurs mostly inside like-minded political camps, rather than
cross-camps. Roughly 97% of retweets in the UK data, and
88% of retweets in the Ireland data occur inside like-minded
groups, while these percentages are much lower for users mentions. Our endorsement filtered connectivity network boosts
the percentage of inner group user mentions from 83% to
97% in the UK data and from 59% to 87% in the Ireland
data evidencing that TSB rule in fact identifies positive user
mentions and retweets with edits with high accuracy.

•

Algorithm

User Graph

k

Purity

ARI

NMI

Louvain

R+M
R + ∆M
R + ∆Mw

13
31
31

.8720
.9186
.9224

.7277
.7453
.7536

.6849
.7393
.7518

CNM

R+M
R + ∆M
R + ∆Mw

10
29
29

.7016
.8333
.8333

.4509
.6426
.6426

.4720
.6381
.6381

Using endorsement filtered user connectivity graph usually gives better clustering performance compared to
using full user connectivity graph. There is a pattern of
weighted graph approach outperforming the others.
TABLE VI
UK E XPERIMENT S ET 2 R ESULTS

TABLE III
DATA ATTRIBUTES
UK

Ireland

962
28

1,652
216

Inner Group Retweet + Mention Links
Inter Group Retweet + Mention Links

1,986
398

3,056
2,092

Inner Group Retweet + ∆Mention Links
Inter Group Retweet + ∆Mention Links

1,456
40

2,820
432

Inner Group Retweet Links
Inter Group Retweet Links

We run each experiment 20 times for every method and pick
the maximum score achieved for reporting. Each regularizer
parameter (α, γ, θ, β) are experimented with values 1, 10,
100 and 1000. Best accuracies are usually reached with
experiments in which α and β equal to 10 or 100 while
γ and θ equal to 1. This shows the contribution of user
connectivity and word similarity regularizers, and considerably
lower contributions of hashtag and domain name regularizers
towards overall performance of the algorithms.

Algorithm

User Content

Purity

ARI

NMI

k-Means
NMF

user x word
user x word

.6738
.6395

.2378
.1541

.2018
.1709

TABLE VII
I RELAND E XPERIMENT S ET 2 R ESULTS
Algorithm

User Content

Purity

ARI

NMI

k-Means
NMF

user x word
user x word

.4651
.4186

.0488
.0434

.1672
.1139

Experiment Set 2 indicates that word usage-only based
clustering yields considerably lower accuracies compared to
user connectivity-only based clustering.
TABLE VIII
UK E XPERIMENT S ET 3 R ESULTS
Algorithm

User Graph

User Content

Purity

ARI

NMI

R+M
R + ∆M
R + ∆Mw

user x word

GNMF[29]

.7854
.8069
.8326

.4955
.6099
.6469

.4120
.4922
.5461

NMTF[3]

R+M
R + ∆M
R + ∆Mw

user x word,
tweet x word

.8197
.8112
.8412

.6448
.5657
.5331

.2593
.2471
.3751

TABLE IV
UK E XPERIMENT S ET 1 R ESULTS
Algorithm

User Graph

k

Purity

ARI

NMI

Louvain

R+M
R + ∆M
R + ∆Mw

20
42
42

.9313
.9613
.9484

.4661
.3691
.4291

.5854
.5916
.5916

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain

.7597
.7940
.8283

.3707
.5566
.6375

.3158
.4595
.5006

CNM

R+M
R + ∆M
R + ∆Mw

17
41
41

.8498
.9700
.9700

.5656
.6150
.6150

.5257
.6496
.6496

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x hashtag

.7897
.8112
.7768

.5232
.4640
.5001

.4320
.3780
.3837

MultiNMF

R+M
R + ∆M
R + ∆Mw

user x word
user x domain,
user x hashtag

.7554
.8112
.8112

.4025
.5726
.6108

.3343
.4404
.4978

R+M
R + ∆M
R + ∆Mw

user x word

DualNMF

.8326
.8927
.8970

.5674
.7291
.7616

.5146
.6086
.6380

Major findings for Experiment Set 1 can be summarized as
follows:
• Relatively larger clustering scores occur due to artificially
large number of clusters that are found. Considering the
number of users in both datasets, the number of clusters
identified in Experiment Set 1 are not practical for use
(e.g. 29 clusters in Ireland data for 7 political parties).

Major findings from Experiment Set 3 can be summarized
as follows;

TABLE IX
I RELAND E XPERIMENT S ET 3 R ESULTS

VI. C ONCLUSION

Algorithm

User Graph

Content

Purity

ARI

NMI

R+M
R + ∆M
R + ∆Mw

user x word

GNMF[29]

.5543
.6279
.8178

.2447
.4557
.6978

.2881
.4652
.6399

NMTF[3]

R+M
R + ∆M
R + ∆Mw

user x word,
tweet x word

.5969
.6860
.7597

.3119
.3986
.5198

.2144
.2384
.4469

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain

.7209
.7946
.8101

.5051
.6045
.6807

.5237
.5313
.6372

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x hashtag

.6938
.7016
.8062

.4202
.5300
.6784

.4431
.4224
.6885

MultiNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain,
user x hashtag

.7481
.6744
.8178

.4777
.4597
.6953

.4938
.4219
.6411

R+M
R + ∆M
R + ∆Mw

user x word

DualNMF

.7364
.7597
.8721

.5561
.6292
.7536

.5397
.6029
.7096

TABLE X
C OMPARISON OF M ETHODS FOR E XPERIMENT S ET 3

Content

•

•

•

•

•

3word
word,
hashtag or domain
word,
hashtag and domain

Connectivity
R+M
3R + ∆Mw
DualNMF
33DualNMF
TriNMF

3TriNMF

MultiNMF

3MultiNMF

Regardless of the experiment set and algorithms used,
endorsement filtered user connectivity graph yields higher
accuracy clustering performance compared to using the
full connectivity graph. Usually weighted graph approach
outperforms the others.
DualNMF method which factorizes user-word matrix
alongside user connectivity and word similarity regularizers yields the highest accuracy clustering performance.
We get much higher scores of clustering accuracy in
Experiment Set 3 compared to Experiment Set 2. Regularizing content-only methods with user connectivity
graphs(GNMF [29]), dramatically increases the quality
of the clustering. DualNMF which incorporates keyword
similarity regularization to GNMF further boosts the
quality of clustering.
Compared to DualNMF method, including tweet messages for NMTF method proposed in [3] does not help to
further improve the clustering quality, while it increases
complexity dramatically. DualNMF provides 9% additional purity, 46% additional ARI score while doubling
the NMI score compared to the baseline NMTF method
of Pei et al. in [3].
Compared to DualNMF method, utilizing hashtag and/or
domain usage information (i.e. TriNMF and MultiNMF)
do not contribute to the overall clustering quality.

In Twitter, content and endorsement filtered connectivity
are complementary to each other in clustering politically
motivated users into pure political communities. Word usage is
the strongest indicator of user’s political orientation among all
content categories. Incorporating user-word matrix and word
similarity regularizer provides the missing link in connectivityonly methods which suffers from detection of artificially large
number of clusters in sparse Twitter networks. Our future work
includes parallel distributed evolutionary community detection
and identification of emerging coalitions and conflicts among
communities.
ACKNOWLEDGMENT
This research was supported by ONR Grants N00014-16-12386 and N00014-15-1-2722.
R EFERENCES
[1] Howard, Philip N., and Aiden Duffy, Deen Freelon, Muzammil Hussain,
Will Mari, and Marwa Mazaid. Opening Closed Regimes: What Was the
Role of Social Media During the Arab Spring? Project on Information
Technology and Political Islam Data Memo 2011.1. Seattle: University
of Washington, 2011.
[2] Girvan,M., Newman M.E.J. (2002). Community structure in social and
biological networks, Proceedings of the National Academy of Sciences,
99(12), pp.7821-7826.
[3] Yulong Pei, Nilanjan Chakraborty, and Katia Sycara. 2015. Nonnegative
matrix tri-factorization with graph regularization for community detection
in social networks. In Proceedings of the 24th International Conference on
Artificial Intelligence (IJCAI’15), Qiang Yang and Michael Wooldridge
(Eds.). AAAI Press 2083-2089.
[4] J. Tang, X. Wang, and H. Liu. Integrating Social Media Data for
Community Detection. In Modeling and Mining Ubiquitous Social Media,
2012.
[5] Mrinmaya Sachan, Danish Contractor, Tanveer A. Faruquie, and L.
Venkata Subramaniam. 2012. Using content and interactions for discovering communities in social networks. In Proceedings of the 21st
international conference on World Wide Web (WWW ’12). ACM, New
York, NY, USA, 331-340.
[6] Y. Ruan, D. Fuhry, and S. Parthasarathy. Efficient community detection
in large networks using content and links. In WWW 13, 2013.
[7] Tufekci, Z. (2014). Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls. In: International
AAAI Conference on Weblogs and Social Media.
[8] M. D. Conover, B. Goncalves, J. Ratkiewicz, M. Francisco, A. Flammini,
and F. Menczer, Political polarization on Twitter, in Proceedings of the
5th InternationalConference on Weblogs and Social Media, 2011
[9] Sandra Gonzlez-Bailn, Ning Wang, Alejandro Rivero, Javier BorgeHolthoefer, Yamir Moreno, Assessing the bias in samples of large online
networks, Social Networks, Volume 38, July 2014, Pages 16-27, ISSN
0378-8733, http://dx.doi.org/10.1016/j.socnet.2014.01.004.
[10] S. A. Myers, A. Sharma, P. Gupta, and J. Lin. Information network
or social network? The structure of the Twitter follow graph. WWW
Companion, 2014
[11] Heider F. The psychology of interpersonal relations. New York: Wiley,
1958. 322 p.
[12] Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and Mung Chiang.
2013. Quantifying political leaning from tweets and retweets. In Proceedings of the International AAAI Conference on Weblogs and Social Media
(ICWSM).
[13] D. Boyd, S. Golder and G. Lotan, ”Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter,” System Sciences (HICSS),
2010 43rd Hawaii International Conference on, Honolulu, HI, 2010, pp.
1-10. doi: 10.1109/HICSS.2010.412
[14] Hubert, L., Arabie, P. (1985). Comparing partitions. Journal of Classification, 2, 193218.
[15] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, Cambridge, 2004.

[16] M. Newman. 2006. Modularity and community structure in networks.
Proceedings of the National Academy of Sciences, vol. 103, no. 23, pp.
85778582.

A PPENDIX
D ERIVATION OF E QUATIONS 2, 3, 4, 5

[17] S. Fortunato. 2010. Community detection in graphs. Physics Reports,
vol. 486, pp. 75174

To follow the conventional theory of constrained optimization we rewrite objective function 1 as;

[18] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne
Lefebvre. Fast unfolding of communities in large networks. Journal of
Statistical Mechanics: Theory and Experiment 2008 (10), P10008 (12pp)
doi: 10.1088/1742-5468/2008/10/P10008.
[19] A. Clauset, M. Newman, and C. Moore, Finding community structure in very large networks, Physical Review E, vol. 70, p. 066111, 2004. [Online]. Available:
http://www.citebase.org/cgibin/citations?id=oai:arXiv.org:condmat/0408187
[20] Waltman, L., Van Eck, N. J.. 2013. A smart local moving algorithm
for large-scale modularity-based community detection. European Physical
Journal B, 86, 471.
[21] Lloyd., S. P. (1982). ”Least squares quantization in PCM”
(PDF). IEEE Transactions on Information Theory 28 (2): 129137.
doi:10.1109/TIT.1982.1056489
[22] Daniel D. Lee, H. Sebastian Seung. 2000. Algorithms for Non-negative
Matrix Factorization. In Neural Information Processing Systems (NIPS),
Vol. 13 , pp. 556-562.

JU,H,D,W = T r((Xuw − UWT )(Xuw − UWT )T )
+ T r((Xuh − UHT )(Xuh − UHT )T )
+ T r((Xud − UDT )(Xud − UDT )T )
+ αT r(UT LC U) + γT r(HT LHsim H)
+ θT r(DT LDsim D) + βT r(WT LWsim W)
JU,H,D,W = T r(Xuw XTuw ) − 2T r(Xuw WUT )
+ T r(UWT WUT ) + T r(Xuh XTuh )
− 2T r(Xuh HUT ) + T r(UHT HUT )
+ T r(Xud XTud ) − 2T r(Xud DUT ) + T r(UDT DUT )
+ αT r(UT LC U) + γT r(HT LHsim H)
+ θT r(DT LDsim D) + βT r(WT LWsim W)

[23] C. J. Lin. On the Convergence of Multiplicative Update Algorithms for Nonnegative Matrix Factorization. In IEEE Transactions on
Neural Networks, vol. 18, no. 6, pp. 1589-1596, Nov. 2007. doi:
10.1109/TNN.2007.895831

Let Φ, η, Ω and Ψ be the Lagrangian multipliers for constraints
U, H, D, W > 0 respectively. So the Lagrangian function L
becomes;

[24] Yuan Yao, Hanghang Tong, Guo Yan, Feng Xu, Xiang Zhang,
Boleslaw K. Szymanski, and Jian Lu. 2014. Dual-Regularized OneClass Collaborative Filtering. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge
Management (CIKM ’14). ACM, New York, NY, USA, 759-768.
DOI=http://dx.doi.org/10.1145/2661829.2662042

L = T r(Xuw XTuw ) − 2T r(Xuw WUT ) + T r(UWT WUT )

[25] Linhong Zhu, Aram Galstyan, James Cheng, and Kristina Lerman.
2014. Tripartite graph clustering for dynamic sentiment analysis on
social media. In Proceedings of the 2014 ACM SIGMOD International
Conference on Management of Data (SIGMOD ’14). ACM, New York,
NY, USA, 1531-1542. DOI=http://dx.doi.org/10.1145/2588555.2593682
[26] Quanquan Gu and Jie Zhou. 2009. Co-clustering on manifolds. In
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ’09). ACM, New York,
NY, USA, 359-368. DOI=http://dx.doi.org/10.1145/1557019.1557063
[27] Fanhua Shang, L. C. Jiao, and Fei Wang. 2012. Graph
dual regularization non-negative matrix factorization for coclustering. Pattern Recogn. 45, 6 (June 2012), 2237-2250.
DOI=http://dx.doi.org/10.1016/j.patcog.2011.12.015
[28] Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal
nonnegative matrix t-factorizations for clustering. In Proceedings of the
12th ACM SIGKDD international conference on Knowledge discovery
and data mining (KDD ’06). ACM, New York, NY, USA, 126-135.
DOI=http://dx.doi.org/10.1145/1150402.1150420
[29] D. Cai, X. He, J. Han and T. S. Huang, Graph Regularized Nonnegative
Matrix Factorization for Data Representation, in IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548-1560,
Aug. 2011. doi: 10.1109/TPAMI.2010.231
[30] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based
on non-negative matrix factorization. In Proceedings of the 26th annual
international ACM SIGIR conference on Research and development in
informaion retrieval (SIGIR ’03). ACM, New York, NY, USA, 267-273.
DOI=http://dx.doi.org/10.1145/860435.860485
[31] Zhou, Quanquan Gu Jie. Local learning regularized nonnegative matrix
factorization. IJCAI 2009, Proceedings of the 21st International Joint
Conference on Artificial Intelligence, Pasadena, California, USA, July
11-17, 2009
[32] Linhong Zhu, Aram Galstyan, James Cheng, and Kristina Lerman. Tripartite graph clustering for dynamic sentiment analysis on social media.
In Proceedings of the 2014 ACM SIGMOD International Conference on
Management of Data, pages 1531-1542. ACM, 2014.

+ T r(Xuh XTuh ) − 2T r(Xuh HUT ) + T r(UHT HUT )
+ T r(Xud XTud ) − 2T r(Xud DUT ) + T r(UDT DUT )
+ αT r(UT LC U) + γT r(HT LHsim H) + θT r(DT LDsim D)
+ βT r(WT LWsim W) + T r(ΦUT ) + T r(ηHT )
+ T r(ΩDT ) + T r(ΨWT )
The partial derivatives of Lagrangian function L with respect
to U, H, D, W are as follows;
∂L
= −2Xuw W + 2UWT W − 2Xuh H + 2UHT H−
∂U
2Xud D + 2UDT D + 2αLC U + Φ
∂L
= −2XTuh H + 2UHT H + 2γLHsim H + η
∂H
∂L
= −2XTud H + 2UDT D + 2θLDsim D + Ω
∂D
∂L
= −2XTuw U + 2WUT U + 2βLWsim W + Ψ
∂W
Setting derivatives equal to zero and using KKT complementarity conditions [15] of nonnegativity of matrices
U, H, D, W, ΦU = 0, ηH = 0, ΩD = 0 and ΨW = 0,
we get the update rules given in Equations 2, 3, 4, 5.

1

Web Intelligence and Agent Systems: An International Journal 5 (2016) 1–5
IOS Press

Directional Prediction of Stock Prices using
Breaking News on Twitter
Hana Alostad a , Hasan Davulcu a
a

School of Computing, Informatics and Decision Systems Engineering, Arizona State University
Tempe, Arizona 85287–8809
E-mail: {Hana.Alostad,Hasan.Davulcu}@asu.edu

Abstract. Stock market news and investing tips are popular topics in Twitter. In this paper, first we utilize a 5-year financial
news corpus comprising over 50,000 articles collected from the NASDAQ website matching the 30 stock symbols in Dow Jones
Index (DJI) to train a directional stock price prediction system based on news content. Next, we proceed to show that information
in articles indicated by breaking Tweet volumes leads to a statistically significant boost in the hourly directional prediction
accuracies for the DJI stock prices mentioned in these articles. Secondly, we show that using document-level sentiment extraction
does not yield a statistically significant boost in the directional predictive accuracies in the presence of other 1-gram keyword
features. Thirdly we test the performance of our system on several time-frames and identify the 4 hour time-frame for both the
price charts and for Tweet breakout detection as the best time-frame combination. Finally, we develop a set of price momentum
based trade exit rules to cut losing trades early and to allow the winning trades run longer. We show that the Tweet volume
breakout based trading system with the price momentum based exit rules not only improves the winning accuracy and the return
on investment, but it also lowers the maximum drawdown and achieves the highest overall return over maximum drawdown.
Keywords: stock prediction, breaking news mining, Twitter analysis, Twitter volume spike, stock trading systems

1. Introduction
Online social networks, like Twitter, are enabling
people who are passionate about trading and investing
to break critical financial news faster and they also go
deep into relevant areas of research and sources leading to real-time insights. Recently Twitter has been
used to detect and forecast civil unrest [12], criminal
incidents [30], box-office revenues of movies [9], and
seasonal influenza [8].
Stock market news and investing tips are popular
topics in Twitter. In this paper, first we utilize a 5-year
financial news corpus comprising over 50,000 articles
collected from the NASDAQ website for the 30 stock
symbols in Dow Jones Index (DJI) to train a directional
stock price prediction system based on news content.
Next we collect over 750,000 Tweets during a 6 month
period in 2014 that mention at least one of the 30 DJI
stock symbols. We utilize the 68-95-99.7 rule, also
known as the three-sigma rule or empirical rule [25], to

define a simple method for detecting hourly stock symbol related Tweet volume breakouts. Then we proceed
to test our hypothesis to determine if “information in
articles indicated by breaking Tweet volumes will lead
to a statistically significant boost in the hourly directional prediction accuracies for the prices of DJI stocks
mentioned in these articles".
The contributions of the paper can be summarized
as follows:

c 2016 – IOS Press and the authors. All rights reserved
1570-1263/16/$17.00 

– Firstly, we show that sparse logistic regression
[19] for this text based classification task with 1gram keyword features filtered by a Chi2 [18] feature selection algorithm lead to the best overall directional prediction accuracy among a set of other
classifiers and feature sets that we tested.
– Secondly, we show that using document-level
sentiment extraction does not yield to a statistically significant boost in the predictive accuracies
in the presence of other 1-gram keyword features.

2

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter
Table 1
Summary of Previous Research Results
Period

Prediction

Algorithm

Accuracy

Tweets

Time-frame

Online News

Data set
Stock Price

Reference

[24]

3

7

7

Daily

9 Yrs

Direction

Naive Bayes

90%

[13]

3

7

7

Monthly

2 Yrs

Direction

Logistic Regression

83%

[26]

3

7

7

Daily

13 Yrs

Price

Linear Regression

2.54 (RMSE)

[23]

3

3

7

2 Hrs

4 Yrs

Direction

SVM

83%

[14]

3

3

7

Daily

14 Yrs

Direction

SVM

79%

[15]

3

3

7

Daily

1 Year

Direction

SVM

61%

[27]

3

3

7

20 Min

1 Mo

Price

SVR

51%,

[16]

3

3

7

Daily

1 Year

Direction

Neural Network

3.70 (RMSE)

[10]

3

7

3

Daily

10 Mos

Direction

Neural Network

88%

[29]

3

7

3

Daily

2 Mos

Direction

Decision Tree

77%

[22]

3

7

3

Daily

3 Mos

Direction

Liner regression

68%

[21]

3

7

3

Daily

1 Year

Price

Bayesian

0.3% (daily)

– Thirdly, we show that information in articles indicated by Tweet volume breakouts leads to a
statistically significant boost in the hourly directional prediction accuracies for the DJI stocks
mentioned in the articles linked by Tweets.
– Fourthly, we compare the performance of the
breaking Tweet volumes based trading system
on different time-frames. We identify the 4 hour
time-frame for both price charts and for Tweet
volume breakouts detection as the best timeframe.
– Finally, we develop a set of price momentum
based trade exit rules to cut losing trades early
and to allow the winning trades run longer. We
show that the Tweet volume breakouts based trading system with the momentum based trade exit
rules not only improves the average winning accuracy and the return on investment, but it also
lowers the maximum drawdown and yields the
highest overall return over maximum drawdown
(RoMaD).
The rest of the paper is organized as follows. Section
2 presents related work. Section 3 presents the problem
definition for the directional prediction of stock prices.
The design of experiments to evaluate the performance
of various trading systems and strategies are presented

in Section 4. Section 5 describes the experimental data
we used and the simulated financial backtesting results
for the experiments. Section 6 concludes the paper and
discusses future work.

2. Related Work
Table 1 contains a summary of previous research
findings related to stock price or direction prediction;
the input data sets used, the time-frames used for prediction, the length of the period of collected data, prediction algorithms used, and the resulting overall accuracies.
These systems have different prediction time-frames
and goals. Some of them predict stock price for the
intended time-frame like [26], [27] and [21]. Time
frames vary between next 20 minutes to up to next
month. Works such as [10], [14], [15], [16], [22] , [24],
and [29] predict stock price direction for the next day.
[23] aims to predict the price direction every 2-hours,
and [13] aims to predict monthly direction.
Related systems collected their input data from various sources and exchanges: [27], [22], and [21] collected stock news, Tweets and price charts related to
S&P 500 companies. [29] collected Tweets and stock
price data related to Nasdaq stocks, [10] collected

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

Tweets and stock price charts related to Dow Jones
Industrial Average (DJIA), [15] collected one year
of data related to Microsoft company. [13] collected
stock price charts from Shenzhen Development Stock
A (SDSA) exchange. [23] collected currency price and
news data related to foreign exchange market (Forex).
[24] collected stock price data from CNX Nifty, S&P
BSE Sensex exchanges and finally [26] collected thirteen years of stock price charts data related to Goldman Sachs Group Inc.
[13], [24], and [26] used only stock price as input to
predict stock price or direction with accuracies varying
between 83% and 90%. [14], [15], [16], and [23] are
examples of papers which utilize news as well as stock
prices to predict price direction with varying accuracies between 51% and 83%.
[22] made correlation analysis between the stock
price and the Tweet volume, and used it to predict
stock market direction with 68% accuracy. Following
work by [21] analyzed Tweet spikes in combination
with price action based technical indicators such as
price breakout direction as an input to a Bayesian classifier for stock price prediction, yielding a daily average gain of approximately 0.3% during a period of
55 days generating a total gain of 15%. [10] used extracted sentiment information from Twitter data and a
neural network classifier to predict Dow Jones Industrial average (DJIA) daily price direction with 88% accuracy. [29] also used sentiment information extracted
from Twitter as input to a decision tree classifier to
predict price direction for four companies in NASDAQ stock exchange with average accuracy of 77%
distributed as APPL at 77%, GOOG at 77%, MSFT at
69% and AMZN at 85% during a two months period
of evaluations.

3. Problem Definition
The correction effect of online news articles covering company related events, announcements and technical analyst reports on the stock price may take some
time to show. Depending on the severity and impact of
the news announcement this period may vary between
few minutes to an hour, and the effect may sometimes
determine the trend direction of the financial instrument for upcoming weeks or months.
One way to measure the impact of news on a stock
price is to analyze the trading volume following the
news announcement. Another indicator of news impact
is the diffusion rates and volumes of messages on so-

3

cial media containing the stock symbol and news links
of interest.
Twitter provides a suitable platform to investigate
properties of such information diffusion. Diffusion
analysis can harness social media to investigate “viral
Tweets” to create early-warning indicators that can signal if a breakout started to emerge in its nascent stages.
In this paper, we utilize the 68-95-99.7 rule to define
a simple method of Tweet volume breakouts. In statistics, the 68-95-99.7 rule, also known as the three-sigma
rule or empirical rule [25], states that nearly all values
lie within three standard deviations (σ) of the mean (µ)
in a normal distribution. We utilize a fixed sized sliding window (of length 20 hour intervals that was determined experimentally), to compute a running average and standard deviation for the hourly volumes of
Tweets that mention a stock symbol. Then, we identify
breakout signals within a time-series of hourly Tweet
volumes for each stock symbol whenever its hourly
volume exceeds (µ(20) + 2σ) of the previous 20 hour
periods. We consider a breakout as an indication that
traders or technical analysts are sharing some exciting or important new information regarding the company or a group of companies. Next, we collect the
URL links mentioned within the breaking-news hour
of Tweets and we designed a pair of experiments to test
the hypothesis whether “information in news indicated
by breaking Tweet volumes will lead to statistically
significant boost in the directional prediction accuracy
for the prices of the related stock symbols mentioned
in these articles”.
Our system has the following characteristics:
1. Input Data: Hourly stock price charts of the 30
stocks comprising the Dow Jones Index (DJI), online stock news articles for a 5 year period spanning 2010 and 2014 from NASDAQ1 news website, the Tweets related the 30 stock symbols collected from Twitter Streaming API2 spanning a 6
months period between March 2014 and September 2014, and online news articles mentioned in
Tweets during breaking news hours.
2. Prediction Time-Frame: The collected data is analyzed and predictions are made on hourly bases.
3. Prediction Goal: To predict the hourly price direction for the stocks mentioned in Tweets during
breaking news hours.
The distinguishing features of our system compared
to systems mentioned in the related work section are:
1 http://www.nasdaq.com/symbol/ibm/news-headlines
2 https://dev.twitter.com/streaming/overview

4

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

Fig. 1. Illustration of System Architecture of Experiment-1

Fig. 2. Illustration of System Architecture of Experiment-2

(1) [21] used Tweeter volume spikes alongside stock
price-based technical indicators for stock price turning point prediction where as our system utilizes textual content of the news mentioned in Tweets during
breaking Twitter volume hours to predict the hourly direction of the stock price following a breakout period.
(2) [10] and [29] used extracted sentiment information
alongside stock price-based technical indicators to determine if sentiment information leads to a boost in the
predicted direction accuracy. Our system primarily relies on textual content of the news linked from breaking Tweet volumes to predict the direction of the stock
price in the next hour. We also experimented with extracted sentiment as an additional feature to determine
if it leads to a boost in the overall prediction accuracy. Unlike [10] and [29], our system did not experience a statistically significant boost in predictive accuracies as a result of including sentiment information
alongside other textual content features. [10]’s accuracy is not comparable to ours since they are reporting
the daily directional prediction accuracy for the Dow
Jones Index Average (DJIA). Compared to predictive
accuracies for four companies listed in [29], we have
only one stock in common with their experiments, i.e.
MSFT, where their system reported a daily directional
predictive accuracy of 69% and our system reported an
hourly directional accuracy of 82%.

4. Design of Experiments
In order to test the hypothesis that “information in
news indicated by breaking Tweet volumes will lead to
statistically significant boost in the directional prediction accuracy for the prices of the relevant stock sym-

bols mentioned in such articles", we designed two experiments. In the first experiment we trained a classifier using all stock news articles for a 5 year period
spanning 2010 and 2014 from NASDAQ news website. Figure 1 illustrates the system architecture used
for the first experiment. For comparison purposes we
experimented with three different types of features extracted from text: 1-gram keywords, 2-gram phrases,
and bi-polar sentiment (i.e. positive and negative) extracted from text. We grouped news hourly, and categorized each hourly collection as one of two categories: (1) those that led to an increased stock price or
(2) those that led to a price reduction during the next
hour. Next, we applied a feature selection method to
reduce the number of features to only relevant ones.
The details of these steps are presented in Section 4.1.
Finally we experimented with two types of text classifiers and evaluated their directional predictive accuracy using 10-fold cross validation. The results of the
first experiment utilizing all stock news for all 30 company stocks are reported in Section 5.2. In our second
experiment, we tested the directional predictive accuracy of our classifier (i.e. trained in the first experiment above) using only online articles collected during hourly breaking Tweet volume periods. Figure 2
illustrates the system architecture used for our second
experiment. Steps involved in the second experiment
were hourly profiling of the Tweets mentioning a stock
symbol, detection of Tweet volume breakout periods,
collection of online news mentioned in Tweets during
the breaking hours, feature extraction from news, and
running of the classifier to predict the stock price direction of the next hour using the collected news content. We compared the accuracies of the classifiers in
both first and second experiments to test the validity of

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

our hypothesis. The details of the steps involved in the
second experiment are explained in Section 4.2, and
the experimental results and evaluations are presented
in Section 5.3.
4.1. Experiment-1: Hourly Price Direction
Prediction using Online News
The following is a detailed description of each step
used in Experiment-1:
1. One Hour Stock Chart: We collected hourly stock
financial price charts for all the companies comprising the Dow Jones Index (DJI) using an API
from ActiveTick 3 . For each trading hour the price
direction was calculated based on the difference
between hourly Open and Close prices according
to the Formula 1 below, where d represents the
trading date and h represents the trading hour:

Dir(d, h) =

1 if Open(d, h) ≤ Close(d, h) )
−1 otherwise
(1)

2. Hourly News: We used Web Content Extractor4
to collect online news articles from NASDAQ
website. We stored all metadata information related to the articles like their title, url, date, time,
and source in a database table. We fetched the
news content using their urls and performed content extraction using Boilerpipe5 .
3. Feature Extraction:
• N-gram Features from News: R for Text Mining(TM)6 package was used to extract keyword
features from the news corpus. First all whitespaces, stop words, numbers, punctuation were removed from the documents, then all the terms
were converted to lowercase and stemmed into
their root words. Next features were recorded in
a document-term matrix. For each stock symbol
we created a pair of document-term matrices: one
with 1-gram features and another with 2-gram
features represented in a binary form. We used
R.Matlab7 package to create Matlab format files
for these matrices.

5

• Sentiment Features: To detect sentiment in news
content we used a Java version of SentiStrength
library8 . SentiStrength is a classifier that uses a
predefined sentiment word list with human polarity and strength judgments, then it applies
rules to detect sentiment in short text [28]. [20]
showed that using general word lists for sentiment analysis of large financial text leads into
mis-classification of common words in the financial domain. So alongside SentiStrenght dictionary [20] we also used Loughran and McDonald Financial Sentiment Dictionaries9 to compute sentiment. Besides using different sentiment
word lists, we also need to get the sentiment for
each document. We used OpenNLP10 Sentence
Detector to extract sentences mentioning a stock
symbol from each document, and then we applied
the SentiStrenght classifier on each sentence. We
determined the majority polarity for the sentences
contained in a document and used the majority
polarity (i.e. positive or negative) as the sentiment
for each stock symbol mentioned in the document.
4. Feature Selection: Feature selection in text mining reduces the number of features to only relevant and discriminative set of features. We used
Chi2 [18] feature selection algorithm from a feature selection package 11 . Chi2 is a two phase general algorithm that automatically selects a proper
critical value for statistical χ2 test and then it removes all irrelevant and redundant features [18].
5. News Labeling: Figure 3 is an illustration of
the news labeling step. In this phase we used the
stock price direction of the following hour to categorize the directionality of the hourly collections
of news articles. In order to align the news article
hours with the stock chart hours we had to standardize and adjust their time zones. Formula 2 is
used to label the news articles where d represents
the publishing date, and h represents the publishing hour.
Label(d, h) = Dir(d, N ext(h))

(2)

In this paper we initially assume that the effect
of published news articles will be reflected on the

3 http://www.activetick.com/
4 http://www.newprosoft.com/

8 http://sentistrength.wlv.ac.uk/

5 http://code.google.com/p/boilerpipe/

9 http://www3.nd.edu/∼mcdonald/Word_Lists.html

6 http://cran.r-project.org/web/packages/tm/index.html

10 https://opennlp.apache.org/

7 http://cran.r-project.org/web/packages/R.matlab/index.html

11 http://featureselection.asu.edu/software.php

6

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

Fig. 3. An Illustration of News Labeling

stock price direction during the next hour. Later
we relax this assumption and test the system for
all time periods varying between 5 mins up to
4 hours. Formula 2 applies to all the news articles published during official trading hours which
starts at 9AM and ends on 3PM in EST time zone.
For articles that are published during the last trading hour, or after trading hours, or during holidays
and weekends we assumed that their effect will
be seen on the direction of the first trading hour
of the next trading day. For this case Formula 3 is
used to label those news articles.
Label(d, h) = Dir(N ext(d), F irst(h)) (3)
6. Classifier: We formulate price direction prediction problem as a classification problem in a general structured sparse learning framework [19].
In particular, the logistical regression formulation
presented below fits this application, since it is a
dichotomous classification problem (e.g. upwards
vs. downwards price correction), In the formula 4,
ai is the vector representation of the news during
the ith hour, wi is the weight assigned to the ith
document (wi =1/m by default), and A=[a1 , a2 ,
âĂ˛e, am ] is the document n-gram matrix, yi is the
directionality of each hour based upon the stock
price action of the next hour, and the unknown xj
, the j-th element of x, is the weight for each ngram feature, λ > 0 is a regularization parameter that controls the sparsity of the solution, |x|1
= Σ|xi | is 1-norm of the x vector. We used the
SLEP [19] sparse learning package that utilizes

gradient descent approach to solve the above convex and non-smooth optimization problem. The
n-grams with non-zero values on the sparse x vector yield the discriminant factors for classifying a
news collection as leading to upwards or downwards price correction. n-grams with positive polarity correspond to upward direction indicators,
and those with negative polarity correspond to
downward direction indicators.

minx

n
X

wi log(1 + exp(1 + yi (xt ai + c)))+λ|x|

i=1

(4)
We also utilized an SVM classifier in our experiments using LIBSVM12 library.
7. 10-fold cross validation: We run a total of 8 experiments for each stock symbol where we experimented: (1) with SVM and sparse logistic regression classifiers, (2) with 1-gram and 2-gram
features, and (3) with and without extracted sentiment features. After the training phase of the
classifier, we validated the accuracies using 10fold cross validation. The evaluation results for
the first experiment are presented in Section 5.2.

12 http://www.csie.ntu.edu.tw/∼cjlin/libsvm/

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

7

Fig. 4. An Illustration of Breaking Tweets

4.2. Experiment-2: Hourly Price Direction
Prediction using Breaking News
We selected the classifier with the best performance
emerging from Experiment-1 to use in Experiment-2.
Experiment-2 was designed to test if the online news
indicated by breaking Tweet volumes would lead to a
statistically significant boost in the directional prediction accuracy for the prices of the relevant stock symbols mentioned in such news. The system architecture
figure in Figure 2 shows the steps used in this experiment. The following is a detailed description of each
step:
1. Twitter Stock Symbol Feed: Twitter streaming
API was used to collect Tweets related to companies in the Dow Jones Index (DJI). In order to collect relevant Tweets we used a keyword filter made from the stock symbols, either prefixed by a dollar sign ($) or prefixed by
“NYSE:" or “NASDAQ:". For example, the keyword filter for Microsoft Corporation are $MSFT
and NYSE:MSFT. For each matching Tweet we
stored the stock symbol, Tweet text, date, time,
and the set of URLs mentioned in the Tweet. If the
Tweet text contained more than one stock symbol
then we stored the same Tweet information for
each mentioned stock symbol.
2. Hourly Tweets Volume Profiling: We utilize a
fixed sized sliding window (of length 20 hour intervals) where the 20 hour intervals was determined by conducting several experiments with
different intervals, to compute a running average

µ[20] and standard deviation σ for the hourly volumes of Tweets that mention a stock symbol.
3. Tweets Volume Breakout Hour: We identify breakout signals within a time-series of hourly Tweet
volumes for each stock symbol using Formula 5.


Breakout =

T rue if N (d, h) ≥ µ[20](d, h) + 2σ(d, h))
F alse otherwise
(5)

In Formula 5, N represents Tweet volume on
specific date d, and hour h. µ[20] is 20-hour simple moving average applied on Tweets’ volume,
µ[20](d, h) + 2σ(d, h) represents the upper band
for simple moving average - a 20-hour moving
average plus 2-times standard deviation. If the
volume of hourly Tweets N exceeds the upper
band value, this would indicate a volume breakout. Otherwise the Tweet volume is non-breaking.
In Figure 4, the pair of dotted arrows shows two
instances of Tweet volume breakouts at 9/5/2014
at 9AM and 9/5/2014 at 2PM, where the corresponding articles from these hours will be used
to predict the price directions of the mentioned
stocks at the following hours.
4. News From Breaking Tweets: In this step the
news content of URLs found in the Tweets during the breaking hours are downloaded and their
textual contents are extracted using the following
steps:

8

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter
Table 2
Tweets Breaking News Grouping and Prediction Time-Frames
News grouping Time-frames
4h

Price Direction Labeling Time-frames
4h

1h

30m

15m

5m

3

3

3

3

3

3

3

3

3

3

3

3

3

3

1h
30m
15m

(a) For each breaking hour of a specific stock
symbol we fetch the URLs found in Tweets
during the breaking hour, i.e. Breakout =
True. In some cases the URLs were mentioned in their short URL forms, so before
fetching them, they were converted to their
long forms.
(b) Fetch the URL links’ content and perform
content extraction from the HTML documents using the JSoup HTML parser 13 .
5. Classifier: After extracting the hourly breaking
news and their 1-gram features we utilized the logistic regression classifier to predict the price direction for the next hour.
6. Evaluation: The predictive accuracies of the news
classifier for the price direction following the
breaking Tweet volume hours are presented in
Section 5.3.
4.3. Experiment-3: Comparison Between Different
Time-Frames For Price Direction Prediction
using Breaking News
In Experiment-3 we used the same steps of the system architecture of Experiment-2 shown in Figure 2,
but instead of using the 1 hour time-frame for news
grouping and the price direction labeling, we tested the
prediction performance of the news content based classifier using all possible time-frame combinations (i.e. 4
hours, 1 hour, 30 minutes, 15 minutes, and 5 minutes)
for both news grouping and news price effects labeling.
Table 2 lists the evaluated time-frame combinations for
news grouping and news effect labeling. The goal of
this experiment is to identify the best time-frame combination that should be used for (i) news grouping and
(ii) news effect labeling which yields the highest directional stock price prediction accuracy. The findings of
Experiment-3 is presented in Section 5.4.
13 http://jsoup.org/

4.4. Experiment-4: Tweets Volume Breakout based
Trading System
We selected the classifier with the best performance
emerging from Experiment-3 for use in Experiment4. Experiment-4 is designed to test if prediction with
the online news linked from breaking Tweet volumes
would lead to a higher performance directional stock
price prediction system. Figure 5 shows the flowchart
of the proposed system for Experiment-4. The following are the trade entry and exit rules that we used:
1. ENTER a trade at the beginning of the next trade
period, if there was a Tweet volume breakout for
Tweets matching a stock’s symbol in the preceding time-frame.
2. The trade DIRECTION (i.e. buy or sell) is determined by the news content-based classifier
applied to the content linked from the Tweets
matching a stock’s symbol during the proceeding
Tweet volume breakout period.
3. EXIT the trade at the end of the next time-frame
period.
4. Evaluate the performance of the resulting trade
(i.e. a win or a loss) and the corresponding
amount according to the real stock price movement during the trade period.
In order to evaluate the performance of the above trading system we performed a backtesting of the system
on real data, recorded the outcomes of its trades accounting for its wins and losses, as follows:
– Percentage of winning trades (Won%)
– Percentage of long positions (Long Won%)
– Percentage of short positions (Short Won%)
– Percentage of Return On Investment (ROI%) according to Equation 6
ROI =

Gain − Cost
Cost

(6)

ROI is a performance measure used to evaluate
the efficiency of an investment or to compare the

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

9

Fig. 5. A Flowchart of Tweets Breakout Trading Strategy

efficiency of a number of different investments
[5]. The higher the value of ROI% the better.
– Percentage of Maximum Drawdown (MDD%), is
the maximum loss from a peak to a trough of a
portfolio, before a new peak is attained. It is an indicator of downside risk over a specified time period [3]. The lower the value of MDD% the better
for investment.
– Return Over Maximum Drawdown (RoMaD)
shown in Equation 7

RoM aD =

ROI
M DD

(7)

RoMad is a risk-adjusted return metric. It enables
investors to ask the question: Are they willing to
accept an occasional drawdown of X% in order to
generate an average return of Y%? [6], for example: An investment with a MDD of 20% and ROI=
10% (RoMAD = 2.0) would be considered the
more attractive investment than one with a MDD
of 50% and a ROI of 10% (RoMAD = 0.2).
The experimental results for the Tweets volume breakout based trading system are presented in Section 5.5.

4.5. Experiment-5: Price Momentum Based Trade
EXIT Rules
In this experiment we develop a set of price momentum based trade exit rules to cut losing trades early
and to allow the winning trades run longer. We apply
these price momentum based EXIT rules to the trading
system developed in Experiment-4, and compare their
performance in Section 5.6.
The rules are based on the Squeeze Momentum
Indicator [7], which is a derivative of John Carter’s
"TTM Squeeze" volatility indicator [11]. This indicator has been used to detect periods while the market is
quiet (i.e. squeeze) and the periods while the market
is volatile (i.e. price breakouts). Squeeze Momentum
Indicator is comprised of three components:
1. Bollinger Bands [2].
U pperBollingerBand = µ[20] + 2σ,
LowerBollingerBand = µ[20] − 2σ,

(8)

M iddleBollingerBand = µ[20]
where µ[20] is the average of the closing prices for the
previous 20 time-periods and σ is their standard deviation.

10

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

Fig. 6. Squeeze Momentum Indicator for AXP Stock symbol on 5 min Chart
Created using https://www.tradingview.com Charts

2. Keltner Channels [4].
U pperChanelLine = µ[20] + 2AT R(10),
LowerChanelLine = µ[20] − 2AT R(10)

(9)

where ATR [1] is defined as follows:
AT R(t) =

AT R(t − 1) × (n − 1) + T R
n

T R = max[(high − low), abs(high, closeP rev ),
abs(low − closeprev )]

where t is the current time, n=10, and true range
T R is the largest of either the most recent period’s
high minus the most recent period’s low, or the
absolute value of the most recent period’s high
minus the previous close, or the absolute value of
the most recent period’s low minus the previous
close.
3. Momentum Indicator [7].
M omentum = close[0] − µ[µ[highest[high, 20]
, lowest[low, 20]], µ[20]]
(10)

where Momentum is the difference between the
current close values to the average of the average
between highest high of the previous 20 time periods and lowest low of the previous 20 time periods, to the average of the closing prices for the
previous 20 time-periods.
Figure 6 illustrates the Squeeze Momentum Indicator components. The Squeeze Momentum Indicator is
used as follows: It signals a red dot when the Bollinger
Bands are inside of the Keltner Channel, whence the
market is said to be in a squeeze. Otherwise, it signals
a green dot signaling that the market is volatile (i.e.
price breakout). In order to determine the direction of
the volatility, we inspect the sign of the Momentum Indicator. If it is positive, then the price momentum is in
the upward direction, otherwise it is in the downward
direction.
The momentum based trade EXIT rules are defined
as follows:
– Cut Losses Early (CLE) EXIT Rule is defined in
Section 4.5.1.
– Conservative Let the Winners Run (ConsLWR)
EXIT Rule is defined in Section 4.5.2.

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

11

– Aggressive Let the Winners Run (AggLWR)
EXIT Rule is defined in Section 4.5.3
– Cut Losses Early (CLE) + Conservative Let the
Winners Run (ConsLWR) EXIT Rule is defined
in Section 4.5.4
– Cut Losses Early (CLE) + Aggressive Let the
Winners Run (AggLWR) EXIT Rule is defined in
Section 4.5.5

– Short/Sell Continuation: Momentum indicator for
the previous pair of bars are negative.
The trade is exited, using the 5 minute chart when one
of the following conditions are met:
– Long EXIT: Momentum indicator for the previous bar is negative (i.e. opposite direction)
– Short EXIT: Momentum indicator for the previous bar is positive (i.e. opposite direction)

4.5.1. Trading with Cut Losses Early (CLE) EXIT
Rule
This rule applies during the initial fixed time-period
of the trade, where we track the price action on the
5 minute chart to cut the losses early if the price is
volatile and the Momentum Indicator points to a direction that is opposite to that of the trade’s direction.
1. Stock price is volatile and not in a squeeze period i.e. TTM squeeze indicator should be off (i.e.
green)
2. The momentum indicator for the previous pair of
bars are rising in an opposite direction.
– Long EXIT: Momentum indicator for the
previous pair of bars are both negative and
declining.
– Short EXIT: Momentum indicator for the
previous pair of bars are positive and rising.

4.5.4. CLE + ConsLWR Trading Strategy
This strategy combines the CLE exit rule during the
initial time-frame with the ConsLWR rule following
the initial time-frame.

4.5.2. Trading with the Conservative Let the Winners
Run (ConsLWR) EXIT Rule
In this strategy a trade is allowed to run, past its fixed
time period, if it is in profit at the end of its fixed timeperiod and while the following conditions remain true
on the 5 minute price chart:
– Long/Buy Continuation: Momentum indicator
for the previous pair of bars are rising.
– Short/Sell Continuation: Momentum indicator for
the previous pair of bars are declining.
The trade is exited, using the 5 minute chart when one
of the following conditions are met:
– Long/Buy EXIT: Momentum indicator for the
previous pair of bars are declining.
– Short/Sell EXIT: Momentum indicator for the
previous pair of bars are rising.
4.5.3. Trading with the Aggressive Let the Winners
Run (AggLWR) EXIT Rule
In this strategy a trade is allowed to run, past its fixed
time period, if it is in profit at the end of its fixed timeperiod and while the following conditions remain true
on the 5 minute price chart:
– Long/Buy Continuation: Momentum indicator
for the previous pair of bars are positive.

4.5.5. CLE + AggLWR Trading Strategy
This strategy combines the CLE exit rule during the
initial time-frame with the AggLWR rule following the
initial time-frame.
The detailed results of financial back-testing of the
Tweets volume breakout trading system with the price
momentum based trade EXIT rules are presented in
Section 5.6.

5. Experimental Results and Evaluations
5.1. Experimental Data
We collected online news articles and stock price
charts related to 30 stock symbols in Dow Jones Index
for the period between October, 2009 and September,
2014. We also collected Tweets matching stock symbols for the period between March, 2014 and September, 2014. The total number of news articles collected
from the NASDAQ website for the 30 stock symbols
in Dow Jones Index is 53,641. The total number of collected Tweets matching 30 stock symbols is 780,139.
Table 3 shows the number of news articles, total number of collected Tweets, and the number of hourly
breakout periods for each symbol.
5.2. Experiment-1: Hourly Price Direction
Prediction using Online News
We executed the steps described in Figure 1 on data
sets collected for the 30 Dow Jones Index companies.
In order to identify the best set of text features and
the best classifier we had to perform several experiments. We run a total of 8 experiments for each stock
symbol where we experimented: (1) with SVM and
sparse logistic regression classifiers, (2) with 1-gram
and 2-gram keyword features, and (3) with and with-

12

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter
Table 3
Counts of Collected News Articles, Tweets, and Calculated Hourly Breaking Tweets for 30 Dow Jones Stock Symbols

Stock Symbol

News

Tweets

Hourly
Breaking
Tweets

Avg
URLs
per
Hourly
Breaking
Tweets

Stock Symbol

News

Tweets

Hourly
Breaking
Tweets

Avg
URLs
per
Hourly
Breaking
Tweets

$AXP

1614

15251

155

10

$MCD

1879

21419

228

17

$BA

2006

19041

207

15

$MMM

1183

11438

122

9

$CAT

1842

19303

188

18

$MRK

1573

28882

199

39

$CSCO

1984

26611

285

19

$MSFT

1733

59469

271

54

$CVX

2168

15897

202

13

$NKE

1080

18206

196

18

$DD

1553

11218

63

11

$PFE

1841

50859

292

57

$DIS

1870

25014

199

17

$PG

1781

15097

192

12

$GE

2260

31336

274

21

$T

1784

30128

263

24

$GS

1878

52888

246

40

$TRV

968

8858

52

7

$HD

1743

18459

164

13

$UNH

1133

11224

144

10

$IBM

2188

80412

248

50

$UTX

1278

10872

68

9

$INTC

2157

30724

273

23

$V

1683

19174

190

13

$JNJ

2232

18236

224

16

$VZ

2194

20896

252

16

2216

30448

236

27

2378

22433

248

15

$JPM

1543

33658

274

27

$WMT

$KO

1899

22688

208

21

$XOM

out extracted sentiment for documents. After the training phase of the classifier, we validated the accuracies
using 10-fold cross validation. The results for this first
experiment is presented in Table 4. The bold numbers on each row indicate the experimental setup which
led to the best accuracy for each stock symbol. Figure 7 also shows the whisker plot for Experiment-1 results. The evaluations show that 1-gram features led
to higher overall accuracies compared to 2-gram features for both SVM and LogisticR experiments. Also,
the experimental setup with the LogisticR classifier
using 1-Gram features where the sentiment features
were excluded led to the maximal accuracies in 19 out
of 30 cases. The second best experimental setup that
achieved the maximal accuracies was also with the LogisticR classifier with 1-gram features integrated with
the sentiment feature. Hence, in order to determine
the utility of extracted sentiment features we formulated the following hypotheses and applied the nonparametric sign test [17] at confidence level 95% to test
if sentiment features would yield a statistically significant boost in the overall prediction accuracies:

1. Null Hypothesis (h0): 1-gram LogisticR classifier
without sentiment features accuracies’ mean = 1gram LogisticR classifier with sentiment accuracies’ mean, indicating that they are at the same
level of performance.
2. Alternative Hypothesis (h1): 1-gram LogisticR
classifier without sentiment features accuracies’
mean 6= 1-gram LogisticR classifier with sentiment features accuracies’ mean, indicating that
they are not at the same level of performance.
The p-value of the sign test to compare 1-gram LogisticR classifier without sentiment features with 1-gram
LogisticR classifier with sentiment features at significance level 0.05 equals to 0.1221, which leads to the
acceptance of the null hypothesis h0 and the rejection
of the alternative hypothesis h1, concluding that using sentiment would not yield a statistically significant
boost in the overall prediction accuracy in this setup.

13

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

Table 4
Accuracy Results of Experiment-1: Hourly Price Direction Prediction using Online News
Classification
Method

SVM

LogisticR

Feature Representation
Sentiment
tures

Fea-

SVM

LogisticR

1-Gram

2-Gram

No

Yes

No

Yes

No

Yes

No

Yes

$AXP

71.44%

70.19%

75.47%

74.28%

67.72%

67.66%

70.82%

70.26%

$BA

68.70%

69.39%

72.64%

72.98%

66.60%

66.50%

68.29%

68.29%

$CAT

72.21%

72.47%

76.49%

75.95%

65.15%

65.63%

66.18%

67.21%

$CSCO

70.36%

71.17%

72.83%

72.68%

67.19%

66.58%

70.51%

70.11%

$CVX

69.00%

69.01%

75.19%

75.19%

65.96%

64.62%

65.82%

65.27%

$DD

66.39%

66.19%

71.73%

72.18%

65.29%

64.97%

68.19%

68.12%

$DIS

72.03%

72.41%

74.76%

75.40%

66.04%

65.99%

68.82%

68.29%

$GE

70.31%

69.78%

63.36%

63.19%

64.96%

64.56%

58.76%

58.19%

$GS

71.19%

71.14%

74.60%

74.07%

66.35%

65.55%

67.62%

68.00%

$HD

70.74%

71.95%

73.84%

74.76%

67.18%

67.64%

67.98%

68.15%

$IBM

70.47%

70.48%

73.99%

73.81%

67.46%

66.68%

66.69%

67.37%

$INTC

69.17%

69.49%

73.39%

73.85%

68.20%

69.36%

69.87%

70.01%

$JNJ

70.39%

70.75%

72.72%

72.18%

64.87%

64.20%

66.22%

66.62%

$JPM

72.13%

72.01%

76.22%

76.54%

66.37%

65.58%

68.38%

67.20%

$KO

72.88%

73.83%

76.20%

76.04%

70.67%

69.88%

70.93%

70.62%

$MCD

71.42%

71.64%

74.30%

74.93%

68.02%

67.91%

70.62%

70.73%

$MMM

73.96%

74.22%

77.85%

77.85%

72.87%

72.44%

74.22%

74.72%

$MRK

72.60%

72.54%

76.10%

75.27%

68.91%

68.34%

70.95%

71.27%

$MSFT

73.92%

73.98%

77.85%

77.84%

68.72%

68.27%

71.61%

71.33%

$NKE

73.24%

72.31%

79.17%

78.80%

72.13%

71.67%

74.54%

73.80%

$PFE

72.95%

72.03%

76.32%

75.83%

70.07%

70.18%

72.03%

71.86%

$PG

71.48%

72.26%

75.91%

76.47%

67.94%

68.16%

67.83%

68.72%

$T

70.90%

70.90%

75.67%

75.67%

63.34%

64.57%

62.95%

65.14%

$TRV

69.83%

70.98%

75.42%

75.11%

69.52%

67.66%

71.79%

72.00%

$UNH

72.11%

72.64%

76.08%

75.82%

71.31%

71.32%

74.76%

74.24%

$UTX

70.89%

71.76%

75.90%

75.67%

67.30%

68.86%

70.51%

70.89%

$V

71.90%

73.50%

75.22%

76.30%

68.15%

67.32%

69.87%

68.87%

$VZ

70.14%

68.96%

72.56%

71.83%

65.36%

64.31%

66.46%

66.22%

$WMT

70.31%

69.90%

65.97%

65.57%

63.81%

64.53%

64.62%

63.99%

$XOM

69.89%

69.72%

72.75%

71.70%

66.48%

66.53%

67.70%

68.04%

14

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

Fig. 7. Whisker Plot of Experiment-1 Accuracy
Table 5
Accuracy Results of Experiment 2: Hourly Price Direction Prediction using Breaking News
Stock
Symbol

Experiment-1
Accuracy

Experiment-2
Accuracy

Stock
Symbol

Experiment-1
Accuracy

Experiment-2
Accuracy

$AXP

67.84%

69.66%

$MCD

67.80%

65.30%

$BA

68.20%

79.00%

$MMM

70.60%

76.50%

$CAT

65.37%

68.39%

$MRK

69.30%

64.80%

$CSCO

67.70%

69.55%

$MSFT

72.60%

81.80%

$CVX

66.60%

65.22%

$NKE

70.30%

71.30%

$DD

67.93%

66.67%

$PFE

70.70%

59.50%

$DIS

65.60%

64.75%

$PG

70.80%

72.70%

$GE

66.60%

66.67%

$T

75.80%

82.90%

$GS

68.30%

71.60%

$TRV

65.40%

75.00%

$HD

68.70%

75.00%

$UNH

68.40%

75.20%

$IBM

66.10%

80.80%

$UTX

67.80%

58.60%

$INTC

69.60%

67.30%

$V

71.20%

71.00%

$JNJ

64.40%

69.20%

$VZ

65.30%

72.80%

$JPM

67.50%

74.00%

$WMT

66.60%

71.00%

$KO

67.90%

68.60%

$XOM

65.20%

67.30%

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter

15

Table 6
Accuracy Results of Experiment-3: Performance Comparison Between Different Time-frames for News Grouping and Direction Labaling
News grouping Time-frames

Price Direction Labeling Time-frames
4h

1h

30m

15m

5m

4h

75%

71%

74%

74%

74%

1h

–

71%

72%

73%

71%

30m

–

–

68%

68%

68%

15m

–

–

–

65%

66%

5.3. Experiment-2: Hourly Price Direction
Prediction using Breaking News
In Experiment-2 we applied steps outlined in Figure 2 to 30 stock symbols in Dow Jones Index using
breaking news periods only as trade triggers. Table 5
shows that Experiment-2 led to a boost in predictive
accuracies for 70% of the stock symbols (i.e. 21 out of
30 cases). In order to prove that Experiment-2 yields
a statistically significant boost in prediction accuracy
compared to Experiment-1 we applied sign test at confidence level 95%. We formulated the following hypotheses:
1. Null Hypothesis (h0): Experiment-1 accuracies
mean = Experiment-2 accuracies mean, indicating that they are at the same level of performance.
2. Alternative Hypothesis (h1): Experiment-1 accuracies mean 6= Experiment-2 accuracies mean, indicating that they are not at the same level of performance.
The p-value of the sign test to compare Experiment-1
with Experiment-2 at significance level 0.05 equals to
0.0357, which leads to the rejection of the null hypothesis h0 and the accepting of the alternative hypothesis
h1 thus confirming that using 1-gram based LogisticR
classifier with breaking news yields a statistically significant boost in directional prediction accuracy for 30
DJI stocks compared to using the same classifier with
all of the stock news every hour.
5.4. Experiment-3: Comparison Between Different
Time-frames For Price Direction Prediction
using Breaking News
13 pairs of time-frame combinations were tested in
Experiment-3 where we applied the steps outlined in
Figure 2 to each of the 30 stock symbols in Dow Jones
Index using breaking news periods. Table 6 shows that
in this experiment 4h4h time-frame combination yields
the best average predictive accuracy for the price direction. This experiment indicates that (i) the 4 hour

time period is the best time-period for detecting Tweet
volume breakouts, and (ii) the 4 hour time-period is
also the best time-frame to label and predict the trend
direction following a Tweet volume breakout session.
5.5. Experiment-4: Tweets Volume Breakout based
Trading System
We performed a simulated financial evaluation of
the proposed trading system by back-testing its trades
and accounting for its return on investment (ROI%)
for a period of 6 months, between March 2014 and
September 2014. In this simulation it is assumed that
no commissions or fees are charged for each trade.
Since the 4h4h time-frame yield the best accuracy results from Experiment-3 the system was tested using
the 4 hours breaking Tweets on the 4 hour stock chart,
for its trade entries and exists. The system entered a
trade following a Tweet volume breakout session for
Tweets matching a stock’s symbol, with a trade fired
in the direction (e.g. a long/buy or short/sell trade)
predicted by our classifier based on the content that
was collected by following the links from the Tweets
during the breakout period, for a fixed duration of 4
hours. For each company, we measured the percentage of winning trades (Won%), percentage of long positions (Long Won%), percentage of short positions
(Short Won%), return on investment (ROI%), maximum drawdown (MDD%), and risk adjusted return
over maximum drawdown (RoMad).
Table 7 shows the results of the simulated backtesting evaluations. The results show that during this
period our system was profitable overall on its recommended trades with each stock symbol. Since each
stock has a different stock price, we performed simulated trading using a diversified portfolio based on
equal exposure to risk or gains from each stock in order to calculate the total and monthly average return
on investment (ROI%). The simulated trades show that
trading with the system during the 6 months period results in a winning ratio of 74% for its long/buy direc-

16

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter
Table 7
Experiment-4 Financial Evaluation of Tweets Breakout Trading System
Stock Symbol

Won%

Short Won%

Long Won%

$AXP

ROI%

MDD%

RoMaD

66%

100.00%

$BA

65%

60.61%

61.76%

6.90%

4.17

1.65

68.75%

13.94%

3.52

3.96

$CAT

69%

$CSCO

63.64%

72.00%

6.06%

2.66

2.28

$CVX

77%

74.19%

78.72%

15.10%

1.47

10.27

82%

85.00%

80.60%

13.67%

1.69

8.09

$DD

76%

88.24%

71.70%

7.70%

2.39

3.22

$DIS

78%

100.00%

75.00%

6.42%

3.99

1.61

$GE

78%

83.87%

75.00%

14.96%

2.65

5.65

$GS

76%

67.50%

83.67%

18.45%

1.52

12.14

$HD

79%

100.00%

76.06%

20.20%

1.07

18.88

$IBM

79%

83.33%

78.38%

10.46%

1.48

7.07

$INTC

81%

100.00%

77.14%

31.90%

1.19

26.81

$JNJ

77%

82.61%

74.55%

8.61%

2.60

3.31

$JPM

77%

67.39%

89.47%

16.96%

2.83

5.99

$KO

77%

86.67%

74.58%

12.46%

1.11

11.23

$MCD

62%

60.42%

64.44%

0.50%

2.39

0.21

$MMM

67%

88.89%

63.89%

0.65%

2.97

0.22

$MRK

80%

85.00%

77.78%

31.41%

2.38

13.20

$MSFT

80%

95.83%

72.88%

12.56%

2.64

4.76

$NKE

75%

50.00%

75.36%

11.22%

1.61

6.97

$PFE

61%

56.52%

68.57%

12.12%

2.97

4.08

$PG

76%

82.14%

72.22%

14.12%

0.91

15.52

$T

86%

95.00%

83.33%

20.13%

0.75

26.84

$TRV

62%

64.71%

60.42%

1.08%

2.04

0.53

$UNH

73%

85.71%

70.31%

22.28%

1.70

13.11

$UTX

67%

76.19%

63.46%

3.15%

5.79

0.54

$V

77%

94.12%

71.93%

12.24%

2.51

4.88

$VZ

87%

80.00%

90.16%

26.46%

1.17

22.62

$WMT

73%

68.97%

75.00%

13.55%

1.85

7.32

$XOM

80%

81.08%

78.72%

20.09%

0.65

30.91

74.73 %

80.25 %

74.20%

13.51 %

2.22

6.09

Average

tional trades and 80% winning ratio for its short/sell
directional trades. Trading with an equally diversified portfolio yields a total (ROI%) of 14% for 6
months, indicating an average monthly (ROI%) of
2.22% and RoMad value of 6.09. The highest total (ROI%) achieved was 31.9% with Intel corporation($INTC), and the lowest total (ROI%) was 0.50%
with Mcdonald’s ($MCD), the highest RoMad value

was achieved by trading AT&T ($T) equals 26.84 and

the lowest RoMaD was 0.21 achieved by trading Mc-

donald’s ($MCD).

17

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter
Table 8
Comparison Between Different Trading Strategy Results
Won%

Short Won%

Long Won%

ROI%

MDD

RoMaD

Tweets breakout

75%

80%

74%

14%

2%

7.00

Tweets breakout + CLE

79%

83%

77%

16%

2%

8.00

Tweets breakout + ConsLWR

74%

79%

73%

14%

3%

4.66

Tweets breakout + AggLWR

72%

78%

72%

16%

6%

2.66

Tweets breakout + CLE + ConsLWR

78%

83%

77%

17%

2%

8.5

Tweets breakout + CLE + AggLWR

78%

83%

77%

19%

2%

9.5

5.6. Experiment-5: Tweets Breakout Stock Trading
System with Price Momentum Based Trade EXIT
Rules
We performed a simulated financial evaluation of
the Tweets breakout stock trading system with the
price momentum based trade EXIT rules defined in
Section 4.5. Table 8 shows a comparison of the average results of the proposed trading strategies based on
the Won%, Short Won%, Long Won%, ROI%, MDD,
and RoMaD. The results show that Tweets Breakout+CLE+AggLWR trading strategy yielded the best
risk adjusted return metric value (RoMad) of 9.5 meaning that, this system would yield 9.5% returns for
an occasional drawdown risk of 1%, or 95% returns
for an occasional drawdown risk of 10%, essentially
almost doubling the initial investment in 6 months.
The second best trading strategy was obtained by the
Tweets Breakout + CLE + ConsLWR with RoMaD
value of 8.5.

6. Conclusion and Future Work
In this paper we start with a system to predict the
hourly stock price direction based on the textual analysis of news articles’ content mentioning a stock symbol. First, we show that using LogisticR classifier with
1-gram keyword features leads to the best overall directional prediction accuracy based on news articles.
Next, we showed that using extracted document-level
sentiment features do not yield to a statistically significant boost in directional predictive accuracies in the
presence of other 1-gram features. Then, we proceed
to show that information in articles indicated by breaking Tweet volumes leads to a statistically significant
boost in the hourly directional prediction accuracies
for the prices of DJI stocks mentioned in these articles. We experiment with all time-frame combinations
and identify the 4h time period as the best time-period

for detecting Tweet volume breakouts, and it is also
as the best time-frame for the price-charts to label and
predict the trend direction following a Tweet volume
breakout session. Finally, we develop price momentum
based trade exit rules to cut losing trades early and to
allow the winning trades run longer. We show that the
Tweet volume breakout based trading system with the
momentum based exit rules not only improves the winning accuracy and the return on investment, but it also
lowers the maximum drawdown and achieves the highest overall return over maximum drawdown. Our future work includes developing a real-time distributed
trading system to monitor the Tweeter streams of different categories of stocks (i.e. large cap, mid cap and
small cap) and trade with the their breaking volumes.
We also plan to develop online learning methods to
maintain the currency of the predictive models.
References
[1] Average true range. https://en.wikipedia.org/
wiki/Average_true_range. Accessed: 20165-05-02.
[2] Bollinger bands. https://en.wikipedia.org/wiki/
Bollinger_Bands. Accessed: 20165-04-29.
[3] Drawdown (economics).
https://en.wikipedia.
org/wiki/Drawdown_(economics). Accessed: 2016505-01.
[4] Keltner
channels.
http://stockcharts.
com/school/doku.php?id=chart_school:
technical_indicators:keltner_channels.
Accessed: 20165-04-29.
[5] Return
on
investment
roi.
http:
//www.investopedia.com/terms/r/
returnoninvestment.asp. Accessed: 20165-05-01.
[6] Return
over
maximum
drawdown
(romad).
http://www.investopedia.com/terms/r/
return-over-maximum-drawdown-romad.asp.
Accessed: 20165-05-01.
[7] Squeeze
momentum
indicator.
https:
//www.tradingview.com/script/
nqQ1DT5a-Squeeze-Momentum-Indicator-LazyBear.
Accessed: 20165-04-29.
[8] Harshavardhan Achrekar, Avinash Gandhe, Ross Lazarus, SsuHsin Yu, and Benyuan Liu. Twitter improves seasonal in-

18

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

H. alostad et al. / Directional Prediction of Stock Prices using Breaking News on Twitter
fluenza prediction. In HEALTHINF 2012 - Proceedings of the
International Conference on Health Informatics, Vilamoura,
Algarve, Portugal, 1 - 4 February, 2012., pages 61–70, 2012.
Sitaram Asur and Bernardo A. Huberman. Predicting the future
with social media. In Proceedings of the 2010 IEEE/WIC/ACM
International Conference on Web Intelligence and Intelligent
Agent Technology - Volume 01, WI-IAT ’10, pages 492–499,
Washington, DC, USA, 2010. IEEE Computer Society.
Johan Bollen, Huina Mao, and Xiaojun Zeng. Twitter mood
predicts the stock market. Journal of Computational Science,
2(1):1–8, 2011.
John F. Carter. Mastering the Trade: Proven Techniques for
Profiting from Intraday and Swing Trading Setups. American
Media International, 2007.
Ryan Compton, Craig Lee, Jiejun Xu, Luis Artieda-Moncada,
Tsai-Ching Lu, LalindraDe Silva, and Michael Macy. Using
publicly visible social media to build detailed forecasts of civil
unrest. Security Informatics, 3(1), 2014.
Jibing Gong and Shengtao Sun. A new approach of stock price
prediction based on logistic regression model. In New Trends in
Information and Service Science, 2009. NISS ’09. International
Conference on, pages 1366–1371, 2009. ID: 1.
Michael Hagenau, Liebmann Michael, and Dirk Neumann.
Automated news reading: Stock price prediction based
on financial news using context-capturing features. volume 55, pages 685; 685–697; 697, /2013 06.
doi:
10.1016/j.dss.2013.02.006 pmid:.
MI Yasef Kaya and M. Elif Karsligil. Stock price prediction
using financial news articles. In Information and Financial Engineering (ICIFE), 2010 2nd IEEE International Conference
on, pages 478–482. IEEE, 2010.
Stefan Lauren and S. Dra Harlili. Stock trend prediction using simple moving average supported by news classification.
In Advanced Informatics: Concept, Theory and Application
(ICAICTA), 2014 International Conference of, pages 135–139.
IEEE, 2014.
E. L. Lehmann. Nonparametrics :statistical methods based
on ranks. Holden-Day, San Francisco. E. L. Lehmann, with
the special assistance of H. J. M. D’Abrera.; ;24 cm; Includes
bibliographical references and index.
Huan Liu and Rudy Setiono. Feature selection via discretization. IEEE Transactions on Knowledge and Data Engineering,
9(4):642–645, 1997.
Jun Liu, Shuiwang Ji, and Jieping Ye. Slep: Sparse learning with efficient projections. Arizona State University, 6:491,

2009.
[20] Tim Loughran and Bill McDonald. When is a liability not a liability? textual analysis, dictionaries, and 10ÃćâĆňÂŘks. The
Journal of Finance, 66(1):35–65, 2011.
[21] Yuexin Mao, Wei Wei, and Bing Wang. Twitter volume spikes:
analysis and application in stock trading. In Proceedings of the
7th Workshop on Social Network Mining and Analysis, page 4.
ACM, 2013.
[22] Yuexin Mao, Wei Wei, Bing Wang, and Benyuan Liu. Correlating s&p 500 stocks with twitter data. In Proceedings of
the First ACM International Workshop on Hot Topics on Interdisciplinary Social Networks Research, pages 69–72. ACM,
2012.
[23] Arman Khadjeh Nassirtoussi, Saeed Aghabozorgi, Teh Ying
Wah, and David Chek Ling Ngo. Text mining of newsheadlines for forex market prediction: A multi-layer dimension
reduction algorithm with semantics and sentiment. Expert Systems with Applications, 42(1):306–324, 1 2015.
[24] Jigar Patel. Predicting stock market index using fusion
of machine learning techniques.
Expert Systems with
Applications, 42(4):2162; 2162–2172; 2172, 2015.
doi:
10.1016/j.eswa.2014.10.031 pmid:.
[25] Friedrich Pukelsheim. The three sigma rule. The American
Statistician, 48(2):88–91, 1994.
[26] Sanjiban Sekhar Roy, Dishant Mittal, Avik Basu, and Ajith
Abraham. Stock market forecasting using lasso linear regression model. In Afro-European Conference for Industrial Advancement, pages 371–381. Springer, 2015.
[27] Robert Schumaker and Hsinchun Chen. Textual analysis of
stock market prediction using financial news articles. AMCIS
2006 Proceedings, page 185, 2006.
[28] Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. Sentiment strength detection for the social web. Journal of the
American Society for Information Science and Technology,
63(1):163–173, 2012.
[29] Tien-Thanh Vu, Shu Chang, Quang Thuy Ha, and Nigel Collier. An experiment in integrating sentiment features for tech
stock prediction in twitter. 2012.
[30] Xiaofeng Wang, Donald Brown, and Matthew Gerber. Spatiotemporal modeling of criminal incidents using geographic, demographic, and twitter-derived information. In International
Conference on Intelligence and Security Informatics, Lecture
Notes in Computer Science. IEEE Press, IEEE Press, 2012.

Partitioning and Scaling Signed Bipartite Graphs
for Polarized Political Blogosphere
Sedat Gokalp

M’hamed Temkit

Hasan Davulcu

I. Hakki Toroslu

Computer Science
Arizona State University
Sedat.Gokalp@asu.edu

Mathematical Statistics
Arizona State University
Mhamed.Temkit@asu.edu

Computer Science
Arizona State University
Hasan.Davulcu@asu.edu

Computer Engineering
Middle East Technical University
Toroslu@ceng.metu.edu.tr

Abstract—Blogosphere plays an increasingly important role
as a forum for public debate. In this paper, given a mixed set
of blogs debating a set of political issues from opposing camps,
we use signed bipartite graphs for modeling debates, and we
propose an algorithm for partitioning both the blogs, and the
issues (i.e. topics, leaders, etc.) comprising the debate into binary
opposing camps. Simultaneously, our algorithm scales both the
blogs and the underlying issues on a univariate scale. Using
this scale, a researcher can identify moderate and extreme blogs
within each camp, and polarizing vs. unifying issues. Through
performance evaluations we show that our proposed algorithm
provides an effective solution to the problem, and performs
much better than existing baseline algorithms adapted to solve
this new problem. In our experiments, we used both real data
from political blogosphere and US Congress records, as well
as synthetic data which were obtained by varying polarization
and degree distribution of the vertices of the graph to show the
robustness of our algorithm.

I.

I NTRODUCTION

Blogosphere plays an increasingly important role [1] as
a forum of public debate, with knock-on consequences for
the media, politics, and policy. Hotly debated issues span
all spheres of human activity; from liberal vs. conservative
politics, to extremist vs. counter-extremist religious debate, to
climate change debate in scientific community, to globalization
debate in economics, and to nuclear disarmament debate in
security. There are many applications [2], [3], [4], [5], [6]
for recognizing politically-oriented sentiment in texts. Previous
work [7] studied linking patterns and discussion topics of
political bloggers by measuring the degree of interaction
between liberal and conservative blogs, and to uncover their
differences. In this paper, given a mixed set of blogs debating
a set of related issues from two opposing camps, we propose
an algorithm to determine (i) which blog lies in which camp,
(ii) what are the contested issues, and, (iii) who are mentioned
as the key individuals within each camp.
Bipartite graphs [8], [9], [10] have been widely used to
represent relationships between two sets of entities. We use
bipartite graphs to model the relationships between blogs
and issues (i.e. topics, individuals, etc.) mentioned within
blogs. We use signed weighted edges to represent opinion
strengths, where positive edges denote support, and negative
edges denote opposition between a blog and an issue.
We develop algorithms to solve the following problems on
signed bipartite graphs modeling blog debates:

1)
2)

Partitioning of both the blogs, and the underlying
issues mentioned in blogs, into two opposing camps;
Scaling of both the blogs and the underlying issues
on a univariate scale such that the position of a
vertex is closer to the positions of the vertices it
is connected with positive edges, and further away
from the positions of the vertices it is connected with
negative edges.

Using this scale, a researcher can identify both the moderate and extreme blogs within each camp, and the polarizing
vs. unifying issues. Partitioning and scaling help a researcher
to better understand the structure of a social, political or
economic debate, or even the details of an emerging geopolitical conflict in the world. While extremist ends of a scale,
may represent blogs with irreconcilable viewpoints, in some
cases, moderate blogs may represent viewpoints that are more
amenable to engage in a constructive dialog through a set
of unifying issues. Moderates may sympathize with some
of the claims and grievances of the other side. Longitudinal
analysis using our proposed algorithms could reveal interesting
dynamics, such as, moderates from opposing camps could be
in the process of forming a coalition by making the necessary
compromises to reach a consensus. All the while, moderates
may be alienating extremists in their own camps who may
choose to focus on polarizing issues only, and lash out violent
or demonizing rhetoric on everyone else who do not share their
exclusivist viewpoints.
To the best of our knowledge, simultaneous scaling and
partitioning on signed weighted bipartite graphs has not been
studied in the literature, and this paper is the first attempt to
introduce the problem and provide an effective solution and
evaluation strategies.
Major contributions of this paper are: (1) an iterative algorithm, named Alternatingly Normalized CO-HITS (ANCOHITS), to propagate the scores on a signed bipartite graph to
solve the partitioning and scaling problems described above;
(2) a convergence proof for the proposed ANCO-HITS algorithm; (3) definition of a new coefficient to measure structural
equilibrium for signed bipartite graphs using the multiplicative transitivity property presented in [11] exemplified by
the phrase the enemy of my enemy is my friend; and (4)
performance evaluations of blog/issue partitioning and scaling
algorithms using synthetic data sets which were obtained by
varying polarization and degree distribution of signed bipartite
graphs, and analysis with two real data sets: (i) partitioning

and scaling of Republicans/Democrats and their roll call votes1
based on the 111th US Congress voting records, and (iii)
partitioning and scaling of the top 22 liberal and conservative
blogs, and the most influential individuals mentioned in these
blogs.
In our experiments, variance in polarization relates to the
distributions of the ratio of vertices corresponding to extremes
vs. moderates.
Alongside our proposed ANCO-HITS, we also evaluated
two baseline algorithms, namely CO-HITS [8] and spectral
clustering [12]. Although Co-HITS was designed for scaling
unsigned bipartite graphs, it can be directly applied for scaling signed bipartite graphs, and partitioning by considering
the signs of vertex values. Spectral clustering algorithm was
designed for partitioning of graphs, and it can also produce
a scale by using the component values of the eigenvector
associated with the second smallest positive eigenvalue of the
graph Laplacian [13], [14].
Our experiments showed that the ANCO-HITS algorithm
is the only robust algorithm in the presence of variance in
polarization and vertex degrees.
The rest of this paper is organized as follows. We review
related work in Section 2. Section 3 presents the problem
formulation. In Sections 4 and 5, we present the spectral
clustering and CO-HITS algorithms as the baselines. In Section 6, we present our proposed ANCO-HITS algorithm, and
its convergence proof. Section 7 presents the definition of
structural equilibrium for signed bipartite graphs. We describe
and report experimental evaluations in Section 8. Finally, we
present conclusions and future work in Section 9.
II.

R ELATED W ORK

Scaling vertices of a graph based on the network structure
rather than individual properties has been of great interest for
more than a decade. Two most well-known algorithms are
the PageRank [15] and the HITS [16] algorithms. They were
designed to rank the vertices of graphs with positive weighted
edges. Spectral analysis show that both PageRank and HITS
algorithms converge. An important distinction between the two
algorithms is that; the HITS algorithm provides two different types of rankings corresponding to hubs and authorities,
whereas PageRank provides only a single ranking.
Many data types from data mining applications can be
modeled as bipartite graphs, examples include terms and
documents in a text corpus, customers and items purchased
in market basket analysis and bloggers writing about current
issues.
Based on variations of HITS and PageRank, many researchers have proposed algorithms. In [8], the authors
propose a modification of the HITS algorithm to work on
bipartite graphs called CO-HITS. The main difference between
HITS and CO-HITS is that; HITS provides two scores for
each vertex, whereas CO-HITS provides one score for each
type of vertex. In this paper, we use CO-HITS as one of the
baseline algorithms, and in order to overcome its deficiencies,
we extend it with normalization steps.
1 http://thomas.loc.gov/home/rollcallvotes.html

The clustering coefficient was first introduced in [17]
to measure how much multiplicative transitivity property the
graph exhibits, which reflects the tendency of the vertices
to form small groups. In [11], authors define a new coefficient using the multiplicative transitivity for signed graphs
to measure structural equilibrium. In this paper, we define
another coefficient through multiplicative transitivity for signed
bipartite graphs.
Data mining methods such as clustering have been used
quite extensively for exploratory data mining applications [18],
[19]. Clustering analysis [20] provides a partitioning of the
data into subsets, called clusters such that the objects in a
cluster are more similar than those in distinct clusters. Spectral
clustering [21], [12], [13] is a powerful clustering method that
is able to outperform K-means clustering [22] in many cases,
especially when the clusters are non convex. The method is
based on computing the eigenvalues of the normalized version
of the graph Laplacian, and has theoretical connections with
the normalized cut of the graph. In particular when clustering a
bipartite graph into two balanced clusters, the second smallest
positive eigenvalue [13] is the solution to the normalized cut of
the graph. In recent years, several authors have used spectral
clustering to analyze bipartite graphs [10]. Furthermore, some
work has been done to take into account a signed adjacency
matrix by using an augmented adjacency matrix [23]. In this
paper, spectral clustering was also used as one of our baseline
methods for partitioning and scaling signed bipartite graphs.
III.

P ROBLEM F ORMULATION

A. CoScaling for signed bipartite graphs
Given
•

G = (U ∪ V, A) is a bipartite graph consisting of
two disjoint sets of vertices U and V , and a signed
adjacency matrix A

•

U = {u1 , u2 , . . . , um }, a set of m vertices

•

V = {v1 , v2 , . . . , vn }, a set of n vertices

•

A ∈ Rm×n , where aij represents the signed edge
between ui and vj

Find
•

X = (x1 , x2 , . . . , xm ), where xi ∈ R is the assigned
value of the vertex ui

•

Y = (y1 , y2 , . . . , yn ), where yi ∈ R is the assigned
value of the vertex vi

such that
•

sgn(xi ) and sgn(yi ) shall determine the polarity of
the vertices i.e. −1 and +1 as the opposing polarities

•

xi value for a vertex ui should be closer to the
yj values of the vertices that it supports (connects
positively), and further away from the yk values of
the vertices that it opposes (connects negatively). The
magnitudes of xi and yj denote the extremity of the
nodes ui and vj . i.e. magnitudes closer to 0 meaning
more moderate and larger magnitudes meaning more
extreme.

0

u1 u2 u3

Perfectly polarized bipartite graph

u1

0

X

u2

0

Y

Fig. 2.

in the presence of non-convex groups of data. This method
has close connections with the normalized cut [12] of the
graph. In particular when clustering a bipartite graph into two
balanced clusters, the second smallest positive eigenvalue of
the Laplacian matrix [21] is the solution to the problem of
minimizing the normalized cut of the graph.

X

vn-2 vn-1 vn Y

v 1 v2 v3

Fig. 1.

um-2um-1 um

X

Y

Extreme vs. Moderate vertices

Figure 1 depicts a perfectly polarized bipartite graph. The
two axes X and Y represent the univariate scale for the nodes
in U and V . The vertices to the right of zero have positive
values, and the vertices to the left have negative values on
the scale. A green solid line between the nodes ui and vj
represents support, and a red dashed line represents opposition.

Spectral clustering embeds the data into the subspace of
the eigenvectors of the Laplacian. In this paper, we will use
spectral clustering to partition both types of vertices of a signed
bipartite graph into two polarities and provide a scale for the
vertices. We will do this by using the sign and the value of
the components of the eigenvector associated with the smallest
positive eigenvalue of the Laplacian.
Spectral clustering uses an adjacency matrix with all positive entries. However, our problem assumes a signed adjacency
matrix. One of the common techniques to circumvent this
problem is to augment the matrix into a bigger matrix [10],
[25], [24], such that all entries are positive. The first half of
the augmented matrix is reserved for the entries with positive
values, and the second half is reserved for the entries with
negative values.
Define Ã ∈ Rm×2n such that


Ã = A+ , A−

Figure 2 shows an example of two vertices; u1 being
extreme and u2 being more moderate. u1 supports the vertices
of same polarity, and opposes the vertices of the opposite
polarity. However, u2 has mixed support and opposition.

where

Although partitioning algorithms can be utilized to detect
the polarity of vertices, it is not possible to distinguish extremes from moderates. Scaling overcomes this problem and
makes it possible to compare two vertices of same polarity.
In this paper, we are not only able to compare pairs of
vertices, but also provide the exact locations on the scale,
therefore providing valuable information about the shape of
the distribution as well.

and similarly

To solve this co-scaling problem, we present two baseline
methods. The first one is a common modification [24], [10],
[25] of the well-known Spectral Clustering approach to work
on graphs with signed edges. The second one is the COHITS [8] algorithm, that is a modification of the well-known
HITS algorithm, designed for bipartite graphs.
Finally, we compare these baseline methods with a novel
algorithm we developed for co-scaling problem, named Alternatingly Normalized CO-HITS (ANCO-HITS).
IV.

S PECTRAL C LUSTERING

Spectral clustering [13] uses spectral graph theory, which
is the study of graphs using linear algebra methods. In this
context, for a given graph, its edge set is represented by
an adjacency matrix. The eigenvectors of the normalized
Laplacian of the adjacency matrix are used to partition the
graph into clusters, where objects in a cluster are more similar
than those in distinct clusters. Spectral clustering incorporates
the properties of a graph via the adjacency matrix and is able to
outperform K-means clustering in many situations, especially

a+
ij

a−
ij


=


=

aij , if aij > 0
0,
otherwise

−aij , if aij < 0
0,
otherwise

In order to partition and scale the nodes ui ∈ U and vi ∈
V , we define the following matrix:


0m×m

Ã

B=




Ã

T

02n×2n

We define the Laplacian of B as L = D − B where D is
m+2n
P
the diagonal degree matrix and dii =
bij . We further
j=1

compute the normalized Laplacian Lsym = D−1/2 LD−1/2 . It
should be noted here that both L and Lsym are positive semidefinite.
Let the eigenvalues of Lsym have the values 0 =
λ1 ≤ λ2 ≤ . . . ≤ λm+2n with associated eigenvectors
v1 , v2 , . . . , vm+2n , our univariate scale being the eigenvector
v2 .
The first m components of v2 are set to be the X vector,
and the following n components are set to be the Y vector,
solutions of the co-scaling problem.

u1

0

u2

X

0

Y

Fig. 3.

For this purpose, we propose ANCO-HITS algorithm,
which introduces a normalization mechanism to address the
issue of degree sensitivity of CO-HITS. The proposed method
uses the same iteration procedure described in Algorithm 1.
The update functions for X and Y are modified such that they
are normalized as follows:
n
P
aij yj<k−1>

X

Y

Extremity vs Degree

j=1

x<k>
=
i
V.

Algorithm 1 describes the steps of the CO-HITS algorithm
for the co-scaling problem.
Data: Adjacency matrix A
Result: Scale vectors X and Y
Initiate X <0> = (1, 1, . . . , 1) ;
Initiate Y <0> = (1, 1, . . . , 1) ;
repeat
Update X;
Update Y ;
until X vector converges;
Algorithm 1: Iterative update procedure for CO-HITS

m
P

yj<k> =

=

yj<k> =

aij x<k>
i

similarly,

(1)

(2)

1
m
P

, if i = j

|aij |

(7)



 i=1
0,

otherwise

x<k> = BAy <k−1>
y

(3)

with  a small positive value.
The drawback of this method is its sensitivity to the degree
of each vertex, in the sense that the higher degree the vertex
has, the higher score it will be assigned on the scale.
For example, let us consider two vertices u1 and u2 with
u1 having a smaller degree than u2 . However, let u1 be more
polarized than u2 as shown in Figure 3. In this scenario,
the corresponding scale values for u1 and u2 should satisfy
|x1 | > |x2 |. But, this will not be the case with the CO-HITS.
This suggests a better algorithm that accounts for the negative
impact of degree variation through some normalization mechanism.
VI.






We should note here that both B and C matrices are symmetric
and positive definite. The update rules for x and y vectors can
be written in matrix notation as follows:

i=1

and convergence is achieved when

 <k>


X
− X <k−1> 
2 < 

(5)
|aij |

Proof: Let B ∈ Rm×m and C ∈ Rn×n diagonal matrices
with positive entries, where

1

 n
, if i = j

P
|aij |
Bij =
(6)

j=1


0,
otherwise

j=1
m
X

i=1
m
P

Theorem 1: ANCO-HITS algorithm will converge for any
matrix A ∈ Rm×n , with |A| having non-zero row-sums and
column-sums.

Cij =
aij yj<k−1>

aij x<k>
i

i=1

The update functions for X and Y are defined as follows:
n
X

(4)
|aij |

j=1

In [8], the authors modify the well-known HITS [16]
algorithm and propose the CO-HITS algorithm which is used
to rank vertices of a bipartite graph. Even though the adjacency
matrix has only positive values in the original HITS paper, the
theory still holds for adjacency matrices with signed entries.

x<k>
i

n
P

CO-HITS

A LTERNATINGLY N ORMALIZED
CO-HITS (ANCO-HITS)

According to our problem formulation, the values of the
vertices on the scale shall not be sensitive to their degrees, but
rather be sensitive to what kind of relations they have with the
other set of vertices.

<k>

T

= CA x

(8)

<k>

(9)

Therefore,
x<k> = (BACAT )x<k−1>

(10)

If there exists a vector x∗ that the x<t> will converge in
direction, it has to satisfy the equation:
cx∗ = (BACAT )x∗

(11)

Even though this is an eigenvalue equation, the eigenvalues
may not be real, because the matrix (BACAT ) is not symmetric. But if we multiply each side of the equation with B −1/2 ,
which exists since B is positive definite, we will get:
cB − /2 x∗ = B /2 ACAT B /2 B − /2 x∗
1

1

1

1

(12)

Define M ∈ Rm×m to be M = B 1/2 ACAT B 1/2 and z ∈
Rm×1 to be z = B −1/2 x∗ , we will get
cz = M z

(13)

which is again an eigenvalue equation. However, in this case
M is a symmetric matrix, and can be shown to be positive
semi-definite with z as an eigenvector c as an eigenvalue. The
M matrix has a set of m eigenvectors that are all unit vectors

and all mutually orthogonal; that is, they form a basis for the
space Rm .

u1

v2

v1

v2

u1

u2

u1

u2

v1

v2

c)

d)

1

1

Fig. 4.

(15)

Any vector v ∈ Rm can be written as a linear combination of
the eigenvectors z1 , z2 , . . . , zm . Therefore,
B − /2 x<0> = (a1 z1 + a2 z2 + . . . + am zm )
1

(16)

x<k> = B /2 M k (a1 z1 + a2 z2 + . . . + am zm )
1

= B /2 (a1 M k z1 + a2 M k z2 + . . . + am M k zm )
1

= B /2 (a1 ck1 z1 + a2 ck2 z2 + . . . + am ckm zm )
1

As k goes to infinity, the x<k> vector will converge to a
multiple of the B 1/2 z1 vector.
x<k>
1
= a1 B /2 z1
k→∞ ck
1
lim

(17)

Similarly, the convergence for the y <k> can be proved in
the same fashion:
y <k> = C

1/2

N k C − /2 y <0>
1

VII.

Bipartite graphs do not have cycles of odd length. Therefore, the structural equilibrium cannot be measured as formalized before. But it can be extended to calculate the ratio of
balanced cycles of length four. For this purpose, we define the
multiplicative transitivity for bipartite graphs as follows.
A signed bipartite graph exhibits multiplicative transitivity
when a path of three edges tend to be completed by a fourth
edge having a sign equal to the product of the three edges’
signs.
This can be rephrased as the enemy of my enemy of my
enemy is my enemy, or the enemy of my friend of my enemy is
my friend, etc. Figure 4 depicts two cycles with odd number of
edges (a and c), and two cycles with even number of edges (b
and d). By definition, the cycles with odd number of negative
edges do not satisfy multiplicative transitivity.
Hence, the corresponding relative signed clustering coefficient can be reformulated for bipartite graphs as follows:



A ◦ AAT A

+
SE(A) = 

Ā ◦ ĀĀT Ā

1/2

S TRUCTURAL E QUILIBRIUM

The relation phrased as the enemy of my enemy is my friend
is observed on various networks. This relation in general can
be formalized for graphs by constraining any cycle of arbitrary
length to have even number of negative edges [26]. In [11],
authors relate this constraint with the multiplicative transitivity
property of an adjacency matrix, which can be measured using
a modification of the clustering coefficient introduced in [17].
The structural equilibrium(SE) can be measured by checking the consistency of the edges forming cycles of length three.
The relative signed clustering coefficient calculates the ratio of
balanced cycles among all possible cycles of length three.



A ◦ A2 

+
SE(A) = 

Ā ◦ Ā2 
+

•

Ā is the absolute adjacency matrix with āij = |aij |

•

C = A ◦ B is defined as the Hadamard product
(element-wise product) for two matrices, such that
cij = aij ∗ bij

•

x = kAk+ is definedPasPthe sum of all matrix
elements, such that x =
aij
i

Cycles of four with negative edges

(18)

where N ∈ Rn×n is N = C AT BAC
and the y <k>
vector will converge to a multiple of the C 1/2 q1 vector, with
q1 being the principal eigenvector of the N matrix.
1/2

j

v2

(14)

We can rewrite the above equation in terms of M matrix
x<k> = B /2 M k B − /2 x<0>

u2

v1

v1

x<k> = (BACAT )k x<0>

u1

b)

a)

Let us denote the eigenvalues of the M matrix by
c1 , c2 , . . . , cm sorted in such a way that c1 ≥ c2 ≥ . . . ≥
cm ≥ 0, with the eigenvectors z1 , z2 , . . . , zm respectively.
Using Equation (10), we can write a compact form for the
k th update iteration of x as follows:

u2

+

For our experimental datasets, we report the corresponding
SE values.
VIII.

E XPERIMENTS & E VALUATIONS

To validate our algorithm, we have used two different
datasets that are US Congress and political blogosphere. In
addition to real data, we introduced a model to generate
synthetic data to analyze the performance of the algorithms
for various parameters.
A. US Congress
The US Congress has been collecting data since the very
first congress of the US history. This data has been encoded
as XML files and publicly shared through the govtrack.us
project2 .
From various types of data available at the project site, we
collected the roll call votes for the 111th US Congress which
includes The Senate and The House of Representatives and
covers the years 2009-2010.

2 http://www.govtrack.us/data

TABLE I.

D ESCRIPTIVE SUMMARIES OF THE GRAPHS FOR EACH DATASET WITH THE PARTITIONING ACCURACIES FOR EACH ALGORITHM

Vertices in U
Vertices in V
Graph Density
Structural Equilibrium

111th US Senate
64 Democrat + 42 Republican
Senators
696
Bills
88.36%
39.47%

111th US House
268 Democrat + 183 Republican
Representatives
1655
Bills
91.23%
39.37%

Political Blogosphere
13 Liberal + 9 Conservative
Blogs
20 Liberal + 14 Conservative
People
39.04%
87.21%

100.00%
100.00%
100.00%

99.11%
99.56%
99.56%

75.39%
98.21%
98.21%

Spectral Clustering
CO-HITS
ANCO-HITS

According to The Library of Congress3 ,
111th US Senate

A roll call vote guarantees that every Member’s vote
is recorded, but only a minority of bills receive a roll
call vote.
The 111th Senate has 1084 senators and the data contains
their votes on 696 bills, and The 111th House has 451 representatives and the data contains their votes on 1655 bills.

111th US Congress

We extracted the adjacency matrix A ∈ {−1, 0, 1}|U |×|V | ,
with U vertices representing the congressmen, and the V
vertices representing the bills. The values aij are 1 if the congressman ui votes ‘Yea’ for the bill vj , -1 if the congressman
votes ‘Nay’, and 0 if he did not attend the session.
The aforementioned scaling algorithms will scale both the
congressmen and the bills. In presence of partisanship5 in the
Congress, the sign of the scale values for the congressmen
should correspond to the Democrat and Republican parties,
and the magnitude of the scale values should represent the
amount of partisanship. The first two columns of Table I
provide information about this data as well as the partitioning
accuracies of the algorithms.

Fig. 5.

We analyzed the congressmen that have been assigned to
be moderate by each algorithm. We observed that the baseline
algorithms tend to have the congressmen with less number of
votes (i.e. lesser degree) to be moderate regardless of their
partisanship. On the other hand, when we queried the names
assigned to be most moderates by the ANCO-HITS, for both
Democrats and Republicans, we were able to identify a number
of supporting articles matching the ANCO-HITS scaling [27],
[28], [29], [30].

Vote matrix after scaling with ANCO-HITS

111th US Senate

Figure 5 depicts the vote matrices of the 111th US
Congress, with rows representing the congressmen and the
columns representing the bills. The light green color represents
’Yea’ votes, and dark red represents ’Nay’ votes. Scaling these
graphs leads to a re-ordering of the rows and columns where
congressmen and bills are co-clustered together.

111th US Congress

Figure 6 represents the bipartite graph of the 111th US
Congress data after scaling both the congressmen and the bills
with ANCO-HITS. The light green colored edges represent
’Yea’ votes, and dark red represents ’Nay’ votes. Similar to our
motivating Figure 1, this figure also shows partisan behavior
in the 111th US Congress.
3 http://thomas.loc.gov/home/rollcallvotes.html
4 Normally, each congress has 100 senators (2 from each state), however in
many of the congresses, there are unexpected changes on the seats caused by
displacements or deaths.
5 Partisanship can be defined as being devoted to or biased in support of a
party.

Fig. 6.

Bipartite graphs after scaling with ANCO-HITS

TABLE II.

L IST OF P OLITICAL B LOGS

Blog name

URL

Huffington Post
Daily Kos
Boing Boing
Crooks and Liars
Firedoglake
AMERICABlog
Think Progress
Talking Points Memo
Wonkette
Balloon Juice
Digby’s Hullabaloo
Informed Comment
Truthdig
Hot Air
Reason - Hit and Run
Little green footballs
Atlas shrugs
Stop the ACLU
Wizbangblog
Michelle Malkin
Red State
Pajamas media

http://www.huffingtonpost.com/
http://www.dailykos.com/
http://www.boingboing.net/
http://www.crooksandliars.com/
http://www.firedoglake.com/
http://americablog.com/
http://thinkprogress.org/
http://www.talkingpointsmemo.com/
http://wonkette.com/
http://www.balloon-juice.com/
http://digbysblog.blogspot.com/
http://www.juancole.com/
http://www.truthdig.com/
http://hotair.com/
http://reason.com/blog
http://littlegreenfootballs.com/
http://atlasshrugs2000.typepad.com/
http://www.stoptheaclu.com/
http://wizbangblog.com/
http://michellemalkin.com/
http://www.redstate.com/
http://pajamasmedia.com/

Political Camp

Posts

Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Liberal
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative

3959
1957
1576
1497
1354
1297
1197
1081
1064
923
553
179
159
1579
1563
787
773
741
621
532
311
97

B. Political Blogosphere

C. Synthetic Data

As Web 2.0 platforms gained popularity, it became easy
for web users to be a part of the web and express their
opinions, mostly through blogs. Most blogs are maintained
by individuals, whereas there are also professional blogs with
a group of authors. In this study, we focus on a set of popular
political liberal or conservative blogs that have a clearly declared positions. These blogs contain discussions about social,
political, economic issues and related key individuals. They
express positive sentiment towards individuals whom they
share ideologies with, and negative sentiment towards others.
In these blogs, it is common to see criticism of people within
the same camp, or support for people from the other camp.

The actual partitioning information for the real datasets
were available, which made it possible to check the partitioning
accuracy of the algorithms. However, to thoroughly check the
scaling accuracy of the algorithms, we developed a method to
generate random bipartite graphs with the following properties:

In this experiment, we collected a list of 22 most popular
liberal and conservative blogs from the Technorati6 rankings.
For each blog, we fetched the posts for the period of 6 months
before the 2008 US presidential elections (May - October,
2008) due to the intensity of the debates and discussions. Table
II shows the list of blogs with their URLs, political camps and
the number of posts for the given period.

•

Following procedure describes the method to generate
random graphs.
RandomGraph(m, n, Ddegree , Dscale )
Let A, U , V , X and Y be defined as in Definition 1.
1)
2)
3)
4)

7

We use AlchemyAPI to run a named entity tagger to
extract people names from the posts, and entity-level sentiment
analysis which provided us with weighted sentiment (positive
values indicating support, and negative indicating opposition)
for each person. This information was used to synthesize a
signed bipartite graph, where the blogs and people correspond
to the two sets of vertices U and V . The aij values of the
adjacency matrix A are the cumulative sum of sentiment values
for each mention of the person vj by the blog ui .
To get a gold standard list of the most influential liberal and
conservative people, we used The Telegraph List8 for 2007.
The third column of Table I provides information about this
data as well as the partitioning accuracies of the algorithms.
6 http://technorati.com/
7 http://www.alchemyapi.com/
8 http://www.telegraph.co.uk/news/uknews/1435447/
The-top-US-conservatives-and-liberals.html

The degrees and the scores for the vertices in U
and V follow independent probability distribution with
varying parameters and shapes.

5)

Create m vertices for U , and n vertices for V
Independently assign degrees 1 ≤ d(ui ) ≤ m and
1 ≤ d(vj ) ≤ n with probability distribution Ddegree
Independently assign −1 ≤ xi , yj ≤ +1 values with
probability distribution Dscale .
Generate an adjacency list of pairs (ui , vj ), where
each node ui and vj occur in the list d(ui ) and d(vj )
times respectively.
For each adjacency pair (ui , vj ), assign the entry in
the adjacency matrix aij with value
a) sgn(xi ) × sgn(yj ), with probability 1 − (1 −
|xi |)(1 − |yj |)
b) −sgn(xi ) × sgn(yj ), with probability (1 −
|xi |)(1 − |yj |)

The difference between the scale obtained for a vertex by
executing the scaling algorithm and its scale assigned by the
random graph generator algorithm defines the error for that
vertex. Table III shows the mean error vs vertex degrees plots
for each algorithm applied to 12 different synthetic data sets.
In our experiments, the number of vertices of the graph is
m = n = 100. We used four different distributions for varying
polarization. These were perfectly polarized, Beta, bimodal
and uniform distributions [31]. Perfectly polarized distribution
was obtained by mapping all vertices to the extremes of both

sides with equal probability. We used three different normal
distributions for varying the degree distributions of vertices.
Degree distributions were obtained by N (µ = 30, σ = 2),
N (µ = 30, σ = 5) and N (µ = 30, σ = 10) in order to
evaluate the effect of degree variance on the performance of
the algorithms. We also experimented with different µ values
of 10, 30, and 50 in order to measure the effect of density
variations of the graph on the performance of the algorithms,
which did not show any significant impact.
For each polarization and degree distribution we tested
the performance of two baseline algorithms and our proposed
algorithm. Table III presents these experimental results corresponding to 12 scenarios. In this table, columns correspond to
the variance in degrees, and rows correspond to the polarization
distributions.
In order to better visualize the effect of the degree of
the vertex in determining its scaling position we used the
mean error as an aggregate score. In the scatter plots, x-axis
corresponds to the degree of vertices of a graph, and y-axis
corresponds to mean scaling error. There are some error peaks
at the boundary degree values due to their low frequencies.

degree senator to a moderate location, even though he or she
can be extremely partisan. ANCO-HITS manages the degree
bias by a normalizing scheme, and places this senator to an
appropriate location.
In order to analyze longitudinal voting patterns for the US
Congresses, we downloaded all voting records since the 1st US
Congress, executed ANCO-HITS, and produced an interactive
visualization system as described in [32]. This system is
accessible online at www.PartisanScale.com
Our future work involves developing techniques for detecting and presenting both friendly and unfriendly neighborhoods
of a blog or an issue, and their agreements and disagreements.
We also plan to incorporate longitudinal analysis to detect
trends and trajectories over time.
ACKNOWLEDGMENTS
We would like to thank Dananjayan Thirumalai for helping
us collect the political blogosphere data. This research was
supported in part by US DOD Minerva Research Initiative
grant N00014-09-1-0815.

From the table, we can make the following observations:
•

Across all polarizations, as the vertex degree variance
increases, overall errors for baseline algorithms increase due to their sensitivity to vertex degrees.

•

Between the baseline algorithms, spectral clustering
consistently outperforms CO-HITS.

•

Even though spectral clustering performs almost as
good as our proposed ANCO-HITS for bimodal and
uniform polarization distributions, when the polarization is high, as in the other two distributions, its
performance degrades.

•

As polarization increases, from U-shaped to perfect
polarization, ANCO-HITS performance also increases.
In case of perfect polarization, ANCO-HITS has almost no error.

R EFERENCES
[1]
[2]

[3]
[4]

[5]

[6]

Overall, in every single case our proposed ANCO-HITS
algorithm outperforms the baselines.
IX.

C ONCLUSIONS & F UTURE W ORK

In this paper, we introduced a new problem for scaling and
partitioning signed weighted bipartite graphs. We adapted two
existing algorithms, and proposed a new algorithm to solve this
problem. We used both real data from political blogosphere
and US Congress records, as well as synthetic data to evaluate
these algorithms. Our experiments showed that our proposed
algorithm is very effective and outperforms the two other
baselines. The algorithms in source code and the test data is
available online at www.PartisanScale.com/paperdata
In real world graphs, it is rarely the case that all the nodes
have the same degree. Some bloggers have more extensive
coverage than others. Similarly, some senators miss or abstain
on more votes than others. Partisanship shall not be affected
by the variance in node degrees. A marginal example is the
case when a senator supersedes a deceased one through the
end of the term. All the baseline algorithms assign this low

[7]

[8]

[9]

[10]

[11]

[12]

D. Drezner and H. Farrell, “The power and politics of blogs,” Public
Choice, vol. 134, pp. 15–30, 2008.
T. Mullen and R. Malouf, “A preliminary investigation into sentiment
analysis of informal political discourse,” in AAAI symposium on computational approaches to analysing weblogs (AAAI-CAAW), 2006, pp.
159–162.
R. Malouf and T. Mullen, Graph-based user classification for informal
online political discourse, 2007.
M. Thomas, B. Pang, and L. Lee, “Get out the vote: Determining
support or opposition from congressional floor-debate transcripts,” in
In Proceedings of EMNLP, 2006, pp. 327–335.
M. Bansal, C. Cardie, and L. Lee, “The power of negative thinking:
Exploiting label disagreement in the min-cut classification framework,”
Proceedings of COLING: Companion volume: Posters, pp. 13–16, 2008.
W. Lin and A. Hauptmann, “Are these documents written from different
perspectives?: a test of different perspectives based on statistical distribution divergence,” in Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational
Linguistics, 2006, pp. 1057–1064.
L. A. Adamic and N. Glance, “The political blogosphere and the 2004
u.s. election: divided they blog,” in Proceedings of the 3rd international
workshop on Link discovery, ser. LinkKDD ’05. New York, NY, USA:
ACM, 2005, pp. 36–43.
H. Deng, M. Lyu, and I. King, “A generalized co-hits algorithm and
its application to bipartite graphs,” in Proceedings of the 15th ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2009, pp. 239–248.
M. Rege, M. Dong, and F. Fotouhi, “Co-clustering documents and words
using bipartite isoperimetric graph partitioning,” in Data Mining, 2006.
ICDM’06. Sixth International Conference on. IEEE, 2006, pp. 532–
541.
H. Zha, X. He, C. Ding, H. Simon, and M. Gu, “Bipartite graph partitioning and data clustering,” in Proceedings of the tenth international
conference on Information and knowledge management. ACM, 2001,
pp. 25–32.
J. Kunegis, A. Lommatzsch, and C. Bauckhage, “The slashdot zoo:
mining a social network with negative edges,” in Proceedings of the
18th international conference on World wide web. ACM, 2009, pp.
741–750.
U. V. Luxburg, “A tutorial on spectral clustering,” 2007.

0

20

40

60

80

100

Count

S YNTHETIC DATA PERFORMANCES
Count

Count

TABLE III.

0

20

40

Degree

60

80

100

0

20

40

Degree

N (µ = 30, σ = 2)
1

80

100

N (µ = 30, σ = 10)

1

1

Count

Spectral Clustering
CO−HITS
ANCO−HITS

1

60
Degree

N (µ = 30, σ = 5)
Spectral Clustering
CO−HITS
ANCO−HITS

Spectral Clustering
CO−HITS
ANCO−HITS

0.8

0.8

0.6

0.6

0.6

0.4

0.2

Mean Error

Mean Error

Mean Error

Perfectly Polarized

−0.5

0
Scale

0.5

0.8

0.4

0.2

0

0.2

0
20

40

−1

0

60

80

100

0.4

0
0

20

40

Degree

60

80

100

0

20

40

Degree

1

80

100

1

Count

Spectral Clustering
CO−HITS
ANCO−HITS

1

60
Degree

1
Spectral Clustering
CO−HITS
ANCO−HITS

Spectral Clustering
CO−HITS
ANCO−HITS

0.8

0.8

0.6

0.6

0.6

0.4

0.2

Mean Error

Mean Error

Mean Error

Beta distribution

−0.5

0
Scale

0.5

0.8

0.4

0.2

0

0.2

0
20

40

−1

0

60

80

100

0.4

0
0

20

40

Degree

60

80

100

0

20

40

Degree

1

1

80

100

1

Count

Spectral Clustering
CO−HITS
ANCO−HITS

1

60
Degree

Spectral Clustering
CO−HITS
ANCO−HITS

Spectral Clustering
CO−HITS
ANCO−HITS

0.8

0.8

0.6

0.6

0.6

0.4

0.2

Mean Error

Mean Error

Mean Error

Bimodal Distribution

−0.5

0
Scale

0.5

0.8

0.4

0.2

0

0.2

0
20

40

−1

0

60

80

100

0.4

0
0

20

40

Degree

60

80

100

0

20

40

Degree

1

1

80

100

1

Count

Spectral Clustering
CO−HITS
ANCO−HITS

1

60
Degree

Spectral Clustering
CO−HITS
ANCO−HITS

Spectral Clustering
CO−HITS
ANCO−HITS

0.8

0.8

0.6

0.6

0.6

0.4

0.2

0.4

0.2

0
20

40

60
Degree

80

100

0.4

0.2

0
0

−1

Mean Error

Mean Error

Mean Error

Uniform Distribution

−0.5

0
Scale

0.5

0.8

0
0

20

40

60
Degree

80

100

0

20

40

60
Degree

80

100

[13]

[14]

[15]
[16]
[17]
[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]
[27]
[28]
[29]
[30]
[31]
[32]

A. Ng, M. Jordan, and Y. Weiss, “On spectral clustering: Analysis and
an algorithm,” in Advances in Neural Information Processing Systems
14: Proceeding of the 2001 Conference, 2001, pp. 849–856.
J. Shi and J. Malik, “Normalized cuts and image segmentation,” Pattern
Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8,
pp. 888 –905, aug 2000.
L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation
ranking: Bringing order to the web.” 1999.
J. Kleinberg, “Authoritative sources in a hyperlinked environment,”
Journal of the ACM (JACM), vol. 46, no. 5, pp. 604–632, 1999.
D. Watts and S. Strogatz, “Collective dynamics of small-world networks,” Nature, vol. 393, no. 6684, pp. 440–442, 1998.
I. Dhillon, J. Fan, and Y. Guan, “Efficient clustering of very large
document collections,” Data mining for scientific and engineering
applications, pp. 357–381, 2001.
N. Slonim and N. Tishby, “Document clustering using word clusters via
the information bottleneck method,” in Proceedings of the 23rd annual
international ACM SIGIR conference on Research and development in
information retrieval. ACM, 2000, pp. 208–215.
P. Berkhin, “Survey of clustering data mining techniques,” Grouping
Multidimensional Data: Recent Advances in Clustering, pp. 25–71,
2006.
I. Dhillon, Y. Guan, and B. Kulis, “Kernel k-means: spectral clustering
and normalized cuts,” in Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2004, pp. 551–556.
J. Hartigan and M. Wong, “Algorithm as 136: A k-means clustering
algorithm,” Journal of the Royal Statistical Society. Series C (Applied
Statistics), vol. 28, no. 1, pp. 100–108, 1979.
J. Kunegis, S. Schmidt, A. Lommatzsch, J. Lerner, E. De Luca,
and S. Albayrak, “Spectral analysis of signed graphs for clustering,
prediction and visualization,” in Proc SDM. Citeseer, 2010.
I. Dhillon, “Co-clustering documents and words using bipartite spectral
graph partitioning,” in Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2001, pp. 269–274.
X. Fern and C. Brodley, “Solving cluster ensemble problems by bipartite
graph partitioning,” in Proceedings of the twenty-first international
conference on Machine learning. ACM, 2004, p. 36.
P. Hage and F. Harary, “Structural models in anthropology.” 1983.
S. T. Dennis, “Senate moderates look for more influence,” http://www.
rollcall.com/issues/56 90/-203808-1.html.
J. Newton-Small, “Can ben nelson get a bipartisan stimulus win,” http:
//www.time.com/time/politics/article/0,8599,1877535,00.html.
“Factions in the republican party (united states),” http://en.wikipedia.
org/wiki/Factions in the Republican Party (United States).
“Blue dog coalition,” http://ross.house.gov/BlueDog/Members/.
V. Rohatgi and A. Saleh, An introduction to probability and statistics.
Wiley-India, 2008.
S. Gokalp and H. Davulcu, “Partisan scale,” in Proceedings of the 21st
international conference companion on World Wide Web. ACM, 2012,
pp. 349–352.

Hindawi Publishing Corporation
Advances in Bioinformatics
Volume 2012, Article ID 509126, 12 pages
doi:10.1155/2012/509126

Research Article
BioEve Search: A Novel Framework to Facilitate Interactive
Literature Search
Syed Toufeeq Ahmed,1 Hasan Davulcu,2 Sukru Tikves,2
Radhika Nair,2 and Zhongming Zhao1, 3
1 Department

of Biomedical Informatics, Vanderbilt University, Nashville, TN 37232, USA
of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA
3 Department of Cancer Biology, Vanderbilt University School of Medicine, Nashville, TN 37232, USA
2 Department

Correspondence should be addressed to Syed Toufeeq Ahmed, syed.t.ahmed@vanderbilt.edu
Received 15 November 2011; Revised 7 March 2012; Accepted 28 March 2012
Academic Editor: Jin-Dong Kim
Copyright © 2012 Syed Toufeeq Ahmed et al. This is an open access article distributed under the Creative Commons Attribution
License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly
cited.
Background. Recent advances in computational and biological methods in last two decades have remarkably changed the scale
of biomedical research and with it began the unprecedented growth in both the production of biomedical data and amount of
published literature discussing it. An automated extraction system coupled with a cognitive search and navigation service over
these document collections would not only save time and eﬀort, but also pave the way to discover hitherto unknown information
implicitly conveyed in the texts. Results. We developed a novel framework (named “BioEve”) that seamlessly integrates Faceted
Search (Information Retrieval) with Information Extraction module to provide an interactive search experience for the researchers
in life sciences. It enables guided step-by-step search query refinement, by suggesting concepts and entities (like genes, drugs, and
diseases) to quickly filter and modify search direction, and thereby facilitating an enriched paradigm where user can discover
related concepts and keywords to search while information seeking. Conclusions. The BioEve Search framework makes it easier
to enable scalable interactive search over large collection of textual articles and to discover knowledge hidden in thousands of
biomedical literature articles with ease.

1. Background
Human genome sequencing marked the beginning of the
era of large-scale genomics and proteomics, leading to large
quantities of information on sequences, genes, interactions,
and their annotations. In the same way that the capability
to analyze data increases, the output by high-throughput
techniques generates more information available for testing
hypotheses and stimulating novel ones. Many experimental findings are reported in the -omics literature, where
researchers have access to more than 20 million publications,
with up to 4,500 new ones per day, available through to
the widely used PubMed citation index and Google Scholar.
This vast increase in available information demands novel
strategies to help researchers to keep up to date with recent
developments, as ad hoc querying with Boolean queries is
tedious and often misses important information.

Even though PubMed provides an advanced keyword
search and oﬀers useful query expansion, it returns hundreds
or thousands of articles as result; these are sorted by
publication date, without providing much help in selecting
or drilling down to those few articles that are most relevant
regarding the user’s actual question. As an example of both
the amount of available information and the insuﬃciency
of naı̈ve keyword search, the name of the protein p53 occurs
in 53,528 PubMed articles, and while a researcher interested
specifically in its role in cancer and its interacting partners
might try the search “p53 cancer interaction” to narrow down
the results, this query still yields 1,777 publications, enough
for months of full-time reading [1]. Nonetheless, PubMed is
a very widely used free service and is providing an invaluable
service to the researchers around the world. In March 2007,
PubMed served 82 million (statistics of Medline searches:
http://www.nlm.nih.gov/bsd/medline growth.html) query

2
searches and the usage is ever increasing. A few commercial
products are currently available that provide additional
services, but they also rely on basic keyword search, with
no real discovery or dynamic faceted search. Examples are
OvidSP and Ingenuity Answers, both of which support bookmarking as one means of keeping track of visited citations.
Research tools such as EBIMed (EBIMed: http://www.ebi.ac
.uk/Rebholz-srv/ebimed/index.jsp) [2] and AliBaba (AliBaba:
http://alibaba.informatik.hu-berlin.de)
[3]
provide
additional cross-referencing of entities to databases
such as UniProt or to the GeneOntology. They also try
to identify relations between entities, such as protein-protein
interactions, functional protein annotations, or gene-disease
associations.
Search tools should provide dedicated and intuitive
strategies that help to find relevant literature, starting
with initial keyword searches and drilling down results via
overviews enriched with autogenerated suggestions to refine
queries. One of the first steps in biomedical text mining is to
recognize named entities occurring in a text, such as genes
and diseases. Named entity recognition (NER) is helpful
to identify relevant documents, index a document collection, and facilitate information retrieval (IR) and semantic
searches [4]. A step on top of NER is to normalize each entity
to a base form (also called grounding and identification); the
base form often is an identifier from an existing, relevant
database; for instance, protein names could be mapped to
UniProt IDs [5, 6]. Entity normalization (EN) is required
to get rid of ambiguities such as homonyms, and map synonyms to one and the same concept. This further alleviates
the tasks of indexing, IR, and search. Once named entities
have been identified, systems aim to extract relationships
between them from textual evidences; in the biomedical
domain, these include gene-disease associations and proteinprotein interactions. Such relations can then be made available for subsequent search in relational databases or used for
constructing particular pathways and entire networks [7].
Information extraction (IE) [8–11] is the extraction of
salient facts about prespecified types of events, entities [12],
or relationships from free text. Information extraction from
free text utilizes shallow-parsing techniques [13], part-ofspeech tagging [14], noun and verb phrase chunking [15],
predicate-subject and object relationships [13], and learned
[8, 16, 17] or hand-build patterns [18] to automate the creation of specialized databases. Manual pattern engineering
approaches employ shallow parsing with patterns to extract
the interactions. In the system presented in [19], sentences
are first tagged using a dictionary-based protein name
identifier and then processed by a module which extracts
interactions directly from complex and compound sentences
using regular expressions based on part of speech tags. IE
systems look for entities, relationships among those entities,
or other specific facts within text documents. The success of
information extraction depends on the performance of the
various subtasks involved.
The Suiseki system of Blaschke et al. [20] also uses regular
expressions, with probabilities that reflect the experimental
accuracy of each pattern to extract interactions into predefined frame structures. Genies [21] utilizes a grammar-based

Advances in Bioinformatics
natural language processing (NLP) engine for information
extraction. Recently, it has been extended as GeneWays [22],
which also provides a Web interface that allows users to
search and submit papers of interest for analysis. The BioRAT
system [23] uses manually engineered templates that combine lexical and semantic information to identify protein
interactions. The GeneScene system [24] extracts interactions using frequent preposition-based templates.
Over the last years, a focus has been on the extraction
of protein-protein interactions in general, recently including
extraction from full text articles, relevance ranking of
extracted information, and other related aspects (see, for
instance, the BioCreative community challenge [25]). The
BioNLP’09 Shared Task concentrated on recognition of more
fine-grained molecular events involving proteins and genes
[26]. Both papers give overviews over the specific tasks and
reference articles by participants.
One of the first eﬀorts to extract information on biomolecular events was proposed by Yakushiji et al. [27]. They
implemented an argument structure extractor based on full
sentence parses. A list of target verbs have specific argument
structures assigned to each. Frame-based extraction then
searches for filler of each slot required according to the particular arguments. On an small in-house corpus, they found
that 75% of the errors can be attributed to erroneous parsing
and another 7% to insuﬃcient memory; both causes might
have less impact on recent systems due to more accurate
parsers and larger memory.
Ding et al. [28] studied the extraction of protein-protein
interactions using the Link Grammar parser. After some
manual sentence simplification to increase parsing eﬃciency,
their system assumed an interaction whenever two proteins
were connected via a link path; an adjustable threshold
allowed to cut oﬀ too long paths. As they used the original
version of Link Grammar, Ding et al. [28] argued that
adaptations to the biomedical domain would enhance the
performance.
An information extraction application analyzes texts
and presents only the specific information from them that
the user is interested in [29]. IE systems are knowledge
intensive to build and are to varying degrees tied to particular
domains and scenarios such as target schema. Almost all IE
applications start with fixed target schema as a goal and are
tuned to extract information from unstructured text that will
fit the schema. In scenarios where target schema is unknown,
open information extraction systems [30] like KnowItNow
[31] and TextRunner [32] allow rules to be defined easily
based on the extraction need. An hybrid application (IR +
IE) that leverages the best of information retrieval (ability
to relevant texts) and information extraction (analyze text
and present only specific information user is interested in)
would be ideal in cases when the target extraction schema is
unknown. An iterative loop of IR and IE with user’s feedback
will be potentially useful. For this application, we will
need main components of IE system (like parts-of-speech
tagger, named entity taggers, shallow parsers) preprocesses
the text before being indexed by a custom-built augmented
index that helps retrieve queries of the type “Cities such
as ProperNoun(Head(NounPhrase)).” Cafarella and Etzioni

Advances in Bioinformatics
[33] have done work in this direction to build a search
engine for natural language and information extraction
applications.
Exploratory search [34] is a topic that has grown from the
fields of information retrieval and information seeking but
has become more concerned with alternatives to the kind of
search that has received the majority of focus (returning the
most relevant documents to a Google-like keyword search).
The research is motivated by questions like “what if the user
does not know which keywords to use?” or “what if the user
is not looking for a single answer?”. Consequently, research
began to focus on defining the broader set of information
behaviors in order to learn about situations when a user
is—or feels—limited by having only the ability to perform a
keyword search (source: http://en.wikipedia.org/wiki/Exploratory search). Exploratory search can be defined as specialization of information exploration which represents the
activities carried out by searchers who are either [35]:
(1) unfamiliar with the domain of their goal (i.e., need to
learn about the topic in order to understand how to
achieve their goal);
(2) unsure about the ways to achieve their goals (either
the technology or the process); or even
(3) unsure about their goals in the first place.
A faceted search system (or parametric search system)
presents users with key value metadata that is used for
query refinement [36]. By using facets (which are metadata
or class labels for entities such as genes or diseases), users
can easily combine the hierarchies in various ways to refine
and drill down the results for a given query; they do not
have to learn custom query syntax or to restart their search
from scratch after each refinement. Studies have shown
that users prefer faceted search interfaces because of their
intuitiveness and ease of use [37]. Hearst [38] shares her
experience, best practices, and design guidelines for faceted
search interfaces, focusing on supporting flexible navigation,
seamless integration with directed search, fluid alternation
between refining and expanding, avoidance of empty results
sets, and most importantly making users at ease by retaining
a feeling of control and understanding of the entire search
and navigation process. To improve web search for queries
containing named entities [39], automatically identify the
subject classes to which a named entity might refer to and
select a set of appropriate facets for denoting the query.
Faceted search interfaces have made online shopping
experiences richer and increased the accessibility of products
by allowing users to search with general keywords and
browse and refine the results until the desired sub-set is
obtained (SIGIR’2006 Workshop on Faceted Search (CFP):
http://sites.google.com/site/facetedsearch/). Faceted navigation delivers an experience of progressive query refinement
or elaboration. Furthermore, it allows users to see the
impact of each incremental choice in one facet on the
choices in other facets. Faceted search combines faceted
navigation with text search, allowing users to access (semi)
structured content, thereby providing support for discovery

3
and exploratory search, areas where conventional search falls
short [40].

2. Approach
In an age of ever increasing published research documents
(available in search-able textual form) containing amounts
of valuable information and knowledge that are vital to
further research and understanding, it becomes imperative
to build tools and systems that enable easier and quick access
to right information the user is seeking for, and this has
already become an information overload problem in diﬀerent domains. Information Extraction (IE) systems provide
an structured output by extracting nuggets of information
from these text document collections, for a defined schema.
The output schema can vary from simple pairwise relations
to a complex, nested multiple events.
Faceted search and navigation is an eﬃcient way to
browse and search over a structured data/document collection, where the user is concerned about the completeness of
the search, not just top ranked results. Faceted search system
needs structured input documents, and IE systems extract
structured information from text documents. By combining
these two paradigms, we are able to provide faceted search
and navigation over unstructured text documents, and, with
this fusion, we are also able to leverage real utility of information extraction, that is, finding hidden relationships as the
user goes through a search process, and to help refine the
query to more satisfying and relevant level, all while keeping
user feel incontrol of the whole search process.
We developed BioEve Search (http://www.bioeve.org/)
framework to provide fast and scalable search service, where
users can quickly refine their queries and drill down to the
articles they are looking for in a matter of seconds, corresponding to a few number of clicks. The system helps identify
hidden relationships between entities (like drugs, diseases,
and genes), by highlighting them using a tag cloud to give
a quick visualization for eﬃcient navigation. In order to
have suﬃcient abstraction between various modules (and
technologies used) in this system, we have divided this framework into four diﬀerent layers (refer to Figure 1) and they
are (a) Data Store layer, (b) Information Extraction layer,
(c) Faceting layer, and (d) Web Interface layer. Next sections
explain each layer of this framework in more details.
2.1. Data Store Layer. The Data Store layer preprocesses
and stores the documents in an indexed data store to make
them eﬃciently accessible to the modules of upper layer
(information extraction layer). Format conversion is needed
sometimes (from ASCII to UTF-8 or vice versa), or XML
documents need to be converted to text documents before
being passed to next module. After the documents are in the
required format and cleansed, they are stored in a indexed
data store for eﬃcient and fast access to either individual
documents or the whole collections. The data store can be
implemented using an Indexer service like (Apache Lucene
(Lucene: http://lucene.apache.org/) or any database like
MySQL). The Medline dataset is available as zipped XML

Advances in Bioinformatics

Data import
(XML)

Solr schema
.XML

Web API
(JSON, HTTP/XML)

Faceting engine
(Apache Solr)

Discovery interface
(AJAX and Javascript)

Web service
(Apache Tomcat)

BioEve event

CRF classifier

Extraction engine
(biomolecular events)

ABNER

Data store
layer

Information
extraction
layer

Faceting layer

Web interface
layer

4

OSCAR3
MeSH

Preprocessing
(XML2text-SAX
parser)
MEDLINE

Apache Lucene (data store)

Figure 1: BioEve search framework architecture.

files that needed XML2 text conversion, after which we could
ingest them into an indexer, Apache Lucene in our case. Such
an indexer allows for faster access and keyword-based text
search to select a particular subset of abstracts for further
processing.

once an article is classified and annotated with diﬀerent
entity types, it does not need to be processed again for each
search query. This step can be done preindexing and as a
batch process.
2.3. Faceting Layer

2.2. Information Extraction Layer. For recognizing diﬀerent
gene/protein names, DNA, RNA, cell line, and cell types,
we leveraged ABNER [41], A Biomedical Named Entity
Recognizer. We used OSCAR3 (Oscar3: http://sourceforge
.net/projects/oscar3-chem/) (Open Source Chemistry Analysis Routines) to identify chemical names and chemical structures. To annotate disease names, symptoms, and causes,
we used a subset of the Medical Subject Heading (MeSH)
dataset (MeSH: http://www.nlm.nih.gov/mesh/).
2.2.1. Annotating Biomolecular Events in the Text. A first
step towards bio-event extraction is to identify phrases in
biomedical text which indicate the presence of an event.
The labeled phrases are classified further into nine event
types (based on the Genia corpus (BioNLP’09 Shared Task 1:
http://www.nactem.ac.uk/tsujii/GENIA/SharedTask/)). The
aim of marking such interesting phrases is to avoid looking
at the entire text to find participants, as deep parsing of sentences can be a computationally expensive process, especially
for the large volumes of text. We intend to mark phrases in
biomedical text, which could contain a potential event, to
serve as a starting point for extraction of event participants.
Section 6.1 gives more details about our experimentations
with classification and annotation of biomedical entities.
All the classification and annotation were done oﬄine
before the annotated articles are indexed for the search as

2.3.1. Faceting Engine. To provide faceted classification and
navigation over these categories (facets), many oﬀ-the-shelf
systems are available such as in academia; Flamenco project
(Flamenco: http://flamenco.berkeley.edu/) (from University
of California Berkeley) and mspace (mspace: http://mspace
.fm/) (University of Southampton) and in enterprise area;
Apache Solr (Apache Solr: http://lucene.apache.org/solr/)
and Endeca (Endeca: http://www.endeca.com/). We used the
Apache Solr library for faceted search, which also provides an
enterprise quality full-text search.
2.3.2. Shared Schema between IE Layer and Faceting Layer. In
order to facilitate indexing and faceting over the extracted
semi-structured text articles, both IE layer and faceting layer
needs to share a common schema. A sample of shared schema
used for enabling interaction between these layers is shown in
Scheme 1.
2.4. Web Interface Layer. With the advent of Web 2.0 technologies, web-based interfaces have undergone delightful
improvements and now provide rich dynamic experiences.
Key component in this layer is a user interface that connects
the user with the web service from the faceting layer and
provides features that allow search, selection of facet/values,
refinement, query restart, and dynamic display of a result

Advances in Bioinformatics

<field
<field
<field
<field
<field
<field

5

name="pmid" type="string" indexed="true" stored="true" required="true"/>
name="text" type="text" indexed="true" stored="true" multiValued="true"/>
name="title" type="text" indexed="true" stored="true"/>
name="gene" type="string" indexed="true" stored="true" multiValued="true"/>
name="drug" type="string" indexed="true" stored="true" multiValued="true"/>
name="disease" type="string" indexed="true" stored="true" multiValued="true"/>
Scheme 1

set as user interacts and navigates. It also provides the bulk
import of data for further analysis of the faceting/extraction.
The web interface provides following features for interactive search and navigation. The interface presents a number
of entities types (on the left panel) along with the specific
instances/values, from previous search results, and the
current query. Users can choose any of the highlighted values
of these entity types to interactively refine the query (add
new values/remove any value from the list with just one
click) and thereby drill down to the relevant articles quickly
without actually reading the entire abstracts. Users can easily
remove any of the previous search terms, thus widening
the current search. We implemented the BioEve user interface using AJAX (AJAX: http://evolvingweb.github.com/ajaxsolr/), Javascript, and JSON to provide rich dynamic experience. The web interface runs on an Apache Tomcat server.
Next section explains about navigation aspect of the user
interface.

3. User Interface: A Navigation Guide
Search interface is divided into left and right panels, see
Figure 2, basically displaying enriched keywords and results,
respectively.
Left panel: it oﬀers suggestions and insights (based on
cooccurrence frequency with the query terms) for diﬀerent
entities types, such as genes, diseases, and drugs/chemicals.
(i) Left panel shows navigation/refinement categories
(genes, diseases, and drugs); users can click on any of
the entity names (in light blue) to refine the search.
By clicking on an entity, the user adds that entity
to the search and the results on the right panel are
refreshed on the y to reflect the refined results.
(ii) Users can add or remove any number of refinements
to the current search query until they reach the
desired results set (shown in the right panel).
Right panel: it shows the user’s current search results
and is automatically refreshed based on user’s refinement and
navigation choices on the left panel.
(i) The top of the panel shows users current query terms
and navigation so far. Here, users can also deselect
any of the previously selected entities or even all of
them by single click on “remove all.” By deselecting
any entities, user is essentially expanding the search
and the results in the right panel are refreshed on
the fly to remaining query entities to oﬀer a dynamic
navigation experience.

(ii) Abstracts results on this panel show “title” of the
abstract (in light red), full abstract text (in black, if
abstract text is available).
(iii) Below the full abstract text, the list of entities mentioned in that abstracts (in light blue) is shown. These
entities names are clickable and will start a new search
for that entity name, with a single click.
(iv) A direct URL is also provided to the abstract page on
http://pubmed.gov in case the user wants to access
additional information such as authors, publication
type, or links to a full-text article.

4. Interactive Search and Navigation:
A Walk through and behind the Scenes
Let us start an example search process, say with the query
“cholesterol” and the paragraph titled “behind-the-scenes”
gives details of the computational process behind the action.
(1) The autocomplete feature helps in completion of the
name while typing if the word is previously mentioned in the
literature, which is the case here with “cholesterol.”
Behind-the-scenes: as user starts typing, the query is
tokenized (in case of multiple words) and search is made to
retrieve word matches (and not the result rows yet) using the
beginning with the characters user has already typed, and this
loop continues. Technologies at play are jQuery, AJAX, and
faceting feature of Apache Solr. Once the query is submitted
by the user, the results rows also contain the annotated entity
names and these are used to generate tag clouds, using the
faceting classification entity frequency count.
The search results in 27177 articles hits (Figure 3). Those
are a lot of articles to read. How about narrowing down these
results with some insights given by BioEve Search?
(2) In left panel, “hepatic lipase” is highlighted; let
us click on that as it shows some important relationship
between “cholesterol” and “hepatic lipase.” The search
results are now narrowed down to 195 articles from 27177
(Figure 4). That is still a lot of articles to read this afternoon,
how about some insights on diseases.
Behind-the-scenes: once user click on a highlighted entity
name in tag cloud, this term (gene: “hepatic lipase”) is added
to the search filter and the whole search process and tag could
be generated again for the new query.
You can see disease “hyperthyroidism” highlighted in
Figure 5.
(3)Selecting “hyperthyroidism” drills results down to 3,
as can be seen in Figure 6.

6

Advances in Bioinformatics

Figure 2: A sample screen shot of the main search screen. Left panel shows clickable top relevant entities, which if selected refines the query
and results dynamically. User can deselect any of the previously selected entities to refine query more, and the results are updated dynamically
to reflect the current selected list of entities.

Figure 3: A sample result set with the query “cholesterol.”

The top result is about “Treatment of hyperthyroidism:
eﬀects on hepatic lipase, lipoprotein lipase, LCAT and plasma
lipoproteins”. With few clicks user can refine search results to
more relevant articles.

5. Initial User Reviews and Feedback
We asked three life science researchers to review and provide
feedback on ease of search and novelty of the system, and

shown below is their feedback (paraphrased). Their names
and other details are removed for privacy purposes.
5.1. Researcher One, P.h.D, Research Fellow, Microbiology,
University of California, Berkeley
“ I am impressed by ease of its use.” “When I
have the confidence that BioEve is indexing all
the data without missing any critical article, I

Advances in Bioinformatics

7

Figure 4: “Hepatic-lipase” selected.

Figure 5: “Hyperthyroidism” highlighted.

Figure 6: Final refined search results.

8

Advances in Bioinformatics
will be compelled to use this search tool. I believe
a finished product will be immensely useful and
could become a popular tool for life science
researchers.”

5.2. Researcher Two, P.h.D, Investigator and Head, Molecular
Genetics Laboratory
“You have a powerful search. Synchronize this
with MEDLINE. Connect with more databases,
OMIM, Entrez Gene . . .. You can get cell line
database from ATCC.org.”

Table 1: Classification approaches used: Naı̈ve Bayes classifier
(NBC), NBC + Expectation Maximization (EM), Maximum
Entropy (MaxEnt), Conditional Random Fields (CRFs).
Granularity
Single label,
Sentence level

Features

Classifier

Bag-of-words (BOW)

NBC

BOW + gene names boosted
BOW + trigger words boosted
BOW + gene names and trigger
words boosted

Multiple labels

BOW

NBC +
EM

5.3. Researcher Three, P.h.D, Postdoc Researcher, Faculty of
Kinesiology, University of Calgary
“I particularly like the idea of having larger fonts
for the more relevant terms highlighting what is
researched more often.”

Sentence level
Event trigger
phrase labeling

MaxEnt
BOW +

CRFs

3-gram and 4-gram
prefixes and suﬃxes +
orthographic features +
trigger phrase dictionary

6. Methods
6.1. Information Extraction: Annotating Sentences with
Biomolecular Event Types. The first step towards bioevent
extraction is to identify phrases in biomedical text which
indicate the presence of an event. The aim of marking such
interesting phrases is to avoid looking at the entire text to find
participants. We intend to mark phrases in biomedical text,
which could contain a potential event, to serve as a starting
point for extraction of event participants. We experimented
with well-known classification approaches, from a naı̈ve
Bayes classifier to the more sophisticated machine classification algorithms Expectation Maximization, Maximum
Entropy, and Conditional Random Fields. Overview of
diﬀerent classifiers applied at diﬀerent levels of granularity
and the features used by these classifiers is shown in Table 1.
For naı̈ve Bayes classifier implementation, we utilized
WEKA (WEKA: http://www.cs.waikato.ac.nz/ml/weka/)
library, a collection of machine learning algorithms for
data mining tasks, for identifying single label per sentence
approach. WEKA does not support multiple labels for the
same instance. Hence, we had to include a tradeoﬀ here by
including the first encountered label in the case where the
instance had multiple labels. For Expectation Maximization
(EM) and Maximum Entropy (MaxEnt) algorithms, we used
classification algorithms from MALLET library (MALLET:
http://mallet.cs.umass.edu/index.php). Biomedical abstracts
are split into sentences. For training purposes, plain text
sentences are transformed into training instances as required
by MALLET.
6.1.1. Feature Selection for Naı̈ve Bayes, EM, and MaxEnt
Classifiers. For the feature sets mentioned below, we used the
TF-IDF representation. Each vector was normalized based
on vector length. Also, to avoid variations, words/phrases
were converted to lowercase. Based on WEKA library token
delimiters, features were filtered to include those which
had an alphabet as a prefix, using regular expressions.

For example, features like −300 bp were filtered out, but
features like p55, which is a protein name, were retained.
We experimented with the list of features described below,
to understand how well each feature suits the corpus under
consideration.
(i) Bag-of-words model: this model classified sentences
based on word distribution.
(ii) Bag-of-words with gene names boosted: the idea was
to give more importance to words, which clearly
demarcate event types. To start with, we included
gene names provided in the training data. Next, we
used the ABNER (ABNER: http://pages.cs.wisc.edu/
∼bsettles/abner/), a gene name tagger, to tag gene
names, apart from the ones already provided to us.
We boosted weights for renamed feature “protein”, by
2.0.
(iii) Bag-of-words with event trigger words boosted: we
separately tried boosting event trigger words. The list
of trigger words was obtained from training data.
This list was cleaned to remove stop words. Trigger
words were ordered in terms of their frequency of
occurrence with respect to an event type, to capture
trigger words which are most discriminative.
(iv) Bag-of-words with gene names and event trigger
words boosted: the final approach was to boost both
gene names and trigger words together. Theoretically,
this approach was expected to do better than previous
two feature sets discussed. Combination of discriminative approach of trigger words and gene name
boosting was expected to train the classifier better.
6.1.2. Evaluation of Sentence Level Classification Using Naı̈ve
Bayes Classifier. This approach assigns a single label to

Advances in Bioinformatics

9

Table 2: Single label, sentence level results.
Classifier Feature set
Bag-of-words
Bag-of-words + gene name boosting
NBC
Bag-of-words + trigger word boosting
Bag-of-words + trigger word boosting +
Gene name boosting
Bag-of-POS tagged words

Precision
62.39%
50.00%
49.92%
49.77%
43.30%

each sentence. For evaluation purposes, the classifier is
tested against GENIA development data. For every sentence,
evaluator process checks if the event type predicted is the
most likely event in that sentence. In case a sentence has more
than one event with equal occurrence frequency, classifier
predicted label is compared with all these candidate event
types. The intent of this approach was to just understand
the features suitable for this corpus. Classifier evaluated was
NaiveBayesMultinomial classifier from Weka (http://www.cs
.waikato.ac.nz/ml/weka/) library, which is a collection of
machine learning algorithms for data mining tasks. Table 2
shows precision results for NBC classifier with diﬀerent
feature sets for single label per sentence classification.
6.2. Conditional Random Fields Based Classifier. Conditional
Random fields (CRFs) are undirected statistical graphical
models, a special case of which is a linear chain that
corresponds to a conditionally trained finite-state machine
[41]. CRFs in particular have been shown to be useful in
parts-of-speech tagging [42] and shallow parsing [42]. We
customized ABNER which is based on MALLET, to suit our
needs. ABNER employs a set of orthographic and semantic
features.
6.2.1. Feature Selection for CRF Classifier. The default model
included the training vocabulary (provided as part of the
BIONLP-NLPBA 2004 shared task) in the form of 17
orthographic features based on regular expressions [41].
These include upper case letters (initial upper case letter, all
upper case letters, mix of upper and lower case letters), digits
(special expressions for single and double digits, natural
numbers, and real numbers), hyphen (special expressions
for hyphens appearing at the beginning and end of a
phrase), other punctuation marks, Roman and Greek words,
and 3-gram and 4-gram suﬃxes and prefixes. ABNER uses
semantic features that are provided in the form of handprepared (Greek letters, amino acids, chemical elements,
known viruses, abbreviations of all these) and databasereferenced lexicons (genes, chromosome locations, proteins,
and cell lines).
6.3. Evaluation of Sentence Classification Approaches. The
framework is designed for large-scale extraction of molecular
events from biomedical texts. To assess its performance, we
evaluated the underlying components on the GENIA event
dataset made available as part of BioNLP’09 Shared Task

[26]. This data consists of three diﬀerent sets: the training
set consists of 800 PubMed abstracts (with 7,499 sentences),
the development set has 150 abstracts (1,450 sentences),
and the test set has 260 abstracts (2,447 sentences). We
used the development set for parameter optimization and
fine tuning and evaluated the final system on the test set.
Employed classifiers were evaluated based on precision and
recall. Precision indicates the correctness of the system,
by measuring number of samples correctly classified in
comparison to the total number of classified sentences. Recall
indicates the completeness of the system, by calculating the
number of results which actually belong to the expected set of
results. Sentence level single label classification and sentence
level multilabel classification approaches were evaluated
based on how well the classifier labels a given sentence from
a test set with one of the nine class labels. Phrase level
classification using CRF model was evaluated based on how
well the model tags trigger phrases. Evaluating this approach
involved measuring the extent to which the model identifies
that a phrase is a trigger phrase and how well it classifies a
tagged trigger phrase under one of the nine predefined event
types. Retrieved trigger phrases refer to the ones which are
identified and classified by the CRF sequence tagger. Relevant
trigger phrases are the ones which are expected to be tagged
by the model. Retrieved and relevant trigger words refer to
the tags which are expected to be classified and which are
actually classified by the CRF model. All the classifiers are
trained using BioNLP shared task training data and tested
against BioNLP shared task development abstracts.
We compare the above three approaches for classification
in Table 3. CRF has a good tradeoﬀ as compared to Maximum Entropy classifier results. As compared to multiple
labels, sentence level classifiers, it performs better in terms
of having a considerably good accuracy for most of the event
types with a good recall. It not only predicts the event types
present in the sentence, but also localizes the trigger phrases.
There are some entries where ME seems to perform better
than CRF; for example, in case of positive regulation, where
the precision is as high as 75%. However, in this case, the
recall is very low (25%). The reason noticed (in training
examples) was that, most of the true example sentences of
positive regulation or negative regulation class type were
misclassified as either phosphorylation or gene expression.
The F1-score for CRF indicates that, as compared to the
other approaches, CRF predicts 80% of the relevant tags, and,
among these predicted tags, 68% of them are correct.
6.3.1. Evaluation of Phrase Level Labeling. Evaluation of
this approach was focused more on the overlap of phrases
between the GENIA annotated development and CRF tagged
labels. The reason being for each abstract in the GENIA
corpus, there is generally a set of biomedical entities present
in it. For the shared task, only a subset of these entities was
considered in the annotations, and accordingly only events
concerning these annotated entities were extracted. However,
based on the observation of the corpus, there was a probable
chance of other events involving entities not selected for the
annotations. So we focused on the coverage, where both the
GENIA annotations and CRF annotations agree upon. CRF

10

Advances in Bioinformatics

Table 3: Summary of classification approaches: test instances (marked events) for each class type in test dataset. Precision, recall, and F1score in percentage. Compared to NB + EM and CRF, Maximum Entropy based classifier had better average precision, but CRF has best
recall and good precision, giving it best F-Measure of the three well-known classifiers.
Event type

NB + EM

Test instances
Total: 942

P

R

MaxEnt
F1

P

R

CRF
F1

P

R

F1

Phosphorylation

38

62

42

50

97

73

83

80

83

81

Protein catabolism

17

60

47

53

97

73

83

85

86

85

Gene expression

200

60

41

49

88

58

70

75

81

78

Localization

39

39

47

43

61

69

65

67

79

72

Transcription

60

24

52

33

49

80

61

57

78

66

Binding

153

56

63

59

65

62

63

65

81

72

Regulation

90

47

69

55

52

67

58

62

73

67

Positive regulation

220

70

27

39

75

25

38

55

74

63

Negative regulation

125

42

46

44

54

38

45

68

82

74

51

48

47

71

61

63

68

80

73

Average

7. Discussion and Conclusions

Table 4: CRF sequence labeling results.
Type of evaluation
Exact boundary matching
Soft boundary matching

Coverage %
79%
82%

performance was evaluated on two fronts in terms of this
overlap.
(i) Exact boundary matching: this involves exact label
matching and exact trigger phrase match.
(ii) Soft boundary matching: this involves exact label
matching and partial trigger phrase match, allowing
1-word window on either side of the actual trigger
phrase.
Checking of the above constraints was a combination
of template matching and manually filtering of abstracts.
Table 4 gives an estimate of the coverage. Soft boundary
matching increases the coverage by around 3%. Table 3
gives the overall evaluation of CRF with respect to GENIA
corpus. With regards to the CRF results, accuracy for positive
regulation is comparatively low. Also, the test instances for
positive regulation were more than any other event type. So
this reduced average precision to some extent.
A detailed analysis of the results showed that around 3%
tags were labeled incorrectly in terms of the event type. There
were some cases where it was not certain whether an event
should be marked as regulation or positive regulation. Some
examples include “the expression of LAL-mRNA,” where
“LAL-mRNA” refers to a gene. As per examples seen in the
training data, the template of the form “expression of <gene
name>” generally indicates presence of a Gene expression
event. Hence, more analysis may be need to exactly filter out
such annotations as true negatives or deliberately induced
false positives.

PubMed is one of the most well known and used citation
indexes for the Life Sciences. It provides basic keyword
searches and benefits largely from a hierarchically organized
set of indexing terms, MeSH, that are semi-automatically
assigned to each article. PubMed also enables quick searches
for related publications given one or more articles deemed
relevant by the user. Some research tools provide additional
cross-referencing of entities to databases such as UniProt
or to the GeneOntology. They also try to identify relations
between entities of the same or diﬀerent types, such as
protein-protein interactions, functional protein annotations,
or gene-disease associations. GoPubMed [43] guides users
in their everyday searches by mapping articles to concept
hierarchies, such as the Gene Ontology and MeSH. For each
concept found in abstracts returned by the initial user query,
GoPubMed computes a rank based on occurrences of that
concept. Thus, users can quickly grasp which terms occur
frequently, providing clues for relevant topics and relations,
and refine subsequent queries by focusing on particular
concepts, discarding others.
In this paper, we presented BioEve Search framework,
which can help identify important relationships between
entities such as drugs, diseases, and genes by highlights
them during the search process. Thereby, allowing the
researcher not only to navigate the literature, but also to see
entities and the relations they are involved in immediately,
without having to fully read the article. Nonetheless, we
envision future extensions to provide a more complete and
mainstream service and here are few of these next steps.
Keeping the search index up-to-date and complete: we
are adding a synchronization module that will frequently
check with Medline for supplement articles as they are
published; these will typically be in the range of 2500–4500
new articles per day. Frequent synchronization is necessary
to keep BioEve abreast with Medline collection and give users
the access to the most recent articles.

Advances in Bioinformatics
Normalizing and grounding of entity names: as the same
gene/protein can be referred by various names and symbols
(e.g., the TRK-fused gene is also known as TF6; TRKT3;
FLJ36137; TFG), a user searching for any of these names
should find results mentioning any of the others. Removal
of duplicates and cleanup of nonbiomedical vocabulary
that occurs in the entity tag clouds will further improve
navigation and search results.
Cross-referencing with biomedical databases: we want
to cross-reference terms indexed with biological databases.
For example, each occurrence of a gene could be linked to
EntrezGene and OMIM; cell lines can be linked and enriched
with ATCC.org’s cell line database; we want to crossreference disease names with UMLS and MeSH to provide
access to ontological information. To perform this task of
entity normalization, we have previously developed Gnat [6],
which handles gene names. Further entity classes that exhibit
relatively high term ambiguity with other classes or within
themselves are diseases, drugs, species, and GeneOntology
terms (“Neurofibromatosis 2” can refer to the disease or
gene).

Conflict of Interests
To the authors knowledge, there is no conflict of interest with
name “BioEve” or with any trademarks.

Acknowledgments
The authors like to thank Jeorg Hakenberg, Chintan Patel,
and Sheela P. Kanwar for valuable discussions, ideas, and
help with writing this paper. They also wish to thank the
researchers who provided an initial user review and gave
them valuable feedback.

References
[1] S. Pyysalo, A dependency parsing approach to biomedical text
mining, Ph.D. thesis, 2008.
[2] D. Rebholz-Schuhmann, H. Kirsch, M. Arregui, S. Gaudan, M.
Riethoven, and P. Stoehr, “EBIMed—text crunching to gather
facts for proteins from Medline,” Bioinformatics, vol. 23, no. 2,
pp. e237–e244, 2007.
[3] C. Plake, T. Schiemann, M. Pankalla, J. Hakenberg, and U.
Leser, “ALIBABA: pubMed as a graph,” Bioinformatics, vol. 22,
no. 19, pp. 2444–2445, 2006.
[4] U. Leser and J. Hakenberg, “What makes a gene name? Named
entity recognition in the biomedical literature,” Briefings in
Bioinformatics, vol. 6, no. 4, pp. 357–369, 2005.
[5] H. Xu, J. W. Fan, G. Hripcsak, E. A. Mendonça, M. Markatou,
and C. Friedman, “Gene symbol disambiguation using knowledge-based profiles,” Bioinformatics, vol. 23, no. 8, pp. 1015–
1022, 2007.
[6] J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and G.
Gonzalez, “Inter-species normalization of gene mentions with
GNAT,” Bioinformatics, vol. 24, no. 16, pp. i126–i132, 2008.
[7] K. Oda, J. D. Kim, T. Ohta et al., “New challenges for
text mining: mapping between text and manually curated
pathways,” BMC Bioinformatics, vol. 9, supplement 3, article
S5, 2008.

11
[8] M. E. Caliﬀ and R. J. Mooney, “Relational learning of patternmatch rules for information extraction,” in Working Notes
of AAAI Spring Symposium on Applying Machine Learning to
Discourse Processing, pp. 6–11, AAAI Press, Menlo Park, Calif,
USA, 1998.
[9] N. Kushmerick, D. S. Weld, and R. B. Doorenbos, “Wrapper
induction for information extraction,” in Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI
’97), pp. 729–737, 1997.
[10] L. Schubert, “Can we derive general world knowledge from
texts?” in Proceedings of the 2nd International Conference on
Human Language Technology Research, pp. 94–97, Morgan
Kaufmann, San Francisco, Calif, USA, 2002.
[11] M. Friedman and D. S. Weld, “Eﬃciently executing information-gathering plans,” in Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI ’97), pp.
785–791, Nagoya, Japan, 1997.
[12] R. Bunescu, R. Ge, R. J. Kate et al., “Comparative experiments
on learning information extractors for proteins and their
interactions,” Artificial Intelligence in Medicine, vol. 33, no. 2,
pp. 139–155, 2005.
[13] W. Daelemans, S. Buchholz, and J. Veenstra, “Memory-based
shallow parsing,” in Proceedings of the Conference on Natural
Language Learning (CoNLL ’99), vol. 99, pp. 53–60, 1999.
[14] E. Brill, “A simple rule-based part-of-speech tagger. In Proceedings of ANLP-92,” in Proceedings of the 3rd Conference
on Applied Natural Language Processing, pp. 152–155, Trento,
Italy, 1992.
[15] A. Mikheev and S. Finch, “A workbench for finding structure
in texts,” in Proceedings of the Applied Natural Language
Processing (ANLP ’97), Washington, DC, USA, 1997.
[16] M. Craven and J. Kumlien, “Constructing biological knowledge bases by extracting information from text sources,” in
Proceedings of the 7th International Conference on Intelligent
Systems for Molecular Biology, pp. 77–86, AAAI Press, 1999.
[17] K. Seymore, A. McCallum, and R. Rosenfeld, “Learning hidden markov model structure for information extraction,” in
Proceedings of the AAAI Workshop on Machine Learning for
Information Extraction, 1999.
[18] L. Hunter, Z. Lu, J. Firby et al., “OpenDMAP: an open source,
ontology-driven concept analysis engine, with applications
to capturing knowledge regarding protein transport, protein
interactions and cell-type-specific gene expression,” BMC
Bioinformatics, vol. 9, article 78, 2008.
[19] T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi, “Automated
extraction of information on protein-protein interactions
from the biological literature,” Bioinformatics, vol. 17, no. 2,
pp. 155–161, 2001.
[20] C. Blaschke, M. A. Andrade, C. Ouzounis, and A. Valencia,
“Automatic extraction of biological information from scientific text: protein-protein interactions,” AAAI, pp. 60–67.
[21] C. Friedman, P. Kra, H. Yu, M. Krauthammer, and A. Rzhetsky,
“GENIES: a natural-language processing system for the extraction of molecular pathways from journal articles,” Bioinformatics, vol. 17, no. 1, pp. S74–S82, 2001.
[22] A. Rzhetsky, I. Iossifov, T. Koike et al., “GeneWays: a system for
extracting, analyzing, visualizing, and integrating molecular
pathway data,” Journal of Biomedical Informatics, vol. 37, no.
1, pp. 43–53, 2004.
[23] D. P. A. Corney, B. F. Buxton, W. B. Langdon, and D. T. Jones,
“BioRAT: extracting biological information from full-length
papers,” Bioinformatics, vol. 20, no. 17, pp. 3206–3213, 2004.

12
[24] G. Leroy, H. Chen, and J. D. Martinez, “A shallow parser based
on closed-class words to capture relations in biomedical text,”
Journal of Biomedical Informatics, vol. 36, no. 3, pp. 145–158,
2003.
[25] M. Krallinger, F. Leitner, C. Rodriguez-Penagos, and A.
Valencia, “Overview of the protein-protein interaction annotation extraction task of BioCreative II,” Genome Biology, vol.
9, no. 2, article S4, 2008.
[26] J. D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii, “Overview
of BioNLP’09 shared task on event extraction,” in Proceedings
of the Workshop Companion Volume for Shared Task (BioNLP
’09), pp. 1–9, Association for Computational Linguistics,
Boulder, Colo, USA, 2009.
[27] A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii, “Event extraction from biomedical papers using a full parser,” Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing,
pp. 408–419, 2001.
[28] J. Ding, D. Berleant, J. Xu, and A. W. Fulmer, “Extracting Biochemical Interactions from MEDLINE Using a Link Grammar
Parser,” in Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence, pp. 467–471,
November 2003.
[29] H. Cunningham, Information Extraction, Automatic, Encyclopedia of Language and Linguistics, 2nd edition, 2005.
[30] O. Etzioni, M. Cafarella, D. Downey et al., “Methods for
domain-independent information extraction from the web: an
experimental comparison,” in Proceedings of the 19th National
Conference on Artificial Intelligence (AAAI ’04), pp. 391–398,
AAAI Press, Menlo Park, Calif, USA, July 2004.
[31] M. Cafarella, D. Downey, S. Soderland, and O. Etzioni,
“KnowItNow: fast, scalable information extraction from the
web,” in Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pp. 563–570, Association for
Computational Linguistics, Morristown, NJ, USA, 2005.
[32] O. Etzioni, M. Banko, S. Soderland, and D. S. Weld, “Open
information extraction from the web,” Communications of the
ACM, vol. 51, no. 12, pp. 68–74, 2008.
[33] M. Cafarella and O. Etzioni, “A search engine for natural
language applications,” in Proceedings of the International
Conference on World Wide Web (WWW ’05), pp. 442–452,
ACM, New York, NY, USA, 2005.
[34] R. White, B. Kules, and S. Drucker, “Supporting exploratory
search, introduction, special issue, communications of the
ACM,” Communications of the ACM, vol. 49, no. 4, pp. 36–39,
2006.
[35] W. T. Fu, T. G. Kannampallil, and R. Kang, “Facilitating
exploratory search by model-based navigational cues,” in
Proceedings of the 14th ACM International Conference on
Intelligent User Interfaces (IUI ’10), pp. 199–208, ACM, New
York, NY, USA, February 2010.
[36] J. Koren, Y. Zhang, and X. Liu, “Personalized interactive
faceted search,” in Proceedings of the 17th International Conference on World Wide Web (WWW ’08), pp. 477–485, ACM,
April 2008.
[37] V. Sinha and D. R. Karger, “Magnet: supporting navigation
in semistructured data environments,” in Proceedings of the
ACM SIGMOD International Conference on Management of
Data (SIGMOD ’05), pp. 97–106, ACM, June 2005.
[38] M. Hearst, “Design recommendations for hierarchical faceted
search interfaces,” in Proceedings of the ACM Workshop on
Faceted Search (SIGIR ’06), 2006.
[39] S. Stamou and L. Kozanidis, “Towards faceted search for
named entity queries,” Advances in Web and Network Technologies, and Information Management, vol. 5731, pp. 100–112,
2009.

Advances in Bioinformatics
[40] D. Tunkelang, Faceted Search, Morgan & Claypool, 2009.
[41] B. Settles, “ABNER: an open source tool for automatically
tagging genes, proteins and other entity names in text,”
Bioinformatics, vol. 21, no. 14, pp. 3191–3192, 2005.
[42] J. Laﬀerty and F. Pereira, “Conditional random fields: probabilistic models for segmenting and labeling sequence data,”
in Proceedings of the 18th International Conference on Machine
Learning (ICML ’01), 2001.
[43] A. Doms and M. Schroeder, “GoPubMed: exploring PubMed
with the gene ontology,” Nucleic Acids Research, vol. 33, no. 2,
pp. W783–W786, 2005.

International Journal of

Peptides

BioMed
Research International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Advances in

Stem Cells
International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Virolog y
Hindawi Publishing Corporation
http://www.hindawi.com

International Journal of

Genomics

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Journal of

Nucleic Acids

Zoology

 International Journal of

Hindawi Publishing Corporation
http://www.hindawi.com

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Volume 2014

Submit your manuscripts at
http://www.hindawi.com
The Scientific
World Journal

Journal of

Signal Transduction
Hindawi Publishing Corporation
http://www.hindawi.com

Genetics
Research International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Anatomy
Research International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Enzyme
Research

Archaea
Hindawi Publishing Corporation
http://www.hindawi.com

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Biochemistry
Research International

International Journal of

Microbiology
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

International Journal of

Evolutionary Biology
Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Molecular Biology
International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Advances in

Bioinformatics
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Journal of

Marine Biology
Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Story Forms Detection in Text through Concept-based Co-clustering
Sultan Alzahrani∗ , Betul Ceran∗ , Saud Alashri∗ , Scott W. Ruston† , Steven R. Corman† and Hasan Davulcu∗
of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809
Email: {ssalzahr, betul, salashri, hdavulcu}@asu.edu
† Hugh Downs School of Human Communication, Arizona State University, Tempe, AZ 85287-1205
Email: {scott.ruston, steve.corman}@asu.edu

∗ School

Abstract—A story is defined as actors taking actions that
culminate in resolutions. In this paper, we extract subject - verb
- object relationships from paragraphs and generalize them into
semantic conceptual representations. Overlapping generalized
concepts and relationships correspond to archetypes/targets
and actions that characterize story forms. We present an
analytic framework which implements co-clustering based on
generalized conceptual relationships to automatically detect
such story forms. Co-clustering can help in identifying similarities that exist in low-dimensional sub-spaces of sparse data
such as textual paragraphs. Through co-clustering, we detect
not only the clusters themselves but also their characteristic
features which can be useful in describing and summarizing
their contents. We perform co-clustering of stories using two
different types of features: standard unigrams/bigrams and
generalized concepts. We show that the residual error of factorization with concept-based features is significantly lower than
the error with standard keyword-based features. Qualitative
evaluations also suggest that concept-based features yield more
coherent, distinctive and interesting story forms compared to
those produced by using standard keyword-based features.
Index Terms—Story forms, Narrative analysis, Non-negative
matrix factorization, Co-clustering.

1. Introduction
A key component of spreading an ideology is the utilization of cultural narratives tailored to specific target audiences. For example, extremists are known to adopt historically deeply rooted narratives from the cultural heritage
of their target audience in order to gain their attention.
Narratives are systems of stories that are linked by common
archetypes, forms and themes. A story is defined as an
actor(s) taking an action(s) that culminates in a resolution(s).
The actors, actions and resolutions in these stories form
the template of a strategic message regarding the current
events which is used by extremists to justify their actions and
policies, persuade their target audience and gain followers.
Hence identifying and countering the story forms found
in these messages is an essential part of counter violent
extremism efforts. This paper presents a framework to help
subject matter experts rapidly analyze large collections of

extremist narratives and discover their underlying story
forms. The framework streamlines the narrative analysis by
providing bi-clusters of stories and their underlying characteristic features. Another contribution of this framework is
the improved clustering performance obtained through the
utilization of concept-based features compared to widelyused standard keyword-based features.
The recurring themes in extremist narratives can be
categorized into general story forms. These story forms are
characterized by archetypes and their actions. We aim to
reveal information about the story forms in our dataset via
clustering analysis. We observe that generalized concepts
derived from extracted overlapping subject - verb - object relationships are better suited to be used as features
in clustering since they provide information regarding the
underlying semantic structure of these story forms.
Clustering is an essential step towards the analysis of
data without prior labels or categories. Two critical aspects
of clustering are; i) semantic quality of resulting clusters
and ii) their descriptive features; i.e. clusters should be
self-descriptive in order to present a meaning to the user.
Conventional clustering methods provide ample ways to
group data. However, they do not automatically yield descriptive features for the groups without further processing
(i.e. through a classifier). Co-clustering, on the other hand,
identifies both the underlying groups, out of the data, along
with their characteristic features. It simultaneously clusters
the rows and columns of an input matrix generating a subset
of instances which exhibit high similarity across a subset of
features, called bi-clusters. Since descriptions of the clusters
are produced simultaneously with clustering, co-clustering
presents an advantage over conventional clustering methods
for our application.
The initial efforts in co-clustering text data relied on
term-document matrices and lexical features, mainly ngrams. In this paper, we perform co-clustering of stories using two different types of features: standard unigrams/bigrams and generalized concepts that rely on extracted linguistic roles. We employ the model developed in
Ceran et. al and Alashri et. al. [1], [2] to produce generalized concept-based representations of extremist stories. We
show that the residual error of factorization with conceptbased features is lower than the residual error with standard
keyword-based features. Qualitative evaluations also suggest

Figure 1. System Architecture

that concept-based features yield more coherent, distinctive
and interesting story forms compared to those produced by
utilizing standard keyword-based features.
The rest of the paper is organized as follows.
§ 2 presents related works. The system architecture is
presented in § 3. § 4 describes the extraction of lexical and
linguistic features and co-clustering algorithms. In § 5, we
present experimental evaluations and results. § 6 concludes
the paper.

2. Related Work
Various types of co-clustering algorithms based on matrix factorization, probabilistic and geometric models have
been developed in literature [3] and they have been applied
in many different domains such as text, bioinformatics, and
image analysis.
One of the pioneering works in this area is Dhillon
et. al. [4] which introduces spectral co-clustering of documents and their terms by leveraging the singular value
decomposition of the term-document matrix. Non-negative
Matrix Factorization (NMF) has also been adapted for coclustering [5]. Different versions of NMF for co-clustering
such as [6] have been developed to improve the initial
model.
Kok and Domingos [7] proposed a clustering framework
to produce generalized concepts and relations similar to the
concept-based features utilized in this paper. Their algorithm
is purely statistical and relies on second order Markov Logic.
They compared their results with other algorithms on a goldstandard clustering scheme created by a human. Kang et
al. [8] used tensor decomposition in order to obtain contextual synonyms, and generalized concepts/relationships;
however, they do not present any formal evaluations of their
results. We generate the concept-based features using the
method proposed in Ceran et. al. [1] and and Alashri et. al.
[2].
Jing et.al. [9] studied co-clustering text along with
term and concept features, which were generated using
Wikipedia. They present a higher-order co-clustering framework where they improve the performance of conventional
clustering. They present an evaluation of their work on

the labeled benchmark Reuters data set. The concepts used
in [9] are named entities which are extracted using an external information source (Wikipedia), whereas the conceptbased features that we use are in the form of generalized
relational semantic triples which are produced by processing
the document set itself.

3. System Architecture
The components of our framework are presented in
Figure 1. Each component is briefly described below, and
the technical details are presented in the following sections.
•

•

•

•
•

•

The input document set consisting of stories. We
analyze the data at the paragraph level, i.e. each
document contains a single story paragraph. We
apply pre-processing in order to clean and prepare
the paragraphs for feature extraction. (Steps 1 and
2)
Three different feature sets (unigrams, bigrams and
generalized concepts/relations) are generated from
the story paragraphs. Concepts/relations are obtained
using the method proposed in [1]. Triplet generation
step has been modified according to [2]. (Steps 3, 4
and 5)
Unigrams and bigrams are ranked based on their
TF-IDF values, by initially generating Vector Space
Model (VSM) corresponding to each feature set
and then followed by constructing the DocumentFeature-Matrices (DFMs), for short, feature matrices
(Steps 6 and 7).
A binary feature matrix is also created with concept/relational features. (Step 8)
Co-clustering algorithm is run on unigram, bigram
and concept-based feature matrices to produce biclusters of story forms and their associated features.
(Steps 9, 10 and 11)
Quantitative and qualitative evaluations are performed on the resulting bi-clusters, § 5.

Figure 2. Generation of concepts/relations form hsubjects, verbs, objectsi triplets

4. Methodology
4.1. Problem Definition
For a given a set of documents comprising stories
{D1 , . . . , DN } where N denotes the number of documents,
we generate two sets of features: n-grams features and
the generalized concepts. Our main objective is to identify
which type of features yield better bi-clustering of stories
into story-forms. We evaluate the quality, initially quantitatively by employing NMF residual error measure presented
in § 5.3.2 and qualitatively in collaboration with a subject
matter expert in § 5.4

4.2. N-gram Features
We extracted highest ranked unigrams and bigrams by
utilizing term frequency - inverse document frequency (TFIDF), a simple form of cross entropy and a popular technique used in informational retrieval tasks.

4.3. Generalized Concept-based Features
Subject - verb - object triplet extraction is the basic
building block towards generalized concepts. We first process the story corpus to resolve its co-references using stateof-the-art coreference resolvers [10], [11], [12], [13]. Previously, Ceran et. al. [14] utilized ClearNLP [15] to extract
triplets. However, using this triplet extractor alone resulted
in poor recall. Alashri et al [2] proposed an enhanced
approach that utilized additional triplet extractors: AlchemyAPI [16], Everest [17], Reverb [18] and implemented a
Cartesian product of the atomized phrases in all argument
positions to double the production of extracted triplets.
Ceran et. al [1] utilized generalized triplets and compared
their performance with keywords in a classification model
to show that the triplets yield a significant 36% boost in
performance for the story detection task. However, although
triplets as features carried more semantic information, they
were showing high sparsity during matching across the
document corpus. Hence, we moved to a generalized triplet
representation by suitably “merging triplets” into generalized concepts without a drift.

In [1], we utilize both syntactic and semantic corpusbased merging criteria to generalize triplets into concepts. A
pair of hsubjecti-hverbsi-hobjectsi triplets is merged further
only if (i) they share a common context among their corresponding terms (i.e. syntactic criteria) and (ii) they satisfy
corpus-based support and similarity measure of “contextual synonymy” (i.e. semantic criteria) between their newly
added terms and existing terms. Next, a hierarchical bottomup merging algorithm allows information to propagate between clusters of related subjects, verbs and objects leading
to a set of generalized concepts.
Figure 2 illustrates an instance of how syntactic and
semantic criteria are applied on a sample set of triplets
extracted from our story corpus. Initially, syntactic criterion is satisfied between the pair of triplets: hBoko Haram,
fight, Christiansi and hBoko Haram, fight, infidelsi since
they share a common (subject, verb) context (Boko Haram,
fight). Hence, this pair becomes a candidate for merging if
a “contextual synonymy” relationship exists between their
newly added and existing terms (i.e. Christians and infidels).
Contextual synonyms are not synonyms in the traditional
dictionary sense, but they are phrases that may occur in
similar semantic roles and associated with similar contexts.
In the next step the resulting generalized concept hBoko
Haram, fight, {Christians, infidels}i can be merged with
hJama’atu Ahlis Sunna, fight, infidelsi due to their shared
(verb, object) context: (fight, infidels) meeting the syntactic
criteria, and due to the existence of contextual synonymy relationship between Boko Haram and Jama’atu Ahlis Sunna.
Syntactic criterion is applied iteratively to identify candidate
concepts for merging in combination with the application
of semantic criterion to screen for the introduction of new
topics that could cause a generalized concept to drift from
it original meaning. Let us consider an additional pair of
candidates for merging based on syntactic criteria: h{Boko
Haram, Jama’atu Ahlis Sunna} fight, {Christians, infidels}i
and hJama’atu Ahlis Sunna, {fight, wage war}, {infidels,
democracy}i. First, a core component is created using only
the intersections of the subject, verb, object sets of these
two concepts: hJama’atu Ahlis Sunna, fight, infidelsi. The
remaining words from the two candidates can be added
to the core concept only if they are among the closest

contextual synonyms of at least one of the already existing
members in the core item. For example, the algorithm
would permit the addition of Boko Haram, wage war and
Christians to the resulting set since the newly added terms
are among the closest contextual synonyms of Jama’atu
Ahlis Sunna, fight and infidels in their respective argument
positions. However, democracy would be left out of the
object argument position in the resulting generalized concept
since it is not among the contextual synonyms of neither
infidels nor the Christians according to the corpus based
definition of “contextual synonymy” (i.e. semantic criteria).

4.4. Co-Clustering
Clustering is an unsupervised learning technique that
tries to draw inferences from given data where data labels
(i.e. classes) are concealed or unknown, as in our case). This
approach is adopted to assist in benchmarking unigrams,
bigrams and the generalized concepts as features when used
for story forms detection in a story corpus. We aim to
investigate which feature set will provide us with the highest
quality bi-clustering results. Comparing different feature sets
while applying co-clustering will not only allow us to determine which feature set can quantitatively perform better but
also, it can prompt us about which feature set could provide
more coherent, distinctive and interesting story forms as
clusters.
To formalize the co-clustering problem, let’s assume
our story corpus contains M documents and N features
provided as the matrix A = (aij )M ×N such that aij
represents the entry value of i-th story document and j th feature. The A feature-term-matrix can be also written as
A = (R, C) ∈ <M ×N where R = {1, 2, . . . , M } denotes
row indices, and C = {1, 2, . . . , N } denotes column indices.
Here, our objective is to find set of sub-matrices or biclusters, say Bk (Xk , Yk ), such that X = M1 , . . . , Ma ⊆ R
and Y = N1 , . . . , Nb ⊆ C as separate subsets of R and
C . This task is an NP-hard problem [19], but an optimization approach with a greedy iterative search utilizing Nonnegative Matrix Factorization (NMF) has been shown to
produce effective results.
4.4.1. Non-negative Matrix Factorization (NMF). Coclustering is an NP-hard problem, yet many different optimization based approximation algorithms have been developed in the literature. One of those is the non-negative
matrix factorization or decomposition based method which
factorizes a given matrix into multiple matrices revealing
substructure patterns within the matrix. This method has
been widely used in many applications such as bioinformatics, image processing, and text mining. NMF can be
used to factorize our A ∈ <M ×N feature-term-matrix into
a pair matrices U ∈ <M ×K and V ∈ <K×N having nonnegative elements, such that A ' U V constructing an approximation, as shown in Equation 1. U represents the basis
vectors (or factors), and V represents the coefficients on the
linear combination of the factors that allows construction
of the original A feature-document-matrix. K variable can

be used as the number of clusters and it has to satisfy
K < min{M, N }. The mathematical formulation of the
optimization problem is written as follows:
n

minimize
U,V

1
λX
1
2
2
kX − U V kF + kU kF +
kvi k1
2
2
2 i=1

subject to U, V ≥ 0
The above optimization problem is a modified version
of NMF proposed by [6] since the standard form of NMF
has shortcomings of non-unique and scale-variance outputs.
Kim et al. [6] enhanced sparseness degree of basis vectors
by introducing regularizations as well as alternating negative
constraints update technique based on the multiplicative
update. The multiplicative update, in the standard NMF,
does not necessarily yield sparse basis vectors. The sparse
optimization problem can be solved using non-negative
quadratic programming (NNQP). The modified NMF compares different feature sets by looking into the residual error
E after factorization, where E , shown in Equation 1, is the
error term after decomposing A matrix into U and V . Lower
E values indicate better underlying structure detection in A.
We used the software package in [20] for the implementation
of the Sparse NMF.
A = UV + E

(1)

5. Experimental Results
5.1. Corpus
Our story corpus consists of 6, 856 paragraphs which are
pulled from a database of Islamist extremist texts. Texts are
collected from online sources websites, blogs and other news
sources that are known to be outlets of extremist groups such
as Al-Qaeda, ISIS or their followers who sympathize with
their cause and methods. Extremists’ texts are not entirely
composed of stories. After the crawling process, subject
matter experts annotated the paragraphs based on a coding
system, consisting of eight mutually-exclusive categories:
story, exposition, imperative, question, supplication, verse,
annotation, and other. A paragraph is labeled as a story if
it tells a sequence of related events, leading to a resolution
or projected resolution. In our experiments, we work on the
paragraphs which are coded as stories.

5.2. Number of Clusters
There is no ground truth of story forms available for
our story corpus therefore, we resort to additional analysis
to determine the number of clusters before we present the
results to subject matter experts for qualitative evaluation.
Determining the right number of clusters has been a challenging problem in clustering and various techniques have
been suggested in the literature to solve this problem. First,
we obtain an embedding of stories × concepts feature

matrix into 2-D. We use t-Distributed Stochastic Neighbor Embedding (t-SNE) [21] technique to reduce the data
dimension and visualize the block diagonal sub-structures.
Next, we use an external measure from literature, CalinskiHarabasz index [22], to measure the quality of a clustering
across different numbers of clusters. Calinski-Harabasz index or variance ratio criterion (VRC) is proportional to the
ratio of the overall between-cluster variance and the overall
within-cluster variance. In this scheme, the higher corresponding VRC value, the better the clustering performance.
Figure 3 (a) shows a plot of VRC values across a number of
clusters ranging from 2 to 14. The rule of thumb suggested
in the literature is to discern the values which cause a sharp
spike in the VRC plots. In Figure 3 (b), we can see that there
is a sharp spike at 6 clusters. This indicates that setting the
number of clusters to 6 is a plausible choice in order to
obtain a good clustering scheme. Figure 3 (b) shows the
scatter plot of 6 clusters obtained using K-means after 2D
embedding. The cluster centroids are also marked alongside
their error ellipses representing their covariance matrices.

Figure 3. Analysis to determine number of clusters

5.3. Quantitative Evaluation
5.3.1. Block diagonal sub-structure. Figure 4 shows the
block diagonal sparsity structures of the feature matrices
after clustering, in which, the bi-clusters (unigrams vs.
stories bi-clusters in (a), bigrams vs. stories in (b), concepts
vs. stories in (c)) are represented along the diagonal blocks.
Block diagonal plots are obtained by reordering the indices
of stories and their characteristic features in each row and
column for each cluster to show their groupings.
5.3.2. NMF residual error. Figure 4 (d) show the residual
error of non-negative matrix factorization of the three different feature matrices across different numbers of clusters.

Residual error is computed by using the formula shown in
Equation 1. The error decreases as the number of clusters
increases since the number of clusters also represents the
dimensionality of the resulting approximation matrices. In
residual error plots, it can be observed that the concept-based
features consistently yield lower residual errors compared to
both the unigram and bigram based features.

5.4. Qualitative Evaluation
To determine if the clusters generated by the conceptrelations technique yielded a valuable analytic tool and an
improvement over other clustering methods for the anticipated use case (rapidly analyzing large amounts of story text
to determine themes and overarching narratives to benefit
strategic communication and counter-messaging activities),
a subject matter expert (SME) conducted a qualitative evaluation.
Six clusters were generated using the concept-relations
technique discussed here and six other clusters were created
using bi-gram co-clustering techniques- Tables 1 and 2
provide a summary for each clustering scheme based on
two feature sets. The SME read the stories drawn from
each of the twelve clusters (i.e. 6 for each) without cluster
identification noting narratively significant features such as
the protagonists and antagonists, types of actions taken,
and evident and implied resolutions. Subsequently, the SME
also conducted an evaluation of the feature sets of each
clustering method, looking for patterns and indicators of
meaning useful to a communication analyst. These efforts
were synthesized to draw conclusions about the clusters.
5.4.1. Concept Clusters vs. Bigram Clusters. In general,
both clustering methods produced some distinct clusters
that make sense under qualitative evaluation. Notably, the
dataset is dominated by stories with an overall structure
described by previous analysis as a “victorious battle story”.
In this story form, a protagonist (member of some extremist
group) takes some form of military action killing or injuring
antagonists (US forces or police) [23]. The prevalence of this
basic story form within the dataset complicates identifying
robustly distinct clusters in terms of narrative significance.
This is because the characteristics that distinguish groups
of stories tend to be the terms used for protagonists and
antagonists and the settings, whereas the general meaning
(successful attack by insurgent forces) remains relatively
constant.
However, despite that limitation, the concept clustering method produced meaningful clusters with notable distinctions and with useful implications for communication
analysis. The bigram cluster method produced clusters with
less distinctiveness and significance in terms of overall
meaning. For example, bigram clusters 2 and 3 are nearly
impossible to distinguish, involving similar stories, nearly
identical actions, and having a wide range of protagonists
and antagonists. In the set of concept clusters, clusters 1,
2, 5 and 6 were the most distinctive clusters, especially 5
and 6. The stories in cluster 5 are very similar: mujahidin in

Figure 4. Feature matrices after co-clustering for all feature sets - block diagonal substructure is more coherent for the concept stories biclusters as in (c)

Afghanistan attack US and Afghan forces with improvised
explosives. The stories consistently refer to the Afghan
forces as “puppets”.
5.4.2. Notable clusters. While the variations across the
dataset are subtle (as noted above), the concept clustering
method did usefully identify some meaningful clusters. Concept Cluster 2, for example, contains stories with a particular
subject-verb construction as this example illustrates: “Another martyr operation was carried out by Mujahid Abdul
Wali, who carried out the attacks on military bases of puppet
Afghan soldiers that were still in the same district with
the first martyr operations.” This construction contributes to
a semi-objective news-narrative, belying the propagandistic
content. This format contributes to the positioning of the
mujahidin as champions of Islam defending the Ummah- the
Islamic Community, and also conveys that they are winning
the war [24].
Concept Cluster 1 exhibits another very consistent formulation that contributes to strategic communication goals.
Stories in this cluster are dominated by the verb construction
of attack-result in which the stories describe an attack and
include the results such as the example: “At 11:45 [ 08:45
GMT] on 5 August, one of our combat groups detonated
a guided explosive device against a patrol belonging to
the Crusader occupation forces on Kirkuk-Al-Riyad Road

in western Kirkuk. The explosion resulted in destroying a
specialized vehicle and killing or wounding all those on
board. Praise be to God, the Lord of all creation.” Like
Concept Cluster 2’s dispassionate, news-style reporting of
operations, this cluster’s emphasis on the successful results
of the attacks convey the meaning that the insurgents are a
strong force, a strong champion and are winning the conflict. Importantly, this meaning is contained in the semantic
combination of attack and result, but these words are often
separated by dependent clauses or in completely separate
sentences. This association of attack and result would not
be detected and clustered by bigram clustering techniques,
giving concept-based features an edge as it is capable of
extending the features into a higher semantic level.
Concept Cluster 1 has an additional significant feature: the antagonists are almost exclusively referred to by
derogatory epithets (“apostates”, “pagan army”, “safavids”,
“Crusaders”). This rhetorical technique dehumanizes these
groups and assigns unsavory and immoral characteristics
to them, thereby emphasizing the threat to the Ummah by
their very existent. Violence by the community’s champion
is therefore justified against these groups that threaten the
community. Identifying the rhetorical techniques is the first
step to defusing their inflammatory and radicalizing power,
and thus a technique that can distill these constructions from
a body of text data is valuable.

TABLE 1. B IGRAMS - BASED C LUSTERS
Cluster
1

2

3
4
5
6

Key narrative features
Protagonists: often unspecified, mujahidin; Actions: attacking
with emphasis on bombs and vehicles, Note: name of Taliban
spokesman frequent; emphasis is Afghanistan;
Protagonists: lions of ansar; Antagonists: inconsistent names, locations action: wide range, with most frequent being detonation
destroying vehicles, with praise and gratitude to God. Emphasis
on date
Actors: security detachment (presumed subject/attacker) Action:
emphasis on attack/result, detonation, and killing result; strong
emphasis on date
Action: attacks in Afghanistan; against Actors: US, NATO, invaders and puppets, vehicle/military base
Actors: Shield of,Islam Brigades, mujahidin, AQIM; Iraqi National
Guard, Mahdi army, and police Action: detonate explosive
Actors: US, ISIS, God, Bin Laden, Shabaab, mujahidin, messenger; Frequent invocations of god, god’s grace and praise; action:
frequent construction ‘carried out’ operation

Notes and significance
Protagonists and antagonists are not consistent; stories contain
repeated phrases; mention of spokesman name a distinguishing
feature
No significant difference in action between cluster 1, 2 and 3
Only difference between cluster 2 and cluster 3 is frequent actor
“security detachment”
Wide variety of actions within general category of ‘attack’; clear
focus on Afghanistan;
Similar to clusters 1-3, but with greater emphasis on claims of
attacks, potentially indicating purpose/intent of story
No consistency to the locations or actors in this cluster; variety
points of view (POV)

TABLE 2. C ONCEPT- BASED C LUSTERS
Cluster
1
2

3

4
5
6

Key narrative features
Protagonists either unspecified or “Lions of Islam”; frequent construction of attack-result; antagonists always labeled with epithets
(apostates, pagans, safavids, crusaders)
Protagonists: Lions or mujahidin; actions: attacks carried out; news
format
Protagonists: mujahidin, Shabaab, Lions; Antagonists: US forces,
apostates, Federal Police; Action: detonation of IEDs, car bombs,
landmines and other explosives; settings: Afghanistan, Iraq, Somalia
Significant variation in protagonists, antagonists, settings and actions; Minor emphasis on the killing of women and children (by
US/allies)
Protagonist: mujahidin; antagonists: US and puppets; action: bomb
blast, detonation
Protagonists: Lions of Ansar Islam; actions: plant or detonate
bombs; antagonists: US, apostates, crusaders

6. Conclusion
Narratives are systems of stories that construct meaning [25]. That meaning is constructed in part by the patterns
of relationships created by the actors and actions that make
up the constituent stories [26]. In order to analyze the
narratives circulating within a discursive environment, the
ability to distill stories from a larger corpus of information, and then cluster those stories into meaningful groups
of story forms is necessary. The clustering method must
account for patterns of relationships of actors and actions.
In this paper, we show that the concept-based co-clustering
method described here, with its attention to subjects and
objects (actors) and verbs (actions) makes a step towards
a robust method that meets this strategic communication
analysis goal. Concept-based features yield a better clustering scheme compared to bigram features both quantitatively
and qualitatively. The lower residual error in NMF achieved
by concept-based features shows that concept-based features
can capture more information than high level noisy outcomes compared to the other feature sets. Additionally, the
content of the clusters produced by concept-based features

Notes and significance
Function: justify the threat to Muslims by ‘others’ who are not
to be respected
Function: legitimize the insurgent/extremist actions by formatting in a news report format; convey the extremists are winning
the war and are champions of Islam
Similar to cluster 5, but with much more variation; Highlights
the vulnerability of adversary forces and highlights effectiveness
across Muslim regions
Very loose cluster with no discernable patterns
Very tight cluster of stories of IED attacks against US and
Afghan forces (puppets) set primarily in Afghanistan
Another very tight cluster, analogous to Cluster 5 but set in Iraq;
illustrates geographically specific epithet

presents a better semantic pattern in terms of strategic
communication as opposed to the repetitive and scattered
content of the bigram clusters as outlined by the SME.

Acknowledgment
This research was partially supported by DoD Minerva
Research Initiative Grant N00014-15-1-2821.

References
[1]

B. Ceran, N. Kedia, S. R. Corman, and H. Davulcu, “Story detection
using generalized concepts and relations,” in Proceedings of the 2015
IEEE/ACM International Conference on Advances in Social Networks
Analysis and Mining 2015. ACM, 2015, pp. 942–949.

[2]

S. Alashri, S. Alzahrani, J.-Y. Tsai, S. R. Corman, and H. Davulcu,
“Climate change frames detection and categorization based on generalized concepts,” International Journal of Semantic Computing,
vol. 10, no. 02, pp. 1–20, 2016.

[3]

H. Zhao, A. Wee-Chung Liew, D. Z Wang, and H. Yan, “Biclustering
analysis for pattern discovery: current techniques, comparative studies
and applications,” Current Bioinformatics, vol. 7, no. 1, pp. 43–55,
2012.

[4]

I. S. Dhillon, “Co-clustering documents and words using bipartite
spectral graph partitioning,” in Proceedings of the Seventh ACM
SIGKDD International Conference on Knowledge Discovery and
Data Mining. ACM, 2001, pp. 269–274.

[15] J. D. Choi, “Optimization of natural language processing components
for robustness and scalability,” Ph.D. dissertation, University of Colorado Boulder, 2014.

[5]

C. Ding, T. Li, W. Peng, and H. Park, “Orthogonal nonnegative
matrix t-factorizations for clustering,” in Proceedings of the 12th
ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining. New York, NY, USA: ACM, 2006, pp. 126–135.

[16] (2015) Alchemy api language features. AlchemyAPI, Inc. [Online].
Available: http://www.alchemyapi.com/products/alchemylanguage

[6]

H. Kim and H. Park, “Sparse non-negative matrix factorizations via
alternating non-negativity-constrained least squares for microarray
data analysis,” Bioinformatics, vol. 23, no. 12, pp. 1495–1502, 2007.

[7]

S. Kok and P. Domingos, “Extracting semantic networks from text via
relational clustering,” in Proceedings of the 2008 European Conference on Machine Learning and Knowledge Discovery in Databases
- Part I, 2008, pp. 624–639.

[8]

U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos, “Gigatensor:
Scaling tensor analysis up by 100 times - algorithms and discoveries,”
in Proceedings of the 18th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, 2012, pp. 316–
324.

[9]

L. Jing, J. Yun, J. Yu, and J. Huang, “High-order co-clustering
text data on semantics-based representation model,” in Advances in
Knowledge Discovery and Data Mining. Springer, 2011, pp. 171–
182.

[10] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu,
D. Jurafsky, and C. Manning, “A multi-pass sieve for coreference
resolution,” in Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguistics, 2010, pp. 492–501.
[11] H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task,” in Proceedings of the Fifteenth
Conference on Computational Natural Language Learning: Shared
Task. Association for Computational Linguistics, 2011, pp. 28–34.
[12] H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Deterministic coreference resolution based on entitycentric, precision-ranked rules,” Computational Linguistics, vol. 39,
no. 4, pp. 885–916, 2013.
[13] M. Recasens, M.-C. de Marneffe, and C. Potts, “The life and death
of discourse entities: Identifying singleton mentions.” in HLT-NAACL,
2013, pp. 627–633.
[14] B. Ceran, R. Karad, A. Mandvekar, S. R. Corman, and H. Davulcu,
“A semantic triplet based story classifier,” in Proceedings of the 2012
IEEE/ACM International Conference on Advances in Social Networks
Analysis and Mining (ASONAM). IEEE, 2012, pp. 573–580.

[17] (2013) Everest triplet extraction. Next Century Corporation. [Online].
Available: https://github.com/NextCenturyCorporation/ EVERESTTripletExtraction
[18] A. Fader, S. Soderland, and O. Etzioni, “Identifying relations for
open information extraction,” in Proceedings of the Conference on
Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 2011, pp. 1535–1545.
[19] S. Busygin, O. Prokopyev, and P. M. Pardalos, “Biclustering in data
mining,” Computers & Operations Research, vol. 35, no. 9, pp. 2964–
2987, 2008.
[20] Y. Li and A. Ngom, “The non-negative matrix factorization toolbox
for biological data mining,” Source code for biology and medicine,
vol. 8, no. 1, p. 1, 2013.
[21] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne,”
Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85,
2008.
[22] T. Caliński and J. Harabasz, “A dendrite method for cluster analysis,”
Communications in Statistics-theory and Methods, vol. 3, no. 1, pp.
1–27, 1974.
[23] C. Lundry, S. R. Corman, R. B. Furlow, and K. W. Errickson, “Cooking the books: Strategic inflation of casualty reports by extremists in
the afghanistan conflict,” Studies in Conflict & Terrorism, vol. 35,
no. 5, pp. 369–381, 2012.
[24] S. Corman, S. Ruston, and M. Fisk, “A pragmatic framework for
studying extremists’ use of cultural narrative,” in 2nd International
Conference on Cross-Cultural Decision Making: Focus 2012, 2012,
pp. 21–25.
[25] J. R. Halverson, S. R. Corman, and H. Goodall Jr, Master narratives
of Islamist extremism. Palgrave Macmillan, 2011.
[26] S. W. Ruston, “More than just a story: Narrative insights into comprehension, ideology and decision-making,” in Modeling sociocultural
influences on decision making: Understanding conflict, enabling stability, J. V. Cohn, S. Schatz, H. Freeman, and D. J. Y. Combs, Eds.
CRC Press.

The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

Partitioning Signed Bipartite Graphs for
Classification of Individuals and Organizations
Sujogya Banerjee, Kaushik Sarkar, Sedat Gokalp,
Arunabha Sen, and Hasan Davulcu
Arizona State University
P.O. Box 87-8809, Tempe, AZ, 85281 USA
{sujogya,kaushik.sarkar,sedat.gokalp,asen,hdavulcu}@asu.edu

Abstract. In this paper, we use signed bipartite graphs to model opinions expressed by one type of entities (e.g., individuals, organizations)
about another (e.g., political issues, religious beliefs), and based on the
strength of that opinion, partition both types of entities into two clusters. The clustering is done in such a way that support for the second
type of entity by the first within a cluster is high and across the cluster
is low. We develop an automated partitioning tool that can be used to
classify individuals and/or organizations into two disjoint groups based
on their beliefs, practices and expressed opinions.

1

Introduction

The goal of the Minerva1 project, currently underway at Arizona State University is to increase understanding of movements within Muslim communities
actively working to counter violent extremism. As a part of this study, we have
collected over 800,000 documents from web sites various organizations in Indonesia. Based on the support and opposition of certain beliefs and practices, we can
partition the set of organizations O into two groups O1 and O2 and the set of
beliefs and practices B into two groups, B1 and B2 , such that organizations in O1
support B1 and oppose B2 , while the organizations O2 support B2 and oppose
B1 . With the domain knowledge of the social scientists in our team regarding
the beliefs and practices of Indonesian community, we can then label one group
as being radical and other as counter-radical.
Although the motivation for our work was driven by Minerva, the the problem
that is being addressed in this paper is much broader in nature. In the mathematical sociology community, the problem is known as the Signed two-mode network
partitioning problem [1]. In its mathematical abstraction, the problem is specified by a bipartite graph G = (U ∪ V, E) and label function σ : E → {P, N }.
The node sets U and V may be representing the set of organizations O and the
set of beliefs B respectively. If the label of an edge from oi ∈ O to bj ∈ B is
P , it implies oi supports (or has positive opinion) about bj . If the label of an
edge is N , it implies oi opposes (or has negative opinion) about bj . The goal of
1

A project sponsored by the U.S. Department of Defense

(a)

(b)

Fig. 1: Partitioning of the node set U and V with the desired goal
the partitioning problem is to divide the node sets U and V into two subsets
(U1 , U2 ) and (V1 , V2 ) respectively, such that
1. number of P edges (positive opinion or support) between nodes within block
1 (P11 between U1 and V1 ) and block 2 (P22 between U2 and V2 ) is high,
2. number of P edges between nodes across block 1 and block 2 (edges P12
between U1 and V2 and P21 between U2 and V1 ) is low,
3. number of N edges (negative opinion or opposition) between nodes within
block 1 (N11 between U1 and V1 ) and block 2 (N22 between U2 and V2 ) is
low and
4. number N edges between nodes across block 1 and block 2 (edges N12 between U1 and V2 and N21 between U2 and V1 ) is high.
The goal of partitioning is depicted in Fig. 1, where the green edges indicate
support (i.e, P edges) and the red edges indicate opposition (i.e, N edges). We
can realize these goals by maximizing [(P 11 + P 22 + N 12 + N 21) − (P 12 + P 21 +
N 11 + N 22)].
Signed two-mode network partitioning problem can be applied in a multitude
of domains, where the node sets U and V can represent different entities. For
example, (i) U and V may represent the members of the U.S. Senate/House of
Representatives and the bills before the senate/house of representatives where
they cast their votes, either supporting or opposing the bill; (ii) U and V may
represent the political blogs/bloggers and various issues confronting the nation,
where they express their opinions either supporting or opposing issues. Clearly,
availability of an automated tool that will co-cluster the entities represented by
U and V , will be valuable to individuals and organizations that need a coarse
grain (two-modal) partitioning of the data set represented by the node set U
and V . This tool can help classify individuals or organizations as radicals vs.
counter-radicals, or liberals vs. conservatives or violent vs. non-violent, etc.
The main contribution of this effort is the development of a fast automated
tool (and associated algorithms) for co-clustering the entities represented by the
node sets U and V . We first compute an optimal solution of the partitioning
problem using an integer linear program to be used as a benchmark for our
heuristic solution. We then develop a heuristic solution and compare its performance using three real data sets. The real data sets include voting records
of the Republican and Democratic members of the 111th US Congress and the
opinions expressed in top twenty two liberal and conservative blogs. In all these

data sets our partitioning tool produces high quality solution (i.e., with low misclassification) at a low cost (in terms of computation time). To the best of our
knowledge, our Minerva research group is the first to present an efficient computational technique for partitioning of signed bipartite graph and apply it to
some real data sets.

2

Related works

As the literature on clustering, classification and partitioning is really vast, due
to page limitations, we only refer to the ones that are most relevant to this
paper [1–4, 8, 7]. The two key features of the partitioning problem addressed in
this paper are (i) the graph is bipartite and (ii) the weights on the edges are
signed (i.e., the weights are both positive and negative). Simultaneous clustering
of two sets of entities (represented by two sets of nodes in the bipartite graph)
was considered in the context of document clustering in [4, 8]. In these studies one
set of entities are the documents and the other set is terms or words. Although
these efforts study the bipartite graph partition problem, they are distinctly
different from our study in one respect. In our study, the edge weights are signed,
whereas the edges weights considered in [4, 8] are unsigned. Graph partitioning
problem with signed edge weights was studied in [2, 3]. However, these studies are
also distinctly different from our study in that, while they focus on partitioning
general (i.e., arbitrary) graphs, we focus our attention to partitioning bipartite
graphs. The study that comes closest to our study is [1, 7], where attention is
focused on partitioning of a signed bipartite graphs. However, neither [1] nor
[7] present any efficient algorithm to solve the partitioning problem in signed
bipartite graph.

3

Problem Formulation

In this section we formally define the partitioning problem.
Signed Bipartite Graph Partition Problem (SBGPP): An edge labeled weighted
bipartite graph G = (U ∪ V, E) where U = {u1 , u2 , . . . , un } represents entities
of type I and V = {v1 , v2 , . . . , vm } represents entities of type II. Each edge
(u, v) ∈ E has two functions associated with it: (i) label function σ : E → {P, N },
which indicates the type of opinion (positive or negative), and (ii) weight function
w : E → Z, which indicates the strength of that opinion. AN = [wn (u, v)] and
AP = [wp (u, v)] are the weighted adjacency matrix for edges with label N and P
respectively. If the node set U is partitioned into U1 and U2 and V is partitioned
into V1 and V2 , the strength of the positive and negative opinions of the entities
of type I regarding the entities of type II are defined as follows:
For all edges (u, v) ∈ E,
X X
X X
X X
P11 =
wp (u, v), P12 =
wp (u, v), P22 =
wp (u, v)
P21 =

u∈U1 v∈V1

u∈U1 v∈V2

X X

X X

u∈U2 v∈V1

wp (u, v), N11 =

u∈U1 v∈V1

u∈U2 v∈V2

wn (u, v), N12 =

X X

u∈U1 v∈V2

wn (u, v)

N22 =

X X

wn (u, v), N21 =

u∈U2 v∈V2

X X

wn (u, v)

u∈U2 v∈V1

Problem: Find a partition of the node set U into U1 and U2 and V into V1 and V2
such that [(P 11 + P 22 + N 12 + N 21) − (P 12 + P 21 + N 11 + N 22)] is maximized.

4

Computational Techniques

In this section we give a mathematical programming technique to find the optimal solution for the SBGPP. Since computational time for finding optimal solution for large graphs is unacceptably high, we present a heuristic in subsequent
section to solve the SBGPP.
4.1

Optimal Solution for SBGPP

The goal of the SBGPP is to partition U into two disjoint sets U1 and U2
(similarly V into V1 and V2 ). For each node in u ∈ U and each partition Ui , i =
1, 2, we use a variable bui . bui is 1 iff in u is in Ui . Similarly we define variable
pvi for all v ∈ V . We will refer B1 = U1 ∪ V1 and B2 = U2 ∪ V2 as blocks 1 and
2 respectively.
V ariables: For each node u ∈ U, v ∈ V and each partition Ui , Vi , i = 1, 2
(
(
1, if node u is in partition Ui
1, if node v is in partition Vi
bui =
pvi =
0, otherwise.
0, otherwise.
The mathematical programming formulation is given as follows:
max

L=

2 X X
X

(wp (u, v) − wn (u, v))bui pvi

i=1 u∈Ui v∈Vi

+

2
X
X X

(wn (u, v) − wp (u, v))bui pvj

i,j=1 u∈Ui v∈Vj
i6=j

s.t

bu1 + bu2 = 1,

∀u ∈ U

(1)

pu1 + pu2 = 1,

∀p ∈ V

(2)

The objective function computes the objective value given by the expression L.
We want to maximize L. It may be noted that the above quadratic objective
function can easily be changed into a linear function by simple variable transformation [6]. Constraint 1 and 2 ensures that each node in U and V belongs to
one particular block.
4.2

Move-based Heuristics

We present a move-based heuristic to find an approximate solution of SBGPP.
The move-based heuristic is a variant of well known FM algorithm [5] for partitioning graphs. The algorithm starts with a random initial partition and iteratively moves nodes from one block to another such that the value of the objective

function is improved. The “gain” of a node is defined as the value by which the
objective function increases if the node is moved from one block to the other.
In each iteration the node with the highest gain is moved from one block to
the other. In case of a tie a node is chosen arbitrarily. After a node is moved,
it is locked and is not moved until the next pass. The heuristic is presented in
Algorithm 1. It should be noted that original FM algorithm will not work for our
problem as SBGPP relates to signed bipartite graphs with a completely different
objective function and doesn’t have any size constraints. As a result the node
gain computation routine Algorithm 2 is considerably different from the original
FM algorithm. Algorithm 1 runs for r different initial random partition of the
nodes to avoid the possibility of being stuck at a local maxima. In practice the
heuristic converges very fast, mostly in 2 to 3 passes.
Algorithm 1: Move-based Heuristic (MBH)

13

Input : A weighted signed bipartite graph H = (U ∪ V, E)
Output: A partition of the nodes U1 , U2 and V1 , V2 such that objective value L
is maximum
L ←− 0;
for i ←− 1 to r do
Generate a random partitioning of the nodes in U into U1 and U2 and nodes
in V into V1 and V2 ;
repeat
Compute gains of all nodes using Algorithm 2 ;
repeat
Among all the unlocked nodes select the node of highest gain. Move
the node to the other block and call it base node. Lock the base
node;
Update the node gains of all the free neighbors of the base node;
until Until all the nodes are locked ;
Change the current partition into a new partition that has the largest
value of the objective function in this pass ;
Unlock all the nodes;
until If the objective value L′ improves during the last pass;
if L′ ≥ L then L′ ← L and save the current partition

14

return L and the final partition of nodes

1
2
3
4
5
6
7

8
9
10
11
12

5

Experimental Results and Discussions

To validate the effectiveness of our heuristic and benchmark its performance we
tested the heuristic both on synthetic and real world data. The real world data
consists of US Congress (SENATE, REP) and political blogosphere (BLOG) data
sets.
5.1

US Congress Data [SENATE, REP]

The US Congress has been collecting data since the very first congress of the US
history. This data has been encoded as XML files and publicly shared through

Algorithm 2: Node Gain Computation
Input : A weighted signed bipartite graph G = (U ∪ V, E)
Output: Gains of all nodes
foreach node u ∈ U ∪ V do
gain(u) ←− 0;
// FBlock = "from block" of node u, ToBlock = "to block" of node
u, w(e) = weight of edge e and # = number
foreach edge e ∈ E with l(e) = N of node u do
if # nodes of e in ToBlock is 0 then gain(u) ← gain(u) + 2 ∗ w(e);
if # nodes of e in FBlock is 1 then gain(u) ← gain(u) − 2 ∗ w(e);

1
2

3
4
5

foreach edge e ∈ E with l(e) = P of node u do
if # nodes of e in ToBlock is 0 then gain(u) ← gain(u) − 2 ∗ w(e);
if # nodes of e in FBlock is 1 then gain(u) ← gain(u) + 2 ∗ w(e);

6
7
8

the govtrack.us project2 . From various types of data available at the project
site, we collected the roll call votes for the 111th US Congress which includes
The Senate and The House of Representatives and covers the years 2009-2010.
The 111th Senate data contains information about 108 senators and their votes
on 696 bills3 . The 111th Congress has 451 representatives and the data contains
their vote on 1655 bills.
We extracted the SENATE and REP data in adjacency matrices A|U|×|V | ,
with U vertices representing the congressmen, and the V vertices representing
the bills. The edge (ui , vj ), ui ∈ U, vj ∈ V has weight 1 if the congressman
ui votes ‘Yea’ for the bill vj , −1 if the congressman votes ‘Nay’, and 0 if he
did not attend the session. We have the original classification vector for both
the congressmen and the bills in terms of which party they represent (or which
party sponsored the bill). The first two columns of Table 1 provide information
about this data as well as the partitioning accuracies of the algorithms. Figure
2 depicts the partitioned vote matrices of the 111th US Congress data, where
rows representing the congressmen and the columns representing the bills. Also,
the light green color represents ‘Yea’ votes, and dark red represents ‘Nay’ votes.
5.2

Blog Data [BLOG]

As Web 2.0 platforms gained popularity, it became easy for web users to be a
part of the web and express their opinions, mostly through blogs. Most blogs
are maintained by individuals, whereas there are also professional blogs with a
group of authors. In this study, we focus on a set of popular political liberal or
conservative blogs that have a clearly declared positions. These blogs contain
discussions about social, political, economic issues and related key individuals.
2
3

http://www.govtrack.us/data
Normally, each congress has 100 senators (2 from each state), however in many of
the congresses, there are unexpected changes on the seats caused by displacements
or deaths.

(a) 111th US House

(b) 111th US Senate

Fig. 2: Vote matrix of US Congress after partitioning

Table 1: Descriptive summaries of the graphs for each dataset with the Heuristic
accuracy

Vertices in V

111th US Senate
64 Democrat
42 Republican
Senator
696 Bills

111th US House
268 Democrat
183 Republican
Representatives
1655 Bills

Graph Density
Heuristic accuracy

88.36 %
100.00%

91.23 %
99.56%

Vertices in U

Political Blogosphere
13 Liberal
9 Conservative
Blogs
20 Liberal
14 Conservative People
39.04 %
98.21%

They express positive sentiment towards individuals whom they share ideologies
with, and negative sentiment towards the others. In these blogs, it is also common
to see criticism of people within the same camp, and also support for people from
the other camp.
In this experiment, we collected a list of 22 most popular liberal and conservative blogs from the Technorati4 rankings. For each blog, we fetched the posts
for the period of 6 months before the 2008 US presidential elections (May - October, 2008). We expected to have high intensity of the debates and discussions
and resulting in a bipolar clustering in the data. Table 2 shows the partial list
of blogs with their URLs, political camps and the number of posts for the given
period.
We use AlchemyAPI5 to run a named entity tagger to extract the people
names mentioned in the posts, and an entity-level sentiment analysis which provided us with weighted and signed sentiment (positive values indicating support,
and negative indicating opposition) for each person. This information was used
to synthesize a signed bipartite graph (the BLOG data), where the blogs and
people correspond to the two sets of vertices U and V . The aij values of the adjacency matrix A are the cumulative sum of sentiment values for each mention
of the person vj by the blog ui .
4
5

http://technorati.com
http://www.alchemyapi.com

To get a gold standard list of the most influential liberal and conservative
people, we used The Telegraph List6 for 2007. The third column of Table 1
provides information about this data as well as the partitioning accuracies of
the algorithm.
Table 2: Political Blogs
Blog name
URL
Huffington Post
http://www.huffingtonpost.com/
Daily Kos
http://www.dailykos.com/
Boing Boing
http://www.boingboing.net/
Crooks and Liars
http://www.crooksandliars.com/
Firedoglake
http://www.firedoglake.com/
Hot Air
http://hotair.com/
Reason - Hit and Run http://reason.com/blog
Little green footballs http://littlegreenfootballs.com/
Atlas shrugs
http://atlasshrugs2000.typepad.com/
Stop the ACLU
http://www.stoptheaclu.com/
Wizbangblog
http://wizbangblog.com/

6

Political view
Liberal
Liberal
Liberal
Liberal
Liberal
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative

Size
3959
1957
1576
1497
1354
1579
1563
787
773
741
621

Conclusion

In this paper we study the problem of partitioning signed bipartite graph with
relevant application in political, religious and social domains. We provided a fast
heuristic to find the solution for this problem. We tested the high accuracy of
our heuristic on three sets of real data collected from political domain.

References
1. Andrej, M., Doreian, P.: Partitioning signed two-mode networks. Journal of Mathematical Sociology 33, 196–221 (2009)
2. Bansal, N., Blum, A., Chawla, S.: Correlation clustering. In: MACHINE LEARNING. pp. 238–247 (2002)
3. Charikar, M., Guruswami, V., Wirth, A.: Clustering with qualitative information.
In: Proceedings of the 44th Annual IEEE FOCS (2003)
4. Dhillon, I.S.: Co-clustering documents and word using bipartite spectral graph partitioning. In: Proceedings of the KDD. IEEE (2001)
5. Fiduccia, C., Mattheyses, R.: A linear-time heuristic for improving network partitions. In: Papers on Twenty-five years of electronic design automation. pp. 241–247.
ACM (1988)
6. Sen, A., Deng, H., Guha, S.: On a graph partition problem with application to vlsi
layout. Inf. Process. Lett. 43(2), 87–94 (1992)
7. Zaslavsky, T.: Frustration vs. clusterability in two-mode signed networks (signed
bipartite graphs) (2010)
8. Zha, H., He, X., Ding, C., Simon, H., Gu, M.: Bipartite graph partitioning and data
clustering. In: Proceedings of the 10th International Conference on Information and
Knowledge Management. pp. 25–32. ACM (2001)
6

The-top-US-conservatives-and-liberals.html

Exploring Evolving Media Discourse Through Event Cueing
Yafeng Lu, Michael Steptoe, Sarah Burke, Hong Wang, Jiun-Yi Tsai,
Hasan Davulcu, Douglas Montgomery, Steven R. Corman, Ross Maciejewski, Senior Member, IEEE

Fig. 1: Overview of the event cueing visual analytics framework. The map view provides a geographical visual analytics environment to enable exploration of frames and entities over space and time. The detailed view to the right of the map switches
between entity wordles and list-based displays. The time series view contains a hierarchical frame analysis visualization. Each
line visualizes significant events and the sentiment associated with a media frame or a frame class in the expanded leaf nodes. The
control pane, which consists of the top left donuts, shows the distribution of frames and events and is used to filter categorical
variables in the linked views.
Abstract— Online news, microblogs and other media documents all contain valuable insight regarding events and responses to
events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously)
to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how
topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these
shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance
the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As
discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is
different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this
cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these
changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two
different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential
relationships between climate change framing and conflicts in Africa.
Index Terms—Media Analysis, Time Series Analysis, Event Detection

1

I NTRODUCTION

Recently, the visual analytics community has begun developing a variety of tools for analyzing media collections. These tools tend to focus
on event detection from text streams [39], correlation analysis [28],

• Yafeng Lu, Michael Steptoe, Sarah Burke, Hong Wang, Jiun-Yi Tsai,
Hasan Davulcu, Douglas Montgomery, Steven R. Corman, and Ross
Maciejewski, are with Arizona State University. E-mail:
{lyafeng,msteptoe,seburke2,hxwang,jtsai8,HasanDavulcu,
doug.montgomery,steve.corman, rmacieje}@asu.edu.
Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of
publication xx Aug. 2015; date of current version 25 Oct. 2015.
For information on obtaining reprints of this article, please send
e-mail to: tvcg@computer.org.

and topic evolution [15]. These tools are often concerned with understanding an ongoing narrative from structured text and focus on
enabling the user to place news stories within the context of other
ongoing events. However, very few tools [11, 12, 13] explore how
media is being framed, and, to our knowledge, none have explored
changes in frames over time and space. In studying public communications, framing is the use of rhetorical devices (e.g., words, phrases,
metaphors, images) to encourage one interpretation of a set of facts
and discourage other interpretations [16]. Examples include efforts by
U.S. conservatives in the 1990s to reframe the estate tax as a “death
tax”, and competing frames of the Occupy Wall Street protests, “the
99%” (vs. the 1%) as opposed to “makers vs. takers”. Framing affects
the attitudes and behavior of audiences [9], and it is also regarded as
a key media effect, in that media “actively set the frames of reference
that readers or viewers use to interpret and discuss public events” [35].

Understanding framing in the media is vital as it influences the way
people interpret the topic under analysis. Framing is also critical to
the success of social movements and can be a driver for change or
stagnation [4]. What is of interest is how these frames are applied and
how they evolve over time in the context of other events. However, it
can be quite difficult to determine when changes in framing occur and
what events may have contributed to changing attitudes.
In this work, we present a visual analytics framework for event cueing from media. For a given collection of documents (related by topic
and coded with frames), we enable analysts to explore ongoing media
discourse with respect to the overall framing and related sentiment of
the narratives. In order to understand when and how framing about a
topic has shifted, we employ intervention models for time series analysis. Such models examine how a measure changes over time and how
this measure is affected by some external event, or intervention, at a
given time t. If the measure is significantly different before and after the intervention, then one can hypothesize that an intervention is
associated with a change in the measurement. By highlighting these
statistically significant intervention points, we can cue analysts to time
periods of interest. Then, by linking the media data source with secondary sources of information relevant to the topic, an analyst can
explore the frame evolution within the context of ongoing events.
This work is directly related to previous works, such as Narratives [18] and EventRiver [27], which focus on placing media stories
into their historical and social context by allowing people to explore
topics and keywords and associate them with other ongoing stories
and events. Unlike previous work, our framework utilizes intervention modeling strategies and multisource data. Our goal is to enable
analysts to cue to important dates in the dataset. Media can then
be explored in the context of the changing sentiment of the framed
documents as well as linked to concurrent events that may have impacted the media discourse. While previous work from Diakopolous
et al. [11, 12, 13] developed tools for frame analysis, their work provided no support for entity extraction, sentiment analysis, or linking
multisource data. Our contributions include:
1) An ensemble of intervention modeling techniques for event cueing and hypothesis generation,
2) The application of visual analytics for media framing in the context of entities, sentiment, geography and multisource data.
2 R ELATED W ORK
As media sources have broadened from network news coverage to microblogs, Twitter, etc., a variety of tools and techniques have been
developed to analyze and explore such data sources. Given that media
data generates events over time in unstructured text, the majority of
tools and techniques developed have focused on temporal visualizations, topic analysis, and pattern and anomaly detection.
2.1 Time Series Visual Analytics
Visualization has been successfully applied to analyze time-oriented
data, most commonly through the use of line graphs and their variations [17], as well as calendar views and clock views for periodical or seasonal patterns [3, 22, 37]. A variety of enhancements to
these techniques have been proposed over the years to enable better
sensemaking of events and records. For example, LifeFlow [42] combines a list-based display for intra-record pattern analysis and an aggregated overview display for inter-record trends analysis to visualize
time-point based event sequences. EventFlow [30] extends LifeFlow
to handle interval events and explore the relationship between event
sequences and associated outcomes. Another extension of LifeFlow,
Outflow [41] aggregates multiple event sequences, visualizes them as
the pathways through different event states, and connects the pathways to their associated outcomes so that users can explore progression paths and results. Of interest to our work is that OutFlow also
incorporates external factors which may influence the event sequence.
Our work differs in that we focus on cueing analysts to events in time
series datasets through the use of intervention models. These models
enable users to find sequences in the data that appear to have deviated.

Our framework then links these deviations to external data sources to
identify potential causes to these deviations.
The incorporation of statistical techniques into time series visualization has led to the development of a variety of visual analytics solutions. A typical example is the visual analytics process proposed
in Bogl et al. [5] where visualization is used to guide domain experts in statistical model parameter selection. Their prototype system,
TiMoVA, is developed to facilitate the process of parameter settings
in autoregressive integrated moving average (ARIMA) and seasonal
ARIMA models. A probabilistic decision tree learner is used in the
e-transaction time-series visual analytics system VAET in [43] to explore transaction patterns among multiple users in a temporal context.
These tools focus on enabling users to visually develop statistical models of the data. In contrast, our work focuses primarily on using statistical models for cueing analysts to events of interest in the time series.
Our proposed type of cueing is similar to work in event detection, and visual analytics has posed a variety of solutions for anomaly
and event detection [8]. Classification-based event detection methods
have been applied in many visualization systems. For example, Scatterblogs [6, 36] is a scalable system enabling analysts to find quantitative information and detect spatiotemporal anomalies within a large
volume of geo-located microblog messages. Work by Chae et al. [7]
utilizes a seasonal-trend-decomposition method to determine anomalous changes in topics in social media. Gotz et al. developed DecisionFlow [20], which integrates interactive multi-view visualizations and
ad hoc statistical models to support the analysis of high-dimensional
temporal event sequence data. While a variety of statistical methods
have been applied for visual analytics of temporal data, these methods
typically focus on anomalous behavior. In our framework, we focus
on the concept of an external intervention causing the system to deviate. This framework requires different statistical analysis and also
needs to integrate multi-source data for analysis. To our knowledge,
this approach is the first such application in visual analytics to explore
time series data in the context of interventions.

2.2

Media Visual Analytics

While applicable to a variety of domains, our focus is specifically on
media data, such as online news and microblogs. Recently, much attention has been paid to this domain area in the visual analytics community, with techniques focusing on knowledge expression, topic extraction, pattern analysis, and storytelling [14, 19, 21, 23, 24]. CloudLines [23] focuses on the detection of visual clusters in a compressed
view of multiple time series to enable the scalable analysis of media streams. To improve sensemaking, LeadLine [14] explores named
entities, locations, and bursts of topic related events by visualizing
the shift of topic volume for different time streams and emphasizing detected events. Contextifier [21] is designed for contextualizing visualization by providing customized annotations for the stock
timeline graph with reference to the content in a news article. StoryTracker [24] combines interactive visualization and text mining techniques to facilitate the analysis of similar topics that split and merge
over time. NewsViews [19] is a novel automated news visualization
system that creates thematic maps automatically for news articles. It
leverages text mining to identify key concepts and locations discussed
in articles. TopicPanorama [25] visualizes the full picture of relevant
topics from different sources to analyze common and distinctive topics. Similar to previous works, we also leverage text mining techniques on media articles. Our system extracts entities and their associated geolocations. Unlike previous works on media frame visual
analytics [11, 12, 13], which typically focus on topic analysis and cooccurring words, our system focuses on frame analysis in conjunction
with multisource data. We focus on a single topic and explore how it
is being discussed (i.e., framed), rather than focusing on multiple topics. In this work, frames are organized into a hierarchical set and the
change in how documents are framed (with respect to space and time)
can be explored. By visualizing statistical results together with the hierarchical frames, we can enhance the hypothesis generation process.

(a) Choropleth

(b) Cluster Pie

(c) Weighted Category Choropleth

Fig. 2: Categorical data spatial distribution visualization view. View (a) shows the default choropleth map which colors each country based
on the density of all frames. View (b) shows pie glyphs on the map displaying the proportional distribution of different frame categories in
each cluster. View (c) shows a weighted choropleth map which colors each country based on the weighted frame density. The weights on each
category can be changed interactively by the analysts.
3 E VENT C UEING E NVIRONMENT
The goal of our visual analytics framework is to facilitate the hypothesis generation process by linking multisource data through statistical
event cueing in the form of intervention models. Our framework consists of three main views: 1) the spatial view (top left Figure 1), which
visualizes the geographic location of media steams and events coded in
secondary data sources; 2) the detail view (top right Figure 1), which
provides a lens into the media text and detailed descriptions of events
from secondary data sources chosen by the analyst, and; 3) the time series view (bottom Figure 1), which shows a hierarchical frame-coded,
time-orientated media stream with sentiment and intervention analysis. All views are linked by the overview timeline shown in the middle
of Figure 1 which displays the trend of a secondary dataset.
3.1 Task Characterization
The basis for this work is founded on an interdisciplinary collaboration between computer science and communication. Partners from the
Hugh Downs School of Human Communication at Arizona State University are interested in applying their knowledge of framing to issues
of national security risks related to climate change. Their work focuses
on exploring the framing of climate change research in Africa and how
(if at all) this is impacted by ongoing conflicts in the region. They posit
that, in order to understand how the media reflect different organizations’ interests in addressing climate change as a social problem, it is
necessary to supplement the social movement focus on resource mobilization to framing processes of collective action problems. To do this,
they developed a nuanced typology for studying climate change framing and its adequacy for supporting social movement that would be
necessary to overcome the collective action problem. They apply this
framework to examine framing of climate change in media and social
media texts collected from the Niger Basin region over eight months
from August 2014 to March 2015, applying a novel coding technique
to assess diagnostic, prognostic and motivational framing as the keys
to effective social movements. While the datasets and examples given
in this application focus on media with regards to climate change and
social unrest, our techniques can be adopted to any multi-source data
in which analysts are looking for changes in media frames due to associated events (for example, severe flooding, prolonged droughts). We
have identified three major intents of the communication scientists in
the context of media analysis:
1) Analysts would like to know how frames are spatially distributed
to understand the international context of framing;
2) Analysts would like to know when the distribution of frames
change and quickly be able to explore events that may have impacted this change in media framing;
3) Analysts would like to know what people, locations and organizations are being discussed in the media before, during and after
changes in framing occur.
As such, our framework has been designed to support the spatiotemporal analysis of frames and cue analysts to when the distribution of

frames has changed. These cues then suggest time windows in which
to explore links to secondary datasets.
3.2

Datasets

To illustrate our framework, we use a climate change media dataset
and the Armed Conflict Location & Event Data Project (ACLED)
dataset [1] as an example. However the proposed framework is flexible
for analyzing any media data.
Media: The media dataset is composed of RSS feeds from 122 English language news outlets in the Niger basin countries since August
2014. RSS feeds were scanned hourly and filtered for relevance in
a two-stage process. First, news texts were matched against a set of
222 keywords developed from the Intergovernmental Panel on Climate
Change (IPCC) report and supplemented by project experts. Subsequently, texts passing the keyword test were analyzed by a machine
classifier, trained on a set of 1,000 texts classified by coders as relevant or irrelevant to social discourse of climate change. News articles passing both tests were placed into the database for analysis.
The RSS news dataset collected 1245 relevant articles with 9070 sentences. For this study, each sentence was coded by trained coders
into one (or none) of 25 categories comprising four classes (cause,
problems/threat, solution, motivation) that represent different types of
framing for climate change. Then each article was represented by a
vector of frame counts normalized by the number of sentences coded.
The average Krippendorff α reliability of the coders on a set of training documents was 0.81 and judged to be acceptable. Future work will
use trained classifiers for frame extraction.
ACLED: The ACLED dataset contains information on the dates and
locations of all reposted political violence events in over 50 developing
countries, with a focus on Africa. Each event record contains information on date, location, event type and actors involved. From August to
December 2014, it contains approximately 6500 events.
3.3

Media Data Processing

Media messages contain large amounts of information which can be
complex to effectively analyze. Our framework applies a variety of
automatic data preprocessing techniques including entity, geolocation,
and sentiment extraction.
Entity Extraction: Entities, such as person names, locations, and organizations, inform much of the underlying media discourse. A variety
of named entity recognition methods have been proposed for different
contexts in natural language processing. We applied the well-known
natural language processing tool CoreNLP [29] to a streaming RSS
news dataset and the secondary dataset (ACLED in our example) to
extract named entities. For the 1245 articles from August to December
2014, we have 19,756 entities in which there are 2107 persons, 5791
locations and 3146 organizations extracted from the RSS dataset. The
same entity recognition process was performed on the ACLED dataset
for the notes in each record, extracting 367 persons, 998 locations, 286
organizations.
Geolocation Extraction: An article may have location attributes, either based on where the article is posted or the region the article

Fig. 3: Entity lens on the map shows the most frequently appearing entities recognized from documents that are geo-located in the lens’ area.
The left figure shows the named entities for the RSS news dataset, the middle figure shows the actors given in the ACLED dataset,and the right
shows a comparison lens with the RSS news’ entities on the left and ACLED events’ actors on the right. The example shown in this Figure
covers the data from Oct. 11th to Oct. 27th for all the ACLED event type and problem frames in the RSS news.
discusses; however, this information is not always explicitly coded.
Given that framing may differ by geographic region, our framework
preprocesses the media stream to extract geographic locations. We use
the Data Science Toolkit [2] to extract and geocode this information.
Sentiment Extraction: Media data encapsulates information about
events, responses, and reviews. In exploring media data, sentiment
analysis can provide a quick overview of the attitude a media document’s author might have with regards to the underlying story. To
extract the sentiment embedded in the media data, three sentiment
analysis classifiers are applied at the per sentence level. Details on
the classifiers are provided in Section 3.6.

To link different datasets and find relationships between them, this
entity lens has three modes: media data entity mode which shows only
the RSS news entities (Figure 3(Left)), secondary data entity mode
which shows only the ACLED actors (Figure 3(Middle)), and the combination mode which is a two-sided lens to encode entities for multisource data (Figure 3(Right)). The combination mode shows the top
entities from the RSS news dataset on the left of the lens and ACLED
actors on the right with a dashed line in the middle to separate one
from the other. All modes are also enabled in a coordinated view in
which the lens can move over the map and update the wordles.
3.5

3.4 Geographical View
Both media data and event sequences from the secondary dataset have
geolocation information. The geographical view is built to explore the
distribution of frames and compare entities between media data and
other data sources.
3.4.1 Exploring Spatial Distribution of Frames
Previous work on frame visualization focused on document keywords.
In this work, we want to allow users to explore frames by country, entities, and sentiment. To analyze the spatial distribution of different
frames, we created a categorical spatial data distribution visualization
view, Figure 2. To show the cumulative frame distribution of a dataset,
Figure 2(a) displays a choropleth map colored by the density of frames
in each country. Users can select any subset of frame categories to analyze. If only one class of frames are selected, the map color matches
the color of the class, otherwise it uses gray. A drawback of this visualization is that only one variable/feature of the underlying data can be
represented, even though there are multiple categories of frames in the
data. To allow for multivariate encoding, we also use a symbol map
with a pie chart, where each segment of the pie represents the number
of sentences of a given frame (Figure 2(b)), and a weighted category
choropleth map (Figure 2(c)) where colors correspond to a multivariate criteria function. Additionally, a tooltip displaying the histogram
of different categories within a country is enabled to help better explore frame distributions.
3.4.2 Exploring Geo-located Entities
Our framework considers two types of entities in the data. One is recognized name entities, which are people, locations and organizations.
The other is the predefined entities that may exist in the structured
datasets that an analyst wishes to explore in the context of media discourse. We created an entity lens to explore geo-located text data.
The geocoding of the entities derives from a sentence’s geocoding for
the RSS news dataset and the reported geolocation from the ACLED
dataset. The entity lens is shown in Figure 3, where the most frequently referenced entities within the lens’s area are extracted and organized around the lens. The most frequent entities are mapped closest
to the lens’s circumference based on available canvas space. The font
size is dependent on the entities’ frequency within the lens’s circumference. The more frequent an entity is, the larger the text.

Hierarchical Frames Timeline View

The previous views are necessary to allow overview and detail views;
however, the major contribution of this paper is the event cueing which
is enabled through the hierarchical frames timeline. Previously mentioned techniques enable exploratory data analysis, the problem is that
purely exploratory techniques put the burden of analysis completely on
the analyst. Our goal is to cue the analyst to events that are statistically
interesting in the data. To enable this, we begin with the timeline view
showing the relative volume of frames per day. Specifically, each document has a number of sentences that are encoded with a single frame.
The percent of framing of a document is the number of sentences in
a document associated with a given frame divided by the total number of sentences framed. The frame volume can be visualized by the
average document percent per day, the average number of sentences
encoded with a frame across all documents in a day, or a variety of
other metrics.
To detect possible interventions, we applied two time series analysis
models and visualized the results on the timeline to cue analysts’ exploration. In addition, sentiment information associated to the underlying data is also revealed by a two-side uncertainty-based stack river.
Figure 4 shows our hierarchical timeline view of the media frames.
Here an analyst expanded the cause frame to explore sentences that
frame the cause of climate change to be due to human, natural effects,
policies, or one uncertain of the cause.
Our approach is centered around the concept of an intervention.
We assume that there may be events intervening with media reactions
that cause a shift in how frames are distributed. We use statistical
hypothesis test to detect the intervention dates. For each day in the
dataset, we assume an intervention may have occurred. If a date under
test indicates that the times before and after are statistically different,
this then cues an analyst to explore related datasets to help enhance
their understanding of what (if any) events may have triggered these
changes in discourse.
Intervention Modeling Intervention models are used to explore what
(if any) impact there is between an event and some secondary measure,
for example, the impact that 9-11 had on George Bush’s approval rating. In this case, we consider our events to be armed conflicts and
the measure to be the amount of sentences that are framed in a document with respect to one of the 25 climate categories. Note that such
models can be used for any event and measure dataset combination.

Fig. 4: Hierarchical timeline view showing intervention modeling results, Before-During-After analysis results and sentiment river for each
expanded frame or frame category are shown. The frame structure is displayed as a dendrogram on the left. Clicking on the node can expand/collapse its children. The timeline associated with each leaf node is shown on the right.
Mathematically, when a time series model is affected by another input
time series, a transfer function-noise model can be used to improve the
model. The general form of this type of model is:
yt = v(B)xt + Nt

(1)

cueing and hypothesis generation. Events and frames of interest found
require further investigation. Initial analysis of each time series (each
frame) indicated that there was no significant autocorrelation present.
Therefore, the intervention model can be simplified to:
(t ∗ )

yt = µ + ωIt
where yt is the time series of interest, v(B) is an autoregressive, integrated, moving average (ARIMA) model for the time series yt , xt is
the input time series, and Nt is a noise process [31]. A specific case
of transfer function-noise models is an intervention model, where the
input time series is an indicator variable that specifies whether some
event has taken place at time t. Such an event may have a temporary
(or permanent) effect on the level or mean of the time series of interest.
An intervention model can model the effect of a known event on
the time series. However, another common application of intervention
models is to identify outliers in the time series. In this case, we do
not know the exact time period in which the event (outlier) has taken
place. The transfer-function model for this application then becomes:
(t ∗ )

(t ∗ )

yt = v(B)εt + ωIT , where It

=

 1 if
0 if

t = t∗
t 6= t ∗

(2)

where ω is the change in the mean of the time series at time t ∗ and
(t ∗ )
It is an indicator function assuming that the effect of the outlier is
temporary and only occurs at time period t ∗ . Other models can be used
to model the case where an outlier may have a lasting impact on the
mean of the time series. An iterative procedure is used to identify multiple outliers in the time series. In this scenario, multiple intervention
(t ∗ )
models are fit, updating It for t ∗ = 1, . . . , N for a time series with N
time periods.
For the media data explored in this paper, intervention models were
used to detect outliers, i.e. cues to events that may be of interest to
the analyst, for each of the 25 frames over the time period of August
2 to December 31, 2014. For our intervention model, Figure 4 shows
the trend of several frame categories. A black dot represents a statistically significant shift in frames between the week before and after this
date. Users can then use the coordinated views to explore events that
occurred at this time and begin forming hypotheses on the impact that
events may have had on the media framing. Note that this is for event

+ εt

(3)

where µ represents the overall mean of the time series and εt
represents the error. Outliers at time t ∗ ,t ∗ = 1, . . . , N, can be identified
by comparing the estimated value of ω, ω̂, to its standard error [31].
A significance level of α = 0.05 was used to determine whether the
value of the frame at time t ∗ was an outlier. The presence of an
outlier cues the analyst to investigate what caused this change in the
frame distribution. Although the intervention model is simplified
because the frame time series were not autocorrelated, this approach
is still valid for time series data that does have autocorrelation and
Equation 2 would be used in such cases. Such models are sensitive
to the time period under exploration. In this case, our analysts were
exploring short term changes (1 week prior to the event, 1 week after
the event). As such, the results of the intervention model tend to
highlight peaks in the data; however, this is likely an artifact of the
chosen window sizes. Future work will explore visual representations
for exploring interventions under varying window sizes.
Before-During-After Analysis Since there was no autocorrelation
in the data, a secondary model which requires an assumption of data
independence, can be applied. The second intervention test defines
a Before, During, and After period (where the during period can be
seen as the intervention) and tests their location based on the data
distributions. We let t denote the start time of the During period,
and the time windows for the Before, During and After segments are
represented by WB ,WD , and WA respectively. In this manner, the three
time segments cover the following time periods: Be f ore : (t −WB ,t),
During : (t,t + WD ), and A f ter : (t + WD ,t + WD + WA ). The data
samples for the three segments are denoted as DB = {x1 , x2 , . . . , xnB },
DD = {y1 , y2 , . . . , ynD }, DA = {z1 , z2 , . . . , znA } and they may vary
in length. Each data sample is the percentage of the frame in one
document. Because there was no significant autocorrelation present
in our underlying dataset, each sample is assumed to be independent
and identically distributed (i.i.d.) where Di ∼ N(µi , σi2 ). Therefore

we form the problem to be tested as follows:
A1 : µD is not significantly different than µB
A2 : µD is not significantly different than µA
H0 : µD is not associated with an intervention (A1 ∩ A2 )
H1 : µD is associated with an intervention (A¯1 ∪ A¯2 )
We test H0 by testing A1 and A2 . To test A1 and A2 , we applied a two-sample location test, Welch’s t-test [40], on DB , DD and
DD , DA individually with significance level α. In these two t-tests,
the statement is the null hypothesis. Because of the multiple comparisons problem (in our case we have two tests one for DB and DD ,
and another for DD and DA ), and based on the Bonferroni inequality
P(A1 ∩A2 ) ≥ 1−2α, we applied Bonferroni correction and set the significance level for each test according to the following equation [32],
α=

αF
,
#test

(4)

where α is the significance level for each two-sized t-test, αF is the
family significance level for the multiple comparison for each During time period, and #test is the number of tests applied at each time
period. In our case, #test equals 2 (the tests of A1 and A2 ). We set
αF = 0.05, which guarantees that the overall significance level for
the 2 hypothesis tests at each frame period is 0.05. To guarantee
αF = 0.05, we set α = 0.025 for each single test on the pair of consequent segments. Given the test result and the estimated µ, we can
form 9 types of volume change patterns listed in Table 1. The 9 types
are visualized in different color blocks on the time line for each frame,
as shown in Figure 4 and Figure 5.
The color scheme also denotes the group of patterns as increasing,
decreasing and oscillating. To change the interval length of each test
time period, the user can change the size of the three windows for
Before During and After using the spinners on the left. To better focus on a particular Before-During-After pattern, the user can click on
the pattern legend to gray out options. In addition to knowing the intervention time point from the results of the intervention model, the
Before-During-After analysis provides an adjustable window size and
shows any significant changes.
The statistical tests’ results are visualized in our timeline view for
each frame and frame categories, shown in Figure 4 and Figure 5.
The intervention modeling result is a set of binary indicators denoting
the statistically significant intervention points. This result is represented as a black dot on our timeline view. The Before-During-After
analysis’s result is a set of patterns describing statistically significant
changes in frame distribution over time. In both cases, the analyst can
adjust the before, after and intervention (during) periods using the controls seen in Figure 1 (lower left). In our case study, the analysts were
interested in a single day intervention with a 7 day news cycle.
Table 1: The pattern summary for Before-During-After analysis. Each
pattern is associated with a unique hue as shown in the lower lefthand
legend of Figure 1
pattern
B=D=A

sketch

description
no significant change

B=D<A
B<D=A

increase

B<D<A
. B=D>A

(blues)

B>D=A

decrease

B>D>A

(greens)

B>D<A

oscillating

B<D>A

(oranges)

Fig. 5: Sentiment stacked area chart on bi-side of the time series view.
The blue area represents positive sentiment and the red area river represents negative sentiment. The darker the area color is the more certain the label is for those sentences’ sentiment class.
3.6

Frame Sentiment Visual Analytics

The underlying sentiment of the media and its relation to the framing
can also provide insight. Sentiment analysis visualization has been
successfully applied across a variety of domains, such as political election analysis [38], and merchandise reviews [33]. However, most classifiers are text context sensitive and need to be trained on a particular
domain’s data to boost performance. Furthermore, the limitation of
sentiment classification accuracy is a problem in sentiment analysis
and is subject to uncertainty [10]. In our visual analytics framework,
we employ anl entropy-based sentiment river to reveal the uncertainty
of sentiment over time using an ensemble voting scheme from multiple
classifiers to determine the final sentiment label [26].
In our previous work [26], the uncertainty was visualized in each
time period along the entropy sentiment river. However, the time information associated with RSS media data is not as precise as online
social media data, such as Twitter. In general, the time parsed out
from the RSS news is at the granularity of a day. In one day, there
can be multiple articles collected relating to the target topic and each
article also contains several frame coded sentences. Instead of exploring only the change of the certainty over the media stream, the volume
of certain and uncertain sentiment labels is also explored. To enhance
the understanding of the volume change for both certain and uncertain sentiment labels, a stacked area graph is used to represent each
uncertain level with a stacked area and low uncertain area is stacked
at bottom. Figure 5 shows this view, where the positive sentiment is
colored in blue on top of the time series, while the negative sentiment
is colored in red on bottom of the time series. The volume of relatively
certain sentiment values are shown with a darker color and the uncertain volume with a lighter color. The height of the stacked area graph
shows the average volume of sentences per document in each sentiment polarity over time as well as the portion of uncertainty. In this
way, we can explore the positive and negative sentiment of the media
documents in conjunction with their underlying frames.
3.7

Detail View

The detailed view, Figure 6, contains two modes, the entity wordle
display and the list-based summary display. The data under analysis
for this view changes along with the time period selection, the subset
data selection for both media data and the secondary data, and the
geospatial selection. When a user is only exploring the frame class
‘Problem’ which is colored in red, only the RSS articles containing at
least one sentence being framed as ‘Problem’ will be displayed in the
detail view. For a geospatial selection, e.g. the user selects a country
to explore, the data displayed in the detail view updates to filter for
only the articles and ACLED events related to this country.
In the entity wordle view, the most frequently named entities extracted from the two datasets are displayed in two wordles. Based on
the entity’s class, which is either person, location, or organization, the
word is colored in red, green, or blue respectively. The actors in the
ACLED dataset, being entities as well, are colored in black. The size
of those entities displayed here is also proportional to its frequency.
In the list-based summary view, the RSS news article is summarized
by showing the title and a list of colored squares, where each square
represents each framed sentence colored by its frame class’s color. The
ACLED data, being the secondary data here, displays its notes for each
event in the selected time period. The background color of each note
matches the color for its event type. To analyze events containing

(a) Detail view in entity wordle display

(b) Detail view in list-based summary display

Fig. 6: The detail view showing the most frequently named entities in a wordle display and document summary information in a list-based
display. Here we show data from Oct.11 to Oct 27. View (a) is the entity wordle display in which user can choose three classes of entities
(person, location, organization) to show. Black text in the ACLED wordle indicates an actor in the events. View (b) is the list-based summary
display in which the title of media articles and the summary information of the secondary dataset are listed in order of time. The frame
information of each article is summarized into colored squares (the color of the square matches the frame class) in the sentence order from the
article. In this example, the ACLED event notes are filtered by clicking on the entity text ‘Boko Haram’.
a particular entity of interest, a user can click on a particular entity
shown in the wordle display and information containing that entity
will show up in the summary display. Users may also filter by location
by selecting a country in the geographical view.
4

C ASE S TUDY: C LIMATE C HANGE F RAMING AND A RMED
C ONFLICT IN A FRICA
In this section, we demonstrate our work by applying the methods described so far to the RSS news dataset collected on Climate Change
from African countries and the ACLED data set, which focuses on
armed conflicts and political violence in Africa. Collaborators were
interested in linking these two data sets based on previous articles and
reports that discussed the impacts of climate change on armed conflicts [34]. Their goal was to explore the framing of news stories related to climate change and see what, if any, armed conflict events may
be linked to that discourse. In this manner, social scientists can begin
to develop models and theories about how framing can help drive political change, or conversely, how armed conflict is driving discourse.
4.1 Exploring Problem Frames in Africa
The analyst begins with an overview of the system and explores the
distribution of frames over the entire time period of data collection.
The main points of interest are the spatial and temporal distributions of
frames, Figure 7. First, the analyst explores the spatial distribution of
frames, looking at the weighted majority choropleth map. The analyst
notes that most regions are discussing climate change either in terms
of problems (red) or solutions (green). Only a few countries, such
as the Republic of Guinea-Bissau and the Republic of Côte d’Ivoire,
have a majority distribution related to causes of climate change, and
Congo has more motivation frames. The analyst drills down into the
data by selecting a country and quickly learns that only one document
has geographic information related to these countries. Thus the analyst
determines that these outliers are of little interest.
Given that the discourse seems to focus heavily on both problems
and solutions, the analyst decides to explore the temporal view with a
focus on problems. The analyst searches the top level problem hierarchy looking for significant events found in both the intervention model
and Before-During-After model. The analyst finds a time period in
late October (highlighted as circle a in Figure 7) with several points
of interest, and highlights this time period for inspection. The analyst then expands the tree and explores the leaf node problem frames,
Figure 7(bottom). The analyst notes that there are significant interventions in many categories, but very few frames regarding security
threats and water problems in this time period. The analyst further
comments on the lack of water framing in the documents noticing that
climate change is often associated with extreme weather, including
drought, yet there is little discussion in Africa about problems related

to water. The analyst does notice that there are many documents discussing problems with food.
The analyst decides to first focus on the food problem frame and
the events leading to this change in the frame distribution. The analyst narrows the time period to October 11th to October 28th, and
then filters for RSS news articles containing frame category ‘ProblemThreat’ and ACLED events Riots and Battles. The analyst wants
to explore what geographic regions are seeing large amounts of armed
conflict during this time period. The analyst selects the most prevalent
ACLED events (Riots-yellow and Battles-red) using the donut control.
The analyst notes that the largest amount of conflicts are occurring in
Nigeria, Sudan and Somalia. Given the importance of the Niger River
Basin in the area, the analyst chooses to explore events in Nigeria that
may be driving the discourse on climate change. The analyst notes
that it is interesting that there is a clear separation between the riots (in
the south) and the battles (in the north). The analyst selects Nigeria
to filter the detailed view to only RSS documents and ACLED events
that are geocoded to Nigeria.
Looking at the RSS articles’ titles, the analyst finds many reports
talking about the problem of food security and famine in Africa. While
exploring the ACLED events in the same time period, the analyst locates several riots and battles discussing the impact of Boko Haram
on farmers, where militarists are killing farmers and forcing them to
flee their homes, exacerbating the food problem. Example articles and
events are shown in Figure 8(Left). What is interesting to the analyst
is that articles are already discussing the famine problems that Africa
will face due to climate change. If this is further exacerbated by wars,
the problem cycle may become more prevalent resulting in displacement, migration, and potential social unrest. From a social science
perspective, our analysts are interested in how to model such phenomena. By cueing them to such events, they are able to begin looking at
how ongoing events could be modeled to predict future problems.
After discussing the events surrounding the food frame cue, the analyst then decides to also explore the ProbThreatHealth frame (problems associated with health) next. The analyst is interested in the
two significant events that occurred between October 11th and October 28th. Again, the analyst begins exploring related ACLED events
during this time period, and quickly finds several riot/protest events
related to the mistreatment of healthcare workers in the region. The
analyst again noted their interest in these articles and the fact that the
event cueing was able to narrow down their search to potentially relevant information. While there are some obvious links between food
security, armed conflicts and riots (for example, Boko Haram displacing farmers), subtle social issues involved with riots may be harder to
spot. Furthermore, given that such riots are taking place at this time
and there is a shift in frames, the analyst hypothesized that this could
represent a shift in the discourse in the hopes to alleviate concerns

Fig. 7: Exploring the whole time period on the RSS news dataset spatially and temporally. The spatial map shows a weighted choropleth map
with all frame class equally weighted. The add-on histogram shows the frame volume and distribution of the Republic of Côte d’Ivoire. The top
timeline view shows the level of four frame classes and two black circles highlight the time period of interest in ProblemThreat and Motivation.
The bottom timeline view shows the expanded timelines in the ProblemThreat frame class and the time period of interest is highlighted.
from the general population. While no definitive conclusions could
be made at this time, this example further illustrates how our framework can enhance the hypothesis generation process. By specifically
cueing an analyst to a time of interest, we can dramatically cut their exploratory analysis time. For example, there are over 40 ACLED events
per day, each with an associated set of documents. Uncued analysis of
such work would be an extremely laborious process.
4.2

Exploring Motivation Frames in Africa

The analyst concentrates on examining press coverage between
November 1st and November 14th, and identifies events accounting
for notable intervention points on November 6th based on the BeforeDuring-After model. Results indicate an increasing trend in the media
discourse on calling for policy actions on November 2nd with a negative tone. The statistically significant interventions and the burst of
the sentiment can be found in the Figure 7(highlighted in circle b).
The changing pattern is predominantly associated with the launch of
an updated synthesis report by the UN’s Intergovernmental Panel on
Climate Change (IPCC) on November 2nd. Several articles reporting
IPCC can be easily found and shown in Figure 9. As the most comprehensive assessment that attracts worldwide attention, the new IPCC
report summarizes alarming evidence detailing severe impacts of climate change. Adverse impacts range from increased risks of extreme
weather events, food shortages, and violent conflicts. The alarming
messages, circulated by several media outlets, were framed in mostly

negative words (e.g. serious impacts, severe impact, dangerous, catastrophic). In addition, analysts find prevalent explicit statements calling
for international governments to take actions now. The following sentences describe examples of motivational framing.
• “Massive cuts to greenhouse gas emissions are needed in the
coming decades to curb temperature rises to no more than 2C, the
level at which it is thought dangerous impact of climate change
will be felt.”
• “Leaders must act.”
• “There is cause for hope if governments take action.”
• “A binding meaning and enforceable framework is needed to
limit the consequences of global warming.”’
• “The world’s largest polluters, the United States and China,
should take the lead in reducing emissions.”
Conversely, there are noticeable spikes of positive sentiment values
between November 9th and November 12th. The pattern is largely
associated with favorable coverage of U.S. and China announcing a
historical climate change agreement on November 11 when President
Obama visited Beijing for the Asia Pacific Economic Cooperation
(APEC) summit. Together, motivation frames in West Africa reflect
a focus of relying on international actors to drive policy negotiation.

Fig. 8: The geographical and detail view for exploring RSS news and ACLED data. This figure shows the analyzing time from October 11th to
October 28th. The geographical view color each country by the majority frame class and displays riots (orange dots) to represent the ACLED
events. The detail view lists the Riots events related to health problem within and outside Nigeria. The left side detail view shows examples of
RSS articles discussing food problem in Africa and the ACLED events are riots and battles expressing problem of food supply.
Results of analysis on motivational frames should be viewed in
light of limitations. In the 1,245 relevant articles collected from West
African news media and twitter links, there is little evidence of motivation framing, as less than 10% of a news story contained statements calling for definitive courses of actions, That is, motivational
frames are very uncommon compared to other three frame classes
(cause, problem/threat, and solution). When a set of news stories highlighted explicit calls for actions to solve climate change issues within
the same time period, it is highly possible that the consistent pattern in
press coverage was statistically significantly different than before and
after in the time series analysis. Despite the low presence of motivational statements in the current dataset, the visualization tool allows
researchers, analysts, and policy makers to explore the potential underlying mechanisms linking adverse impacts of climate change and
increased risk of political conflicts.
4.3

Analyst Feedback

Our case study involved two analysts from the Department of Communication at Arizona State University. Feedback on the system was
positive with analysts indicating that the event cueing features were extremely useful in providing a starting point for searching linked data.
Case Study 1 was done as a paired analysis demonstrating the tool
with the computer scientists manipulating the controls and discussing
how the system worked. Case Study 2 was done at the communication
lab with no assistance from the computer science group (the tool is
web-deployed).
Overall feedback was positive with the analysts stating that they
were “fascinated by the visualization tool’s ability to map out temporal and spatial components of media discourse”. In addition, the
analysts also mentioned that this tool can help to tackle co-occurrence
patterns of conflicting events, limiting the possibility of bridging distinct lines of scholarship together–media research, climate change and
conflicts. However, there were suggestions for future work and improvement. Specifically, the analysts were interested in the difference
between the change models and their disagreements. For example, in
Figure 1, there is an intervention marker (black dot) near October 5th
for motivation, but no colored squares from the before-during-after
analysis. The relationship between these two models required more
explanation and future work will explore creating a single ensemble
metric. Along with the intervention model, the analysts also requested
the ability to reconfigure layouts for improved storytelling. They indicated that they would be able to better explore relationships with a
series of small multiples and better alignment between the temporal
components of the unrest data and the framing data.

Fig. 9: Example RSS articles and the entity wordle for the time period of Oct. 28th to Nov. 11th exploring motivation frame. The left
side article summaries show examples of news report relating to the
IPCC and the right side wordle emphasizes the most frequent entities
appearing in those articles, such as IPCC, Obama, and China.

F UTURE W ORK

enables users to explore more complex hypotheses that can enable
analysts to link potential cues between disparately collected sources.
While several visual analytics methods [11, 12, 13] have explored
frames in the context of comparing corpora of text and topical terms
within these text, our framework enables sentiment analysis and intervention modeling which can provide different insights than previous
work.
Our framework was evaluated through collaboration with domain
experts from the School of Communication and findings from their
exploration have prompted new questions and directions to explore.
While our examples focused on climate change and conflicts in Africa,
the tools developed are applicable for a variety of media sources. Furthermore, it is important to note that our intervention strategy can be
applied to any temporal variable, and, by utilizing multiple models,
we are able to strengthen the analysts’ confidence in the findings. This
was particularly evident in the exploration process. Anomaly detection methods, intervention models and others often have a large false
positive rate. By using an ensemble of models, one can begin defining uncertainty. Future work will focus on a combination of anomaly
models and intervention models as well as a weighted output for defining uncertainty in the detection, similar to our sentiment modeling approach. We also plan to explore a combination of sentiment analysis, frames and clustering for defining geo-political regions that share
common framing strategies. We believe that such methods can further
enable multisource data exploration and provide new cues to analysts
who are developing hypotheses and exploring the evolution of topics,
events and discourse both locally and globally.

In this paper, we have demonstrated a framework for event cueing that
enables the exploration of evolving media discourse. Our framework
focuses on both the spatial and temporal distribution of frames, and
allows experts to quickly explore spatial trends in the underlying discourse. By linking multisource data for exploration, our framework

ACKNOWLEDGEMENT
Some of the material presented here was sponsored by Department of
Defense and is approved for public release, case number 15-365 and
upon work supported by the NSF under Grant No. 1350573.

5

C ONCLUSIONS

AND

R EFERENCES
[1] Armed conflict location & event data project. http://http://www.
acleddata.com/. Accessed: 2015-03-28.
[2] Data science toolkit. http://www.datasciencetoolkit.org.
Accessed: 2015-03-18.
[3] W. Aigner, S. Miksch, H. Schumann, and C. Tominski. Visualization of
time-oriented data. Springer Science & Business Media, 2011.
[4] R. D. Benford and D. A. Snow. Framing processes and social movements:
An overview and assessment. Annual Review of Sociology, pages 611–
639, 2000.
[5] M. Bogl, W. Aigner, P. Filzmoser, T. Lammarsch, S. Miksch, and A. Rind.
Visual analytics for model selection in time series analysis. IEEE Transactions on Visualization and Computer Graphics, 19(12):2237–2246,
2013.
[6] H. Bosch, D. Thom, M. Worner, S. Koch, E. Puttmann, D. Jackle, and
T. Ertl. Scatterblogs: Geo-spatial document analysis. In Proceedings of
IEEE Conference on Visual Analytics Science and Technology (VAST),
pages 309–310. IEEE, 2011.
[7] J. Chae, D. Thom, H. Bosch, Y. Jang, R. Maciejewski, D. S. Ebert, and
T. Ertl. Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition. In Proceedings of IEEE Conference on Visual Analytics Science and Technology
(VAST), pages 143–152. IEEE, 2012.
[8] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey.
ACM Computing Surveys (CSUR), 41(3):15, 2009.
[9] D. Chong and J. N. Druckman. Framing theory. Annual Review of Political Science, 10:103–126, 2007.
[10] N. F. da Silva, E. R. Hruschka, and E. R. Hruschka. Tweet sentiment
analysis with classifier ensembles. Decision Support Systems, 66:170–
179, 2014.
[11] N. Diakopoulos, D. Elgesem, A. Salway, A. Zhang, and K. Hofland.
Compare clouds: Visualizing text corpora to compare media frames. In
Proceedings of IUI Workshop on Visual Text Analytics, 2015.
[12] N. Diakopoulos, A. Zhang, D. Elgesem, and A. Salway. Identifying and
analyzing moral evaluation frames in climate change blog discourse. In
Proceedings of International Conference on Weblogs and Social Media
(ICWSM), 2014.
[13] N. Diakopoulos, A. X. Zhang, and A. Salway. Visual analytics of media frames in online news and blogs. In Proceedings of IEEE InfoVis
Workshop on Text Visualization, 2013.
[14] W. Dou, X. Wang, D. Skau, W. Ribarsky, and M. X. Zhou. Leadline:
Interactive visual analysis of text data through event identification and
exploration. In IEEE Conference on Visual Analytics Science and Technology (VAST), pages 93–102. IEEE, 2012.
[15] W. Dou, L. Yu, X. Wang, Z. Ma, and W. Ribarsky. Hierarchicaltopics:
Visually exploring large text collections using topic hierarchies. IEEE
Transactions on Visualization and Computer Graphics, 19(12):2002–
2011, Dec 2013.
[16] R. M. Entman. Framing: Toward clarification of a fractured paradigm.
Journal of Communication, 43(4):51–58, 1993.
[17] P. Federico, S. Hoffmann, A. Rind, W. Aigner, and S. Miksch. Qualizon
graphs: Space-efficient time-series visualization with qualitative abstractions. In Proceedings of the 2014 International Working Conference on
Advanced Visual Interfaces, pages 273–280. ACM, 2014.
[18] D. Fisher, A. Hoff, G. Robertson, and M. Hurst. Narratives: A visualization to track narrative events as they develop. In IEEE Symposium on
Visual Analytics Science and Technology, pages 115–122. IEEE, 2008.
[19] T. Gao, J. R. Hullman, E. Adar, B. Hecht, and N. Diakopoulos.
Newsviews: An automated pipeline for creating custom geovisualizations
for news. In Proceedings of the 32nd annual ACM conference on Human
Factors in Computing Systems, pages 3005–3014. ACM, 2014.
[20] D. Gotz and H. Stavropoulos. Decisionflow: Visual analytics for highdimensional temporal event sequence data. IEEE Transactions on Visualization and Computer Graphics, 20(12):1783–1792, 2014.
[21] J. Hullman, N. Diakopoulos, and E. Adar. Contextifier: Automatic generation of annotated stock visualizations. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’13, pages
2707–2716, New York, NY, USA, 2013. ACM.
[22] S. Ko, S. Afzal, S. Walton, Y. Yang, J. Chae, A. Malik, Y. Jang, M. Chen,
and D. Ebert. Analyzing high-dimensional multivariate network links
with integrated anomaly detection, highlighting and exploration. Proceedings of IEEE Conference on Visual Analytics Science and Technol-

ogy, pages 83–92, 2014.
[23] M. Krstajic, E. Bertini, and D. A. Keim. Cloudlines: Compact display of
event episodes in multiple time-series. IEEE Transactions on Visualization and Computer Graphics, 17(12):2432–2439, 2011.
[24] M. Krstajić, M. Najm-Araghi, F. Mansmann, and D. A. Keim. Story
tracker: Incremental visual text analytics of news story development. Information Visualization, 12(3-4):308–323, 2013.
[25] S. Liu, X. Wang, J. Chen, J. Zhu, and B. Guo. Topicpanorama: A full
picture of relevant topics. In Proceedings of IEEE Conference on Visual
Analytics Science and Technology, pages 183–192. IEEE, 2014.
[26] Y. Lu, X. Hu, F. Wang, S. Kumar, H. Liu, and R. Maciejewski. Visualizing social media sentiment in disaster scenarios. In Proceedings of the
24th international conference on World Wide Web companion. International World Wide Web Conferences Steering Committee, 2015.
[27] D. Luo, J. Yang, M. Krstajic, W. Ribarsky, and D. Keim. Eventriver: Visually exploring text collections with temporal references. IEEE Transactions on Visualization and Computer Graphics, 18(1):93–105, 2012.
[28] A. Malik, R. Maciejewski, N. Elmqvist, Y. Jang, D. S. Ebert, and
W. Huang. A correlative analysis process in a visual analytics environment. In Proceedings of IEEE Conference on Visual Analytics Science
and Technology, pages 33–42. IEEE, 2012.
[29] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and
D. McClosky. The Stanford CoreNLP natural language processing
toolkit. In Proceedings of 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations, pages 55–60, 2014.
[30] M. Monroe, K. Wongsuphasawat, C. Plaisant, B. Shneiderman, J. Millstein, and S. Gold. Exploring point and interval event patterns: Display
methods and interactive visual query. University of Maryland Technical
Report, 2012.
[31] D. C. Montgomery, C. L. Jennings, and M. Kulahci. Introduction to Time
Series Analysis and Forecasting. Hoboken, NJ: John Wiley & Sons, 2008.
[32] J. Neter, M. H. Kutner, C. J. Nachtsheim, and W. Wasserman. Applied
linear statistical models, 5th edition, volume 4. Irwin Chicago, 1996.
[33] D. Oelke, M. Hao, C. Rohrdantz, D. Keim, U. Dayal, L. Haug, and
H. Janetzko. Visual opinion analysis of customer feedback data. In IEEE
Symposium on Visual Analytics Science and Technology, pages 187–194,
2009.
[34] J. OLoughlin, A. M. Linke, and F. D. Witmer. Effects of temperature and precipitation variability on the risk of violence in sub-saharan
africa, 1980–2012. Proceedings of the National Academy of Sciences,
111(47):16712–16717, 2014.
[35] D. A. Scheufele. Framing as a theory of media effects. Journal of Communication, 49(1):103–122, 1999.
[36] D. Thom, H. Bosch, S. Koch, M. Worner, and T. Ertl. Spatiotemporal anomaly detection through visual analysis of geolocated twitter messages. In Pacific Visualization Symposium, pages 41–48. IEEE, 2012.
[37] J. J. Van Wijk and E. R. Van Selow. Cluster and calendar based visualization of time series data. In IEEE Symposium on Information Visualization,
pages 4–9. IEEE, 1999.
[38] F. Wanner, C. Rohrdantz, F. Mansmann, D. Oelke, and D. A. Keim. Visual sentiment analysis of RSS news feeds featuring the US presidential
election in 2008. In Workshop on Visual Interfaces to the Social and the
Semantic Web, 2009.
[39] F. Wanner, A. Stoffel, D. Jäckle, B. Kwon, A. Weiler, D. Keim, K. E.
Isaacs, A. Giménez, I. Jusufi, T. Gamblin, et al. State-of-the-art report
of visual analysis for event detection in text data streams. In Computer
Graphics Forum, volume 33, 2014.
[40] B. L. Welch. The generalization of ‘student’s’ problem when several
different population variances are involved. Biometrika, pages 28–35,
1947.
[41] K. Wongsuphasawat and D. Gotz. Exploring flow, factors, and outcomes
of temporal event sequences with the outflow visualization. IEEE Transactions on Visualization and Computer Graphics, 18(12):2659–2668,
2012.
[42] K. Wongsuphasawat, J. A. Guerra Gómez, C. Plaisant, T. D. Wang,
M. Taieb-Maimon, and B. Shneiderman. Lifeflow: Visualizing an
overview of event sequences. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, pages 1747–1756. ACM, 2011.
[43] C. Xie, W. Chen, X. Huang, Y. Hu, S. Barlowe, and J. Yang. VAET: A visual analytics approach for e-transactions time-series. IEEE Transactions
on Visualization and Computer Graphics, 20(12):1743–1752, 2014.

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Story Detection Using Generalized Concepts and
Relations
∗ School

Betul Ceran∗ , Nitesh Kedia∗ , Steven R. Corman† and Hasan Davulcu∗
of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809
Email: {betul, nitesh.kedia, hdavulcu}@asu.edu
† Hugh Downs School of Human Communication, Arizona State University, Tempe, AZ 85287-1205
Email: steve.corman@asu.edu

victims) as different objects, potentially missing common
narrative patterns. We address this problem by discovering
“contextual synonyms” [1] which are verb and noun phrases
that occur in similar contexts. After revealing contextual
similarity, we generalize such references to a common node
in a semantic network.
We developed an unsupervised and domain-independent
framework which extracts high-level information from text
as relationships and concepts forming a semantic network.
It first uses a semantic role labeler to obtain ground facts as
semantic triplets from text, and then proceeds to generalize
them through a bottom-up agglomerative clustering algorithm. Semantic role labeling, i.e. shallow semantic parsing,
is a task in natural language processing which recognizes
the predicate or verb phrases in a sentence along with its
semantic arguments and classifies them into their specific
roles as subjects and objects. For example, we would like
to merge extracted triplets such as hmujahidin→kill→kafiri
and hISIS →demolish→shrinesi into high level generalized
concepts and relations, such as h{ISIS, mujahidin}→{kill,
demolish}→{kafir, shrines}i by discovering contextual synonyms such as {ISIS, mujahidin}, {kafir, shrines} and {kill,
demolish}. Note that contextual synonyms are not synonyms
in the traditional dictionary sense, but they are phrases that
may occur in similar semantic roles and associated with
similar contexts.

Abstract—A major challenge in automated text analysis is that
different words are used for related concepts. Analyzing text
at the surface level would treat related concepts (i.e. actors,
actions, targets, and victims) as different objects, potentially
missing common narrative patterns. Shallow parsers reveal
semantic roles of words leading to subject-verb-object triplets.
We developed a novel algorithm to extract information from
triplets by clustering them into generalized concepts by utilizing syntactic criteria based on common contexts and semantic
corpus-based statistical criteria based on “contextual synonyms”. We show that generalized concepts representation of
text (1) overcomes surface level differences (which arise when
different keywords are used for related concepts) without drift,
(2) leads to a higher-level semantic network representation of
related stories, and (3) when used as features, they yield a
significant 36% boost in performance for the story detection
task.

1. Introduction
Extremist groups use stories to frame contemporary
events and persuade audiences to adopt their extremist ideology. Foreign policy of most countries in the twenty first
century is centrally influenced by perceptions, attitudes and
beliefs of societies. Therefore, it’s of critical importance
to fully understand the means by which extremist groups
leverage cultural narratives in support of their ideological
agenda. Understanding the structure of extremist discourse
will provide better intelligence on what kinds of narrative
and persuasive appeals they are making, allowing both better
detection of trends and better knowledge of which themes
might best be countered and how this might be accomplished. The research presented in this paper mainly focuses
on extracting high-level relations and concepts which are
then utilized for detecting stories and themes embedded in
longer messages of extremist discourse.
A major challenge facing automated discourse analysis
is that word usage can differ between two texts even though
they are talking about the same thing. For example, violent
extremists may use words such as “brothers”, “mujahidin”,
“mujahedeen” and even “lions of Islam” to refer to the
same group of people. Analyzing text at the surface level
would treat related concepts (i.e. actors, actions, targets, and

ASONAM '15, August 25-28, 2015, Paris, France
© 2015 ACM. ISBN 978-1-4503-3854-7/15/08 $15.00
DOI: http://dx.doi.org/10.1145/2808797.2809312

mujahidin → kill → kafir


{ISIS, mujahidin}




↓
and
{kill, demolish}


↓


ISIS → demolish → shrines
{kafir, shrines}
Triplets extracted with semantic role labeling are noisy
and sparse. We develop a hierarchical bottom-up merging
algorithm that generalizes triplets into meaningful high level
relationships. We achieve this by employing syntactic and
semantic corpus-based criteria. Syntactic criteria are developed to merge a pair of subjects-verbs-objects only if they
share common context related to their different arguments
(i.e. a pair of different subjects are merged only if they cooccur with an identical verbs-objects context). The details
of the syntactic criteria are presented in Section 5.1. Furthermore, a corpus-based semantic criterion is developed

942

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

story detection task. We aim to to develop a story classifier
that can discriminate between stories and non-stories. A
story is defined as an actor(s) taking action(s) that culminates in a resolution(s), e.g. “Mujahedeen Imarah Islam
Afghanistan attacked a military base in Hisarak district
of Nangarhar province with heavy weapons on Tuesday.
Reports indicate about 22 mortars landed on the base and
causing a fatal loss enemy side.” A non-story paragraph is
one in which there is no explicit resolution but hypothetical
ones, e.g. “Praised be God. We praise Him and seek His
help and forgiveness. God save us from our evils and bad
deeds. Whoever is guided by God, no one can mislead him,
and whoever deviates no one can guide him.”
We use a corpus of 39, 642 paragraphs where 9, 058
paragraphs coded as stories, and 30, 584 paragraphs coded
as non-stories by domain experts. We experiment with (i)
standard keyword-based features, (ii) triplet-based features
which generate sets of subjects and sets of objects associated
with distinct verbs as features [4], and (iii) generalized
concept/relation based features developed in this paper. Previously in [4], we obtained a precision of 73%, recall of
56% and F-measure of 63% for the detection of minority
class (i.e. stories) by using triplet-based features, which
provided a 161% boost in recall, and an overall 90% boost
in F-measure over keyword-based features. In this paper,
we show that when we utilize generalized concepts/relations
extracted from the entire corpus of stories and non-stories as
features, we obtain new highs in story detection accuracies
as 86% precision, 82% recall and 85% F-measure. Generalized concepts/relations as features yield a 50% boost
in recall at higher precision, and an overall 36% boost
in story detection accuracy over verb-based triplet features
developed earlier.
The contributions of this paper are: (i) a generalized concept/relationship representation of text that overcomes surface level differences (which arise when different keywords
are used for related concepts) without drift (ii) a higher-level
semantic network representation of related stories, and (iii)
a 36% boost in the challenging automated story detection
[5] task.

Figure 1. A sample semantic network learned from stories

for subjects, verbs and objects based on their shared verbobject, subject-object and subject-verb contexts correspondingly. The details of the semantic criterion are presented in
Section 5.2. A hierarchical bottom-up merging algorithm,
similar to the one employed in [2], allows information to
propagate between clusters of relations and clusters of objects and subjects as they are created. Each cluster represents
a high-level relation or concept. A concept cluster can be
viewed as a node in a graph, and a relation cluster can be
viewed as a link between the concept clusters that it relates.
Our proposed algorithm utilizes both syntactic and semantic corpus-based merging criteria. A pair of hSubject,
Verb, Objecti triplets is merged only if (i) they share a
common context among their corresponding terms (i.e. syntactic criteria) and (ii) they satisfy corpus-based support
and similarity measure thresholds (i.e. semantic criteria).
A corpus-based measure of “contextual synonymy” will be
defined based on their shared contexts of subjects, verbs and
objects. We observed that this combination of criteria helps
to generalize triplets into meaningful high-level concepts
without drift. For example, Table 1 shows top ten contextual
synonyms identified for three keywords selected from our
extremist discourse corpus.
Generalized concept and relation clusters define a semantic network [3]. Collections of co-related contextual
synonyms can be used to construct meta-nodes and links
in a network describing the semantic space of the underlying texts. Components of the graph reveal networks
of generalized concepts expressed as different groups of
actors (subjects) performing various sets of actions (verbs)
on different groups of targets/victims (objects). A sample
network extracted from stories that mention Afghanistan and
Iraq is shown in Figure 1. This technique contributes to the
detection of narratives used by extremist groups to convey
their ideology.
We evaluate the utility of generalized concepts by comparing their predictive accuracy when used as features in a

2. Related Work
Our paper has contributions in two distinct areas; unsupervised relation extraction and story detection. We present
related work in these two areas separately.

2.1. Unsupervised Relation Extraction
Unsupervised learning of concepts and relations has become very popular in the last decade. One of the pioneering
studies in the field, by Hasegawa et al. [6], clusters pairs of
named entities according to the similarity of context words
(predicates) in between. Each cluster represents a relation,
and a pair of objects can appear in at most one cluster
(relation). Our framework does not depend on the use of
a Named Entity Recognition (NER) system and it allows
subject and objects to appear in more than one relation.

943

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Bank et al. [7] build soft clusters of named entities however
their system require an external knowledge-base/ontology of
relations to operate.
Kok and Domingos presented a similar framework to
ours in their 2008 paper [2], which extracts concepts and
relations together from ground facts also learning a semantic
network. They use a purely statistical model based on second
order Markov Logic and report performance in comparison
with other clustering algorithms based on a manually created
gold-standard. Our evaluation strategy compares the efficacy
of concepts/relations as features with other feature sets on
story detection task.
Recently, the focus of unsupervised information extraction has moved on to large web data sets creating the need
for more scalable approaches. Kang et al. [1] deals with
this problem using a parallel version of tensor decomposition. They learn contextual synonyms and generalized concepts/relations simultaneously, however they do not present
any formal evaluation of their concepts/relations.
Another problem of dealing with web-scale discovery
is polysemy and synonymy of verbs. Polysemy becomes
a problem when the two occurrences of the same word
which have different meanings are placed into the same
cluster. Min et al. [8] addresses this problem by incorporating various semantic resources such as hyponymy relations,
coordination patterns, and HTML structures. They observe
that the best performance is achieved when various resources
are combined together. We address word sense disambiguation by incorporating features from words’ context. The
contextual information flow via alternating merging of nouns
and verbs handles the problems due to polysemy.

2)

3)

4)

5)
6)

7)
8)

Paragraphs are loaded into a SRL component. First,
we apply co-reference resolution. Then, we use a
shallow NLP parser and a post-processing step on
the parse-tree in order to obtain the semantic role
labels for hSubject, V erb, Objecti triplets found in
sentences. (See section 3.1).
Using the triplets, we create three separate pairwise
contextual similarity matrices for subjects, verbs
and objects based on their co-occurrences with
verb-object, subject-object, and subject-verb pairs
respectively. (See section 4).
Triplets and contextual similarity matrices are used
as inputs to our clustering engine, which selectively
merges and grows combined clusters of related
subjects, verbs and objects. (See section 5).
Step 4 yields a number of concept (i.e. subject/object) clusters linked by relation (i.e. verb)
clusters. (See section 5).
We further experiment with expanding these concepts and generalized relations with word-sense disambiguated dictionary look-ups in WordNet [12].
(See section 6.2).
We further expand these concepts and relations
with word-sense disambiguated dictionary lookups. (See section 6.2).
Both original and expanded concepts/relations are
tested as features for the story/non-story classification task using ten-fold cross validation. (See
section 6.4).

3.1. Semantic Role Labeler
2.2. Story Detection
We study the problem of predicting whether or not a
given paragraph tells a story. A story can be defined as
“a sequence of events leading to a resolution or projected
resolution”. We perform supervised learning using a training
set of stories and non-stories annotated by domain experts.
Gordon et al. has published related work about story detection in conversational speech [9] and weblogs [10]. They
use a confidence-weighted linear classifier with a variety of
lexical features to classify weblog posts in the ICWSM 2009
Spinn3r Dataset and obtained the best performance [11]
using unigram features with precision 66%, recall = 48%,
F-score = 55%.

We use the Stanford Deterministic Coreference Resolution System [13], [14], [15], [16] as a pre-processing step.
Next, each paragraph is processed by ClearNLP shallow
parser [17], which assigns a semantic role label to each
word in a sentence. There are more than 40 possible labels1
provided by ClearNLP. Currently, we are only interested
in subjects, predicates and objects. In the final step, we
apply post-processing on the output of ClearNLP to handle
complex sentences with multiple verbs or some considerations for active or passive voice in order to extract related
subject-verb-object triplets expressed in the sentence. Our
framework can be adapted for languages other than English,
provided that a semantic role labeler exists for that language.

3. System Architecture

4. Contextual Synonyms

The main components of our system architecture are
shown in Figure 2. The numbers on the top left corner of
each box represent the order in which these processes are
executed. Each process is briefly described below, while the
details are presented in following sections.

We observe that a meaningful measure of pairwise similarity for subjects, verbs and objects can be obtained based
on their shared verb-object, subject-object and subject-verb
contexts, respectively. Therefore, we adapted the standard
bag-of-words approach [18] to be used with triplets rather
than regular text. For example, the similarity between a

1)

Paragraphs in our dataset are annotated by human
experts as Story and Non-Story. We treat each
paragraph as a single data item to be classified.

1

944

https://github.com/clir/clearnlp-guidelines/blob/master/md/dependency/
dependency guidelines.md

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

1:
2:
3:
4:

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

F IND CONCEPTS W / UNIQUE PAIRS(T , C 0 )
X , Y, Z ← ∅
for all hsi , vj , ok i ∈ T do
Find and add unique pairs to:
X ← X ∪ {hsi , vj i}
Y ← Y ∪ {hvj , ok i}
Z ← Z ∪ {hsi , ok i}
end for
for all hsi , vj i ∈ X and hsi , vj , ok i ∈ T do
C 0 ← C 0 ∪ {hsi , vj , Oi} where ok ∈ O.
end for
for all hvj , ok i ∈ Y and hsi , vj , ok i ∈ T do
C 0 ← C 0 ∪ {hS, vj , ok i} where si ∈ S .
end for
for all hsi , ok i ∈ Z and hsi , vj , ok i ∈ T do
C 0 ← C 0 ∪ {hsi , V, ok i} where vj ∈ V .
end for
end
Figure 3. Algorithm: Find concepts with unique pairs

Let S, V and O be the set of all unique subjects, verbs
and objects in our data set, respectively. And let T be the
set of all hs, v, oi extracted triplets from our corpus, where
s ∈ S , v ∈ V , o ∈ O denote a single subject, verb
and object respectively. We calculate pairwise contextual
similarity matrices (SS ) for subjects, (SV ) for verbs and
(SO ) for objects using the algorithms described in Figures 3
and 4. Throughout the rest of this paper, we refer to both
noun and verb clusters as ‘concepts’ for simplicity.
Initially, we create concepts comprised of distinct pairs
of subjects, verbs or objects with common context. We will
name this initial set of concepts C 0 in order to avoid confusion with the resulting set of concepts. Set C 0 is composed of
concepts c, each of which has a set of subjects (S ), verbs (V )
and objects (O), which co-occur with unique hverb, objecti,
hsubject,objecti and hsubject,verbi pairs respectively. The
pseudo-code given in Figure 3 describes this procedure.
In the first for-loop (lines 3–5, we iterate over all the
hs, v, oi triplets and create a list of unique hsubject,verbi,
hverb,objecti and hsubject,objecti pairs. In the subsequent
three for-loops, we grow our concept set at each iteration
by adding a unique pair along with a set of all co-occurring
words. Lines 6–8 perform this operation for hsubject,verbi
pairs, lines 9–11 for hverb,objecti pairs and lines 12–14 for
hsubject,objecti pairs.
After producing concepts with unique pairs, we proceed
to calculate pairwise contextual similarity for subjects, verbs
and objects. Let ns = |S|, nv = |V| and no = |O|
be the number of all unique subjects, verbs and objects
in our corpus, respectively. We create similarity matrices
SS ∈ Rns ×ns for subjects, SV ∈ Rnv ×nv for verbs, and
SO ∈ Rno ×no for objects. The algorithm in Figure 4 is
used to fill in the similarity matrices. The similarity between
a pair of words is defined as the number of common cooccurring unique contexts, i.e. if any of the two subjects,
verbs or objects appear with the same verb-object, subject-

Figure 2. System Architecture
TABLE 1. T OP TEN CONTEXTUAL SYNONYMS FOR mujahedeen, attack
AND base
mujahedeen

attack

base

mujahidin

storm

area

group

hit

house

soldier

seize

area

force

loot

home

lion

raid

station

hero

shoot

center

fighter

ambush

checkpoint

mujahid

assassinate

headquarters

brigade

bomb

land

mujahedeen

capture

location

detachment

disrupt

region

pair of subjects is determined by the frequency of their cooccurrences with the same verb-object pairs. In our preliminary experiments, we applied various clustering algorithms
comparing different similarity measures such as euclidean
and cosine however the contextual similarity measure defined in Figure 4 provides the most meaningful results. For
example, in Table 1, lion is indeed among the most similar
words for mujahedeen based on the contextual similarity
measure, whereas none of the other standard similarity
measures are able to retrieve this keyword.

945

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

C ALCULATE CONTEXTUAL SIMILARITY(C 0 )
SS , SV , SO ← 0
for all c ∈ C 0 do
if c = hS, v, oi then
SS (i, j) ← SS (i, j) + 1, ∀si , sj ∈ S .
else if c = hs, V, oi then
SV (i, j) ← SV (i, j) + 1, ∀vi , vj ∈ V .
else if c = hs, v, Oi then
SO (i, j) ← SO (i, j) + 1, ∀oi , oj ∈ O.
end if
end for
end

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

Figure 4. Algorithm: Calculate contextual similarity

object or subject-verb pair respectively, then we increase
the similarity count between those two words by one. In
Figure 4, lines 4–5 calculate pairwise similarities between
subjects, lines 6–7 for verbs and lines 8–9 for objects.

C LUSTER C ONCEPTS(T , SS , SV , SO , C 0 )
C ← C0
while f lag = 1 do
f lag ← 0
for all c ∈ C 0 do
Find matching concepts M using Syntactic Criteria
if |M| ≥ 1 then
f lag ← 1
for all m ∈ M do
{c} ← {c} ∪ {m}
Prune c using Semantic Criteria.
C ← C ∪ {c}
end for
end if
end for
end while
end
Figure 5. Bottom-Up Agglomerative Clustering Algorithm

to c2 ’s subject set, we also require that the intersection of
c1 and c2 ’s verb and object sets, {v1 } and {o1 }, should be
non-empty as well. Since these conditions are satisfied in
this case, we can merge c1 and c2 into the same concept
provided they satisfy the semantic criteria discussed in the
next section.
On the other hand, let us consider c1 = h{s1 , s2 }, v1 , o1 i
and c2 = hs1 , {v1 , v2 }, o2 i. If we merge these concepts,
the new concept will be c3 = h{s1 , s2 }, {v1 , v2 }, {o1 , o2 }i.
Since c3 adds a new object, o2 , to c1 , we require that the
intersection of c1 and c2 ’s subject and verb sets, {s1 } and
{v1 }, should be non-empty, which is the case. c3 would
also add a new verb, v2 , to c1 , hence we require that the
intersection of c1 and c2 ’s subject and object sets should
be non-empty as well, which is not the case. There is a
common subject but objects are totally distinct. Therefore
we should not merge these concepts into the same one since
there is not enough common context to justify the merged
concept. We express these conditions in a more formal way,
as follows.
Let C1 = hS1 , V1 , O1 i and C2 = hS2 , V2 , O2 i be two
concepts. We merge C1 and C2 if they meet all of the
following conditions:

5. Concept and Relation Clustering
We follow a bottom-up agglomerative merging approach
in order to populate our noun and verb clusters. The pseudocode for the algorithm is as shown in Figure 5. We start
with the initial concept set, C 0 , that we created in Figure 3
and iteratively expand each element. First, each element
of C 0 is compared with the rest in order to create a set
of candidates for merging based on the syntactic criteria
(lines 5–6) described in Section 6.1. Next, we process each
candidate and eliminate the words which fail the semantic
criteria (lines 9–10) described in Section 6.2. We grow our
candidate concepts by adding the elements which pass both
tests (line 12). The main while-loop, beginning at line 3,
continues to iterate until there are no more candidates suitable for merging. We explain the details of these syntactic
and semantic criteria in the following two sections.

5.1. Syntactic Criteria
One of the major challenges in obtaining information
via generalization is to maintain meaningful concepts as
they grow. We address this problem by merging concepts
only if they have a common context in all three semantic arguments (i.e. subject, verb, object). Given a generalized concept, h{s1 , s2 , ...}, {v1 , v2 , ...}, {o1 , o2 , ...}i ∈ C ,
we maintain that all subjects (si ), verbs (vj ) and objects
(ok ) are “ contextually synonymous” among themselves and
can be used interchangeably to generate meaningful triplets.
Let c1 = h{s1 , s2 }, v1 , o1 i and c2 = hs1 , v1 , {o1 , o2 }i be
two concepts with unique pairs, i.e. c1 , c2 ∈ C 0 . Consider
merging these concepts into a more generalized concept
c3 = h{s1 , s2 }, v1 , {o1 , o2 }i. Since c3 adds a new object,
o2 , to c1 , we require that c1 and c2 have a common context
in order to justify the merge, i.e. the intersection of c1 and
c2 ’s subject and verb sets, {s1 } and {v1 }, should be nonempty. Similarly, since we are adding a new subject, s2 ,

•
•
•

S1 6= S2 ⇒ {V1 ∩ V2 6= ∅ and O1 ∩ O2 =
6 ∅}
V1 6= V2 ⇒ {S1 ∩ S2 6= ∅ and O1 ∩ O2 =
6 ∅}
O1 6= O2 ⇒ {S1 ∩ S2 6= ∅ and V1 ∩ V2 =
6 ∅}

5.2. Semantic Criteria
While the syntactic criteria ensure inter-relatedness of
distinct members of concepts to their contexts, we also
utilize a secondary measure to establish intra-relatedness
between the distinct members of concepts in each argument
position. We utilize the contextual similarity measure (defined in Figure 4) that relates subjects, verbs, and objects
among themselves. The semantic test requires that only the

946

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

6.3. Feature Matrix Generation

most similar candidate keywords can be added to a concept.
We use these criteria to grow the concepts without drift. We
formally present semantic criteria as follows.
Let C1 = hS1 , V1 , O1 i and C2 = hS2 , V2 , O2 i be two
concepts which passes the syntactic criteria and let C3 be
the new concept after merging. Semantic criteria are applied
as follows:
•
•

•

•

We report the results of story detection task using five
different feature sets: (i) keywords, (ii) verb-based features
extracted from triplets [4], (iii) concepts-based features
(Tier 1) developed in this paper, (iv) concepts expanded
with contextual similarity index (Tier 1 + Similarity) and
(v) concepts expanded with WordNet (Tier 1 + WordNet).

Define Sint = S1 ∩S2 , Vint = V1 ∩V2 , Oint = O1 ∩O2 .
Define
Sdiff = (S1 \ S2 ) ∪ (S2 \ S1 )
Vdiff = (V1 \ V2 ) ∪ (V2 \ V1 )
Odiff = (O1 \ O2 ) ∪ (O2 \ O1 )

6.3.1. Verb-based Features. In our previous paper [4], we
followed a standard verb-based approach to extract simple
subject, object and preposition clauses associated with verbs
found in story and non-story paragraphs. For each verb (V)
mentioned in a story (S), and non-story (NS), we generated
following set-valued features by using the training data:

∗
∗
∗
Define Sint
, Vint
and Oint
to be the sets composed
of the closest contextual synonyms of all words in
Sint , Vint and Oint , respectively. In this step, we use
the contextual similarity metric from the algorithm
presented in Figure 4.
Initially, C3 contains only the intersections of C1 and
C2 , i.e. C3 = hSint , Vint , Oint i . We grow C3 by adding
words from the difference sets of C1 and C2 only if
they are among the closest contextual synonyms of
the words in the intersections. Formally,

•
•

Argument list for S.V.Subjects, S.V.Objects,
S.V.Prepositions for each verb V and story S.
Argument list for NS.V.Subjects, NS.V.Objects,
NS.V.Prepositions for each verb V and non-story
NS.

For each test paragraph P, for each verb V in P, we
extracted its typed argument lists P.V.Subjects, P.V.Objects
and P.V.Prepositions. Then, we matched them to the argument lists of the same verb V. A match succeeds if the
overlap between a feature’s argument list (e.g. S.V.Subjects,
or NS.V.Subjects) covers the majority of the test paragraph’s
corresponding verb argument list (e.g. P.V.Subjects).

∗
C3 = h (Sdiff ∩ Sint
) ∪ (S1 ∩ S2 ),
∗
(Vdiff ∩ Vint ) ∪ (V1 ∩ V2 ),
∗
(Odiff ∩ Oint
) ∪ (O1 ∩ O2 ) i.

6. Experimental Evaluation

6.3.2. Concepts-based Features. In this paper, first, we
generate the concepts for story and non-story paragraphs by
using the training data. Next, we process each test paragraph
P, and generate its semantic triplets, hs, v, oi. A binary
feature matrix is created by checking if any of the semantic
triplets of P matches a concept, hS, V, Oi, where S, V and
O are related sets of subjects, verbs and objects respectively.
A match succeeds if s ∈ S, v ∈ V and o ∈ O.

6.1. Data Set
We use a corpus of 39, 642 paragraphs where 9, 058 are
coded as stories and 30, 584 as non-stories by domain experts. Text is collected from websites, blogs and other news
sources that are known to be outlets of extremist groups
such as Al-Qaeda, ISIS or their followers who sympathize
with their cause and methods.

6.4. Cross Validation for Detecting Stories

6.2. Expansion of Concepts with Dictionary-based
Synonyms

We evaluate the quality of generalized concepts and
relations by their performance as features in story detection.
The goal is to improve the predictive accuracy of story/nonstory classifier through the use of these new features. We
experiment with several different supervised learning packages including SVM [19], decision trees [20] and SLEP [21]
concluding that SLEP outperforms others for this task. We
use the MATLAB implementation of SLEP package [22]
and obtained the best results using LogisticR model. Training and testing are performed using ten-fold cross validation
and repeated with random shuffling over multiple iterations.
The results are averaged over all iterations. We report the
predictive performance of SLEP classifier using various
feature sets for story and non-story categories in Tables 2
and 3.
The feature sets we used are keywords, verb-based features [4] (Triplets), concepts and relations (Tier 1), concepts/relations expanded with contextual similarity index

After the Bottom-Up Agglomerative Clustering procedure (in Figure 5) terminates, we obtain high-level concepts
and relations, which we refer to as ‘Tier 1’. In order to
expand the concepts further with keywords that are missing
in the training corpus, we experiment with adding sensedisambiguated WordNet [12] synonyms to ‘Tier 1’ obtaining
‘Tier 1 + WordNet’. Alternatively, we also utilize contextual similarity index to create ‘Tier 1 + Similarity’ as
follows. For each concept c = h{s1...m }, {v1...n }, {o1...l }i ∈
C, where m, n, l > 1, we create a set of candidates to merge
by picking the synonyms of each subject, (si , 1 ≤ i ≤ m),
verb (vj , 1 ≤ j ≤ n) and object (ok , 1 ≤ k ≤ l). Without
loss of generality, we add w, synonym of si to cluster c, only
if there is at least one triplet in our database, hs, v, oi ∈ T
such that w = s, v ∈ {v1...n } and o ∈ {o1...l }.

947

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

TABLE 2. P ERFORMANCE OF CLASSIFIER FOR S TORIES
Method

Precision

Recall

F-Measure

Keywords

0.81

0.20

0.33

Triplets

0.73

0.55

0.62

Tier 1

0.87

0.78

Tier 1 + Similarity

0.86

Tier 1 + WordNet

0.87

TABLE 3. P ERFORMANCE OF CLASSIFIER FOR N ON -S TORIES
Method

Precision

Recall

F-Measure

Keywords

0.90

0.98

0.94

Triplets

0.89

0.99

0.92

0.83

Tier 1

0.80

0.89

0.84

0.82

0.84

Tier 1 + Similarity

0.83

0.86

0.84

0.80

0.83

Tier 1 + WordNet

0.82

0.88

0.85

(Tier 1 + Similarity) and concepts/relations expanded with
WordNet (Tier 1 + WordNet). The feature sets produced
by the bottom-up agglomerative clustering algorithm outperform others in the story detection category. We gained
7% boost in precision, 50% boost in recall and 36% boost
in F-Measure over the best performance of keyword and
triplet features (see Table 2). We observe that high-level
concepts/relations have far more discriminative power compared to other features. A key reason is that they are able
to eliminate dependent features by generalization. There is
not a big difference in performance among the original and
expanded concept-based feature sets and we can clearly see
that WordNet expansion did not contribute to the performance of concepts expanded by contextual similarity. This
finding presents another strong point in favor of our framework since adding information from an external knowledgebase was not able to provide a boost.
In the non-story category (Table 3), concept-based features are lagging behind in performance. This may be due
to the structural diversity of non-story paragraphs since
there are several different sub categories among them [23].
Another observation is that concept-based features help
overcome the performance bias between story and non-story
categories due to the imbalance in the number of training
samples. Overall, concepts/relations deliver a 36% boost in
performance for story detection.

7. Conclusion

6.5. Sensitivity Analysis

References

We assess concept/relation based features against the
possibility of over-fitting since they are highly dependent
on the training corpus. We explore this issue by using the
regularization parameter, λ in SLEP’s optimization formulation. We can pin-point the optimal number of features
and avoid over-fitting by observing the performance of
the system as the value of λ changes. The plots given
in Figure 6 display the change in λ versus the number
of features (middle row) and the performance (precision,
recall and F-Measure) for story (top row) and non-story
(bottom row) categories. In both cases, we can observe that
there is a sharp drop in the number of features (12, 000
to 2, 000) around 10−5 ≤ λ ≤ 10−4 while the precision,
recall and F-Measure are preserved. The data cursor box in
the middle plot mark the point of optimal value for λ and
the corresponding number of features. Experimentally, we
identified the optimal number of features as 7, 563 which
prevents over-fitting of the model and preserves the gains in
performance.

[1]

U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos, “Gigatensor:
Scaling tensor analysis up by 100 times - algorithms and discoveries,”
in Proceedings of the 18th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, 2012, pp. 316–
324.

[2]

S. Kok and P. Domingos, “Extracting semantic networks from text via
relational clustering,” in Proceedings of the 2008 European Conference on Machine Learning and Knowledge Discovery in Databases
- Part I, 2008, pp. 624–639.

[3]

M. R. Quillian, “Semantic memory,” in Semantic Information Processing, 1968, pp. 227–270.

[4]

B. Ceran, R. Karad, A. Mandvekar, S. R. Corman, and H. Davulcu, “A
semantic triplet based story classifier,” 2012 IEEE/ACM International
Conference on Advances in Social Networks Analysis and Mining
(ASONAM 2012), vol. 0, pp. 573–580, 2012.

[5]

J. Allan, V. Lavrenko, and H. Jin, “First story detection in tdt is hard,”
in Proceedings of the Ninth International Conference on Information
and Knowledge Management, ser. CIKM ’00, 2000, pp. 374–381.

[6]

T. Hasegawa, S. Sekine, and R. Grishman, “Discovering relations
among named entities from large corpora,” in Proceedings of the
42Nd Annual Meeting on Association for Computational Linguistics,
ser. ACL ’04, 2004.

We presented an algorithm for discovering generalized
concept/relationship representation of a collection of related
documents that overcomes surface level differences which
arise when different keywords are used for related concepts.
This representation provides a 36% boost in the challenging
automated story detection task and a higher-level semantic
network representation of related stories. In future work, we
plan to gauge the utility of generalized concepts in document
clustering tasks. We plan to use a bi-clustering approach
which can point to subsets of stories and associated generalized concepts/relations as their themes. Since clustering is
unsupervised, we need to rely on domain expert knowledge
to evaluate the quality of the detected clusters and their
themes. We also plan to develop visualization tools for
exploring document collections and their clusterings through
their high-level semantic network representations.

Acknowledgment
This research was supported by an Office of Naval
Research grants N00014-09-1-0872 and N00014-14-1-0477
performed at Arizona State University. Some of the material
presented here was sponsored by Department of Defense and
is approved for public release, case number:15-467.

948

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Figure 6. Sensitivity Analysis

[7]

M. Banko and O. Etzioni, “Strategies for lifelong knowledge extraction from the web,” in Proceedings of the 4th International
Conference on Knowledge Capture, ser. K-CAP ’07, 2007, pp. 95–
102.

[15] H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Deterministic coreference resolution based on entitycentric, precision-ranked rules,” Comput. Linguist., vol. 39, no. 4, pp.
885–916, Dec. 2013.

[8]

B. Min, S. Shi, R. Grishman, and C.-Y. Lin, “Ensemble semantics
for large-scale unsupervised relation extraction,” in Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.
Association for Computational Linguistics, 2012, pp. 1027–1037.

[16] M. Recasens, M. C. de Marneffe, and C. Potts, “The life and death of
discourse entities: Identifying singleton mentions,” in Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
2013, pp. 627–633.

[9]

A. S. Gordon and K. Ganesan, “Automated story capture from conversational speech,” in K-CAP ’05: Proceedings of the 3rd international
conference on Knowledge capture, 2005, pp. 145–152.

[17] J. D. Choi, “Optimization of natural language processing components
for robustness and scalability,” Ph.D. dissertation, University of Colorado at Boulder, 2012.
[18] N. Ide and J. Véronis, “Introduction to the special issue on word sense
disambiguation: The state of the art,” Comput. Linguist., vol. 24, pp.
2–40, 1998.

[10] A. Gordon, Q. Cao, and R. Swanson, “Automated story capture from
internet weblogs,” in Proceedings of the 4th international conference
on Knowledge capture, 2007, pp. 167–168.

[19] C. Chang and C. Lin, “Libsvm: a library for support vector machines,”
ACM Transactions on Intelligent Systems and Technology (TIST),
vol. 2, no. 3, p. 27, 2011.

[11] A. Gordon and R. Swanson, “Identifying personal stories in millions
of weblog entries,” in Third International Conference on Weblogs and
Social Media, Data Challenge Workshop, 2009.

[20] (2013) Matlab statistics and machine learning toolbox. The
MathWorks Inc. [Online]. Available: http://www.mathworks.com/
help/stats/classification-trees-and-regression-trees-1.html

[12] (2010) About wordnet. Princeton University. [Online]. Available:
http://wordnet.princeton.edu

[21] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 547–556.

[13] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu,
D. Jurafsky, and C. Manning, “A multi-pass sieve for coreference
resolution,” in Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguistics, 2010, pp. 492–501.

[22] J. Liu, S. Ji, and J. Ye. (2009) Slep: Sparse learning with
efficient projections. Arizona State University. [Online]. Available:
http://www.public.asu.edu/∼jye02/Software/SLEP

[14] H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Stanfords multi-pass sieve coreference resolution system
at the conll-2011 shared task,” CoNLL 2011, p. 28, 2011.

[23] H. L. Halverson, J. R. Goodall and S. R. Corman, Master Narratives
of Islamist Extremism. New York: Palgrave Macmillan, 2011.

949

2016 IEEE Tenth International Conference on Semantic Computing

“Climate Change” Frames Detection and
Categorization Based on Generalized Concepts
Saud Alashri

Jiun-Yi Tsai

Sultan Alzahrani

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona
Email: salashri@asu.edu

Hugh Downs School of
Human Communication
Arizona State University
Tempe, Arizona
Email: jtsai8@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona
Email: ssalzahr@asu.edu

Steven R. Corman

Hasan Davulcu

Hugh Downs School of
Human Communication
Arizona State University
Tempe, Arizona
Email: steve.corman@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona
Email: hdavulcu@asu.edu

and metaphors). Increasingly, governments and international
communities are concerned about the security implications
of climate change as empirical research has documented that
climate change is linked to increased risk of violent conﬂict
[4]. For example, in May 2015, U.S. President Barack Obama
suggested that extreme weather is a threat to national security
and elevates the risk of global instability and conﬂict. Some
popular press adopted security threat frame to gain public attention. Therefore, systematic detection of news frames related
to climate change offers better understanding of stakeholders
and their competing perspectives.
Politicians have used framing on hotly debated issues to
shift public opinion, gain support and pursue their agenda.
A frame is the bundling of a component of oratory to urge
certain perceptions and to dishearten others [5]. Framing is
accomplished when a choice of words, expressions, subjects
and other logical gadgets support one understanding of an
arrangement of realities, and debilitate other interpretations.
One of those framed issues is climate change. Internet created
a public space for politicians and stakeholders to frame climate
change and related issues to push for their agenda. Online
tools such as blogsphere, microblogging and social media
streams have increased the availability of data on climate
change related debate and made it feasible for researchers to
analyze them.
Framing research requires qualitative analysis of a number
of texts by subject matter experts to identify and code a set
of frames. This is a time consuming process that does not
scale well. In order to address the scalability problem, machine
learning techniques can be utilized to detect and classify
frames. In this paper we propose a system for automatic
detection of frames in sentences in a climate change related
corpus, and map them to one of four expert identiﬁed frame

Abstract— The subliminal impact of framing of social,
political and environmental issues such as climate change
has been studied for long time in political science and
communications research. Media framing offers “interpretative
package” for average citizens on how to make sense of climate
change and its consequences to their livelihoods, how to deal
with its negative impacts, and which mitigation or adaptation
policies to support. A line of related work has used bag of
words and word-level features to detect frames automatically
in text. Such works face limitations since standard keyword
based features may not generalize well to accommodate surface
variations in text when different keywords are used for similar
concepts. In this paper, we develop a new type of textual
features that generalize (subject,verb,object) triplets extracted
from text, by clustering them into high-level concepts. We utilize
these concepts as features to detect frames in text. Our corpus
comprises more than 45,000 climate change related sentences.
Expert coders annotated those sentences as frame/non-frame and
framed sentences were mapped into one of four general frame
categories: solution, problem threat, cause, and motivation.
Compared to unigram and bigram based models, classiﬁcation
using our generalized concepts yielded better discriminating
features and a higher accuracy classiﬁer with a 12% boost (i.e.
from 74% to 83% in f-measure) for frame/no frame detection.
Keywords—Text mining, Frames Detection, Concepts, Big
Data, Climate Change, Natural Language Processing

I. I NTRODUCTION
Climate change has provoked heated debates on the global
political and media arenas. Media framing offers “interpretative package” for average citizens on how to make sense
of climate change and its consequences to their livelihoods,
how to deal with its negative impacts, and which mitigation
or adaptation policies to support [1], [2], [3]. News frames
encourage salient interpretation of debated issues through the
usage of rhetorical devices (e.g. words, repetitive phrases,
978-1-5090-0662-5/16 $31.00 © 2016 IEEE
DOI 10.1109/ICSC.2016.14

277

Fig. 1.

Example of merging two related concepts

[12].
Media representation of climate change plays a vital role
in shaping ongoing policy discourse, public perception and
attitudes. [14] suggests that prominent political actors frame
climate risk for their own purposes, and align frames with their
interests and perspectives through media feedback processes
of representing climate change risk. Studies have shown that
the lay people learn about climate change mainly through
consuming mainstream media news [15]. Consequently, [2]
argued news media framing can catalyze public engagement
and help trigger collective concern of climate change. Put
differently, media framing is a powerful tool to highlight
different aspects of the policy options, and promote speciﬁc
interpretations or evaluations that inﬂuence decision making
[16].
Existing typologies of climate change framing, focusing
on dichotomous categories, are limited by their inability to
link framing processes with movement interaction. We argue
that, in order to understand how the media reﬂect different
organizations interests in addressing climate change as a social
problem, it is necessary to supplement the social movement
focus on resource mobilization to framing processes of collective action problems. To do that, this study develops a nuanced
typology for studying climate change framing and its adequacy
for supporting a social movements that would be necessary to
overcome the collective action problem. Our typology provides
a holistic map to evaluate how climate change media framing
can enable appropriate social and policy actions that ultimately
can mitigate risks of social unrest. We apply this framework to
examine framing of climate change in media and social media
texts collected from the Niger Basin region over seven months
from August 2014 to February 2015, using a novel coding
technique to assess diagnostic, prognostic, and motivational
framing described by [17] as the keys to effective social
movements.

categories: solution, problem threat, cause, and motivation.
Our problem here can be described as a multi-level multi-class
classiﬁcation problem where we ﬁrst classify each sentence as
Frame or Non-Frame. Then, the Frame sentences are further
classiﬁed into one of four predeﬁned frame categories. In
particular, we show that generalized concepts and relations [6]
as features outperform classical textual features (e.g. uni-grams
and bi-grams) while detecting and categorizing Frame/NonFrame sentences. We experimented with SVM [7], Random
Forests [8] and sparse logistic regression [9] classiﬁers, and
identiﬁed sparse logistic regression as the best performing
classiﬁer for these tasks.
Generalized concepts approach extracts high-level information from text as relationships and concepts forming a
semantic network. It ﬁrst uses shallow semantic parser to generate POS tags to obtain semantic triplets (subject,verb,object)
from text. Next, it utilizes a bottom-up agglomerative clustering approach to merge and generalize those triplets into
concepts. In NLP, shallow parsing is the task of extracting the subjects, predicates or verb phrases, and objects.
Figure 1 shows how two related triplets could be merged
into a higher level generalized concept. In this ﬁgure, two
extracted triplets: action plan→build→sustainability and
policy →consolidate→sustainability are merged to form a
high level generalized concept and relationship as: {action
plan, policy}→{build, consolidate}→{sustainability} by discovering contextual synonyms such as {action plan, policy}
and {build, consolidate}. Here the deﬁnition of contextual
synonyms is not based on the one in the traditional dictionary.
Rather, they correspond to phrases that may occur in similar
semantic roles and associated with similar contexts. In Figure
1 the two triplets share the same object {sustainability} and
semantically similar verbs; hence, we can merge their subjects
{action plan,policy} as contextual synonyms.
II. R ELATED W ORK

B. Framing Research in Computer Science

A. Media Framing

[18] examined twitter stream on extracted frames and
pointed out a strong ties between frames collected from news
with the public opinions expressed in tweeter feeds. [19] went
further to distill agenda from news and link them to action.
Content analysis of frames in news is performed either by (1)
manual frame coding, that is done by trained coders, which is
costly as well as not scalable, or by (2) frame identiﬁcation
by using machine learning techniques that overcome human

Mainstream media serve as the main arena where international governments, social and political actors, scientists,
social movement organizations interact and make competing
claims about climate change issues [10]. Communication
surrounding climate change can inhibit or support science
and policy interactions, propagate consensus or disagreements
[11], and ultimately facilitate social change [12], [13], depending on how messages about climate change have been framed
278

Threat, Cause, Motivation}. Figure 2 shows our multi-level
multi-class classiﬁcation problem for a given sentence.
IV. M ETHODOLOGY
A. Overall System Model
The overall system consists of documents collected from
nearly 100 RSS feeds that are related to climate change in
the Niger Delta region. We also perform sentence splitting of
documents, identiﬁcation of key frames and their categories
by the coders, feature extraction (uni-grams, bi-grams, and
generalized concepts), identiﬁcation of discriminative features,
and a predictive model to detect and identify the frame
categories for sentences containing frame references.
B. Climate Change Corpus
Fig. 2.

Our climate change corpus is comprised of nearly 45, 054
sentences extracted from news and social media websites, that
are related to climate change topics in Niger Basin region
over a seven months period from August 2014 to February
2015. There are 16, 050 sentences coded as frame sentences
and 29, 004 as non-frame sentences by domain experts. Frame
sentences are further categorized into one of four categories:
Solution, Problem Threat, Cause, and Motivation.

Multi-level multi-class classiﬁcation

limitations by automatically detecting frames after training a
classiﬁer [20]. Many studies have addressed media framing
as a document classiﬁcation problem by building a learning
model to classify documents or paragraphs by utilizing different features. Aside from document level, [21], [22] examined
the classiﬁcation task at the sentence level and even at the
phrase level. Previous work on sentence level classiﬁcation
has focused on experimenting with different classiﬁers and
different features. [23] examined: bag of words, n-grams,
and topic models to classify news articles and map them to
a set of frames. Others, [24] employed POS-tags [25] and
named entities [26] as features to detect and classify frames.
Ceran et al [27] experimented with {subject,verb,object} based
features and benchmarked “ paragraph level” classiﬁer for
story detection against standard keyword based features, which
showed signiﬁcant improvement in classiﬁcation accuracy.
More advanced conceptual features engineering was developed
in [6] as they showed how generalized concepts performed
better in detecting stories in paragraphs. We utilize their generalized concepts as features to detect and categorize frames.
Our paper works on sentence level classiﬁcation compared to
their paragraph level. Also, our task is a multi-level multiclass classiﬁcation task where we ﬁrst examine if a sentence
contains a frame, and then we identify which one of four
frame categories it belongs to. Moreover, we developed tripleextraction techniques where we can extract more features
and incorporate a larger percentage of sentences into the
classiﬁcation model (i.e. 80% of sentences compared to 40%).

C. N-gram Features
We experimented with both uni-gram and bi-gram features.
We run a simple term frequency - inverse document frequency
(TF-IDF) [28] based technique on the entire corpus to generate
a large ranked list of, stopword eliminated, uni-grams and bigrams, and we experimented with them separately as features
in our classiﬁcation models.
D. Generalized Concepts Features
In [6], they extracted concepts from paragraphs where only
40% of the paragraphs generated concepts. In this paper, since
we are working on sentence level, we improved the concept
extraction approach, by extracting more triplets by utilizing
a larger number of triplet extractors and pre-processing their
output to include about 80% of the sentences in our experimental evaluations.
1) Triplets
Extraction:
In
order
to
extract
Subject,Verb,Object triplets, ﬁrst we run a pronoun resolver
[29], [30], [31], [32]. Since triplets extraction is an ongoing
research topic in NLP, we proceeded to use four state-ofthe-art triplets extraction tools: ClearNLP [33], Reverb [34],
Everest [35], AlchemyAPI [36] as complementary systems.
Additionally, any triplet slots with phrases were segmented
into keywords, stemmed, stop-word removed and their
cartesian product were produced as additional triplets.
2) Concepts Generation: Triplets extraction algorithms typically produce noisy and sparse triplets. Therefore, we apply
a hierarchical bottom-up clustering algorithm that generalizes
triplets into more meaningful relationships. To do so, we
employ both syntactic and semantic criteria that are based
on the corpus to generalize triplets into high level concepts

III. P ROBLEM D EFINITION
Given a set of documents {D1 ,...,DM } where each document contains one or more paragraphs. First, we split documents into sentences {S1 ,...,SN }. Next, using sentences as
data points, we aim to resolve whether a sentence Si contains
a frame or not. And, if the sentence contains a frame, then
we aim to identify its category, as one of: {Solution, Problem

279

without drift. In syntactic criteria, a pair of subjects-verbsobjects are merged only if they share common context related
to their different arguments (i.e. a pair of different subjects
are merged only if they co-occur with an identical verb-object
context).
Additionally, we capture contextual synonyms for subjects,
verbs and objects by deﬁning a semantic criterion which is
based on our corpus as well as WordNet [37]. Corpus-based
contextual synonyms for subjects, verbs and objects is based
on their common verb-object, subject-object and subject-verb
contexts respectively. Also, we capture contextual synonyms
that are not derivable from our corpus by applying WordNet
synonyms and hyponyms on the memebers of the concepts to
further expand and generalize them.
In order for the information to propagate between clusters of
subjects/objects and clusters of relations, we apply a hierarchical bottom-up clustering algorithm [38]. High level concepts
and relations are merged to form clusters. Each cluster is represented by graph of nodes and edges where nodes represent
concepts and edges represent relations between concepts. The
details of the above criteria and the generalization algorithm
are available in [6].

and non-smooth optimization problem. The features with nonzero values on the sparse x vector yield the discriminant
factors for classifying a sentence.
V. E XPERIMENTAL E VALUATION
A. Sentence Annotation
Our experts developed four categories of climate change
related frames as follows:
• Solution framing (prognostic): Covering the prognostic
function of deﬁning what should be done about problems,
solution framing refers to actions taken to prevent further
impact of climate change effects or further impact of
the causes of climate change such as greenhouse gas
emissions. Solutions can also emphasize ongoing measures to deal with existing effects of climate change.
Six frames capture an array of mitigation and adaptation
efforts conservation, education, investment, infrastructure
and development, creation or implementation of policy
and programs, and goal.
• Problem Threat framing (diagnostic): This diagnostic
framing class stresses on how climate change or outcomes
of climate change impact various actors, industries, human health, and the environment, Eight codes capture
negative consequences and threats brought by climate
change, including environmental systems and ecosystem,
public health, economic development, food security, water scarcity, national security, social unrest, and general or
multiple impacts. Both cause framing and problem/threat
framing comprise the diagnostic function in deﬁning
social problems.
• Cause framing: This group of diagnostic frames focus on
attributing the blame for causing climate change to either
human activity, natural variation or other reasons. Six
subcategories captured different explanations for causal
attribution of climate change: (a) human activity, (b)
natural variation, (c) scientiﬁc uncertainty, (d) policy
causes, (e) insufﬁcient actions, and (f) human disruption
to mitigate climate change impact.
• Motivation framing (motivational): Motivational framing refers to statements that explicit call for deﬁnitive
course(s) of action and explain why the audience should
make an effort to enact solutions [17]. In other words,
motivational frames elaborate on the rationale for action
that goes beyond diagnosis and prognosis, and include
vocabularies of severity, urgency, efﬁcacy, and propriety
[40]. We added a general category to analyze statements
that call for actions without providing readers with abovementioned reasons.
We assigned sentence annotation to three different expert
coders where we break ties by using the majority vote.

E. Frame Classiﬁcation
To classify each sentence as Frame/Non-Frame and identify
its relevant frame category we utilize sparse learning framework [9], with the underlined motivation to select a subset
of discriminating concepts that can (1) identify sentences
containing frame references and (b) classify a sentence into
a frame category. The following steps describe our algorithm:
1) Generate features from the entire corpus
2) Filter the features × sentences matrix to include only
resultant generalized concepts/features
3) Formulate the problem in a general sparse learning
framework [9]. In particular, the logistical regression
formulation presented below ﬁts this application, since it
is a dichotomous frame classiﬁcation problem (e.g. each
sentence classiﬁed as Frame/Non-Frame), and multiclass classiﬁcation problem (e.g. each Frame sentence
is further classiﬁed as one of four frame {Solution,
Problem Threat, Cause, and Motivation}):

minx

m


wi log(1 + exp(−yi (xt ai + c))) + λ|x|

(1)

i=1

In formula (1), ai is the vector representation of the ith
sentence, wi is the weight assigned to the ith sentence
(wi = 1/m by default), and A = [a1 , a2 , . . . , am ] is the
features × sentences matrix, yi is the label of each sentence,
and the xj , the j th element of x, is the unknown weight for
each feature, (λ > 0) is a regularization
 parameter that controls
|xi | is 1-norm of the x
the sparsity of the solution, |x|1 =
vector. We used the SLEP [39] sparse learning package that
utilizes gradient descent approach to solve the above convex

B. Quantitaive Evaluation
Once sentences are labeled as Frame/Non-Frame and categorized with their corresponding frame category, we utilize
uni-gram keywords, bi-gram terms, and generalized concepts
separately as features and the sparse logistical regression

280

TABLE I
F RAME /N ON -F RAME C LASSIFICATION
Method

Concepts

Bi-grams

Class Label

Precision

Recall

F-measure

Frame

0.80

0.88

0.84

Non Frame

0.87

0.77

0.82

Method

Concepts

Frame Category

Precision

Recall

F-measure

Solution

0.75

0.93

0.83

Problem Threat

0.77

0.84

0.79

Average

0.83

0.83

0.83

Cause

0.85

0.77

0.80

Frame

0.75

0.42

0.54

Motivation

0.89

0.62

0.73

0.82

0.79

0.79

Non Frame

0.74

0.92

0.82

Average

Average

0.74

0.67

0.68

Solution

0.87

0.77

0.81

0.59

Problem Threat

0.84

0.77

0.80

Cause

0.86

0.73

0.76

Motivation

0.90

0.58

0.71

Average

0.87

0.71

0.77

Solution

0.78

0.87

0.82

Problem Threat

0.81

0.81

0.81

Cause

0.83

0.62

0.82

Motivation

0.85

0.57

0.64

Average

0.82

0.72

0.77

Frame
Uni-grams

TABLE II
F RAME C LASSIFICATION INTO FOUR CATEGORIES

Non Frame
Average

0. 75
0.76
0.75

0.48
0.91
0.70

Bi-grams

0.89
0.74

classiﬁer SLEP [39] to identify weighted discriminative features and classify sentences. We experimented with three
different classiﬁers (SVM [7], Random Forests [8]) and found
that SLEP outperformed both these other classiﬁers. Using
different types of features generated from the entire corpus, we
perform ten-fold cross-validation for measuring the classiﬁer’s
predictive accuracy to detect Frame/Non-Frame sentences.
Next, using features generated from frame sentences only, we
train a multi-class model to classify sentences into their corresponding frame category. We report precision, recall, and Fmeasure as quantitative evaluation metrics. Qualitative analysis
of the identiﬁed discriminating concepts is also presented in
the next section.
Table 1 presents the accuracies for detecting Frame/NonFrame sentences using different features. Using generalized
concepts approach as features, the resultant average accuracy
(F-measure of 83%) outperforms both accuracies with unigrams (74%) and bi-grams (68%) features by 12% and 22%
respectively.
Table 2 shows the accuracies for identifying the corresponding frame category. Using generalized concepts, these
accuracies vary between 73% and 83% (F-measure) for different categories. In this table, utilizing generalized concepts
yields slightly better performance compared to both uni-grams
and bi-grams with an overall average accuracy (F-measure) of
79%.

Uni-grams

associated global warming with carbon dioxide emissions
using the following triplets to construct a cohesive story:
• Scientiﬁc research indicate that atmospheric carbon dioxide increase at a large level.
• Cars and trucks were major sources of air pollution and
carbon dioxide emissions, which directly increased local
temperature.
2) Problem Threat Framing: Next, we turned our attention
to identify the dominant concepts representing the problem
and threat framing of climate change. Media texts tended
to highlight devastating environmental impacts caused by
climate change, such as ﬂoods, prolonged drought, loss of
landmass and soil, desertiﬁcation, sea-level rise, storm surge,
heat waves, and more. Flooding, in particular, is a severe
concern as nine out of sixteen triplets of high weigh values
explicitly mentioned the negative impacts of heavy rainfall
or torrential rain. Consequently, economic condition and food
insecurity were inﬂuenced, infrastructure was damaged, and
health diseases were exacerbated with the increased intensity
and frequency of ﬂoods.
3) Solution Framing: The most representative discourse of
solution framing is discussed next in Section D.
4) Motivation Framing: When discussing motivation for
why policy actors and citizens should act upon, the most
salient concepts emphasized that international communities
(e.g. U.S., EU, and China) should negotiate a legal agreement
to reduce greenhouse gas emissions at the end of 2015.
There is little attention to stating speciﬁc reasons for offering localized adaptation strategies that people can undertake.
Although the awareness of climate change impacts among
African government ofﬁcials was generally high, the prevailing
generalized concept of calling for international actions on
mitigation from mainstream media discourse reﬂected a lack

C. Qualitative Analysis of Resultant Concepts
Table III shows top ﬁve discreminative concepts for each
frame category. Our team of experts explored the highly
signiﬁcant generalized concepts germane to four-class framing
in media discourse surrounding climate change across West
African RSS feeds and provided qualitative evaluations as
follows:
1) Cause Framing: Causal responsibility of climate change
and its effects was often attributed to anthropogenic activities, particularly man-made greenhouse gas emissions, humaninduced pollution, and fossil fuel use. Carbon dioxide and
greenhouse gas emission emerged as highly signiﬁcant concepts, as indicated by high weigh value. Media texts often

281

TABLE III
T OP FIVE GENERATED CONCEPTS FOR EACH FRAME CATEGORY
Cause

Problem Threat

Solution

Motivation

{Greenhouse,Emissions,Gases}
↓
{Cause,Attribute to}
↓
{Global warming}

{Flood}
↓
{Associate,Create}
↓
{Poverty,Disease}

{Action plan,Policy}
↓
{Build,Consolidate}
↓
{Sustainability,Resilience
future}

{International,Community}
↓
{Urge,Warn}
↓
{Threat}

{Industry,Anthropogenic}
↓
{Raise}
↓
{Earth temperature,CO2,CO5}

{Heavy rainfall, Torrential rain}
↓
{Create,Bring,Increase}
↓
{Flooding,Disaster,Landslide}

{Development,
Sustainability,National
program}
↓
{Enhance}
↓
{Community}

{Agreement,Leaders,World}
↓
{Help}
↓
{Future,Hope}

{Fossil fuel}
↓
{Impact,Harm}
↓
{Planet,Environment,Weather}

{Drought}
↓
{Cause,Impact,Reduce}
↓
{Food-shortage,Foodproduction,Crop}

{Brown}
↓
{Sign}
↓
{Local legislation, CA
groundwater,Management
framework}

{USA,EU,China}
↓
{Recognize,Reduce}
↓
{Emissions}

{Coal
combustion,Diesel,Man-Made}
↓
{Create}
↓
{Extreme
weather,Temperature-up}

{Sea-level rise}
↓
{Result in,Cause}
↓
{Tsunami,Damage,Flood}

{Sustainability,Energy}
↓
{Can help,Improve}
↓
{Food security,Households}

{Truck,Car}
↓
{Rise}
↓
{Carbon pollution,Pollute}

{Extreme Weather, Hailstorm}
↓
{Cause,Affect}
↓
{Mudslide,Floods,Farming}

{Smart agriculture,Africa
countries}
↓
{Meet,Breathe}
↓
{Life}

of effective national and local polices.

{Africa}
↓
{Need,Implement}
↓
{Policy,Awareness,Partnership}

{Nigerian}
↓
{Apply,Take}
↓
{Measures,Renewable
Energy,Policy}

actions) can enhance local community resilience. According to
the IPCC (Intergovernmental Panel on Climate Change) report,
majority of rural communities rely on rain-fed agriculture
to sustain their livelihoods in West Africa, the region worst
affected by climate change. With changing rainfall patterns,
prolonged droughts and ﬂooding, sustainable system of developing agriculture-smart technologies can help improve food
security at the household level. Interestingly, the African media
discussed that California Governor Jerry Brown has signed the
most signiﬁcant framework for regulating underground water
resources to achieve sustainable development in September,
2014.

D. Visualizing Concepts
To visualize the generalized concept and relation clusters,
we utilize a semantic network [41] of nodes (V) and edges
(E) to describe the semantic space of the underlying texts.
Circle nodes represent subjects/objects and square nodes represent verbs. Edges represent relations between concepts. In
such a network, distinct combinations of actors (subjects)
perform or recommend various sets of actions (verbs) on
distinct combinations of targets (objects). The sample semantic
network in Figure 3 (next page) illustrates how sustainability
emerges as a concept that is central to addressing climate
change impacts. The semantic network represents the contextual relationships between generalized triplets relating to
strategies for sustainable adaptation. In the media discourse,
sustainable adaptation is predominantly framed as an effective
solution to reduce impacts of climate change and contribute to
social, economic, and environmental development. As shown
in Figure 3, developing sustainable national programs (or

VI. C ONCLUSION AND F UTURE W ORK
Climate change framing has pervasive inﬂuence, and this paper presents a new computational approach based on generalized concepts to identify popular media frames and map them
to different categories: solution, problem threat, cause, and
motivation. A line of related work has used bag of words and
word-level features to detect frames automatically in text. Such

282

Fig. 3.

A sample semantic network of frame concepts

work face limitations since standard keyword based features
may not generalize well to accommodate surface variations in
text when different keywords are used for similar concepts. In
this paper, we developed a new type of textual features that
generalize (subject,verb,object) triplets extracted from text,
by clustering them into high-level concepts. Compared to
unigram and bigram based models, frame classiﬁcation using
our generalized concepts yielded better discriminating features
with a 12% boost in accuracy (i.e. from 74% to 83% in fmeasure) for frame/no frame detection. In our future work, we
plan to utilize discriminating generalized concepts indicating
actor-action-target sequences to infer causal chains of events,
frames, and actions that might lead to better indicators of
climate-change related social unrest.

[4] J. Barnett and W. N. Adger, “Climate change, human security
and violent conﬂict,” Political Geography, vol. 26, no. 6, pp. 639
– 655, 2007, climate Change and Conﬂict. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S096262980700039X
[5] S. Alashri, S. Alzahrani, L. Bustikova, D. Siroky, and H. Davulcu, “What
animates political debates? analyzing ideological perspectives in online
debates between opposing parties,” in Proceedings of the ASE/IEEE
International Conference on Social Computing (SocialCom-15), 2015.
[6] B. Ceran, N. Kedia, S. Corman, and H. Davulcu, “Story detection using
generalized concepts and relations,” in Proceedings of International
Symposium on Foundation of Open Source Intelligence and Security
Informatics (FOSINT-SI), in conj. with IEEE ASONAM, 2015.
[7] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,
vol. 20, no. 3, pp. 273–297, 1995.
[8] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp.
5–32, Oct. 2001. [Online]. Available: http://dx.doi.org/10.1023/A:
1010933404324
[9] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ser. KDD ’09. NY, USA:
ACM, 2009, pp. 547–556. [Online]. Available: http://doi.acm.org/10.
1145/1557019.1557082
[10] S. Hilgartner and C. L. Bosk, “The rise and fall of social problems: A
public arenas model,” American journal of Sociology, pp. 53–78, 1988.
[11] M. Hulme, Why we disagree about climate change: Understanding
controversy, inaction and opportunity. Cambridge University Press,
2009.
[12] M. T. Boykoff, Who speaks for the climate?: Making sense of media
reporting on climate change. Cambridge University Press, 2011.
[13] S. C. Moser and L. Dilling, Creating a climate for change. Cambridge
University Press, 2006.
[14] A. Carvalho, “Ideological cultures and media discourses on scientiﬁc
knowledge: re-reading news on climate change,” Public understanding
of science, vol. 16, no. 2, pp. 223–243, 2007.
[15] R. J. Brulle, J. Carmichael, and J. C. Jenkins, “Shifting public opinion on
climate change: an empirical assessment of factors inﬂuencing concern
over climate change in the us, 2002–2010,” Climatic change, vol. 114,
no. 2, pp. 169–188, 2012.

ACKNOWLEDGMENT
Some of the material presented here was sponsored by
Department of Defense and it is approved for public release,
case number:15-467.
R EFERENCES
[1] D. Chong and J. N. Druckman, “A theory of framing and
opinion formation in competitive elite environments,” Journal of
Communication, vol. 57, no. 1, pp. 99–118, 2007. [Online]. Available:
http://dx.doi.org/10.1111/j.1460-2466.2006.00331.x
[2] M. C. Nisbet, “Communicating climate change: Why frames matter for
public engagement,” Environment: Science and Policy for Sustainable
Development, vol. 51, no. 2, pp. 12–23, 2009. [Online]. Available:
http://dx.doi.org/10.3200/ENVT.51.2.12-23
[3] A. Shehata and D. N. Hopmann, “Framing climate change,” Journalism
Studies, vol. 13, no. 2, pp. 175–192, 2012. [Online]. Available:
http://dx.doi.org/10.1080/1461670X.2011.646396

283

[16] R. M. Entman, “Framing: Towards clariﬁcation of a fractured paradigm,”
McQuail’s reader in mass communication theory, pp. 390–397, 1993.
[17] R. D. Benford and D. A. Snow, “Framing processes and social movements: An overview and assessment,” Annual review of sociology, pp.
611–639, 2000.
[18] S. M. Jang and P. S. Hart, “Polarized frames on climate change and
global warming across countries and states: evidence from twitter big
data,” Global Environmental Change, vol. 32, pp. 11–17, 2015.
[19] K. Stalpouskaya and C. Baden, “To do or not to do: the role of agendas
for action in analyzing news coverage of violent conﬂict,” ACL-IJCNLP
2015, p. 21, 2015.
[20] B. Burscher, D. Odijk, R. Vliegenthart, M. de Rijke, and C. H. de Vreese,
“Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis,” Communication
Methods and Measures, vol. 8, no. 3, pp. 190–206, 2014.
[21] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,”
arXiv preprint arXiv:1408.5882, 2014.
[22] T. Wilson, J. Wiebe, and P. Hoffmann, “Recognizing contextual polarity
in phrase-level sentiment analysis,” in Proceedings of the conference on
human language technology and empirical methods in natural language
processing. Association for Computational Linguistics, 2005, pp. 347–
354.
[23] D. Odijk, B. Burscher, R. Vliegenthart, and M. De Rijke, “Automatic
thematic content analysis: Finding frames in news,” in Social Informatics. Springer, 2013, pp. 333–345.
[24] E. Baumer, E. Elovic, Y. Qin, F. Polletta, and G. Gay, “Testing and
comparing computational approaches for identifying the language of
framing in political news.” in HLT-NAACL, 2015.
[25] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer, “Featurerich part-of-speech tagging with a cyclic dependency network,” in
Proceedings of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on Human Language
Technology-Volume 1. Association for Computational Linguistics, 2003,
pp. 173–180.
[26] J. Finkel, T. Grenager, and C. Manning, “Incorporating non-local information into information extraction systems by gibbs sampling,” in
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2005,
pp. 363–370.
[27] B. Ceran, R. Karad, A. Mandvekar, S. R. Corman, and H. Davulcu,
“A semantic triplet based story classiﬁer,” in Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International
Conference on. IEEE, 2012, pp. 573–580.
[28] J. A. Hartigan and M. A. Wong, “Algorithm AS 136: A k-means
clustering algorithm,” Applied Statistics, vol. 28, no. 1, pp. 100–108,
1979. [Online]. Available: http://dx.doi.org/10.2307/2346830
[29] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu,
D. Jurafsky, and C. Manning, “A multi-pass sieve for coreference
resolution,” in Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics, 2010, pp. 492–501.
[30] H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Stanfords multi-pass sieve coreference resolution system
at the conll-2011 shared task,” CoNLL 2011, p. 28, 2011.
[31] H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Deterministic coreference resolution based on entitycentric, precision-ranked rules,” Comput. Linguist., vol. 39, no. 4, pp.
885–916, Dec. 2013.
[32] M. Recasens, M. C. de Marneffe, and C. Potts, “The life and death of
discourse entities: Identifying singleton mentions,” in Proceedings of the
2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2013, pp.
627–633.
[33] J. D. Choi, “Optimization of natural language processing components for
robustness and scalability,” Ph.D. dissertation, University of Colorado at
Boulder, 2012.
[34] A. Fader, S. Soderland, and O. Etzioni, “Identifying relations for open
information extraction,” in Proceedings of the Conference of Empirical
Methods in Natural Language Processing (EMNLP ’11), Edinburgh,
Scotland, UK, July 27-31 2011.
[35] (2013) Everest triplet extraction. Next Century Corporation. [Online]. Available: https://github.com/NextCenturyCorporation/
EVEREST-TripletExtraction

[36] (2015) Alchemyapi language features. AlchemyAPI, Inc. [Online].
Available: http://www.alchemyapi.com/products/alchemylanguage
[37] (2010) About wordnet. Princeton University. [Online]. Available:
http://wordnet.princeton.edu
[38] S. Kok and P. Domingos, “Extracting semantic networks from text via
relational clustering,” in Proceedings of the 2008 European Conference
on Machine Learning and Knowledge Discovery in Databases - Part I,
2008, pp. 624–639.
[39] J. Liu, S. Ji, and J. Ye, SLEP: Sparse Learning with Efﬁcient
Projections, Arizona State University, 2009. [Online]. Available:
http://www.public.asu.edu/∼jye02/Software/SLEP
[40] R. D. Benford, “” you could be the hundredth monkey”: Collective
action frames and vocabularies of motive within the nuclear disarmament
movement,” Sociological Quarterly, pp. 195–216, 1993.
[41] M. R. Quillian, “Semantic memory,” in Semantic Information Processing, 1968, pp. 227–270.

284

A Dynamic Modularity Based Community Detection
Algorithm for Large-scale Networks: DSLM
Riza Aktunc and Ismail Hakki Toroslu

Mert Ozer and Hasan Davulcu

Computer Engineering Department
Middle East Technical University
Ankara, Turkey 06530
Email: {riza.aktunc, toroslu}@ceng.metu.edu.tr

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
Email: {mozer, hdavulcu}@asu.edu

Abstract—In this work, a new fast dynamic community
detection algorithm for large scale networks is presented. Most
of the previous community detection algorithms are designed
for static networks. However, large scale social networks are
dynamic and evolve frequently over time. To quickly detect communities in dynamic large scale networks, we proposed dynamic
modularity optimizer framework (DMO) that is constructed
by modifying well-known static modularity based community
detection algorithm. The proposed framework is tested using
several different datasets. According to our results, community
detection algorithms in the proposed framework perform better
than static algorithms when large scale dynamic networks are
considered.

I.

I NTRODUCTION

In the last decade, the notion of social networking is
emerged and produced very large graphs that consist of the
information of their users. These graphs generally consist of
the nodes that represent the users; and edges that represent the
relations among users. The nodes in these graphs generally
tend to get together and construct communities of their own.
Thus, it can be stated that social networks commonly have
a community structure. These networks can be divided into
groups of nodes that have denser connections inside the group;
but fewer connections to the outside of the group. For example,
in a GSM network, a group of users who call each other more
densely than they call other users may construct their own
community. In this case, the nodes represent the users and the
edges represent the calls that users made. The detection of
communities in these large networks is a problem in this area;
therefore a lot of community detection algorithms such as [1],
[2], [3], [4], [5], [6], [7], [8], [9] proposed in the literature.
Almost all these community detection algorithms are static
and designed for static networks.
However, most of the social networks are not static because
they evolve in many ways. They may gain or lose users that
are represented as nodes in the graphs over time. The users of
these social networks may lose contact from each other or there
can be new connections among users. In other words, some
edges in the graphs may be removed or new edges may be
added to the graph over time. All these processes may happen
in a very small amount of time in a social network if it has
a lot of active users. This kind of a social network may be
called as highly dynamic. For example, popular social sites
such as Facebook, Twitter, LinkedIn and so on have highly
dynamic social networks. Moreover, most GSM networks have

millions of users and hundreds of calls made in seconds;
therefore, they can also be labeled as highly dynamic networks.
Addition or deletion of an edge or a node from a network
which has millions of edges might seem insignificant; but when
this additions or deletions of an edge or a node happen very
frequently, they begin to change the community structure of
the whole network and become very important. This change in
the community structure raises the need of re-identification of
communities in the network. This need arises frequently and
creates a new problem in the community detection research
area. This new problem requires somehow fast detection of
communities in dynamic networks.
The first solution that comes to mind for community
detection in large dynamic networks problem is the execution
of static community detection algorithms already defined in
the literature all over again to detect the new community
structure whenever the network is modified. Nevertheless, this
solution takes too much time in every modification of the large
networks since it runs the community detection algorithm from
scratch each time. A much efficient and less time consuming
solution is to run the community detection algorithms not
from scratch but from a point in the history of the network
by storing and using the historical results of executions of
the algorithms whenever network is evolved. In other words,
updating previously discovered community structure instead of
trying to find communities from scratch each time the network
evolves consumes much less time and thus much efficient. This
solution method for the problem of detecting communities in
large dynamic networks is the main focus of our study in this
paper.
In this paper, we modified the smart local moving (SLM)
algorithm defined by Waltman & Van Eck [9] so that it would
detect the communities in rapidly growing large networks dynamically and efficiently. As a result, we propose the dynamic
SLM (dSLM) algorithm that dynamically detects communities
in large networks by optimizing modularity and using its
own historical results. We tested our proposed approach on
several different datasets. We demonstrated the effects of our
contribution to the SLM algorithm in two ways. One of
them is the change in modularity value which determines
the quality of the community structure of the network. The
other one is the change in running time that determines the
pace of the algorithm. The latter is more significant than the
former because the community structure of the network must
be quickly identified at the given timestamp before the next

timestamp is reached. We realized that dSLM improved SLM
by decreasing its running time incredibly. Moreover, there
are some experiments where modularity value increases while
running time decreases.
The rest of the paper is organized as follows. Section II
introduces previous researches done in the area. Section III
explains the modularity. The proposed solution for dynamic
community detection in large networks called as dSLM and
its static version SLM are described in Section IV. In Section
V, the results of the experiments of SLM and dSLM are
demonstrated. Finally, the paper is concluded in Section VI.

II.

R ELATED W ORK

The idea of modularity-based community detection is to
try to assign each vertex of the given network to a community
such that it maximizes the modularity value of the network.
Optimizing modularity is an NP-hard problem. [10] Exact
algorithms that maximize modularity such as [11], [10], [12]
can be used only for small networks.
For large-scale modularity optimization, heuristic algorithms are proposed. We basically focus on three well known
algorithms, namely; CNM, Louvain and SLM. The first one is
Clauset et al.’s [13] CNM algorithm. It is a greedy modularity
maximization algorithm that searches for best community
assignment for each node. The second one is referred as
Louvain algorithm and proposed by Blondel et al. [7] in 2008.
By considering each community as a single node, it further
searches for new community merges after the local optimum
satisfied using CNM. The last one is called as Smart Local
Moving (SLM) algorithm that is proposed by Waltman and
Jan van Eck in 2013. [9] SLM algorithm is explained in detail
in chapter III.
Due to the dynamic features of many social networks
[14], the need for detecting communities dynamically in the
large networks is emerged in the latest years. There have
been many community detection algorithms proposed in the
literature to fulfill this need. Xu et al. divides the current
research on community evolution into the following categories.
Parameter estimation methods and probabilistic models have
been proposed in the literature. [15], [16] A methodology
that tries to find an optimal cluster sequence by detecting
a cluster structure at each timestamp that optimizes the incremental quality can be classified as evolutionary clustering.
[17], [18] Furthermore, tracking algorithms based on similarity
comparison have also been studied in order to be able to
describe the change of communities on the time axis. [19], [20]
Apart from these algorithms that are focused on the evolution
procedures of communities, community detection in dynamic
social networks aims to detect the optimal community structure
at each timestamp. For this purpose, incremental versions of
both CNM and Louvain algorithm are proposed by Dinh et
al.[21] and Aynaud et al. [22]. To the best of our knowledge,
this is the first work considering the incremental version of
Smart Local Moving algorithm in literature. Our algorithm
can be classified as the last mentioned category which aims
to detect optimal community structure at each timestamp with
minimum running time.

III.

M ODULARITY

Modularity is a function that is used for measuring the
quality of the results of community detection algorithms.
If the modularity value of a partitioned network is high,
it means that the network is partitioned well. Apart from
quality measurement, modularity is used as the basis of some
community detection algorithms. These algorithms try to detect
communities (partitions) in a network by trying to maximize
the modularity value of the network. Thus, modularity is
a function that is used for both quality measurement and
community detection.
Modularity is based on the idea that a randomly created
graph is not expected to have community structure, so comparing the graph at hand with a randomly created graph would
reveal the possible community structures in the graph at hand.
This comparison is done through comparing the actual density
of edges in a subgraph and the expected edge density in the
subgraph if the edges in the subgraph were created randomly.
This expected edge density depends on how random the edges
created. This dependency is tied to a rule that defines how
to create the randomness and called as null model. A null
model is a copy of an original graph and it keeps some
of this original graphs structural properties but not reflects
its community structure. There can be multiple null models
for a graph such that each of them keeps different structural
properties of the original graph. Using different null models for
the calculation of the modularity leads to different modularity
calculation methods and values. The most common null model
that is used for modularity calculation is the one that preserves
the degree of each vertex of the original graph. With this null
model, modularity is calculated as the fraction of edges that
fall in the given communities minus such fraction in the null
model. [23], [24] The formula of modularity can be written as
in Equation 1

Q=

1 X
(Aij − Pij )δ(Ci , Cj )
2m ij

(1)

m represents the total number of edges of the graph. Sum
iterates over all vertices denoted as i and j. Aij is the number
of edges between vertex i and vertex j in the original graph.
Pij is the expected number of edges between vertex i and
vertex j in the null model. The δ function results as 1 if the
vertex i and vertex j are in the same community (Ci = Cj ),
0 otherwise. The null model can be created by cutting the
edges between vertices; thus, creating stubs (half edges) and
rewiring them to random vertices. Thus, it obeys the rule of
keeping degrees of vertices unchanged. Cutting edges into half,
creates m ∗ 2 = 2m stubs. In the null model, a vertex could be
attached to any other vertex of the graph and the probability
that vertices i and j, with degrees ki and kj , are connected,
can be calculated. The probability pi to pick a ramdom stub
ki
connection for vertex i is 2m
, as there are ki stubs of i out of a
total of 2m stubs. The probability of vertex i and vertex j being
connected is pi pj , since stubs are connected independently
of each other. Since there are 2m stubs, there are 2mpi pj
expected number of edges between vertex i and vertex j. [24]
This yields to equation 2

Pij = 2mpi pj = 2m

ki kj
ki kj
=
2
4m
2m

(2)

By placing equation 2 into equation 1, modularity function
is presented as in equation 3.
Q=

1 X
ki kj
(Aij −
)δ(Ci , Cj )
2m ij
2m

(3)

The resulting
 values of this modularity function lie in the
range −1
,
1
. It would be positive if the number of edges
2
within subgraphs is more than the number of expected edges
in the subgraphs of null model. Higher values of the modularity
function mean better community structures. [9]
This modularity function also applies to weighted networks. [9], [25] The modularity function for the weighted
graphs can be calculated as in equation 4.
Qw =

1 X
si sj
(Wij −
)δ(Ci , Cj )
2W ij
2W

(4)

There are three differences. The first difference is that in
the case of a weighted network Wij , instead of Aij , may take
not just 0 or 1 but any non-negative value that represents the
weight of the edge. The second one is that instead of m, which
is total number of edges, W , which is the sum of the weights
of all edges is used in the equation. The last one is that si and
sj which represents the sum of the weights of edges adjacent
to vertex i and vertex j respectively is used in the equation
instead of ki and kj which means the degree of vertex i and
vertex j respectively. [24]
Apart from weighted networks, the modularity function
defined in 3 has been extended in order to be also applicable
to directed networks. [26], [27] When the edges are directed,
stubs will also be directed and it changes the possibility of
rewiring stubs and connecting edges. The calculation of this
possibility in the directed case depends on the in- and outdegrees of the end vertices. For instance, there are two vertices
A and B. A has a high in-degree and low out-degree. B has a
low in-degree and high out-degree. Thus, in the null model of
modularity, an edge will be much more likely to point from
B to A than from A to B. [24] Therefore, the expression of
modularity for directed graphs can be written as in equation 5
Qd =

kiout kjin
1 X
(Aij −
)δ(Ci , Cj )
m ij
m

There have been a few proposals of modified version of the
modularity functions defined above as alternative modularity
functions. These modified, extended versions for instance offer
a resolution parameter that makes it possible to customize
the granularity level at which communities are detected and
to mitigate the resolution limit problem defined by Fortunato
and Barthlemy [28]. [29] Moreover, there are modularity
functions with a somewhat modified mathematical structure
in the literature such as Reichardt & Bornholdt, 2006; Traag,
Van Dooren, & Nesterov, 2011; Waltman, Van Eck, & Noyons,
2010. [9], [28], [30], [31]
IV.

A. SLM Algorithm
SLM is a community detection algorithm that is evolved
from Louvain algorithm. Louvain algorithm is a large scale
modularity based community detection algorithm that is proposed by Blondel et al in 2008. [7] The quality of detected
communities by Louvain algorithm is measured by the method
called modularity. The modularity of a network is a value
that is between -1 and 1. This value presents the density of
links inside communities over the density of links between
communities. [23] When this value is close to 1, then the
measured network can be called as modular network. In
the case of weighted networks, modularity function can take
weights into consideration and measure the quality of detected
communities. Louvain algorithm uses modularity function as
not only a measurement function but also an objective function
to optimize.
Louvain algorithm is a recursive algorithm which has two
steps running in each recursive call. Before the recursion starts,
the algorithm assigns a different community to each node of
the network whose communities are going to be detected.
Therefore, in the initial case each node has its own community.
In each recursive call the following steps are run:
1)

(5)
2)

The sum of the in-degrees (out-degrees) equals m not
2m as in the case of undirected graph. Therefore, the factor
2 in the denominator of the first and second summand has
been dropped. In order to get the modularity function to be
applicable to directed weighted networks, the equations 4 and
5 can be merged; thus, equation 6 can be constructed as the
most general expression of modularity. [24]
Qdw =

in
sout
1 X
i sj
(Wij −
)δ(Ci , Cj )
W ij
W

(6)

SLM AND D SLM A LGORITHMS

It runs a local moving heuristic in order to obtain
an improved community structure. This heuristic basically moves each node from its own community
to its neighbors’ community and run the modularity
function. If the result of the modularity function,
which means quality, increased, the node would be
kept in the new community; else, the node would be
moved back to its previous community. This process
is applied to each node for its each neighbor in
random order and thereby heuristically the quality is
tried to be increased.
The algorithm constructs a reduced network whose
nodes are the communities that are evolved in the
first step. Moreover, the weights of the edges in this
reduced network are given by the sum of weights
of the links between the nodes which reside in
the corresponding two communities. Links between
nodes of the same community in the old network are
presented as self-links for the node that represents
that community in the new reduced network. When
this reduced network is fully constructed, then algorithm calls itself recursively and first step is applied
to this reduced network.

The algorithm keeps recursing until no further improvement
in modularity is measured and thereby there are no changes in
the community structure. [7]
Louvain algorithm detects community structures whose
modularity values are locally optimal with respect to community merging, but not necessarily locally optimal with respect
to individual node movements. Since the Louvain algorithm
applies local moving heuristic in the beginning of its recursive
block and merges communities by reducing network in the
end of its recursive block, calling it iteratively ensures that
the resulting community structure cannot be improved further
either by merging communities or by moving individual nodes
from one community to another. Like the iterative variant
of these algorithms SLM algorithm constructs community
structures that are locally optimal with respect to both individual node movements and community merging. Besides
these capabilities, SLM also tries to optimize modularity by
splitting up communities and moving sets of nodes between
communities. This is done by changing the way that local
moving heuristic and network reduction runs.[9]
Louvain algorithm runs local moving heuristic algorithm
on the present network as the first step, and then construct
the reduced network as the second step. However, the SLM
algorithm changes the reduced network construction step by
applying following processes:
1)

2)
3)

It iterates over all communities that are formed by the
first step. It copies each community and constructs a
subnetwork that contains only the specific community’s nodes.
It then runs the local moving heuristic algorithm on
each subnetwork after assigning each node in the
subnetwork to its own singleton community.
After local moving heuristic constructs a community
structure for each subnetwork, the SLM algorithm
creates the reduced network whose nodes are the
communities detected in subnetworks. The SLM algorithm initially defines a community for each subnetwork. Then, it assigns each node to the community
that is defined for the node’s subnetwork. Thus,
there is a community defined for each subnetwork
and detected communities in subnetworks are placed
under these defined communities as nodes in the
reduced network.

Procedure: Initialize Communities
Input: Old Communities, Old Network, New Network
Output: New Communities
1: j = 0
2: for i = 0 → Old Communities.size do
3:
N ew Communities = ReadCommunitiesF ile()
4: end for
5: Delta N etwork = N ew N etwork − Old N etwork
6: for j = i → Delta N etwork.size do
7:
new communities[j] = Delta N etwork[j − i]
8: end for
Fig. 1.

Initialize Communities Procedure

•

Existing communities are read from file as New Communities.

•

If exists, the extensions to the network has been
determined.

•

For each new node, singleton new communities are
constructed and added to New Communities.

The effects of other changes in the network, such as adding
new edges and deletions of nodes and edges, are handled
while executing standard SLM procedure. The new dSLM is
available at https://github.com/mertozer/dSLM.
Since after some iterations, the increase of modularity
drops to very small values, it might make sense to stop the
iterations using either the amount of changes or by setting
a target modularity value. We have implemented the second
option, which is called dSLMEVS in the experiments. We
have made the tests by setting the modularity values as the one
obtained for SLM in order to be able to observe the differences
in the execution times for exactly the same modularity values.
C. Running Example

This is the way that the SLM algorithm constructs the reduced
network. After these processes, the SLM algorithm gives the
reduced network to the recursive call as input and all the
processes starts again for the reduced network. The recursion
continues until a network is constructed that cannot be reduced
further. To sum up, the SLM algorithm has more freedom in
trying to optimize the modularity by having the ability to move
sets of nodes between communities which cannot be done by
Louvain algorithm. [9]
B. Dynamic Smart Local Moving Algorithm
SLM algorithm initially assigns each node to a different
community, so each node has its own singleton community. In
order convert SLM to dynamic form, we replace that operation
with a newly defined procedure, called initialize communities
which is given in Figure 1. This procedure works as follows:

Fig. 2.

Network in time t (analyzed by SLM)

Let the sample network depicted in Figure 2 to be a
network that changes in time and needs to be analyzed
continuously in each time frame. So, the network is analyzed
and communities are detected in time t. Figure 2 presents the

beginning and end states of the community structure of the
network analyzed by SLM algorithm in time t. Solid rectangles
present the initial community structure; whereas the colors
of the nodes (and dashed rectangles) present the resulting
community structure. From time t to time t+1, a node which
is numbered as 10 and an edge between this new node and
the node which is numbered as 9 are added to the network.
This evolved network in time t+1 can be seen in Figure 3 and
Figure 4. Figure 3 presents the community detection process

structure from scratch by trying and finding node movements
that maximize the modularity of the network. However, dSLM
needs to try only one node movement which is to move newly
added node from its singleton community to its only neighbor
(blue) community. Since it appears to increase the modularity
of the network, dSLM places the new node to blue community
and that is it. Because the initial community structure is known
to be the one that maximizes the modularity of the network,
there is no other node movement trying that can increase
modularity. By this way, the dSLM runs faster than SLM .
This run time difference between SLM and dSLM gets much
greater while the network size increases.
V.

E XPERIMENTS & R ESULTS

We evaluate our proposed approach dSLM on five realworld datasets which are the arXiv citation dataset, the GSM
calls dataset, Google Plus, Twitter and Youtube user network
datasets..
The arXiv1 citation dataset is published in the KDD Cup
2003. It contains approximately 29,000 papers and their citation graph. In this graph, each vertex represents a paper
and each edge represents the citation between its connected
vertexes. There are around 350,000 edges which represent
citations in this graph.

Fig. 3.

Network in time t+1 (analyzed by SLM)

of the network in time t+1 performed by SLM algorithm in
the same way as Figure 2. As the difference of Figure 2
and Figure 3, a new node and a new edge are only seen in
Figure 3. Since they both demonstrate the SLM process, the
initial communities are singleton. Figure 4 demonstrates the

We have used call detail record (CDR) dataset, obtained
from one of the largest GSM operators in Turkey. This
produced GSM calls dataset contains 12,521,352 nodes and
44,768,912 edges. These two datasets are used for edge
deletion and addition experiments purposes.
The Google Plus and Twitter user network data is collected
by Stanford Network Analysis Project2 . The Google Plus
data consists of 107,614 nodes and 13,673,453 edges. The
Twitter data consists of 81,306 nodes and 1,768,149 edges.
The Youtube user network data is provided by Mislove et
al. [32]. It consists of 1,134,890 nodes and 2,987,624 edges.
The users of the Youtube are the nodes, and the friendships
are represented by edges. We used these 3 datasets for node
deletion and addition experiments purposes.
In the first set of experiments we have assumed the dynamic
feature is in the form of edge insertions and deletions. Table I
and II gives the results for these experiements. In the second
set, on the other hand, node insertions and deletions represent
the dynamic feature. Table III and IV presents the results for
these experiments.
TABLE I.

Fig. 4.

Network in time t+1 (analyzed by dSLM)

dSLM process of the network in time t+1. In this process, the
community structure of the network in time t is used as the
initial states of communities which can be seen as rectangles
in Figure 4. Both SLM and dSLM algorithms place the newly
added node in blue community. SLM constructs the community

T HE EFFECT OF D SLM FOR EDGE INSERTIONS

Algorithm

Dataset

Base (#
of Edges)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

arxiv
arxiv
GSM
GSM
GSM
GSM
GSM
GSM
GSM
GSM

300,000
300,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000

#
of
Edges
Added
1,000
10,000
1,000
10,000
100,000
1,000,000
1,000
10,000
100,000
1,000,000

Change in Modularity Value
0.02% increased
0.15% increased
no change
no change
no change
0.02% increased
no change
no change
no change
no change

1 http://www.cs.cornell.edu/projects/kddcup/datasets.html
2 http://snap.stanford.edu/data

Decrease
in Running
Time
26%
26%
27%
29%
20%
17%
91%
63%
64%
80%

TABLE II.

T HE EFFECT OF D SLM FOR EDGE DELETIONS

Algorithm

Dataset

Base (#
of Edges)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

arxiv
arxiv
GSM
GSM
GSM
GSM
GSM
GSM
GSM
GSM

300,000
300,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000

TABLE III.

Change in Modularity Value
0.08% increased
0.04% increased
no change
no change
0.01% increased
0.01% increased
no change
no change
no change
no change

Decrease
in Running
Time
32%
7%
38%
27%
24%
16%
92%
61%
91%
80%

T HE EFFECT OF D SLM FOR NODE INSERTIONS

Algorithm

Dataset

Base (#
of Nodes)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube
Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube

81,296
81,206
80,306
71,306
107,604
107,514
106,614
97,614
1157728
1156828
1147828
1057828
81,296
81,206
80,306
71,306
107,604
107,514
106,614
97,614
1,157,728
1,156,828
1,147,828
1,057,828

TABLE IV.

#
of
Edges
Deleted
1,000
10,000
1,000
10,000
100,000
1,000,000
1,000
10,000
100,000
1,000,000

#
of
Nodes
Added
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000

Change in Modularity Value
no change
no change
no change
0.04% increased
0.02% decreased
no change
no change
0.97% decreased
1.77% increased
1.36% increased
0.03% increased
0.12% increased
no change
no change
no change
no change
0.02% decreased
no change
no change
0.04% increased
0.04% increased
0.13% increased
0.08% increased
0.15% increased

Decrease
in Running
Time
67%
90%
89%
70%
72%
53%
54%
11%
73%
65%
77%
41%
82%
90%
79%
75%
70%
83%
83%
30%
92%
99%
98%
98%

T HE EFFECT OF D SLM FOR NODE DELETIONS

Algorithm

Dataset

Base (#
of Nodes)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube
Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube

81,306
81,306
81,306
81,306
107,614
107,614
107,614
107,614
1,157,828
1,157,828
1,157,828
1,157,828
81,306
81,306
81,306
81,306
107,614
107,614
107,614
107,614
1,157,828
1,157,828
1,157,828
1,157,828

#
of
Nodes
Added
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000

Change in Modularity Value
0.06% increased
no change
0.02% increased
0.03% decreased
0.35% decreased
0.37% decreased
0.35% decreased
0.35% decreased
0.28% decreased
0.03% decreased
0.21% decreased
0.09% decreased
0.05% increased
0.02% decreased
0.02% increased
0.05% decreased
0.36% decreased
0.35% decreased
0.36% decreased
0.34% decreased
0.29% decreased
0.01% decreased
0.19% decreased
0.07% decreased

Decrease
in Running
Time
85%
79%
87%
28%
90%
78%
75%
73%
88%
87%
73%
73%
92%
81%
87%
28%
90%
78%
84%
69%
99%
99%
99%
98%

In general, dSLM does not decrease the number iterations
of convergence of SLM, however, it decreases the number
of node movements needed in each iteration of SLM. This
indicates that each iteration of dSLM runs faster than each
iteration of SLM. Therefore, overall running time of dSLM is
less than SLM’s overall execution time. The overall results can
be seen in Table I, II, III and IV.
In order to be able to decrease the overall running time
of dSLM algorithm even more, we added another parameter
called expected modularity value that enables the algorithm
stop when it is reached. We named this kind of new algorithm
as dSLMEVS and made same experiments on it with this
new parameter set to the modularity value resulted from SLM
algorithm. By this new algorithm and parameter, we aimed to
decrease running time as much as possible while keeping the
modularity value unchanged or increased. We reached our aim
and decreased running time drastically and keep modularity
value unchanged or increased as seen in all of the tables.
VI.

C ONCLUSION

Waltman & Van Eck proposed and implemented the SLM
algorithm in order to detect communities in large networks.
We extended their implementation to define the community
structure in a dynamic rather than static way. We made use of
the past calculation results of the SLM algorithm in order to
calculate the current networks community structure. This usage
is the main extension and contribution to the SLM algorithm.
In the basics, it is what extends the SLM to be dSLM.
To sum up, we extended SLM to be incremental and
dynamic by using the historical results of community detection
algorithms for the initial community assignments of the nodes.
Thus, the number of node movement actions tried to maximize
the modularity value is decreased. This led to decrease in
running time of the algorithms. Moreover, it can lead to
decrease in number of iterations to converge. Thus, if the
algorithms run with a constant number of iterations parameter,
the modularity value may result as increased.
ACKNOWLEDGMENT
This research was supported partially by USAF Grant
FA9550-15-1-0004.
R EFERENCES
[1]

[2]

[3]

[4]

[5]

A. Clauset, M. Newman, and C. Moore, “Finding community
structure in very large networks,” Physical Review E, vol. 70,
p. 066111, 2004. [Online]. Available: http://www.citebase.org/cgibin/citations?id=oai:arXiv.org:cond-mat/0408187
R. Guimera, M. Sales-Pardo, and L. Amaral, “Modularity from fluctuations in random graphs and complex networks,” Physical Review E,
vol. 70, no. 2, p. 025101, 2004.
J.
Duch
and
A.
Arenas,
“Community
detection
in
complex networks using extremal optimization,” Physical
Review E, vol. 72, p. 027104, 2005. [Online]. Available:
http://www.citebase.org/abstract?id=oai:arXiv.org:cond-mat/0501368
M. Newman, “Finding community structure in networks using the
eigenvectors of matrices,” Physical Review E, vol. 74, no. 3, p. 36104,
2006.
S. Lehmann and L. K. Hansen, “Deterministic modularity optimization,”
The European Physical Journal B, vol. 60, no. 1, pp. 83–88, 2007.
[Online]. Available: http://dx.doi.org/10.1140/epjb/e2007-00313-2

[6]
[7]
[8]

[9]

[10]

[11]

[12]

[13]

[14]
[15]

[16]
[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]
[25]

[26]

[27]

J. Lee, S. P. Gross, and J. Lee, “Mod-csa: Modularity optimization by
conformational space annealing,” CoRR, vol. abs/1202.5398, 2012.
V. Blondel, J. Guillaume, R. Lambiotte, and E. Mech, “Fast unfolding
of communities in large networks,” J. Stat. Mech, p. P10008, 2008.
R. Rotta and A. Noack, “Multilevel local search algorithms for modularity clustering.” ACM Journal of Experimental Algorithmics, vol. 16,
2011.
L. Waltman and N. J. van Eck, “A smart local moving algorithm
for large-scale modularity-based community detection.” CoRR, vol.
abs/1308.6604, 2013.
U. Brandes, D. Delling, M. Gaertler, R. Goerke, M. Hoefer,
Z. Nikoloski, and D. Wagner, “On modularity clustering,” IEEE Transactions on Knowledge and Data Engineering, vol. 20, no. 2, pp. 172–
188, 2008.
D. Aloise, S. Cafieri, G. Caporossi, P. Hansen, L. Liberti, and S. Perron,
“Column generation algorithms for exact modularity maximization in
networks,” Physical Review E, vol. 82, no. 4, article, pp. –, jan 2010.
G. Xu, S. Tsoka, and L. G. Papageorgiou, “Finding community
structures in complex networks using mixed integer optimisation,” The
European Physical Journal B, vol. 60, no. 2, pp. 231–239, 2007.
[Online]. Available: http://dx.doi.org/10.1140/epjb/e2007-00331-0
A. Clauset, M. E. J. Newman, and C. Moore, “Finding
community structure in very large networks,” Phys. Rev.
E, vol. 70, p. 066111, Dec 2004. [Online]. Available:
http://link.aps.org/doi/10.1103/PhysRevE.70.066111
P. Holme and J. Saramäki, “Temporal networks,” Physics Reports, vol.
519, no. 3, pp. 97–125, 2012.
T. Yang, Y. Chi, S. Zhu, Y. Gong, and R. Jin, “Detecting communities
and their evolutions in dynamic social networks - a bayesian approach.”
Machine Learning, vol. 82, no. 2, pp. 157–189, 2011.
X. Tang and C. C. Yang, “Dynamic community detection with temporal
dirichlet process.” in SocialCom/PASSAT. IEEE, 2011, pp. 603–608.
D. Chakrabarti, R. Kumar, and A. Tomkins, “Evolutionary clustering,”
in Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining, ser. KDD ’06. New
York, NY, USA: ACM, 2006, pp. 554–560. [Online]. Available:
http://doi.acm.org/10.1145/1150402.1150467
M.-S. Kim and J. Han, “A particle-and-density based evolutionary
clustering method for dynamic networks.” PVLDB, vol. 2, no. 1, pp.
622–633, 2009.
D. Greene, D. Doyle, and P. Cunningham, “Tracking the evolution of
communities in dynamic social networks.” in ASONAM, N. Memon and
R. Alhajj, Eds. IEEE Computer Society, 2010, pp. 176–183.
P. Brodka, S. Saganowski, and P. Kazienko, “Group evolution discovery
in social networks,” in Advances in Social Networks Analysis and
Mining (ASONAM), 2011 International Conference on, 2011, pp. 247–
253.
T. Dinh, Y. Xuan, and M. Thai, “Towards social-aware routing in
dynamic communication networks,” in Performance Computing and
Communications Conference (IPCCC), 2009 IEEE 28th International,
Dec 2009, pp. 161–168.
T. Aynaud and J.-L. Guillaume, “Static community detection algorithms
for evolving networks,” in Modeling and Optimization in Mobile, Ad
Hoc and Wireless Networks (WiOpt), 2010 Proceedings of the 8th
International Symposium on, May 2010, pp. 513–519.
M. Newman, “Modularity and community structure in networks,” Proceedings of the National Academy of Sciences, vol. 103, no. 23, pp.
8577–8582, 2006.
S. Fortunato, “Community detection in graphs,” Physics Reports, vol.
486, pp. 75–174, 2010.
M. E. J. Newman, “Analysis of weighted networks,” Phys. Rev.
E, vol. 70, no. 5, p. 056131, Nov. 2004. [Online]. Available:
http://pre.aps.org/abstract/PRE/v70/i5/e056131
A. Arenas, J. Duch, A. Fernandez, and S. Gmez, “Size reduction of complex networks preserving modularity,” CoRR, vol.
abs/physics/0702015, 2007.
E. A. Leicht and M. E. J. Newman, “Community structure in directed
networks,” Phys. Rev. Lett., vol. 100, no. 11, p. 118703, Mar. 2008.
[Online]. Available: http://prl.aps.org/abstract/PRL/v100/i11/e118703

[28]

[29]
[30]

[31]

[32]

S. Fortunato and M. Barthlemy, “Resolution limit in community detection,” Proceedings of the National Acadamy of Sciences of the United
States of America (PNAS), vol. 104, no. 1, pp. 36–41, 2007.
J. Reichardt and S. Bornholdt, “Statistical mechanics of community
detection,” Arxiv preprint cond-mat/0603718, 2006.
V. A. Traag, P. Van Dooren, and Y. Nesterov, “Narrow scope for
resolution-limit-free community detection,” Physical Review E, vol. 84,
no. 1, p. 016114, 2011.
L. Waltman, N. J. van Eck, and E. C. Noyons, “A unified
approach to mapping and clustering of bibliometric networks,”
Journal of Informetrics, vol. 4, no. 4, pp. 629–635, 2010. [Online]. Available: http://www.sciencedirect.com/science/article/B83WV50RFN28-1/2/809f89176e8076cac3862b6589bc6fd5
A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, and B. Bhattacharjee, “Measurement and Analysis of Online Social Networks,” in
Proceedings of the 5th ACM/Usenix Internet Measurement Conference
(IMC’07), San Diego, CA, October 2007.

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/251795842

Functional	ìAJAXî	in	Secure	Synchronous
Programming
Article

CITATIONS

READS

0

17

2	authors,	including:
Ramesh	Bharadwaj
United	States	Naval	Research	Laboratory
60	PUBLICATIONS			1,058	CITATIONS			
SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Ramesh	Bharadwaj	on	09	March	2014.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

Functional “AJAX” in Secure Synchronous Programming
Ramesh Bharadwaj

Center for High Assurance Computer Systems
Naval Research Laboratory
4555 Overlook Avenue
Washington, District of Columbia 20375-5337

ramesh@itd.nrl.navy.mil

Supratik Mukhopadhyay

Department of Computer Science
West Virginia University
Morgantown, West Virginia 26506-6109

supratik@csee.wvu.edu

ABSTRACT

1. INTRODUCTION

AJAX (Asynchronous Javascript and XML) is a combination of
technologies aimed at supporting an improved user/application interactivity in the context of web-based service-oriented computing.
Based on the XMLHttpRequest API, AJAX provides an engine for
handling service invocations asynchronously while interacting with
other applications/users in the foreground. The AJAX combination
of technologies have already been deployed in popular applications
like Google Maps. The adherence to XML-based format for data
exchange makes this combination of technologies and similar other
frameworks suitable for deployment in service-oriented architectures based on lightweight services (REST or Web) augmenting
existing architectures with increased capabilities of interaction.
While AJAX promises improved interaction capabilities, it is
also accompanied by its baggage of problems. The lack of formal
semantics makes it difficult to understand and validate the functionalities that an application is supposed to provide. The support for
individual component technologies of AJAX (e.g., XMLHttpRequest, Javascript etc.) are different for different infrastructures (and
browsers). The adoption of Javascript (an interpreted scripting language) makes it inefficient for running heavyweight processes. Besides, the source code must be downloaded by the client for execution which raises concerns in security and intellectual property
issues. These problems render the existing AJAX framework unsuitable for deployment in mission-critical enterprise applications.
In this paper, we present an “AJAX”-like framework in an eventdriven secure synchronous programming environment. More precisely, we present a synchronous programming programming language called SOL (Secure Operations Language) that has capabilities for handling service invocations asynchronously, strong typing (including dynamic) to ensure enforcement of information flow
and security policies and the ability to deal with failures (both benign and byzantine) of components. While our framework provides
“AJAX”-like functionalities in a synchronous programming environment, unlike AJAX, it is not a combination of disparate technologies. As opposed to the AJAX framework, it is supported by
formal operational semantics. Applications written in our framework can be verified using formal static checking techniques like
theorem proving. The framework runs on the top of the SINS
(Secure Infrastructure for Networked Systems) infrastructure developed at the Naval Research Laboratory.

Service-oriented architectures (SOAs) [23] are becoming more
and more common as platforms for implementing large scale distributed applications. In an SOA, applications are built by combining services, which are platform independent components running on different hosts of a network. They are now being deployed in mission-critical applications that include space, healthcare, electronic commerce, and military. Client requests are met
by on-demand discovery of a set of suitable services which, when
appropriately composed, will satisfy the client’s service requirements. Delivery of services to clients is governed by service level
agreements (SLAs) which additionally specify the quality of service (QoS) that the service provider needs to guarantee and the
appropriate penalties for their violation. QoS constraints that a
service provider guarantees may include security, timeliness, and
availability. Such guarantees are difficult to satisfy when services
are spatially distributed over a network which is subject to active
attacks, network congestion, and link delays. Such attacks and failures, coupled with the need for migration of services and clients,
poses a formidable challenge in delivering services that meet the
SLAs.
SOA’s typically are characterized by the high level of interactivity between applications (i.e., between services, brokers, and
clients) and users. AJAX [12] (Asynchronous Javascript and XML)
is a combination of technologies aimed at supporting an improved
user/application interaction in the context of web-based serviceoriented computing (or more generally development of web applications). Based on the XMLHttpRequest API, AJAX provides
an engine for handling service invocations asynchronously while
interacting with other applications/users in the foreground. The
AJAX combination of technologies have already been deployed in
popular applications like Google Maps. The adherence to XMLbased format for data exchange makes this combination of technologies and similar other frameworks suitable for deployment in
service-oriented architectures (using a web browser as a client interface) based on lightweight services (REST [15] or Web) augmenting existing architectures with increased capabilities of interaction.
While AJAX promises improved interaction capabilities, it does
come with its baggage of problems. The lack of formal semantics makes it difficult to understand and validate the functionalities that an application is supposed to provide. The support for
individual component technologies of AJAX (e.g., XMLHttpRequest, Javascript etc: XML Namespaces are not well supported
by Internet Explorer) are different for different infrastructures (and
browsers). The adoption of Javascript (an interpreted scripting language) makes it inefficient for running heavyweight processes. Besides, the source code must be downloaded by the client for exe-

Keywords
AJAX, SOA
Copyright is held by the author/owner(s).
WWW2006, May 22–26, 2006, Edinburgh, UK.
.

cution which raises concerns in security and intellectual property
issues. These problems render the existing AJAX framework unsuitable for deployment in mission-critical enterprise applications
where formal guarantees with respect to functionalities and security
are of utmost importance.
In this paper, we present an “AJAX”-like framework in an eventdriven synchronous programming [4] environment (a’ la’ LUSTRE
[17], SCR [8], and Esterel [5]). More precisely, we present a synchronous programming language SOL (Secure Operations Language)
that has capabilities of handling service invocations asynchronously,
provides strong typing to ensure enforcement of information flow
and security policies, and has the ability to deal with failures (both
benign and byzantine) of components. In the synchronous programming paradigm, the programmer is provided with an abstraction that respects the synchrony hypothesis, i.e., one may assume
that an external event is processed completely by the system before the arrival of the next event. One might wonder how a synchronous programming paradigm can be effective for dealing with
widely distributed systems where there is inherent asynchrony. The
answer may seem surprising to some, but perfectly reasonable to
others: We have shown elsewhere [10] that under certain sufficient
conditions (which are preserved in our case) the synchronous semantics of a SOL application are preserved when it is deployed on
an asynchronous, distributed infrastructure. While our framework
provides “AJAX”-like functionalities in a (functional) synchronous
programming environment, unlike AJAX, it is not a combination
of disparate technologies. As opposed to the AJAX framework, it
is supported by formal operational semantics. The individual modules follow a “publish-subscribe” pattern of interaction while asynchronous service invocations akin to the XMLHttpRequest (API)
are provided continuation-passing-based [3] semantics. The design
of SOL was heavily influenced by the design of SAL (the SCR
Abstract Language), a specification language based on the SCR
Formal Model [18]. Applications written in our framework can
be verified using formal static checking techniques like theorem
proving. We provide both a static and a dynamic type system to
ensure respectively (1) static type soundness and (2) to ensure runtime type soundness in the presence of third party (possibly COTS)
component services that may undergo reconfigurations at runtime
due to network faults or malicious attacks. The framework runs on
the top of the SINS (Secure Infrastructure for Networked Systems)
infrastructure developed at the Naval Research Laboratory. SINS
is built on the top of the Spread toolkit [1] which provides a high
performance virtual synchrony messaging service that is resilient to
network faults. A typical SINS system comprises SINS Virtual Machines (SVMs), running on multiple disparate hosts, each of which
is responsible for managing a set of modules on that host. SVMs
on a host communicate with SVMs on other hosts using the secure
group communication infrastructure of Spread. SINS provides the
required degree of trust for the modules, in addition to ensuring
compliance of modules with a set of requirements, including security policies.
The rest of the paper is organized as follows. Section 2 presents
related work. Section 3 provides a brief description of the SOL
language along with several illuminating examples. Section 4 provides a brief description of the SINS platform. Formal semantics
of SOL as well as a static type system are provided in Section 5.
Section 6 concludes the paper.

2.

RELATED WORK

Service-based systems (some times identified with web services
even though the scope of service-based systems is much broader)
have traditionally adopted document-oriented SOAP-based [23] mes-

saging for communicating XML data across a network. SOAP, by
default, is bound to the HTTP [11] transport layer. HTTP does not
provide any means of correlating requests with responses. SOAP
over HTTP provides a basic one-way synchronous communication
framework on the top of which other protocols like request/response
type RPC [11] can be implemented. SOAP and WSDL specifications are not executable; the protocol adopted by a particular application needs to be supported by the underlying runtime infrastructure. SOAP, as it is, does not support interaction patterns like
request/callback, publish/subscribe or asynchronous store and forward messaging. The definition of SOAP can be extended to provide such interaction patterns; such extensions require providing
new semantics to an existing system.
In contrast, our framework is based on the synchronous programming language SOL. In SOL, the message passing between modules (henceforth we will use the term agent for module instances)
is based on a (push) publish-subscribe. A module listens to those
“controlled variables” of another module that it “subscribes to” by
including them as its “monitored variables”. A module receives the
values of its monitored variables as input and computes a function
whose output can change the values of its controlled variables. Service invocations (both synchronous and asynchronous) needed to
compute the function are dealt uniformly using continuation passing. SOL agents run on the SINS platform which is built on the
top of the Spread toolkit that provides guaranteed message delivery
and resilience to network faults.
Traditionally, BPEL4WS [14] and semantic web-based frameworks (e.g., OWL-S) corresponding to business processes have been
used for describing, modeling and executing workflows for web
service-based systems. It is difficult in such frameworks to interoperate in networks involving sensors and other physical devices
having complex dynamical behavior, for continuously accessing information, monitoring the environment and reacting to changes in
it.
Both BPEL4WS and OWL-S [2] lack satisfactory formal operational semantics - while OWL-S specifications are assigned semantics based on first order logic, such axiomatic semantics are
not helpful in building a programming model with formal operational semantics. In particular, both BPEL4WS and OWL-S lack
a satisfactory programming model with formal semantics. Lack
of a proper programming model with formal operational semantics
makes it difficult to understand, predict and debug the behavior of
service compositions described in such languages. As a result it
is risky to deploy an application based on such a framework in a
mission-critical environment.
Both BPEL4WS and OWL-S do not provide an effective framework for modeling non-functional properties/QoS goals such as
security (policies), context-aware reconfiguration, time-deadlines,
availability, physical/logical migration of processes etc. Security
policies can be expressed in the BPEL4WS framework using external plug-ins like WS-Security. Embedding external plug-ins respecting different standards creates problems in interoperability.
We believe that security should be provided as an integral component of service-based systems in order to free application developers of security concerns.
Service-based systems executing workflows specified by BPEL4WS
(or OWL-S) processes do not provide any techniques for automatically reconfiguring a workflow dynamically in response to fast
changing contexts.
In our framework, workflows are expressed as a collection of
SOL modules with “proper plumbing” to ensure desired coordination. As already stated above, the SOL language is supported by
formal operational semantics. Besides the event-driven publish-

subscribe-based interaction between the individual modules make
SOL ideal for programming service-based systems that are deployed
in networks involving sensors and other physical devices having
complex dynamical behavior. The SINS platform provides the required degree of trust for the modules, in addition to ensuring compliance of modules with a set of requirements, including security
policies. Dynamic reconfiguration of workflows can be obtained
using a “hierarchical plumbing” a’ la’ [27].
AJAX provides an engine that acts as a client-side brokerage and
orchestration point for web services and provides support for the
XMLHttpRequest API, XSLT, DOM and Javascript. Calls to the
services are handled asynchronously using the XMLHttpRequest
API. Responses from the server are handled by Javascript code
running at the client end. AJAX can utilize the XML and XSL services available in browsers to provide a highly interactive interface
for web services. AJAX libraries are available for platforms such
as .NET. In our case, the SOL agents can be directly deployed on
the SINS platform that acts as a coordination point for the different
agents and the services. We have already outlined the shortcomings
of the conventional AJAX framework in the Introduction and have
also outlined how our framework ameliorates them. In [25], the
authors use a synchronous framework for globally asynchronous
designs. However, their framework is more suited to a hardware
design environment rather than a large scale distributed computing
one.
The nesC [16] programming language at U.C. Berkeley has been
designed for programming networked embedded systems. It supports asynchronous calls to components using events to signify the
completion of a call. In the polyphonic C [22] programming language, asynchronous method calls are supported using queues. A
set of methods at the server end defines a “chord”. A method call is
delayed until all methods in the corresponding chord are invoked.
The communicating concurrent processes, the dominant paradigm
for distributed application development, has remained unchallenged
for almost 40 years. Not only is this model difficult to use for the
average developer, but in addition it fails as a paradigm for designing applications that must satisfy critical requirements such as
real-time guarantees [21]. Therefore, applications developed using conventional programming models are vulnerable to deadlocks,
livelocks, starvation, and synchronization errors. Moreover, such
applications are vulnerable to catastrophic failures in the event of
hardware or network malfunctions. Here we present an alternative
approach. We embed an “AJAX”-like framework in an event-driven
synchronous programming environment (a’ la’ LUSTRE [17], SIGNAL [17] SCR [8], and Esterel [5]). As opposed to other synchronous programming languages like ESTEREL, LUSTRE and
SIGNAL, SOL is a synchronous programming language for distributed applications.
Preliminary versions of SOL and SINS have been introduced
in [6, 7]. The current paper extends those versions by providing
asynchronous service-invocation management functionalities, type
systems for safe information down grading and secure information
flow. Besides, it provides operational semantics for the SOL language.

3.

SOL: THE SECURE OPERATIONS LANGUAGE

A module is the unit of specification in SOL and comprises of
type definitions, flow control rules, unit declarations, unit conversion rules, variable declarations, service declarations, assumptions
and guarantees, and definitions. A module in SOL may include
one or more attributes. The attribute deterministic declares

the module as being free of nondeterminism (which is checked by
the SOL compiler). Attribute reactive declares that the module will not cause a state change or invoke a method unless its
(visible) environment initiates an event by changing state or invoking a method (service); moreover, the module’s response to an
environmental event will be immediate; i.e., in the next immediate step. The attribute continuation declares that the module
will serve as a continuation for some (external) service invocation.
Each (asynchronous) external service invocation is managed by a
continuation module that receives the response for the invocation
and informs the module that originally invoked the service of the
response as soon as it arrives. As defined previously, an agent is
a module instance. In the sequel, we use module and agent interchangeably.
C-style comments are supported – all text between an opening
“/*” and closing “*/” is ignored. Alternately, comments may begin with “//” and terminate by the end of the line. Comments may
be nested. The module definition comprises a sequence of sections,
all of them optional, each beginning with one or more keywords.
“Integer”, “Real”, “Boolean”, and “String” are the built-in data
types in SOL. User-defined types as well as enumerated types can
be defined in the type definitions section. Each entry in this section consists of an identifier for the type, followed by its definition,
which may be in terms of the built-in types, their subranges, or
enumerated types. Besides, this section allows the user to declare
“secrecy” types (e.g., secret, classified, unclassified etc.) in order
to enforce information flow policies and prevent unwanted downgrading of sensitive information from “secret” variables to “public” variables. The flow control rules section provides rules that
govern the downgrading/flow of information between variables of
different “secrecy” types (e.g., the rule unclassified = classified,
signifies that a variable of type unclassified can be assigned to a
variable of type classified, i.e., information flow from an unclassified to a classified variable is allowed). The flow control rules can
be used to compute the secrecy types of expressions from those of
its constituent variables. If not specified in the flow control section, information flow between variables/expressions with different
secrecy types is allowed only in the presence of explicit coercion
provided by the programmer. These policies are enforced statically
by a static type system. The unit declaration section declares units
for the physical quantities that the module monitors and manipulates (e.g., lb, kg, centigrade etc.). This section provides conversion (coercion) rules between the different units (e.g., kg=2.2 lb).
Units of expressions can be computed from the units of their constituent subexpressions. The variable declaration section for reactive/deterministic modules is subdivided into five subsections. The
continuation variable declaration subsection defines continuation
variables that will be used for service invocations. There will be
one continuation variable for each service invocation in a module.
The type “continuation” before a variable designates it as a continuation variable (e.g., continuation cont;). Corresponding to each
continuation variable, there will be a continuation module handling
the service invocation associated with that variable. The other four
subsections declare the “monitored” variables in the environment
that an agent monitors, the “controlled” variables in the environment that the agent controls, “service” variables that only external
service invocations can update, and, “internal” variables introduced
to make the description of the agent concise. The monitored variables section can include failure variables that are boolean variables indicating the failure of other modules (e.g., the declaration
failure boolean I; declares a boolean variable I that will
become true if a module named I in the environment fails). A
variable declaration can specify the unit (declared in the unit dec-



laration section) of the physical quantity that it is supposed to assume values for (e.g., int weight unit lb;). Assignment of a variable/expression with a unit to a variable with unit is allowed
only if it is specified in the unit conversion rules section. In that
case, the value of the variable/expression is converted to the unit
using the corresponding conversion rule before being assigned to a
variable with unit . The declaration of a monitored variable can
be accompanied by failure handling information that may specify it
being substituted in all computations by another monitored variable
in case the module publishing it fails (e.g., the declaration integer x on I y specifies that the monitored variable y should
replace the variable x if the failure variable I corresponding to the
module named I in the environment is true). The service declarations section declares the methods that are invoked within a module along with the services providing them. It also describes for
each method the preconditions that are to be met before invoking
the method as well as the post conditions that the return value(s)
from the method is/are supposed to respect. The preconditions and
postconditions consist of conjunctions of arithmetic constraints as
well as type expressions. A type expression is a set of atomic type
judgements of the form
where is a variable and is a
type. These conditions are enforced dynamically under a runtime
environment.
The assumptions section includes assumptions upon which
correct operation of the agent depends. Execution aborts when
any of these assumptions are violated by the environment resulting in the failure variable corresponding to that agent to be set to
true. The required safety properties of the agent are specified in the
guarantees section. Variable definitions, provided as functions
or more generally relations in the definitions section, specify
values of internal and controlled variables. A SOL module specifies the required relation between monitored variables, variables in
the environment that the agent monitors, and controlled variables,
variables in the environment that the agent controls. Additional internal variables are often introduced to make the description of the
agent concise. In this paper, we often distinguish between monitored variables, i.e., variables whose values are specified by the
environment, and dependent variables, i.e., variables whose values
are computed by a SOL module using the values of the monitored
variables as well as those returned by the external service invocations. Dependent variables of a SOL module include the controlled
variables, service variables, and internal variables. SOL provides
type constructors such as arrays and tuples. In this paper, we shall
not elaborate on the tuple and array constructs of SOL (see [24] for
details).









3.1 Events
SOL borrows from SCR the notion of events [18]. Informally,
an SCR event denotes a change of state, i.e., an event is said to
occur when a state variable changes value. SCR systems are eventdriven and the SCR model includes a special notation for denoting
them. The following are the notation for events that can trigger
reactive/deterministic modules. The notation @T(c) denotes the
event “condition became true”, @F(c) denotes “condition became false” , @Comp(cont) denotes that “the result of the service
invocation associated with the continuation variable cont (i.e., the
service invocation in which cont was passed) is available”, and
@C(x) the event “the value of expression x has changed”. These
constructs are explained below. In the sequel, PREV(x) denotes




@T(c)
@F(c)
@C(c)





	

the value of expression x in the previous state.





      !
"#

    %&$ 



Events may be triggered predicated upon a condition by including a
“when” clause. Informally, the expression following the keyword
when is “aged” (i.e., evaluated in the previous state) and the event
occurs only when this expression has evaluated to true. Formally,
a conditioned event, defined as


 ' # !()*+,.denotes the event “condition 
 became true when condition /
@T(c) when d

was true in the previous state”. Conditioned events involving
the @F and @C constructs are defined along similar lines. The event
@Comp(cont) is triggered by the environment in which the agent
is running and is received as an event by the agent whenever the
result of a service invocation is received by the continuation module
associated with the continuation variable cont that was passed as
a continuation while invoking the service. We will define the event
@Comp in terms of associated continuation modules in Section 3.3.
Each controlled and internal variable of a module has one and
only one definition which determines when and how the variable
gets updated. All definitions of a module implicitly specify a desuch that a variable depends on variable
pendency relation
(i.e.,
) if and only if appears in the definition of
. Note that variable may depend on the previous values of other
variables (including itself) which has no effect on the dependency
relation. A dependency graph may be inferred from the dependency relation by taking each variable in the module to be a node
and including an edge from to if depends on 1 . It is required
that the dependency graph of each module is acyclic.
Intuitively, the execution of a SOL program proceeds as a sequence of steps, each initiated by an event (known as the triggering
event). Each step of a SOL module comprises a set of variable
updates and service invocations that are consistent with the depenof that module. Computation of each step of a
dency relation
module proceeds as follows: the module or its environment nondeterministically initiates a triggering event; each module in the
system responds to this event by updating all its dependent (i.e.,
internal, service, and controlled) variables. In the programmer’s
view all updates and service invocations of the system are assumed
to be synchronous (similar to the Synchrony Hypothesis of languages such as Esterel, LUSTRE, etc. [17]) – it is assumed that the
response to a triggering event is completed in one step, i.e, all updates to dependent variables and all method calls are performed by
the modules of the system before the next triggering event. Moreover, all updates are performed in an order that is consistent with
the partial order imposed by the dependency graph.

5

4

 4 - 5 76 1 132 2

5

4

4 5 4

0

4

5

132

3.2 An Automated Therapeutic Drug Monitoring System in SOL
In this subsection, we present a (part of a) skeleton in SOL of a
distributed automated therapeutic drug monitoring system in a hospital. We will use this as a running example later in this paper. A
scenario of the operation of the system is depicted in Figure 1. A
sensor (can be a nurse sitting at a terminal) at a patient’s bed in the
hospital monitors the patient’s vital data (e.g., saturation, heartbeat,
blood pressure etc.). As soon as the vital data indicate that the patient’s condition is critical, the sensor reports the vital data to the
1
The notion of a dependency relation is easily extended to the entire
system.

deterministic reactive module doctor {
type definitions
dosage = Integer;
condition={critical,not_critical};
units
lb_per_sqinch, mg, cc;
unit conversion rules
mg=cc;
services
dosage pharmserv:compute_dosage(x,y,z),
pre= x::Integer, y::Integer, z:: Integer
-- post=true;

Figure 1: Automated therapeutic drug monitoring scenario

monitored variables
Integer heartrate;
Integer pressure unit lb_per_sqinch;
Integer saturation;
condition patient_cond;
service variables
dosage c_dosage unit mg;

contination variables
central hospital server along with a report on the patient’s condition
continuation cont;
(critical). The central hospital server contacts the patient’s doctor
(e.g., by sending a message to her palmpilot) with the patient’s vicontrolled variables
tal data and the report (critical) from the sensor. The doctor can
dosage output_dosage unit cc;
look up a drug appropriate for the patient’s condition and invoke
Boolean oxygen;
a service provided by the pharmaceutical company (producing the
drug), with the vital data of the patient, that computes the correct
definitions
dosage corresponding to the patient’s current state. Further, if the
// definitions of controlled
patient’s saturation is below a certain threshold, the doctor can or//and service variables
c_dosage = initially null then
der her to be put on oxygen. The doctor communicates her response
if{
(dosage, oxygen) to the central hospital server which in turn com[] @C(patient_cond) && @C(heartrate)
municates it to the nurse (patient sensor and actuator) that attends
&& @C(pressure)
the patient by administering the required dosage of the drug or by
-> pharmserv:
putting her on oxygen. The patient sensor (or the nurse) reports
compute_dosage(
to the hospital service whenever the state of the patient changes
heartrate,pressure,
saturation)
(e.g., turns from critical to noncritical) which in turn reports to
ˆcont;
the doctor for appropriate action. Due to space limitations, we
}// service invocation
show here only the SOL module running on the doctor’s palmpilot in Figure 2. The complete therapeutic drug monitoring system
output_dosage= initially null then
consists of SOL modules for the “doctor”, the “hospital server”
if{
and the “nurse/patient sensor and actuator”. The modules translate
[] @Comp(cont)-> c_dosage;
} //update of controlled variable
directly into Java and runs unmodified on the SINS middleware.
oxygen= initially false then
Interested readers may refer to [17] to compare SOL with other
if{
synchronous programming languages such as Esterel, Argos, LUS[] @T(saturation<65) -> true;
TRE, and SIGNAL.
[] @T(saturation>90) -> false;
The doctor module is implemented as a deterministic reac}
tive module. We identify four monitored variables – heartrate,
}
pressure (unit lb/sqinch), saturation and patient condition
corresponding to the vital data heart rate, blood pressure and saturation of the patient as well as the condition of the patient (critical
Figure 2: Doctor module in SOL.
or noncritical) that the module obtains from the hospital server. We
also identify a service variable c dosage (unit mg) that is defined by invoking the pharmaceutical service, a continuation variable that cont that is passed as a continuation while invoking the
service, and two controlled variables output dosage (unit cc)
and oxygen that correspond respectively to the dosage and the de-

cision whether to put the patient on oxygen or not sent back to the
name of the method invoked, var list is the list of variables
hospital server. The hospital server listens to these two controlled
passed as arguments to the method, and, cont is the passed convariables (among others). We also identify a service invocation
tinuation variable. In this case, the service variable depends on the
pharmserv:compute dosage that invokes the compute dosage variables in var list. For each service invocation in a module,
method of the pharmaceutical service named (and addressed) pharmserva distinct continuation variable is used. Internally, corresponding
with the vital data of the patient as arguments and the variable
to each continuation variable there is a continuation module hancont being passed as a continuation. The service invocation is
dling the result of the service invocation in which the variable is
used to obtain the required dosage of the patient and defines the
passed. A continuation module has the same structure as the reacservice variable c dosage. The preconditions for invoking the
tive/deterministic ones except that it can have an additional subsection in the variable declaration section: channel variables. Channel
service provided in the services section specify that the types of
all the three formal parameters x, y and z should be Integer while
variables receive values from external services. In addition, it can
have another section called triggers that lists actions in the environthe postcondition always holds true. The return value from the service invocation should be of type dosage. The unit conversion
ment that the module can trigger. Actions in the trigger section can
times a cc so that
be defined in the same way as variables. Along with the usual notarules section defines an mg to be equal to
the value of the variable c dosage is to be multiplied by
(by
tion for events as in reactive/deterministic modules, a continuation
module can have an additional event denoted by @Rec(Chan),
the runtime environment) before being assigned to the controlled
variable output dosage.
where Chan is a channel variable, which is triggered as soon as
The module doctor responds to a triggering event2 by upa value is received from an external service on the variable Chan.
dating its dependent variables in compliance with the dependency
A continuation module for a service invocation is generated inter(partial) order. One possible order is oxygen
nally automatically by the SOL compiler from the SOL definitions
saturation,
and is kept away from the view of the programmer. For example,
c dosage heartrate, c dosage pressure, c dosage
the continuation module corresponding to the service invocation in
saturation, c dosage cont, and output dosage
Figure 2, where the service variable c dosage (with type dosage)
c dosage.
defined by a two-state definition is given below.

898:

;

;

;

88:

;

;

;

3.3 SOL Definitions

The definitions section is at the heart of a SOL module.
The syntax of SOL definitions is shown in Figure 3. This section
determines how each internal, service, and controlled variable of
the module is updated in response to events (i.e., state changes)
generated either internally or by the module’s environment.
A variable definition is either a one-state or a two-state definition. A one-state definition, of the form
(where
is an expression), defines the value of variable x in terms of the
values of other variables in the same state. A two-state variable
definition, of the form
(where
is a two-state expression), requires the initial value of x to
equal expression
; the value of x in each subsequent state is
determined in terms of the values of variables in that state as well
as the previous state (specified using operator PREV or by a when
clause).
A conditional expression, consisting of a sequence of branches
“[] guard
expression”, is introduced by the keyword “if” and
enclosed in braces ("{" and "}"). A guard is a boolean expression. The informal semantics of the conditional expression
expr
expr
is defined along the lines of Dijkstra’s
guarded commands [13] – in a given state, its value is equivalent
to expression expr whose associated guard is true. If more than
one guard is true, the expression is nondeterministic. It is an error
if none of the guards evaluates to true, and execution aborts setting the failure variable corresponding to that module to true. The
case expression
expr
expr
is
equivalent to the conditional expression
expr
expr
. The conditional expression and the case expression may optionally have an otherwise
clause with the obvious meaning.

< =Q>

HKJHL

 <  =?>
< =?>
 '
 A@ BC@ED,@ 4FF9GIHKJHL D9MON9BP< =?>

;

Z U W[X\ ;

a

\^]
]
]`_

continuation module cont{
type definitions
dosage = Integer;
controlled variables
dosage c_dosage;
channel variables
dosage Chan;
triggers
Boolean @Comp(cont);
definitions
@Comp(cont) =
if{
[]@Rec(Chan)-> true;
}
c_dosage= initially null then
if{
[]@Rec(Chan)->Chan;
}

@SR3TVU WYXQZ ;

Xa


 4b N3< =?> cT U W[d,Z ; @ER3Z T7U W[dU W\K< ; ce^\ ]
d ]
Z ]	 _
=?>
;
Z U WfK<  =?> cgd \  ; \ ]
]
]`_

3.4 Service Invocation
A service variable is defined by a one-state or a two-state definition in terms of a service invocation expression (service invocation).
A service invocation expression is of the form A:B(var list)ĉont
where the identifier A is the name/URL of the service, B is the
2
Since doctor is reactive, all triggering events are external to
the module.

}
When the agent doctor defining the service variable c dosage
is executed, the agent environment invokes the service by sending
it a message. The preparation of this message involves marshaling the arguments as well as the continuation, which includes information about the channel Chan on which the result of the service invocation is to be returned. Once the service returns the result on the channel Chan, the guard @Rec(Chan) in the continuation module associated with the continuation variable becomes
true. This event results in the controlled variablec dosage (in
the continuation module) being set the value received on Chan as
the response for the service invocation. Also @Comp(cont) in
the environment gets set to true. In module doctor, this in turn
sets the value of the service variable c dosage to the value received as the response from the service (i.e., the value of the controlled variable c dosage of the continuation module cont) and
triggers the event @Comp(cont). The triggering of the event
@Comp(cont) in the doctor module results in the controlled

defn

:

lvalue "=" expr

lvalue

:

ID

expr

:

if expr

:

case expr

:

h

h

lvalue "=" "initially" expr "then" expr ";"

h

ID "[" index "]"

h h
h
h
h h

i

"[" lvalue [ "," lvalue ] "]"

h

h h
h
h
h
"if" "{" [ "[]" expr "->" expr ] k [ "otherwise" "->" expr ] j "}"
"case" expr "{" [ "[]" value [ "," value ] i "->" expr ] k
[ "otherwise" "->" expr ] j "}"
h

cond event

:

basic event "when" expr

basic event

:

"@ID " [ "(" expr l ")" ]
"@T" "(" expr ")"
"("cont var")" "@C" "(" expr ")"

expr l

:

expr [ "," expr ]

value

:

index

index

:

scalar value

scalar value

:

ID

bool binop

:

"&"

rel binop

:

"<"

arith binop

:

"+"

service invocation

:

ID ":"ID"(" var list ")ˆcont"

hj
ki

Choice
Optional
Zero or more
One or more

Legend:

[]
[]
[]

hj h h

value
"!" expr
expr bool binop expr
if expr
case expr
basic event
cond event
service invocation
"PREV" "(" expr ")"
expr rel binop expr
"+"
expr
"-" expr
expr arith binop expr
ID "[" index "]"
ID "(" [ expr l ] ")"
"[" expr l "]" "(" expr ")"

h

h

REAL

INT

h

h

h

"<="
"-"

i

h

STRING

h

"true"

h

"false"

scalar value ":" scalar value

"&&"

h

jlh

h

h

h

h

h

"|"
"=="
"*"

h

h

"||"
"!="

h
h

"=>"
">"

h

h

h
h

"@F" "(" expr ")"

h

"@Comp"

"infinity"

"<=>"
">="

"/"

Figure 3: The syntax of SOL definitions.

variable output dosage being assigned the value of c dosage
which at that point is the value returned as a response to the service invocation. Note that the invocation of the service can be
asynchronous, i.e., the response from the service may not arrive
instantaneously. Computations that do not depend on the response
received from the service invocation (i.e., definitions of dependent
variables that do not depend on the service variable receiving the response from the service invocation) are not blocked waiting for the
response from the service. For example, in Figure 2, the decision
whether to put the patient on oxygen can be made without waiting for the pharmaceutical service to return the required dosage.
Hence the definition of the variable oxygen can be executed while
waiting for the response from the pharmaceutical service, if one of
the events @T(saturation<65) or @T(saturation>90) is
triggered. Computations dependent on the result of the service invocation must be guarded by @Comp(cont), where cont is the
variable passed as continuation in the service invocation, so that
they wait until the result of the service invocation is available (signaled by the triggering of the @Comp(cont) event). The asynchronous nature of the service invocations create the effect of the
XMLHttpRequest API in AJAX-like applications.
The asynchronous nature of the service invocations can be used
to define a timer. We assume the existence of a timer method provided by a time service that when invoked with a (Integer or Real)
delay provides a response after the delay specified by the argument.
If the variable timer cont is passed as a continuation while invoking the service, the triggering of the event @Comp(timer cont)

signifies passage of the delay. The implementation of the timer is
illustrated in the example below.
deterministic reactive module delay{
...
services
String time:timer(x),
pre=x::Integer && x>0 -- post = true;
controlled variables
Integer x;
monitored variables
Boolean clock;
service variables
String t;
continuation variables
continuation timer_cont;
...
definitions
...
t=initially null then
if{
[] @T(clock)->time:timer(10)ˆtimer_cont;
}
x=initially null then
if{
[] @Comp(timer_cont)-> ...
}
}

mn

mn

The module delay ensures that the controlled x is output
time
units after the arrival of a clock pulse i.e., there is a delay of
time units between an input event (arrival of a clock pulse) and the
corresponding output.

Host

3.5 Failure Handling

Agents
E
n
c
SINS Virtual
r
Machine (SVM) y
p
t
Encrypt

Host
Agents
E
n
c
r
y
p
t

SINS Virtual
Machine (SVM)

Benign failures (we deal with byzantine failures elsewhere) in
the environment are handled by program transformations incorpoHost
Encrypt
E
rated in the SOL compiler that automatically transform a SOL modn
c
r
ule based on the failure handling information provided in the moniSINS Virtual
y
p
Machine (SVM)
tored variable declaration section. Given the declaration failure
t
Agents
Boolean I in the monitored variable section of a failure variable
signifying the (benign) failure of a module I in the environment
Figure 4: Architecture of SINS.
and the declaration Integer x on I y of a monitored variable
x (y is also a monitored variable), the SOL compiler transforms
5.1 Static Type Checking for Information Flow
each two-state definition z=initially null then expr, where
Let denote a typing environment,
range over the variables
z is a dependent variable and expr is an expression in which x ocof a module,
over the set of expressions in the module,
curs, to
over the set of types defined in the type definition section of the
z= initially null then
module, and
over the set of units defined in the unit definition
if{
section of the module. A typing environment is defined as
[] I -> expr[y/x];
}

o

where expr[y/x] is the expression obtained by replacing each
occurrence of the variable x by the variable y. One-state definitions
are transformed similarly.

3.6 Assumptions and Guarantees
The assumptions of a module, which are typically assumptions
about the environment of the subsystem being defined, are included
in the assumptions section. It is up to the user to make sure
that the set of assumptions is not inconsistent, i.e., a logical contradiction. Users specify the module invariants in the guarantees
section, which is automatically verified by a theorem prover such as
Salsa [9]. The syntax for specifying module assumptions and guarantees is identical to that of module definitions, in other words, we
have the expressiveness of the full language in these clauses. This
does not have a detrimental effect on the proof tools, since most
commonly encountered theorems about SOL programs are decidable.

4.

SINS

SOL agents execute on a distributed run-time infrastructure called
SINS (see Figure 4). A typical SINS implementation comprises
one or more SINS Virtual Machines (SVMs), each of which is responsible for a set of agents on a given host. SVMs on disparate
hosts communicate using the Agent Control Protocol (ACP) [26]
for exchanging agent and control information. An ancillary protocol, termed the Module Transfer Protocol (MTP) manages all
aspects of code distribution including digital signatures, authentication, and code integrity. Agents in SOL are allowed access to
local resources of each host in compliance with locally enforced
security policies. An inductive theorem prover is used to statically
verify compliance of an agent with certain local security policies.
Other safety properties and security requirements are enforced by
observer agents (termed “security agents”) that monitor the execution of application-specific agents and take remedial action when a
violation is detected.

5.

THE FORMAL SEMANTICS OF SOL

In this section, we provide the formal semantics of SOL. We
first present a static type system that enforces the information flow
policies ensuring safe downgrading of information.

 -qp

< =?>
s -td

r -L

o
w

v
T
ou h.oyx {z;|Ls?JHL!s _
where }z;~LsQJ!HLs denotes that  is of type L an unit s . Here
the unit qualifier is optional. Let us define o   y L if gz;
L`s?JHLs 6 6 o or Az;L 6 o , and o#E aY    s if Az;
Ls?JHLs o . We will write og. if the definition . is
well-typed under the typing environment o . The typing rules for

the static type system for SOL is given in Figure 5. The judgements [type] and [unit] are obvious. The judgement [expr] is infers
the secrecy type of an expression from those of its subexpressions
( is a binary operator/relation symbol). If under the typing environment , the secrecy types of the expressions
and
are and respectively, and
is a flow conversion rule (i.e.,
belongs to
), then the secrecy type of the expression
is . Informally, the rule states that, if binary operation/relation is applied on values, one of which is classified and
the other unclassified, then the secrecy type of the result is still classified. The judgements [expru1], [PREV1], [PREV2], and [if] are
straightforward. In [if],
denotes the if expression if []expr ->
otherwise ->
. The
judgement [expru2] states that if under the typing environment ,
the expressions
and
have units and respectively,
then a binary operation can be applied on the expressions if there
exists a conversion rule from the unit to the unit (or viceversa) declared in the unit conversion rules section of the module
(here
is an expression containing ). In case is defined in
terms of , the unit of the resultant expression will be . The judgements [odeft], [odefu], [tdeft], and [tdefu] provide the type and unit
checking rules for one-state definitions and two-state definitions respectively. We explain [odeft]; the others are similar. Intuitively the
rule [odeft] states that the value of an unclassified expression can be
assigned to a variable declared as classified. More formally, under
the typing environment , the value of an expression of type can
be assigned to a variable of type only if it is permitted by a rule in
the flow conversion section. Finally, the judgements [onecast] and
[twocast] state that an assignment of an expression of type to a
variable of type is allowed if explicitly coerced by the programmer. A module typechecks if
where
is the set of declarations in the module,
is the set of flow control rules and
is the set of unit
conversion rules. A module is secure if it typechecks.

=

o

L L
< =?> Z .= < =?> 9\ #,L Y.

LV;L

< =?> \

Hq K< < =Q> -tZ < =Q> Z-< =Q> \  < \
=?>
=?>
o
<  =?> Z < =Q> \
s d
d
s
d
sd

<Od,
d

o



< =?> Z

+O< 
 

L

L



L

tL 
+O<  x¡ !Q¢£¤VCY.¥x¦E^VCY.
9#,Y.
 §C¢£¤,Y.

5.2 Formal Operational Semantics

¨o "Y9L if o   ^ L
[unit]
o¨( s o E aY    s
o < =Q> <Z L Z < o¨ <\ =?> \ 9L  L)©|L  6 9#,Y.
[expr]
o =Q> .= =?> 9L 
o¨ < =?> Z < s Z < o¨ \ < =Q> \ s
[expru1]
o¨ =Q> .= =?> s
o < =?> Z < s Z < o¡ \ < =?d > \ d s &<ªd,6  !Q¢£¤VCY.
[expru2]
o¡ =?> = =?>
o < =?K<> s 
[PREV1]
o¨"«	V¬*­ =?> s
o¨ < =?K<>cY9L 
[PREV2]
o¨(«V¬7­ =?> 9L
o¡ < =?>K< Z 9L -< o¨Z-t<  < =Q\  > \ L  L©|L  6 9#,Y.
[if]
o¨{Hf =?> =?> =?> L 
o¨"L  o- < < =? >®Y9L  L  ©|L 6 9#,Y.
[odeft]
o¡¯ª  =?>
o s  o¡-t<  < =? > d s &<ªd,6  !Q¢¤£¤,Y.
[odefu]
o¡¯ª.  =?>
o{"9L o¨y¢¤Q¢£	- Y9L  -- < o¡  < =?>®9L  L  ©°L - L   ©°L 6 9#,Y.
[tdeft]
o¨±£	 ª.  HKJ!HL =?>
¦³ - o¡ < =?> d &<ªd,.-   ³ V6
¨
o



s

o
²


¢
Q

¤
¢
£
s
s 
 §C¢£¤,Y.
[tdefu]
o¡y£	 ª.   - HKJ!HL -< =?> 
o{"9L
o"Y9L
[onecast]
oyª.   - L < =?>  [twocast] o¨±£	 ª   -
 L  HJHL -S L < =?> 
[type]

Figure 5: A static type system for SOL

´
µQ¶.·C.
E
¸

¹

  6 µº«	¬¼»  - p
´
 v h
´x T {z; dª½, _ h ´x T «	¬7­    z; dª½, _
´  w
°
where Iz; dª½C denotes that  assumes value dª½, . We will write
´¾¿z; dª½, if e] z; ]
] _ÃdªÂ ½,(6 _ ´ . We will denoteaY by {EÀ  ´ 
´ . Let us write ´E  s if
the set T Áh T ;z
s 6 EÀ  ´  and the unit of  is s . For a module  , we denote
by ´2 the restriction of ´ to the variables in  . The judgements

In this section, we provide (a part of) the formal operational sebe the set
mantics of SOL. Let be an environment. Let
of all types in a SOL program. We let
range over values of
type for
. Let
range over the variables in SOL
program. An environment is defined as

for the operational semantics of SOL are given in Figure 6. For
sake of brevity, we do not include the full operational semantics;
rather we only provide a sampling of some of the more informative rules. The first judgement [no action] states that for a module
if no monitored variable changes, then no computation is done.



Ä&­ 2



Here
denotes the set of all monitored variables of . The
second judgement [unit conv] shows how unit conversion is done
automatically at runtime. If the unit of an expression is , under
the current environment its value is
, and a variable of unit
is assigned the value of the expression, then the value
is first
transformed to unit using the rules in the unit conversion section
of concerned module before assignment to . The third judgement
[CV] states that if is a controlled variable in the module and a
monitored variable in the module and if has value
under the
environment
then it has the same value under the environment
(here
is the set of controlled variables of the module ).
The judgement [@Comp] describes the
event. Assume
that is an internal or a controlled variable defined by an expression. Assume also that the definition is guarded by
where
is a continuation variable. If under the current environment , the expression in the definition evaluates to
and
is true, then the variable evaluates to
and
the environment turns off
. The other rules that we

´



´2
Åc­2

 9J¥L
´
ÆcÇÈE3=  9J¥L 



dª½,

s

J





ÆcÇÈ9¦=

ÆcÇÈE3=  9J¥L 



d
 dª½C

dª½, 

s



ÆcÇÈE3=  9J¥L 
dª½,
dª½,

´±7É¥ 6 ÄÊ­ 2 «	¬7­     É  EJ¥Lt´ Ë$ ÆcÇÈE3=  EJL 
´² < =?>Èz; dª½, ´ E aY K<ª< dª=?½,> # Ìd?- ´ E aY   ^ s s Ê<ªd,6  §C¢£¤,Y. -t+O<    -t< =?> 6 
[unit conv]
´"(z;
´ 2 {{z; dª½, dª½,  6 Åc­¥Í  6 Ä&­Î
[CV]
´*{z;
- +O<   .- -< )6
´² < =?U >Èz; dª½,   ´² ËÆc½,ÇÈ E<
3W =  9J¥L  dªz;u

£
.
Ï
Q


[@Comp]
 ÇÈ9¦= EJ¥L  =?> 
´ ÆcÇÈ9¦= EJ¥L   r ({;z ½,
[no action]

Figure 6: Operational Semantics for SOL
do not present here deal with checking the preconditions of a service before a service invocation, checking the postconditions after
a service has responded and dealing with the external events. Note
that at runtime the preconditions and postconditions of a service invocation (including types) are checked to ensure soundness in the
presence of third party (possibly COTS) component services that
may undergo reconfigurations at runtime due to network faults or
malicious attacks.

6.

CONCLUDING REMARKS

[11]
[12]
[13]
[14]
[15]

SOL is based on ideas introduced in the Software Cost Reduction
(SCR) project [19, 20] of the Naval Research Laboratory which
dates back to the late seventies. The design of SOL was directly
influenced by the sound software engineering principles in the design of SAL (the SCR Abstract Language), a specification language
based on the SCR Formal Model [18]. SOL provides a functionality akin to the XMLHttpRequest framework while maintaining a
formal setting.
The goal of SINS is to provide an infrastructure for deploying and protecting time- and mission-critical applications on a distributed computing platform, especially in a hostile computing environment, such as the Internet. The criterion on which this technology should be judged is that critical information is conveyed to
principals in a manner that is secure, safe, timely, and reliable.

7.[1] E.REFERENCES
Amir and J. Stanton. The spread wide area group communication

system. Technical report, Johns Hopkins University, 1998.
[2] G. Antoniou and F. van Harmelen. A Semantic Web Primer. MIT
Press, 2004.
[3] A. W. Appel. Compiling with Continuations. Cambridge University
Press, 1992.
[4] A. Benveniste, P. Caspi, S. A. Edwards, N. Halbwachs, P. L. Guernic,
and R. de Simone. The synchronous languages 12 years later.
Proceedings of the IEEE, 91(1):64–83, 2003.
[5] G. Berry and G. Gonthier. The Esterel synchronous programming
language: Design, semantics, implementation. Sci. of Computer
Prog., 19, 1992.
[6] R. Bharadwaj. Development of dependable component-based
applications. In In Proceedings of the First International Symposium
on Leveraging Applications of Formal Methods (ISOLA). IEEE
Computer Society, 2004.
[7] R. Bharadwaj. Development of dependable component-based
distributed applications. Technical report, Naval Research
Laboratory, 2005.
[8] R. Bharadwaj and C. Heitmeyer. Model checking complete
requirements specifications using abstraction. Automated Softw.
Engg., 6(1), Jan. 1999.
[9] R. Bharadwaj and S. Sims. Salsa: Combining constraint solvers with
BDDs for automatic invariant checking. In Proc.
International

Ð Ñ

View publication stats

[10]

[16]
[17]

[18]

[19]
[20]
[21]
[22]
[23]
[24]

[25]
[26]
[27]

Conference on Tools and Algorithms for the Construction and
Analysis of Systems (TACAS’2000), ETAPS 2000, Berlin, Mar. 2000.
R. Bharadwaj and S.Mukhopadhyay. From synchrony to sins.
Technical report, West Virginia University, 2005.
K. P. Birman. Reliable Distributed Systems. Springer, 2005.
D. Crane, E. Pascarello, and D. James. Ajax in Action. Manning,
2005.
E. W. Dijkstra. A Discipline of Programming. Prentice-Hall, 1976.
F. C. et. al. Business Process Execution Language for Web Services.
IBM, 2002.
R. Fielding. Architectural Styles and the Design of Network-based
Software Architectures. PhD thesis, UNIVERSITY OF
CALIFORNIA, IRVINE, 2000.
D. Gay, P. Levis, J. R. von Behren, M. Welsh, E. A. Brewer, and
D. E. Culler. The nesc language: A holistic approach to networked
embedded systems. In PLDI, pages 1–11, 2003.
N. Halbwachs. Delay analysis in synchronous programs. In
C. Courcoubetis, editor, the International Conference on
Computer-Aided-Verification, volume 697 of LNCS, pages 333–346.
Springer-Verlag, 1993.
C. L. Heitmeyer, R. D. Jeffords, and B. G. Labaw. Automated
consistency checking of requirements specifications. ACM
Transactions on Software Engineering and Methodology,
5(3):231–261, April–June 1996.
K. Heninger, D. L. Parnas, J. E. Shore, and J. W. Kallander. Software
requirements for the A-7E aircraft. Technical Report 3876, Naval
Research Lab., Wash., DC, 1978.
K. L. Heninger. Specifying software requirements for complex
systems: New techniques and their application. IEEE TSE,
SE-6(1):2–13, Jan. 1980.
E. A. Lee. Absolutely positively on time: What would it take?
Computer, 38(7):85–87, 2005.
G. Neumann and U. Zdun. Pattern-based design and implementation
of an xml and rdf parser and interpreter: A case study. In ECOOP,
pages 392–414, 2002.
E. Newcomer. Understanding Web Services. Addison Wesley, 2002.
F. Rocheteau and N. Halbwachs. POLLUX: A Lusture based
hardware design environment. In P. Quinton and Y. Robert, editors,
Proc. Conf. on Algorithms and Parallel VLSI Arch. II, Chateau de
Bonas, June 1991.
J.-P. Talpin, P. L. Guernic, S. K. Shukla, R. K. Gupta, and F. Doucet.
Polychrony for formal refinement-checking in a system-level design
methodology. In ACSD, pages 9–19, 2003.
E. Tressler. Inter-agent protocol for distributed SOL processing.
Technical Report To Appear, Naval Research Laboratory,
Washington, DC, 2002.
S. S. Yau, S. Mukhopadhyay, and R. Bharadwaj. Specification,
analysis, and implementation of architectural patterns for dependable
software systems. In IEEE WORDS, 2005.

MultiScale Modeling of Islamic Organizations in UK
Nyunsu Kim∗ , Sukru Tikves∗ , Zheng Wang∗ , Jonathan Githens-Mazer† and Hasan Davulcu∗
∗ CIDSE,

Arizona State University, Tempe, AZ, USA
of Politics, University of Exeter, Exeter, UK

† Department

Abstract—In this paper we utilize an efficient sparse inverse
covariance matrix (precision matrix) estimation technique to
identify a set of highly correlated discriminative perspectives. We
develop a ranking system that utilizes ranked perspectives to map
26 UK Islamic organizations on a set of socio-cultural, political
and behavioral scales based on their web corpus. We create a gold
standard ranking of these organizations through an expertise
elicitation tool. We compute expert-to-expert agreements, and
we present experimental results comparing the performance of
the QUIC based scaling system to another baseline method. The
QUIC based algorithm not only outperforms the baseline method,
but it is also the only system that consistently performs at area
expert-level accuracies for all scales.

I.

I NTRODUCTION

We propose a multi-scaling based methodology that represents an important step change in how we might observe
and analyze radical and counter-radical Islamic groups in any
specific region. Rather than placing external forms of analysis
that color and tautological define what is radical or not, we
propose a more ontologically oriented approach. We seek to
develop a methodology to allow the orientations of these
groups to define themselves via their own discourse within
their own universe and understanding of actions, rather than
an external and potentially poorly calibrated analysis of what
constitutes radical. Without this kind of fundamental reorientation to research of religiously or politically inspired groups,
we get the poor assumption based analysis that (incorrectly)
predicts and champions ill-defined relationships between certain religious or political sects and violence, for example.
With our reorientation of approach, we are more fundamentally
able to examine such relationships in a way that should allow
researchers to take other kinds of nuance and understanding
into account.
In the case of Islamic social movements, Edward Said [1]
observed, the boundary between political rhetoric and scholarship concerning Islam is often blurred. The problem is particularly acute when it comes to the study of violent forms of political Islam and others deemed to be potentially violent. Much
of the analytic and policy oriented literature relies on binary
distinctions such as “radical/moderate”, “modern/traditional”,
“conservative/progressive” etc. Binary models map enormous
diversity into ill-defined categories that often measure a mix
of attitudes about democracy, secularism, attitudes about the
West and proclivity to violence.
This paper explores these problems and presents a multiscale model based on more precise and objective criteria
that can be used to evaluate and compare movements in
diverse cultural, historical, and political contexts and how they
change over time. The analysis and modeling efforts build on
previous studies [2] of change oriented social movements and

use a combination of ethnographic, discourse analysis, and
computational methods as well as a case study involving 26
Islamic groups from the United Kingdom (UK). The model
presented here aims to broaden the base of discussion and
analysis, recapture and build upon previous observations, and
establish a general framework within which critically needed
comparative studies looking at both violent and non-violent
groups can be conducted.
One of the fundamental issues with interpretative and
qualitative data collection and analysis of groups and social
movements has been the researchers’ bias while conducting
the research. Goertz [3] makes the crucial point that, in
their enthusiasm for reifying complex sociological, cultural or
political concepts, theorists and empiricists often focus too
much on what a concept is, rather than on identifying the
concept on a continuum, in order to assess when a concept
is present versus when it is absent.
In the social sciences, scaling is the process of measuring
and ordering actors (subjects) with respect to quantitative
attributes or traits (items). In this paper, we present graphical tools and computational techniques so that both social
movements (subjects) and their socio-economic, political, or
religious beliefs, goals and practices (items) can be mapped
simultaneously on a set of continuous scales via expert inputs
and also via algorithms.
Earlier we developed an algorithm [4] that utilize large
amounts of multilingual text collected from a wide variety
of organizations media outlets (e.g. web sites, blogs, news,
RSS feeds, leaders speeches etc.) and we showed that Islamic
movements in Muslim societies exhibit distinct combinations
of perspectives on various social, political, and religious issues,
and those perspectives can be mapped to a latent linear
continuum, or a scale using Rasch modeling [5]. The Rasch
model is a psychometric probabilistic model for analyzing
categorical data, such as questionnaire responses, as a function
of the trade-off between (a) the respondent’s attitudes and (b)
the item difficulty. The resulting model allowed us to measure
the distance between organizations and their spatial-temporal
shifts. In order to evaluate the model, we computed expert-toexpert scaling agreements, and compared the performance of
three baseline methods, including random sorting, sorting with
an aggregate score and sorting with principal component analysis to the ranking performance of Rasch model. We showed
that scaling with Rasch model not only outperformed the
baseline methods, but the system also performed at area expertlevel accuracy for a single scale mapping a diverse range of
radical and counter-radical Indonesian religious groups.
In our prior work [4], we utilized a simple term frequency
- inverse document frequency (TF-IDF) [6] based technique
to generate a candidate list of keywords that can be utilized

as items during scaling analysis. Top 100 n-grams from each
organization’s web site were aggregated into a list of candidate
items, and we asked social scientists on our team to scan
the list manually, and select relevant keywords indicating
different perspectives. Upon analyzing selected keywords and
phrases, we realized that these features can be organized into
a three level hierarchy: (i) a set of top level scales comprising
epistemology, religious diversity tolerance, change orientation,
and violence ideology, (ii) a set of scale specific hotly debated
topics such as democracy, education, family law etc., and (iii)
two sets of discriminative topic specific perspectives voiced by
opposing camps on each scale. For example, relevant to the
social chance scale, a debate on education topic by the UK
organizations includes opposing perspectives, such as secular,
multi-cultural education on the no-change polarity of the scale
vs. religious, sharia based education on it’s radical-change
polarity. In a follow up paper [7], we developed automated
discriminative perspective mining algorithms for any given
topic and a text corpus comprising documents from opposing
camps of any bipolar scale.
A Guttman scale [8] utilizes a number of items, corresponding to socio-cultural, political beliefs, goals and
practices, and each group’s dichotomous response, e.g.
agree/disagree. Guttman scaling procedure is based on the
premise that items can be ranked in some order so that, for a
rational respondent, the response pattern can be captured by a
single index on the ordered scale. The Guttman pattern appears
on the response tables of groups (subjects) when perspectives
(items) can be arranged in an order on a scale so that an
organization who voices a particular perspective also voices
most of the other perspectives of lower rank-order. In order
to synthesize high accuracy multi-scaling models that can
process large document collections (tens of thousands) from
a large number of organizations’ web sites we need scalable
algorithms (i) to rapidly identify highly correlated subsets of
discriminant perspectives and (ii) to rank both discriminant
perspectives (items) and organizations (subjects) according to
neutral-to-extreme positions on any scale accurately.
The contributions of this paper can be summarized as
follows:
•

First we utilize an efficient sparse inverse covariance
matrix (precision matrix) estimation [9] technique
to identify a sorted subset of perspectives that are
likely to reveal a Guttman pattern in the corpus of
organizations, and hence suitable for utilization as
items during scaling. The QUIC algorithm presented
in Section III-C has superlinear convergence - it uses
O(log(1/e)) iterations for error e, which makes it
suitable for large-scale problems.

•

Second, we provide experimental results showing that
for a corpus of nearly 10,000 documents downloaded
from 26 UK Islamic organizations’ web sites, the
QUIC algorithm consistently identifies subsets of discriminant perspectives that reveal the Guttman pattern
by showing that the corresponding Rasch models fits
the data using the Andersens LR-test [10].

•

Third, we show that a heuristic ranking technique
based on the QUIC algorithm performs at higher
accuracy than Rasch model itself while performing

at area expert-level accuracies in ranking 26 British
Islamic organizations on all six socio-cultural political
and behavioral scales presented below in Section II.
Rest of the paper is organized as follows: Section II
presents related social theory and a multi-scale model of
Islamic organizations developed in collaboration with social
scientists on our team. Section III provides brief descriptions
of the techniques utilized in our item selection and scaling algorithms. Section IV describes the overall system architecture.
Section V presents the UK case study, experimental design and
evaluations.
II.

M ULTI -S CALE M ODELING OF S OCIAL M OVEMENTS

Our modeling leverages social theory including Durkheim’s
research on collective representations [11], Simmel’s work on
conflict and social differentiation [12], Wallace’s writings on
revitalization movements [13], and Tilly and Bayat’s studies on
contemporary social movement theory [14], [15] to understand
features shared by violent religious movements and by those
opposing them. Radicalism is the ideological conviction that
it is acceptable and sometimes obligatory to use violence to
effect profound political, cultural and religious transformations and to change the existing social order fundamentally.
Radical movements have complex origins and depend on
diverse factors that enable the translation of their radical
ideology into social, political and religious movements [16].
Crelinsten [17] states, “both violence and terrorism possess
a logic and grammar that must be understood if we are to
prevent or control them.” Binary labeling does not capture the
overlaps, movement and interactivity among these actors.
The model described below defines a six dimensional
possibility space within which diverse organizations, social
movements, and individuals can be located. The variables are
treated as continuous bipolar scales. Each scale is measured
independently of the others.
The choice of scales relies on the work of a combination
of American, European, African, and Southeast Asian scholars
and the literature on similar movements in various regions. The
variables are generalizations based on ethnographic research
that involved observation of public events, extended interviews
and informal conversations with leaders and rank and file members of organizations and movements, and online discourse
analysis. The scales used in our model for characterizing
diverse Islamic movements are:
•

Epistemology: This refers to the ways in which religious groups interpret core texts. Foundationalism is at
one end of a continuum. It fixes meaning in invariant,
“literal” readings of core religious texts. Foundationalists claim that their readings are ahistorical and not
influenced by cultural considerations. Constructivism
is at the other end of the scale. It acknowledges that
all variants of a religious tradition are constructed in
historical, social, and cultural contexts and they can,
and indeed must, change over time. Proponents of this
position maintain that to determine the meaning of
a scriptural passage appropriate for a particular time,
place, and culture, both the context of revelation and
the context of exegesis must be considered.

•

•

•

Religious Diversity Tolerance: Exclusivists, who insist on universal adherence to their own beliefs and
social norms and who claim exclusive possession
of complete truth, are at one end. Pluralists, who
understand difference as a social and religious good
or theological pluralism, are at the other. An entity at
the extreme pluralist end of the tolerance scale holds
the view that all religions should be tolerated and that
all are based on truths that transcend confessional and
sectarian differences.
Change Orientation: Change orientation aims to
capture the degree to which an entity wishes to
effect social, political, and/or religious change. It is
also a measure of the degree to which an individual
or group attempts to influence others. Revitalization
movements [13] that seek to destroy the world as it is
and rebuild it from scratch are at one end of the scale.
Defenders of the social, political, and religious status
quo are at the other end.
Violence Ideology: Violence is defined broadly to
include more than killing, inflicting physical injury,
and destruction of property. Symbolic and discursive
violence are included in this scale because they are
often steps leading toward physical violence. They
can cause havoc, especially when the manipulation
of symbols and discourse is purposively articulated
to provoke adversaries, demonize opponents, incite
mobs to action, or to provide justifications for the
necessity of violence. Unlike physical violence that
can be seen and clearly understood for what it is,
symbolic and discursive violence are not necessarily
self-evident; hence both require knowledge of their
contexts to identify them and assess their real and
potential danger. Dehumanization, demonization, and
the desecration of sacred places and objects are among
the most common and provocative forms of symbolic
violence committed in contexts of ethnic and religious
conflict. Violence Ideology scale represents the degree
to which an entity supports or rejects violence as a
matter of principle. Though some of the movements
scaled rely on reasoned argumentation appealing to
concepts of justice and oppression in addition to, or
in place of narratives. At one end are those who would
support any type of violence; at the other are pacifists
who are ideologically committed to nonviolence. A
lack of violent rhetoric is insufficient to classify an
organization as pacifist if the organization is silent in
the face of others’ violent acts and violent rhetoric.
III.

M ULTI -S CALE M ODELING T ECHNIQUES

A. SLEP: A Sparse Learning Package
We formulate discriminative perspective mining problem
in a general structured sparse learning framework [18]. The
following steps describe our algorithm:
1)
2)

For each topic, calculate the frequency of the words
occurring within a fixed sized window of the topic
keyword.
Create the term × document matrix using cooccurrence frequencies.

3)

Logistic formulation fits our application, since discriminative perspective mining is a dichotomous classification problem:

min
x

m
X

wi log(1 + exp(−yi (xT ai + c))

(1)

i=1

+ λ|x|1
ρ
+ ||x||22
2

(2)
(3)

In the formula above, ai is the vector representation of the
ith document, wi is the weight assigned to the ith document
(wi =1/m by default), and A=[a1 , a2 , ..., am ] is the document
keyword matrix, yi is the polarity of each document based
upon the scale polarity of the organization that the document
belongs to, and the unknown xj , the j-th element of x, is the
weight for each keyword, λ > 0 is a regularization parameter
that controls the sparsity of the solution, |x|1 = Σ|xi | is 1-norm
of the x vector. Let us explain further the three terms involved
in the convex optimization problem.
Pm
T
•
i=1 wi log(1 + exp(−yi (x ai + c)), this first term
is related to the logistic classification error. We set the
weights wi values to be all 1/m so that all documents
have the same weight.
•

λ|x|1 , this term involving the L1 norm deals with the
sparsity of the solution vector x. We experimented
with several λ values which resulted in x vectors of
various sparsity.

•

ρ
2
2 ||x||2 ,

this last term deals with the ridge regression,
which is an extra level of shrinkage. We set the weight
of this term ρ = 0 as we were mainly driven by
sparsity.

•

We used the M ATLAB implementation of the SLEP
package1 which utilizes gradient descent approach to
solve the aforementioned optimization problem. This
package can handle matrices of 20M entries within
a couple of seconds on a machine with standard
configuration.

•

Positive or negative polarities of the non-zero values
on the x feature vector correspond to the discriminant
perspectives corresponding to each side of a scale.

Note that the set of features indicated by non-zero values
of the x vector may not satisfy the sorted Guttman pattern.
Hence, they need to be further analyzed and filtered in order
to identify a suitable subset (if one exists) that would reveal
a Guttman pattern in organizational response tables. Next
section discusses the QUIC algorithm for filtering discriminant
perspectives and a heuristic ranking technique.
B. Guttman Pattern Detection
We summarize the process we used to automatically select
a subset of discriminant perspectives that can (a) well classify
the two different classes of the documents corresponding
to different polarities of each scale and (b) approximately
1 http://www.public.asu.edu/∼jye02/Software/SLEP

satisfy Guttman scaling requirements [19]. The following steps
describe our implementation:
1)
2)

3)

4)

5)
6)

For each topic relevant to a scale, calculate the frequency of the keywords co-occurring with the topic
phrase in a document.
Use a sparse regression method with logistic loss
discussed in previous section to learn the discriminant
perspectives for each class using SLEP logistic sparse
learning function [18].
Use the identified perspectives to create a document
x perspective matrix. Use QUIC algorithm presented
below to learn a sparse inverse covariance matrix of
the perspectives. In this matrix, the non-zero terms
indicate that the corresponding pairs of perspectives
are conditionally dependent.
Threshold the elements of the inverse covariance
matrix with a small value (0.05 was used in our
experiments): If the absolute value of the element is
smaller than this value, substitute it with 0, otherwise
substitute it with 1.
Rank the selected perspectives, showing 1’s in their
respective rows, in descending order by the number
of their conditionally dependent perspectives.
Rank the organizations by the number of perspectives
observed in their response tables. The details of
response table construction for a set of organizations
is presented in Section IV-D below.

C. QUIC: QUadratic Inverse Covariance
By assuming the data are independently distributed according to Gaussian distribution N (0, Σ), a zero in an offdiagonal element of Σ−1 corresponds to a pair of variables that
are conditionally independent given all other variables [20],
[21]. For example, if Σ−1 (i, j) = 0, then the variable i and
variable j are conditionally independent. Therefore, we use the
inverse covariance matrix of the perspectives to represent the
concurrent relationships among the corresponding perspective
pairs. Quadratic Approximation Method for Sparse Inverse
Covariance Learning (QUIC) [9] is a very efficient method
to estimate a sparse inverse covariance matrix for the features
of a given sample set.
Given the samples X from Gaussian distribution N (0, Σ),
the log likelihood of these data is
Pn
log P (X) = i=1 log P (xi )
=

Pn

i=1

1
e
(2π)p |Σ|
−1

log √

∝ log det(Σ

xi Σ−1 xT
i
−
2

) − trace(SΣ−1 ),

where S = cov(X) = XT X/n is the empirical covariance
matrix for the samples X. QUIC uses the maximum likelihood
principle to estimate the inverse covariance matrix Θ = Σ−1 ,
with an extra sparse regularization term as follows:
min − log det(Θ) + trace(SΘ) + λ|Θ|1 ,
Θ0

(4)

λ is a nonnegative tunable parameter which controls the
sparsity of the matrix Σ.
QUIC solve the problem in (4) iteratively based on Newton method, by using the second-order information. In each

iteration, it uses a quadratic approximation for the objective
function around the current estimated matrix Θt , and finds
the Newton direction Dt for the next estimate by solving a
regularized quadratic program.
We denote the objective function as
f (Θ) = − log det(Θ) + trace(SΘ) + λ|Θ|1 .
It contains two parts f (Θ) = g(Θ) + h(Θ), where
g(Θ) = − log det(Θ) + trace(SΘ),
which is twice differentiable and strictly convex, and
h(Θ) = λ|Θ|1 ,
which is convex but non-differentiable.
In the (t + 1)-th step, as we have obtained the estimate Θt ,
the log determinant of (Θt + ∆) can be approximated as
log det(Θt + ∆)

≈ log det(Θt ) + trace(Θ−1
t ∆)
−1
− 12 trace(Θ−1
t ∆Θt ∆).

Let Wt = Θ−1
t . Define the second-order approximation of
g(Θ) = g(Θt + ∆) as ḡΘt (∆). It is written as
ḡΘt (∆)

−1
= trace((S − Wt )∆) + 12 trace(Θ−1
t ∆Θt ∆)

− log det(Θt ) + trace(SΘt ).
Then the Newton direction Dt for the entire objective f (Θ)
can be written as the solution of the following problem:
Dt = arg min ḡΘt (∆) + h(Θt + ∆).
∆

(5)

QUIC computes the Newton direction iteratively with a proper
step-size, which is selected by Armijo-rule, until it finds a
satisfactory estimate of Θ.
To compute the Newton direction is an `1 regularized
least squares problem, which is also called Lasso [18]. It is
time consuming for directly solving (5). QUIC adapts the
coordinate descent and a screening heuristic to accelerate this
optimization procedure [9]. It is proved that QUIC has superlinear convergence, which is suitable for large-scale problems.
The implementation of this algorithm is available online2 .
D. Rasch Model
Rasch model [5] provides a probabilistic framework for
Guttman scaling to accommodate incomplete observations and
measurement errors. In Rasch model, the probability of a
specified binary response (e.g. a subject agreeing or disagreeing with an item) is modeled as a function of subject’s and
item’s parameters. Specifically, in the simple Rasch model,
the probability of a positive response (yes) is modeled as a
logistic function of the difference between the subject and
item’s parameters. Item parameters pertain to the difficulty
of items while subject parameters pertain to the ability of
subjects who are assessed. A subject of higher ability relative
to the difficulty of an item, has higher probability to respond
to an item affirmatively. In this paper Rasch models are used
2 http://www.cs.utexas.edu/∼sustik/QUIC/

to assess the organizations’ degree of radicalism or counterradicalism on six scales based on the religio-social perspectives
(items) appearing in their online rhetoric.
Rasch model maps the responses of the subjects to the
items in binary or dichotomous format , i.e., 1 or 0. Let
Bernoulli variable Xvi denotes the response of a subject v
to the item i, variable θv denotes the ability attribute of the
subject v and βi denotes the difficulty attribute of an item i.
According to simple Rasch model the probability that subject
v responds affirmatively (as 1) for item i is given by
P (Xvi = 1|θv , βi ) =

exp(θv − βi )
1 + exp(θv − βi )

in Section II. In our application, Rasch model subjects correspond to a group of religious organizations, and items correspond to a set of conditionally dependent discriminant perspectives on socio-cultural, political, religious beliefs, goals and
practices. An organization responding “yes” to a perspective
means the organization exhibit that perspective prominently
in its narrative, while an organization responding “no” to a
perspective indicates that the organization does not exhibit that
perspective prominently. In our model difficulty of an item
corresponds to the strength of the corresponding attitude in
defining neutral-to-extreme position of any organization on a
scale. Similarly ability of a subject, in this case, means the
degree of polarization exhibited by an organization’s rhetoric
on a continuous scale.

Rasch model assumes that the data under analysis have the
following properties
Unidimensionality: P (xvi = 1|θv , βi , α) = P (xvi =
1|θv , βi ), i.e., the response probability does not depend on other variables
2) Sufficiency: sum of responses contains all information
on ability of a subject, regardless which item it has
responded
3) Monotonicity: response probability increases with
higher values of θ, i.e., subject’s ability
Pn
Items with si = v xvi value of 0 or n, and subjects with
Pk
rv = i xvi value of 0 or k are removed prior to estimation,
where n is the total number of subjects and k is the total
number of items. Running Rasch model on the data gives us
an item parameter estimate or a score for each item. Generally
the estimation of βi or score for a item i is calculated through
Conditional Maximum Likelihood (CML) estimation [22]. The
conditional likelihood function for measuring item parameter
estimate is defined as
Y
exp(−βi si )
Lc =
P (xvi |rv ) = Q P
r
x|r exp(−βi xvi )
v

IV.

S YSTEM A RCHITECTURE

1)

where r represents the sum over all combinations of r items.
Similarly maximum likelihood is used to calculate subject
parameter estimation θv or score for each subject. Expectationmaximization algorithms [23] are used in implementing Conditional Maximum Likelihood (CML) estimation in Rasch
model. We can also assess whether the data fits the model
by looking at goodness of fit indices, such as the Andersen’s
LR-test.

Fig. 1.

The system architecture.

The experimental and evaluation data consists of positions
of 26 UK based Islamic religious organizations on six scales
scaled by three independent experts, topic-to-scale mapping
information provided by experts, and an online web corpus
of nearly 10,000 documents downloaded from the web sites
of these organizations. The steps for processing and scaling
these 26 UK Islamic organizations is summarized in Figure 1.
1)
2)
3)
4)
5)

Currently six scales are used in this study.
Experts identify the relevant list of organizations.
Web sites of these organizations are downloaded.
A text mining system identifies top 100 n-grams as
candidate topics from each web site.
Experts map relevant topics to scales. A snapshot of
the Topic-Scale Mapping Tool in shown in Figure 2.

In order to evaluate the quality of these measurements we
run Anderson Likelihood Ratio test (LR-test) [24] on the set
of data. The test gives us a goodness of fit of the data in Rasch
model, i.e., it tells us whether the data follows the assumptions
of Rasch model. A p-value, returned by the test, indicates the
goodness of fit and a p-value3 higher than 0.05 indicates no
presence of lack of fit.
E. Applying Rasch Model in the Text Mining Domain
In this paper, we use Guttman scaling with Rasch model to
find rankings of 26 Islamic organizations in the UK based on
extremity of their perspectives on six bipolar scales presented
3 http://en.wikipedia.org/wiki/P-value

Fig. 2.

A segment of the topic-scale mapping tool used by area experts.

Fig. 3.

A snapshot of the graphical scaling tool. Each expert independently scales the organizations on six bipolar continous scales.

A. Graphical Scaling Tool
We built a graphical scaling tool to collect and record the
opinions of area experts to be used for mining discriminant
perspectives and for evaluating our scaling algorithms (Section II). Each expert independently classified and ranked all the
UK organizations relative to each other on all six linear bipolar
scales described in Section II. The positions of each individual
organization provided by three experts are then averaged to
generate the gold standard of rankings of all organizations on
all scales. A snapshot of the graphical scaling tool showing
the gold standard rankings is shown in Figure 3.
B. Perspective Mining
A debate on a topic is a formal discussion in which
opposing perspectives are put forward. In this step, our focus is
the determination of discriminant topic-specific perspectives,
which would contribute to understanding of features (i.e.
social, political, cultural, religious beliefs, goals, and practices)
shared by one side of a debate, and by those opposing them.
We formulate the perspective mining problem in a general
structured sparse learning framework as an optimization problem presented in Section III-A. The keyword phrases with
non-zero values on the minimized solution vector yields the
discriminant perspectives. Figure 4 on the next page shows
radical-Islamist and counter radical-Islamist perspectives identified by the algorithm for five sample topics of the Violence
Ideology scale.
C. Scaling with QUadratic Inverse Covariance (QUIC)
We would like to identify the perspective characteristics of
varying degrees of polarization, from neutral to more extreme
positions, on either side of each scale. A Guttman scale [8]
presents a number of items, corresponding to socio-cultural,
political beliefs, goals and practices, if these items can be
ranked in some order so that, for a rational respondent, the

Fig. 5.

A sample guttman pattern identified by the QUIC method.

response pattern can be captured by a single index on that
ordered scale. In other words, on a Guttman scale, items can
be arranged in an order so that an organization who voices a
particular item also voices most of the other items of lower
rank-order. We utilize the sparse inverse covariance estimation
technique presented in Section III-C to identify the candidatesorted subset of perspectives that are likely to reveal a Guttman
pattern and hence are suitable for utilization as reliable items
in Guttman scaling. A sample probabilistic Guttman pattern
discovered by the QUIC algorithm in the response table
of the UK organizations for the Violence Ideology scale is
shown above in Figure 5 – where rows correspond to sorted
organizations, and columns correspond to sorted items, and
each dot represents an affirmative response of an organization
for an item.
D. Response Tables of Organizations
A response table is calculated based on the normalized
frequency with which organizations voice various perspectives
in their web sites. The normalized frequencies of perspectives
for each organization are calculated by using Formula 6. In
Formula 6, k is the perspective, o is the organization, and Do
is the entire document set for organization o.

fo,k =

|{d | k ∈ d, d ∈ Do }|
|Do |

(6)

Fig. 4.

A sample set of radical and counter-radical perspectives for five different topics on the Violence Ideology scale.

The median frequency of each perspective is used as a
threshold. Organizations’ normalized perspective frequencies
and the threshold of each perspective are used to build a dichotomous [0/1] response matrix as the organizations’ response
table.
E. Ranking with Rasch Modeling
A true Guttman scale is deterministic, i.e. if an actor
subscribes to a certain perspective, then it must also agree
with all lower order perspectives on the scale. But, perfect
order is rare in the real world. The Rasch [25] model provides
a probabilistic framework for Guttman scales to accommodate
incomplete observations and measurement error. We employed
the Rasch model presented in Section III-D to rank both the
organizations (subjects) and corresponding perspectives (items)
on each scale as an alternative ranking algorithm alongside the
QUIC algorithm. Rasch Modeling algorithm4 also produces a
metric [10] to validate the fitness of the model. A p-value,
returned by the test, indicates the goodness of fit and a pvalue5 higher than 0.05 indicates no presence of lack of fit.
V.

order to build a gold standard of rankings of these organizations, each expert independently used a graphical scaling tool
to rank the organizations relative to every other organization.
A screenshot of the tool is shown in Figure 3.
Each expert independently ranked the organizations that
they are familiar with according to six socio-cultural, political
and ideological scales. The individual scores for each organization by each expert were combined and averaged to obtain the
consensus gold standard rankings on each of the six scales. A
random ordering would theoretically amount to an error rate of
0.375 according to the displacement error measure we defined
in Equation 7. The consensus rankings among our three experts
was high; since their average error rate compared to the gold
standard of all organizations were 0.198 for the epistemology
scale, 0.146 for the religious diversity tolerance scale, 0.145
for the political change scale, 0.127 for the religious change
scale, 0.113 for the social change scale, and 0.127 for the
violence ideology scale.
The version of the graphical scaling tool that our
experts used is online at: http://www.minerva-project.org/
DataCollector.

E XPERIMENTAL E VALUATIONS

A. UK Corpus

C. Computationally Generated Scales

The experimental corpus comprises articles published online by the 26 Islamic organizations identified in the UK.
Online sources correspond to web sites, RSS and Tweet
feeds, and blogs of known leaders of these organizations.
We downloaded a total of 10,521 articles published by these
organizations. For HTML pages, the boilerpipe toolkit6 was
used to clean the headers, footers and extract plain text.

The organizational rankings discovered by both the Rasch
model and the QUIC algorithm have been evaluated against the
gold standard rankings of the experts by using the following
displacement error measure defined in Equation 7.
P
error(G, R) =

o∈O

|G(o)−R(o)|
|O|

|O|

(7)

B. Expert Opinion and Gold Standard of Rankings
We collaborated with three highly trained area experts with
social science and British and Islamic cultural knowledge. In
4 http://r-forge.r-project.org/projects/erm/
5 http://en.wikipedia.org/wiki/P-value
6 https://code.google.com/p/boilerpipe/

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1, |O|]. For
two exactly matching rankings, the error(G, R) will be zero,
whereas for two inversely sorted rankings it will be 0.5 (when
the size of O is even). A random ranking is expected to have
an error measure of 0.375.

Fig. 6.

Computational and expert rankings of epistemology scale

Fig. 7.

Computational and expert rankings of political change scale

Fig. 8.

Computational and expert rankings of religious change scale

D. Epistemology Scale Evaluation
We calculated the displacement error between each expert’s
ranking and the consensus gold standard of rankings. For the
epistemology scale, the first expert’s displacement error is
0.127, and the second and third experts’ displacement errors
are 0.319 and 0.148 correspondingly as shown in the last
row of the table in Figure 6. The average error of all three
experts against the gold standard ranking is 0.198. The Rasch
model shows a displacement error of 0.299, which is better
than random ranking. The QUIC algorithm performed like an
experts’ ranking with a displacement error measure of 0.175,
which beats the ranking performance of both the Rasch model
and the average displacement error of our three experts.
E. Political Change Scale Evaluation
For the Political Change scale, the first expert’s displacement error is 0.068, and the second and third experts’ displacement errors are 0.324 and 0.041 correspondingly as shown in
the last row of the table in Figure 7. The average error of all
three experts against the gold standard ranking is 0.144. The
Rasch model shows a displacement error of 0.225, which is
better than random ranking. The QUIC algorithm performed
like an expert’s ranking with a displacement error measure
of 0.198 falling within the experts’ error range of 0.041 and
0.324. It also beats the ranking performance of the Rasch
model.

performed like an expert’s ranking with a displacement error
measure of 0.183 falling within the experts’ error range of
0.041 and 0.324. QUIC also beats the ranking performance of
the Rasch model.

F. Religious Change Scale Evaluation

G. Social Change Scale Evaluation

For the Religious Change scale, the first expert’s displacement error is 0.089, and the second and third experts’
displacement errors are 0.256 and 0.036 correspondingly as
shown in the last row of the table in Figure 8. The average
error of all three experts against the gold standard ranking is
0.127. The Rasch model shows a displacement error of 0.186,
which is better than random ranking. The QUIC algorithm

For the Social Change scale, the first expert’s displacement
error is 0.068, and the second and third experts’ displacement
errors are 0.195 and 0.077 correspondingly as shown in the
last row of the table in Figure 9. The average error of all three
experts against the gold standard ranking is 0.113. The Rasch
model shows a displacement error of 0.163, which is better
than random ranking. Both the QUIC algorithm and the Rasch

Fig. 9.

Computational and expert rankings of social change scale

Fig. 11.

Computational and expert rankings of violence ideology scale

measure of 0.169 falling within the experts’ error range. QUIC
also beats the ranking performance of the Rasch model.
I. Violence Ideology Scale Evaluation
For the Violence Ideology scale, the first expert’s displacement error is 0.062, and the second and third experts’
displacement errors are 0.248 and 0.071 correspondingly as
shown in the last row of the table in Figure 11. The average
error of all three experts against the gold standard ranking is
0.127. The Rasch model shows a displacement error of 0.214,
which is better than random ranking. The QUIC algorithm
performed like an expert’s ranking with a displacement error
measure of 0.213 falling within the experts’ error range.
VI.

Fig. 10.

Computational and expert rankings of tolerance diversity scale

model performed like an expert’s ranking performance with a
displacement error measure of 0.163 falling within the experts’
error range of 0.068 and 0.195 for this scale.
H. Religious Diversity Tolerance Scale Evaluation
For the Religious Diversity Tolerance scale, the first expert’s displacement error is 0.092, and the second and third experts’ displacement errors are 0.219 and 0.127 correspondingly
as shown in the last row of the table in Figure 10. The average
error of all three experts against the gold standard ranking is
0.143. The Rasch model shows a displacement error of 0.194,
which is better than random ranking. The QUIC algorithm
performed like an expert’s ranking with a displacement error

C ONCLUSION

Scaling with the QUIC algorithm consistently performs
at area expert-level accuracies for all the evaluated scales
used for modeling the UK Islamic organizations. This preliminary analysis with all six scales show that when experts
can bootstrap the system with a list of organizations and
assist it with topic-to-scale mapping, then the web corpus of
these organizations provides sufficient information to enable
a computational method to rank and model organizations at
area expert-level accuracies. Our future work includes investigations on automated discovery of new and emerging
groups, as well as utilization of clustering techniques using the
inverse covariance matrix to automatically synthesize scales
representing highly correlated sets of topics.
ACKNOWLEDGMENT
The authors would like to thank our three area experts: Our
co-author Professor Jonathan Githens-Mazer, a political scientist at Institute of Arab and Islamic Studies and Department
of Politics at University of Exeter, UK. Dr. Zacharias Pieri,
a post-doc at University of South Florida, USA. Dr. Sajjad

Rizvi, Professor of Islamic Intellectual History, and Director
of Education at University of Exeter, UK.
R EFERENCES
[1] E. Said, Orientalism. Pantheon Books, 1978.
[2] A. Wallace and R. Grumet, Revitalizations and Mazeways: Essays on
Culture Change, Volume 1. University of Nebraska Press, 2003.
[3] G. Goertz, Social science concepts : a users guide.
Princeton
University Press, 2006.
[4] S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen,
S. Corman, M. Woodward, S. Nair, I. Rohmaniyah, and A. Amin, “A
system for ranking organizations using social scale analysis,” Social
Network Analysis and Mining (SNAM), 2012. [Online]. Available:
http://link.springer.com/article/10.1007%2Fs13278-012-0072-x
[5] D. Andrich, Rasch models for measurement. Sage, 1988.
[6] G. Salton and C. Buckley, “Term-weighting approaches in automatic
text retrieval,” 1988, p. 513523.
[7] S. Tikves, S. Gokalp, M. H. Temkit, S. Banerjee, J. Ye, and H. Davulcu,
“Perspective analysis for online debates,” in Proceedings of the IEEE
International Conference on Advances in Social Networks Analysis and
Mining, ser. ASONAM. IEEE Computer Society, 2012, pp. 898–905.
[8] L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60–90, 1950.
[9] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar, “Sparse
inverse covariance matrix estimation using quadratic approximation,”
vol. 24, 2011, pp. 2330–2338.
[10] E. B. Andersen, “A goodness of fit test for the rasch model,” Psychometrika, vol. 38, pp. 123–140, 1973.
[11] E. Durkheim, “The cultural logic of collective representations,” Social
theory the multicultural and classic readings, Wesleyan University:
Westview Press, 2004.
[12] G. Simmel, Sociological Theory. New York: McGraw-Hill, 2008.
[13] A. Wallace, “Revitalization movements,” American Anthropologist,
vol. 58, pp. 264–281, 1956.
[14] A. Bayat, Making Islam Democratic: Social Movements and the PostIslamist Turn. Stanford University Press, 2007.
[15] C. Tilly, Social Movements. Boulder, CO, USA: Paradigm Publishers,
2004.
[16] J. Githens-Mazer, “The rhetoric and reality: radicalization and political
discourse,” vol. 33, no. 5, 2012, pp. 556–567.
[17] R. Crelinsten, “Analysing terrorism and counter-terrorism: A communication model,” Terrorism and Political Violence, vol. 14, pp. 77–122,
2002.
[18] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 547–556.
[19] J. McIver and E. Carmines, Unidimensional Scaling. Sage Publications,
Inc, 1981, vol. 24.
[20] M. Yuan and Y. Lin, “Model selection and estimation in the gaussian
graphical model,” Biometrika, vol. 94, pp. 19–35, 2007.
[21] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance
estimation with the graphical lasso,” Biostatistics, vol. 9, pp. 432–441,
2008.
[22] Y. Pawitan, In all likelihood: statistical modelling and inference using
likelihood. Oxford University Press, USA, 2001.
[23] D. Hunter and K. Lange, “A tutorial on mm algorithms,” The American
Statistician, vol. 58, no. 1, pp. 30–37, 2004.
[24] D. Hessen, “Likelihood ratio tests for special rasch models,” Journal of
Educational and Behavioral Statistics, vol. 35, no. 6, p. 611, 2010.
[25] G. Rasch, “On general laws and the meaning of measurement in
psychology,” in Proceedings of the Fourth Berkeley Symposium on
Mathematical Statistics and Psychology, 4, 1961, p. 332.

K–partitioning of Signed or Weighted Bipartite
Graphs
Nurettin B. Omeroglu, Ismail H. Toroslu
Middle East Technical University, Dep. of Computer
Engineering, Ankara, Turkey
{omeroglu, toroslu}@ceng.metu.edu.tr
Abstract— In this work, K-partitioning of signed or weighted
bipartite graph problem has been introduced, which appears as a
real life problem where the partitions of bipartite graph
represent two different entities and the edges between the nodes
of the partitions represent the relationships among them. A
typical example is the set of people and their opinions, whose
strength is represented as signed numerical values. Using the
weights on the edges, these bipartite graphs can be partitioned
into two or more clusters. In political domain, a cluster
represents strong relationship among a group of people and a
group of issues. In the paper, we formally define the problem and
compare different heuristics, and show through both real and
simulated data the effectiveness of our approaches.
Keywords—
Partitioning

Social

I.

Networks,

Bipartite

Graphs,

Graph

INTRODUCTION

Social networks became one of the hottest topics of computer
science in recent years. One very common form of a social
network is actually a simple bipartite graph where one
partition U represents actors (e.g., people, organizations) and
the other partition V represents a set of issues (e.g., political
issues, beliefs). One of the earliest definitions of this problem
is given in [1]. An edge between a person and an issue
represents the opinion of that person on that issue. This
opinion expressed with a sign, as positive or negative, (no
edge between a person-issue pair expresses “no opinion”),
and, a numerical value representing the strength of the opinion
of person.

Sedat Gokalp, Hasan Davulcu
Arizona State University, Sch of Computing, Inf. and
Decision Sys. Eng., Tempe, AZ, USA
{sedat.gokalp, hasan.davulcu}@asu.edu

This work extends previously introduced idea of [2] to be able
to partition bipartite graphs into k clusters (k-way partitioning)
based on the opinions expressed on the edges. Notice that the
clustering should produce sub-bipartite graphs such that
people in a sub-bipartite graph should have strong positive
opinions on the issues of that sub-bipartite graph, and they
should have strong negative opinion towards the issues in
other sub-bipartite graphs.
The inputs of k-way partitioning of signed bipartite graph
(
), label function
problems are bipartite graph
and partition count . Label of an edge can be
positive or negative real value. In most cases the range of the
mapping is either a small subset of integers of real values. For
this modeling, we assume that a positive edge from
to
where
and
means that
supports , and a
negative edge implies that is against . The goal of the
partitioning problem is to divide the sets
and
into
(
) and (
) simultaneously to form
disjoint max
clusters (
), such
that,
1. The sum of the weights of the positive edges within
clusters is maximized,
2. The sum of the weights of the positive edges between
clusters is minimized,
3. The sum of the weights of the negative edges within
clusters is minimized,
4. The sum of the weights of the negative edges between
clusters is maximized.

Fig. 1. Partitioning of A into A1, A2, ... Ak and B into B1, B2, ... Bk

Fig. 2. Nodes are distributed among blocks
In Fig. 1, sets and are partitioned into clusters from 1 to
k, where clustering is done simultaneously. Simultaneous (i.e.,
vertical) partitioning is a little bit confusing; some may think
that there are
clusters in the figure instead of clusters.
To clarify the key point of simultaneous partitioning, we can
say that and jointly forms
like shown in Figure 2.
Each line in Fig. 1 represents the sum of the weights of the
edges from
to ,
where label N (i.e., red
lines) denotes negative and label P (i.e., green lines) denotes
positive signs. Note that some of the lines are thicker than the
others. Thick lines represent higher values than the thin ones.
In Figure 2, six nodes from and four nodes from are
distributed into three blocks. In this example, separators (i.e.,
thick vertical red lines) cut negatively weighted edges between
blocks and divide nodes into 3 partitions. For illustration, all
nodes are distributed ideally so that edges within blocks are all
has positive and edges between blocks are all has negative
sign. As one would predict, most of the time, partitioning may
not be perfect, meaning that there can be negative edges
within clusters and positive edges between clusters.
To our knowledge, no efficient algorithm was presented for
k-way partitioning of signed bipartite graph problem before
our study, as stated in [2]. This study is the extension of the
work in [2] in two folds: One of the extensions is k-way
partitioning of bipartite graphs, and the other one is the
reduction of the execution time almost by half using a simple
observation about the hill-climbing algorithm.
Although clustering is a very well-known problem, there
are not many works on bipartite graph clustering. In [6] and
[8], two sets of entities (represented by two sets of nodes in
the bipartite graph) were clustered. These works were related
to document clustering, where one set of entities was set of
words, and the other one was set of documents. In these works
there was no information on the edges. In [3] and [5], signed
arbitrary graphs were considered, but they were not focused to
bipartite graphs. In [1] and [7], similar problems were
introduced; however, no effective algorithm has been
introduced.
Kernighan-Lin (KL) [10] and Fiduccia-Mattheyses (FM)
[9] algorithms are two fundamental move-based heuristic
algorithms used for graph partitioning from which several
algorithms such as [2] have been inspired. While the first one
works locally, the second one considers global connectivity.
KL algorithm is an efficient heuristic method which finds
effective optimal solutions for arbitrary unsigned-weighted

graphs. The objective of the algorithm is to divide the graph
into subsets of no larger than a given maximum size in order
to minimize the sum of the weights on all edges cut. FM is a
mincut heuristic algorithm which iteratively partitions
networks.
The rest of the paper is organized as follows; mathematical
model of the problem given in Section 2. Generic and movebased heuristic are presented in Section 3 and 4 respectively.
The results obtained from real and randomly generated
datasets are presented at the 4th Section. Finally, the last
Section contains the conclusions and the future work.
II.

MATHEMATICAL MODEL

Let
and
partitioning of the nodes of bipartite graph
and let


(

| |)

be indicator vectors for
Thus,




and

(

| |)

respectively,


.



{


be
),

(

{



An example of partitioned bipartite graph is given in Figure 3,
where
,
and the vertices of two
partitions are connected by weighted edges. In the example,
vertices of
and
are divided into three blocks (
),
where
,
and
. The indicator vectors for this graph are as
following:

Fig. 3. Illustrative Example

[ ]

[ ]

[ ]

[ ]

[ ]

[ ]

It is not possible to convert this problem into linear
programming (LP) problem as it is and solve with a LP solver,
since there are nonlinear terms (i.e.,
) in (3).

Clearly,

III.
| |

∑

∑

∑

∑|

| | and ∑

|

| |

| | and ∑|

∑

| |

|

| |

| | and ∑

| |.

Let
( ) represents the adjacency matrix for the bipartite
graph
(
). The sum of all edges in the clusters is
given by [5].
∑ ∑
∑
∑ ∑| | ∑| |
∑
The mathematical programming formulation can be written as
follows:
∑

∑|

|

∑|

|

∑|

∑

|

∑|

|

( )

Subject to
∑
∑

| |
| | ( )

The objective function (1) gives the maximized L value as the
objective value, by the help of constraints (2).
’s and
’s
expressed in (1) and (2) are the variables of these equations.
As seen in the above formulation (1), L value can be obtained
by subtracting the sum of edges across clusters (let’s say “O”out) (i.e., right part) from the sum of edges within clusters
(let’s say “I”-in) (i.e., left part). Clearly, we can find O by
subtracting I from the total sum of edges (let’s say T), since T
= I + O. Thus, the above formulation (max L= I - O) can be
rewritten as follows (max L= 2I - T):
∑

∑|

|

∑|

|

∑|

|

∑|

|

As the right part of the formulation (T) is constant, to
maximize L we need to maximize the left part of the
formulation (let’s say );
∑ ∑| | ∑| |
∑
∑|

|

∑|

|

∑

( )

Using values of Figure 3 for equation (5), we can find
(

)(

( )(
( )(
( )(

)
)
)
)

. Thus

( )(
( )(
( )(

( )(

)
)
)
)

( )(
( )(
( )(

(

)(
)

as
)

)
)

GENERIC ALGORITHMS

Two well-known generic heuristics, namely genetic
algorithms and simulated annealing, have been applied to
solve k-way partitioning of signed bipartite graphs problem.
A genetic algorithm (GA) [11] is a heuristic algorithm,
inspired by evolutionary processes of ecological systems, that
finds optimal (or near-optimal) solutions to complex
optimization problems. In GAs, possible solutions to the
problem are coded in chromosomes. A “chromosome” (or
“individual”) can be designed as a string, binary digit or other
symbols that corresponds to a solution of the problem at hand.
The fitness function of GA analyzes “genes” in the
chromosomes, makes some qualitative assessment and
provides a meaningful and comparable fitness value for that
solution. Basically, thanks to the fitness function, candidate
solutions pass to the next generation of solutions by discarding
solutions with a “poor” fitness and accepting any with a
“good” fitness value.
A typical GA works as follows:
 Construct an initial population of chromosomes by
generating randomly attempted solutions to a problem
 Do the following until a satisfactory fitness level has
been reached or run out of time:
─ Evaluate each fitness of the solutions
─ Keep a subset of these solutions (using different
heuristics)
─ Use these solutions to generate a new population by
using the crossover and mutation operators.
There are many different techniques which a genetic algorithm
can use to select the individuals to be copied over into the next
generation [11].
There are two basic reproduction strategies, which are
crossover and mutation. Crossover is a reproduction technique
to generate two offspring from two selected parents. The
chromosomes of the two parents are recombined according to
some techniques to form offspring. Mutation is a reproduction
mechanism, which generates new offspring from single parent.
Each binary digit of the chromosome is subject to inversion
under a given probability (most of the time small).
Simulated annealing (SA) is a generic probabilistic metaalgorithm used to find an approximate solution to global
optimization problems, which was introduced by Kirkpatrick
[12]. It is inspired by annealing in metallurgy which is a
technique of controlled cooling of material to reduce defects.
A typical SA algorithm works as follows:





Initialize temperature , epsilon , alpha
Generate a random initial solution as current solution
Do the following till
or run out of time
While stopping criteria not met do

─ Find the neighbor of the current solution
( )
o Compute
( ) (i.e.,
: fitness
function)
o Randomly generate a real number from 0 to 1
o If
(
) then
─ Reduce T by multiplying with
SA starts with some solution that is totally random, and
changes it to another solution that is similar to the previous
one. Newly generated solutions are generally chosen
randomly, though more sophisticated methods can be applied.
If this solution is a better solution, it will replace the current
solution. If it is a worse one, it may be chosen to replace the
current solution with a probability that depends on the
temperature (i.e., cooling process, decreases with time) and
the distance (i.e., difference between new (worse) solution
and the old one) parameters. As the algorithm progresses, the
temperature parameter decreases by multiplying , giving
worse solutions a lesser chance of replacing the current
solution.
IV.

Fig. 4 depicts the gain computation on a simple example.
As seen from the figure, it is clear that the movement of the
selected node to the 3rd block gives the largest increase for the
value of L.
In MBH Algorithm, which is given below, L represents the
result of the objective function, and K is the number of
clusters, which is given as an input. Hill-climbing algorithms
usually improve the result, but it is always possible to strike at
local maximum. In order to avoid this problem we repeat the
process several times. Therefore, we use one more parameter,
R, to randomly start the process more than once in order to be
able to avoid local minimum.

MOVE-BASED HEURISTIC

Move based heuristic (MBH) is a typical hill-climbing
algorithm. For k-partitioning of signed bipartite graphs, movebased heuristics work as follows:
 Nodes are randomly placed into the blocks at the
beginning.
 Then, through iterations, the node with the highest
gain value is selected and moved to the block that
maximizes the gain. After each move, that node is
locked. Until all the nodes are locked, the iteration
continues.
 After all nodes are locked, the change in the result
value L (the objective value which is defined in
Section 2) is checked. If the completed iteration
increases the value of L, a new iteration starts by
configuring the initial state with the best state found
in the previous iteration. Otherwise, iterations end
and best solution is returned.
In order to measure the quality of clustering of bipartite
graphs, as in [2], we have defined a gain function that
recalculates the gains of all nodes as the vertices are placed
into blocks. The gain calculation is done as follows:
 If both vertices are in the same block, and the edge
between them is positively weighted, then moving
either one will reduce the gain.
 Similarly, if the vertices are in different blocks and
the edge between them is negatively weighted, then
putting them into the same block will also reduce the
gain.
 If the vertices are in the same block, but the edge
between them is negative weighted, then, moving one
of them to a different block will increase the gain.
 Finally, if two vertices are in different blocks, but the
edge between them are positively weighted, then
moving them into the same block will increase the
gain.

Fig. 4. Gain Computation
Move-Based Heuristic (MBH) Algorithm
(
)
Input : Graph:
Number of clusters: , Number of iterations: .
Output: Maximal value and partitions of nodes.
1:
2: while
3: Initially, place each node into block 1 to randomly
4:
5: do
6:
7:
Compute gains of all nodes (Refer to Figure 4)
8:
do
9:
nod1 select the unlocked node with max gain
10:
blck1 select the best block for nod1
11:
place the nod1 into blck1
12:
update gains of nod1’s neighbors
13:
New RESULT
14:
lock nod1
15:
until all nodes are locked
16: while
17:
18: end while
19: print

Objective Value

31
3385
4774
9875
11622
13494
17410
19079

800000
600000
400000
200000
0

Time (ms)

800000

Fig. 6. Questionnaire Experiment (K=9), Opt-MBH Stats

600000

The standard version of MBH locks all of the nodes (traverses
all of the nodes) in each iteration even after a local maximum
has been reached, which causes the decline of the total gain. In
order to cut this wasted time we have modified the algorithm
just to detect whether the local maximum has been reached.
This is done simply by comparing the current objective value,
, with the one that is already been obtained, . If the trend
of the current value is a decrease, then, the rest of the
traversing the nodes has been abandoned. Below is the
extension on MBH Algorithm for this addition. This addition
should be made to the end of the inner loop between the lines
8-15 of MBH Algorithm.

400000
200000
59358

51356

43665

36005

28813

20811

14165

7613

47

0

Time (ms)
Fig. 5. Questionnaire Experiment (K=9), MBH Stats
Objective Value

Objective Value

While we were analyzing the outputs of MBH, we have
observed that there have been some unnecessary moves in the
process of MBH algorithm. These redundant moves become
clear by displaying a sample run of the algorithm as in Figure
5. This figure has been obtained from questionnaire dataset
(which will be explained in the following section) experiment
for K=9. In this experiment, the R value is 3, and therefore the
MBH algorithm has been applied 3 times with 3 random initial
solutions.

800000
Optimized MBH Algorithm

600000
400000
200000
59358

51356

43665

36005

28813

20811

14165

7613

47

0

Time (ms)
Fig. 5 Figure 5, we see that objective values are increasing and
decreasing in a systematic way. Decreasing parts are not
necessary for our solution, since we have been trying to find
the global maximum. Furthermore, as the chart presents,
calculation of the descending values is really time consuming.
Therefore, we wanted to remove the declining parts from the
execution (chart) to reduce the total execution time. This is
done by detecting when the values start to fall below the local
maxima. In this way, we have managed to cut the unnecessary
parts of the computation as shown in Figure 6. Note that the
elapsed time has fallen below by half (approximately 57000 to
20000 ms).

At each iteration, we set the
…
1: if (
)
(
2:
3: else if
then
4: ; // Do Nothing
5: else
6: if
then
7: else
8: end if
9: if
then
10: exit loop
11: end if
…

V.

) then

EXPERIMENTAL RESULTS

The methods expressed in this work are all implemented in
C++, using the Visual Studio 2005 development environment.
Tests are done on a commodity computer having Windows 7
x86 OS, Intel Core 2 Duo 2.00 GHz CPU, and 3 GB RAM.
In this work, the algorithms have been tested with both real
datasets and with randomly generated data. We have generated
random datasets with dimensions 10x10, 20x20, 40x40 and
80x80. For each size, sparse (less than 20% of the edges have
non-zero values) vs. dense (more than 55% of the edges have
non-zero values), signed (i.e., -1, 0, 1) vs. ranged (-10, …, 0,
…, 10) versions are also generated.
In Table 1, first 16 rows correspond to randomly generated
datasets. 17th and 18th rows show the characteristics of the real

world data sets. 17th dataset contains questionnaire with 48
questions, which are applied to 7572 people. The questions
were ranked between -5 and +5. 18th dataset corresponds to
US Congress (SENATE) dataset which is published publicly
in www.govrack.us. From this site, we have used the roll call
votes for the 111th US Congress Senate that covers the years
2009-2010. The 111th Senate data contains information about
108 senators and their votes on 696 bills. We have constructed
a signed bipartite graph as in [3] based on the votes of the
senators on the bills.

In our experiments, we have run each algorithm 10 times on
18 datasets. In these executions, the parameters used in
algorithms and their values are as follows:
 GA: Iteration Count: 50, Population Size: 500, Elitist
Selection: 5%, Roulette-Wheel Selection: 90%, Random
Generation: 5%, Uniform Crossover Rate: 0.6, Mutation
Rate for Each Chromosome: 0,0001
 SA: Alpha: 0.99999, Temperature: 400.0, Epsilon: 0.001
 MBH: R (Randomly Restart Number): 25

Table 1. Characteristics of Datasets

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

GA
1281
1403
1262
1150
2320
2846
1526
1657
6289
7020
3268
4523
20458
20892
8802
9201
1115816
271807

K=2
SA
MBH
962
2
713
3
961
0
805
3
858
6
749
6
1134
8
881
6
984
17
842
22
1206
20
975
25
1301
98
1103
119
1306
97
1315
106
12321
41116
2136
878

OptMBH
0
3
5
0
5
3
2
3
8
8
8
10
39
41
25
20
11023
289

GA
1249
1316
1094
1069
2225
2814
1546
1724
6185
6922
3295
4116
20132
20171
8574
8917
1105766
271807

K=3
SA
MBH
1229
2
853
0
1131
0
1022
3
1048
8
989
8
1449
5
1142
6
1452
28
1203
30
1647
27
1317
30
1863
133
1691
148
1972
19
1952
128
14602
57415
3451
1033

A. Senator Experiment (Dataset Number 18)
Opt-MBH algorithm had clustered 108 senators and 696 bills
into 3 clusters. That is, the gain has increased when the cluster
size is increased from 2 to 3, but, there were no increase
afterwards. Figure 7 shows the 2-way and the 3-way
partitioning of the bipartite graphs. In these figures columns
correspond to the bills and the rows correspond to the
senators. The colors (green and red) correspond to the votes of
senators on the bills (favor or against). Notice that blue lines
have been inserted into these figures in order to make clusters
more visible.
The US Senate has 2-party system (with 2 independents,
mostly inclined to Democrats), with 100 members. During the
2 years of 111th Senate, the numbers of the members of both
parties have changed due to different circumstances.
Therefore, the total number of senators has also increased to
108. In two clustering, the clusters were roughly representing
the party lines. During 111th Senate, the number of
Republicans was 39 in its minimum level, and one of the
clusters our system has obtained exactly had that many
senators. Of course, there are several Senators voting quite
independently from their respective parties. However, even in

OptMBH
6
5
2
24
14
2
3
5
14
14
13
14
58
58
59
55
19580
401

GA
1264
1243
1042
1090
2198
2587
1520
1686
5956
6401
3159
3849
19453
20263
8590
9343
1090272
269835

K=9
SA
MBH
1524
5
1198
2
1366
3
1295
8
1552
19
1535
17
2042
11
1668
11
2315
69
2133
66
2702
47
2288
50
3537
289
3309
348
3697
265
3906
259
23489
143526
7193
2002

OptMBH
5
6
3
0
13
8
5
6
30
38
28
24
155
148
126
114
54711
948

3-cluster structure, it has been observed that senators were not
clustered forming a 3rd group. Only, a small number of bills
have been discovered, which are mostly been rejected by the
senators of both parties. The structures of 2 and 3 clusters are
as follows:



in 2-way, 39 senators and 257 bills formed one cluster
and 69 senators and 439 bills formed the other one,
in 3-way, the number of senators were the same for the
first two clusters, and the third cluster had 0 senators,
however, 7 bills from the first cluster, and 4 bills from the
second cluster had been moved into the third one making
it with 0 senators and 11 bills.

B. Questionnaire Dataset (Dataset Number 17
In the experiment corresponding to the 17th dataset since the
data size was very large and dimensions were disproportional,
we could not print the results in a figure similar to the one that
we have done for the Senate experiment. The clusters are
shown in Table 5.

Fig. 7. Partitioning of Senators and Bills with K = 2 (top) and K = 3 (bottom)

Table 2. Clusters for Questionnaire Experiment (P: # of persons, Q: # of questions in a cluster)
Clusters

K=2

K=3

K=4

K=5

K=6

K=7

K=8

K=9

P

Q

P

Q

P

Q

P

Q

P

Q

P

Q

P

Q

P

Q

1

7458

33

6320

34

5864

33

6083

34

5941

33

6088

34

6080

34

6074

34

2

114

15

1252

0

981

3

637

1

883

3

611

1

572

1

531

1

3

NA

NA

0

14

727

0

447

0

463

0

417

1

423

1

498

1

4

NA

NA

NA

NA

0

12

405

1

203

1

287

0

162

0

198

0

5

NA

NA

NA

NA

NA

NA

0

12

82

2

130

1

126

1

114

1

6

NA

NA

NA

NA

NA

NA

NA

NA

0

9

39

0

123

0

64

0

7

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

0

11

86

1

52

0

8

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

0

10

41

2

9

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

0

9

10

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

NA

RESULT

642291

^ 686261

^ 693867

^ 694499

^ 694551

^ 694759

^ 694843

^ 694851

Table 2. Comparison of All Results (K=2, K=3, K=9)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

GA
35
177
14
120
96
591
49
261
273
1652
143
940
798
4993
438
2647
642282
46066

K=2
SA
MBH
35
35
171
177
14
14
114
120
96
96
578
591
49
49
253
261
275
276
1678
1683
146
147
932
944
809
815
5092
5108
440
444
2680
2697
642291 64221
46066
46066

OptMBH
35
177
14
120
96
591
49
261
275
1684
147
944
815
5106
444
2692
642291
46066

GA
43
205
16
122
107
712
57
302
307
2044
174
1105
895
5817
514
3068
685281
46066

K=3
MBH
43
43
199
205
16
16
120
122
108
108
706
719
58
59
295
307
315
316
2108
2115
179
178
1116
1119
945
936
6073
6041
535
531
3235
3202
678842 686261
46422
46422
SA

OptMBH
43
205
16
122
108
719
59
307
316
2111
179
1131
939
6026
530
3223
686261
46422

GA
42
204
16
122
105
691
61
320
300
2178
182
1134
896
5898
507
3085
682912
46376

K=9
MBH
43
43
200
205
16
16
121
122
111
112
718
723
62
63
315
325
326
326
2275
2274
190
188
1184
1178
983
973
6463
6382
561
551
3360
3276
682900 695053
46422
46422
SA

OptMBH
43
205
16
122
112
723
63
325
324
2274
188
1177
967
6356
555
3259
694831
46422

Table 3. Time to Find the Best Solutions (K=2, K=3, K=9)
K=2
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

GA
212
231
187
229
502
643
372
440
2573
2883
1470
1623
8707
11179
4672
5001
923707
33924

SA
863
587
860
662
752
591
1019
728
850
611
1073
774
1056
755
1089
1017
7454
1406

MBH
0
0
0
0
0
0
0
2
2
5
6
6
36
50
20
48
746
14

K=3
OptMBH
GA
0
379
2
309
3
239
0
231
0
1006
0
1386
0
668
0
699
2
2977
2
3728
3
1966
6
2336
14
14140
14
16875
13
6254
5
6134
437 1071273
11
33924

SA
1114
710
1025
780
934
788
1309
977
1260
869
1481
1027
1557
1178
1685
1482
10518
2321

MBH
0
0
0
2
2
6
2
2
3
9
6
14
55
64
39
80
4855
16

K=9
OptMBH
0
0
2
2
9
0
2
2
13
10
8
6
31
32
33
40
1407
15

GA
796
608
337
367
1479
1797
1021
1017
4582
5438
2427
3287
17056
18411
7850
8777
1079663
191094

SA
1402
941
1215
932
1357
1201
1821
1342
2075
1643
2390
1811
2944
2298
3235
2847
17068
4607

MBH
0
0
0
0
10
5
5
0
22
41
22
33
147
152
128
136
61346
39

OptMBH
3
3
2
0
3
3
0
2
17
19
6
17
87
58
81
84
32776
34

Table 4. Execution Time of Algorithms (K=2, K=3)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

GA
1281
1403
1262
1150
2320
2846
1526
1657
6289
7020
3268
4523
20458
20892
8802
9201
1115816
271807

K=2
SA
MBH
962
2
713
3
961
0
805
3
858
6
749
6
1134
8
881
6
984
17
842
22
1206
20
975
25
1301
98
1103
119
1306
97
1315
106
12321
41116
2136
878

OptMBH
0
3
5
0
5
3
2
3
8
8
8
10
39
41
25
20
11023
289

GA
1249
1316
1094
1069
2225
2814
1546
1724
6185
6922
3295
4116
20132
20171
8574
8917
1105766
271807

K=3
SA
MBH
1229
2
853
0
1131
0
1022
3
1048
8
989
8
1449
5
1142
6
1452
28
1203
30
1647
27
1317
30
1863
133
1691
148
1972
19
1952
128
14602
57415
3451
1033

OptMBH
6
5
2
24
14
2
3
5
14
14
13
14
58
58
59
55
19580
401

GA
1264
1243
1042
1090
2198
2587
1520
1686
5956
6401
3159
3849
19453
20263
8590
9343
1090272
269835

K=9
SA
MBH
1524
5
1198
2
1366
3
1295
8
1552
19
1535
17
2042
11
1668
11
2315
69
2133
66
2702
47
2288
50
3537
289
3309
348
3697
265
3906
259
23489
143526
7193
2002

OptMBH
5
6
3
0
13
8
5
6
30
38
28
24
155
148
126
114
54711
948

Objective Value

Opt-MBH algorithm had partitioned this weighted bipartite
graph into 9 clusters, as the best clustering structure. We
tried all the cluster sizes from 2 to 9. We have discovered
that the objective value increased for each cluster size as it
can be seen from Figure 8. Similar to the Senate
experiment, some clusters had only vertices from one of the
partitions of the bipartite graphs.
700000
Results
500000
300000
1 2 3 4 5 6 7 8 9

Trendline
(Moving Average)

Partition Number K

Fig. 8. Results of Questionnaire Experiment with Moving
Average Trendline
C. Comparing MBH and Optimized-MBH
In the final experiment, MBH and Opt-MBH have been
compared with inputs of not only the same datasets but also
the same randomly generated initial solutions. “17 th dataset
with K=9” and “18th dataset with K=8” results are displayed in
the figures below. As can be seen from the minimum values in
these graphics, R=3 has been used in these experiments.
Figure 9 contains two graphics which emphasize total
execution time of the algorithms and the cut points (i.e.,
vertical lines) in Opt-MBH for the questionnaire dataset. In
Figures 10 and 11, senator dataset has been used and
approximate saved times are also shown.

Fig. 10. Execution times of Senator Dataset with MBH

Fig. 111. Execution times of Senator Dataset with Opt-MBH
VI.

Fig. 9. Execution times of Questionnaire Dataset with MBH
and Opt-MBH

CONCLUSION AND FUTURE WORK

This work extends previous work on 2-way clustering of
signed bipartite graphs [2] to k-way clustering of signed or
weighted bipartite graphs. This problem appears in social
networks in many different forms.
In this study, for k-way partitioning of the signed bipartite
graphs problem, mathematical methods, generic algorithms
and various move-based heuristics have been developed. We
have shown that our approaches are quite effective through
experiments on not only randomly generated data, but also
real world data. Our experiments show that optimized movebased heuristic algorithm produces the best result and has the
best execution time.

REFERENCES
[1]
[2]

[3]
[4]

[5]
[6]

Andrej, M., and Doreian, P. Partitioning signed two-mode networks.
Journal of Mathematical Sociology 33, pages 196–221, 2009.
Banerjee, S., Sarkar, K., Gokalp, S., Sen, A., and Davulcu, H.
Partitioning Signed Bipartite Graphs for Classification of Individuals
and Organizations. Social Computing, Behavioral - Cultural Modeling
and Prediction Lecture Notes in Computer Science Volume 7227, 2012,
pp 196-204
Bansal, N., Blum, A., and Chawla, S. Correlation clustering. In Machine
Learning, pp. 238–247, 2002.
Charikar, M., Guruswami, V., and Wirth, A. Clustering with qualitative
information. Journal of Computer and System Sciences. Volume 71,
Issue 3, October 2005, Pages 360–383.
Sen, A., Deng, H., and Guha, S. On a graph partition problem with
application to VLSI layout. Inf. Process. Lett. 43(2), 87–94, 1992.
Dhillon, I.S. Co-clustering documents and word using bipartite spectral
graph partitioning. In Proceedings of the KDD, 2001, pp. 269-274.

[7]

Zaslavsky, T. Frustration vs. clusterability in two-mode signed networks
(signed bipartite graphs), Unpublished manuscript
(http://www.math.binghamton.edu/zaslav/Tpapers/fvc.pdf).
[8] Zha, H., He, X., Ding, C., Simon, H., and Gu, M. Bipartite graph
partitioning and data clustering. In Proceedings of the 10th International
Conference on Information and Knowledge Management, pp. 25–32,
2001. ACM.
[9] Fiduccia, C.M., and Mattheyses, R.M. A Linear-Time Heuristic for
Improving Network Partitions. In Design Automation, pp. 175-181,
1982.
[10] Kernighan, B.W., and Lin, S. An Efficient Heuristic Procedure for
Partitioning Graphs. In Bell System Technical, vol.49, pp. 291-307,
1970.
[11] Holland, J. H. Adaption in Natural and Artificial Systems. University of
Michigan Press, 1975.
[12] Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. Optimization by
simulated annealing. Science 220, 671-680, 1983.

Soc. Netw. Anal. Min.
DOI 10.1007/s13278-012-0072-x

ORIGINAL ARTICLE

A system for ranking organizations using social scale analysis
Sukru Tikves • Sujogya Banerjee • Hamy Temkit • Sedat Gokalp •
Hasan Davulcu • Arunaba Sen • Steven Corman • Mark Woodward
Shreejay Nair • Inayah Rohmaniyah • Ali Amin

•

Received: 20 December 2011 / Revised: 4 April 2012 / Accepted: 20 April 2012
Ó Springer-Verlag 2012

Abstract In this paper, we utilize feature extraction and
model-fitting techniques to process the rhetoric found in
the web sites of 23 Indonesian Islamic religious organizations to profile their ideology and activity patterns along a
hypothesized radical/counter-radical scale, and present an
end-to-end system that is able to help researchers to visualize the data in an interactive fashion on a timeline. The
subject data of this study is 37,000 articles downloaded
from the web sites of these organizations dating from 2001
to 2011. We develop algorithms to rank these organizations
by assigning them to probable positions on the scale. We
show that the developed Rasch model fits the data using
Andersen’s LR-test. We create a gold standard of the
ranking of these organizations through an expertise elicitation tool. We compute expert-to-expert agreements,
and we present experimental results comparing the performance of three baseline methods to show that the Rasch model not only outperforms the baseline methods, but it
is also the only system that performs at expert-level
accuracy.

1 Introduction

S. Tikves (&)  S. Banerjee  H. Temkit  S. Gokalp 
H. Davulcu  A. Sen  S. Corman  M. Woodward  S. Nair
Arizona State University, P.O. Box 87-8809,
Tempe, AZ 85281, USA
e-mail: sukru@asu.edu; stikves@asu.edu

S. Corman
e-mail: scorman@asu.edu

S. Banerjee
e-mail: sujogya@asu.edu

S. Nair
e-mail: snair8@asu.edu

H. Temkit
e-mail: mtemkit@asu.edu

I. Rohmaniyah
Center for Religious and Cross Cultural Studies,
Gadjah Mada University, Yogyakarta, Indonesia
e-mail: rochmaniyah@yahoo.com

S. Gokalp
e-mail: sgokalp@asu.edu
H. Davulcu
e-mail: hdavulcu@asu.edu
A. Sen
e-mail: asen@asu.edu

Being able to asses information on radical and moderate actors
in a geographic area is an important research topic for national
security. Radicalism is the ideological conviction that it is
acceptable, and in some cases, obligatory to use violence to
effect profound political, cultural and religious transformations and change the existing social order fundamentally.
Muslim radical movements have complex origins and depend
on diverse factors that enable translation of their radical ideology into social, political and religious movements. Crelinste
(2002), in his work, states that ‘‘both violence and terrorism
possess a logic and grammar that must be understood if we are
to prevent or control them’’. Therefore, analysis of Muslim
radical and counter-radical movements requires attention to
the global, national and local social, economic and political
contexts in which they are located. Similarly, in the Islamic
context, counter-radical discourse takes various different
forms; discursive and narrative refutations of extremist
claims, symbolic action such as ritual and other religious and

M. Woodward
e-mail: mataram@asu.edu

A. Amin
State College of Islamic Studies (STAIN),
Manado, Indonesia
e-mail: aleejtr77@yahoo.com

123

S. Tikves et al.

cultural practices, and Islamic arguments for pluralism,
peaceful relations with non-Muslims, democracy, etc. The
most effective counter-radicals are likely to be religiously
conservative Muslims. Effective containment and defeat of
radicalism depends on our ability to recognize various levels
of radicalization, and detection of counter-radical voices.
In our previous work (Davulcu et al. 2010), we attempted a
clustering approach to obtain ‘‘natural groupings’’ of a number
of local non-government religious social movements and
organizations in Indonesia. Social scientists on our team
observed that clustering results were not fully able to separate
all counter-radical or radical organizations into pure clusters.
Pure radical clusters were easily identified due to high similarity among their support for violent practices. Pure counterradical clusters were identified due to their strong reactionary
opposition to violent practices through protests and rhetoric.
But the rest of the groupings were mixed. We realized that
binary labeling as counter-radical or radical does not capture
the overlap, movement and interactivity among these organizations. In this paper, we hypothesize that both counterradical and radical movements in Muslim societies exhibit
distinct combinations of discrete states comprising various
social, political, and religious beliefs, attitudes and practices,
that can be mapped to a latent linear continuum or a scale.
Using such a scale, an analyst can determine where exactly
along the spectrum any particular group lies, and also potentially where it is heading with its rhetoric and activity.
Given the complex nature of the task, such as regional
differences in local cultures, beliefs and practices, and in the
absence of readily available high-accuracy parsers, highly
structured religio-social ontologies, and information
extraction systems; we decided to devise a multi-lingual nonlinguistic text processing pipeline that relies on only statistical modeling of keyword frequency and co-occurrence
information. However, we designed the system to be able to
incorporate additional information extracted from the text, if
available. For example, named entity recognition (NER),
machine translation, and GIS-based location lookup information are part of the user-interface presentation.
We (Tikves et al. 2011) worked with social scientists on
our team to come up with an orthogonal model comprising
of two primary dimensions. Both dimensions, (1) radical/
counter-radical and (2) violent/non-violent, are characterized as latent, partial orders of discrete beliefs and practices
based on a generalization of item order in Guttman scaling
(Guttman 1950) using a Rasch model (Andric 1988). A true
Guttman scale is a deterministic process, i.e., if a social
movement subscribes to a certain belief or practice, then it
must also agree with all lower-order practices and beliefs
on the scale. Of course, such perfect order is rare in the
social world. The Rasch model provides a probabilistic
framework for Guttman scales to accommodate for
incomplete observations and measurement errors.

123

We have designed a web-based system to visualize this
orthogonal model. The web tools provided by the system
allows drilling down on specific data, and plotting the trends
and trajectories of organizations on a timeline. It consists of
several modules: an off-line web mining, and data-processing pipeline, two web services for application logic, and an
AJAX-based presentation layer. The web-based interface
built for this study can be accessed through the web site at
http://www.demo.minerva-project.org. In this paper, we
present several scenarios with this tool in Sect. 5.
In this paper, we present feature extraction, feature
selection, and model-fitting techniques to process the rhetoric found in the web sites of 23 religious Indonesian organizations—comprising a total of 37,000 articles dating from
2001 to 2011. We aim to identify their ideology and activity
patterns along a hypothesized radical/counter-radical scale,
and rank them to probable positions on this scale (McPhee
1995). The automated ordering of organizations is formed by
ranking the organizations according to their estimated
positions on the latent scale. We used the eRm1 package to fit
the Rasch model on this data set, and identify organizations’
positions based on maximum likelihood estimation (Le Cam
1990). We show that the model fits the data using the
Andersen’s likelihood ratio test (LR-test) (Hessen 2010). We
also created a gold standard of the ranking of these organizations through an expert-opinion elicitation tool, and
through the opinions of three ethnographers on our team who
collectively possess 35 years of scholarly expertise on
Indonesia and Islam. We computed expert-to-gold standard
agreements, as well as compared the performance of three
different baseline computational methods to show that the
Rasch model presented here not only performs the best
among the baseline methods but that it is also the only
method that performs at an expert level of accuracy.
1.1 Organization of the paper
Next section provides an introduction to the theory of
Guttman scaling and Rash models. Section 3 defines the
problem, presents the system architecture, and the methods
used to solve the problem. Section 4 describes the Indonesian corpus, expert-opinion elicitation tool, baseline
computational methods, and experimental evaluations.
Section 5 discusses the user-interface designed for navigating our findings. Section 6 concludes the paper.

2 Introduction of Guttman scaling and Rasch model
In social science, scaling is a process of measuring and
ordering entities called subjects, based on their qualitative
1

http://www.r-forge.r-project.org/projects/erm/.

A system for ranking organizations using social scale analysis

attributes called items. In general, subjects are requested to
respond to surveys conducted by means of structured
interviews or questionnaires. Items are presented to the
subjects in form of questions. Statistical analysis of the
response of the subjects on the questions about items are
used in scaling the subjects. Some of the widely followed
scaling procedure in social science surveys are Likert scale
(Likert 1932), Thurnstone scale (Thurnstone 1928), and
Guttman scale (McIver 1981). In Likert scale, subjects
indicate their magnitude of agreement or disagreement
about an item (from strongly agree to strongly disagree) on
a five- to ten-point scale. On the other hand, Thurnstone
scale is a formal method of ordering the attitudes of the
subjects toward the items. Guttman scaling procedure
orders both the subjects and the items simultaneously with
respect to some underlying cumulative continuum. In this
paper, we follow the Guttman scaling process to rank the
organizations based on their response on the radical and
counter-radical keywords.
2.1 Guttman scaling
A Guttman scale (Guttman 1950) presents a number of items
to which each subject is requested to provide a dichotomous
response, e.g., agree/disagree, yes/no, or 1/0. This scaling
procedure is based on the premise that the items have strict
orders (i.e., the items are presented to the subjects ranked
according to the level of the item’s difficulty). An item ‘‘A’’
is said to be ‘‘more difficult’’ than an item ‘‘B’’, if any subject
answering ‘‘yes’’ on item ‘‘A’’ implies that the subject will
also answer ‘‘yes’’ on item ‘‘B’’. A subject who responds to
an item positively is expected to respond positively to all the
items of lesser difficulty. For example, to find out how
extreme a subject’s view is on Guttman scale, the subject is
presented with the following series of items in question form.
(1) Are you willing to permit immigrants to live in your
country? (2) Are you willing to permit immigrants to live in
your community? (3) Are you willing to permit immigrants
to live in your neighborhood? (4) Are you willing to permit
immigrants to live to your next door? (5) Are you willing to
permit your child to marry an immigrant? If the items form a
Guttman scale, any subject agreeing with any item in this
series, will also agree with other items of lower rank-order in
this series. Guttman scale is a deterministic process and the
score of a subject depends on the number of affirmative
responses he has made on the items. So, a score of 2 for a
subject in the above Guttman scale not only means he has
given affirmative response to two of the questions or items
but also indicates that he agrees with two particular questions, namely the first and second. Scores in Guttman scale
can also be interpreted as the ‘‘ability’’ of a subject in
answering questions sorted in increasing order of ‘‘difficulty’’. These scores when presented on an underlying scale,

give us an ordering of the subjects based on their ‘‘ability’’
also.
The objective of our paper is to order the Indonesian
Islamic organizations based on their views on religio-social
keywords which have an inherent ordering. For example,
two such keywords are ‘‘Quran’’ and ‘‘Sharia’’. An organization supporting ‘‘Sharia’’ will also likely to ‘‘believe in
Quran’’. So it makes sense to use Guttman scaling procedure to rank the organizations and their beliefs and practices. One drawback of Guttman scale is that it is
deterministic and assumes a strict ordering of the items. In
real world, it is difficult to order all the items in such a
strict level of increasing difficulty, therefore, perfect scales
are not often observed in practice. Furthermore, many
times, the order of the items are not known since they are
not straightforwardly comparable. In addition, measurement errors might lead to responses that do not strictly fit
the ordering. As a result, we can no longer conclude
deterministically that if a subject answers a question
affirmative, whether she will be able to give affirmative
answers to other questions of lower order in the same
questionnaire. We use Rasch model to overcome this
drawback by taking into account measurement error.
2.2 Rasch model
Rasch model (Andric 1988) provides a probabilistic framework for Guttman scales. In Rasch model, the probability of a
specified binary response (e.g., a subject agreeing or disagreeing to an item) is modeled as a function of subject’s and
item’s parameters. Specifically in the simple Rasch model,
the probability of a positive response (yes) is modeled as a
logistic function of the difference between the subject and
item’s parameters. Item parameters pertain to the difficulty
of items while subject parameters pertain to the ability of
subjects who are assessed. A subject of higher ability, related
to the difficulty of an item, has higher probability to respond
to a question affirmatively. In this paper, Rasch models are
used to assess the organizations degree of being radical or
counter-radical based on the religio-social keywords (items)
appearing in their rhetoric.
Rasch model also maps the responses of the subjects to the
items in binary or dichotomous format, i.e., 1 or 0. Let
Bernoulli variable Xvi denotes the response of a subject v to
the item i, variable hv denotes the parameter of ‘‘ability’’ of
the subject v and bi denotes the parameter of ‘‘difficulty’’ of
an item i. According to the simple Rasch model, the probability that the subject v responds 1 for item i is given by:
PðXvi ¼ 1jhv ; bi Þ ¼

expðhv  bi Þ
:
1 þ expðhv  bi Þ

Rasch model assumes that the data under analysis have
the following properties.

123

S. Tikves et al.

Unidimensionality P(xvi = 1|hv, bi, a) = P(xvi = 1|hv,
bi), i.e., the response probability does not depend on
other variable
2. Sufficiency sum of responses contains all information
on ability of a subject, regardless which item it has
responded
3. Conditional independence for a fixed subject, there is
no correlation between any two items
4. Monotonicity response probability increases with
higher values of h, i.e., subject’s ability.
P
Items with si = nv xvi value of 0 or n, and subjects with
Pk
rv = i xvi value of 0 or k are removed prior to estimation,
where n is the total number of subjects and k is the total
number of items. Running Rasch model on the data gives us
an item parameter estimate or a score for each item. In
general, the estimation of bi or score for an item i is calculated through conditional maximum likelihood (CML) estimation (Pawitan 2001). The conditional likelihood function
for measuring item parameter estimate is defined as:
Y
expðbi si Þ
Lc ¼
Pðxvi jrv Þ ¼ Q P
r
xjr expðbi xvi Þ
v
1.

where r represents the sum over all combinations of
r items. Similarly, the maximum likelihood is used to
calculate subject parameter estimation hv or score for each
subject. Expectation-maximization algorithms (Hunter
2004) are used in implementing CML estimation in Rasch
model. We can also assess whether the data fit the model
by looking at goodness of fit indices, such as the Andersen’s LR-test.
To evaluate the quality of these measurements, we run
Anderson LR-test (Hessen 2010) on the set of data. The test
gives us a goodness of fit of the data in Rasch model, i.e., it
tells us whether the data follows the assumptions of Rasch
model. A p value, returned by the test, indicates the
goodness of fit and a p value2 higher than 0.05 indicates no
presence of lack of fit.
2.3 Implementing Rasch model in the text mining
domain
In this paper, we use Guttman scaling and Rasch model to
find a ranking of 23 religious organizations based on
extremity of their views are on radicalism and counterradicalism. In our application, Rasch-model subjects correspond to a group of religious organizations, and items
correspond to a set of keywords for socio-cultural, political, religious radical and counter-radical beliefs, and
practices. An organization responding ‘‘yes’’ to a feature
means the organization exhibits that feature in its narrative,
2

http://www.en.wikipedia.org/wiki/P-value.

123

while an organization responding ‘‘no’’ to a feature indicates that the organization does not exhibit such a feature.
Difficulty of an item translates to strength of the corresponding attitude in defining radical or counter-radical
ideology of any organization. Similarly ability of a subject
in this case means the degree of radicalism or counterradicalism exhibited by an organization’s rhetoric. Other
works in text-mining domain, such as sentiment analysis,
have used Rasch model in their analysis (Drehmer et al.
2000). Details of keyword extraction and selection are
presented in Sect. 3.3.

3 Methods
3.1 Problem definition
The primary goal of this study is to build a semi-automated
method to rank religious organizations from a certain
geographical region on a scale of radicalism versus counterradicalism using their web sites. The efficacy of the generated model is evaluated by comparing it against baseline
methods and expert-level performance. In addition to
accomplishing these goals, we also present an end-to-end
system architecture, and a graphical user-interface design to
facilitate faceted search and browsing of this corpus.
3.2 System architecture
A summary of the system architecture can be seen in
Fig. 1. The system is a composition of four components: a
data-gathering component, which does web crawling, and
text extraction; a scale generation component, performing
scaling algorithms; application services component, which
consists of several web services, and finally, a web userinterface component, presenting the data to the end user.
3.2.1 Data gathering
Initially, social scientists are invited to use their domain
and area expertise to identify a set of organizations, and
hypothesize any number of unipolar or bipolar scales that
could explain the variance among their beliefs and practices. Next, a set of web crawling scripts are created for
extraction of articles from those organizations’ web sites.
For each organization’s corpus, we extract their top-k
n-grams, and a union of all these phrases are presented to
experts for feature selection. Downloaded articles are then
converted into XML structures, containing their original
text, their set of keywords, and extracted information such
as person, location and organization names using a NER
tool for Indonesian language, and their machine translations into English.

A system for ranking organizations using social scale analysis

economic, and religious} keywords corresponding to
beliefs, goals and practices. During this process, our team
of experts screened a total of 790 candidate keywords and
they selected 29 keywords for inclusion in the radical scale,
and 26 keywords for inclusion in the counter-radical scale.
3.4 Debates and perspective analysis

Fig. 1 An overview of the system architecture

An example document snippet is shown in Fig. 2. Here
the original input (content, source), and a sample of the
automatically extracted information corresponding to
DATE, PERSON, and LOCATION can be seen. The corresponding XML versions for each input document are then
stored in a document database for processing.

Upon inspecting the keywords selected by our team of
experts, we observed that some of these keywords correspond to differing perspectives on a set of topics that are
debated within these web sites. Definition of debate is ‘‘a
formal discussion on a particular topic in a public meeting or
legislative assembly, in which opposing arguments are put
forward’’.3 During a debate on a particular topic, like education, both radical and counter-radical organizations discuss different perspectives such as ‘‘secular multi-cultural
education’’ versus ‘‘Sharia based religious education’’.
To design an automated perspective detection algorithm,
we made the following simplifying assumptions.
1.

3.3 Keyword extraction and selection
2.
To identify candidate keywords, one option was to translate
the documents into English and apply readily available
keyword-extraction methods (Michael 2010). However, it
was preferable to preserve the original expression of the
phrases in the original language. Hence, we utilized a nonlinguistic technique that relies only on statistical occurrence, and frequency information.
Within each document, the words were separated by
whitespace or punctuation marks. We considered each
keyword to be an n-gram of one to three words. We treated
each organization as one document and calculated the term
frequency-inverse document frequency (TF-IDF) (Salton
1988) values for every single n-gram mentioned by these
organizations. Top 100 n-grams with the highest TF-IDF
values from each organization were used to generate a
candidate list of topics that these organizations discuss
most frequently. Next, we asked our team of experts to
screen and manually select identify {social, political,

Fig. 2 A portion of a document represented in the system

Organizations will mostly discuss their own perspective in a debate.
Organizations will occasionally mention others perspectives, however, then relate them back to their own
perspective.

In the following sections, we present a mathematical
formulation of the perspective keyword-generation problem
for a given topic, provide an NP-completeness proof, and
design an exact solution through an integer linear programming (ILP)-based solver. Our future work involves finding
an efficient approximation algorithm for this problem.
3.4.1 Perspective keywords-generation problem
Perspective keywords-generation problem (PKGP) is
defined as follows. Given a topic (a keyword) T, and two sets
of documents TR and TCR where TR contains n documents
TR ¼ fDR;1 ; DR;2 ; . . .; DR;n g and TCR contains m documents
TCR ¼ fDCR;1 ; DCR;2 ; . . .; DCR;m g: From each document
DR;i 2 TR ðDCR;j 2 TCR Þ, we collect a set of words
WR,i, V1 B i B n (WCR,j, V1 B j B m) which appear two
words before and two words after each occurrence of the
topic T in that document. Let us define W as the union of all
the WR,i, V1 B i B n and WCR,j, V1 B j B m. If the cardinality of W is p, then W can be given as W ¼ fw1 ; w2 ; . . .;
wp g ¼ fWR;1 [ WR;2 [    [ WR;n [ WCR;1 [ WCR;2 [    [
WCR;m g
Let the frequency of word wk in document DR,i is given
as fR,i(wk) and the frequency of word wk in document DCR,j
as fCR,j(wk).
3

Oxford online dictionary.

123

S. Tikves et al.

Question: Are there two non-empty disjoint subsets of
W, named W 0 and W 00 and W 0 \ W 00 ¼ ;; such that for
every DR,i V1 B i B n,
X
X
fR;i ðwk Þ 
fR;i ðwl Þ
ð1Þ
wk 2W 0

wl 2W 00

and for every DCR,j V1 B j B m,
X
X
fCR;j ðwk Þ 
fCR;j ðwl Þ
wk 2W 0

ð2Þ

wl 2W 00

and jW 0 j þ jW 00 j  K?
In optimization version of the problem, we will try to
minimize jW 0 j þ jW 00 j:
3.4.2 Computational complexity of PKGP
Definition 1 [Weak Partition problem (WPP)] Instance
A finite set A ¼ fa1 ; . . .; an g and a size sðai Þ 2
Z þ ; 8i; 1  i  n: Question Does the set A contain two nonempty sub-sets A1 and A2 that (1) A1 \ A2 ¼ ;; (2) A1 [
P
P
A2  A and (3) ai 2A1 sðai Þ ¼ aj 2A2 sðaj Þ?
WPP has been shown to be NP-complete in (van Emde
Boa 1981).
Theorem 1

PKGP is NP-complete.

Proof It is easy to see that PKGP is in NP since a nondeterministic algorithm needs only to guess a partition of
the word set W into W 0 and W 00 and check in polynomial
time if all the constraints hold for this partition and also if
jW 0 j þ jW 00 j  K:
WPP is a restricted version of PKGP. First we create a
restricted instance of PKGP as follows: let TR and TCR
contains one documents each, i.e, TR = {DR,1} and
TCR = {DCR,1}. Frequency of a word wi 2 W; 81  i  n in
document DR,1 and DCR,1 is taken to be equal, i.e.,
fR,1(wi) = fCR,1(wi) = s(ai). The parameter K is taken to be
equal to |W|. This instance of PKGP is similar to an
instance of WP in the following way: the set A contains
element ai for every word wi 2 W: So, |W| = |A|. In addition, s(ai) = fR,1(wi) = fCR,1(wi), V1 B i B n.
If we find a weak partition of A, as sets A1 and A2 such that
P
P
ai 2A1 sðai Þ ¼
aj 2A2 sðaj Þ, then we can find subsets of
W, as sets W1 and W2, such that wi 2 W1 if ai 2 A1 and
P
wj 2 W2 if aj 2 A2 , respectively. In addition, ai 2A1 sðai Þ ¼
P
P
0
aj 2A2 sðaj Þ; implies that both the constraints
P
P
P wi 2W
fR;1 ðwi Þ  wj 2W 00 fR;1 ðwj Þ and wi 2W 0 fCR;1 ðwi Þ  wj 2W 00
fCR;1 ðwj Þ are true, because s(ai) = fR,1(wi) = fCR,1(wi),
V1 B i B n. Since K = |W|, the constraint jW 0 j þ jW 00 j  K
will trivially hold. So, WPP is a restricted version of PKGP.

123

Since WPP is known to be NP-complete, PKGP is also NPcomplete.
3.4.3 Integer linear programming formulation for PKGP
We formulate an ILP to solve the PKGP optimally. For
each word wi 2 W, we use two variables xi and yi. xi is 1 if
and only if the word wi is in W1 and yi is 1 if and only if the
word wi is in W2. Then constraint (3) means sets W1 and W2
disjoint. Constraint (4) ensures that these sets (W1 and W2)
are also non-empty. Constraints (5) and (6) ensure the
constraints 1 and 2 in problem statement. The objective
minimizes the summation of cardinality of W1 and W2.
Variables: For each word wi,

1; if word wi is assigned to set W1
xi ¼
0; otherwise.

1; if word wi is assigned to set W2
yi ¼
0; otherwise.
Pp
min
i¼1 xi þ yi
s:t: xi þ yi  1;
n
X

xi  1 and

i¼1

n
X

8i ¼ 1; . . .; p
yi  1;

8i ¼ 1; . . .; p

ð3Þ
ð4Þ

i¼1

p
X

fR;i ðwk Þðxk  yk Þ  0;

8i ¼ 1; . . .; n

ð5Þ

fR;i ðwk Þðxk  yk Þ  0;

8i ¼ 1; . . .; m

ð6Þ

wk 2W
p
X
wk 2W

xi 2 f0; 1g; yi 2 f0; 1g;

8i ¼ 1; . . .; p

ð7Þ

3.4.4 Social scale generation
Social scale generation is done by building response tables;
a pair of tables for a bipolar scale, such as radical/counterradical (R/CR), or a single table for a unipolar scale, by
thresholding the occurrence frequencies of the selected
keywords in the organizations’ web corpus.
The scale-generation architecture is shown in Fig. 3.
Here, the flow of the processes and data can be seen as
interactions between experts and automated modules. The
system works as follows.
–

–

Initially, area experts to identify a set of organizations,
and hypothesize any number of unipolar or bipolar
scales that could explain the variance among the beliefs
and practices of the organizations.
Next, we crawl and download the web sites of the
organizations, and the system automatically extracts
the top-k candidate keywords for consideration in the

A system for ranking organizations using social scale analysis

–

Two types of other information are collected for
evaluation purposes. First, expert rankings of the
organizations, using a graphical drag-and-drop expertopinion elicitation tool shown in Fig. 11. Expert
rankings are merged into a consensus gold standard
of rankings. Next, two other computational baseline
methods; one based on simple sorting, and another
based on principal component analysis (Jolliffe 2002),
are used to generate alternative computational rankings
shown in Fig. 12.

In addition, the data for the violence/non-violence are
gathered using a separately developed tool, by collecting
the opinion of the experts. A future work will also include
automated generation of this dimension, as well.
3.5 Feature extraction

Fig. 3 A model of the system architecture

–

–

hypothesized scale. Social scientists screen the list of
extracted keywords, and select the relevant ones for
inclusion in further analysis.
The system builds response tables; a pair of tables for a
bipolar scale (such as radical/counter-radical R/CR), or
a single table for a unipolar scale, by thresholding the
occurrence frequencies of the selected keywords in
the organizations’ web corpus. See Figs. 4 and 5 for the
response tables for the R/CR scale.
The response tables are fed as input to the Rasch Model
building algorithm. The algorithm produces a metric to
validate the fitness of the model, and rankings of the
organizations and keywords. Figures 6 and 7 show the
relative positions of the organizations and keywords on
the latent scales. The algorithm also produces a metric
to validate the fitness of the model.

After identifying the keywords for the analysis, we
needed to search the web site corpus of the organizations
for the matching items. This yielded a term-document
matrix.
This task was performed in a simple three-step procedure; initially, the occurrence frequencies of particular
keywords were counted within each organization’s corpus, then, a threshold matrix was calculated from the
initial values, and finally, a binary response matrix was
generated by applying these thresholds to the initial
values.
The frequency metric is shown in formula 8, where k is
the keyword, o is the organization, and Do is the document
set pertaining to that particular organization.
fo;k ¼

jfdjk 2 d; d 2 Do gj
jDo j

ð8Þ

A threshold value for each keyword is calculated by taking
the median of the values in the related column. Median was
preferred over mean as a threshold, since the distribution of
the values did not fit Gaussian distribution, yet median
empirically proved to be a better measure.

Fig. 4 Radical subset of
organizations and keywords,
sorted according to aggregate
row values

123

S. Tikves et al.
Fig. 5 Counter-radical subset
of organizations and keywords,
sorted according to aggregate
row values

Fig. 6 Radical subset of organizations and keywords

Finally, each element was converted into a binary value by
comparing it to the column’s threshold. English translations of
the keywords are presented for clarity in Figs. 4 and 5.
3.6 Model fitting
We fit the Rasch model on two datasets: (1) radical organizations with radical keywords and (2) counter-radical
organizations with counter-radical keywords. We used the
eRm package in R, an open source statistical software
package,4 to fit a Rasch model to the dataset, and obtain the
organizations’ scores on the latent scale, which are the the
subject parameter estimates (hv) discussed in the previous
section. The eRm package5 fits Rasch models and provide
subjects or organizations parameter estimates based on
maximum likelihood estimation.
4
5

http://www.cran.r-project.org/.
http://www.r-forge.r-project.org/projects/erm/.

123

The automated scale of the organizations is formed by
ranking the organizations according to their estimates on
the latent scale. Not only we can provide the organization
estimates but we can also assess whether the model fits the
data by looking at several goodness of fit indices, such as
the Andersen’s LR-test.
3.7 Application services
We use two backend services in the application layer to
present the data to the user interface. First, all the extracted
textual information are stored in Apache Solr,6 providing
facilities like full-text search and faceting (Tunkelang
2009), using an AJAX interface. In addition, a WCF-based
scaling service is used to infer scales in real time. This
particular service loads the response table, and the previously generated scale data, and estimates the R/CR scale
6

http://www.lucene.apache.org/solr/.

A system for ranking organizations using social scale analysis

Fig. 7 Counter-radical subset of organizations and keywords

–

It would be preferable to plot the locations on the same
range as the input collection. However, the Rasch scale
is on a latent range (Figs. 8, 9).

We resolve the first issue by uniformly scaling the
ranges into [-10, 10], making it consistent with the inputs.
The second issue requires a more specific solution. We
make use of the fact that the raw person scores pertaining
to number of positive responses is a sufficient statistics for
the Rasch model (G 1961) to estimate scale values on the
fly. Since we know the date range, and the selected organizations currently visible in the user interface, it is possible to quickly generate a response matrix for this subset
of the data, and merge it with the previously known scale
information to generate interpolated scale values.

0

2

Radical Scale

−2

The user interface is responsible for representing our
input data, and the findings to the experts in an interactive fashion. Users should be in control of the selection
of the data displayed, and filtering with organization
names, or a specific date range, or using other parameters
such as arbitrary keywords, or geographic locations.
While performing these tasks, it should provide results to
the user with a minimum of delay, allowing quick drilling down to interactively model the scenarios that users
have in mind.
The user interface is implemented as an interactive
AJAX-based application, using ajaxsolr7 framework. In
addition to the search and navigation capabilities provided
with ajaxsolr, it also adds functional widgets for visualizing
the organizations on a scale, mapping the intensity of the
locations, displaying demographics trends, and so on. A
more detailed discussion of the user interface is provided in
Sect. 5.
The presentation of the scale, however, brings the following challenges.

Since this will be an interactive application, users
would prefer to see almost instantaneous results. Yet,
the eRm model generation is computationally
expensive.

−4

3.8 User interface

–

Person Parameters (Theta)

for a subset of the input. Number of positive responses are
interpolated on the scale to generate the scale, and the
expert opinion is used for a static violent/non-violent
(V/NV) scale. While the interpolation is based on a sufficient statistics, future work on speeding up Rasch model
generation for real-time use would be beneficial.

0

5

10

15

20

25

30

Person Raw Scores
7

http://www.evolvingweb.github.com/ajax-solr/.

Fig. 8 Radical scale

123

S. Tikves et al.

Here we have opted to include all the organizations in
threshold calculations. This is because, the radical or
counter-radical activity intensities are always measured
relative to the other organizations participating in the same
time period. However, while the scale is based on all the
organizations, only the ones specifically asked will be
presented to the user.

4 Experimental evaluation
4.1 Indonesian corpus
The corpus domain is the online articles published by the
web sites of the 23 religious organizations identified in
Indonesia, in the Indonesian language. These sources are
the web sites or blogs of the identified think tanks and
organizations. As discussed in the Sect. 1, each source
was classified as either radical or counter-radical by the
area experts. We downloaded a total of 37,000 Indonesian articles published in these 23 web sites, dating from
2001 to 2011. For each web site, a specific REGEX filter
was used to strip off the headers, footers, advertising
sections and to extract the plain text from the HTML
code.
The psuedo-code for the subset scale-generation procedure is presented in Algorithm 1. The process starts with
identifying the subset of documents in the (start, end) date
range (lines 2–5). Then the keyword frequencies, and
thresholds are calculated for the entire set of organizations
on this document subset (lines 6–14). Finally, response
tables for the subset of organizations is generated (lines
15–17), and then the sums need to be interpolated (lines
18–23), to be able to generate a scale on the [-10,10] range
(line 24).

2
0
−2
−4

Person Parameters (Theta)

Counter−Radical Scale

0

5

10

15

Person Raw Scores

Fig. 9 Counter-radical scale

123

20

25

4.2 The quadrants model
Our project leverages the results of our previous work,
which relied on social theory including Durkheim’s (2004)
research on collective representations, Simmel’s (2008)
work on conflict and social differentiation, Wallace’s
(1956) writings on revitalization movements, and Tilly and
Bayat’s studies on contemporary social movement theory
(Tilly 2004; Bayat 2007). Our team has also developed,
and is currently testing a theoretically based class model
comprised of continuous latent scales. The first pair of
scales focus on distinctions between the goals and methods
of counter-radical and radical discourse, and capture the
degree to which individuals, groups, and behaviors aim to
influence the social order (change orientation) and the
methods by which they attempt to do so (change
strategies).
Quadrants model (see Fig. 10) captures multiple social
trends in four quadrants (A, B, C, and D), and it makes the
significant distinction between violent and not-violent
dimensions of both radicalisms and counter radicalisms.
Using the quadrants model, a researcher can locate organizations, individuals, and discourses in broader categories
while still considering subtle differences between groups
within categories. A researcher can document movement
and trends from category to category, and identify points
where movement is likely to happen.

A system for ranking organizations using social scale analysis

rankings. The individual scores for each organization were
combined and averaged to obtain the consensus gold
standard rankings along the hypothesized R/CR scale.
A work is in progress for building a publicly accessible
expert opinion collection toolkit. The preliminary version
can be accessed at: http://www.minerva-project.org/
DataCollector.
4.4 Computationally generated scale

Fig. 10 The quadrants model

4.3 Expert opinion and gold standard of rankings
We collaborated with three area experts, who collectively
possess 35 years of scholarly expertise on Indonesia and
Islam. To build a gold standard of orderings of the organizations, we built a graphical drag-and-drop user-interface
tool to collect the opinions of each of the area experts.
A screenshot of the tool is shown in Fig. 11.
Each expert, separately evaluated and ranked the organizations in the dataset according to a two dimensional
scale of radical/counter-radical (R/CR) and violent/nonviolent (V/NV) axis. The consensus among the experts was
high; since per item standard deviations among the experts’
scores along the R/CR axis over a range of [-10,
10], across all organizations were 2.75. In addition, 90 %
of the items have less than 22.6 % difference in their

The ranking discovered by the Rasch model fitting the corpus
has been evaluated against the gold standard rankings of the
organizations provided by the experts. The difference
between two separate rankings have been calculated using
the following misplacement error measure in Eq. (9).
P
jGðoÞRðoÞj
errorðG; RÞ ¼

o2O

jOj

jOj

ð9Þ

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1,|O|].
For two exactly matching rankings, the error(G, R) will be
zero, whereas for two inversely sorted rankings it is
expected to be 0.5 (when the size of O is even). In addition,
a random ranking is expected to have a error of 0.375.
4.5 Expert-to-gold standard error
We calculated the error between each expert’s ranking and
their consensus gold standard of rankings. The first expert’s
error measure is 0.06, and the second and third expert’s
errors are 0.12 and 0.14 correspondingly as shown in the
last row of the table in Fig. 12. The average error of our
experts against their gold standard ranking is 0.11.
4.6 Baseline: sorting with aggregate score
The first baseline we used was constructed by sorting the
organizations according to the number of different keywords
observed in their corpus. While this provided a pattern
similar to a Guttman scale, and orderings of the organizations
matched to a certain degree with the gold standard as shown
in Fig. 12, the error for this baseline was 0.19, which is
higher than the average expert’s performance.
4.7 Baseline: principal component analysis

Fig. 11 The visual interface of the expert-opinion collector for
manually placing the organizations on the two dimensional scale

A stronger baseline was built by employing principal
component analysis (Jolliffe 2002), and sorting the organizations according to their projections in the first principal
component of the term–document matrix. Since experts
selected the R/CR scale relevant keywords only, it was
expected that the first principal component would reflect
the corresponding scale. PCA proved to be performing

123

S. Tikves et al.
Fig. 12 Computational and
expert rankings

better than the aggregate score sorting, with an error
measure of 0.18. However, this error rate is still higher than
the error rate of each expert.

information and patterns to enable a computational method
to rank them accurately.

4.8 Performance of the Rasch model ranking system

5 Web application overview

The p values from the Anderson LR goodness of fit test from
model (1) and model (2) (mentioned in Sect. 3.6) are 0.85 and
0.669, respectively, suggesting no evidence of lack of fit. The
Rasch models allow us to get a natural order of the organizations, according to their ‘‘abilities’’, i.e., radicalism and
counter-radicalism in this case. This system had an error
measure of 0.10, which actually provided a higher ranking
performance than the average performance of our experts’—
performing better than the majority of our area experts.

A sample snapshot of the web application can be seen in
Fig. 13. It is composed of four main widgets for visualization and navigation. The top-left section which contains
the Search and Navigation widget (1) that allows filtering
of the document subset using parametric search queries and
keyword based search criteria. The top-right section is the
Quadrant widget (2) which displays the organizations
active in the currently selected time frame on a twodimensional axis, using violence and radicalism scales. The
bottom-left section consists of two Treemap widgets (3)
which displays the demographics and the top keywords
(markers) of the current selection. The bottom-right section
has a Timeline widget (4) which provides a visualization
of the keywords (markers) trends on a time line.
The navigation in the user interface starts with the
Navigation widget (top-left) of the web application. Here
the user is able to filter down the corpus utilizing full-text
search queries, or faceting using keywords, locations,
demographics, or choosing a subset of organizations.
The Quadrant widget (top-right) provides a plot of
the currently selected organizations on the two dimensional scale. The radical/counter-radical (R/CR) axis is

4.9 Evaluations
Our experiments showed that the hypothesized compatibility
of the R/CR scale for the Indonesian corpus is valid. Not only
the Rasch model was statistically fitting the response matrix
but also the generated ranking performance was better than
the average expert performance. Among our computational
baseline methods, the Rasch Model was the only method
producing expert-level performance as shown in Fig. 12.
This preliminary analysis with the R/CR scale shows that
when experts assist the system with keyword selection, the
web corpus of organizations provides rich-enough

123

A system for ranking organizations using social scale analysis

Fig. 13 A sample snapshot of the web application (color figure online)

dynamically calculated in real time, using the subset of
organizations, and the time range of the current selection.
The location change on the time range for each organization is shown as a color-coded path, with three markers, a
light circle corresponding to the position at the beginning
of the period, a dark circle corresponding to the end of the
period, and a dark-small circle for the middle. A red line
between the circle denotes the rise of radical activities in
the organization’s behavior. A blue line denotes the
opposite. The smaller circle is useful to see the overall
movement of an organization. For example, between the
range Aug 2005 and Aug 2007, EraMuslim’s activities
were radical (center of A quadrant), then became almost
counter radical (the smaller circle denotes this mid point in
the movement), and then jumped up again. The V/NV axis
is retrieved from expert opinion in the current version, and
dynamic calculation of this axis is left for a future version.
The Timeline widget (bottom-right) displays the trends of
the most frequent markers on a time line. Initially the subset
of markers presented defaults to all available, however it is
possible to restrict the selection of markers to a more limited
set among radical/counter-radical, economical, political,
religious, or social domains. Timeline widget can also be
used for selecting a date range of interest.
The Treemap widgets (bottom-left) are used to display
the relative frequencies of demographics and keywords

(markers). The displayed marker category selection for this
widget is synchronized with the Timeline widget.
In the following sections, we present some scenarios and
findings to illustrate the capabilities of the web interface.
5.1 Scenario 1: radical organizations’ trends
In this scenario, we analyze both violent and non-violent
radical organizations. Our web application shows the ideologies that these organizations are propagating. We can
see8 the most prominent markers associated with these
radical organizations. Markers such as ‘‘infidel’’, ‘‘Sharia’’,
and ‘‘violence’’ show an increasing trend between 2001 and
2011. A very strict interpretation of ‘‘Sharia’’ is used by
radical organizations to justify their actions (Widhiarto
2010; Hasan 2009). ‘‘Sharia’’ peaks during this period as
shown in Fig. 14.
5.2 Scenario 2: C-quadrant organizations’ trends
We now analyze Front Pembela Islam (FPI), an Islamic
organization in Indonesia established in 1998. FPI is well
known for its violent acts (Frost et al. 2010; Rondonuwu
8

Select the filter ‘‘Radical’’ from the search options and then in the
Markers Menu select [Religious ! Radical Markers].

123

S. Tikves et al.

Fig. 14 Trend of radical markers

2010) justified by a strict interpretation of Sharia (for the
Study of Terrorism 2011). Our documents for FPI ranges
between 2000 and 2010. Using our web application’s plots of
the movement of FPI in the C Quadrant, we found that FPI
consistently rised higher on the radical scale as shown in
Fig. 15. We selected the following time ranges, 2000–2003,
2002–2006, 2006–2010 and analyzed the trends of various
markers associated with FPI. There was a substantial increase
in the intensity of various radical markers such as ‘‘infidel’’,
‘‘Mujahedin’’, ‘‘pornography’’.9 Since 2006, we also saw a
steep increase in the frequency of marker ‘‘Ahmadiyya’’, as
shown in Fig. 16, which indicates FPI’s increased opposition
to this heretical sect (Rahmat and Sihaloho 2011).

Fig. 15 Consistent rise of FPI on the radical scale

5.3 Scenario 3: A-quadrant organizations’ trends
We analyze Hizb ut-Tahrir also known as Hizb ut-Tahrir
Indonesia (HTI), a radical organization widely believed to
be non-violent (Ward 2009), which has been active in
Indonesia since 1982 (Osman 2011). Between 2007 and
2009, our web application shows various radical and nonradical markers associated with this organization.

Radical

Non-Radical

‘‘Sharia’’, ‘‘Infidel’’,
‘‘Caliph’’, ‘‘Violence’’

‘‘Politics’’, ‘‘Indonesian Islam’’,
‘‘Election’’, ‘‘Liberal’’,
‘‘Democracy’’

During the same period, we see a steady increase in the
frequency of the radical marker ‘‘Sharia’’. This is consistent with one of HTI’s goals of implementing Sharia in
Indonesia (Hasan 2009). Hizb ut-Tahrir openly propagates
9

Select ‘‘Radical’’ and ‘‘FPI’’ from the filters, then select the time
range 2002–2006 or 2006–2010, then select ‘‘radical’’ markers under
‘‘R/CR’’ menu.

123

Fig. 16 ‘‘Ahmadiyya’’ peaking during the period 2006–2010

Fig. 17 ‘‘Khilafah’’ ideology of Hizb ut-Tahrir

the ideology of Khilafah, which believes in unification of
all Muslim countries as a single Islamic State (Zakaria
2011; Mohamed Osman 2010). Figure 17 shows

A system for ranking organizations using social scale analysis

Searching for the text ‘‘suicide bombing’’, we see that
one of the related markers is ‘‘ideology’’. Adding the
keyword ‘‘ideology’’ to the search filter reveals a new set of
markers including the ‘‘sin’’ keyword. Adding ‘‘sin’’ to our
search, we obtain a set of matching documents. One of the
top matches, is titled ‘‘Mengapa Saya Berubah?’’ (english
translation: ‘‘Why I changed?’’)13. This article is by a
reformed terrorist, debunking the misinterpretation of the
jihad-related verses used by violent groups.

6 Conclusions and future work

Fig. 18 Decline of the HTI in the radical scale

‘‘Khilafah’’ as the most prominent marker10 in Hizb utTahrir’s discourse.
By looking at the Quadrants widget (in Fig. 18), we can
infer that HTI has been moderating its narrative.
5.4 Scenario 4: B-quadrant organizations’ trends
In this scenario, we discuss the trends of counter radical
organizations like NU and DaarulUluum. We also show an
interesting scenario on the topic of ‘‘Suicide Bombing’’
using the keyword based Navigation widget.
The ‘‘counter radical’’ markers11 associated with these
organizations are: ‘‘politics’’, ‘‘election’’, ‘‘Indonesian
Islam’’, ‘‘liberal’’, ‘‘human rights’’. These organizations
support democracy and elections, which is shown by the
high frequency of the markers ‘‘politics’’ and ‘‘election’’.
Their narrative has local interpretation of Islam at its core,
which is shown by the marker ‘‘Indonesian Islam’’.
On analyzing the occurrences of radical markers12 in
B-Quadrant, we find that counter radical organizations are
very vocal against all of radical markers. One of the
interesting radical markers is ‘‘Suicide Bombing’’. Most of
the counter radical organizations are against suicide
bombings.(Malang 2006). We will now demonstrate how
combination of parametric and keyword search, and various widgets in the web application can help reveal opposition to ‘‘Suicide Bombing’’ by counter-radical
organizations.
10

Select ‘‘Hizb ut-Tahrir’’ and ‘‘radical’’ from filters. Select the time
range 2007–2009. The markers can be seen by selecting the options of
Markers Menu [Religious ! Religious Markers].
11
Select CounterRadical filter in the search option, then from the
Markers Menu select [R/CR ! Counter Radical].
12
In the Markers Menu select [R/CR ! Radical].

In our experiments, not only did the data show fitness with
the Rasch Model for the R/CR scale but also the Rasch
rankings of the organizations are better than the output of
the other baseline computational methods, and they are at
expert-level performance when compared with the consensus gold standard rankings.
Rasch model also provided us with another output,
namely the ranking of selected keywords (items) on the
R/CR scale. Although preliminary observations indicates
that this can be a valuable asset by itself, we plan to further
investigate the quality and utility of this ranking as future
work.
While the model has been demonstrated to fit on the
R/CR scale, two major expansion points can be investigated in the future work, namely the violent/non-violent
scale, and enhancement of feature selection. Although our
experts have identified a second dimension, evaluating its
correlation to R/CR axis, or existence of other significant
ones could be beneficial. In addition, the features can be
enhanced by experimenting with the significance of the
radical keywords in the counter-radical organization corpora, and vice-versa.
A practical method to increase the automation of keyword generation has been discussed in Sect. 3.4. Future
work will involve finding an efficient approximation
algorithm for this model, for decreasing the necessity of
expert interaction for this particular step.
Other interesting work includes making our expert
opinion elicitation tool available online to a wider and
more geographically distributed audience to crowdsource
(Snow et al. 2008) the needed expertise for making lists of
local organizations, identifying their web sources, and
overcome the complex task of construction and validation
of significant and fitting scales (work is currently underway
to build this tool). Another interesting dimension is to look
at synthesis and analysis of scales that do have a strict
hierarchy of keywords, but adhere to more flexible partial
order models (James and John 2002).
13

http://www.islamlib.com/id/artikel/mengapa-saya-berubah/.

123

S. Tikves et al.
Acknowledgments This research was supported by US DoDs
Minerva Research Initiative Grant N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the project title
is ‘‘Finding Allies for the War of Words: Mapping the Diffusion and
Influence of Counter-Radical Muslim Discourse’’.

References
Andrich D (1988) Rasch models for measurement. Sage, USA
Bayat A (2007) Making Islam Democratic: social movements and the
post-Islamist turn. Stanford University Press, USA
Crelinsten R (2002) Analysing terrorism and counter-terrorism: a
communication model. Terror Political Violence 14:77–122
Davulcu H, Ahmed ST, Gokalp S, Temkit MH, Taylor T, Woodward
M, Amin A (2010) Analyzing sentiment markers describing
radical and counter-radical elements in online news. In:
Proceedings of the 2010 IEEE second international conference
on social computing, IEEE Computer Society, SOCIALCOM’10, pp 335–340
Drehmer D, Belohlav J, Coye R (2000) An exploration of employee
participation using a scaling approach. Group Org Manage
25(4):397
Durkheim E (2004) The cultural logic of collective representations:
social theory the multicultural and classic readings. Wesleyan
University: Westview Press
Frost F, Rann A, Chin A (2010) Terrorism in southeast asia.
http://www.aph.gov.au/library/intguide/FAD/sea.html [Online
accessed 21 Nov 2011]
Guttman L (1950) The basis for scalogram analysis. Meas Predict
4:60–90
Hasan N (2009) Islamic militancy, Sharia, and democratic consolidation in post-Suharto Indonesia. RSIS Working Papers. 143/07
Hessen D (2010) Likelihood ratio tests for special Rasch models.
J Edu Behav Stat 35(6):611
Hunter D, Lange K (2004) A tutorial on mm algorithms. Am Stat
58(1):30–37
James AW, John LM (2002) Algebraic representations of beliefs and
attitudes: partial order models for item responses. Sociol
Methodol 29:113–146
Jolliffe I (2002) Principal component analysis: Springer series in
statistics. Springer, Germany
Le Cam L (1990) Maximum likelihood an introduction. ISI Rev
58(2):153–171
Likert R (1932) A technique for the measurement of attitudes. Arch
Psychol 140:1–55
Malang (2006) NU chairman deplores suicide bombing attempt.
http://www.nu.or.id/page/en/dinamic_detil/15/28282/News/NU_
chairman_deplores_suicide_bombing_attempt.html [Online
accessed 22 Nov 2011]
McIver J, Carmines E (1981) Unidimensional scaling, vol 24. Sage
Publications Inc, USA
McPhee RD, Corman S (1995) An activity-based theory of communication networks in organizations, applied to the case of a local
church. Commun Monogr 62:1–20
Michael WB, Kogan J (2010) Text mining: applications and theory.
Wiley, London
Mohamed Osman MN (2010) Reviving the Caliphate in the
Nusantara: Hizbut Tahrir Indonesia’s mobilization strategy and
its impact in Indonesia. Terror Political Violence 22(4):601–622.

123

doi:10.1080/09546553.2010.496317.
http://www.tandfonline.
com/doi/abs/10.1080/09546553.2010.496317
Osman MNM (2011) Preparing for the caliphate. Asian Stud Assoc
Aust E-Bull 80:14–16. ISSN:1449-4418
Pawitan Y (2001) In all likelihood: statistical modelling and inference
using likelihood. Oxford University Press, USA
Rahmat Sihaloho M (2011) FPI vows to disband ahmadiyah
’whatever it takes’. http://www.thejakartaglobe.com/home/fpivows-to-disband-ahmadiyah-whatever-it-takes/423477 [Online
accessed 21 Nov 2011]
Rasch G (1961) On general laws and the meaning of measurement in
psychology. In: Proceedings of the fourth Berkeley symposium
on mathematical statistics and psychology, 4, p 332
Rondonuwu O, Creagh S (2010) Opposition grows to indonesia’s
hardline fpi islamists. http://www.in.reuters.com/article/
2010/06/30/idINIndia-49777620100630 [Online accessed 21
Nov 2011]
Salton G, Buckley C (1988) Term-weighting approaches in automatic
text retrieval. In: Information Processing and Management, vol
25, pp 513–523
Simmel G (2008) Sociological theory. McGraw-Hill, New York
Snow R, O’Connor B, Jurafsky D, Ng AY (2008) Cheap and fast—
but is it good?: evaluating non-expert annotations for natural
language tasks. In: Proceedings of the conference on empirical
methods in natural language processing, EMNLP ’08,
pp 254–263. Association for Computational Linguistics, Stroudsburg, PA, USA. http://www.portal.acm.org/citation.cfm?
id=1613715.1613751
for the Study of Terrorism NC, to Terrorism R (2011) Terrorist
organization profile: front for defenders of Islam. http://www.
start.umd.edu/start/data_collections/tops/terrorist_organization_
profile.asp?id=4026 [Online accessed 21 Nov 2011]
Thurstone LL (1928) Attitudes can be measured. Am J Sociol
33:529–554
Tikves S, Banerjee S, Temkit H, Gokalp S, Davulcu H, Sen A,
Corman S, Woodward M, Rochmaniyah I, Amin A (2011) A
system for ranking organizations using social scale analysis. In:
EISIC. IEEE, pp 308–313. http://www.ieeexplore.ieee.org/xpl/
mostRecentIssue.jsp?punumber=6059524
Tilly C (2004) Social Movements. Paradigm Publishers, USA
Tunkelang D (2009) Faceted Search: synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool
Publishers, UK. doi:10.2200/S00190ED1V01Y200904ICR005
van Emde Boas P (1981) Another NP-complete partition problem and
the complexity of computing short vectors in a lattice: Tech.
Rep. 81-04. Mathematisch Instituut, Amsterdam
Wallace A (1956) Revitalization movements. Am Anthropol
58:264–281
Ward K (2009) Non-violent extremists? Hizbut Tahrir Indonesia.
Aust J Intl Affairs 63(2):149–164 doi:10.1080/1035771090
2895103.http://www.tandfonline.com/doi/abs/10.1080/10357710
902895103
Widhiarto H (2010) Radical groups urge Bekasi administration to
implement Sharia law. http://www.thejakartapost.com/news/2010/
06/27/radical-groups-urge-bekasi-administration-implementsharia-law.html [Online accessed 21 Nov 2011]
Zakaria Y (2011) A Global Caliphate: reality or fantasy? http://www.
usa.mediamonitors.net/content/view/full/91207 [Online accessed
21 Nov 2011]

Soc. Netw. Anal. Min.
DOI 10.1007/s13278-012-0072-x

ORIGINAL ARTICLE

A system for ranking organizations using social scale analysis
Sukru Tikves • Sujogya Banerjee • Hamy Temkit • Sedat Gokalp •
Hasan Davulcu • Arunaba Sen • Steven Corman • Mark Woodward
Shreejay Nair • Inayah Rohmaniyah • Ali Amin

•

Received: 20 December 2011 / Revised: 4 April 2012 / Accepted: 20 April 2012
Ó Springer-Verlag 2012

Abstract In this paper, we utilize feature extraction and
model-fitting techniques to process the rhetoric found in
the web sites of 23 Indonesian Islamic religious organizations to profile their ideology and activity patterns along a
hypothesized radical/counter-radical scale, and present an
end-to-end system that is able to help researchers to visualize the data in an interactive fashion on a timeline. The
subject data of this study is 37,000 articles downloaded
from the web sites of these organizations dating from 2001
to 2011. We develop algorithms to rank these organizations
by assigning them to probable positions on the scale. We
show that the developed Rasch model fits the data using
Andersen’s LR-test. We create a gold standard of the
ranking of these organizations through an expertise elicitation tool. We compute expert-to-expert agreements,
and we present experimental results comparing the performance of three baseline methods to show that the Rasch model not only outperforms the baseline methods, but it
is also the only system that performs at expert-level
accuracy.

1 Introduction

S. Tikves (&)  S. Banerjee  H. Temkit  S. Gokalp 
H. Davulcu  A. Sen  S. Corman  M. Woodward  S. Nair
Arizona State University, P.O. Box 87-8809,
Tempe, AZ 85281, USA
e-mail: sukru@asu.edu; stikves@asu.edu

S. Corman
e-mail: scorman@asu.edu

S. Banerjee
e-mail: sujogya@asu.edu

S. Nair
e-mail: snair8@asu.edu

H. Temkit
e-mail: mtemkit@asu.edu

I. Rohmaniyah
Center for Religious and Cross Cultural Studies,
Gadjah Mada University, Yogyakarta, Indonesia
e-mail: rochmaniyah@yahoo.com

S. Gokalp
e-mail: sgokalp@asu.edu
H. Davulcu
e-mail: hdavulcu@asu.edu
A. Sen
e-mail: asen@asu.edu

Being able to asses information on radical and moderate actors
in a geographic area is an important research topic for national
security. Radicalism is the ideological conviction that it is
acceptable, and in some cases, obligatory to use violence to
effect profound political, cultural and religious transformations and change the existing social order fundamentally.
Muslim radical movements have complex origins and depend
on diverse factors that enable translation of their radical ideology into social, political and religious movements. Crelinste
(2002), in his work, states that ‘‘both violence and terrorism
possess a logic and grammar that must be understood if we are
to prevent or control them’’. Therefore, analysis of Muslim
radical and counter-radical movements requires attention to
the global, national and local social, economic and political
contexts in which they are located. Similarly, in the Islamic
context, counter-radical discourse takes various different
forms; discursive and narrative refutations of extremist
claims, symbolic action such as ritual and other religious and

M. Woodward
e-mail: mataram@asu.edu

A. Amin
State College of Islamic Studies (STAIN),
Manado, Indonesia
e-mail: aleejtr77@yahoo.com

123

S. Tikves et al.

cultural practices, and Islamic arguments for pluralism,
peaceful relations with non-Muslims, democracy, etc. The
most effective counter-radicals are likely to be religiously
conservative Muslims. Effective containment and defeat of
radicalism depends on our ability to recognize various levels
of radicalization, and detection of counter-radical voices.
In our previous work (Davulcu et al. 2010), we attempted a
clustering approach to obtain ‘‘natural groupings’’ of a number
of local non-government religious social movements and
organizations in Indonesia. Social scientists on our team
observed that clustering results were not fully able to separate
all counter-radical or radical organizations into pure clusters.
Pure radical clusters were easily identified due to high similarity among their support for violent practices. Pure counterradical clusters were identified due to their strong reactionary
opposition to violent practices through protests and rhetoric.
But the rest of the groupings were mixed. We realized that
binary labeling as counter-radical or radical does not capture
the overlap, movement and interactivity among these organizations. In this paper, we hypothesize that both counterradical and radical movements in Muslim societies exhibit
distinct combinations of discrete states comprising various
social, political, and religious beliefs, attitudes and practices,
that can be mapped to a latent linear continuum or a scale.
Using such a scale, an analyst can determine where exactly
along the spectrum any particular group lies, and also potentially where it is heading with its rhetoric and activity.
Given the complex nature of the task, such as regional
differences in local cultures, beliefs and practices, and in the
absence of readily available high-accuracy parsers, highly
structured religio-social ontologies, and information
extraction systems; we decided to devise a multi-lingual nonlinguistic text processing pipeline that relies on only statistical modeling of keyword frequency and co-occurrence
information. However, we designed the system to be able to
incorporate additional information extracted from the text, if
available. For example, named entity recognition (NER),
machine translation, and GIS-based location lookup information are part of the user-interface presentation.
We (Tikves et al. 2011) worked with social scientists on
our team to come up with an orthogonal model comprising
of two primary dimensions. Both dimensions, (1) radical/
counter-radical and (2) violent/non-violent, are characterized as latent, partial orders of discrete beliefs and practices
based on a generalization of item order in Guttman scaling
(Guttman 1950) using a Rasch model (Andric 1988). A true
Guttman scale is a deterministic process, i.e., if a social
movement subscribes to a certain belief or practice, then it
must also agree with all lower-order practices and beliefs
on the scale. Of course, such perfect order is rare in the
social world. The Rasch model provides a probabilistic
framework for Guttman scales to accommodate for
incomplete observations and measurement errors.

123

We have designed a web-based system to visualize this
orthogonal model. The web tools provided by the system
allows drilling down on specific data, and plotting the trends
and trajectories of organizations on a timeline. It consists of
several modules: an off-line web mining, and data-processing pipeline, two web services for application logic, and an
AJAX-based presentation layer. The web-based interface
built for this study can be accessed through the web site at
http://www.demo.minerva-project.org. In this paper, we
present several scenarios with this tool in Sect. 5.
In this paper, we present feature extraction, feature
selection, and model-fitting techniques to process the rhetoric found in the web sites of 23 religious Indonesian organizations—comprising a total of 37,000 articles dating from
2001 to 2011. We aim to identify their ideology and activity
patterns along a hypothesized radical/counter-radical scale,
and rank them to probable positions on this scale (McPhee
1995). The automated ordering of organizations is formed by
ranking the organizations according to their estimated
positions on the latent scale. We used the eRm1 package to fit
the Rasch model on this data set, and identify organizations’
positions based on maximum likelihood estimation (Le Cam
1990). We show that the model fits the data using the
Andersen’s likelihood ratio test (LR-test) (Hessen 2010). We
also created a gold standard of the ranking of these organizations through an expert-opinion elicitation tool, and
through the opinions of three ethnographers on our team who
collectively possess 35 years of scholarly expertise on
Indonesia and Islam. We computed expert-to-gold standard
agreements, as well as compared the performance of three
different baseline computational methods to show that the
Rasch model presented here not only performs the best
among the baseline methods but that it is also the only
method that performs at an expert level of accuracy.
1.1 Organization of the paper
Next section provides an introduction to the theory of
Guttman scaling and Rash models. Section 3 defines the
problem, presents the system architecture, and the methods
used to solve the problem. Section 4 describes the Indonesian corpus, expert-opinion elicitation tool, baseline
computational methods, and experimental evaluations.
Section 5 discusses the user-interface designed for navigating our findings. Section 6 concludes the paper.

2 Introduction of Guttman scaling and Rasch model
In social science, scaling is a process of measuring and
ordering entities called subjects, based on their qualitative
1

http://www.r-forge.r-project.org/projects/erm/.

A system for ranking organizations using social scale analysis

attributes called items. In general, subjects are requested to
respond to surveys conducted by means of structured
interviews or questionnaires. Items are presented to the
subjects in form of questions. Statistical analysis of the
response of the subjects on the questions about items are
used in scaling the subjects. Some of the widely followed
scaling procedure in social science surveys are Likert scale
(Likert 1932), Thurnstone scale (Thurnstone 1928), and
Guttman scale (McIver 1981). In Likert scale, subjects
indicate their magnitude of agreement or disagreement
about an item (from strongly agree to strongly disagree) on
a five- to ten-point scale. On the other hand, Thurnstone
scale is a formal method of ordering the attitudes of the
subjects toward the items. Guttman scaling procedure
orders both the subjects and the items simultaneously with
respect to some underlying cumulative continuum. In this
paper, we follow the Guttman scaling process to rank the
organizations based on their response on the radical and
counter-radical keywords.
2.1 Guttman scaling
A Guttman scale (Guttman 1950) presents a number of items
to which each subject is requested to provide a dichotomous
response, e.g., agree/disagree, yes/no, or 1/0. This scaling
procedure is based on the premise that the items have strict
orders (i.e., the items are presented to the subjects ranked
according to the level of the item’s difficulty). An item ‘‘A’’
is said to be ‘‘more difficult’’ than an item ‘‘B’’, if any subject
answering ‘‘yes’’ on item ‘‘A’’ implies that the subject will
also answer ‘‘yes’’ on item ‘‘B’’. A subject who responds to
an item positively is expected to respond positively to all the
items of lesser difficulty. For example, to find out how
extreme a subject’s view is on Guttman scale, the subject is
presented with the following series of items in question form.
(1) Are you willing to permit immigrants to live in your
country? (2) Are you willing to permit immigrants to live in
your community? (3) Are you willing to permit immigrants
to live in your neighborhood? (4) Are you willing to permit
immigrants to live to your next door? (5) Are you willing to
permit your child to marry an immigrant? If the items form a
Guttman scale, any subject agreeing with any item in this
series, will also agree with other items of lower rank-order in
this series. Guttman scale is a deterministic process and the
score of a subject depends on the number of affirmative
responses he has made on the items. So, a score of 2 for a
subject in the above Guttman scale not only means he has
given affirmative response to two of the questions or items
but also indicates that he agrees with two particular questions, namely the first and second. Scores in Guttman scale
can also be interpreted as the ‘‘ability’’ of a subject in
answering questions sorted in increasing order of ‘‘difficulty’’. These scores when presented on an underlying scale,

give us an ordering of the subjects based on their ‘‘ability’’
also.
The objective of our paper is to order the Indonesian
Islamic organizations based on their views on religio-social
keywords which have an inherent ordering. For example,
two such keywords are ‘‘Quran’’ and ‘‘Sharia’’. An organization supporting ‘‘Sharia’’ will also likely to ‘‘believe in
Quran’’. So it makes sense to use Guttman scaling procedure to rank the organizations and their beliefs and practices. One drawback of Guttman scale is that it is
deterministic and assumes a strict ordering of the items. In
real world, it is difficult to order all the items in such a
strict level of increasing difficulty, therefore, perfect scales
are not often observed in practice. Furthermore, many
times, the order of the items are not known since they are
not straightforwardly comparable. In addition, measurement errors might lead to responses that do not strictly fit
the ordering. As a result, we can no longer conclude
deterministically that if a subject answers a question
affirmative, whether she will be able to give affirmative
answers to other questions of lower order in the same
questionnaire. We use Rasch model to overcome this
drawback by taking into account measurement error.
2.2 Rasch model
Rasch model (Andric 1988) provides a probabilistic framework for Guttman scales. In Rasch model, the probability of a
specified binary response (e.g., a subject agreeing or disagreeing to an item) is modeled as a function of subject’s and
item’s parameters. Specifically in the simple Rasch model,
the probability of a positive response (yes) is modeled as a
logistic function of the difference between the subject and
item’s parameters. Item parameters pertain to the difficulty
of items while subject parameters pertain to the ability of
subjects who are assessed. A subject of higher ability, related
to the difficulty of an item, has higher probability to respond
to a question affirmatively. In this paper, Rasch models are
used to assess the organizations degree of being radical or
counter-radical based on the religio-social keywords (items)
appearing in their rhetoric.
Rasch model also maps the responses of the subjects to the
items in binary or dichotomous format, i.e., 1 or 0. Let
Bernoulli variable Xvi denotes the response of a subject v to
the item i, variable hv denotes the parameter of ‘‘ability’’ of
the subject v and bi denotes the parameter of ‘‘difficulty’’ of
an item i. According to the simple Rasch model, the probability that the subject v responds 1 for item i is given by:
PðXvi ¼ 1jhv ; bi Þ ¼

expðhv  bi Þ
:
1 þ expðhv  bi Þ

Rasch model assumes that the data under analysis have
the following properties.

123

S. Tikves et al.

Unidimensionality P(xvi = 1|hv, bi, a) = P(xvi = 1|hv,
bi), i.e., the response probability does not depend on
other variable
2. Sufficiency sum of responses contains all information
on ability of a subject, regardless which item it has
responded
3. Conditional independence for a fixed subject, there is
no correlation between any two items
4. Monotonicity response probability increases with
higher values of h, i.e., subject’s ability.
P
Items with si = nv xvi value of 0 or n, and subjects with
Pk
rv = i xvi value of 0 or k are removed prior to estimation,
where n is the total number of subjects and k is the total
number of items. Running Rasch model on the data gives us
an item parameter estimate or a score for each item. In
general, the estimation of bi or score for an item i is calculated through conditional maximum likelihood (CML) estimation (Pawitan 2001). The conditional likelihood function
for measuring item parameter estimate is defined as:
Y
expðbi si Þ
Lc ¼
Pðxvi jrv Þ ¼ Q P
r
xjr expðbi xvi Þ
v
1.

where r represents the sum over all combinations of
r items. Similarly, the maximum likelihood is used to
calculate subject parameter estimation hv or score for each
subject. Expectation-maximization algorithms (Hunter
2004) are used in implementing CML estimation in Rasch
model. We can also assess whether the data fit the model
by looking at goodness of fit indices, such as the Andersen’s LR-test.
To evaluate the quality of these measurements, we run
Anderson LR-test (Hessen 2010) on the set of data. The test
gives us a goodness of fit of the data in Rasch model, i.e., it
tells us whether the data follows the assumptions of Rasch
model. A p value, returned by the test, indicates the
goodness of fit and a p value2 higher than 0.05 indicates no
presence of lack of fit.
2.3 Implementing Rasch model in the text mining
domain
In this paper, we use Guttman scaling and Rasch model to
find a ranking of 23 religious organizations based on
extremity of their views are on radicalism and counterradicalism. In our application, Rasch-model subjects correspond to a group of religious organizations, and items
correspond to a set of keywords for socio-cultural, political, religious radical and counter-radical beliefs, and
practices. An organization responding ‘‘yes’’ to a feature
means the organization exhibits that feature in its narrative,
2

http://www.en.wikipedia.org/wiki/P-value.

123

while an organization responding ‘‘no’’ to a feature indicates that the organization does not exhibit such a feature.
Difficulty of an item translates to strength of the corresponding attitude in defining radical or counter-radical
ideology of any organization. Similarly ability of a subject
in this case means the degree of radicalism or counterradicalism exhibited by an organization’s rhetoric. Other
works in text-mining domain, such as sentiment analysis,
have used Rasch model in their analysis (Drehmer et al.
2000). Details of keyword extraction and selection are
presented in Sect. 3.3.

3 Methods
3.1 Problem definition
The primary goal of this study is to build a semi-automated
method to rank religious organizations from a certain
geographical region on a scale of radicalism versus counterradicalism using their web sites. The efficacy of the generated model is evaluated by comparing it against baseline
methods and expert-level performance. In addition to
accomplishing these goals, we also present an end-to-end
system architecture, and a graphical user-interface design to
facilitate faceted search and browsing of this corpus.
3.2 System architecture
A summary of the system architecture can be seen in
Fig. 1. The system is a composition of four components: a
data-gathering component, which does web crawling, and
text extraction; a scale generation component, performing
scaling algorithms; application services component, which
consists of several web services, and finally, a web userinterface component, presenting the data to the end user.
3.2.1 Data gathering
Initially, social scientists are invited to use their domain
and area expertise to identify a set of organizations, and
hypothesize any number of unipolar or bipolar scales that
could explain the variance among their beliefs and practices. Next, a set of web crawling scripts are created for
extraction of articles from those organizations’ web sites.
For each organization’s corpus, we extract their top-k
n-grams, and a union of all these phrases are presented to
experts for feature selection. Downloaded articles are then
converted into XML structures, containing their original
text, their set of keywords, and extracted information such
as person, location and organization names using a NER
tool for Indonesian language, and their machine translations into English.

A system for ranking organizations using social scale analysis

economic, and religious} keywords corresponding to
beliefs, goals and practices. During this process, our team
of experts screened a total of 790 candidate keywords and
they selected 29 keywords for inclusion in the radical scale,
and 26 keywords for inclusion in the counter-radical scale.
3.4 Debates and perspective analysis

Fig. 1 An overview of the system architecture

An example document snippet is shown in Fig. 2. Here
the original input (content, source), and a sample of the
automatically extracted information corresponding to
DATE, PERSON, and LOCATION can be seen. The corresponding XML versions for each input document are then
stored in a document database for processing.

Upon inspecting the keywords selected by our team of
experts, we observed that some of these keywords correspond to differing perspectives on a set of topics that are
debated within these web sites. Definition of debate is ‘‘a
formal discussion on a particular topic in a public meeting or
legislative assembly, in which opposing arguments are put
forward’’.3 During a debate on a particular topic, like education, both radical and counter-radical organizations discuss different perspectives such as ‘‘secular multi-cultural
education’’ versus ‘‘Sharia based religious education’’.
To design an automated perspective detection algorithm,
we made the following simplifying assumptions.
1.

3.3 Keyword extraction and selection
2.
To identify candidate keywords, one option was to translate
the documents into English and apply readily available
keyword-extraction methods (Michael 2010). However, it
was preferable to preserve the original expression of the
phrases in the original language. Hence, we utilized a nonlinguistic technique that relies only on statistical occurrence, and frequency information.
Within each document, the words were separated by
whitespace or punctuation marks. We considered each
keyword to be an n-gram of one to three words. We treated
each organization as one document and calculated the term
frequency-inverse document frequency (TF-IDF) (Salton
1988) values for every single n-gram mentioned by these
organizations. Top 100 n-grams with the highest TF-IDF
values from each organization were used to generate a
candidate list of topics that these organizations discuss
most frequently. Next, we asked our team of experts to
screen and manually select identify {social, political,

Fig. 2 A portion of a document represented in the system

Organizations will mostly discuss their own perspective in a debate.
Organizations will occasionally mention others perspectives, however, then relate them back to their own
perspective.

In the following sections, we present a mathematical
formulation of the perspective keyword-generation problem
for a given topic, provide an NP-completeness proof, and
design an exact solution through an integer linear programming (ILP)-based solver. Our future work involves finding
an efficient approximation algorithm for this problem.
3.4.1 Perspective keywords-generation problem
Perspective keywords-generation problem (PKGP) is
defined as follows. Given a topic (a keyword) T, and two sets
of documents TR and TCR where TR contains n documents
TR ¼ fDR;1 ; DR;2 ; . . .; DR;n g and TCR contains m documents
TCR ¼ fDCR;1 ; DCR;2 ; . . .; DCR;m g: From each document
DR;i 2 TR ðDCR;j 2 TCR Þ, we collect a set of words
WR,i, V1 B i B n (WCR,j, V1 B j B m) which appear two
words before and two words after each occurrence of the
topic T in that document. Let us define W as the union of all
the WR,i, V1 B i B n and WCR,j, V1 B j B m. If the cardinality of W is p, then W can be given as W ¼ fw1 ; w2 ; . . .;
wp g ¼ fWR;1 [ WR;2 [    [ WR;n [ WCR;1 [ WCR;2 [    [
WCR;m g
Let the frequency of word wk in document DR,i is given
as fR,i(wk) and the frequency of word wk in document DCR,j
as fCR,j(wk).
3

Oxford online dictionary.

123

S. Tikves et al.

Question: Are there two non-empty disjoint subsets of
W, named W 0 and W 00 and W 0 \ W 00 ¼ ;; such that for
every DR,i V1 B i B n,
X
X
fR;i ðwk Þ 
fR;i ðwl Þ
ð1Þ
wk 2W 0

wl 2W 00

and for every DCR,j V1 B j B m,
X
X
fCR;j ðwk Þ 
fCR;j ðwl Þ
wk 2W 0

ð2Þ

wl 2W 00

and jW 0 j þ jW 00 j  K?
In optimization version of the problem, we will try to
minimize jW 0 j þ jW 00 j:
3.4.2 Computational complexity of PKGP
Definition 1 [Weak Partition problem (WPP)] Instance
A finite set A ¼ fa1 ; . . .; an g and a size sðai Þ 2
Z þ ; 8i; 1  i  n: Question Does the set A contain two nonempty sub-sets A1 and A2 that (1) A1 \ A2 ¼ ;; (2) A1 [
P
P
A2  A and (3) ai 2A1 sðai Þ ¼ aj 2A2 sðaj Þ?
WPP has been shown to be NP-complete in (van Emde
Boa 1981).
Theorem 1

PKGP is NP-complete.

Proof It is easy to see that PKGP is in NP since a nondeterministic algorithm needs only to guess a partition of
the word set W into W 0 and W 00 and check in polynomial
time if all the constraints hold for this partition and also if
jW 0 j þ jW 00 j  K:
WPP is a restricted version of PKGP. First we create a
restricted instance of PKGP as follows: let TR and TCR
contains one documents each, i.e, TR = {DR,1} and
TCR = {DCR,1}. Frequency of a word wi 2 W; 81  i  n in
document DR,1 and DCR,1 is taken to be equal, i.e.,
fR,1(wi) = fCR,1(wi) = s(ai). The parameter K is taken to be
equal to |W|. This instance of PKGP is similar to an
instance of WP in the following way: the set A contains
element ai for every word wi 2 W: So, |W| = |A|. In addition, s(ai) = fR,1(wi) = fCR,1(wi), V1 B i B n.
If we find a weak partition of A, as sets A1 and A2 such that
P
P
ai 2A1 sðai Þ ¼
aj 2A2 sðaj Þ, then we can find subsets of
W, as sets W1 and W2, such that wi 2 W1 if ai 2 A1 and
P
wj 2 W2 if aj 2 A2 , respectively. In addition, ai 2A1 sðai Þ ¼
P
P
0
aj 2A2 sðaj Þ; implies that both the constraints
P
P
P wi 2W
fR;1 ðwi Þ  wj 2W 00 fR;1 ðwj Þ and wi 2W 0 fCR;1 ðwi Þ  wj 2W 00
fCR;1 ðwj Þ are true, because s(ai) = fR,1(wi) = fCR,1(wi),
V1 B i B n. Since K = |W|, the constraint jW 0 j þ jW 00 j  K
will trivially hold. So, WPP is a restricted version of PKGP.

123

Since WPP is known to be NP-complete, PKGP is also NPcomplete.
3.4.3 Integer linear programming formulation for PKGP
We formulate an ILP to solve the PKGP optimally. For
each word wi 2 W, we use two variables xi and yi. xi is 1 if
and only if the word wi is in W1 and yi is 1 if and only if the
word wi is in W2. Then constraint (3) means sets W1 and W2
disjoint. Constraint (4) ensures that these sets (W1 and W2)
are also non-empty. Constraints (5) and (6) ensure the
constraints 1 and 2 in problem statement. The objective
minimizes the summation of cardinality of W1 and W2.
Variables: For each word wi,

1; if word wi is assigned to set W1
xi ¼
0; otherwise.

1; if word wi is assigned to set W2
yi ¼
0; otherwise.
Pp
min
i¼1 xi þ yi
s:t: xi þ yi  1;
n
X

xi  1 and

i¼1

n
X

8i ¼ 1; . . .; p
yi  1;

8i ¼ 1; . . .; p

ð3Þ
ð4Þ

i¼1

p
X

fR;i ðwk Þðxk  yk Þ  0;

8i ¼ 1; . . .; n

ð5Þ

fR;i ðwk Þðxk  yk Þ  0;

8i ¼ 1; . . .; m

ð6Þ

wk 2W
p
X
wk 2W

xi 2 f0; 1g; yi 2 f0; 1g;

8i ¼ 1; . . .; p

ð7Þ

3.4.4 Social scale generation
Social scale generation is done by building response tables;
a pair of tables for a bipolar scale, such as radical/counterradical (R/CR), or a single table for a unipolar scale, by
thresholding the occurrence frequencies of the selected
keywords in the organizations’ web corpus.
The scale-generation architecture is shown in Fig. 3.
Here, the flow of the processes and data can be seen as
interactions between experts and automated modules. The
system works as follows.
–

–

Initially, area experts to identify a set of organizations,
and hypothesize any number of unipolar or bipolar
scales that could explain the variance among the beliefs
and practices of the organizations.
Next, we crawl and download the web sites of the
organizations, and the system automatically extracts
the top-k candidate keywords for consideration in the

A system for ranking organizations using social scale analysis

–

Two types of other information are collected for
evaluation purposes. First, expert rankings of the
organizations, using a graphical drag-and-drop expertopinion elicitation tool shown in Fig. 11. Expert
rankings are merged into a consensus gold standard
of rankings. Next, two other computational baseline
methods; one based on simple sorting, and another
based on principal component analysis (Jolliffe 2002),
are used to generate alternative computational rankings
shown in Fig. 12.

In addition, the data for the violence/non-violence are
gathered using a separately developed tool, by collecting
the opinion of the experts. A future work will also include
automated generation of this dimension, as well.
3.5 Feature extraction

Fig. 3 A model of the system architecture

–

–

hypothesized scale. Social scientists screen the list of
extracted keywords, and select the relevant ones for
inclusion in further analysis.
The system builds response tables; a pair of tables for a
bipolar scale (such as radical/counter-radical R/CR), or
a single table for a unipolar scale, by thresholding the
occurrence frequencies of the selected keywords in
the organizations’ web corpus. See Figs. 4 and 5 for the
response tables for the R/CR scale.
The response tables are fed as input to the Rasch Model
building algorithm. The algorithm produces a metric to
validate the fitness of the model, and rankings of the
organizations and keywords. Figures 6 and 7 show the
relative positions of the organizations and keywords on
the latent scales. The algorithm also produces a metric
to validate the fitness of the model.

After identifying the keywords for the analysis, we
needed to search the web site corpus of the organizations
for the matching items. This yielded a term-document
matrix.
This task was performed in a simple three-step procedure; initially, the occurrence frequencies of particular
keywords were counted within each organization’s corpus, then, a threshold matrix was calculated from the
initial values, and finally, a binary response matrix was
generated by applying these thresholds to the initial
values.
The frequency metric is shown in formula 8, where k is
the keyword, o is the organization, and Do is the document
set pertaining to that particular organization.
fo;k ¼

jfdjk 2 d; d 2 Do gj
jDo j

ð8Þ

A threshold value for each keyword is calculated by taking
the median of the values in the related column. Median was
preferred over mean as a threshold, since the distribution of
the values did not fit Gaussian distribution, yet median
empirically proved to be a better measure.

Fig. 4 Radical subset of
organizations and keywords,
sorted according to aggregate
row values

123

S. Tikves et al.
Fig. 5 Counter-radical subset
of organizations and keywords,
sorted according to aggregate
row values

Fig. 6 Radical subset of organizations and keywords

Finally, each element was converted into a binary value by
comparing it to the column’s threshold. English translations of
the keywords are presented for clarity in Figs. 4 and 5.
3.6 Model fitting
We fit the Rasch model on two datasets: (1) radical organizations with radical keywords and (2) counter-radical
organizations with counter-radical keywords. We used the
eRm package in R, an open source statistical software
package,4 to fit a Rasch model to the dataset, and obtain the
organizations’ scores on the latent scale, which are the the
subject parameter estimates (hv) discussed in the previous
section. The eRm package5 fits Rasch models and provide
subjects or organizations parameter estimates based on
maximum likelihood estimation.
4
5

http://www.cran.r-project.org/.
http://www.r-forge.r-project.org/projects/erm/.

123

The automated scale of the organizations is formed by
ranking the organizations according to their estimates on
the latent scale. Not only we can provide the organization
estimates but we can also assess whether the model fits the
data by looking at several goodness of fit indices, such as
the Andersen’s LR-test.
3.7 Application services
We use two backend services in the application layer to
present the data to the user interface. First, all the extracted
textual information are stored in Apache Solr,6 providing
facilities like full-text search and faceting (Tunkelang
2009), using an AJAX interface. In addition, a WCF-based
scaling service is used to infer scales in real time. This
particular service loads the response table, and the previously generated scale data, and estimates the R/CR scale
6

http://www.lucene.apache.org/solr/.

A system for ranking organizations using social scale analysis

Fig. 7 Counter-radical subset of organizations and keywords

–

It would be preferable to plot the locations on the same
range as the input collection. However, the Rasch scale
is on a latent range (Figs. 8, 9).

We resolve the first issue by uniformly scaling the
ranges into [-10, 10], making it consistent with the inputs.
The second issue requires a more specific solution. We
make use of the fact that the raw person scores pertaining
to number of positive responses is a sufficient statistics for
the Rasch model (G 1961) to estimate scale values on the
fly. Since we know the date range, and the selected organizations currently visible in the user interface, it is possible to quickly generate a response matrix for this subset
of the data, and merge it with the previously known scale
information to generate interpolated scale values.

0

2

Radical Scale

−2

The user interface is responsible for representing our
input data, and the findings to the experts in an interactive fashion. Users should be in control of the selection
of the data displayed, and filtering with organization
names, or a specific date range, or using other parameters
such as arbitrary keywords, or geographic locations.
While performing these tasks, it should provide results to
the user with a minimum of delay, allowing quick drilling down to interactively model the scenarios that users
have in mind.
The user interface is implemented as an interactive
AJAX-based application, using ajaxsolr7 framework. In
addition to the search and navigation capabilities provided
with ajaxsolr, it also adds functional widgets for visualizing
the organizations on a scale, mapping the intensity of the
locations, displaying demographics trends, and so on. A
more detailed discussion of the user interface is provided in
Sect. 5.
The presentation of the scale, however, brings the following challenges.

Since this will be an interactive application, users
would prefer to see almost instantaneous results. Yet,
the eRm model generation is computationally
expensive.

−4

3.8 User interface

–

Person Parameters (Theta)

for a subset of the input. Number of positive responses are
interpolated on the scale to generate the scale, and the
expert opinion is used for a static violent/non-violent
(V/NV) scale. While the interpolation is based on a sufficient statistics, future work on speeding up Rasch model
generation for real-time use would be beneficial.

0

5

10

15

20

25

30

Person Raw Scores
7

http://www.evolvingweb.github.com/ajax-solr/.

Fig. 8 Radical scale

123

S. Tikves et al.

Here we have opted to include all the organizations in
threshold calculations. This is because, the radical or
counter-radical activity intensities are always measured
relative to the other organizations participating in the same
time period. However, while the scale is based on all the
organizations, only the ones specifically asked will be
presented to the user.

4 Experimental evaluation
4.1 Indonesian corpus
The corpus domain is the online articles published by the
web sites of the 23 religious organizations identified in
Indonesia, in the Indonesian language. These sources are
the web sites or blogs of the identified think tanks and
organizations. As discussed in the Sect. 1, each source
was classified as either radical or counter-radical by the
area experts. We downloaded a total of 37,000 Indonesian articles published in these 23 web sites, dating from
2001 to 2011. For each web site, a specific REGEX filter
was used to strip off the headers, footers, advertising
sections and to extract the plain text from the HTML
code.
The psuedo-code for the subset scale-generation procedure is presented in Algorithm 1. The process starts with
identifying the subset of documents in the (start, end) date
range (lines 2–5). Then the keyword frequencies, and
thresholds are calculated for the entire set of organizations
on this document subset (lines 6–14). Finally, response
tables for the subset of organizations is generated (lines
15–17), and then the sums need to be interpolated (lines
18–23), to be able to generate a scale on the [-10,10] range
(line 24).

2
0
−2
−4

Person Parameters (Theta)

Counter−Radical Scale

0

5

10

15

Person Raw Scores

Fig. 9 Counter-radical scale

123

20

25

4.2 The quadrants model
Our project leverages the results of our previous work,
which relied on social theory including Durkheim’s (2004)
research on collective representations, Simmel’s (2008)
work on conflict and social differentiation, Wallace’s
(1956) writings on revitalization movements, and Tilly and
Bayat’s studies on contemporary social movement theory
(Tilly 2004; Bayat 2007). Our team has also developed,
and is currently testing a theoretically based class model
comprised of continuous latent scales. The first pair of
scales focus on distinctions between the goals and methods
of counter-radical and radical discourse, and capture the
degree to which individuals, groups, and behaviors aim to
influence the social order (change orientation) and the
methods by which they attempt to do so (change
strategies).
Quadrants model (see Fig. 10) captures multiple social
trends in four quadrants (A, B, C, and D), and it makes the
significant distinction between violent and not-violent
dimensions of both radicalisms and counter radicalisms.
Using the quadrants model, a researcher can locate organizations, individuals, and discourses in broader categories
while still considering subtle differences between groups
within categories. A researcher can document movement
and trends from category to category, and identify points
where movement is likely to happen.

A system for ranking organizations using social scale analysis

rankings. The individual scores for each organization were
combined and averaged to obtain the consensus gold
standard rankings along the hypothesized R/CR scale.
A work is in progress for building a publicly accessible
expert opinion collection toolkit. The preliminary version
can be accessed at: http://www.minerva-project.org/
DataCollector.
4.4 Computationally generated scale

Fig. 10 The quadrants model

4.3 Expert opinion and gold standard of rankings
We collaborated with three area experts, who collectively
possess 35 years of scholarly expertise on Indonesia and
Islam. To build a gold standard of orderings of the organizations, we built a graphical drag-and-drop user-interface
tool to collect the opinions of each of the area experts.
A screenshot of the tool is shown in Fig. 11.
Each expert, separately evaluated and ranked the organizations in the dataset according to a two dimensional
scale of radical/counter-radical (R/CR) and violent/nonviolent (V/NV) axis. The consensus among the experts was
high; since per item standard deviations among the experts’
scores along the R/CR axis over a range of [-10,
10], across all organizations were 2.75. In addition, 90 %
of the items have less than 22.6 % difference in their

The ranking discovered by the Rasch model fitting the corpus
has been evaluated against the gold standard rankings of the
organizations provided by the experts. The difference
between two separate rankings have been calculated using
the following misplacement error measure in Eq. (9).
P
jGðoÞRðoÞj
errorðG; RÞ ¼

o2O

jOj

jOj

ð9Þ

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1,|O|].
For two exactly matching rankings, the error(G, R) will be
zero, whereas for two inversely sorted rankings it is
expected to be 0.5 (when the size of O is even). In addition,
a random ranking is expected to have a error of 0.375.
4.5 Expert-to-gold standard error
We calculated the error between each expert’s ranking and
their consensus gold standard of rankings. The first expert’s
error measure is 0.06, and the second and third expert’s
errors are 0.12 and 0.14 correspondingly as shown in the
last row of the table in Fig. 12. The average error of our
experts against their gold standard ranking is 0.11.
4.6 Baseline: sorting with aggregate score
The first baseline we used was constructed by sorting the
organizations according to the number of different keywords
observed in their corpus. While this provided a pattern
similar to a Guttman scale, and orderings of the organizations
matched to a certain degree with the gold standard as shown
in Fig. 12, the error for this baseline was 0.19, which is
higher than the average expert’s performance.
4.7 Baseline: principal component analysis

Fig. 11 The visual interface of the expert-opinion collector for
manually placing the organizations on the two dimensional scale

A stronger baseline was built by employing principal
component analysis (Jolliffe 2002), and sorting the organizations according to their projections in the first principal
component of the term–document matrix. Since experts
selected the R/CR scale relevant keywords only, it was
expected that the first principal component would reflect
the corresponding scale. PCA proved to be performing

123

S. Tikves et al.
Fig. 12 Computational and
expert rankings

better than the aggregate score sorting, with an error
measure of 0.18. However, this error rate is still higher than
the error rate of each expert.

information and patterns to enable a computational method
to rank them accurately.

4.8 Performance of the Rasch model ranking system

5 Web application overview

The p values from the Anderson LR goodness of fit test from
model (1) and model (2) (mentioned in Sect. 3.6) are 0.85 and
0.669, respectively, suggesting no evidence of lack of fit. The
Rasch models allow us to get a natural order of the organizations, according to their ‘‘abilities’’, i.e., radicalism and
counter-radicalism in this case. This system had an error
measure of 0.10, which actually provided a higher ranking
performance than the average performance of our experts’—
performing better than the majority of our area experts.

A sample snapshot of the web application can be seen in
Fig. 13. It is composed of four main widgets for visualization and navigation. The top-left section which contains
the Search and Navigation widget (1) that allows filtering
of the document subset using parametric search queries and
keyword based search criteria. The top-right section is the
Quadrant widget (2) which displays the organizations
active in the currently selected time frame on a twodimensional axis, using violence and radicalism scales. The
bottom-left section consists of two Treemap widgets (3)
which displays the demographics and the top keywords
(markers) of the current selection. The bottom-right section
has a Timeline widget (4) which provides a visualization
of the keywords (markers) trends on a time line.
The navigation in the user interface starts with the
Navigation widget (top-left) of the web application. Here
the user is able to filter down the corpus utilizing full-text
search queries, or faceting using keywords, locations,
demographics, or choosing a subset of organizations.
The Quadrant widget (top-right) provides a plot of
the currently selected organizations on the two dimensional scale. The radical/counter-radical (R/CR) axis is

4.9 Evaluations
Our experiments showed that the hypothesized compatibility
of the R/CR scale for the Indonesian corpus is valid. Not only
the Rasch model was statistically fitting the response matrix
but also the generated ranking performance was better than
the average expert performance. Among our computational
baseline methods, the Rasch Model was the only method
producing expert-level performance as shown in Fig. 12.
This preliminary analysis with the R/CR scale shows that
when experts assist the system with keyword selection, the
web corpus of organizations provides rich-enough

123

A system for ranking organizations using social scale analysis

Fig. 13 A sample snapshot of the web application (color figure online)

dynamically calculated in real time, using the subset of
organizations, and the time range of the current selection.
The location change on the time range for each organization is shown as a color-coded path, with three markers, a
light circle corresponding to the position at the beginning
of the period, a dark circle corresponding to the end of the
period, and a dark-small circle for the middle. A red line
between the circle denotes the rise of radical activities in
the organization’s behavior. A blue line denotes the
opposite. The smaller circle is useful to see the overall
movement of an organization. For example, between the
range Aug 2005 and Aug 2007, EraMuslim’s activities
were radical (center of A quadrant), then became almost
counter radical (the smaller circle denotes this mid point in
the movement), and then jumped up again. The V/NV axis
is retrieved from expert opinion in the current version, and
dynamic calculation of this axis is left for a future version.
The Timeline widget (bottom-right) displays the trends of
the most frequent markers on a time line. Initially the subset
of markers presented defaults to all available, however it is
possible to restrict the selection of markers to a more limited
set among radical/counter-radical, economical, political,
religious, or social domains. Timeline widget can also be
used for selecting a date range of interest.
The Treemap widgets (bottom-left) are used to display
the relative frequencies of demographics and keywords

(markers). The displayed marker category selection for this
widget is synchronized with the Timeline widget.
In the following sections, we present some scenarios and
findings to illustrate the capabilities of the web interface.
5.1 Scenario 1: radical organizations’ trends
In this scenario, we analyze both violent and non-violent
radical organizations. Our web application shows the ideologies that these organizations are propagating. We can
see8 the most prominent markers associated with these
radical organizations. Markers such as ‘‘infidel’’, ‘‘Sharia’’,
and ‘‘violence’’ show an increasing trend between 2001 and
2011. A very strict interpretation of ‘‘Sharia’’ is used by
radical organizations to justify their actions (Widhiarto
2010; Hasan 2009). ‘‘Sharia’’ peaks during this period as
shown in Fig. 14.
5.2 Scenario 2: C-quadrant organizations’ trends
We now analyze Front Pembela Islam (FPI), an Islamic
organization in Indonesia established in 1998. FPI is well
known for its violent acts (Frost et al. 2010; Rondonuwu
8

Select the filter ‘‘Radical’’ from the search options and then in the
Markers Menu select [Religious ! Radical Markers].

123

S. Tikves et al.

Fig. 14 Trend of radical markers

2010) justified by a strict interpretation of Sharia (for the
Study of Terrorism 2011). Our documents for FPI ranges
between 2000 and 2010. Using our web application’s plots of
the movement of FPI in the C Quadrant, we found that FPI
consistently rised higher on the radical scale as shown in
Fig. 15. We selected the following time ranges, 2000–2003,
2002–2006, 2006–2010 and analyzed the trends of various
markers associated with FPI. There was a substantial increase
in the intensity of various radical markers such as ‘‘infidel’’,
‘‘Mujahedin’’, ‘‘pornography’’.9 Since 2006, we also saw a
steep increase in the frequency of marker ‘‘Ahmadiyya’’, as
shown in Fig. 16, which indicates FPI’s increased opposition
to this heretical sect (Rahmat and Sihaloho 2011).

Fig. 15 Consistent rise of FPI on the radical scale

5.3 Scenario 3: A-quadrant organizations’ trends
We analyze Hizb ut-Tahrir also known as Hizb ut-Tahrir
Indonesia (HTI), a radical organization widely believed to
be non-violent (Ward 2009), which has been active in
Indonesia since 1982 (Osman 2011). Between 2007 and
2009, our web application shows various radical and nonradical markers associated with this organization.

Radical

Non-Radical

‘‘Sharia’’, ‘‘Infidel’’,
‘‘Caliph’’, ‘‘Violence’’

‘‘Politics’’, ‘‘Indonesian Islam’’,
‘‘Election’’, ‘‘Liberal’’,
‘‘Democracy’’

During the same period, we see a steady increase in the
frequency of the radical marker ‘‘Sharia’’. This is consistent with one of HTI’s goals of implementing Sharia in
Indonesia (Hasan 2009). Hizb ut-Tahrir openly propagates
9

Select ‘‘Radical’’ and ‘‘FPI’’ from the filters, then select the time
range 2002–2006 or 2006–2010, then select ‘‘radical’’ markers under
‘‘R/CR’’ menu.

123

Fig. 16 ‘‘Ahmadiyya’’ peaking during the period 2006–2010

Fig. 17 ‘‘Khilafah’’ ideology of Hizb ut-Tahrir

the ideology of Khilafah, which believes in unification of
all Muslim countries as a single Islamic State (Zakaria
2011; Mohamed Osman 2010). Figure 17 shows

A system for ranking organizations using social scale analysis

Searching for the text ‘‘suicide bombing’’, we see that
one of the related markers is ‘‘ideology’’. Adding the
keyword ‘‘ideology’’ to the search filter reveals a new set of
markers including the ‘‘sin’’ keyword. Adding ‘‘sin’’ to our
search, we obtain a set of matching documents. One of the
top matches, is titled ‘‘Mengapa Saya Berubah?’’ (english
translation: ‘‘Why I changed?’’)13. This article is by a
reformed terrorist, debunking the misinterpretation of the
jihad-related verses used by violent groups.

6 Conclusions and future work

Fig. 18 Decline of the HTI in the radical scale

‘‘Khilafah’’ as the most prominent marker10 in Hizb utTahrir’s discourse.
By looking at the Quadrants widget (in Fig. 18), we can
infer that HTI has been moderating its narrative.
5.4 Scenario 4: B-quadrant organizations’ trends
In this scenario, we discuss the trends of counter radical
organizations like NU and DaarulUluum. We also show an
interesting scenario on the topic of ‘‘Suicide Bombing’’
using the keyword based Navigation widget.
The ‘‘counter radical’’ markers11 associated with these
organizations are: ‘‘politics’’, ‘‘election’’, ‘‘Indonesian
Islam’’, ‘‘liberal’’, ‘‘human rights’’. These organizations
support democracy and elections, which is shown by the
high frequency of the markers ‘‘politics’’ and ‘‘election’’.
Their narrative has local interpretation of Islam at its core,
which is shown by the marker ‘‘Indonesian Islam’’.
On analyzing the occurrences of radical markers12 in
B-Quadrant, we find that counter radical organizations are
very vocal against all of radical markers. One of the
interesting radical markers is ‘‘Suicide Bombing’’. Most of
the counter radical organizations are against suicide
bombings.(Malang 2006). We will now demonstrate how
combination of parametric and keyword search, and various widgets in the web application can help reveal opposition to ‘‘Suicide Bombing’’ by counter-radical
organizations.
10

Select ‘‘Hizb ut-Tahrir’’ and ‘‘radical’’ from filters. Select the time
range 2007–2009. The markers can be seen by selecting the options of
Markers Menu [Religious ! Religious Markers].
11
Select CounterRadical filter in the search option, then from the
Markers Menu select [R/CR ! Counter Radical].
12
In the Markers Menu select [R/CR ! Radical].

In our experiments, not only did the data show fitness with
the Rasch Model for the R/CR scale but also the Rasch
rankings of the organizations are better than the output of
the other baseline computational methods, and they are at
expert-level performance when compared with the consensus gold standard rankings.
Rasch model also provided us with another output,
namely the ranking of selected keywords (items) on the
R/CR scale. Although preliminary observations indicates
that this can be a valuable asset by itself, we plan to further
investigate the quality and utility of this ranking as future
work.
While the model has been demonstrated to fit on the
R/CR scale, two major expansion points can be investigated in the future work, namely the violent/non-violent
scale, and enhancement of feature selection. Although our
experts have identified a second dimension, evaluating its
correlation to R/CR axis, or existence of other significant
ones could be beneficial. In addition, the features can be
enhanced by experimenting with the significance of the
radical keywords in the counter-radical organization corpora, and vice-versa.
A practical method to increase the automation of keyword generation has been discussed in Sect. 3.4. Future
work will involve finding an efficient approximation
algorithm for this model, for decreasing the necessity of
expert interaction for this particular step.
Other interesting work includes making our expert
opinion elicitation tool available online to a wider and
more geographically distributed audience to crowdsource
(Snow et al. 2008) the needed expertise for making lists of
local organizations, identifying their web sources, and
overcome the complex task of construction and validation
of significant and fitting scales (work is currently underway
to build this tool). Another interesting dimension is to look
at synthesis and analysis of scales that do have a strict
hierarchy of keywords, but adhere to more flexible partial
order models (James and John 2002).
13

http://www.islamlib.com/id/artikel/mengapa-saya-berubah/.

123

S. Tikves et al.
Acknowledgments This research was supported by US DoDs
Minerva Research Initiative Grant N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the project title
is ‘‘Finding Allies for the War of Words: Mapping the Diffusion and
Influence of Counter-Radical Muslim Discourse’’.

References
Andrich D (1988) Rasch models for measurement. Sage, USA
Bayat A (2007) Making Islam Democratic: social movements and the
post-Islamist turn. Stanford University Press, USA
Crelinsten R (2002) Analysing terrorism and counter-terrorism: a
communication model. Terror Political Violence 14:77–122
Davulcu H, Ahmed ST, Gokalp S, Temkit MH, Taylor T, Woodward
M, Amin A (2010) Analyzing sentiment markers describing
radical and counter-radical elements in online news. In:
Proceedings of the 2010 IEEE second international conference
on social computing, IEEE Computer Society, SOCIALCOM’10, pp 335–340
Drehmer D, Belohlav J, Coye R (2000) An exploration of employee
participation using a scaling approach. Group Org Manage
25(4):397
Durkheim E (2004) The cultural logic of collective representations:
social theory the multicultural and classic readings. Wesleyan
University: Westview Press
Frost F, Rann A, Chin A (2010) Terrorism in southeast asia.
http://www.aph.gov.au/library/intguide/FAD/sea.html [Online
accessed 21 Nov 2011]
Guttman L (1950) The basis for scalogram analysis. Meas Predict
4:60–90
Hasan N (2009) Islamic militancy, Sharia, and democratic consolidation in post-Suharto Indonesia. RSIS Working Papers. 143/07
Hessen D (2010) Likelihood ratio tests for special Rasch models.
J Edu Behav Stat 35(6):611
Hunter D, Lange K (2004) A tutorial on mm algorithms. Am Stat
58(1):30–37
James AW, John LM (2002) Algebraic representations of beliefs and
attitudes: partial order models for item responses. Sociol
Methodol 29:113–146
Jolliffe I (2002) Principal component analysis: Springer series in
statistics. Springer, Germany
Le Cam L (1990) Maximum likelihood an introduction. ISI Rev
58(2):153–171
Likert R (1932) A technique for the measurement of attitudes. Arch
Psychol 140:1–55
Malang (2006) NU chairman deplores suicide bombing attempt.
http://www.nu.or.id/page/en/dinamic_detil/15/28282/News/NU_
chairman_deplores_suicide_bombing_attempt.html [Online
accessed 22 Nov 2011]
McIver J, Carmines E (1981) Unidimensional scaling, vol 24. Sage
Publications Inc, USA
McPhee RD, Corman S (1995) An activity-based theory of communication networks in organizations, applied to the case of a local
church. Commun Monogr 62:1–20
Michael WB, Kogan J (2010) Text mining: applications and theory.
Wiley, London
Mohamed Osman MN (2010) Reviving the Caliphate in the
Nusantara: Hizbut Tahrir Indonesia’s mobilization strategy and
its impact in Indonesia. Terror Political Violence 22(4):601–622.

123

doi:10.1080/09546553.2010.496317.
http://www.tandfonline.
com/doi/abs/10.1080/09546553.2010.496317
Osman MNM (2011) Preparing for the caliphate. Asian Stud Assoc
Aust E-Bull 80:14–16. ISSN:1449-4418
Pawitan Y (2001) In all likelihood: statistical modelling and inference
using likelihood. Oxford University Press, USA
Rahmat Sihaloho M (2011) FPI vows to disband ahmadiyah
’whatever it takes’. http://www.thejakartaglobe.com/home/fpivows-to-disband-ahmadiyah-whatever-it-takes/423477 [Online
accessed 21 Nov 2011]
Rasch G (1961) On general laws and the meaning of measurement in
psychology. In: Proceedings of the fourth Berkeley symposium
on mathematical statistics and psychology, 4, p 332
Rondonuwu O, Creagh S (2010) Opposition grows to indonesia’s
hardline fpi islamists. http://www.in.reuters.com/article/
2010/06/30/idINIndia-49777620100630 [Online accessed 21
Nov 2011]
Salton G, Buckley C (1988) Term-weighting approaches in automatic
text retrieval. In: Information Processing and Management, vol
25, pp 513–523
Simmel G (2008) Sociological theory. McGraw-Hill, New York
Snow R, O’Connor B, Jurafsky D, Ng AY (2008) Cheap and fast—
but is it good?: evaluating non-expert annotations for natural
language tasks. In: Proceedings of the conference on empirical
methods in natural language processing, EMNLP ’08,
pp 254–263. Association for Computational Linguistics, Stroudsburg, PA, USA. http://www.portal.acm.org/citation.cfm?
id=1613715.1613751
for the Study of Terrorism NC, to Terrorism R (2011) Terrorist
organization profile: front for defenders of Islam. http://www.
start.umd.edu/start/data_collections/tops/terrorist_organization_
profile.asp?id=4026 [Online accessed 21 Nov 2011]
Thurstone LL (1928) Attitudes can be measured. Am J Sociol
33:529–554
Tikves S, Banerjee S, Temkit H, Gokalp S, Davulcu H, Sen A,
Corman S, Woodward M, Rochmaniyah I, Amin A (2011) A
system for ranking organizations using social scale analysis. In:
EISIC. IEEE, pp 308–313. http://www.ieeexplore.ieee.org/xpl/
mostRecentIssue.jsp?punumber=6059524
Tilly C (2004) Social Movements. Paradigm Publishers, USA
Tunkelang D (2009) Faceted Search: synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool
Publishers, UK. doi:10.2200/S00190ED1V01Y200904ICR005
van Emde Boas P (1981) Another NP-complete partition problem and
the complexity of computing short vectors in a lattice: Tech.
Rep. 81-04. Mathematisch Instituut, Amsterdam
Wallace A (1956) Revitalization movements. Am Anthropol
58:264–281
Ward K (2009) Non-violent extremists? Hizbut Tahrir Indonesia.
Aust J Intl Affairs 63(2):149–164 doi:10.1080/1035771090
2895103.http://www.tandfonline.com/doi/abs/10.1080/10357710
902895103
Widhiarto H (2010) Radical groups urge Bekasi administration to
implement Sharia law. http://www.thejakartapost.com/news/2010/
06/27/radical-groups-urge-bekasi-administration-implementsharia-law.html [Online accessed 21 Nov 2011]
Zakaria Y (2011) A Global Caliphate: reality or fantasy? http://www.
usa.mediamonitors.net/content/view/full/91207 [Online accessed
21 Nov 2011]

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Co-Clustering Signed 3-Partite Graphs
Sefa Şahin Koç

İsmail Hakkı Toroslu

Hasan Davulcu

Computer Science Department
Computer Engineering Department
Aselsan Rehis
Middle East Technical University, Ankara, Turkey Arizona State University, Tempe, AZ
Ankara, Turkey
Email: hdavulcu@asu.edu
Email: toroslu@ceng.metu.edu.tr
Email: sskoc@aselsan.com.tr

Abstract—In this paper, we propose a new algorithm, called
ST RI C LUSTER, to find tri-clusters from signed 3-partite graphs.
The dataset contains three different types of nodes. Hyperedges
connecting three nodes from three different partitions represent
either positive or negative relations among those nodes. The
aim of our algorithm is to find clusters with strong positive
relations among its nodes. Moreover, negative relations up to
a certain threshold is also allowed. Also, the clusters can have
no overlapping hyperedges. We show the effectiveness of our
algorithm via several experiments.

I. I NTRODUCTION
A hyperedge in a tri-partite graph represents the relationship
among the three nodes it connects. For example, in a social
tagging system, which contains three types of nodes (users,
tags, resources), a hyperedge means that a user annotates a
resource with a tag [1]. A tripartite cluster of these hyperedges
may give many information such as users' attitudes to multiple
resources or users with common interests. As another example,
in a biological analysis system, a level of gene in a sample at
a particular time can be represented as a tripartite hyperedge.
By mining tripartite clusters, genes showing common characteristics in samples at common time slots could be extracted
[2].
Finding biclusters with maximum size from a bipartite graph
is proven to be NP-hard, as well as discovering tripartite
clusters with maximum size [3]. Therefore, works in literature
[2], [4], [1] apply heuristics to determine clusters. As a common strategy, tri-clusters are generated by first constructing
biclusters between each pair of three partitions [4]. Then, each
bicluster is matched with two others in order to construct triclusters. Since this approach is very costly, as an alternative,
first, two partitions are selected, and, then, biclusters of these
bipartite graphs are constructed. After that, by iterating each
one of these biclusters on the third partition, tripartite clusters
can be constructed [2]. However, since the first two partitions
are fixed, this approach has bias against the third partition.
As another approach, tripartite clusters focus on one-to-one
correspondence among the nodes [5], [6]. However, the realworld data is usually more complex. For example, in a social
tagging system, a group of users may tag multiple sources
with the same set of tags, which corresponds to many-to-many
relationship.
In this paper, we present an effective algorithm which
generates tri-clusters from tripartite hyperedges with positive
signs. Our method has the following properties: 1) A minimum

IEEE/ACM ASONAM 2016, August 18-21
2016, San Francisco, CA, USA
978-1-5090-2846-7/16/$31.00 © 2016 IEEE

threshold for positive signed hyperedge density ratio over
all possible hyperedges among tri-partitions of the cluster is
defined, and, it must be satisfied by clusters. 2) A simple
greedy approach is used in order to trim the hyperedges from
tri-clusters with negative signs to increase the positive density
ratio of the cluster. 3) In order to prevent constructing very
small clusters, both negative signed hyperedges and triples
with no connections are also allowed as long as they satisfy
user defined density threshold constraints. 4) Clusters are not
allowed to have overlaps in terms of hyperedges. A simple
heuristic is used to mark hyperedges in order to prevent
hyperedge overlaps among clusters, and fast termination of
the algorithm while searching potentially maximal clusters. 5)
The effectiveness of our approach is shown using a coveragebased metric.
To the best of our knowledge this is the first work that
attempts to find co-clusters with potentially overlapping nodes
from signed tri-partite graphs. In our work, the clusters
are constructed considering the hyperedges, and thus, it is
possible to generate clusters with common nodes from the
same dimension. A typical problems that motivates this work
is finding co-clusters from sentiments of tweets on issues.
The three dimensions of this problems are people who write
tweets, the selected set of issues (or named entities) and the
chosen sentiment words by the users on these issues. The
sign of the sentiment words also sepresent the sign of the
hyperedge between the three nodes of these dimenstions. It is
very likely that same sentiment words are used by people with
different clusters corresponding to different camps. Similarly
more than one camp may have similar sentiments towards the
same issue as well. Therefore, clusters generated on sentiment
words and on issues which corresponds to positive feelings
of different camps may have many common items. Even on
people dimension, it is likely to generate clusters with several
common people, which may be interpreted as these people
being close to more than one different political camps.
The rest of the paper is organized as follows. Section
II introduces ST RI C LUSTER algorithm. Section III presents
experiments and section IV concludes the paper.
II. T HE ST RI C LUSTER A LGORITHM
In this paper, we use the notations given in Table 1.
ST RI C LUSTER algorithm takes a set of hyperedges, Γ as an
input, such that each hyperedge h connects three nodes from
three different types U = (U1 , U2 , U3 ). Figure 1 illustrates

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)
for 1 ≤ i ≤ 3, and this constraint should also be satisfied by
every cluster.

TABLE I
S YMBOL TABLE
Symbol
Γ
Γv/iv
α
p
n
λi
Li
<
h+/−
Ui
Eir
Uir
Si

Meaning
set of hyperedges
set of valid/invalid hyperedges
a tripartite cluster
minimum ratio of h+ in a cluster
maximum ratio of h− in a cluster
minimum size for type i in a cluster
number of nodes for type i in a cluster (size of type i)
set of tripartite clusters
a hyperedge with positive/negative label
set of nodes for type i
affectiveness value for node r of type i
minimum Eir in Ui , which belongs to node r
maximum number of h in which a node from type i can be

hyperedges given as 3D matrix. These hyperedges have either
positive or negative labels which are also represented by green
and red colors respectively in Figure 1. Remaining entries
(white cells) corresponds to node triples without connecting
hyperedges. In the example, nodes are {{A,B}, {a,b,c,d,e},
{1,2,3,4,5}} from types U1 , U2 , U3 respectively.
A

1

a

3

-

b

-

c

+

d
e

2

+

+

4

5

B

+

-

a

+

-

+

2

3

+

-

4

5
+

b

+

+

+

c

+

+

-

-

+

d

-

+

+

+

e

+

+

-

1

+

Algorithm 1 STriCluster Algorithm
1: procedure ST RI C LUSTER (Γ, p , n , λ1 , λ2 , λ3 )
2:
loop
3:
generate α from Γ
4:
β = C LEAN I NVALIDS(Γ, Γiv , α, λ1 , λ2 , λ3 )
5:
if not β then
6:
return <
7:
end if
8:
D ENSITY C HECKING(α, p , n , λ1 , λ2 , λ3 )
9:
if α * f ormula (3) then
. if α satisfies
10:
<←<⊕α
. ⊕ means appending
11:
end if
12:
for each h in α do
. h is a hyperedge in α
13:
Γiv ← Γiv ⊕ h
14:
end for
15:
end loop
16: end procedure
ST RI C LUSTER algorithm (Algorithm 1) starts by generating
a potential cluster α which contains all hyperedges in Γ. For
the example input data in Figure 1, α initially is equal to the
whole graph. If there are invalid hyperedges (used to prevent
hyperedge overlaps), they will be removed from α (Section
2.B). After invalid hyperedges are removed, if α does not
satisfy the condition (3), (i.e., β is FALSE), the algorithm
terminates.

Fig. 1. Input Data

The aim of ST RI C LUSTER is to find tripartite clusters of
hyperedges with highly positive labels. To be a valid tripartite
cluster, it has to satisfy threshold values for both density and
size. The density threshold values are p and n , such that
0 ≤ p , n ≤ 1, (p + n ) ≤ 1. The former one represents the
minimum ratio density of positive hyperedges (h+ ) among all
possible hyperedges (i.e., there may be L1 × L2 × L3 number
of possible hyperedges for a cluster with size (L1 , L2 , L3 ),
where Li is number of nodes with Ui type in the cluster). If
Cp is the number of h+ , then:
Cp
,
(1)
L1 × L2 × L3
If p = 1, generated tripartite clusters become tripartite cliques
as well. n is the value to control the density of negatively
signed hyperedges (h− ). If Cn represnts the number of h− ,
then:
Cn
n ≥
,
(2)
L1 × L2 × L3
shows maximum allowed tolerance of h− in a cluster if n 6=
0.
In order to prevent constructing very small clusters λi is
defined, such that:
Li ≥ λi ,
(3)
p ≤

A

1

a

3

-

b

-

c

+

d
e

2

+

+

4

5

B

+

-

a

+

-

+

2

3

+

-

4

5
+

b

+

+

+

c

+

+

-

-

+

d

-

+

+

+

e

+

+

-

1

+

Fig. 2. Removing Node (3) From a Potential Cluster

After a potential cluster α is generated, density check
operation is applied on α (Section 2.A). This operation aims
to get α to satisfy conditions (1), (2), and (3). There are three
possible cases that can happen: In the first case (case I), if
conditions (1) or (2) are not satisfied, the least useful node is
removed from α (Figure 2) iteratively, until both constraints
are satisfied. For the example in Figure 1, this step will remove
nodes {{a,d,e}, {3,4,5}} from types U2 , U3 respectively from
α. As the second case (case II), if the removal of a node
from α violates the constraint (3), the process stops. In this
case, one h− in α is labeled as invalid. This prevents the
construction of exactly the same potential tripartite cluster
again, because of C LEAN I NVALIDS operation (Line 4) of the
algorithm. As the third case (case III), α satisfies conditions
(1), (2), and (3). Then, D ENSITY C HECKING operation returns

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

α (as a reference parameter). In the example, returned cluster
α contains nodes {{A,B}, {b,c}, {1,2}} shown in Figure 3. In
this case, α is added to the cluster list < (Line 7). After that,
all hyperedges in α are labeled as invalid and added into Γiv
which is the list of invalid hyperedges (Line 13). The following
sub-section describes the details of density checking procedure
which handles these operations.

(Figure 2). Node 3 has the lowest effectiveness value, E33 ,
5 = 0.1 uscompared to others. E33 is obtained as 2+2−1−1−1
2×
ing formula (5). In following iterations, nodes {a, d, e, 3, 4, 5}
will be removed from α. Then, D ENSITY C HECKING will
reach to case III satisfying all three conditions (1), (2), and (3)
and, then returns α which contains nodes {A, B, b, c, 1, 2}.
If α does not satisfy conditions (1) and (2) and if node
removal results the violation of condition (3), it means that
D ENSITY C HECKING is in case II. In this case, the procedure
marks the first h− as invalid.

A

1

2

B

1

2

B. Clean Invalids

b

-

+

b

+

+

c

+

c

+

+

Invalid hyperedges include the hyperedges of all tripartite
clusters previously generated as well as all edges marked
as invalid by D ENSITY C HECKING. In order to remove a
hyperedge one of the nodes from this hyperedge should be
removed. To do this, C LEAN I NVALIDS procedure picks a node
to remove and it repeats the same action until no invalid
hyperedge is left in α. While selecting a node, it uses a
heuristic that reduces the cluster size as minimum as possible.
In order to do this, we first determine the number of invalid
hyperedges connected to each node. If the ratio of this number
to Si is high, that node is more likely to be removed. This ratio,
called as θir for node r from type i. Then, we calculate the
effectiveness for all the valid nodes of α, which is called as
v
Eir
. These two values values are combined with the following
formula:
θir
(7)
γir =
v .
0.9 + Eir

(a) Matrix Representation

(b) Cluster Representation

Fig. 3. A Tripartite Cluster Mined in Given Input

A. Density Checking
If given cluster α does not satisfy conditions (1) and (2),
the density checking algorithm searches for nodes to exclude
until α satisfies these constraints. If a node is connected by
high number of h+ , it should be less likely to be removed.
We define hyperedge’s usefullness as follows:
(
2
if h has positive label
val(h) =
(4)
−1
if h has negative label.
Then, the effectiveness of a node (r) is determined with the
following formula where Si represents the maximum number
of hyperedges which contains that node in Ui :
(
P
val(h)
if r ∈ h
h∈α
0
otherwise
Eir =
.
(5)
Si
Then, for each partition, all nodes are checked to find a node
with minimum effectiveness value:
X
Uir = min(
Eir ).
(6)
r∈Ui

At the end of this stage, there will be three Uir values
which are U1a , U2b , U3c corresponding to partitions U1 , U2 , U3
respectively. Minimum of them will be the effectiveness value
Eix of node x. This node will be the one to be removed from
type i in α in this iteration.
TABLE II
N UMBER OF POSSIBLE HYPEREDGES FOR EACH NODE TYPE
Type
S1
S2
S3

Value
L2 × L3
L1 × L3
L1 × L2

For the input in Figure 1, when D ENSITY C HECKING operation applied on α, node 3 will be removed in the first iteration

Among all nodes, the one which has highest γ value is the
one to be removed.
As a constraint, if removing node x will result violation
of condition (3), C LEAN I NVALIDS procedure returns FALSE.
If there is no invalid hyperedge left in α, C LEAN I NVALIDS
returns TRUE.
For the input data in Figure 1, ST RI C LUSTER algorithm
finds the cluster in Figure 3 in the first iteration. Then,
hyperedges of this newly generated cluster are labeled as
invalid. In the next iteration, new potential cluster α (Figure 4-a) is generated from Γ. But α contains some invalid
hyperedges (colored with blue in Figure 4-a). Therefore, α
is passed to C LEAN I NVALIDS procedure to be cleaned from
invalid hyperedges. First, node c is removed since γ2c is
3 ÷ 0.9 = 3.33, is the maximum among γ values. Then, nodes
2 and 1 are selected and removed respectively (Figure 4-b,
4-c). This will result a clean α (Figure 4-d) and the procedure
terminates.
III. E XPERIMENTS
In order to evaluate our algorithm, we have generated data
sets with varying sizes. We have done all the experiments on
MacBook Pro Mid 2015 (Intel i7 2,5 GHz, 16GB memory).
In the first set of experiments, we have fixed h+ and h−
density ratios while changing input sizes. Other parameters
are also fixed as p = 0.75, n = 0.10, λi = (2,2,2). In this
test, we have generated 6 sample datasets. Each one contains

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

A

1

2

-

+

a
b
c

-

4

5

+

-

+

+

-

+

+

B

1

a
b

+

d
e

3

2

3

+

-

+

+

4

5

A

+

b

+

c

+

+

-

-

+

d

-

+

+

+

e

+

1

3

-

+

a
b

4

5

B

+

-

a

+

d
e

+
+

-

+

1

3

4

-

b

+

d

-

e

e

+

+

3

-

4

5

+

-

+

+

-

+

B

-

+

+

1

a
b

2

3

+

-

+

+

d

-

+

e

+

4

5
+

+
+

+

(b) Removing Second Node

5

A

+

a

+
+

2

-

d

(a) Removing First Node

A

1

a

+

b
+

3

4

5

B

3

+

-

a

-

+

+

-

+

d
e

(c) Removing Third Node

4

+

5
+

b

+

d

+

+

e

(d) A Clean Tripartite Cluster

Fig. 4. A Scenario of C LEAN I NVALIDS Procedure

positive hyperedges with 60%, negative hyperedges with 20%,
and 20% is empty. (L1 × L2 × L3 ) values for these samples
are (31.25K, 62.5K, 125K, 250K, 500K, 1M) respectively.
Figure 5 presents the results.
Time

Cover

IV. C ONCLUSION
300K

5250 sn

225K

3500 sn

150K

1750 sn

75K

31.25K

62.5K

125K

250K

500K

1M

Number of Hyperedges

Time

7000 sn

0 sn

clusters being non-trivial. The results show that we have
achieved very high coverage in that sense, since constructed
clusters include almost as many hyperedges as the half of the
number of positively signed hyperedges.
We have also tested our approach using real data set, which
corresponds to the tweets of users on selected issues. These
tweets are processed in order to determine the sentiments of
users towards these issues. From these tweets, a three dimensional data set is generated. These dimensions are users, issues,
and sentiment words chosen by the users on these issues,
which also represent the sign of the hyperedge connecting
these three items. We have large datasets with 10K users, 45K
sentiment words and 20 different issues. This 3-dimensional
data is very sparse with only 280K non-empty entries. So far,
we have applied our algorithm to a fraction of this dataset
which corresponds to randomly select few percentages of it.
We have obtained fairly large and overlapping clusters in all
three dimensions. Some largest clusters have as many items as
the 10% of the nodes of its corresponding dimensions, even
for user or sentiment words dimensions.

0K

In this paper, we have proposed a new method, called
ST RI C LUSTER, to mine tripartite clusters of positively labeled
hyperedges. The input data is composed of three dimensions.
Each hyperedge connects three nodes from each dimension.
Clusters are generated depending on density ratio of positively
(minimum) and negatively (maximum) labeled hyperedges.
We have showed the effectiveness of our approach using
both syntetic and real data sets.

Input Size

ACKNOWLEDGMENT
Fig. 5. Test Scenario Depending on Input Size

In the second test, we have generated 5 datasets. In this test,
we have fixed the size as (L1 × L2 × L3 ) = 125K and we
have varying density ratios for (h+ , h− ) pairs as {(0.2,0.4),
(0.2,0.2), (0.4,0.2), (0.4,0.4), (0.6,0.2)}. The results are shown
in Figure 6.
Cover
35K

768 sn

26K

512 sn

18K

256 sn

9K

0 sn

0.2-0.4

0.2-0.2

0.4-0.4

0.4-0.2

0.6-0.2

Number of Hyperedges

Time

Time
1024 sn

0K

Density Ratio Values

Fig. 6. Test Scenario Depending on Density Ratios in Input Data

The figures show both execution times and the number of
hyperedges included in the constructed clusters. We prefer
most (positive) hyperedges to be included in clusters while

This research was supported partially by USAF Grant
FA9550-15-1-0004.
R EFERENCES
[1] C. Lu, X. Chen, and E. Park, “Exploit the tripartite network of social
tagging for web clustering,” in Proceedings of the 18th ACM conference
on Information and knowledge management. ACM, 2009, pp. 1545–
1548.
[2] L. Zhao and M. J. Zaki, “Tricluster: an effective algorithm for mining
coherent clusters in 3d microarray data,” in Proceedings of the 2005 ACM
SIGMOD international conference on Management of data. ACM, 2005,
pp. 694–705.
[3] M. Dawande, P. Keskinocak, J. M. Swaminathan, and S. Tayur, “On
bipartite and multipartite clique problems,” Journal of Algorithms, vol. 41,
no. 2, pp. 388–403, 2001.
[4] L. Zhu, A. Galstyan, J. Cheng, and K. Lerman, “Tripartite graph clustering for dynamic sentiment analysis on social media,” in Proceedings
of the 2014 ACM SIGMOD international conference on Management of
data. ACM, 2014, pp. 1531–1542.
[5] X. Liu and T. Murata, “Detecting communities in tripartite hypergraphs,”
arXiv preprint arXiv:1011.1043, 2010.
[6] Y.-R. Lin, J. Sun, P. Castro, R. Konuru, H. Sundaram, and A. Kelliher,
“Metafac: community discovery via relational hypergraph factorization,”
in Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 527–536.

PREDICTING THE LOCATION AND TIME OF MOBILE PHONE USERS BY
USING SEQUENTIAL PATTERN MINING TECHNIQUES

A THESIS SUBMITTED TO
THE GRADUATE SCHOOL OF NATURAL AND APPLIED SCIENCES
OF
MIDDLE EAST TECHNICAL UNIVERSITY

BY

MERT ÖZER

IN PARTIAL FULFILLMENT OF THE REQUIREMENTS
FOR
THE DEGREE OF MASTER OF SCIENCE
IN
COMPUTER ENGINEERING

JULY 2014

Approval of the thesis:
PREDICTING THE LOCATION AND TIME OF MOBILE PHONE USERS BY
USING SEQUENTIAL PATTERN MINING TECHNIQUES

submitted by MERT ÖZER in partial fulfillment of the requirements for the degree of
Master of Science in Computer Engineering Department, Middle East Technical
University by,

Prof. Dr. Canan Özgen
Dean, Graduate School of Natural and Applied Sciences
Prof. Dr. Adnan Yazıcı
Head of Department, Computer Engineering
Assoc. Prof. Dr. Pınar Karagöz
Supervisor, Computer Engineering Department, METU
Prof. Dr. İ. Hakkı Toroslu
Co-supervisor, Computer Engineering Department, METU

Examining Committee Members:
Prof. Dr. Ahmet Coşar
Computer Engineering Department, METU
Assoc. Prof. Dr. Pınar Karagöz
Computer Engineering Department, METU
Assoc. Prof. Dr. Halit Oğuztüzün
Computer Engineering Department, METU
Assoc. Prof. Dr. Osman Abul
Computer Engineering Department, TOBB ETU
Dr. Cevat Şener
Computer Engineering Department, METU

Date:

I hereby declare that all information in this document has been obtained and
presented in accordance with academic rules and ethical conduct. I also declare
that, as required by these rules and conduct, I have fully cited and referenced all
material and results that are not original to this work.

Name, Last Name:

Signature

iv

:

MERT ÖZER

ABSTRACT

PREDICTING THE LOCATION AND TIME OF MOBILE PHONE USERS BY
USING SEQUENTIAL PATTERN MINING TECHNIQUES

Özer, Mert
M.S., Department of Computer Engineering
Supervisor

: Assoc. Prof. Dr. Pınar Karagöz

Co-Supervisor

: Prof. Dr. İ. Hakkı Toroslu

July 2014, 63 pages

Predicting the location of people from their mobile phone logs has become an active
research area. Due to two main reasons this problem is very challenging: the log
data is very large and there is a variety of granularity levels both for specifying the
spatial and the temporal attributes, especially with low granularity level it becomes
much more complicated to define common user behaviour patterns. For the location
prediction problem domain, we focused on 3 sub-problems and proposed 3 different
methods for these problems. The idea in all of the three methods follows these two
steps; cluster the spatial data into the regions and group temporal data into the time intervals to get higher granularity level, and apply sequential pattern mining techniques
to extract frequent movement patterns to predict accordingly. We have validated our
results with real data obtained from one of the largest mobile phone operators in
Turkey. Our results are very encouraging, and we have obtained very high accuracy
results in predicting the location of mobile phone users.

Keywords: Location Prediction, Mobile Phone Users, Sequential Pattern Mining,
AprioriAll Algorithm

v

ÖZ

MOBİL TELEFON KULLANICILARININ SIRALI ÖRÜNTÜ MADENCİLİĞİ
TEKNİKLERİ İLE KONUM VE ZAMAN TAHMİNİ

Özer, Mert
Yüksek Lisans, Bilgisayar Mühendisliği Bölümü
Tez Yöneticisi

: Doç. Dr. Pınar Karagöz

Ortak Tez Yöneticisi

: Prof. Dr. İ. Hakkı Toroslu

Temmuz 2014 , 63 sayfa

Telefon kullanım kayıtlarından insanların konumlarının tahmini aktif bir araştırma
alanı haline gelmiştir. Kullanım kayıtlarının büyüklüğü ve mekansal ve zamansal bilgilerin oldukça farklı tanecik seviyelerinde incelenebilir olması bu problemin zorlaşmasının iki ana sebebini oluşturur; özellikle küçük tanecik seviyelerinde kullanıcıların ortak davranış örüntülerini çıkarmak çok daha zorlaşır. Konum tahmini problemi
alanı için 3 tane alt problem tanımladık ve bu problemler için 3 farklı metod önerdik. Bütün metodlardaki temel düşünce şu iki adımı takip eder; konum bilgisini daha
büyük alanlara grupla ve zaman bilgisini daha büyük zaman aralıklarına grupla ve
daha sonra sıralı örüntü madenciliği yöntemleri uygulayarak sonuçlara göre konum
tahmininde bulun. Sonuçlarımızı Türkiye’nin en büyük mobil operatörlerinden birinden alınan gerçek veriler ile doğruladık. Sonuçlarımız oldukça cesaret verici ve cep
telefonu kullanıcılarının konumlarının tahminlerinde çok yüksek doğruluk değerleri
elde ettik.

Anahtar Kelimeler: Yer Tahmini, Cep Telefonu Kullanıcıları, Sıralı Örüntü Madenciliği, AprioriAll Algoritması

vi

To the beauty of ruins

vii

ACKNOWLEDGMENTS

I would like to thank my supervisor Associate Professor Pınar Karagöz and cosupervisor Associate Professor İ. Hakkı Toroslu for their constant support, guidance
and friendship and I also would like to thank them for listening my personal problems
as well. It was a great honour and pleasure to work with them for the last two years
and our cooperation influenced my academical view and my self confidence highly.
I want to thank my colleague İlkcan Keleş for his incredible effort and understanding
during our co-working experience. Without him, this work would be finished much
harder than it has been.
I want to thank my parents Bahriye and Tayfur and my sister Göknil for their constant
support and understanding. Without knowing that they are always there for me, it
would be very hard to overcome the problems that I faced during my graduate studies.
There are lots of friends that do not participate with me in this work but support
me generously during my undergraduate and graduate studies. This work has been
done by the help of them as well. It is great to have them in my life, Hanen Çiftdoğan, Utku Şirin, Fikret Kurt, Pınar Şen, Bahadır Eken, Gülşah Çolak, Deren Atlı,
Hasan Güntürkün, Hüsnü Zeybek, Barış Uğurlu and Behlül Uçar. This work is also
supported by Ministry of Science, Industry and Technology of Turkey with project
number 01256.STZ.2012-1.

viii

TABLE OF CONTENTS

ABSTRACT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

v

ÖZ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

vi

ACKNOWLEDGMENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii
TABLE OF CONTENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ix

LIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

x

LIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xi

LIST OF ABBREVIATIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . xii

CHAPTERS
1

INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

2

RELATED WORK . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

3

BACKGROUND . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

3.1

Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

3.1.1

K-Means Algorithm . . . . . . . . . . . . . . . .

7

Sequential Pattern Mining . . . . . . . . . . . . . . . . . . .

9

3.2

3.2.1
4

AprioriAll Algorithm . . . . . . . . . . . . . . . . 10

DATA AND PROBLEM DEFINITION . . . . . . . . . . . . . . . . 13
ix

4.1

Call Detail Record Data . . . . . . . . . . . . . . . . . . . . 13
4.1.1

4.2

5

Attributes . . . . . . . . . . . . . . . . . . . . . . 14

Problem Definition . . . . . . . . . . . . . . . . . . . . . . . 15
4.2.1

Next Location and Time Prediction Using SpatioTemporal Data . . . . . . . . . . . . . . . . . . . 15

4.2.2

Next Location Change Prediction Using Spatial Data 16

4.2.3

Next Location Change and Time Prediction Using
Spatio-Temporal Data . . . . . . . . . . . . . . . . 16

PROPOSED METHODS . . . . . . . . . . . . . . . . . . . . . . . . 17
5.1

5.2

5.3

Next Location and Time Prediction Using Spatio-Temporal
Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5.1.1

Preprocessing . . . . . . . . . . . . . . . . . . . . 17

5.1.2

Extracting the Regions . . . . . . . . . . . . . . . 18

5.1.3

Extracting Frequent Patterns . . . . . . . . . . . . 20

5.1.4

Prediction . . . . . . . . . . . . . . . . . . . . . . 21

Next Location Change Prediction Using Spatial Data . . . . 23
5.2.1

Extracting the Regions . . . . . . . . . . . . . . . 23

5.2.2

Preprocessing . . . . . . . . . . . . . . . . . . . . 24

5.2.3

Extracting Frequent Patterns . . . . . . . . . . . . 24

5.2.4

Prediction . . . . . . . . . . . . . . . . . . . . . . 25

Next Location Change and Time Prediction Using SpatioTemporal Data . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.3.1

Extracting the Regions . . . . . . . . . . . . . . . 26
x

6

5.3.2

Preprocessing . . . . . . . . . . . . . . . . . . . . 26

5.3.3

Extracting Frequent Patterns . . . . . . . . . . . . 27

5.3.4

Prediction . . . . . . . . . . . . . . . . . . . . . . 27

EVALUATION AND EXPERIMENTAL RESULTS . . . . . . . . . . 29
6.1

Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
6.1.1

6.2

7

Evaluation Metrics . . . . . . . . . . . . . . . . . 30

Experimental Results . . . . . . . . . . . . . . . . . . . . . 31
6.2.1

Results for Next Location and Time Prediction using Spatio-Temporal Data . . . . . . . . . . . . . 32

6.2.2

Results for Next Location Change Prediction using Spatial Data . . . . . . . . . . . . . . . . . . . 35

6.2.3

Results for Next Location Change and Time Prediction using Spatio-Temporal Data . . . . . . . . 43

DISCUSSION AND CONCLUSION . . . . . . . . . . . . . . . . . 59

REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

xi

LIST OF TABLES

TABLES
Table 5.1 Before preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 18
Table 5.2 After preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Table 5.3 Sample Frequent Patterns . . . . . . . . . . . . . . . . . . . . . . . 21
Table 5.4 Before preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 24
Table 5.5 After preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Table 5.6 Before preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 27
Table 5.7 After preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Table 6.1 Number of Frequent Patterns for Different Pattern Lengths . . . . . 33
Table 6.2 Number of Average Total Predictions Per Instance for Different Pattern Lengths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
Table 6.3 Length Tolerance vs g-Accuracy . . . . . . . . . . . . . . . . . . . 40
Table 6.4 Length Tolerance vs Precision and Recall . . . . . . . . . . . . . . 41

xii

LIST OF FIGURES

FIGURES
Figure 5.1 Regions in Zoom Level 1 . . . . . . . . . . . . . . . . . . . . . . 19
Figure 5.2 Regions in Zoom Level 2 . . . . . . . . . . . . . . . . . . . . . . 19
Figure 5.3 Regions in Zoom Level 3 . . . . . . . . . . . . . . . . . . . . . . 20
Figure 6.1 Pattern Length vs g-Accuracy . . . . . . . . . . . . . . . . . . . . 32
Figure 6.2 Minimum Support vs g-Accuracy . . . . . . . . . . . . . . . . . . 34
Figure 6.3 Pattern Length vs g-Accuracy . . . . . . . . . . . . . . . . . . . . 35
Figure 6.4 Pattern Length vs p-Accuracy . . . . . . . . . . . . . . . . . . . . 36
Figure 6.5 Pattern Length vs Prediction Count . . . . . . . . . . . . . . . . . 37
Figure 6.6 Precision vs Recall . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Figure 6.7 Minimum Support vs g-Accuracy . . . . . . . . . . . . . . . . . . 38
Figure 6.8 Minimum Support vs p-Accuracy . . . . . . . . . . . . . . . . . . 39
Figure 6.9 Minimum Support vs Prediction Count . . . . . . . . . . . . . . . 39
Figure 6.10 Multi Prediction Limit vs g-Accuracy . . . . . . . . . . . . . . . . 41
Figure 6.11 Multi Prediction Limit vs p-Accuracy . . . . . . . . . . . . . . . . 42
Figure 6.12 Multi Prediction Limit vs Prediction Count . . . . . . . . . . . . . 42
Figure 6.13 Pattern Length vs g-Accuracy . . . . . . . . . . . . . . . . . . . . 43
Figure 6.14 Pattern Length vs p-Accuracy . . . . . . . . . . . . . . . . . . . . 44
Figure 6.15 Pattern Length vs Prediction Count . . . . . . . . . . . . . . . . . 44
Figure 6.16 Minimum Support vs g-Accuracy . . . . . . . . . . . . . . . . . . 45
Figure 6.17 Minimum Support vs p-Accuracy . . . . . . . . . . . . . . . . . . 46
xiii

Figure 6.18 Minimum Support vs Prediction Count . . . . . . . . . . . . . . . 46
Figure 6.19 Length Tolerance vs g-Accuracy . . . . . . . . . . . . . . . . . . . 48
Figure 6.20 Length Tolerance vs p-Accuracy . . . . . . . . . . . . . . . . . . . 48
Figure 6.21 Length Tolerance vs Prediction Count . . . . . . . . . . . . . . . . 49
Figure 6.22 Multi Prediction Limit vs g-Accuracy . . . . . . . . . . . . . . . . 50
Figure 6.23 Multi Prediction Limit vs p-Accuracy . . . . . . . . . . . . . . . . 50
Figure 6.24 Multi Prediction Limit vs Prediction Count . . . . . . . . . . . . . 51
Figure 6.25 Cluster Count vs g-Accuracy . . . . . . . . . . . . . . . . . . . . . 51
Figure 6.26 Cluster Count vs p-Accuracy . . . . . . . . . . . . . . . . . . . . . 52
Figure 6.27 Cluster Count vs Prediction Count . . . . . . . . . . . . . . . . . . 53
Figure 6.28 Time Interval Length vs g-Accuracy . . . . . . . . . . . . . . . . . 53
Figure 6.29 Time Interval Length vs p-Accuracy . . . . . . . . . . . . . . . . . 54
Figure 6.30 Time Interval Length vs Prediction Count . . . . . . . . . . . . . . 55
Figure 6.31 Time Tolerance vs g-Accuracy . . . . . . . . . . . . . . . . . . . . 55
Figure 6.32 Time Tolerance vs p-Accuracy . . . . . . . . . . . . . . . . . . . . 56
Figure 6.33 Time Tolerance vs Prediction Count . . . . . . . . . . . . . . . . . 56

xiv

LIST OF ABBREVIATIONS

CDR

Call Detail Record

xv

xvi

CHAPTER 1

INTRODUCTION

In today’s world, mobile phones are commonly used devices. Basic usage information
including base station, call records, short message records, GPS records are logged
and used by mobile phone operator companies for various purposes. One of these
purposes is location prediction which helps companies to model their users’ daily life
behaviour. By modelling behaviour of their users, companies build more reasonable
advertisement strategies. These results may also be used by city administrators to
determine mass people movement patterns (in terms of location and time) around the
city.
User location prediction can be studied in terms of different levels of granularities.
Determining the exact coordinate and the time of the next location of a person is
almost impossible. Mobile phones are usually attached to the nearest base stations.
Therefore, each base station coordinate can be considered as the center of a region,
and location prediction can be made at this granularity level. In densely populated
city centers, these regions will be very small and in rural areas they will be very large.
Also due to large number of base stations in densely populated areas, the movements
of people will correspond to jumping over many areas because the number of records
during the movement potentially will not be large enough to have records for each
region that have been passed. Therefore, it is worth to cluster base stations using their
coordinates to define fewer number of regions. Moreover patterns involving high
number of very small regions are not suitable for interpreting mass people movements
in an urban area.
In this work we have empirically set the number of regions to 100 after trying some
1

larger and smaller values. With this many regions we had small number of jump overs
and still the area sizes of the regions became small enough to capture the details of
the people movements. Also notice that by clustering we have obtained regions with
sizes closer to each other. Moreover different number of regions are analyzed as well.
In this work Call Detail Record (CDR) data obtained from one of the largest mobile
phone operators in Turkey has been used. A quick analysis of our data shows that
more than 80% of users’ next location is their current location. Only 20% of the data
contains different locations between two consecutive records of each user. Therefore,
although we present the results for next location prediction here, it makes more sense
to predict this change rather than predicting the next location, which will be the same
one for 80% of data. This idea comprises our second problem definition and second
proposed method; next location change prediction using spatial data.
These two approach are based on constructing the regions by clustering the base stations and then applying sequence pattern mining techniques. In order to realize this,
we follow four phases, which are preprocessing the data, clustering base stations, extracting sequence patterns mining methods and predicting the change of location for
mobile phone user.
User location change prediction can also be studied with different conducive attributes
to increase accuracy rates such as temporal attributes. As a third problem definition,
we introduced the next location change prediction using spatio-temporal data. In
this domain, we both used historical temporal information and predicted the time of
the next location change. Experiments shows that spatio-temporal sequence mining
gives more valuable prediction accuracies rather than simply using spatial attribute.
Moreover alignments on spatial and temporal attribute of the sequences augment the
probability of pattern matching. All three solutions embraces the basics of AprioriAll algorithm. The experiments show that the methods we proposed generates both
concrete and complete location predictions.

2

CHAPTER 2

RELATED WORK

In this chapter, we give information about the previous works that deal with the problem of location prediction. We also summarize various aspects of each technique.
In recent years, a variety of location prediction schemes and scientific findings about
human mobility have been presented in [5], [3], [21], [17], [7], [9], [6], [15], [22],
[20], [19], [8], [4].
Some findings about the human mobility habits and its predictability are presented in
[6] and [15]. In [6], Montjoye et al. proposes a method using both voronoi diagrams
involving base stations and spatial and temporal properties of users’ movement data
to find the minimum number of points enough to uniquely identify individuals. They
propose that four randomly chosen points are enough to characterize 95% of the users
while two of them characterize more than 50%.
In [15], Song et al. analyze the limits of predictability in human mobility. They
used the data collected from mobile phone carriers for 3-month-long of 50,000 individuals. They propose three entropy measures which is believed to be the most
fundamental quantity to analyze the limits of predictability, the random entropy, the
temporal-uncorrelated entropy and the actual entropy. They also use a probability
measure for correctly predicted user’s future movements. They find that 93% potential predictability in user mobility at best and it is not under 80% for any user.
There are other methods to use for location prediction problem rather than sequential
pattern mining such as markov models and expectation maximization algorithms. In
[17], Thanh et al. make use of Gaussian distribution and expectation maximization

3

algorithm to learn the model parameters. Then, mobility patterns, where each is characterized by a combination of common trajectory and a cell residence time model,
are used for making predictions. They use Gaussian mixture models to find similarities in cell-residence times of mobile users. They outperform the methods that ignore
temporal characteristics of user movements. However they are in need of studying
their method in real data.
In [7], Gao et al. use both spatial and temporal data to predict users’ location. They
propose 10 models which can be categorized as spatial-based, temporal-based and
spatio-temporal. They make use of Bayes’ rules for their prediction models which
use historical data while predicting the next location. They also make use of Markov
Models to build 2 of their models. For the best model named as HPY Prior Hour-Day
Model, they managed to predict user locations with an accuracy rate of 50%. They
do not use any social network information together with spatio-temporal patterns.
In [9], Gidofalvi et al., proposes a method which use both spatial and temporal GPS
data for building Markov Model which is used for next location and time prediction
of user. In other words, they both predict the change of location and when this change
occurs. They use an Inhomogeneous Continuous-Time Markov(ICTM) model since
the prediction depends on the previous locations and time. They use both spatial and
temporal information for building the model. Their ICTM model predict the departure
time correctly with the 45 minute error and the next region correctly 67% of the cases.
Similar to our work, in [19], [8] and [4], they propose sequential pattern mining
techniques for the location prediction problem. In [19], Yavas et al. propose an
AprioriAll-based algorithm which is similar to our three methods. They extract frequent user trajectories which they name user mobility patterns (UMP) from a user
move database and predict the user’s next movement accordingly. However they do
not use any spatial or temporal information while extracting UMPs or predicting. The
rules are consist of only cell ids rather than any spatial attribute. They introduce alignment parameters on the length of the sequences and maximum number of predictions
as ours. They claim that they get higher accuracies than the methods of Mobility
Prediction based on Transition Matrix and Ignorant Prediction.
In [8], Giannotti et al. propose methods to solve different trajectory pattern mining
4

problems. They define spatio-temporal sequences as the pairs of spatial attribute and
the time that user has spent in there. They also try to detect the popular regions
which is named as ROI. The difference with the conventional sequence pattern mining
technique is the use of trajectories (T-patterns) rather than itemsets. Their method for
mining T-patterns extract both computationally feasible and useful patterns.
In [4], Cao et al. introduces a method for discovering of periodic patterns in spatiotemporal sequences. They also make use of an AprioriAll-based algorithm for
extraction of periodic patterns. The distinctive feature of these periodic patterns is
that they are not frequent in the whole time span but in some time interval, so they
change their support definition accordingly.
There are various works that try to further increase the prediction accuracies by the
help of social networks. In [5], Cho et al. proposes that general human mobility
do not have a high degree of freedom and variation as it is believed. They work
on three features of human mobility; geographic movement, temporal dynamics and
the social network. Social network is used since human mobility is partly driven
by our social relationships, e.g. we move to visit our friends. They use three main
data source where two of them are popular online location based social networks,
Gowalla and Brightkite and the other is a trace of 2 million mobile phone user’s
phone activity in Europe. They find that social relationships can explain about 10%
of human movement in cell phone data and 30% of movement in location based social
networks. However periodic movement behaviour explains about 50% to 70% of it.
They develop an expectation maximization based prediction model and they reach
40% accuracy while predicting user’s location at any time.
In [3], Boldrini et al. propose a model that integrates 3 main properties believed to
be fundemental for human mobility. First, user mobility largely depends on their
social relationships. Second, users are disposed to spend their most of time in a few
locations. Third, users mostly move shorter distances rather than the longer ones. The
main novelty of their model named Home-cell Community-based Mobility Model
(HCMM) is to integrate these three features. They incrementally improved HCMM
starting with a pure social-based model and mathematically justifying the need for
extending the features. Finally they claim that HCMM is able to regenerate the main

5

properties of human movement patterns.
In [21], Zhang et al. further improves the user mobility models of [3] and [5] by
amplifying the effect of social network information in location prediction. They also
claim that call patterns are strongly related with co-locate patterns and mainly affect
user short-time mobility. They further propose a method named NextMe which takes
social interplay into consideration as well. However this time, when the social interplay will affect social mobility is identified and used accordingly. They validate their
scores with the MIT Reality Mining dataset. They reach up to 60% accuracy levels
for the prediction with their NextMe method.
Rather than using social relationships or networks of the user, in [22] and [20] they
make use of the distinctive features of spatial attribute in the data. In [22], Zheng et
al. aim to extract interesting locations such as culturally significant places, shopping
malls, city centers etc., and travel sequences from multiple users’ GPS logs. They
used tree-based hierarchical (TBHG) to model user’s historical movement patterns
then introduce a HITS (Hypertext Induced Topic Search)-based inference model,
which represents one of the users’ travel to a location as a vertex. The weight of
the vertex is defined by user’s experience. Location’s interest is also defined by user’s
experience as well as the number of user’s visit. They claim that such model can
be used for location recommendation like a mobile tourist guidance. They evaluated
their method with the GPS data of the 107 users of a 1 year period.
In [20], Ying et al. proposes an algorithm which uses semantic labels for locations
rather than just using spatial attributes. They explore semantic trajectories of the
users and predict the next location of the user accordingly. Rather than using sequential pattern mining techniques, they use clustering methods for next location prediction. They group users hierarchically according to their semantic trajectories by using
Maximal Semantic Trajectory Pattern Similarity (MSTP-Similarity) which they define. It was the first work which combines the semantic tags for location and spatial
attributes for next location prediction problem and their proposed location prediction
model has excellent performance.

6

CHAPTER 3

BACKGROUND

In this chapter, basics of conventional algorithms used in this work are introduced. In
the first section, definition of clustering and one clustering method namely k-means
are presented. In the second section, the definition of sequential pattern mining and
the AprioriAll algorithm are presented.

3.1

Clustering

Cluster analysis groups data objects based only on information found in the data that
describes the objects and their relationships. The goal is that the objects within a
group be similar (or related) to one another and different from (or unrelated to) the
objects in other groups. The greater the similarity (or homogeneity) within a group
and the greater the difference between groups, the better or more distinct the clustering [16]. Clustering methods can be categorized according to their two attributes;
nested or not and exclusive or overlapping or fuzzy. We preferred a non-nested and
exclusive clustering method for clustering the base stations since we needed each
base station to be the member of only one cluster and did not need any hierarchical
connection between clusters.

3.1.1

K-Means Algorithm

K-Means Algorithm is a non-nested and exclusive clustering method which embraces
the idea of grouping similar objects into same clusters and non-similar objects into
7

different clusters. It was the first time when its name was used in [12] while the more
efficient version of it is introduced in [11]. Formally, given a data set, D, of n objects,
and k, the number of clusters to form, k-means algorithm organizes the objects into
k partitions (k <= n), where each partition represents a cluster[10]. The number
of partitions k is expected to be defined by the user. Partitioning is done according
to the centroids of the clusters. Each data object is assigned to the nearest cluster
while the concept of nearness can be defined using several distance metrics such as
Euclidian, Manhattan, Chebychev distances etc. Distance metric is chosen according
to the problem definition.
Algorithm can be divided into two steps; data assignment and relocation of centroids.
After every data object assignment to the partitions are completed, new centroids
are found by computing the means of the elements of that particular partition. This
process is iterated recursively until the members of partitions do not change or some
user defined condition is satisfied.
Algorithm 1 K-Means Algorithm
Input: Dataset D, number of clusters k
Output: Set of cluster centroids C, cluster membership vector m
1:

function K M EANS(D, k, C, m)

2:

Randomly choose k data points from D

3:

Use these k points as initial set of cluster representatives C

4:

repeat

5:

Reassign points in D to closest cluster mean

6:

Update m such that mi is cluster id of ith point in D

7:

Update C such that cj is the mean of points in j th cluster

8:
9:

until convergence of objective function
end function

Usually, algorithm’s objective function is to minimize the total squared Euclidean
distance between each point and its closest centroid. It can be formulated as follows
where xi represents the ith data object and cj represents the centroid of the j th cluster;
k
X
i=1

argmin ||xi − cj ||22
j

8

(3.1)

Drawbacks of the algorithm can be summarized in 4 aspects. First, because of the
greedy nature of the algorithm, user defined initial centroids matter. In other words,
there is no one correct clustering output for the algorithm, it is affected by the initial
centroids. Second, choosing k can be difficult since the default algorithm do not
generate or suggest any optimal number for how many clusters there should be. Third,
standard algorithm is sensitive to outliers and last, it does not guarantee against empty
clusters.

3.2

Sequential Pattern Mining

The concept of sequential pattern mining is first introduced by Agrawal and Sirikant
in [2] as follows; Given a set of sequences, where each sequence consists of a list
of elements and each element consists of a set of items, and given a user-specified
minimum support threshold, sequential pattern mining is to find all frequent subsequences, i.e., the subsequences whose occurrence frequency in the set of sequences
is no less than minimum support.
In the spatio-temporal context, sequential pattern mining can be expressed as follows
similar to Agrawal and Sirikant’s definition. Given a set of sequences, where each
sequence consists of a list of elements and each element consists of a spatial and
temporal attributes, and given a user-specified minimum support threshold, spatiotemporal sequential pattern mining is to find all frequent time ordered movement
pattern subsequences. In general, sequence of k elements is denoted in a form such
as s =< s1 , s2 , s3 , ..., sk >. A sequence s1 is subsequence of s2 if and only if all elements of s1 is contained in s2 in the same order. The concept of minimum support is
the same as in the conventional itemset problems. Support of a sequence is the ratio of
the number of the occurence of the sequence in the whole database to the total number of sequences with the same length in the whole database. A sequence satisfying
the minimum support constraint is called a frequent sequence [18] or a large/maximal
sequence. A sequence containing k elements is represented by k-sequence.
9

3.2.1

AprioriAll Algorithm

AprioriAll is a sequential pattern mining algorithm first introduced in [2] after introducing Apriori algorithm in [1] which constitutes a base for the AprioriAll. It is
designed for extracting maximal sequences from a database. It consists of five phases,
namely sort phase, fitemset (frequent itemset) phase, transformation phase, sequence
phase and maximal phase. The main phase is the sequence phase while first three
phase can be considered as a preprocessing phases and the last phase as a postprocessing phase.
In the first phase, database D is modified by taking sequence id and transaction time
into consideration. Transaction time is used for creating time ordered sequences.
Sequence id is used for making elements with same id appear in the same sequence.
This phase is needed for the sake of convenience of the following phases.
In the second phase, the set of all large 1-sequences are extracted. In this phase, all
fitemsets can be obtained by using conventional Apriori algorithm with the relevant
modifications in counting and support. These fitemsets are mapped to ordinal integers
so that comparing two fitemsets takes constant time.
In the third phase, each database entry or in other words transaction is modified such
that the elements that are not the member of any fitemset are eliminated in that transaction. If there exist no element in transaction after elimination, it is not retained in
the transformed database DT anymore. However it is still used for counting the total
number of sequences.
In the fourth phase, new candidate sequences are generated. The candidates are generated by using the previously generated fitemsets or maximal sequences. To generate
k-sequence candidates algorithm uses (k-1)-sequences. It basically joins the (k-1)sequences to find candidate k-sequences. After each candidate generation, algorithm
counts the occurrences of the candidates in the database. This information is used for
eliminating the sequences that fall below the predefined minimum support value.
In the fifth phase, frequent sequences that are not maximal are eliminated and a set
containing maximal sequences are generated. Algorithmic definition of the Apriori10

All algorithm can be seen in Algorithm 2 taken from [18].
Algorithm 2 AprioriAll Algorithm
Input: Dt : transformed database of transaction sequences
minsup: minimum support parameter
Output: f requentP atterns: the set of large sequences
1:

F1 = frequent 1-sequences;//Result of fitemset phase

2:

for k = 2; Fk−1 6= ∅; k++ do

3:

Ck = apriori-gen(Fk−1 );//New candidate sequences

4:

for all transaction sequence t ∈ Dt do

5:

Ct = subseq(Ck , t);//Candidate sequences contained in t

6:

for all candidate c ∈ Ct do c.count++

7:

end for

8:

end for

9:

Fk = {c ∈ Ck |c.count ≥ minsup};

10:

end for

11:

f requentP atterns = maximal sequences in ∪k Fk

11

12

CHAPTER 4

DATA AND PROBLEM DEFINITION

In this section, we present details of data used in this work and we give the definitions
of three problems related with location prediction problem.

4.1

Call Detail Record Data

In this work we utilized the CDR data of one of the largest mobile phone operators
of Turkey. The data corresponds to an area of roughly 25000 square km with a population around 5 million. Almost 70% of this population is concentrated in a large
urban area of approximately 1/3 of the whole region. The rest of the region contains
some mid-sized and small towns and large rural area with a very little population.
The CDR data contains roughly 1 million user’s log records for a period of 1 month.
For each user there are 30 records per day on average. The whole area contains more
than 13000 base stations.
Each record in data represents one of the followings; voice caller, voice callee, sms
sender, sms receiver, gprs connection. Besides these cases, no record exists in the
CDR data. These records consists of 11 attributes namely, base station id #1, phone
number #1, city plate of the phone number #1, base station id #2, other phone number,
city plate of the other phone number, call time, cdr type, url, duration, call date. Definition of these attributes and example record attributes are presented in the following
subsection.
13

4.1.1

Attributes

• base station id#1: unique integer representing the base station which caller, sms
sender or gprs user connected to. e.g. 17083
• phone number#1: unique string representing the caller, sms sender or gprs user.
Due to the privacy reasons, it is not a regular phone number.
e.g. 7bcfc0259b9c8a4af95177a7e79bcd28
• city plate of the phone number #1: an integer that represents the city user started
a call or a gprs connection, or sent an sms. e.g. 06
• base station id #2: unique integer representing the base station which callee
or sms receiver is connected to. It is null if the type of the record is gprs
connection. e.g. 17083
• other phone number: unique string that represents the callee or sms receiver.
Due to the privacy reasons, it is not a regular phone number. It is null if the type
of the record is gprs connection. e.g. 28119ffa652d31607a3bb573bd3d594b
• city plate of the other phone number: an integer that represents the city callee
or sms receiver is in. e.g. 06
• call time: The time that action started in a "hhmmss" format. e.g. 170251
• cdr tpye: It can be one of the following;
– mmo: voice caller
– mmt: voice callee
– msmo: SMS sender
– msmt: SMS receiver
– gprs: GPRS connection
• url: It is used only for GPRS data. It represents the url that user tries to get.
• duration: It is an integer that represents the duration of the call. It is null for
sms. e.g. 47
• call date: it is the date that action performed in a "yyyyMMdd" format. e.g.
20120907
14

4.2

Problem Definition

In this section, we introduce the 3 narrower problem definitions for broader location
prediction problem namely, next location and time prediction using spatio-temporal
data, next location change prediction using spatial data and next location change and
time prediction using spatio-temporal data. For all three problems there are common
unnecessary attributes in data such as, city plate, cdr type, url, duration. These attributes are eliminated since they are not used in further computations. Call time can
also be eliminated according to the problem type. We use the term action for any type
of phone activity such as voice call, sms, gprs.

4.2.1

Next Location and Time Prediction Using Spatio-Temporal Data

The aim for this problem is to predict the location and the time of the next action in the
next time interval of the user roughly. Rather than predicting exact coordinate or base
station, it is aimed to find the next region of the user. For this reason, base stations are
grouped into the regions and prediction is done accordingly. Moreover, rather than
using exact time information, predicting the time interval of the next action is aimed.
Daily user sequences are temporal ordered location and time information pair sequences of the user. For one time interval, there exists one location and time information pair. The location with the most occurrence in the time interval represents the
location attribute for that particular time interval. These records are stored in the user
sequence database D0 .
The problem can be formalized as follows; given a user sequence database D0 (obtained from CDR database D) containing daily user sequences, the problem is to find
the region and time interval of the next action in the next time interval by using the
historical movement sequences.
15

4.2.2

Next Location Change Prediction Using Spatial Data

The aim for this problem is to predict the next location of the user when he/she
changes his/her location. This problem is defined since people usually do not change
their location between two actions and this causes misleading high accuracies for the
solution of the first problem. Rather than trying to find the location of the next action,
we focused on the prediction of the location when the user changes his/her.
For this problem, daily user sequences do not contain the time information. It is
used only for temporal ordering while converting CDR database D to user sequence
database D0 .
The problem can be formalized as follows; given a user sequence database D0 (obtained from CDR) database D containing daily user sequences, the problem is to find
the next location of the user when he/she changes his/her location by using historical
movement sequences.

4.2.3

Next Location Change and Time Prediction Using Spatio-Temporal Data

The aim for this problem is to predict the next location and time of the user when
he/she changes his/her location. The difference is that, it is tried to predict the temporal information of the next action when the user changes its location, compared to
the second problem definition. Rather than predicting exact time of the action, it is
aimed to find the time interval that action takes place in.
Daily user sequences contain both spatial and temporal information of the actions.
Different than the one in the first problem definition they contain successive repetitive
time intervals.
The problem can be formalized as follows; given a user sequence database D0 (obtained from CDR database D) containing daily user sequences, the problem is to find
the next location of the user and the time of the action when he/she changes his/her
location by using historical movement sequences.

16

CHAPTER 5

PROPOSED METHODS

In this section we introduce our three solution for the three problems defined in the
section 4.2.

5.1

Next Location and Time Prediction Using Spatio-Temporal Data

The method is designed for the problem of predicting the location and time of the next
action in the next time interval. Both spatial and temporal attributes of the data is used
while building the model. The method consists of 4 steps namely, preprocessing,
extracting the regions, extracting frequent patterns and prediction. Details of these
steps are given in the following subsections.

5.1.1

Preprocessing

Due to the high volume of the data and high number of attributes, which of them are
not relevant for our analysis such as city code, phone number etc., it is necessary to
apply some basic preprocessing tasks on the data. First, we filter the unnecessary
attributes. Date and time information are merged into a single column and, it is used
for sorting records in temporal order. Second information is not used. We further
combine call data records of a user on the same day into a single record. By this way,
each record, which is structured as a sequence of <base station id, time of the day>
pairs, represents a user’s daily movement. Time of the day attribute is formatted as
’hhmm’. An example preprocessing step can be seen in 5.1 and 5.2. B stands for base
17

station id while R stands for region id.
Table 5.1: Before preprocessing

B17083
B17083
B10592
B10592
B10592
B10592

phone#1
phone#1
phone#1
phone#1
phone#1
phone#1

06
06
06
06
06
06

B17083
B28744
B20062
B37382
B10593
B12912

phone#2
phone#3
phone#4
phone#4
phone#5
phone#6

06
06
06
06
06
06

20120907
20120907
20120907
20120907
20120907
20120907

010251
071008
092231
111540
144332
170304

mmo
mmo
mmo
mmo
mmo
mmo

47
3
11
8
14
12

Table 5.2: After preprocessing

B17083,0102 B17083,0710 B10592,0922 B10592,1115 B10592,1443 B10592,1703

5.1.2

Extracting the Regions

In populated parts of the cities, such as downtowns, the base stations are placed very
close to each other. Under high number of base stations, it is not practical to consider each station as the center of a movement region to interpret the semantics of
the movements. Therefore, in this work, we define regions by grouping the base
stations. In spatial clustering, K-Means and K-Medoids are commonly used partitional algorihms[14]. To this aim, we cluster base stations according to their location
information (x and y coordinate attributes) using k-means algorithm. The aim of kmeans clustering algorithm is to partition n observations into k clusters in which each
observation belongs to the cluster with the nearest cluster mean. There are 13281
base station ids in the original data and after exploring several other k values, we
group them into 100 clusters which we name as regions. Then, base station ids in the
preprocessed data are replaced with the corresponding region ids. At the end of this
process, the largest cluster contains 656 base stations and the smallest cluster contains
only 6 base stations. Visualization of the regions can be seen in Figure 5.1, Figure 5.2
and Figure 5.3 in three different zoom levels.
18

Figure 5.1: Regions in Zoom Level 1

Figure 5.2: Regions in Zoom Level 2

19

Figure 5.3: Regions in Zoom Level 3

5.1.3

Extracting Frequent Patterns

In this approach we use both spatial and temporal information of each call record.
Spatial attribute is the region id and temporal attribute is the time of the day information of the call. We do not use day information because each user’s daily sequence
corresponds to a single day and we do not take the day of the week or the month into
consideration.
Our operation of extracting frequent patterns work with four arguments, namely preprocessed CDR data, pattern length, minimum support and time interval length. Pattern length describes the length of the desired frequent patterns. Minimum support
describes the candidate frequent patterns’ required proportion in data. Time interval
length is used for discretizing time of the day. It indicates the span of discretized
time interval. Rather than using exact time information of the action, we prefer to
use discretized format for time of the day to be able to augment frequent patterns in
data. Applying discretization allows us to eliminate small time differences. Each day
is divided into predefined number of equal length time intervals. Action’s time of the
day information is replaced with the starting time of its corresponding interval. After
this change, user’s daily sequence may have more than one base station id and time of
20

the day pair having the same time interval value. In order to reduce them to a single
pair, for each time interval, the most frequently observed station id is selected as the
representative of that interval.
Our extraction method can be defined as a modified version of AprioriAll algorithm.
As given in the literature, AprioriAll algoritm consists of two phases, namely; candidate generation and elimination. Difference is in the candidate generation. Normally
k-length candidates are generated from (k-1)-length patterns. Since this operation is
costly in time for big data and we are only interested in patterns of a given length, we
have changed the candidate generation phase. Our algorithm generates all candidates
while traversing the data. Two index pointers are used such that one of them points
the start of candidate pattern while the other one points the end of it. Count of each
candidate pattern observed is recorded for elimination phase. As in the conventional
AprioriAll algorithm’s candidate elimination phase, candidate patterns whose support
value falls below minimum support are eliminated, while the others constitute the frequent pattern set. Table 5.3 demonstrates 3 sample frequent patterns for pattern length
4. In the table, the pairs separated by paranthesis represent region id and discretized
time of the day. Region id and discretized time of the day are separated by comma.
Table 5.3: Sample Frequent Patterns

Frequent Pattern (Sequence)
<(R91,1000), (R95,1215), (R45,1615), (R48,1800)>
<(R91,1000), (R95,1215), (R45,1615), (R70,1900)>
<(R91,1000), (R95,1215), (R45,1615), (R55,1915)>

5.1.4

Support
4.0212e-06
3.6897e-06
2.5369e-06

Prediction

In the prediction phase, initially, traversal data of the user for whom prediction will
be performed is preprocessed and formatted same as frequent patterns. Assume that
the traversal pattern is of length (k-1) and we want to predict the next step, which is
the kth element, for this user’s traversal. Then, this (k-1) length pattern is used as the
test sequence for prediction and we search this test sequence in the frequent pattern
set that is created in the extraction phase. If patterns starting with the test sequence
21

have been found, the last element of the matching pattern with the maximum support
is generated as the prediction. This process is given in Algorithm 3
Algorithm 3 Prediction Algorithm
Input: testsequence
Output: prediction
1:

maximumSupport ← 0

2:

for all pattern ∈ f requentP atterns do
if testsequence == pattern[1 : k − 1] then

3:

if maximumSupport < pattern.support then

4:
5:

prediction ← pattern[k]

6:

maximumSupport ← pattern.support
end if

7:

end if

8:
9:
10:

end for
return prediction

Although we take this approach as our base method, we added two tolerance parameters to the prediction algorithm to improve our results. These are tolerance in time
and the multi prediction limit allowed for one instance. Under tolerance in time, patterns are not fixed to some time interval value anymore. They are moved forward or
backward in time with a tolerance value. If one pattern is not in frequent pattern set,
then tolerance mechanism runs and tries to find tolerated prediction value. Assume
that we have a traversal instance
<(R91,1015), (R95,1230), (R45,1630)>
and we want to predict next location time pair for that instance but our frequent pattern set does not have a pattern starting with
<(R91,1015), (R95,1230), (R45,1630)>
but has
<(R91,1000), (R95,1245), (R45,1630), (R52,1700)>
As it can be easily seen, <(R91,1000), (R95,1245), (R45,1630)> is in the range of 15
minute tolerance of <(R91,1015), (R95,1230), (R45,1630)>. If tolerance value for
time is greater than 15 minutes, then our method gives the result (R52,1700) as the
prediction.
22

The second tolerance parameter, namely the multi prediction limit allowed for one
instance, is introduced to utilize the cases in which there are more than one frequent
pattern starting with traversal instance. As it can be seen in Algorithm 3, the method
returns the last element of the frequent pattern starting with traversal instance with
maximum support and does not take other possible matchings into consideration.
However by adding multi prediction limit parameter, more than one prediction value
are generated. This parameter puts a limit to the proportion of the total support of
the patterns in the prediction set, to the total support of all patterns that start with
given test sequence. All frequent patterns starting with traversal instance are sorted in
decreasing support value order and prediction set is populated by adding kth elements
of frequent patterns until the multi prediction limit is satisfied.
For example, <(R91,1000), (R95,1215), (R45,1615)> is the traversal instance and
there are frequent patterns with length 4 as given in Table 5.3. For this test sequence, in the single prediction method, among the matching patterns, it chooses
only (R48,1800), which has the maximum support. If the multi prediction limit is
0.5, it only gives one prediction value which is (R48,1800). However if the limit is
0.8, then it gives two predictions which are (R48, 1800) and (R70, 1900).

5.2

Next Location Change Prediction Using Spatial Data

The method is designed for the problem of predicting the location of the user when
he/she changes his/her location. Only spatial attribute of the data is used while extracting frequent patterns without any successively repetitive region ids. The method
consists of 4 steps namely, extracting the regions, preprocessing, extracting frequent
patterns and prediction. Details of these steps are given following subsections.

5.2.1

Extracting the Regions

Clustering the base stations are done in the same way with the method described in
section 5.1.2. We need to cluster base stations before preprocessing because we need
to use the region ids in the user’s daily sequence which is created in the preprocessing
step. The base stations are grouped into 100 regions for this method as well. Then,
23

base station ids in the CDR data are replaced with the corresponding region ids.

5.2.2

Preprocessing

As in the first method, we filter the unnecessary attributes such as city code, phone
number etc. Date and time information are also merged into a single column and,
it is used for sorting records in temporal order. We again combine call data records
of a user on the same day into a single record but this time successive region ids are
deleted. By this way, each record, which is structured as a sequence of region ids,
represents a user’s daily location change pattern. An example preprocessing step can
be seen in 5.4 and 5.5
Table 5.4: Before preprocessing

R91
R91
R55
R55
R55
R55

phone#1
phone#1
phone#1
phone#1
phone#1
phone#1

06
06
06
06
06
06

R91
R21
R27
R27
R91
R3

phone#2
phone#3
phone#4
phone#4
phone#5
phone#6

06
06
06
06
06
06

20120907
20120907
20120907
20120907
20120907
20120907

010251
071008
092231
111540
144332
170304

mmo
mmo
mmo
mmo
mmo
mmo

47
3
11
8
14
12

Table 5.5: After preprocessing

R91

5.2.3

R55

Extracting Frequent Patterns

Except for the use of time information, basic intuition behind the extraction method
is nearly the same as that of the method described in section 5.1.3. In this approach,
the patterns are generated in order to keep only the change of region ids in a single
day. To this aim, pairs having the same region id as in the previous pair are eliminated. This guarantees that there will be no successive repetition of region ids in one
frequent pattern, and predictions never have the same region id with the last region id
of traversal instance. In addition, time interval elements are also deleted.
24

5.2.4

Prediction

The basic idea of the prediction that deals with spatio-temporal data is also used in
this approach. However, one of our tolerance parameters is different in this prediction
method. Since we do not have any time information, time tolerance is not applicable
to this approach and it is replaced with the tolerance in pattern length.
Tolerance in pattern length can be applied in two ways. In the first way, tolerance
in pattern length gives us opportunity to predict one traversal instance’s next region
id by examining the shorter frequent patterns. This can be possible when the shorter
frequent pattern is a subset of the exact traversal instance (order of region ids are
important). Assume that, we have the test sequence;
<R77, R91, R95, R16, R22, R41>
however, there is no exactly matching pattern. Instead, we have the following frequent
pattern,
<R77, R95, R16, R22, R41>
Since the set <R77, R95, R16, R22, R41> is a subset of <R77, R91, R95, R16, R22,
R41>, in which the second element of the larger pattern is missing, the last element
of frequent pattern that starts with <R77, R95, R16, R22, R41> can be given as the
prediction result for test sequence <R77, R91, R95, R16, R22, R41>.
Second way of tolerating the pattern length gives us opportunity to predict the next
region by examining the longer frequent patterns. This is possible when the longer
frequent pattern contains the exact traversal instance (order of region ids are important) but also contains some additional region ids. Assume that, we have the following
test sequence;
<R77, R91, R95, R16, R22, R41>
however, there is no exactly matching pattern. Instead, we have the frequent pattern
that starts with the following,
<R77, R91, R95, R18, R16, R22, R41>
Since the set <R77, R91, R95, R18, R16, R22, R41> contains <R77, R91, R95, R16,
R22, R41> in the same order, which also has the fourth element (R18) as the difference from the traversal instance, last element of frequent pattern that starts with
<R77, R91, R95, R18, R16, R22, R41> is given as the prediction result for traversal
25

instance <R77, R91, R95, R16, R22, R41>.

5.3

Next Location Change and Time Prediction Using Spatio-Temporal Data

The method is designed for the problem of predicting the location and time of the user
when it changes its location. Both spatial and temporal attributes of the data is used
while extracting the frequent patterns. The method consists of 4 steps namely, extracting the regions, preprocessing, extracting frequent patterns and prediction. Details of
these steps are given following subsections.

5.3.1

Extracting the Regions

Clustering the base stations are done in the same way with the two methods described
in 5.1.2 and 5.2.2. We need to cluster base stations before preprocessing because
we need to use the region ids in the user’s daily sequence which is created in the
preprocessing step. The base stations are grouped into 100, 200, 400, 800, 1600,
3200 and 6400 regions for this method to analyze the effect of the different region
numbers. Then, base station ids in the CDR data are replaced with the corresponding
region ids.

5.3.2

Preprocessing

As in the first and second method, after preprocessing, each record, which is structured as a sequence of region ids, represents a user’s daily location change pattern.
Difference with the second method is the usage of temporal information. For this
method user’s daily sequence contains not only spatial attribute but also temporal attribute. Repetitive time is allowed for this method while it is not for the first method.
An example preprocessing step can be seen in Table 5.6 and Table 5.7
26

Table 5.6: Before preprocessing

R91
R91
R55
R55
R55
R55

phone#1
phone#1
phone#1
phone#1
phone#1
phone#1

06
06
06
06
06
06

R91
R21
R27
R27
R91
R3

phone#2
phone#3
phone#4
phone#4
phone#5
phone#6

06
06
06
06
06
06

20120907
20120907
20120907
20120907
20120907
20120907

010251
071008
072231
111540
144332
170304

mmo
mmo
mmo
mmo
mmo
mmo

47
3
11
8
14
12

Table 5.7: After preprocessing

R91,0102

5.3.3

R55,0722

Extracting Frequent Patterns

Basic intuition behind the extraction method is nearly the same as that of the first
proposed method. In this approach, the patterns are generated in order to keep only
the change of region ids in a single day. The difference with the second method is the
use of temporal information. This time user’s daily sequences have pairs of region
id and time information as in the first method. To this aim, pairs having the same
region id as in the previous pair are eliminated. This guarantees that there will be no
successive repetition of region ids in one frequent pattern, and predictions never have
the same region id with the last region id of traversal instance.

5.3.4

Prediction

In this method, we use both tolerance parameters, time tolerance and tolerance in
pattern length for prediction. Apart from that the prediction algorithm works the
same with the first method. For the traversal pattern with the length (k-1), we again
predict its next location by searching the frequent patterns starting with that traversal
pattern.

27

28

CHAPTER 6

EVALUATION AND EXPERIMENTAL RESULTS

In this section, first we introduce our evaluation method and evaluation metrics, and
then we give the experimental results for three methods explained in the previous
section namely, next location and time prediction using spatio-temporal data, next location change prediction using spatial data, next location change and time prediction
using spatio-temporal data.

6.1

Evaluation

In order to asses the quality of the predictions made by the methods proposed in
the previous section we have used k-fold cross validation technique with k=5, on a
real CDR data set that has been introduced earlier. Training phase of the evaluation
process is nothing but applying the frequent pattern extraction steps of the proposed
methods on the training data, in order to generate frequent patterns.
The testing phase has two steps: In step one, the test data is processed as in the training
phase to extract all sequential patterns, except this time with no minimum support,
in order to generate all traversal patterns. For each one of the traversal patterns,
prediction algorithm introduced in the previous section has been applied to predict the
last elements of these patterns. The result of the prediction is compared against the
actual last element of the traversal pattern. These results are used in the calculations
of the evaluation metrics which is introduced below.
For the method proposed for the next location and time prediction using spatio29

temporal data problem, we do not prefer to present detailed results. The reason for
this preference is the nature of human mobile telephone usage routines. They usually
do not change their location between two mobile telephone activities. Because of that
prediction accuracy results are misleading. Further analysis will be discussed in the
following sections.
For the method proposed for the next location change prediction using spatial data
problem, we analyze the effects of minimum support, multi prediction limit, pattern
length and length tolerance parameters.
For the method proposed for the next location change and time prediction using
spatio-temporal data problem, we analyze the effects of minimum support, multi prediction limit, time interval length, time tolerance, pattern length and the cluster count
of base station ids.

6.1.1

Evaluation Metrics

This section describes the method and the metrics that we used in order to measure the
success of the proposed prediction methods. We used three different metrics, namely
p-accuracy, g-accuracy and prediction count.
Accuracy measures how much of our predictions match with exact next region id of
the test pattern. It simply can be defined as the ratio of true predictions to the all
predictions. In our case, we have two types of accuracy. The first one, which is the gaccuracy (general accuracy), is the ratio of number of true predictions to the number
of all patterns with the same length in the test set. The second one, which is the paccuracy (predictions’ accuracy), is the ratio of the number of true predictions to the
number of all predictions we are able to make. The reason for using two different
accuracy calculation is due to the fact that the proposed algorithm may not be able
to generate prediction for each of the test instances, if there is no matching frequent
pattern found for the queried instance. In the first form of accuracy calculation, the
accuracy result superficially drops for such cases. For each of the methods g-accuracy
and p-accuracy metrics are used for representing and interpreting the results.
Prediction Count metric is required because of the multi prediction limit parameter.
30

It quantifies the size of the prediction set when correct prediction result is in the
prediction set.
In addition to true prediction; true positive, false positive and false negative values
are calculated for the first two methods that are related with the next location and
time prediction using spatio-temporal data and next location change prediction using
spatial data problems as given in Algorithm 4
Algorithm 4 Evaluation Algorithm
1: for all prediction in predictionSet do
if prediction = actualN extRegionId then

2:
3:

incrementTruePositive(prediction)

4:

return
else

5:

incrementFalsePositive(prediction)

6:

end if

7:
8:

end for

9:

incrementFalseNegative(actualNextRegionId)

For multi prediction, since we increment false positives for each item in our prediction
set, and increment false negatives only when none of our predictions do not hold, our
results are biased through the recall, rather than precision.
Precision can be defined as the ratio of the number of true positives to the sum of the
number of true positives and false positives.
Recall can be defined as ratio of the number of true positives to the sum of the number
of true positives and false negatives. Further definitions of these metrics can be found
at [13]

6.2

Experimental Results

In this section, the results of the experiments of three proposed methods under different parameters are given.
31

6.2.1

Results for Next Location and Time Prediction using Spatio-Temporal
Data

In this subsection the effect of pattern length and minimum support on g-accuracy are
experimentally analyzed.

Pattern Length

In this set of experiments, we analyze the effect of length of the frequent patterns on
the g-accuracy of prediction. For this set of experiments, time tolerance is 75 minutes
for 15 minute time interval length, minimum support is 10−6 , cluster count is 100, and
multi prediction support limit is 1.0, which means use all frequent patterns matching
with test set patterns.

Figure 6.1: Pattern Length vs g-Accuracy

As it can be seen from Figure 6.1, when the pattern length increases, prediction gaccuracy decreases. This is due to the fact that the number of longer frequent patterns
is much fewer than the number of shorter frequent patterns. The number of frequent
patterns for various pattern lengths are given in Table 6.1.
32

Table 6.1: Number of Frequent Patterns for Different Pattern Lengths

Pattern Length
2
3
4
5
6
7
8
9
10
11
12

Number of Frequent Patterns
1777423
1706778
1186798
796505
539586
381818
281931
214897
168218
134827
110334

An important observation in this result is that using multi prediction, a very high gaccuracy has been obtained for patterns with length smaller than 5. However, when
we have analyzed the number of predictions made with multi prediction method as a
potential next region we have observed that these numbers are quite high as presented
in Table 6.2.

Table 6.2: Number of Average Total Predictions Per Instance for Different Pattern
Lengths
Pattern Length
2
3
4

Average Total Prediction Count
59.7937065534
11.8247757538
6.91793091885

When the total number of regions, which is 100 in our case, are considered, the number of predictions obtained from multi prediction method is not practical and useful
for real cases. For example, for length 2, the size of the prediction is almost 60 on
average. This explains the superficially high g-accuracy values for patterns shorter
than five.
33

Minimum Support

In this set of experiments, we analyze the effect of change in minimum support threshold on the prediction g-accuracy. For this set of experiments, time tolerance is 75
minutes for 15 minute time interval length, pattern length is 6, cluster count is 100,
and multi prediction limit is 1.0 which means use all frequent patterns matching with
test set patterns.

Figure 6.2: Minimum Support vs g-Accuracy

As it can be seen from Figure 6.2, when minimum support threshold value increases,
prediction g-accuracy drops. This is due to the fact that as minimum support threshold
increases, the number of generated frequent patterns decreases.
The most remarkable result that we found in this analysis is the ratio of the number
of the patterns (any length n) that have the same region id for nth and (n-1)th time
interval to the number of all patterns. It holds for almost 80% of patterns having
lengths greater than 4. This causes prediction for test set pattern to be the last element
of the matching key in frequent pattern, in other words causes to predict one person’s
next location as the current location for 80% of the test data. Since our first motivation
was change of location problem, we did not try to evolve this method and do not
present further results of this method in this work.
34

6.2.2

Results for Next Location Change Prediction using Spatial Data

In this subsection, the effect of the pattern length, minimum support, length tolerance
and multi prediction limit on the success of the prediction are experimentally analyzed
for our second method which aims to predict the change of the location of the users.

Pattern Length
In this part, we analyze the effect of the pattern length on prediction in terms of
accuracy, precision and recall. In this set of experiments, multi prediction limit is 0.8,
the length tolerance is 2, cluster count is 100, and the minimum support is 4 ∗ 10−7 .

Figure 6.3: Pattern Length vs g-Accuracy

As it can be seen from Figure 6.3, when the pattern length increases, prediction gaccuracy drops. It is because of the decreasing number of frequent patterns as the
pattern length increases. We did not include patterns shorter than 5 since for patterns
with length 4, multi prediction method generates 7 alternatives on average. For pattern length 5, our method with multi prediction limit 0.8 generated 2.3 predictions on
average for successful prediction, which is reasonable value for number of generated
predictions.
35

Figure 6.4: Pattern Length vs p-Accuracy

Figure 6.4 shows the relationship between pattern length and p-accuracy. Although it
does not present a regular behaviour compared to that of Figure 6.3, it is an expected
result. Since p-accuracy is the ratio of true predictions to the number of predictions
made (instead of the total number of test patterns), it is expected not to almost linear
decline when pattern length increases. Reason of the lower g-accuracies of higher
pattern lengths in the Figure 6.3 is non-predicted instances in test data. However, in
the Figure 6.4, we do not include non-predicted patterns in p-accuracy. Prediction
count also affects it which can be seen in 6.5. When the quick reduction of prediction
count finished at the pattern length 7, p-accuracy starts to increase. It is expected to
have greater p-accuracy for the longer patterns with nearly same prediction count.
36

Figure 6.5: Pattern Length vs Prediction Count

Figure 6.6: Precision vs Recall

As it can be seen from Figure 6.6, both precision and recall values increase as pattern lengths increase from five to twelve. They both increase because the number of
true positives grow more than both false positives and false negatives. Reason of getting much larger values of recall than precision is the bias of false positives to false
negatives.
37

Minimum Support

In this set of experiments, we analyze the effect of minimum support on the prediction g-accuracy. In the experiments, pattern length is 5, length tolerance is 2, multi
prediction limit is set to 0.8 and cluster count is 100.

Figure 6.7: Minimum Support vs g-Accuracy

As it can be seen from Figure 6.7, when minimum support value increases, prediction
g-accuracy drops as in our first method. Similarly, this is due to that fact that as the
minimum support increases, the number of generated frequent patterns decreases.
When compared to the first method’s minimum support vs. g-accuracy graphics, it
can be seen that g-accuracy values are much higher in the second method. There
are two reasons for it; length tolerance and eliminating successively repetitive region
ids. Length tolerance gives us ability to search test set pattern throughout different
lengths of frequent patterns. Eliminating repetitive region ids gives us less variety
in frequent patterns. These factors reduced the number of non-predicted patterns as
expected (from 2,214,700 to 1,237,313), and incremented true and false predictions
biased to true predictions.
38

Figure 6.8: Minimum Support vs p-Accuracy

As can be seen in the Figure 6.8, when minimum support value increases, p-accuracy
also increases. Since p-accuracy is the prediction accuracy in the predicted test sequences it increases when frequent patterns with higher support values are used. Frequent patterns with lower support values increase makes number of false predictions
increase eventually decrease the p-accuracy.

Figure 6.9: Minimum Support vs Prediction Count
39

As can be seen in the Figure 6.9, minimum support value does not affect the number
of predictions made for one correctly predicted test sequence. However it is expected
to decrease while minimum support value increases. The reason for stable prediction
count is the multi prediction limit value. Only for this experiment multi prediction
limit is set to 0.5 to compare the effects of minimum support and multi prediction limit
on prediction count. This experiment show that multi prediction limit outweighs the
effect of minimum support on prediction count. To see the effect of only minimum
support on prediction it can be referred to Figure 6.18 where multi prediction limit is
set to 0.8.

Length Tolerance
In this set of experiments, we analyze the effect of length tolerance on the g-accuracy,
precision and recall performance of the prediction. In the experiments, multi prediction limit is 0.5 and pattern length is 7. As given in Table 6.3, g-accuracy values are
lower than previous algorithm, since minimum support used in this set of experiments
is 0.0001 for the sake of execution time.
Table 6.3: Length Tolerance vs g-Accuracy

Length Tolerance
0
1
2

g-Accuracy
0.199340297199
0.234839334017
0.289309194395

As it can be seen in the table, when the length tolerance increases, prediction gaccuracy also increases. G-accuracy values increase since tolerating length feature
provides the opportunity to look up different frequent patterns with different lengths
for non-predicted test set patterns.
As seen in Table 6.4, precision values decrease when length tolerance increases. It
is due to the fact that extra frequent patterns are traversed that have different lengths
for non-predicted test set patterns. This increases the number of false positives much
more than that of true positives, since the prediction set for one test instance gets
larger for higher length tolerance values.
40

Table 6.4: Length Tolerance vs Precision and Recall

Length Tolerance
0
1
2

Precision
0.873208275761
0.793619440058
0.537772714164

Recall
0.878258665055
0.896896539759
0.913341913657

Multi Prediction Limit

In this set of experiments, we analyze the effect of multi prediction limit on the accuracy. In the experiments, length tolerance is 2, pattern length is 5 and minimum
support is 4e-7.

Figure 6.10: Multi Prediction Limit vs g-Accuracy

As it can be seen from Figure 6.10, when multi prediction limit increases, prediction
g-accuracy also increases. It is due to the fact that it enlarges the prediction set for
each test set pattern, although number of non-predicted test set patterns remains same.
Since prediction set increases, the number of false predictions that were made with
lower multi prediction limit decreases and the number of true predictions increases,
when multi prediction limit increases.
41

Figure 6.11: Multi Prediction Limit vs p-Accuracy

As it can be seen from Figure 6.11, when multi prediction limit increases, prediction
p-accuracy also increases. The important thing in Figure 6.11 is the identicalness
of the curve, without taken accuracy values into consideration. This visualize that
although prediction accuracy values increase, the number of non-predicted test set
patterns remain same.

Figure 6.12: Multi Prediction Limit vs Prediction Count

In the Figure 6.12 it can be seen that when multi prediction limit increases, prediction
42

count also increases. It is the expected behaviour by definition since multi prediction
limit is introduced to limit the prediction count for the prediction of one test sequence.

6.2.3

Results for Next Location Change and Time Prediction using SpatioTemporal Data

In this subsection the effect of minimum support, multi prediction limit, length tolerance, pattern length, cluster count, time interval length and time tolerance on gaccuracy, p-accuracy and prediction count are experimentally analyzed for our third
method which aims to predict the change of the location and time of the users.

Pattern Length
For this set of experiments, length tolerance is 2, time interval length is 60, time
tolerance is 120, multi prediction limit is 0.8, cluster count is 100, and minimum
support is 4e-7.

Figure 6.13: Pattern Length vs g-Accuracy
43

Figure 6.14: Pattern Length vs p-Accuracy

As it can be seen from Figure 6.13, when pattern length increases, g-accuracy decreases. Since it is much harder to find frequent patterns for longer patterns, gaccuracy eventually decreases. However as can ben seen in Figure 6.14 p-accuracy do
not have a continuous increase or decrease when pattern length increases. The reason
for this different behavior is presented in the previous section.

Figure 6.15: Pattern Length vs Prediction Count
44

Minimum Support

For this set of experiments, pattern length is 5, length tolerance is 2, time interval
length is 60, time tolerance is 120, cluster count is 100, and multi prediction limit is
0.8.

Figure 6.16: Minimum Support vs g-Accuracy

As it can be seen from Figure 6.16, when minimum support value increases, gaccuracy decreases. It is an expected behaviour to have smaller g-accuracy values for
the greater minimum support values since the greater minimum support value means
the less frequent pattern which eventually causes the predicted sequence’s number
superficially drop. Consequently the g-accuracy decreases.
45

Figure 6.17: Minimum Support vs p-Accuracy

As it can be seen from Figure 6.17, when minimum support value increases, paccuracy decreases. When compared to the effect on g-accuracy, reduction in paccuracy is much less than it. The reason is related with the definition of p-accuracy.
P-accuracy does not take unpredicted sequences into consideration. However still
there is a small decline in the graph, since the extracted frequent patterns are much
lower for the greater minimum support values.

Figure 6.18: Minimum Support vs Prediction Count
46

As it can be seen from Figure 6.18, when minimum support value increases, prediction count decreases. The reason is the same with the previous two graphs, lower
number of extracted frequent patterns.

47

Length Tolerance
For this set of experiments, pattern length is 5, time interval length is 60, time tolerance is 120, multi prediction limit is 0.8, cluster count is 100, and minimum support
is 4e-7.

Figure 6.19: Length Tolerance vs g-Accuracy

Figure 6.20: Length Tolerance vs p-Accuracy

As it can be seen from Figure 6.19 and 6.20, when length tolerance increases g48

accuracy and p-accuracy increases. Increasing length tolerance makes some unpredicted test sequences predictable which increases the g-accuracy. True predicted with
greater length tolerance sequences also increases the p-accuracy although its increase
is much lower than the g-accuracy. Moreover, as it can be seen from 6.21 more length
tolerance makes prediction sets larger.

Figure 6.21: Length Tolerance vs Prediction Count

Multi Prediction Limit

For this set of experiments, pattern length is 5, length tolerance is 2, time interval
length is 60, time tolerance is 120, cluster count is 100, and minimum support is
4e-7.
49

Figure 6.22: Multi Prediction Limit vs g-Accuracy

Figure 6.23: Multi Prediction Limit vs p-Accuracy

As it can be seen from the 6.22 and 6.23, when multi prediction limit increases gaccuracy and p-accuracy increase. The greater multi prediction limit means the larger
prediction set for one test sequence. Therefore it increases the value of the both of
the accuracy metrics. In addition to it, prediction count increases by definition which
can be seen in 6.24.
50

Figure 6.24: Multi Prediction Limit vs Prediction Count

Cluster Count

For this set of experiments, pattern length is 5, length tolerance is 2, time interval
length is 60, time tolerance is 120, multi prediction limit is 0.8 and minimum support
is 4e-7.

Figure 6.25: Cluster Count vs g-Accuracy
51

Figure 6.26: Cluster Count vs p-Accuracy

As it can be seen from Figure 6.25, when cluster count increases g-accuracy decreases
slightly. It is because of the unpredicted test sequences rather than false predictions
since increasing cluster count makes frequent patterns harder to extract. However
as it can be seen Figure 6.26, p-accuracy increases when cluster count increases. It
is because of the fact that when cluster count increases movement patterns of users
can be defined more precisely which makes frequent patterns harder to find but more
accurate ones. Therefore, they usually give correct predictions when compared to the
less cluster counts. It also eventually decrease the size of prediction set in other words
prediction count which can be seen in Figure 6.27. It should also be noted that for
this analysis we used multi prediction limit as 0.5 rather than 0.8.
52

Figure 6.27: Cluster Count vs Prediction Count

Time Interval Length
For this set of experiments, pattern length is 5, length tolerance is 2, time tolerance is
0, multi prediction limit is 0.8, cluster count is 100, and minimum support is 4e-7.

Figure 6.28: Time Interval Length vs g-Accuracy

As it can be seen from the Figure 6.28 and 6.29, when time interval length increases,
53

g-accuracy and p-accuracy increase. Since the larger time interval means the more
similar daily sequences and eventually higher number of frequent patterns, increase in
the values accuracy metrics is an expected behavior. We can say that prediction count
increases in general while the time interval length increases although for time interval
length 360 it decreases, but it is a negligible. As can ben seen in 6.30 the reason for
increase in the size of the prediction set is the same reason for the g-accuracy and
p-accuracy increase; higher number of frequent patterns.

Figure 6.29: Time Interval Length vs p-Accuracy
54

Figure 6.30: Time Interval Length vs Prediction Count

Time Tolerance
For this set of experiments, pattern length is 5, length tolerance is 2, time interval
length is 60, multi prediction limit is 0.8, cluster count is 100, and minimum support
is 4e-7.

Figure 6.31: Time Tolerance vs g-Accuracy
55

As it can be seen from Figure 6.31 and 6.32, when time tolerance increases g-accuracy
and p-accuracy increases. It is expected since the greater time tolerance gives prediction model ability to search for different time intervals when it can not create prediction for a fixed time interval sequences or can not predict true region id and time
interval.

Figure 6.32: Time Tolerance vs p-Accuracy

Figure 6.33: Time Tolerance vs Prediction Count

As it can be seen from Figure 6.33, when time tolerance increases prediction count
56

decreases. It is because prediction count represents the size of the prediction set
when it only gives correct prediction. Since the correct predictions increase while
unpredicted sequences decrease, prediction count decreases.

57

58

CHAPTER 7

DISCUSSION AND CONCLUSION

In this work, we applied sequence pattern mining techniques for location prediction
problem domain. We used one of the largest mobile phone operator companies’ CDR
data. We focused on three different subproblems in the location prediction problem
space namely, next location and time prediction using spatio-temporal data, next location change prediction using spatial data, next location change and time prediction using spatio-temporal data. The main novelties are time prediction and spatio-temporal
alignments for the prediction task. In experiments, we have evaluated our model’s
prediction quality with respect to g-accuracy, p-accuracy and prediction count and
further analyzed the effects of change of minimum support, multi prediction limit,
length tolerance, pattern length, cluster count, time interval length and time tolerance on prediction accuracies and count. Here are the some basic findings and most
valuable prediction results for these three methods;
• For the spatio-temporal next location prediction, it does not make sense to
present the results below or around 80% accuracy since 80% of the user’s next
location is their current location.
• For the spatial next location change prediction g-accuracies differ between 48%
and 84% for the prediction counts 2.4 and 14 for 100 regions while p-accuracies
differ between 74% and 99% for the same prediction counts. These values show
that our proposed model for this problem can generate successful accuracy values with acceptable prediction counts.
• For the spatio-temporal next location change and time prediction while it predicts nearly half of the test sequences, p-accuracies reach up to 93% for 14
59

prediction count for possible 9600 ([24 x 1 hour time interval] x 400 clusters) spatio-temporal prediction combination. Moreover it generates 87% paccuracy for 3.44 prediction count for possible 153600 ([24 x 1 hour time interval] x 6400 clusters) prediction combination.
As a future work, we plan to enlarge our problem space we focoused with the followings; next location change prediction using spatio-temporal data, next action time
prediction using temporal data, location and time prediction of the next action using
spatio-temporal data.

60

REFERENCES

[1] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules in large databases. In Jorge B. Bocca, Matthias Jarke, and Carlo
Zaniolo, editors, VLDB’94, Proceedings of 20th International Conference on
Very Large Data Bases, September 12-15, 1994, Santiago de Chile, Chile, pages
487–499. Morgan Kaufmann, 1994.
[2] Rakesh Agrawal and Ramakrishnan Srikant. Mining sequential patterns. In
Philip S. Yu and Arbee L. P. Chen, editors, ICDE, pages 3–14. IEEE Computer
Society, 1995.
[3] Chiara Boldrini and Andrea Passarella. Hcmm: Modelling spatial and temporal
properties of human mobility driven by users’ social relationships. Computer
Communications, 33(9):1056–1074, June 2010.
[4] Huiping Cao, Nikos Mamoulis, and David W. Cheung. Discovery of periodic
patterns in spatiotemporal sequences. IEEE Trans. on Knowl. and Data Eng.,
19(4):453–467, April 2007.
[5] Eunjoon Cho, Seth A. Myers, and Jure Leskovec. Friendship and mobility: user
movement in location-based social networks. In KDD ’11 Proceedings of the
17th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 1082–1090. ACM, 2011.
[6] Yves-Alexandre de Montjoye, César A. Hidalgo, Michel Verleysen, and Vincent D. Blondel. Unique in the crowd: The privacy bounds of human mobility.
Scientific Reports, 3(1376), March 2013.
[7] Huiji Gao, Jiliang Tang, and Huan Liu. Mobile location prediction in spatiotemporal context. In the Procedings of Mobile Data Challenge by Nokia Workshop at the Tenth International Conference on Pervasive Computing. Nokia,
June 2012.
[8] Fosca Giannotti, Mirco Nanni, Fabio Pinelli, and Dino Pedreschi. Trajectory
pattern mining. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07, pages 330–339,
New York, NY, USA, 2007. ACM.
[9] Győző Gidófalvi and Fang Dong. When and where next: individual mobility
prediction. In Proceedings of the First ACM SIGSPATIAL International Work61

shop on Mobile Geographic Information Systems, MobiGIS ’12, pages 57–64,
New York, NY, USA, 2012. ACM.
[10] Jiawei Han. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2005.
[11] John A. Hartigan. Clustering Algorithms. John Wiley & Sons, Inc., New York,
NY, USA, 99th edition, 1975.
[12] J. Macqueen. Some methods for classification and analysis of multivariate observations. In In 5-th Berkeley Symposium on Mathematical Statistics and Probability, pages 281–297, 1967.
[13] Mert Ozer, Ilkcan Keles, İsmail Hakki Toroslu, and Pinar Karagoz. Predicting
the change of location of mobile phone users. In Proceedings of the Second
ACM SIGSPATIAL International Workshop on Mobile Geographic Information
Systems, MobiGIS ’13, pages 43–50, New York, NY, USA, 2013. ACM.
[14] Shashi Shekhar, Michael R. Evans, James M. Kang, and Pradeep Mohan. Identifying patterns in spatial information: A survey of methods. Wiley Interdisc.
Rew.: Data Mining and Knowledge Discovery, 1(3):193–214, 2011.
[15] Chaoming Song, Zehui Qu, Nicholas Blumm, and Albert-László Barabási.
Limits of predictability in human mobility. Science, 327(5968):1018–1021,
2010.
[16] Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. Introduction to Data
Mining, (First Edition). Addison-Wesley Longman Publishing Co., Inc.,
Boston, MA, USA, 2005.
[17] Nguyen Thanh and Tu Minh Phuong. A gaussian mixture model for mobile
location prediction. The 9th International Conference on Advanced Communication Technology, 2(9):914 – 919, February 2007.
[18] Xindong Wu, Vipin Kumar, J. Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi Motoda, Geoffrey J. McLachlan, Angus Ng, Bing Liu, Philip S. Yu, ZhiHua Zhou, Michael Steinbach, David J. Hand, and Dan Steinberg. Top 10 algorithms in data mining. Knowl. Inf. Syst., 14(1):1–37, December 2007.
[19] Gökhan Yavas, Dimitrios Katsaros, Özgür Ulusoy, and Yannis Manolopoulos.
A data mining approach for location prediction in mobile environments. Data
Knowl. Eng., 54(2):121–146, August 2005.
[20] Josh Jia-Ching Ying, Wang-Chien Lee, Tz-Chiao Weng, and Vincent S. Tseng.
Semantic trajectory mining for location prediction. In Proceedings of the 19th
ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, GIS ’11, pages 34–43, New York, NY, USA, 2011. ACM.
62

[21] Daqiang Zhang, Athanasios V. Vasilakos, and Haoyi Xiong. Predicting location
using mobile phone calls. SIGCOMM Comput. Commun. Rev., 42(4):295–296,
August 2012.
[22] Yu Zheng, Lizhu Zhang, Xing Xie, and Wei-Ying Ma. Mining interesting locations and travel sequences from gps trajectories. In Proceedings of the 18th
International Conference on World Wide Web, WWW ’09, pages 791–800, New
York, NY, USA, 2009. ACM.

63

Spatio-Temporal Signal Recovery from Political
Tweets in Indonesia
Anisha Mazumder, Arun Das, Nyunsu Kim, Sedat Gokalp, Arunabha Sen, Hasan Davulcu
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, Arizona - 85287
Email: {Anisha.Mazumder, adas22, nkim30, Sedat.Gokalp, asen, hdavulcu}@asu.edu
Abstract—Online social network community now provides an
enormous volume of data for analyzing human sentiment about
people, places, events and political activities. It is increasingly
clear that analysis of such data can provide great insights on the
social, political and cultural aspect of the participants of these
networks. As part of the Minerva project, currently underway
at Arizona State University, we have analyzed a large volume
of Twitter data to understand radical political activity in the
provinces of Indonesia. Based on analysis of radical/counter
radical sentiments expressed in tweets by Twitter users, we
create a Heat Map of Indonesia which visually demonstrates the
degree of radical activities in various provinces of Indonesia.
We create the Heat Map of Indonesia by computing (i) the
Radicalization Index and (ii) the Location Index of each Twitter
user from Indonesia, who has expressed some radical sentiment
in her tweets. The conclusions derived from our analysis matches
significantly with the analysis of Wahid Institute, a leading
political think tank of Indonesia, thus validating our results.
Index Terms—radical, tweet, Radicalization Index, Location
Index, Heat Map

I. I NTRODUCTION
The sheer popularity of online social media nowadays is
reflected by the immense amount of data being fed every
second by people from all over the world. It is becoming
increasingly evident that analysis of this huge online dataset
can provide great insights on the social, political and cultural
aspect of the Twitter users and possibly the non-Twitter users
as well. In [2], the authors have developed Socioscope, a tool
for extracting signal from noisy social media data. Utilizing
a Socioscope like mechanism, we have developed a tool for
recovering spatio-temporal signals from tweets generated in
Indonesia. Our interest in analyzing tweets from Indonesia
developed in the context of the Minerva1 project, currently
underway at Arizona State University. The goal of this project
is to increase the understanding of movements within Muslim
communities towards radicalism or counter radicalism. Based
on the support and opposition of certain beliefs and practices
of an individual (as expressed in her tweet), we can assign a
Radicalization Index to that individual. In addition, from the
self declared home location of a Twitter user and the locations
of her tweets, we can compute a distribution of Location
Index for that user. The map of Indonesia is divided up into
a set of regions and the Location Index of a user provides the
1A

project sponsored by the U.S. Department of Defense

probability of the user to be in a specific region at a specific
time. For this analysis a region corresponds to a province of
Indonesia. Finally, from the Radicalization Index and Location
Index of individuals, Heat Index of a region , which is a
composite measure of the number of radical tweeters of that
region and their ‘degree of radicalism’, is computed.
In our model we have a set of tweeters (or users), U =
{U1 , U2 , . . . , Un }. Each user Ui , 1 ≤ i ≤ n creates a set of
tweets Ti = {Ti,1 , Ti,2 , . . . , Ti,t
Sin}. The set of all tweets by
all users is denoted by T = i=1 Ti . The geographic area
from where the tweets originate is divided into a set of regions
R = {R1 , R2 , . . . , Rm }. In our study m is equal to thirty four,
the number of provinces and special administrative regions of
Indonesia. Each user Ui , 1 ≤ i ≤ n has a home location
HLi , 1 ≤ i ≤ n associated with her, which may or may not
be declared. Each tweet Ti,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti has a
geo-location GLi,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti associated with
it. However, GLi,k for some tweets Ti,k may not be known
as the user Ui might turn her GPS off. Accordingly, we can
divide the set of users in four different classes:
(i) Class 1: user Ui whose home location is declared and
geo-location of at least one tweet is known,
(ii) Class 2: Ui whose home location is not declared and
geo-location of at least one tweet is known,
(iii) Class 3: Ui whose home location is declared and geolocation of none of the tweets are known, and
(iv) Class 4 : Ui whose home location is not declared and
geo-location of none of the tweets are known.
From the input data set (U, T, R ), we compute, (i) Location
Index, Li of each user Ui , 1 ≤ i ≤ n, (ii) Radicalization Index,
RDi of each user Ui , 1 ≤ i ≤ n, and finally, combining Li
and RDi , we compute (iii) Heat Index, Hj of each region
Rj , 1 ≤ j ≤ m. It may be noted that whereas RDi , 1 ≤ i ≤ n
is a scalar value, Li is a vector of size m, (Li,1 , . . . , Li,m ),
where Li,j indicates the probability of user Ui being located
in region Rj i.e. Li,j indicates the probability of the Actual
home location of Ui being Rj . Finally, the HeatPIndex Hj of
n
region Rj , 1 ≤ j ≤ m is computed as Hj = i=1 RDi ×
Li,j , ∀j, 1 ≤ j ≤ m. We thus provide a generic technique for
generating time-varying political Heat Maps of a geographical
region based on the Twitter data analysis. Throughout this
paper we have used ‘region ’ and ‘location’ interchangeably

to mean an ‘Indonesian Province’. It is to be noted that for
our calculations, we have considered all Indonesian provinces
including special administrative regions such as Yogyakarta
and special capital region such as Jakarta.
II. R ELATED W ORK
Computation of Heat Map of Indonesia requires the computation of the following: First, we compute the Radicalization
Index of a user Ui by analyzing the content of her tweets.
Second, Location Index of the user Ui is computed from her
geo-location containing tweets (if any) and also from her home
location declared as a part of her Twitter profile (if at all
provided). It is to be noted that we do not consider users
who have neither of these two sources of location information
present.
Identification of the location of users using Twitter data
has been quite a focus of recent research. Inferring location
from tweets have been pursued by [14], [15], [16]. Studies
conducted in [4], [5], [6], [7] combine location information
and text from social-network data history to infer various questions such as user preferences and provide recommendations.
However, we do not rely on any ‘checking in’ information for
our computations and providing recommendations is not our
goal.
We do employ the notion of regions - the thirty four
provinces of Indonesia are the regions of interest for our problem. Thus, ‘geo-coding’ (the use of gazetteers) is applicable to
our problem. However, just as in [3], we too argue that location
estimates are multi-modal probability distributions, rather than
particular points or regions. However, it may be noted that in
contrast to [3], we are interested only in Indonesia and in
Indonesian provinces - thus our estimate of the location of the
user must be the probability of each Indonesian province as the
Actual home location of the user under consideration, rather
than the probability of the user being located in each and every
point on the surface of the earth. This implies that our world
comprises of Indonesia only and individual geo-co-ordinates
are bunched into the corresponding province of Indonesia. As
a result, we apply the combination of ‘geocoding’ and the
modification of the techniques in [3]. Thus, we use gazetteers
for the Declared Home Location of the Twitter Users to map
those to a specific province of Indonesia (This is explained in
further details in Section VII ). This combined with the geo-coordinate information about the user (obtained from her tweets
containing geo-location) gives us the probability distribution
of the user across the thirty four provinces of Indonesia. We
thus obtain a simple yet effective means of computing the
geo-location of the user as compared to other more complex
methodologies such as Topic Detection Techniques [20], [21],
[22].
Human mobility is modeled as a stochastic process in [8].
Following the studies of [8], in [1], the authors study the
manner in which the movements of human beings are related
to time of the day, geography as well as social ties. They intend
to predict the exact location of a person based on various
factors which the authors have identified, including impact of

social network. Similar problems have been studied by [9],
[10], [11]. However, in our problem, there is no notion of
prediction of location of users involved. Besides, we consider
categorical distribution. However, we do use the concept of
mixture of distributions in the lines of [1].
Another line of research which focuses on location estimation by content-analysis of the tweets of a user has been
studied by [12], [13]. They use the techniques of feature
selection from tweets of users, following it up with training
and classification. However, we do not apply content based
analysis in this current work, but rely on the geo-location
containing tweets of users in our dataset and also the user
declared home location to obtain the location distribution of
the users.
In [17], the authors analyze tweets generated during the
United Kingdom 2010 General Election to measure political
sentiments as well. They have identified the specific features
of the political parties and their ultimate goal is to infer
the political affiliation of a user based on her tweets. We
also study a similar problem, however our goal is not to
identify the political affiliations of users, rather we compute
the ‘degree of radicalism’ of the user. Besides, our technique
is completely different from theirs. Unlike them, we apply a
very simple yet effective term-frequency analysis of tweets and
leverage heavily on our team of domain experts. We validated
our classification of users into radicals and counter radicals
by classifying some well-known counter radical leaders of
Indonesia (Our validation process is discussed in further details
in Section IX).
The work in [35] which is followed by [18] is very relevant
to our technique of Radicalization Index assignment to users.
These works too deal with the recovery of radical signals
from the online posts of social media users and thereby
identify individuals as potential ‘lone wolf terrorists’. They
specifically focus on presenting a framework for combining
entity matching techniques for detecting extremist behavior
on discussion boards. These ‘lone wolf terrorists’ might leave
weak signals of radicalism through their comments or posts
on discussion boards, signals made further weaker by the use
of aliases. Identification and analysis of such weak signals
of radicalism by the use of topic-filtered web harvesting as
well as application of natural language processing techniques,
thereby fusing aliases for identifying the person form the basis
of the works of [35]. Their work is fundamentally different
from ours because we deal specifically with the users’ publicly
available tweets only - this eliminates the availability of the
vital background information such as characteristic ( ‘radical
internet forum’, ‘capability internet forum’ ) annotation of particular discussion boards that is leveraged in [35] . However,
we also have used the technique of application of keyword
analysis and crawling of web-sites of well-known radical/
counter radical organizations of Indonesia as discussed in later
sections. Furthermore, [35] and [18] do not deal with location
profiling of users which is one of the two major goals of our
work.

Fig. 1: The flow diagram of our Heat Map computation technique. The Web data mentioned here refers to the documents
generated by crawling the web pages of radical and counter radical organizations of Indonesia.

III. M OTIVATION AND D ISTINGUISHING F EATURES OF
OUR WORK

The motivation for our work is to provide a visualization
of the spatio-temporal distribution of the radical population of
Indonesia by recovering political signals from Twitter data.
A pictorial description of our methodology is provided in
Figure 1. Similar retrieval of signals using Twitter data is the
motivation of the work of [2]. In [2], they find the location
distribution of tweets mentioning roadkills using human beings
as sensors. Similarity of our work with [2] is that we too use
human beings as sensors to the extent that we use tweets of
people of Indonesia to infer radicalism Heat Indices of the
provinces of Indonesia. However, our work is significantly
different from theirs. First, unlike [2], we intend to find the
distribution of (radical) individuals, so we should not factor
in any ‘human population bias’ i.e variation of densities of
people across the different provinces of Indonesia. Second,
our problem is much more complex because we not only
need to know from which location have the radical tweets
come in greater number, but also the ‘degree of radicalism’ of
the tweets - so we need to comprehend the sentiment of the
tweets. The major difference here is that in our case, we need
a finer grained distinction among the radical tweets specifying
which tweets are more radical and which tweets are less. So,
questions of interest for us are(Qs1) the ‘degree of radicalism’ of tweet tw
(Qs2) the originating location of tweet tw
Thus, Heat Index of a region factors in both the count of
the radical tweets from the region as well as the ‘degree of
radicalism’ of the tweets. However, there are certain challenges
in answering these questions. As for Qs1, a tweet can at most
be 140 characters long. This is indeed too little information
to ascertain the ‘degree of radicalism’ of tweets on individual
basis. Thus, we go one level up the hierarchy and consider
individual users instead of individual tweets and try to answer
the two questions in the context of individual users. We collect
all the tweets from individual users and assign the ‘degree of
radicalism’ to the user based on her tweets. Now, Qs2 would
have been easy to answer with respect to individual tweets if
all the tweets had geo-co-ordinate information because Twitter
API2 provides geo-location information of tweets if the user
2 https://dev.twitter.com/docs/streaming-apis
and
ter.com/docs/platform-objects/tweets have been used

https://dev.twit-

had chosen to reveal her location at the time of tweeting.
However, there are certain problems with this approach - first,
the percentage of tweets containing geo-location information
is very scarce (such tweets constitute less than 1% of our
dataset). Second, when we consider individual users, it is unjustified to assume that all her tweets containing geo-location
information will point to a single region, even if all her tweets
contained geo-location information. Thus, the best estimate of
the location of the user is the probability distribution of the
user’s location over the Indonesian provinces.
We consider categorical distribution of the users into the
thirty four provinces of Indonesia. The motivation behind
employing categorical distribution instead of say Gaussian
distribution over the entire landscape of Indonesia is that we
want to obtain a political Heat Map of Indonesia with the
granularity level of a province. Another possibility, that is
feasible however not pursued by us in this current work, is
dividing Indonesia in the form of grids with varied granularity.
Finer granularity poses the problem of insufficient data from
every grid, because, as mentioned previously, most tweets
do not contain geo-location information. So, most grids will
have no geo-location containing tweet. Our technique of
Location Index computation is discussed in further details in
the following section.
Sentiment Analysis using social media data has been attempted by works such as [31] which tries to exploit patterns in online social media communication and also by [32]
which uses background lexical information and refining of the
same for specific domains by supervised learning techniques.
However, we have computed Radicalization Indices using
simpler text regression techniques similar to [33] and [34].
Our technique of Radicalization Index computation, which is
verified to be quite accurate is discussed in further details in
Section V.
In summary, individual Twitter users are our chosen level of
granularity - we obtain all the necessary information pertaining
to each user. Next we characterize the user based on those
information, not only on the radicalization scale but we also
obtain a location distribution of the user over the regions of
Indonesia. Hence, there is no prediction of the location of
the user involved as in [1]. It is to be noted that we consider
only the users classified as radical by our Radicalization Index
computation method. Hence, our major contribution is the

development of a robust technique to obtain the political Heat
Map of any geographic area.
IV. L OCATION I NDEX C OMPUTATION
As discussed earlier, each user Ui , 1 ≤ i ≤ n has a home
location HLi , 1 ≤ i ≤ n associated with her, which may or
may not be declared. Each tweet Ti,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti
has a geo-location GLi,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti associated
with it. However, GLi,k for some tweets Ti,k may not be
known as the user Ui might turn her GPS off. Even when
user Ui has a Declared Home Location DHLi , it may not be
accurate. User Ui might intentionally or inadvertently misstate
her location. Accordingly, we do not accept the DHLi at its
face value as the Actual home location of Ui . Instead, we
compute a matrix, which we term as the general Computed
Home Location matrix gCHL, from the entire dataset barring
the timespan (month in our case) for which the Heat Map
is being generated. The created matrix gCHL is an m × m
matrix where gCHLa,b , 1 ≤ a ≤ m, 1 ≤ b ≤ m, is the
conditional probability of the Actual home location of a user
being region Rb , when her Declared Home Location is region
Ra , as learnt from the dataset. The gCHL matrix is computed
using the following three steps provided in Algorithm 1. Thus,
gCHLa,b is given by:
gCHLa,b = X
Y
where,
X = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra and geo-location of
the tweet is Rb
Y = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra
Let the ath row of the gCHL matrix be denoted by gCHLa .
Now Computed Home Location vector for the user Ui denoted
by CHLi is assigned the value of gCHLa if the Declared
Home Location of Ui is region Ra . It is to be noted that
the gCHL matrix is general (and not user specific) and is
computed using the entire Twitter data set comprising all users.
From those tweets Ti,k , 1 ≤ k ≤ ti of user Ui , that contain
the geo-location information GLi,k (i.e., when the GPS is not
turned off at the time of the tweet), we compute the Computed
Geo Location vector CGLi of length m, where CGLi,j , 1 ≤
j ≤ m, is the probability of the Actual home location of
user Ui being region Rj , as learnt from the tweets of Ui . The
CGLi,j is computed in the following way:
A
CGLi,j = B

where,
A = The number of tweets in Ti whose geo-location is
Rj and
B = The number of tweets in Ti whose geo-location is
known
We thus obtain two pieces of information about the Actual
home location of the user Ui in the form of two distributions:

CHLi and CGLi , where CGLi is completely user-specific.
However, CHLi is partially user-specific - it does depend
on the user because CHLi is based on her Declared Home
Location, but it also depends on the general distribution which
depends on the entire population mass. It is evident that both
CHLi and CGLi are categorical distribution over the thirty
four Indonesian provinces. Now, we know that a mixture of
discrete distributions over any finite number of categories is
just another distribution over those categories. In order to
combine CGLi and CHLi we obtain a convex combination
of the two to obtain Li,j in the following way:
Li,j = (1 − ωi ) ∗ CHLi,j + ωi ∗ CGLi,j

(1)

Now, the mixture weights ωi for the user Ui is learnt from
the data itself and is calculated as ωi = |Ti0 |/|Ti |.
Li,j essentially is given by
00

0

Li,j = |Ti | ∗ CHLi,j + |Ti | ∗ CGLi,j

(2)
0

which gives equation (1) when normalized by |Ti | = |Ti | +
00
|Ti | i.e the total number of tweets posted by the user Ui
where,
• Ti = set of tweets produced by user Ui
0
• Ti = subset of Ti and represents the set of tweets by Ui
that contains geo-location information
00
• Ti = subset of Ti and represents the set of tweets by Ui
that do not contain geo-location information
The motivation behind this definition of the mixture weight is
0
that for the Ti tweets which contain geo-location information,
we consider the user-specific location distribution information
inferred from the particular user’s geo-location containing
00
tweets. However, for the tweets of Ti , we have no location
information except for the general information that given a
Declared Home Location for any user Uv in our dataset as
Ra , the location distribution for Uv is CHLv = gCHLa .
Thus, if the Declared Home Location of Ui is given to be
Ra , we consider CHLi = gCHLa . Evidently, we depend
on this semi-user-specific location distribution information for
00
the tweets Ti of Ui . This simple formulation of Li,j also
captures the fact that we rely more on CGLi than on CHLi
when the number of tweets with geo-location information,
generated by Ui is high, however if that count is low ( or even
absent), instead of discarding the particular user’s information,
we obtain the location distribution of Ui from her Declared
Home Location. We experimented by using only geo-location
containing tweets and we saw that the results are far more
accurate if we included users of Type 3 - This is intuitively
correct because the geo-location containing tweets form less
than 1% of the entire dataset.
As noted earlier, the set of users can be divided into four
different classes. We do not try to compute Li,j values for the
users belonging to Class 4. For users belonging to the other
three classes, we compute Li,j using equation (1). For the
users belonging to Class 3, we obtain ωi to be zero, as we
do not have any geo-location data from the tweets to compute
ωi .

Algorithm 1 Counting Algorithm for computation of the general Computed Home Location gCHL
•
•
•

Step 1: Initialize gCHLa,b = 0, 1 ≤ a ≤ m, 1 ≤ b ≤ m
Step 2: For each tweet tw in T , increment gCHLa,b if Declared home location of the author of tw and the geo-location
of tw are Ra and Rb respectively
Step 3: Make each row gCHLa of gCHL matrix row stochastic, 1 ≤ a ≤ m

V. R ADICALIZATION I NDEX C OMPUTATION
We intend to assign a Radicalization Index RDi to a Twitter
user Ui based on the content of her tweets. Each tweet can
contain up to 140 characters. Thus the content of a single
tweet does not provide adequate information regarding the
user’s ideology. We collect tweets from users over a period
of time (in our case a month) and for each user Ui we create
a document Di that contains all the tweets Ti of that user,
during that period of time. As there exists a one-to-one correspondence between Ui and Di , by assigning a Radicalization
Index to Di , we essentially assign a Radicalization Index RDi
to Ui . Classical predictive model Multiple Linear regression
[23], [24], [25] fits our application, since it is a dichotomous
classification problem with multiple predictor variables, where
the predictor variables are the terms of our “vocabulary”.
Classical classification methods such as Logistic Regression
which has applications in a wide variety of domains can
also be used for document classification [26]. Thus, Logistic
Regression can also be applied for our problem. However,
Linear Regression was selected instead of Logistic Regression
because it out-performed the Logistic one through 10-fold
cross validation. This well-known technique divides the given
dataset into 10 segments and then uses 90% of the data ( i.e
nine segments) as training data and 10% of the data ( i.e. one
segment ) as the test data. Linear Regression showed around
98% of accuracy, but Logistic Regression showed 83-85%
of accuracy. The implementation of our approach proceeds
in the following way: First, we identify a set of Indonesian
political organizations. Next, social scientists in our Minerva
team, who are domain experts for Indonesia, hypothesize a
classification to label each organization as radical or counter
radical based on these organizations beliefs and practices.
Using web crawling tools, we download a large number of
documents from the web sites of these organizations. We
use the term “vocabulary” to mean the set of all unique
terms that appear in all documents from all organizations.
All the documents of an organization are assigned the same
Radicalization Index as the Radicalization Index assigned to
the organization by the domain experts in our team. This set of
documents together with their Radicalization Indices form the
training dataset for our model. After that we use the model to
assign a Radicalization Index to the document Di created from
the tweets of user Ui . This Radicalization Index of document
Di is taken to be the Radicalization Index of user Ui .
A. Problem Formulation:
We formulate the problem in a general sparse learning
framework and solve the following optimization problem (3)

using the techniques from [27] . This is indeed a sparse learning problem because the vocabulary is very large compared to
the number of words used in a document.
1
ρ
2
2
min kAx − yk2 + kxk2 + λ kxk1
x 2
2
where A ∈ Rs×p , y ∈ Rs×1 , and x ∈ Rp×1

(3)

In our application, we have
• A is Document × Term matrix which is constructed as
follows: The set of terms (t1 , . . . tp ) includes all the terms
from all the documents by all the organizations, barring
the stop words. The size of the vocabulary in this case is
p.
If data is collected by crawling web sites of different organization (O1 , . . . , Oq ) and documents (di,1 , . . . , d1,ri ) are
collected from the web site of organization Oi , 1 ≤ i ≤ q,
the total number of rows of the matrix A is
s = Σqi=1 ri ,
and it has the following structure.
Document/T erm
d1
d2
....
ds
•
•

•

t1
....
....
....
....

t2
....
....
....
....

....
....
....
....
....

tp
....
....
....
....

Aij = term f requency of the j th term in the ith
document such that Aij ≥ 0, 1 ≤ i ≤ s, 1 ≤ j ≤ p.
yi ∈ {+1, −1} is the class of each document Di , 1 ≤
i ≤ s. As indicated earlier, the Radicalization Index of
a document is the same the Radicalization Index of the
organization that created that document. Thus, when an
organization is labeled as radical (or counter radical) by
the domain experts, all the documents pertaining to that
organization is marked as +1 (or −1). Thus yi = +1
if Di , 1 ≤ i ≤ s belongs to an organization marked as
radical by the experts, or yi = −1 if Di , 1 ≤ i ≤ s
belongs to an organization marked as counter radical by
the experts.
xj is the weight for each term tj , 1 ≤ j ≤ p. This is the
parameter estimated by optimizing the objective function
(3). The xj ’s thus form the predictor variables of the
model.

Let us further clarify the three terms involved in the convex
optimization problem:
•

1
2

2

kAx − yk2 - this first term is related to the sum of
the squared errors to fit a straight line to a set of data

points. The objective function (3) thus is the optimization
problem of minimizing this sum of squared-errors.
2
ρ
• 2 kxk2 - this term deals with the ridge regression, which
is an extra level of shrinkage. We set ρ = 0 as we were
mainly driven by sparsity.
• λ kxk1 - this term involving the L1 norm deals with the
sparsity of the solution vector x. For different values of
λ we obtain a solution vector x which represents the
weights associated with each term tj , 1 ≤ j ≤ p ( the
same terms which are considered in the A matrix). Some
of these weights are positive, some negative (values can
be very close to 0). The terms with positive (or negative)
weights are the radical (or counter radical) words. The
top (ones with weights having high magnitude) radical
and counter radical words are presented to the experts for
validation. We experiment with several λ values resulting
in x vectors of various sparsity until the list of top
radical and counter radical words are approved by the
field experts.
We use the Matlab implementation of the SLEP package [28]
that utilizes gradient descent approach to solve the optimization problem (3). This package can handle matrices of 20M
entries within a couple of seconds on a machine with standard
configuration. The input to the SLEP package are the values
of A, λ, and y. The SLEP model outputs the weight vector x.

For each time period (in our case one month), each user
Ui will be assigned Radicalization Index RDi based on their
tweets within that period. This is done as follows:
• As mentioned earlier, each tweet which can only contain
a maximum of 140 characters is too insufficient for
inferring the radicalism of the user. Hence, from the
tweets of each user Ui we form a User Document Di
which is the conglomeration of all her tweets over a
period of one month. It is to be noted here that many
users choose to tweet quite infrequently, hence even if we
collect tweets for one month, a user might have tweeted
only once or twice during the entire one month which
defeats the purpose of collecting tweets for a month.
Hence, we further apply the constraint that we consider
only those users who have tweeted at least seven times
in a month. The value of this threshold has been arrived
at empirically after experimentation with various values
of the threshold.
• With the help of the model that has been fitted using the
organization documents, we classify the User Documents.
Let each User Document Di which is a termf requency
row be denoted by the row vector tc of count of terms
from our “vocabulary”.
• Each user Ui receives a ‘score’ which we refer to as
Radicalization Index RDi of user Ui . RDi is given by
p
X
j=1

VI. H EAT I NDEX C OMPUTATION
Once we have obtained the Location Indices Li , 1 ≤ i ≤ n
and Radicalization Indices RDi , 1 ≤ i ≤ n, for all the users
Ui , 1 ≤ i ≤ n , the Heat Index Hj of region Rj , 1 ≤ j ≤ m
is computed as
Pn
Hj = i=1 RDi × Li,j , ∀j, 1 ≤ j ≤ m.
The Heat Index Hj for a region Rj indicates the degree of
prevalence of radical ideologies among the people of Rj by
taking into account both the number of radical tweeters living
in Rj and also their ‘degree of radicalism’. In Table I we
present a time-varying Heat Map of Indonesia by computing
the map in three different time intervals of October 10 November 10, November 11 - December 10 and December
11 - January 10. We found a drastic change in the heat
indices during the interval of November 10 – December 10.
But we could not discern any particular event which could
have triggered the same.
VII. DATA C OLLECTION

B. Assignment of Radicalization Index:

RDi = tc .x =

where p is the size of our vocabulary.
This provides us a time-series of RDi values for the users,
which will make it possible to analyze the transition dynamics
for each user. It is evident that a high positive RDi indicates
that Ui is highly radical whereas a high negative RDi indicates
that Ui is highly counter radical.

tcj xj

Since our model requires the computation of both the Radicalization Index RDi as well as the Location Index Li for each
user Ui , we followed a two step data collection procedure
described as follows:
• For the purpose collecting the training data set for computing the Radicalization Index, we crawled the websites
of 36 well-known Indonesian organizations which are
classified as radical or counter radical by our field experts.
A few of the organizations are mentioned in Table II. We
crawled the websites of all these different organizations
and collected a total of 78,135 documents which after
pre-processing and filtering resulted into 49,250 documents. The reason for the reduction from the number of
crawled documents to the number of useful documents
is that many of the crawled documents did not have
any relevant information (for example documents having
only advertisements) and hence were discarded during
pre-processing. Each of the documents on a average
contained 280 words i.e on an average 2880 characters.
All documents pertaining to an organization were labeled
as radical or counter radical depending on the outlook
professed by the organization itself. These were then used
for fitting our Radicalization Index computation model.
• For our study on recovery of political signals pertaining to
trend of radical activities in Indonesia, we chose Twitter
as the data collection platform as Indonesia features as
one of the top five global market segments of Twitter
by reach, and accounts for 19.0% to 20.8% of Twitter’s

TABLE I: The table provides the top 5 province or special region names based on their computed Heat Index values (also
mentioned alongwith) for October 10 - November 10, November 11 - December 10, December 11- January 10
Province Name
Jakarta
East Java
West Java
Yogyakarta
Central Java

Heat Index
5.48
2.95
2.68
1.74
1.68

Province Name
Jakarta
East Java
Yogyakarta
Central Java
West Java

TABLE II: Table showing some of the well-known radical and
counter radical organizations of Indonesia
Radical Organizations
AdianHusaini
PKS
Arrahmah
AbuJibriel
EraMuslim
HizbutTahrir
MillahIbrahim

Counter radical Organizations
DaarulUluum
Interfidei
IslamLiberal
NU
PPIM
Paramadina
LKIS

TABLE III: Keyword markers used for filtering Twitter Stream
API
Keyword
“penegakan syariah”
“jihad majelis”
“mati syahid”
“ajaran islam”
“pendidikan agama di sekolah”
“asasi manusia”
“demokrasi yang”
“kebebasan beragama”
“sekularisme”
“di negeri negeri islam”

Interpretation
enforcement of Sharia
jihad assemblies
martyrdom
the teaching of Islam
religious education in schools
human rights
democracy
religious freedom
secularism
Islamic state in the country

total reach by country (Dec 2010)3 . No other publicly
available portal offers access to opinions posted online
by the Indonesian populace on a similar scale as does
Twitter. For gathering tweets, we use Twitter’s Stream
API to access Twitter’s global stream of publicly available
tweet data. Since our goal is to recover “political signals”,
we setup a keyword filter on the Stream API to gather
tweets that relate to radical and counter radical ideologies.
The keywords used for this filtration have been identified
by the social scientists in our Minerva project team and
are considered to be significant markers of radical and
counter radical ideologies in the Indonesian context. The
keyword list includes radical markers and a few such
markers are listed in Table III.

Heat Index
16.16
12.33
4.53
3.7
3.39

Province Name
Jakarta
Yogyakarta
West Java
East Java
Central Java

Heat Index
4.71
1.82
1.25
1.20
0.69

Location Index Li of user Ui over the thirty four provinces of
Indonesia, thus we focus only on users from Indonesia. The
keywords used are in Indonesian language and narrows down
the tweets we obtained from the Twitter API. Thus, the geocode in majority of cases indicated a location in Indonesia.
However, not all geo-codes are from Indonesia. We ignore
those tweets in the current work. Thus, out of these 12 million
tweets, 110,063 tweets contained geo-locations that mapped to
regions within Indonesia. To apply this reverse geo-coding,
we used the OpenStreetMap API4 . A user repository was
constructed by including only those users whose Declared
Home Locations matched with an identifiable Indonesian city
or province. Now the user declared home location which the
user mentions as a part of her profile could consist of any
text according to the user’s whim. We found texts such as
“Dark side of the moon” or “somewhere in this big world”
or “Here” or “infront of my laptop” and hence, there is a
need for pre-processing of the text. Also, the users provided
location information to varied degrees of granularity ranging
from continents to towns, however we are interested in the
fixed granularity level of Indonesian provinces and the special
regions such as Jakarta and Yogyakarta. Hence we manually
created a database of towns and cities of all of the Indonesian
provinces. Each of the provinces were annotated with 42 cities/
towns on an average with Papua being the highest which
was annotated with 70 cities/towns. Using this database we
then assigned a legitimate Declared Home Location to as
many users as possible. The final user repository consisted
of 959,911 unique users.

Fig. 2: Figure showing the number of tweets collected over
our observation period

We collected tweet data for a three-month interval and gathered a total of 12,152,874 tweets from October 10, 2012
to January 10, 2013 ( Figure 2) that matched the keyword
filtration criteria ( Table III ). We used the three months of
data to calculate the Radicalization Indices of the users. In
this research, we are interested in the probability distribution
3 http://www.billhartzer.com/pages/comscore-twitter-latin-america-usage/

http://www.comscoredatamine.com/2011/02/the-netherlands-leads-globalmarkets-in-twitter-reach/

4 The relevant information about the API could be found at http://wiki.openstreetmap.org/wiki/Nominatim

VIII. E XPERIMENTAL R ESULTS
We created Heat Maps of Indonesia on a monthly basis. We
computed the RDi of each user U i for each month from
October 10 to January 10, as long as Ui sent at least 7 tweets in
that month Again, for each user Ui we computed the Location
Index Li by considering all her tweets over the period of the
month.. For that we computed the general Computed Home
Location gCHL matrix. The(i, j)th entry gives the probability
of a user with a Declared Home Location of Ri being located
in Rj , 1 ≤ i ≤ m, 1 ≤ j ≤ m.
•

gCHL matrix : The gCHL matrix provides interesting
insights on the Indonesian population. We computed the
gCHL - matrix on all possible doublets among the
three months of observation period. i.e for each month
for calculating the Location Indices Li of users, we
have generated the gCHL matrix using the other two
months of data. Thus, in each case, we had training
data of two months and test data of one month. Among
the three gCHL - matrices generated, we saw that the
Declared Home Location of users give us a good insight
on the Actual home location of the user. Thus, instead of
merely depending on the geo-co-ordinates of users, we
should consider the home location from the user’s profile
and home location declarations are much more abundant
than geo-location containing tweets. However, depending
solely on Declared Home Locations can be deceptive. We
also observed that people with Declared Home Locations
in various different provinces from all around Indonesia
such as Bangka Belitung, Banten, Maluku, West Nusa
Tenggara, East Nusa Tenggara and Papua have a very
strong tendency to have high probability of having Actual
home location in Jakarta (as observed from our results
over three months). This is very intuitive because Jakarta
being the Capital Region must have attracted people from
different parts of Indonesia for prospective settlement.
We further made an interesting observation that people
with Declared Home Location of East Kalimantan have
considerable geo-location containing tweets from Central
Kalimantan.

The Heat Indices values for the thirty four Indonesian
provinces are computed using our approach for three months
of our observation period - namely October 10 - November
10, November 11 - December 10, December 11 -January
10. Among all Indonesian provinces the top five provinces
and special regions along with their Heat Index values are
presented in Table I for the three months. Color maps of
Indonesia with Heat Indices is shown in Figure 3, where darker
colors indicate a higher level of radical tweeting, and lighter
colors indicate a lower level of radical tweeting. It may be
seen from Figure 3 that the area around Jakarta and the Java
provinces are highly active in radical tweet creation. According
to our Twitter data analysis, the provinces Jakarta, East Java,
Yogyakarta and Central Java, along with West Java are the top
provinces that generate a high level of radical activities.

IX. VALIDATION
For the purpose of validation of the Radicalization Index,
we computed the Radicalization Indices of some well-known
counter radical leaders of Indonesia for the months that they
had tweeted for more than 7 times which we consider as our
threshold. Our classifier gave perfect accuracy. By accuracy of
the classification we mean the percentage of time the leaders
who are thus known to be counter radical were classified as
counter radical by our classifier. We did not validate the Location Index computation technique because of the lack of the
ground truth of the Actual home location of users. However,
our results of Heat Index are validated by the findings of the
Indonesia-based Wahid Institute5 (named after Abdurrahman
Wahid, an Indonesian Muslim religious and political leader
who served as the President of Indonesia from 1999 to 2001).
Wahid Institute promotes a moderate version of Islam through
dialogue events, publications, and public advocacies. The
institute also releases an annual religious freedom report on
religious life in Indonesia. According to the Wahid Institute’s
Annual Report of 20126 , the top four provinces of Indonesia
where radical activities are most observable are West Java,
Aceh, East Java, and Central Java. It may be noted here,
that three out of the four most radical provinces identified
by the Wahid Institute, also appear at the very top of our
list. Also, our field experts have confirmed Jakarta to be a
center of radical activities. It may be mentioned here that field
studies7 in January 2012 by Setara Institute8 , a well-known
NGO based in Indonesia, showed that the strong radicalism
of the young muslim population in Yogyakarta and Central
Java are making them hot targets to be recruited as Jihadists. In
May 2012, a mob attack by Indonesian Mujahidin Council on a
book launch of a well-known Canadian author, an advocate of
LGBT, took place in Yogyakarta. In September 2012, there has
been arrests of potential terrorists from Yogyakarta9 . Because,
Wahid Institute has mentioned about Indonesian provinces
only, it might be expected that Jakarta and Yogyakarta, being
special administrative regions, are missing from their list however, we do not have access to their full report. The
high radicalism of the Java provinces are also corroborated
by reports of the Setara Institute. The only radically active
province that shows up in the Wahid Institute report but does
not appear at the top of our list is Aceh, located at the north
west corner of Indonesia. It is worth mentioning here that Aceh
was completely devastated by the 2004 Indian Ocean Tsunami
and is still recovering from its effects. Aceh is also one of
the least economically developed provinces of Indonesia. We
believe that due to the lack of economic advancement in Aceh,
the level of Internet penetration in Aceh is fairly small and not
many people from Aceh are active tweeters. This may explain
5 http://berkleycenter.georgetown.edu/resources/organizations/wahidinstitute
6 Released on December 28, 2012
7 http://www.setara-institute.org/en/content/study-shows-how-youngradical-indonesian-muslims-become-terrorists
8 http://www.setara-institute.org/
9 http://www.washingtontimes.com/multimedia/image/indonesia-terrorjpg/

(a) Heat Map for October 10 to November 10

(b) Heat Map for November 11 to December 10

(c) Heat Map for December 11 to January 10

(d) Radicalism
Scale

Index

Fig. 3: Heat Maps of Indonesia

the reason for Aceh not showing up among our list of top
radically active provinces.
X. C ONCLUSION
We have developed a generic robust technique for recovering signals pertaining to a geographical area such as a
country using Twitter Data. We have applied our technique
to our Indonesian dataset and have observed high accuracy.
The goal of our work is the generation of a political Heat
Map of Indonesia which will clearly indicate the provinces of
Indonesia where radical narrative is prominent. The granularity
of the Radicalization Index Assignment used is a Twitter
user, while the granularity of regions used is an Indonesian
Province. Thus, we have analyzed tweets made by a user Ui
in a month to assign a Radicalization Index RDi to Ui - Thus
RDi indicates how radical is Ui in her political outlook . Also,
by mining the tweets in our database we assign a Location
Index Li to Ui . For computation of the Location Index, the
sources of location information used are not only the geolocation tagging as provided by Twitter API for the tweets
which contain that information (such tweets constitute less
than 1% of our dataset), but we also use the user declared
home location information from her profile (after considerable
amount of pre-processing and cleansing). We have combined
these two sources of information for inferring the probability
distribution of the location of the users among the provinces
of Indonesia. Thus, by considering the RDi ’s for all the users
over a period of one month, in conjunction with the Location
Indices Li ’s we generate the political Heat Maps of the likes

of Figure 3. We have got a time series of Heat Maps, which
can help us in mapping trends by regions in radical discourse.
Such Heat Maps can prove to be very useful in studying the
spatio-temporal dynamics of the people of Indonesia so far as
their political outlook is concerned
R EFERENCES
[1] Cho, E., Myers, S. A., & Leskovec, J. (2011, August). Friendship
and mobility: user movement in location-based social networks. In
Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1082-1090). ACM.
[2] Xu, J. M., Bhargava, A., Nowak, R., & Zhu, X. (2012). Socioscope:
spatio-temporal signal recovery from social media. In Machine Learning
and Knowledge Discovery in Databases (pp. 644-659). Springer Berlin
Heidelberg.
[3] Priedhorsky, R., Culotta, A., & Del Valle, S. Y. (2013). Inferring the
Origin Locations of Tweets with Quantitative Confidence. arXiv preprint
arXiv:1305.3932.
[4] Yang, D., Zhang, D., Yu, Z., & Wang, Z. (2013, May). A sentimentenhanced personalized location recommendation system. In Proceedings
of the 24th ACM Conference on Hypertext and Social Media (pp.
119-128). ACM.
[5] Rahimi, S. M., & Wang, X. (2013). Location Recommendation Based on
Periodicity of Human Activities and Location Categories. In Advances in
Knowledge Discovery and Data Mining (pp. 377-389). Springer Berlin
Heidelberg.
[6] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. Exploring Venue
Popularity in Foursquare.
[7] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. (2012, December).
Dissecting foursquare venue popularity via random region sampling.
In Proceedings of the 2012 ACM conference on CoNEXT student
workshop (pp. 21-22). ACM.
[8] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008). Understanding individual human mobility patterns. Nature, 453(7196), 779-782.

[9] Sadilek, A., Kautz, H., & Bigham, J. P. (2012, February). Finding your
friends and following them to where you are. In Proceedings of the
fifth ACM international conference on Web search and data mining (pp.
723-732). ACM.
[10] Noulas, A., Scellato, S., Lambiotte, R., Pontil, M., & Mascolo, C.
(2012). A tale of many cities: universal patterns in human urban mobility.
PloS one, 7(5), e37027.
[11] Lu, X., Bengtsson, L., & Holme, P. (2012). Predictability of population displacement after the 2010 Haiti earthquake. Proceedings of the
National Academy of Sciences, 109(29), 11576-11581.
[12] Chang, H. W., Lee, D., Eltaher, M., & Lee, J. (2012, August). @
Phillies Tweeting from Philly? Predicting Twitter User Locations with
Spatial Word Usage. In Advances in Social Networks Analysis and
Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp.
111-118). IEEE.
[13] Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from?
inferring home locations of twitter users. Proc AAAI ICWSM, 12.
[14] Eisenstein, J., O’Connor, B., Smith, N. A., & Xing, E. P. (2010,
October). A latent variable model for geographic lexical variation. In
Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing (pp. 1277-1287). Association for Computational
Linguistics.
[15] Cheng, Z., Caverlee, J., & Lee, K. (2010, October). You are where
you tweet: a content-based approach to geo-locating twitter users. In
Proceedings of the 19th ACM international conference on Information
and knowledge management (pp. 759-768). ACM.
[16] Hecht, B., Hong, L., Suh, B., & Chi, E. H. (2011, May). Tweets from
Justin Bieber’s heart: the dynamics of the location field in user profiles.
In Proceedings of the 2011 annual conference on Human factors in
computing systems (pp. 237-246). ACM.
[17] Boutet, A., Kim, H., & Yoneki, E. (2012, August). What’s in Twitter: I
Know What Parties are Popular and Who You are Supporting Now!. In
Advances in Social Networks Analysis and Mining (ASONAM), 2012
IEEE/ACM International Conference on (pp. 132-139). IEEE.
[18] Dahlin, J., Johansson, F., Kaati, L., Martenson, C., & Svenson, P.
(2012, August). Combining Entity Matching Techniques for Detecting
Extremist Behavior on Discussion Boards. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International
Conference on (pp. 850-857). IEEE.
[19] Eisenstein, J., Ahmed, A., & Xing, E. P. (2011, June). Sparse additive
generative models of text. In International Conference on Machine
Learning (ICML).
[20] Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis,
K. (2012, April). Discovering geographical topics in the twitter stream.
In Proceedings of the 21st international conference on World Wide Web
(pp. 769-778). ACM.
[21] O’Connor, B., Eisenstein, J., Xing, E. P., & Smith, N. A. (2010).
Discovering demographic language variation. In Workshop on Machine
Learning for Social Computing at NIPS.
[22] Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011, March).
Geographical topic discovery and comparison. In Proceedings of the
20th international conference on World wide web (pp. 247-256). ACM.
[23] Zhang, T. (2009). Some sharp performance bounds for least squares
regression with L1 regularization. The Annals of Statistics, 37(5A),
2109-2144.
[24] S.Kim, K.Koh, M.Lustig, S.Boyd and D. Gorinevsky, An Interior-Point
Method for Large-Scale l1 - Regularized Least Squares Journal of
seleccted topics in Signal Processing, Vol. 1, No. 4, pages 606 - 617,
Dec 2007
[25] Kolter, J. Z., & Ng, A. Y. (2009, June). Regularization and feature
selection in least-squares temporal difference learning. In Proceedings
of the 26th Annual International Conference on Machine Learning (pp.
521-528). ACM.
[26] Brzezinski, J. R., & Knafl, G. J. (1999). Logistic regression modeling for context-based classification. In Database and Expert Systems
Applications, 1999. Proceedings. Tenth International Workshop on (pp.
755-759). IEEE.
[27] Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.
[28] J. Liu, S. Ji and Jieping Ye. SLEP: Sparse Learning with Efficient
Projections, Arizona State University (2009)
[29] Lee, S. I., Lee, H., Abbeel, P., & Ng, A. Y. (2006, July). Efficient
L˜ 1 Regularized Logistic Regression. In Proceedings of the National

[30]
[31]

[32]

[33]

[34]

[35]

Conference on Artificial Intelligence (Vol. 21, No. 1, p. 401). Menlo
Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.
T. P. Minka, A comparison of numerical optimizaers for logistic regression, Technical report, 2007
Thelwall, M., Buckley, K., Paltoglou, G., Skowron, M., Garcia, D.,
Gobron, S., ... & Holyst, J. A. (2013). Damping Sentiment Analysis
in Online Communication: Discussions, Monologs and Dialogs. In
Computational Linguistics and Intelligent Text Processing (pp. 1-12).
Springer Berlin Heidelberg.
Melville, P., Gryc, W., & Lawrence, R. D. (2009, June). Sentiment analysis of blogs by combining lexical knowledge with text classification.
In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1275-1284). ACM.
Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010, June). Movie
reviews and revenues: An experiment in text regression. In Human
Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics (pp.
293-296). Association for Computational Linguistics.
Kogan, S., Levin, D., Routledge, B. R., Sagi, J. S., & Smith, N. A.
(2009, May). Predicting risk from financial reports with regression.
In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for
Computational Linguistics (pp. 272-280). Association for Computational
Linguistics
Brynielsson, J., Horndahl, A., Johansson, F., Kaati, L., Martenson, C.,
& Svenson, P. (2012, August). Analysis of weak signals for detecting
lone wolf terrorists. In Intelligence and Security Informatics Conference
(EISIC), 2012 European (pp. 197-204). IEEE.

c 2015 Society for Industrial and Applied Mathematics


Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SIAM J. SCI. COMPUT.
Vol. 37, No. 1, pp. A488–A514

ORTHOGONAL RANK-ONE MATRIX PURSUIT
FOR LOW RANK MATRIX COMPLETION∗
ZHENG WANG† , MING-JUN LAI‡ , ZHAOSONG LU§ , WEI FAN¶, HASAN DAVULCU ,
AND JIEPING YE#

Abstract. In this paper, we propose an eﬃcient and scalable low rank matrix completion
algorithm. The key idea is to extend the orthogonal matching pursuit method from the vector case
to the matrix case. We further propose an economic version of our algorithm by introducing a novel
weight updating rule to reduce the time and storage complexity. Both versions are computationally
inexpensive for each matrix pursuit iteration and ﬁnd satisfactory results in a few iterations. Another
advantage of our proposed algorithm is that it has only one tunable parameter, which is the rank.
It is easy to understand and to use by the user. This becomes especially important in large-scale
learning problems. In addition, we rigorously show that both versions achieve a linear convergence
rate, which is signiﬁcantly better than the previous known results. We also empirically compare the
proposed algorithms with several state-of-the-art matrix completion algorithms on many real-world
datasets, including the large-scale recommendation dataset Netﬂix as well as the MovieLens datasets.
Numerical results show that our proposed algorithm is more eﬃcient than competing algorithms while
achieving similar or better prediction performance.
Key words. low rank, singular value decomposition, rank minimization, matrix completion,
matching pursuit
AMS subject classifications. 15A83, 68W40, 90C06
DOI. 10.1137/130934271

1. Introduction. Recently, low rank matrix learning has attracted signiﬁcant
attention in machine learning and data mining due to its wide range of applications, such as collaborative ﬁltering, dimensionality reduction, compressed sensing,
multiclass learning, and multitask learning. See [1, 2, 3, 7, 9, 23, 34, 40, 37] and
the references therein. In this paper, we consider the general form of low rank matrix completion: given a partially observed real-valued matrix Y ∈ n×m , the low
rank matrix completion problem is to ﬁnd a matrix X ∈ n×m with minimum rank
that best approximates the matrix Y on the observed elements. The mathematical
formulation is given by
(1.1)

min

rank(X)

s.t.

PΩ (X) = PΩ (Y),

X∈n×m

∗ Submitted to the journal’s Methods and Algorithms for Scientiﬁc Computing section August 26,
2013; accepted for publication (in revised form) November 18, 2014; published electronically February
19, 2015. This research was partially supported by NSF (IIS-0953662, CCF-1025177, IIS-1421057),
NIH (LM010730), China 973 Fundamental R&D Program (2014CB340304), NSERC Discovery Grant
and a collaboration grant from the Simons Foundation.
http://www.siam.org/journals/sisc/37-1/93427.html
† Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor,
MI 48109 (zhengwang@umich.edu).
‡ Department of Mathematics, University of Georgia, Athens, GA 30602 (mjlai@math.uga.edu).
§ Department of Mathematics, Simon Fraser University, Burnaby, BC V5A 156, Canada
(zhaosong@sfu.ca).
¶ Huawei Noah’s Ark Lab, Units 520–530 Core Building 2, Hong Kong Science Park, Hong Kong
(wei.fan@gmail.com).
 School of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, AZ 85287 (hasandavulcu@asu.edu).
# Department of Computational Medicine and Bioinformatics, and Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109 (jpye@umich.edu).

A488

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A489

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

where Ω is the set of all index pairs (i, j) of observed entries, and PΩ is the orthogonal
projector onto the span of matrices vanishing outside of Ω.
1.1. Related works. As it is intractable to minimize the matrix rank exactly
in the general case, many approximate solutions have been proposed to attack the
problem (1.1) (cf., e.g., [7, 24, 28]). A widely used convex relaxation of matrix rank is
the trace norm or nuclear norm [7]. The matrix trace norm is deﬁned by the Schatten
p-norm
with p = 1. For matrix X with rank r, its Schatten p-norm is deﬁned by
r
( i=1 σip )1/p , where {σi } are the singular values of X and without loss of generality
we assume they are sorted in descending order.
r Thus, the trace norm of X is the 1
norm of the matrix spectrum as ||X||∗ = i=1 |σi |. Then the convex relaxation for
problem (1.1) is given by
(1.2)

min

||X||∗

s.t.

PΩ (X) = PΩ (Y).

X∈Rn×m

Cai, Candès, and Shen [6] propose an algorithm to solve (1.2) based on soft singular
value thresholding (SVT). Keshavan and Oh [21] and Jain, Meka, and Dhillon [18]
develop more eﬃcient algorithms by using the top-k singular pairs.
Many other algorithms have been developed to solve the trace norm penalized
problem:
(1.3)

min

X∈Rn×m

||PΩ (X) − PΩ (Y)||2F + λ||X||∗ .

Ji and Ye [20], Liu, Sun, and Toh [27], and Toh and Yun [44] independently propose to
employ the proximal gradient algorithm to improve the algorithm of [6] by signiﬁcantly
√
reducing the number of iterations. They obtain an -accurate solution in O(1/ )
steps. More eﬃcient soft singular vector thresholding algorithms are proposed in
[29, 30] by investigating the factorization property of the estimated matrix. Each step
of the algorithms requires the computation of a partial singular value decomposition
(SVD) for a dense matrix. In addition, several methods approximate the trace norm
using its variational characterizations [32, 40, 46, 37] and proceed by alternating
optimization. However, these methods lack global convergence guarantees.
Solving these low rank or trace norm problems is computationally expensive for
large matrices, as it involves computing SVD. Most of the methods above involve
the computation of SVD or truncated SVD iteratively, which is not scalable to largescale problems. How to solve these problems eﬃciently and accurately for large-scale
problems has attracted much attention in recent years.
Recently, the coordinate gradient descent method has been demonstrated to be
eﬃcient in solving sparse learning problems in the vector case [11, 39, 47, 48]. The key
idea is to solve a very simple one-dimensional problem (for one coordinate) in each
iteration. One natural question is whether and how such a method can be applied to
solve the matrix completion problem. Some progress has been made recently in this
direction. Dudı́k, Harchaoui, and Malick [9] propose a coordinate gradient descent
solution for the trace norm penalized problem. They recast the nonsmooth objective
in problem (1.3) as a smooth one in an inﬁnite dimensional rank-one matrix space,
then apply the coordinate gradient algorithm on the collection of rank-one matrices.
Zhang, Yu, and Schuurmann [49] further improve the eﬃciency using the boosting
method, and the improved algorithm guarantees an -accuracy within O(1/) iterations. Although these algorithms need slightly more iterations than the proximal

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A490

WANG, LAI, LU, FAN, DAVULCU, AND YE

methods, they are more scalable as they only need to compute the top singular vector pair in each iteration. Note that the top singular vector pair can be computed
eﬃciently by the power method or Lanczos iterations [13]. Jaggi and Sulovský [17]
propose an algorithm which achieves the same iteration complexity as the algorithm in
[49] by directly applying Hazan’s algorithm [15]. Tewari, Ravikumar, and Dhillon [42]
solve a more general problem based on a greedy algorithm. Shalev-Shwartz, Gonen,
and Shamir [38] further reduce the number of iterations based on heuristics without
theoretical guarantees.
Most methods based on the top singular vector pair include two main steps in
each iteration. The ﬁrst step involves computing the top singular vector pair, and the
second step reﬁnes the weights of the rank-one matrices formed by all top singular
vector pairs obtained up to the current iteration. The main diﬀerences among these algorithms lie in how they reﬁne the weights. Jaggi’s algorithm (JS) [17] directly applies
Hazan’s algorithm [15], which relies on the Frank–Wolfe algorithm [10]. It updates
the weights with a small step size and does not consider further reﬁnement. It does
not choose the optimal weights in each step, which leads to a slow convergence rate.
Similar to JS, Tewari, Ravikumar, and Dhillon [42] use a small update step size for a
general structure constrained problem. The greedy eﬃcient component optimization
(GECO) [38] optimizes the weights by solving another time-consuming optimization
problem. It involves a smaller number of iterations than the JS algorithm. However,
the sophisticated weight reﬁnement leads to a higher total computational cost. The
lifted coordinate gradient descent algorithm [9] updates the weights with a constant
step size in each iteration and conducts a LASSO-type algorithm [43] to fully correct
the weights. The weights for the basis update are diﬃcult to tune as a large value
leads to divergence and a small value makes the algorithm slow [49]. The matrix
norm boosting approach (Boost) [49] learns the update weights and designs a local
reﬁnement step by a nonconvex optimization problem which is solved by alternating
optimization. It has a sublinear convergence rate.
We summarize their common drawbacks as follows:
• Some weight reﬁnement steps are ineﬃcient, resulting in a slow convergence
rate. The current best convergence rate is O(1/). Some reﬁnement steps
themselves contain computationally expensive iterations [9, 49], which do not
scale to large-scale data.
• They have heuristic-based tunable parameters which are not easy to use.
However, these parameters severely aﬀect their convergence speed and the
approximation result. In some algorithms, an improper parameter even makes
the algorithm diverge [6, 9].
In this paper, we present a simple and eﬃcient algorithm to solve the low rank
matrix completion problem. The key idea is to extend the orthogonal matching pursuit (OMP) procedure [35] from the vector case to the matrix case. In each iteration,
a rank-one basis matrix is generated by the left and right top singular vectors of the
current approximation residual. In the standard version of the proposed algorithm,
we fully update the weights for all rank-one matrices in the current basis set at the
end of each iteration; this is achieved by performing an orthogonal projection of the
observation matrix onto the spanning subspace of those rank-one matrices. The most
time-consuming step of the proposed algorithm is to calculate the top singular vector pair of a sparse matrix, which involves O(|Ω|) operations in each iteration. An
appealing feature of the proposed algorithm is that it has a linear convergence rate.
This is diﬀerent from traditional OMP or weak orthogonal greedy algorithms, whose
convergence rate for sparse vector recovery is sublinear, as shown in [26]. See also

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A491

[8], [41], [45] for an extensive study on various greedy algorithms. With this rate of
convergence, we only need O(log(1/)) iterations for achieving an -accuracy solution.
One drawback of the standard algorithm is that it needs to store all rank-one
matrices in the current basis set for full weight updating, which contains r|Ω| elements
in the rth iteration. This makes the storage complexity of the algorithm dependent on
the number of iterations, which restricts the approximation rank especially for largescale matrices. To tackle this problem, we propose an economic weight updating rule
for this algorithm. In this economic version of the proposed algorithm, we only track
two matrices in each iteration. One is the current estimated matrix and the other
is the pursued rank-one matrix. When restricted to the observations in Ω, each has
|Ω| nonzero elements. Thus the storage requirement, i.e., 2|Ω|, remains the same in
diﬀerent iterations, which is the same as the greedy algorithms [17, 42]. Interestingly,
we show that using this economic updating rule we still retain the linear convergence
rate. Besides the convergence property, we also analyze the recovery guarantee of our
proposed algorithm. Speciﬁcally, we extend our proposed algorithm to a more general
matrix sensing problem and show the recovery guarantee of the proposed algorithm
under the rank-restricted isometry property [25]. We verify the eﬃciency of our
algorithm empirically on large-scale matrix completion problems, such as MovieLens
[31] and Netﬂix [4, 5].
The main contributions of our paper are as follows:
• We propose a computationally eﬃcient and scalable algorithm for matrix
completion, which extends OMP from the vector case to the matrix case.
• We theoretically prove the linear convergence rate of our algorithm. As a
result, we only need O(log(1/)) iterations to obtain an -accuracy solution,
and in each iteration we only need to compute the top singular vector pair,
which can be computed eﬃciently.
• We further reduce the storage complexity of our algorithm based on an economic weight updating rule while retaining the linear convergence rate. This
version of our algorithm has a constant storage complexity which is independent of the approximation rank and is more practical for large-scale matrices.
• We extend our proposed algorithm to a more general matrix sensing problem
and show the recovery guarantee of the proposed algorithm under the rankrestricted isometry property.
• Both versions of our algorithm have only one free parameter, i.e., the rank
of the estimated matrix. The proposed algorithm is guaranteed to converge,
i.e., no risk of divergence.
1.2. Notation and organization. Let Y = (y1 , . . . , ym ) ∈ n×m be an n × m
real matrix, and let Ω ⊂ {1, . . . , n} × {1, . . . , m} denote the indices of the observed
entries of Y. PΩ is the projection operator onto the space spanned by the matrices van(i, j) ∈ Ω
ishing outside of Ω so that the (i, j)th component of PΩ (Y) equals to Yi,j for

2
and zero otherwise. The Frobenius norm of Y is deﬁned as ||Y||F =
i,j Yi,j .
T T
Let vec(Y) = (y1T , . . . , ym
) denote a vector reshaped from matrix Y by concatenating all its column vectors. Let ẏ = vecΩ (Y) = {(yω1 , . . . , yω|Ω| )T ∀ ωi ∈ Ω}
denote a vector generated by concatenating all observed elements of Y indexed by
Ω. The Frobenius inner product of two matrices X and Y is deﬁned as X, Y =
trace(XT Y), which also equals the componentwise inner product of the corresponding vectors as vec(X), vec(Y). Given a matrix A ∈ n×m , we denote PΩ (A)
by AΩ . For any two matrices A, B ∈ n×m , we deﬁne A, BΩ = AΩ , BΩ  and

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A492

WANG, LAI, LU, FAN, DAVULCU, AND YE


	A	Ω =
A, AΩ . Without further declaration,
 the matrix norm refers to the
Frobenius norm, which is also written as 	A	 = A, A.
The rest of the paper is organized as follows. We present the standard version of
our algorithm in section 2. Section 3 analyzes the convergence rate of the standard
version of our algorithm; we further propose an economic version of our algorithm
and prove its linear convergence rate in Section 4. Section 5 extends the proposed
algorithm to a more general matrix sensing case and presents its guarantee of ﬁnding
the optimal solution under the rank-restricted isometry property condition. In section
6 we analyze the stability of both versions of our algorithm; empirical evaluations are
presented in section 7 to verify the eﬃciency and eﬀectiveness of our algorithm. We
ﬁnally conclude our paper in section 8.
2. Orthogonal rank-one matrix pursuit. It is well-known that any matrix
X ∈ n×m can be written as a linear combination of rank-one matrices, that is,

(2.1)
X = M(θ) =
θi M i ,
i∈I

where {Mi : i ∈ I} is the set of all n × m rank-one matrices with unit Frobenius
norm. Clearly, there are inﬁnitely many choices of Mi ’s. Such a representation can
be obtained via the standard SVD of X.
The original low rank matrix approximation problem aims to minimize the zeronorm of θ subject to the constraint
min ||θ||0
θ

(2.2)

s.t.

PΩ (M(θ)) = PΩ (Y),

where ||θ||0 denotes the number of nonzero elements of the vector θ.
If we reformulate the problem as
min

||PΩ (M(θ)) − PΩ (Y)||2F

s.t.

||θ||0 ≤ r,

θ

(2.3)

we could solve it by an OMP type algorithm using rank-one matrices as the basis. In
particular, we are to ﬁnd a suitable subset of overcomplete rank-one matrix coordinates and learn the weight for each selected coordinate. This is achieved by executing
two steps alternatively: one is to pursue the basis, and the other is to learn the weight
of the basis.
Suppose that after the (k − 1)th iteration, the rank-one basis matrices M1 , . . . ,
Mk−1 and their current weight vector θ k−1 are already computed. In the kth iteration,
we are to pursue a new rank-one basis matrix Mk with unit Frobenius norm, which is
mostly correlated with the current observed regression residual Rk = PΩ (Y) − Xk−1 ,
where
Xk−1 = (M(θ k−1 ))Ω =

k−1


θik−1 (Mi )Ω .

i=1

Therefore, Mk can be chosen to be an optimal solution of the following problem:
(2.4)

max {M, Rk  : rank(M) = 1, 	M	F = 1} .
M

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A493

Notice that each rank-one matrix M with unit Frobenius norm can be written as the
product of two unit vectors, namely, M = uvT for some u ∈ n and v ∈ m with
	u	 = 	v	 = 1. We then see that problem (2.4) can be equivalently reformulated as
(2.5)

max{uT Rk v : 	u	 = 	v	 = 1}.
u,v

Clearly, the optimal solution (u∗ , v∗ ) of problem (2.5) is a pair of top left and right
singular vectors of Rk . It can be eﬃciently computed by the power method [17, 9].
The new rank-one basis matrix Mk is then readily available by setting Mk = u∗ v∗T .
After ﬁnding the new rank-one basis matrix Mk , we update the weights θ k for all
currently available basis matrices {M1 , . . . , Mk } by solving the following least squares
regression problem:
(2.6)

min ||

θ∈k

k


θi Mi − Y||2Ω .

i=1

By reshaping the matrices (Y)Ω and (Mi )Ω into vectors ẏ and ṁi , we can easily see
that the optimal solution θ k of (2.6) is given by
(2.7)

θ k = (M̄Tk M̄k )−1 M̄Tk ẏ,

where M̄k = [ṁ1 , . . . , ṁk ] is the matrix formed by all reshaped basis vectors. The
row size of matrix M̄k is the total number of observed entries. It is computationally
expensive to directly calculate the matrix multiplication. We simplify this step by an
incremental process and give the implementation details in the appendix.
We run the above two steps iteratively until some desired stopping condition is
satisﬁed. We can terminate the method based on the rank of the estimated matrix
or the approximation residual. In particular, one can choose a preferred rank of the
solution matrix. Alternatively, one can stop the method once the residual 	Rk 	 is less
than a tolerance parameter ε. The main steps of orthogonal rank-one matrix pursuit
(OR1MP) are given in Algorithm 1.
Algorithm 1. OR1MP.
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk vkT .
Step 2: Compute the weight vector θ k using the closed form least squares solution θ k = (M̄Tk M̄k )−1 M̄Tk ẏ.

Step 3: Set Xk = ki=1 θik (Mi )Ω and k ← k + 1.
until stopping criterion is satisﬁed
k
Output: Constructed matrix Ŷ = i=1 θik Mi .
Remark 2.1. In our algorithm, we adapt OMP on the observed part of the matrix.
This is similar to the GECO algorithm. However, GECO constructs the estimated
matrix by projecting the observation matrix onto a much larger subspace, which is a
product of two subspaces spanned by all left singular vectors and all right singular
vectors obtained up to the current iteration. So it has a much higher computational
complexity. Lee and Bresler [25] recently proposed the ADMiRA algorithm, which is

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A494

WANG, LAI, LU, FAN, DAVULCU, AND YE

also a greedy approach. In each step it ﬁrst chooses 2r components by top-2r truncated SVD and then uses another top-r truncated SVD to obtain a rank-r estimated
matrix. Thus, the ADMiRA algorithm is computationally more expensive than the
proposed algorithm. The diﬀerence between the proposed algorithm and ADMiRA is
somewhat similar to the diﬀerence between OMP [35] for learning sparse vectors and
CoSaMP [33]. In addition, the performance guarantees (including recovery guarantee
and convergence property) of ADMiRA rely on strong assumptions, i.e., the matrix
involved in the loss function satisﬁes a rank-restricted isometry property [25].
3. Convergence analysis of Algorithm 1. In this section, we will show that
Algorithm 1 is convergent and achieves a linear convergence rate. This result is given
in the following theorem.
Theorem 3.1. OR1MP satisfies


1
1−
min(m, n)

||Rk || ≤

k−1
	Y 	Ω

∀k ≥ 1.

Before proving Theorem 3.1, we need to establish some useful and preparatory
properties of Algorithm 1. The ﬁrst property says that Rk+1 is perpendicular to all
previously generated Mi for i = 1, . . . , k.
Property 3.2. Rk+1 , Mi  = 0 for i = 1, . . . , k.
Proof. Recall that θ k is the optimal solution of problem (2.6). By the ﬁrst-order
optimality condition, one has


	
k

k
θi M i , M i
= 0 for i = 1, . . . , k,
Y−
Ω

i=1

k
which together with Rk = YΩ − Xk−1 and Xk = i=1 θik (Mi )Ω implies that Rk+1 ,
Mi  = 0 for i = 1, . . . , k.
The following property shows that as the number of rank-one basis matrices Mi
increases during our learning process, the residual 	Rk 	 does not increase.
Property 3.3. 	Rk+1 	 ≤ 	Rk 	 for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
	Rk+1 	2 = min {	Y −
θ∈k

k


θi Mi 	2Ω }

i=1
k−1


≤ min {	Y −
=

θ∈k−1
	Rk 	2 ,

θi Mi 	2Ω }

i=1

and hence the conclusion holds.
We next establish that {(Mi )Ω }ki=1 is linearly independent unless 	Rk 	 = 0. It
follows that formula (2.7) is well-deﬁned and hence θ k is uniquely deﬁned before the
algorithm stops.
Property 3.4. Suppose that Rk 
= 0 for some k ≥ 1. Then, M̄i has a full
column rank for all i ≤ k.
Proof. Using Property 3.3 and the assumption Rk 
= 0 for some k ≥ 1, we see
that Ri 
= 0 for all i ≤ k. We now prove the statement of this lemma by induction
on i. Indeed, since R1 
= 0, we clearly have M̄1 
= 0. Hence the conclusion holds for

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A495

i = 1. We now assume that it holds for i − 1 < k and need to show that it also holds
for i ≤ k. By the induction hypothesis, M̄i−1 has a full column rank. Suppose for
contradiction that M̄i does not have a full column rank. Then, there exists α ∈ i−1
such that
(Mi )Ω =

i−1


αj (Mj )Ω ,

j=1

which together with Property 3.2 implies that Ri , Mi  = 0. It follows that
σ1 (Ri ) = uTi Ri vi = Ri , Mi  = 0,
and hence Ri = 0, which contradicts the fact that Rj 
= 0 for all j ≤ i. Therefore,
M̄i has a full column rank and the conclusion holds for general i.
We next build a relationship between two consecutive residuals 	Rk+1 	 and 	Rk 	.
For convenience, deﬁne θkk−1 = 0 and let
θ k = θ k−1 + η k .
In view of (2.6), one can observe that
(3.1)

η = arg min ||
k

η

k


ηi Mi − Rk ||2Ω .

i=1

Let
(3.2)

Lk =

k


ηik (Mi )Ω .

i=1

By the deﬁnition of Xk , one can also observe that
Xk = Xk−1 + Lk ,
Rk+1 = Rk − Lk .
Property 3.5. ||Rk+1 ||2 = ||Rk ||2 − ||Lk ||2 and ||Lk ||2 ≥ Mk , Rk 2 , where Lk
is defined in (3.2).

Proof. Since Lk = i≤k ηik (Mi )Ω , it follows from Property 3.2 that Rk+1 , Lk  =
0. We then have
||Rk+1 ||2 = ||Rk − Lk ||2
= ||Rk ||2 − 2Rk , Lk  + ||Lk ||2
= ||Rk ||2 − 2Rk+1 + Lk , Lk  + ||Lk ||2
= ||Rk ||2 − 2Lk , Lk  + ||Lk ||2
= ||Rk ||2 − ||Lk ||2 .
We next bound 	Lk 	2 from below. If Rk = 0, ||Lk ||2 ≥ Mk , Rk 2 clearly holds.
We now suppose throughout the remaining proof that Rk 
= 0. It then follows from
Property 3.4 that M̄k has a full column rank. Using this fact and (3.1), we have
−1 T

M̄k ṙk ,
η k = M̄Tk M̄k

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A496

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

where ṙk is the reshaped residual vector of Rk . Invoking that Lk =
we then obtain


i≤k

ηik (Mi )Ω ,

||Lk ||2 = ṙTk M̄k (M̄Tk M̄k )−1 M̄Tk ṙk .

(3.3)

Let M̄k = QU be the QR factorization of M̄k , where QT Q = I and U is a k × k
nonsingular upper triangular matrix. One can observe that (M̄k )k = ṁk , where
(M̄k )k denotes the kth column of the matrix M̄k and ṁk is the reshaped vector of
(Mk )Ω . Recall that 	Mk 	 = 	uk vkT 	 = 1. Hence, 	(M̄k )k 	 ≤ 1. Due to QT Q = I,
M̄k = QU, and the deﬁnition of U, we have
0 < |Ukk | ≤ 	Uk 	 = 	(M̄k )k 	 ≤ 1.
In addition, by Property 3.2, we have
T

M̄Tk ṙk = [0, . . . , 0, Mk , Rk ] .

(3.4)

Substituting M̄k = QU into (3.3), and using QT Q = I and (3.4), we obtain that
	Lk 	2 = ṙTk M̄k (UT U)−1 M̄Tk ṙk
= [0, . . . , 0, Mk , Rk ] U−1 U−T [0, . . . , 0, Mk , Rk ]

T

= Mk , Rk 2 /(Ukk )2 ≥ Mk , Rk 2 ,
where the last equality follows since U is upper triangular and the last inequality is
due to |Ukk | ≤ 1.
We are now ready to prove Theorem 3.1.
Proof of Theorem 3.1. Using the deﬁnition of Mk , we have
Mk , Rk  = uk vkT , Rk  = σ1 (Rk )

≥

σi2 (Rk )
=
rank(Rk )
i



	Rk 	2
≥
rank(Rk )



	Rk 	2
.
min(m, n)

Using this inequality and Property 3.5, we obtain that
||Rk+1 ||2

= ||Rk ||2 − ||Lk ||2 ≤ ||Rk ||2 − Mk , Rk 2
≤ (1 −

1
2
min(m,n) )||Rk || .

In view of this relation and the fact that 	R1 	 = 	Y	2Ω , we easily conclude that
k−1

1
||Rk || ≤
1−
	Y	Ω .
min(m, n)
This completes the proof.
Remark 3.6. If Ω is the entire set of all indices of {(i, j), i = 1, . . . , n, j =
1, . . . , m}, our OR1MP algorithm equals the standard SVD using the power method.
In particular, when Ω is the set of all indices while the given entries are noisy values
of an exact matrix, our OR1MP algorithm can help remove the noise.
Remark 3.7. In a standard study of the convergence rate of OMP or the orthogonal greedy algorithm, one can only get |Mk , Rk | ≥ 	Rk 	2 , which leads to a
sublinear convergence. Our Mk is a data dependent construction which is based on
the top left and right singular vectors of the residual matrix Rk . It thus has a better
estimate which gives us the linear convergence.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A497

Algorithm 2. EOR1MP.
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk vkT .
Step 2: Compute the optimal weights αk for Xk−1 and Mk by solving
min ||α1 Xk−1 + α2 (Mk )Ω − YΩ ||2 .
α

Step 3: Set Xk = αk1 Xk−1 + αk2 (Mk )Ω ; θkk = αk2 and θik = θik−1 αk1 for i < k;
k ← k + 1.
until stopping criterion is satisﬁed
k
Output: Constructed matrix Ŷ = i=1 θik Mi .

4. An economic OR1MP algorithm. The proposed OR1MP algorithm has
to track all pursued bases and save them in the memory. It demands O(r|Ω|) storage
complexity to obtain a rank-r estimated matrix. For large-scale problems, such storage
requirement is not negligible and restricts the rank of the matrix to be estimated. To
adapt our algorithm to large-scale problems with a large approximation rank, we
simplify the orthogonal projection step by only tracking the estimated matrix Xk−1
and the rank-one update matrix Mk . In this case, we only need to estimate the
weights for these two matrices by solving the following least squares problem:
(4.1)

αk = arg

min

α={α1 ,α2 }

||α1 Xk−1 + α2 Mk − Y||2Ω .

This still fully corrects all weights of the existed bases, though the correction is suboptimal. If 
we write the estimated matrix as a linear combination of the bases, we
have Xk = ki=1 θik (Mi )Ω with θkk = αk2 and θik = θik−1 αk1 , for i < k. The detailed
procedure of this simpliﬁed method is given in Algorithm 2.
The proposed economic orthogonal rank-one matrix pursuit algorithm (EOR1MP)
uses the same amount of storage as the greedy algorithms [17, 42], which is signiﬁcantly
smaller than that required by our OR1MP algorithm, i.e., Algorithm 1. Interestingly,
we can show that the EOR1MP algorithm is still convergent and retains the linear
convergence rate. The main result is given in the following theorem.
Theorem 4.1. Algorithm 2, the EOR1MP algorithm, satisfies

||Rk || ≤

1
1−
min(m, n)

k−1
	Y	Ω

∀k ≥ 1.

Before proving Theorem 4.1, we present several useful properties of our Algorithm 2. The ﬁrst property says that Rk+1 is perpendicular to matrix Xk−1 and
matrix Mk .
Property 4.2. Rk+1 , Xk−1  = 0 and Rk+1 , Mk  = 0.
Proof. Recall that αk is the optimal solution of problem (4.1). By the ﬁrst-order
optimality condition according to Xk−1 and Mk , one has
Y − αk1 Xk−1 − αk2 Mk , Xk−1 Ω = 0
and

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A498

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Y − αk1 Xk−1 − αk2 Mk , Mk Ω = 0,
which together with Rk = YΩ − Xk−1 imply that Rk+1 , Xk−1  = 0 and Rk+1 ,
Mk  = 0.
Property 4.3. 	Rk+1 	2 = 	YΩ 	2 − 	Xk 	2 for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
	YΩ 	2 = 	Rk+1 + Xk 	2
= 	Rk+1 	2 + 	Xk 	2 + 2Rk+1 , Xk 
= 	Rk+1 	2 + 	Xk 	2
as Rk+1 , Xk  = αk1 Rk+1 , Xk−1  + αk2 Rk+1 , Mk  = 0, and hence the conclusion
holds.
The following property shows that as the number of rank-one basis matrices Mi
increases during our iterative process, the residual 	Rk 	 decreases.
Property 4.4. 	Rk+1 	 ≤ 	Rk 	 for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
	Rk 	2 = min2 	Y − α1 Xk−2 − α2 Mk−1 	2Ω
α∈

= 	Y − (αk−1
Xk−2 + αk−1
Mk−1 )	2Ω
1
2

≥ min2 	Y − α1 (αk−1
Xk−2 + αk−1
Mk−1 ) − α2 Mk 	2Ω
1
2
α∈

= min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

= 	Rk+1 	2 ,
and hence the conclusion holds.
Let
Ak =

BTk Bk

=



Xk−1 , Xk−1  Xk−1 , Mk 
Mk , Xk−1 

Mk , Mk Ω

and Bk = [vec(Xk−1 ), vec((Mk )Ω )]. The solution of problem (4.1) is αk = A−1
k
BTk vec(YΩ ). We next establish that vec(Xk−1 ) and vec((Mk )Ω ) are linearly independent unless 	Rk 	 = 0. It follows that Ak is invertible and hence αk is uniquely
deﬁned before the algorithm stops.
Property 4.5. If Xk−1 = β(Mk )Ω for some β 
= 0, then 	Rk+1 	 = 	Rk 	.
Proof. If Xk−1 = β(Mk )Ω with nonzero β, we get
	Rk+1 	2 = min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

= min2 	Y − (α1 + α2 /β)Xk−1 	2Ω
α∈

= min 	Y − γXk−1 	2Ω
γ∈

= min 	Y − γαk−1
Xk−2 − γαk−1
Mk−1 	2Ω
1
2
γ∈

≥ min2 	Y − γ1 Xk−2 − γ2 Mk−1 	2Ω
γ∈

= 	Y − Xk−1 	2Ω
= 	Rk 	2 ,

and hence the conclusion holds with 	Rk 	2 ≥ 	Rk+1 	2 given in Property 4.4.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A499

Property 4.6. Let σ1 (Rk ) be the maximum singular value of Rk . Mk , Rk  =
for all k ≥ 1.
σ1 (Rk ) ≥ √ Rk 
min(m,n)

Proof. The optimum Mk in our algorithm satisﬁes
Mk , Rk  =

M, Rk  = σ1 (Rk ).

max

rank(M)=1


Using the fact that rank(Rk )σ1 (Rk ) ≥ 	Rk 	 and rank(Rk ) ≤ min(m, n), we get
the conclusion.
Property 4.7. Suppose that Rk 
= 0 for some k ≥ 1. Then, Xk−1 
= β(Mk )Ω
for all β 
= 0.
Proof. If Xk−1 = β(Mk )Ω with β 
= 0, we have
	Rk+1 	2 = 	Y − Xk 	2Ω
= min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

= min2 	Y − (α1 + α2 /β)Xk−1 	2Ω
α∈

= min 	Y − γXk−1 	2Ω
γ∈

= 	Y − γ k Xk−1 	2Ω
= 	Rk 	2
= 	Y − Xk−1 	2Ω .
As Rk 
= 0, we have (Mk )Ω 
= 0 and Xk−1 
= 0. Then from the above equality, we
conclude that γ k = 1 is the unique optimal solution of the minimization in terms of
γ, and thus we obtain its ﬁrst-order optimality condition: Xk−1 , Rk  = 0. However,
this contradicts
Xk−1 , Rk  = βMk , Rk  = βσ1 (Rk ) 
= 0.
This completes the proof.
We next build a relationship between two consecutive residuals 	Rk+1 	 and 	Rk 	.
σ2 (R )
Property 4.8. 	Rk+1 	2 ≤ 	Rk 	2 − M1k ,Mkk 	Ω .
Proof.
	Rk+1 	2 = min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

≤ min 	Y − Xk−1 − α2 Mk 	2Ω
α2 ∈

= min 	Rk − α2 Mk 	2Ω .
α2 ∈

This has a closed form solution as α∗2 =
into the formulation, we get

Rk ,Mk 	
Mk ,Mk 	Ω .

	Rk+1 	2 ≤ 	Rk −

Plugging this optimum α∗2 back

Rk , Mk 
Mk 	2Ω
Mk , Mk Ω

= 	Rk 	2 −

Rk , Mk 2
Mk , Mk Ω

= 	Rk 	2 −

σ12 (Rk )
.
Mk , Mk Ω

This completes the proof.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A500

WANG, LAI, LU, FAN, DAVULCU, AND YE

We are now ready to prove Theorem 4.1.
Proof of Theorem 4.1. Using the deﬁnition of Mk with its normalization property
Mk , Mk Ω ≤ 1, Property 4.8, and Property 4.6, we obtain that
||Rk+1 ||2

σ2 (R )

≤ ||Rk ||2 − M1k ,Mkk 	Ω ≤ ||Rk ||2 − σ12 (Rk )


1
≤
1 − min(m,n)
||Rk ||2 .

In view of this relation and the fact that 	R1 	 = 	Y	2Ω , we easily conclude that

||Rk || ≤

1
1−
min(m, n)

k−1
	Y	Ω .

This completes the proof.
5. An extension to the matrix sensing problem and its convergence
analysis. In this section, we extend our algorithm to deal with the following matrix
sensing problem (cf. [36, 25, 18, 19]):
(5.1)

min

X∈n×m

rank(X) : A(X) = A(Y),

where Y is a target low rank matrix and A is a linear operator, e.g., A consists of a
set of measurements Ai , X = Ai , Y for a sequence of matrices {Ai }. A(X) could
be written in a compact form as
⎤
⎡
vec(A1 )T
⎥
⎢
..
A(X) = ⎣
⎦ vec(X)
.
vec(Ad )T
for d measurements. Clearly, the matrix completion studied in the previous sections
is a special case of the above problem by setting the linear operator A to be the
observation operator PΩ .
We ﬁrst explain how to use our algorithm to solve this matrix sensing problem (5.1). Recall a linear operator vec which maps a matrix X of size n × m to a
vector vec(X) of size mn × 1. We now deﬁne an inverse operator matnm which converts a vector v of size mn × 1 to a matrix V = matnm (v) of size n × m. Note that
when X is vectorized into vec(X), the linear operator A can be expressed in terms
T
of matrix A = [vec(A1 ), . . . , vec(Ad )] . That is, A(X) = A(Y) can be rewritten
as Avec(X) = Avec(Y). For convenience, we can write A = Avec. It is clear that
A is a matrix of size d × mn. Certainly, one can ﬁnd its pseudoinverse A† which
is A
 (AA
 )−1 as we have assumed that A is of full row rank. We note that since
d << mn, AA† = Id while A† A 
= Imn , where Id and Imn are the identity matrices of
size d× d and mn× mn, respectively. For convenience, we let A−1 denote matnm ◦ A† ,
where ◦ is the Hadamard product. The linear operators satisfy
AA−1 b = b
for any vector b of size d × 1, while A−1 A is not an identity operator. We are now
ready to tackle the matrix sensing problem (5.1) as follows: let b = A(Y) = Avec(Y)
and R0 = A−1 (b) be the given matrix. We apply Algorithm 3 to obtain M(θ k ) in
k ≥ r steps.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A501

Algorithm 3. Rank-one matrix pursuit for matrix sensing.
Input: R0 and stopping criterion.
Initialize: Set X0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the residual
matrix Rk by using the power method and set Mk = uk vkT .
Step 2: Compute the weight vector θ k using the closed form least squares approximation of R0 by the best rank-one matrices Mi , i = 1, . . . , k:
θ = arg min 	R0 −
k

θ1 ,...,θk

k


θi A−1 A(Mi )	2F .

i=1

k
Step 3: Set M(θ k ) = i=1 θik Mi , Rk+1 = R0 − A−1 A(M(θ k )) and set k ←
k + 1.
until stopping criterion is satisﬁed
Output: the constructed matrix Ŷ = M(θ k ).

We shall show that M(θ k ) converges to the exact rank-r matrix Y. First of all,
Algorithm 3 can also be proved to be linearly convergent using the same procedure as
in the proof of Theorem 3.1. We thus have the following theorem without presenting
a detailed proof.
Theorem 5.1. Each step in Algorithm 3 satisfies

||Rk || ≤

1
1−
min(m, n)

k−1
	A−1 (b)	

∀k ≥ 1.

This holds for all matrices Y of rank at most r.
We now show M(θ k ) approximates the exact matrix Y for a large k. In the
setting of matrix sensing, we are able to use the rank-RIP condition. Let us recall the
following.
Definition 5.2. Let A be a linear map on linear space of matrices of size n × m
with n ≤ m. For every integer r with 1 ≤ r ≤ n, let the rank-r restricted isometry
constant be the smallest number δr (A) such that
(1 − δr (A))	X	2F ≤ 	A(X)	22 ≤ (1 + δr (A))	X	2F
holds for all matrices X of rank at most r.
It is known that for some random matrices A, A = Avec satisﬁes the rank-RIP
condition with high probability [36]. Armed with the rank-RIP condition, we are able
to establish the following result.
Theorem 5.3. Let Y be a matrix of rank r. Suppose the measurement mapping
A(X) satisfies rank-RIP for rank-r0 with δr0 = δr0 (A) < 1 with r0 ≥ 2r. The output
matrix M(θ k ) from Algorithm 3 approximates the exact matrix Y in the following
sense: there is a positive constant τ < 1 such that
C
τk
	M(θ k ) − Y	F ≤ 
1 − δr 0
for all k = 1, . . . , r0 − r, where C > 0 is a constant dependent on A.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A502

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Proof. Using the deﬁnition of δr0 , for k + r ≤ r0 , we have
(1 − δr0 )	M(θ k ) − Y	2F ≤ 	A(M(θ k )) − A(Y)	22
= 	A(Rk )	22 = 	Avec(Rk )	22
≤ 	A	22 	vec(Rk )	22 = 	A	22 	Rk 	2F
≤ 	A	22 τ 2k 	A−1 (b)	2F ,
where the last inequality follows from Theorem 5.1 with τ =
follows that
	M(θ k ) − Y	2F ≤


1−

1
min{m,n} .

It

	A	22 τ 2k −1
	A (b)	2F .
1 − δr 0

Therefore, we have the desired result.
Similarly we can extend our economic algorithm to the setting of matrix sensing.
We leave it to the interested reader. In the above convergence analysis, we require
k ≤ r0 − r, which guarantees the matrix-RIP condition for all estimated matrices
during the learning process. It will be interesting to explore if a similar result can be
obtained for any k > 0.
6. Eﬀect of inexact top singular vectors. In our rank-one matrix pursuit
algorithms, we need to calculate the top singular vector pair of the residual matrix in
each iteration. We rewrite it here as


(6.1)
max uT Rk v : 	u	 = 	v	 = 1 .
u,v

We solve this problem eﬃciently by the power method, which is an iterative method.
In practice, we obtain a solution with approximation error less than a small tolerance
δk ≥ 0, that is,
(6.2)

ũT Rk ṽ ≥ (1 − δk )

max

{uT Rk v}.

u=v=1

We show that the proposed algorithms still retain the linear convergence rate when
the top singular pair computed at each iteration satisﬁes (6.2) for 0 ≤ δk < 1. This
result is given in the following theorem.
Theorem 6.1. Assume that there is a tolerance parameter 0 ≤ δ < 1 such that
δk ≤ δ for all k. Then the orthogonal rank-one matrix pursuit algorithms achieve a
linear convergence rate

||Rk || ≤

q2
1−
min(m, n)

k−1
	Y	Ω ,

where q = 1 − δ satisfies 0 < q ≤ 1.
Proof. In Step 1 of our algorithms, we iteratively solve the problem (6.1) using
the power method. In this method, we stop the iteration such that
 T

ũTk Rk ṽk ≥ (1 − δk )
u Rk v ≥ 0
max
u=1,v=1

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A503

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

with 0 ≤ δk ≤ δ < 1. Denote M̃k = ũk ṽkT as the generated basis. Next, we show that
the following holds for both OR1MP and EOR1MP:
	Rk+1 	2 ≤ 	Rk 	2 − M̃k , Rk 2 .
For the OR1MP algorithm, we have
	Rk+1 	2 = min 	Y −
θ∈k

k


θi M̃i 	2Ω

i=1

≤ min 	Y − Xk−1 − θk M̃k 	2Ω
θk ∈

= min 	Rk − θk M̃k 	2Ω .
θk ∈

For the EOR1MP algorithm, we have
	Rk+1 	2 = min2 	Y − α1 Xk−1 − α2 M̃k 	2Ω
α∈

≤ min 	Y − Xk−1 − α2 M̃k 	2Ω
α2 ∈

= min 	Rk − α2 M̃k 	2Ω .
α2 ∈

Rk ,M̃k 	
. Plugging the optiIn both cases, we obtain closed form solutions as M̃
k ,M̃k 	Ω
mum solution into the corresponding formulations, we get

	Rk+1 	2 ≤ 	Rk −

Rk , M̃k 
M̃k 	2Ω
M̃k , M̃k Ω

= 	Rk 	2 −

Rk , M̃k 2
M̃k , M̃k Ω
M̃k , M̃k 2Ω

≤ 	Rk 	2 − Rk , M̃k 2 ,

as M̃k , M̃k Ω ≤ 1. It follows from Properties 4.5 and 4.6 that
Rk , M̃k  ≥ (1 − δk )σ1 (Rk ) ≥ (1 − δk ) 

	Rk 	
.
rank(Rk )

Combining the above two results, we get


(1 − δk )2
2
	Rk+1 	 ≤ 1 −
	Rk 	2 .
min(m, n)
In view of this relation and the fact that 	R1 	 = 	Y	2Ω , we conclude that

||Rk || ≤

q2
1−
min(m, n)

k−1
	Y	Ω ,

where q = 1 − δ ≤ inf(1 − δk ) = 1 − sup δk and is a constant between (0, 1]. This
completes the proof.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A504

WANG, LAI, LU, FAN, DAVULCU, AND YE

7. Experiments. In this section, we compare the two versions of our algorithm,
e.g., OR1MP and EOR1MP, with several state-of-the-art matrix completion methods
in the literature. The competing algorithms include SVP [18], SVT [7], Jaggi’s fast
algorithm for trace norm constraint (JS) [17], the spectral regularization algorithm
(SoftImpute) [30], low rank matrix ﬁtting (LMaFit) [46], a boosting-type accelerated
matrix-norm penalized solver (Boost) [49], atomic decomposition for minimum rank
approximation (ADMiRA) [25], and GECO [38]. The ﬁrst three solve trace norm
constrained problems; the next three solve trace norm penalized problems; the last
two directly solve the low rank constrained problem. The general greedy method [42]
is not included in our comparison, as it includes JS and GECO (included in our
comparison) as special cases for matrix completion. The lifted coordinate descent
method [9] is not included in our comparison as it is sensitive to the parameters and
is less eﬃcient than Boost proposed in [49].
The codes for most of these methods are available online:
• SVP, http://www.cs.utexas.edu/∼pjain/svp;
• SVT, http://svt.stanford.edu;
• SoftImpute, http://www-stat.stanford.edu/∼rahulm/software.html;
• LMaFit, http://lmaﬁt.blogs.rice.edu;
• Boost, http://webdocs.cs.ualberta.ca/∼xinhua2/boosting.zip;
• GECO, http://www.cs.huji.ac.il/∼shais/code/geco.zip.
We compare these algorithms in two applications: image recovery and collaborative ﬁltering or recommendation problem. The data size for image recovery is
relatively small, and the recommendation problem is large-scale. All the competing
methods are implemented in MATLAB1 and call some external packages for fast computation of SVD2 and sparse matrix computations. The experiments are run on a PC
with the windows 7 system, Intel 4 core 3.4-GHz CPU, and 8G RAM.
In the following experiments, we follow the recommended settings of the parameters for the competing algorithms. If no recommended parameter value is available,
we choose the best one from a candidate set using cross validation. For our OR1MP
and EOR1MP algorithms, we only need a stopping criterion. For simplicity, we stop
our algorithms after r iterations. In this way, we approximate the ground truth using
a rank-r matrix. We present the experimental results using two metrics, peak signalto-noise ratio (PSNR) [16] and root-mean-square error (RMSE) [22]. PSNR is a test
metric speciﬁc for images. A higher value in PSNR generally indicates better quality
[16]. RMSE is a general metric for prediction. It measures the approximation error
of the corresponding result.
7.1. Convergence and eﬃciency. Before we present the numerical results
from these comparison experiments, we shall include another algorithm called the
forward rank-one matrix pursuit algorithm (FR1MP), which extends the matching
pursuit method from the vector case to the matrix case. The detailed procedure of
this method is given in Algorithm 4.
In FR1MP, we add the pursued rank-one matrix with an optimal weight in each iteration, which is similar to the forward selection rule [14]. This is a standard algorithm
to ﬁnd SVD of any matrix Y if all its entries are given. In this case, the FR1MP
algorithm is more eﬃcient in ﬁnding SVD of the matrix than our two proposed al1 GECO

is written in C++ and we call its executable ﬁle in MATLAB.
is used in SVP, SVT, SoftImpute and Boost. It is an eﬃcient SVD package,
which is implemented in C and Fortran. It can be downloaded from http://soi.stanford.edu/∼
rmunk/PROPACK.
2 PROPACK

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A505

Algorithm 4. FR1MP.
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk vkT .
Step 2: Set θkk = (uTk Rk vk )/	Mk 	Ω , and θik = θik−1 for i ≤ k − 1.
Step 3: Set Xk = Xk−1 + θkk (Mk )Ω ; k ← k + 1.
until stopping criterion is satisﬁed
k
Output: Constructed matrix Ŷ = i=1 θik Mi .

gorithms. However, when only partial entries are known, the FR1MP algorithm will
not be able to ﬁnd the best low rank solution. The computational step to ﬁnd θ k in
our proposed algorithms is necessary.
The empirical results for convergence eﬃciency of our proposed algorithms are
reported in Figures 1 and 2. They are based on an image recovery experiment as
well as an experiment of a movie recommendation dataset, Netﬂix [22, 4, 5]. The
Netﬂix dataset has 108 ratings of 17,770 movies by 480,189 Netﬂix3 customers. This
is a large-scale dataset, and most of the competing methods are not applicable for
this dataset. In Figure 1, we present the convergence characteristics of the proposed
OR1MP algorithm. As the memory demand is increasing w.r.t. the iterations, we can
run it for only about 40 iterations on the Netﬂix dataset. The EOR1MP algorithm
has no such limitation. The results in Figure 2 show that our EOR1MP algorithm
rapidly reduces the approximation error. We also present the same residual curves in
logarithmic scale with a relatively large number of iterations in Figure 3, which verify
the linear convergence property of our algorithms. These results are consistent with
our theoretical analysis.
In the convergence analysis, we derive the upper bound for the convergence
speed of our proposed algorithms. From Theorems 3.1 and 4.1, the convergence
2
, where σk,∗ is the maximum singuspeed is controlled by the value of 	Rk 	2F /σk,∗
lar value of the residual matrix Rk in the kth iteration. A smaller value indicates
a faster convergence of our algorithms. Though it has a worst-case upper bound of
2
≤ rank(Rk ) ≤ min(m, n), in the following experiments, we empirically
	Rk 	2F /σk,∗
verify that its value is much smaller than the theoretical worst case. Thus the convergence speed of our algorithms is much faster than the theoretical worst case. We
2
at diﬀerent iterations on the Lenna image and the
present the values of 	Rk 	2F /σk,∗
MovieLens1M dataset for both of our algorithms in Figure 4. The results show that
2
is much smaller than min(m, n).
the quantity 	Rk 	2F /σk,∗
In the following experiments, we plot the residual curves over iterations for different rank-one matrix pursuit algorithms, including our OR1MP algorithm, our
EOR1MP algorithm, and the FR1MP algorithm. The evaluations are conducted
on the Lenna image and the MovieLens1M dataset, which are given in Figure 5.
The results show that among the three algorithms, EOR1MP and OR1MP perform
better than the forward pursuit algorithm. It is interesting to note that EOR1MP
achieves a similar performance as OR1MP, while it demands much less computational
cost.
3 http://www.netﬂixprize.com.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A506

WANG, LAI, LU, FAN, DAVULCU, AND YE
Lenna

Lenna
7
6

Time (seconds)

0.04
0.035

RMSE

0.03
0.025
0.02
0.015

5
4
3

0.01

2

0.005

1

0
0

100

200

0
0

300

50

100

150

rank

200

250

300

350

rank

Netflix

Netflix
8000

0.02

7000

Time (seconds)

RMSE

0.018
0.016
0.014
0.012

6000
5000
4000
3000
2000
1000

0.01
0

10

20

30

0
0

40

10

20

30

40

rank

rank

Fig. 1. Illustration of convergence of the proposed OR1MP algorithm on the Lenna image and
the Netﬂix dataset: the x-axis is the rank, the y-axis is the RMSE (left column), and the running
time is measured in seconds (right column).
Lenna

Lenna

0.04

1
0.9

0.035

0.8
0.03
Time (seconds)

0.7
RMSE

0.025
0.02
0.015

0.6
0.5
0.4
0.3

0.01
0.2
0.005
0
0

0.1
50

100

150
200
Iteration

250

0
0

300

50

100

150
200
Iteration

0.02

4500

0.019

4000

0.018

3500
Time (seconds)

0.017
0.016
0.015
0.014
0.013

300

3000
2500
2000
1500
1000

0.012

500

0.011
0.01
0

250

Netflix

Netflix

RMSE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

8

20

40

60
Iteration

80

100

0
0

20

40

60
Iteration

80

100

Fig. 2. Illustration of convergence of the proposed EOR1MP algorithm on the Lenna image and
the Netﬂix dataset: the x-axis is the rank, the y-axis is the RMSE (left column), and the running
time is measured in seconds (right column).

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A507

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION
Netflix

Lenna

−1

RMSE

RMSE

−2

10

−2

10

−3

10

0

50

100

150

200

250

300

0

5

10

15

Lenna

−1

20

25

30

35

40

rank

rank

Netflix

RMSE

10

RMSE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

10

−2

10

−3

10

−2

0

50

100

150

200

250

10

300

20

40

rank

60

80

100

rank

Fig. 3. Illustration of the linear convergence of diﬀerent rank-one matrix pursuit algorithms
on the Lenna image and the Netﬂix dataset: the x-axis is the iteration, and the y-axis is the RMSE
in log scale. The curves in the ﬁrst row are the results for OR1MP and the curves in the second
row are the results for EOR1MP.
OR1MP on MovieLens1M

OR1MP on Lenna
5000
2

||R|| /
600

σ2*

min(m,n)

||R||2 / σ2*
min(m,n)

4000

500
3000

400
300

2000

200
1000
100
0
0

20

40
60
Iteration

80

100

0
0

EOR1MP on Lenna

10

20
30
Iteration

40

50

EOR1MP on MovieLens1M
5000
||R||2 / σ2*

600

min(m,n)

||R||2 / σ2*
min(m,n)

4000

500
3000

400
300

2000

200
1000
100
0
0

20

40
60
Iteration

80

100

0
0

10

20
30
Iteration

40

50

Fig. 4. Illustration of the values of R2 /σ∗2 at diﬀerent iterations and the value of min(m, n)
on the Lenna image and MovieLens1M for both R1MP and ER1MP algorithms: the x-axis is the
iteration number; the y-axis is the value.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A508

WANG, LAI, LU, FAN, DAVULCU, AND YE
Lenna

MovieLens1M
OR1MP
EOR1MP
FR1MP

0.035

0.017
0.0165
0.016

0.025

RMSE

RMSE

OR1MP
EOR1MP
FR1MP

0.0175

0.03

0.02

0.0155
0.015
0.0145

0.015

0.014
0.01

0.0135

0.005
0

20

40

60
Iteration

80

0.013
0

100

20

40

60
Iteration

80

100

Fig. 5. Illustration of convergence speed of diﬀerent rank-one matrix pursuit algorithms on the
Lenna image and the MovieLens1M dataset: the x-axis is the iteration; the y-axis is the RMSE.
OR1MP on MovieLens1M

EOR1MP on MovieLens1M
iteration = 1
iteration = 2
iteration = 5
iteration = 10
iteration = 20

0.018

RMSE

0.017

0.016

0.016

0.015

0.015

0.014

0.014

0.013
0

iteration = 1
iteration = 2
iteration = 5
iteration = 10
iteration = 20

0.018

0.017
RMSE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

0.04

20

40

60
Iteration

80

100

0.013
0

20

40

60
Iteration

80

100

Fig. 6. Illustration of convergence property of the proposed algorithms with diﬀerent iteration
numbers in the power method on the MovieLens1M dataset: the x-axis is the outer iteration number;
the y-axis is the RMSE.

7.2. Inexact top singular vectors. We empirically analyze the performance
of our algorithms with inexact singular vector computation. In the experiments, we
control the total number of iterations in the power method for computing the top
singular vector pair. The numbers of iterations are set as {1, 2, 5, 10, 20}. We plot
the learning curves for the OR1MP and EOR1MP algorithms on the MovieLens1M
dataset in Figure 6. The results show that the linear convergence speed is preserved
for diﬀerent iteration numbers. However, the results under the same outer iterations
depend on the accuracy of the power methods. This veriﬁes our theoretical results.
Our empirical results also suggest that in practice we need to run more than 5 iterations in the power method, as the learning curves for 5, 10, and 20 power method
iterations are close to each other but are far away from the other two curves, especially
for the EOR1MP algorithm.
7.3. Recovery on synthetic data. In this experiment, we use synthetic data
to evaluate the recovery performance of diﬀerent matrix completion algorithms. We
generate a square n × n matrix Y of rank r as the ground truth. We construct Y by
ﬁrst generating a random matrix with i.i.d. entries drawn from the standard normal
distribution and then setting its ith singular value to 2r−i+1 . Given this matrix Y, we

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A509

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

Recovery Error

SVT
SVP
LMaFit
ADMiRA
SoftImpute
JS
OR1MP
EOR1MP

0.6
0.4
0.2
0
0

0.1

0.2

0.3

0.4

0.5
0.6
Observation Ratio

0.7

0.8

0.9

1

Recovery Error

1
SVT
SVP
LMaFit
ADMiRA
SoftImpute
JS
OR1MP
EOR1MP

0.8
0.6
0.4
0.2
0
0

0.1

0.2

0.3

0.4

0.5
0.6
Observation Ratio

0.7

0.8

0.9

1

1
Recovery Error

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1
0.8

SVT
SVP
LMaFit
ADMiRA
SoftImpute
JS
OR1MP
EOR1MP

0.8
0.6
0.4
0.2
0
0

0.1

0.2

0.3

0.4

0.5
0.6
Observation Ratio

0.7

0.8

0.9

1

Fig. 7. Comparison of recovery performance of diﬀerent matrix completion algorithms with
diﬀerent percentages of observations: the three ﬁgures correspond to the results on three rank-10
random matrices of size 50 × 50 without noise (top ﬁgure), size 100 × 100 without noise (middle
ﬁgure), and size 100 × 100 with Gaussion noise (bottom ﬁgure); the x-axis is the percentage of
observations; the y-axis is the recovery error.

sample a subset Ω of l entries uniformly at random as the observations. We run the
experiment in two diﬀerent settings—noise-free matrix completion and noisy matrix
completion. We ﬁx the rank of the ground truth matrices as r = 10 in all experiments.
In the noise-free case, we use two diﬀerent matrix sizes in the experiment: n = 50 and
n = 100. In the noisy case, we use n = 100 with 5% Gaussion noise. The entries of
the noise matrix are drawn from the standard normal distribution and are normalized
to make the matrix Frobenius norm equal to 0.05	Y	F . We evaluate the recovery
performance of the algorithms based on the relative reconstruction error calculated as
Y−ŶF
YF , with Ŷ as the reconstructed matrix. In the experiment, we ﬁx the number
of iterations to 200 for the JS algorithm. For OR1MP and EOR1MP, we stop the
algorithm after 50 iterations. For other algorithms, we use the true rank r = 10 for
the estimated matrix.
For each algorithm, we present its average result with 50 runs at diﬀerent percentages of observations in Figure 7. We can observe from the ﬁgure that for most
algorithms the recovery error decreases with an increasing number of observations.
The proposed algorithms are very competitive in most cases, particularly when the
observations are scarce.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A510

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 1
Image recovery results measured in terms of the PSNR.
Dataset
Barbara
Cameraman
Clown
Couple
Crowd
Girl
Goldhill
Lenna
Man
Peppers

SVT
26.9635
25.6273
28.5644
23.1765
26.9644
29.4688
28.3097
28.1832
27.0223
25.7202

SVP
SoftImpute LMaFit ADMiRA
JS
OR1MP EOR1MP
25.2598
25.6073
25.9589 23.3528 23.5322 26.5314
26.4413
25.9444
26.7183
24.8956 26.7645 24.6238 27.8565 27.8283
19.0919
26.9788
27.2748 25.7019 25.2690 28.1963
28.2052
23.7974
26.1033
25.8252 25.6260 24.4100 27.0707 27.0310
22.2959
25.4135
26.0662 24.0555 18.6562 26.0535
26.0510
27.5461
27.7180
27.4164 27.3640 26.1557 30.0878 30.0565
16.1256
27.1516
22.4485 26.5647 25.9706 28.5646 28.5101
25.4586
26.7022
23.2003 26.2371 24.5056 28.0115
27.9643
25.3246
25.7912
25.7417 24.5223 23.3060 26.5829
26.5049
26.0223
26.8475
27.3663 25.8934 24.0979 28.0781 28.0723

7.4. Image recovery. In the image recovery experiments, we use the following
benchmark test images: Barbara, Cameraman, Clown, Couple, Crowd, Girl, Goldhill,
Lenna, Man, and Peppers.4 The size of each image is 512×512. We randomly exclude
50% of the pixels in the image, and the remaining ones are used as the observations.
As the image matrix is not guaranteed to be low rank, we use rank 50 for the estimated
matrix for each experiment. In our OR1MP and EOR1MP algorithms, we stop the
algorithms after 150 iterations. The JS algorithm does not explicitly control the
rank, thus we ﬁx its number of iterations to 2000. The numerical results in terms of
the PSNR are listed in Table 1. We also present the images recovered by diﬀerent
algorithms for Lenna in Figure 8. The results show SVT, our OR1MP, and EOR1MP
achieve the best numerical performance. However, our algorithm is much better than
SVT for Cameraman, Couple, Peppers but only slightly worse than SVT for Lenna,
Barbara, and Clown. Besides, our algorithm is much faster and more stable than SVT
(SVT may diverge). For each image, EOR1MP uses around 3.5 seconds, but SVT
consumes around 400 seconds. Image recovery needs a relatively higher approximation
rank; both GECO and Boost fail to ﬁnd a good recovery in most cases, so we do not
include them in the result tables.
7.5. Recommendation. In the following experiments, we compare diﬀerent
matrix completion algorithms using large recommendation datasets: Jester [12] and
MovieLens [31]. We use six datasets: Jester1, Jester2, Jester3, MovieLens100K,
MovieLens1M, and MovieLens10M. The statistics of these datasets are given in Table 2. The Jester datasets were collected from a joke recommendation system. They
contain anonymous ratings of 100 jokes from the users. The ratings are real values
ranging from −10.00 to +10.00. The MovieLens datasets were collected from the
MovieLens website.5 They contain anonymous ratings of the movies on this web
made by its users. For MovieLens100K and MovieLens1M, there are 5 rating scores
(1–5), and for MovieLens10M there are 10 levels of scores with a step size 0.5 in the
range of 0.5 to 5. In the following experiments, we randomly split the ratings into
training and test sets. Each set contains 50% of the ratings. We compare the running
time and the prediction result from diﬀerent methods. In the experiments, we use
100 iterations for the JS algorithm, and for other algorithms we use the same rank
for the estimated matrices; the values of the rank are {10, 10, 5, 10, 10, 20} for the
six corresponding datasets. We ﬁrst show the running time of diﬀerent methods in
4 Images

are downloaded from http://www.utdallas.edu/∼ cxc123730/mh bcs spl.html.

5 http://movielens.umn.edu.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

Original

SVT

SVP

SoftImpute

LMafit

ADMiRA

JS

OR1MP

EOR1MP

A511

Fig. 8. The original image and images recovered by diﬀerent methods used on the Lenna image.
Table 2
Characteristics of the recommendation datasets.
Dataset

# row

# column

# rating

Jester1
Jester2
Jester3
MovieLens100k
MovieLens1M
MovieLens10M

24983
23500
24983
943
6040
69878

100
100
100
1682
3706
10677

106
106
6×105
105
106
107

Table 3. The reconstruction results in terms of the RMSE are given in Table 4. We
can observe from the above experiments that our EOR1MP algorithm is the fastest
among all competing methods to obtain satisfactory results.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A512

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 3
The running time (measured in seconds). Boost fails on MovieLens10M.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
18.35
16.85
16.58
1.32
18.90
> 103

SoftImpute
161.49
152.96
10.55
128.07
59.56
> 103

LMaFit
3.68
2.42
8.45
2.76
30.55
154.38

Boost
93.91
261.70
245.79
2.87
93.91
–

JS

GECO

OR1MP

EOR1MP

29.68
28.52
12.94
2.86
13.10
130.13

>
> 104
> 103
10.83
> 104
> 105

1.83
1.68
0.93
0.04
0.87
23.05

0.99
0.91
0.34
0.04
0.54
13.79

OR1MP
4.3418
4.3649
4.9783
1.0168
0.9595
0.8621

EOR1MP
4.3384
4.3546
5.0145
1.0261
0.9462
0.8692

104

Table 4
Recommendation results measured in terms of the RMSE.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
4.7311
4.7608
8.6958
0.9683
0.9085
0.8611

SoftImpute
5.1113
5.1646
5.4348
1.0354
0.8989
0.8534

LMaFit
4.7623
4.7500
9.4275
1.2308
0.9232
0.8625

Boost
5.1746
5.2319
5.3982
1.1244
1.0850
–

JS
4.4713
4.5102
4.6866
1.0146
1.0439
0.8728

GECO
4.3680
4.3967
5.1790
1.0243
0.9290
0.8668

8. Conclusion. In this paper, we propose an eﬃcient and scalable low rank
matrix completion algorithm. The key idea is to extend the OMP method from the
vector case to the matrix case. We also propose a novel weight updating rule under
this framework to reduce the storage complexity and make it independent of the
approximation rank. Our algorithms are computationally inexpensive for each matrix
pursuit iteration and ﬁnd satisfactory results in a few iterations. Another advantage
of our proposed algorithms is they have only one tunable parameter, which is the rank.
It is easy to understand and to use by the user. This becomes especially important in
large-scale learning problems. In addition, we rigorously show that both algorithms
achieve a linear convergence rate, which is signiﬁcantly better than the previous known
results (a sublinear convergence rate). We also extend our proposed algorithm to a
more general matrix sensing case and analyze its recovery guarantee under rankrestricted isometry property. We empirically compare the proposed algorithms with
state-of-the-art matrix completion algorithms, and our results show that the proposed
algorithms are more eﬃcient than competing algorithms while achieving similar or
better prediction performance. We plan to generalize our theoretical and empirical
analysis to other loss functions in the future.
Appendix A. Inverse matrix update. In our OR1MP algorithm, we use
the least squares solution to update the weights for the rank-one basis matrices.
In this step, we need to calculate (M̄Tk M̄k )−1 . To directly compute this inverse is
computationally expensive, as the matrix M̄k has a large row size. We implement
this eﬃciently using an incremental method. As
M̄Tk M̄k = [M̄k−1 , ṁk ]T [M̄k−1 , ṁk ],
its inverse can be written in block matrix form:
(M̄Tk M̄k )−1

 T
M̄k−1 M̄k−1
=
ṁTk M̄Tk−1

M̄Tk−1 ṁk
ṁTk ṁk

−1
.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A513

Then it is calculated by blockwise inverse as


A + dAbbT A −dAb
,
−dbT A
d
where A = (M̄Tk−1 M̄k−1 )−1 is the corresponding inverse matrix in the last step, b =
M̄Tk−1 ṁk is a vector with |Ω| elements, and d = (bT b− bT Ab)−1 = 1/(bT b− bT Ab)
is a scalar. M̄Tk ẏ is also calculated incrementally by [M̄Tk−1 ẏ, ṁTk ẏ], as ẏ is ﬁxed.
REFERENCES
[1] A. Argyriou, T. Evgeniou, and M. Pontil, Convex multi-task feature learning, Mach.
Learn., 73 (2008), pp. 243–272.
[2] F. Bach, Consistency of trace norm minimization, J. Mach. Learn. Res., 9 (2008), pp. 1019–
1048.
[3] L. Balzano, R. Nowak, and B. Recht, Online identiﬁcation and tracking of subspaces from
highly incomplete information, in Proceedings of the Allerton Conference on Communication, Control and Computing, 2010.
[4] R. Bell and Y. Koren, Lessons from the netﬂix prize challenge, ACM SIGKDD Explorations,
9 (2007), pp. 75–79.
[5] J. Bennett and S. Lanning, The netﬂix prize, in Proceedings of KDD Cup and Workshop,
2007.
[6] J.-F. Cai, E. J. Candès, and Z. Shen, A singular value thresholding algorithm for matrix
completion, SIAM J. Optim., 20 (2010), pp. 1956–1982.
[7] E. J. Candès and B. Recht, Exact matrix completion via convex optimization, Found. Comput. Math., 9 (2009), pp. 717–772.
[8] R. A. DeVore and V. N. Temlyakov, Some remarks on greedy algorithms, Adv. Comput.
Math., 5 (1996), pp. 173–187.
[9] M. Dudı́k, Z. Harchaoui, and J. Malick, Lifted coordinate descent for learning with tracenorm regularization, in Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2012.
[10] M. Frank and P. Wolfe, An algorithm for quadratic programming, Naval Res. Logist. Quart.,
3 (1956), pp. 95–110.
[11] J. H. Friedman, T. Hastie, and R. Tibshirani, Regularization paths for generalized linear
models via coordinate descent, J. Statist. Software, 33 (2010), pp. 1–22.
[12] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, Eigentaste: A constant time collaborative ﬁltering algorithm, Inform. Retrieval, 4 (2001), pp. 133–151.
[13] G. H. Golub and C. F. V. Loan, Matrix Computations, 3rd ed., Johns Hopkins University
Press, Baltimore, MD, 1996.
[14] T. Hastie, R. Tibshirani, and J. H. Friedman, The elements of statistical learning: Data
mining, inference, and prediction, Springer-Verlag, New York, 2009.
[15] E. Hazan, Sparse approximate solutions to semideﬁnite programs, in Proceedings of the 8th
Latin American Conference on Theoretical Informatics, 2008.
[16] Q. Huynh-Thu and M. Ghanbari, Scope of validity of psnr in image/video quality assessment,
Electron. Lett., 44 (2008), pp. 800–801.
[17] M. Jaggi and M. Sulovský, A simple algorithm for nuclear norm regularized problems, in
Proceedings of the 27th International Conference on Machine Learning (ICML), 2010,
pp. 471–478.
[18] P. Jain, R. Meka, and I. S. Dhillon, Guaranteed rank minimization via singular value
projection, Adv. Neural Inf. Process. Syste. 22 (2010), pp. 937–945.
[19] P. Jain, P. Netrapalli, and S. Sanghavi, Low-rank matrix completion using alternating
minimization, in Proceedings of the 45th Annual ACM Symposium on Symposium on
Theory of Computing (STOC), 2013, pp. 665–674.
[20] S. Ji and J. Ye, An accelerated gradient method for trace norm minimization, in Proceedings
of the 26th International Conference on Machine Learning (ICML), 2009, pp. 457–464.
[21] R. Keshavan and S. Oh, Optspace: A Gradient Descent Algorithm on the Grassmann Manifold
for Matrix Completion, http://arxiv.org/abs/0910.5260 (2009).
[22] Y. Koren, Factorization meets the neighborhood: A multifaceted collaborative ﬁltering model,
in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2008.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A514

WANG, LAI, LU, FAN, DAVULCU, AND YE

[23] Y. Koren, R. Bell, and C. Volinsky, Matrix factorization techniques for recommender
systems, Computer, 42 (2009), pp. 30–37.
[24] M.-J. Lai, Y. Xu, and W. Yin, Improved iteratively reweighted least squares for unconstrained
smoothed q minimization, SIAM J. Numer. Anal., 51 (2013), pp. 927–957.
[25] K. Lee and Y. Bresler, Admira: atomic decomposition for minimum rank approximation,
IEEE Trans. Inform. Theory, 56 (2010), pp. 4402–4416.
[26] E. Liu and T. N. Temlyakov, The orthogonal super greedy algorithm and applications in
compressed sensing, IEEE Trans. Inform. Theory, 58 (2012), pp. 2040–2047.
[27] Y.-J. Liu, D. Sun, and K.-C. Toh, An implementable proximal point algorithmic framework
for nuclear norm minimization, Math. Program., 133 (2012), pp. 399–436.
[28] Z. Lu and Y. Zhang, Penalty Decomposition Methods for Rank Minimization,
http://arxiv.org/abs/1008.5373 (2010).
[29] S. Ma, D. Goldfarb, and L. Chen, Fixed point and bregman iterative methods for matrix
rank minimization, Math. Program., 128 (2011), pp. 321–353.
[30] R. Mazumder, T. Hastie, and R. Tibshirani, Spectral regularization algorithms for learning
large incomplete matrices, J. Mach. Learn. Res., 99 (2010), pp. 2287–2322.
[31] B. N. Miller, I. Albert, S. K. Lam, J. A. Konstan, and J. Riedl, MovieLens unplugged:
Experiences with an occasionally connected recommender system, in Proceedings of the 8th
International Conference on Intelligent User Interfaces, 2003, pp. 263–266.
[32] B. Mishra, G. Meyer, F. Bach, and R. Sepulchre, Low-rank optimization with trace norm
penalty, SIAM J. Optim., 23 (2013), pp. 2124–2149.
[33] D. Needell and J. A. Tropp, Cosamp: Iterative signal recovery from incomplete and inaccurate samples, Comm. ACM, 53 (2010), pp. 93–100.
[34] S. Negahban and M. Wainwright, Estimation of (near) low-rank matrices with noise and
high-dimensional scaling, in Proceedings of the 27th International Conference on Machine
Learning (ICML), 2010.
[35] Y. C. Pati, R. Rezaiifar, Y. C. P. R. Rezaiifar, and P. S. Krishnaprasad, Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition, in Proceedings of the 27th Annual Asilomar Conference on Signals, Systems, and
Computers, 1993, pp. 40–44.
[36] B. Recht, M. Fazel, and P. A. Parrilo, Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization, SIAM Rev., 52 (2010), pp. 471–501.
[37] B. Recht and C. Ré, Parallel stochastic gradient algorithms for large-scale matrix completion,
Math. Program. Comput., 5 (2013), pp. 201–226.
[38] S. Shalev-Shwartz, A. Gonen, and O. Shamir, Large-scale convex minimization with a lowrank constraint, in Proceedings of the 28th International Conference on Machine Learning
(ICML), 2011, pp. 329–336.
[39] S. Shalev-Shwartz and A. Tewari, Stochastic methods for l1 regularized loss minimization,
in Proceedings of the 26th International Conference on Machine Learning (ICML), 2009,
pp. 929–936.
[40] N. Srebro, J. Rennie, and T. Jaakkola, Maximum-margin matrix factorizations, Adv. Neural Inf. Process. Syst., 17 (2004), pp. 1329–1336.
[41] V. N. Temlyakov, Greedy approximation, Acta Numer., 17 (2008), pp. 235–409.
[42] A. Tewari, P. Ravikumar, and I. S. Dhillon, Greedy algorithms for structurally constrained
high dimensional problems, Adv. Neural Inf. Process. Syst., 24 (2011), pp. 882–890.
[43] R. Tibshirani, Regression shrinkage and selection via the lasso, J. R. Stat. Soc. Ser. B, 58
(1994), pp. 267–288.
[44] K.-C. Toh and S. Yun, An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems, Paciﬁc J. Optim., 6 (2010), pp. 615–640.
[45] J. A. Tropp, Greed is good: Algorithmic results for sparse approximation, IEEE Trans. Inform.
Theory, 50 (2004), pp. 2231–2242.
[46] Z. Wen, W. Yin, and Y. Zhang, Solving a low-rank factorization model for matrix completion
by a nonlinear successive over-relaxation algorithm, Math. Program. Comput., 4 (2012),
pp. 333–361.
[47] T. T. Wu and K. Lange, Coordinate descent algorithms for lasso penalized regression, Ann.
Appl. Stat., 2 (2008), pp. 224–244.
[48] S. Yun and K.-C. Toh, A coordinate gradient descent method for l1-regularized convex minimization, Comput. Optim. Appl., 48 (2011), pp. 273–307.
[49] X. Zhang, Y. Yu, and D. Schuurmans, Accelerated training for matrix-norm regularization:
A boosting approach, Adv. Neural Inf. Process. Syst., 25 (2012), pp. 2906–2914.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Perspective Analysis for Online Debates
Sukru Tikves, Sedat Gokalp, Mhamed Temkit, Sujogya Banerjee, Jieping Ye, Hasan Davulcu
Arizona State University, P.O. Box 87-8809, Tempe, AZ, 85281 USA
{Sukru.Tikves, Sedat.Gokalp, Mhamed.Temkit, Sujogya.Banerjee, Jieping.Ye, Hasan.Davulcu}@asu.edu
Tel:(602) 206-8641, Fax: (480) 965-2751

Abstract—Internet and social media devices created a new
public space for online debate on political and social topics.
A debate is defined as a formal discussion on a set of related
topics in a public meeting, in which opposing perspectives and
arguments are put forward. In this paper, we develop automated
perspective discovery techniques which would contribute to the
understanding of features (i.e. social, political, cultural, religious
beliefs, goals, and practices) shared by each side of the debate.
Secondly, we show that, compared to a semi-automated process,
our perspective discovery algorithms not only identify larger
number of relevant features, but they also yield a higher accuracy
scaling of moderate to extreme organizations on both sides of a
debate.

I. I NTRODUCTION
Internet and social media devices created a new public space
for debate on political and social topics [1], [2]. Hotly debated
issues span all spheres of human activity; from liberal vs.
conservative politics, to radical vs. counter-radical religious
debate, to climate change debate in scientific community.
Many prominent ’camps’ have emerged within Internet debate
rhetoric and practice [3]. There are many applications [4]–[8]
for recognizing politically-oriented sentiment in texts.
A debate is defined as a formal discussion on a set of related
topics in a public meeting, in which opposing arguments
are put forward. Initially, we observe that given a certain
topic, each organization’s web site mostly discusses their own
perspectives related to that topic, and occasionally discusses
others’ perspectives, relating them back to their own perspectives. As a case study of an ongoing large scale online debate,
we utilize the discourse found in the web sites of 10 radical,
and 13 counter-radical Indonesian religious organizations comprising a total of 37,000 articles dating from 2001 to 2011.
Radicalism [9] is the ideological conviction that it is acceptable
and in some cases obligatory to use violence to effect profound
political, cultural and religious transformations and change the
existing social order. Counter-radicals oppose violent social
and political movements.
In our prior work [10] we showed that both counter-radical
and radical movements in Muslim societies exhibit distinct
combinations of perspectives on various social, political, and
religious issues, and those perspectives can be mapped to
a latent linear continuum, or a scale. The resulting model
allowed us to measure the distance between organizations
and movements over the underlying scale. It also facilitates
tracking the ways in which movements and organizations
change over time and space [11].

In [10], we utilized a simple term frequency - inverse document frequency (TF-IDF) [12] based technique to generate a
large candidate list of topics and perspectives for inclusion in
scaling analysis. Top 100 n-grams from each organization’s
web site were collected into a list of candidate keywords.
Next, we asked social scientists to scan this list manually, and
identify all significant keywords belonging to social, political,
economic, and religious perspectives. During this process,
social scientists on our team assessed a total of 790 candidate
keywords; of which 29 and 26 were selected by experts for
inclusion in the radical and counter-radical scaling analysis
respectively.
Upon analyzing the results of this study, we have identified
that automatically generating the items of the radical and
counter-radical scales would be an important contribution to
the research. For example, among the included scale items
were phrases like “religious education”. However, reaching
that item from a seed topic (like “education”), instead of
manual selection would be desirable. This would not only
decrease the expert intervention in scale generation, it would
also provide us with useful perspective of organizations on
these topics aligned with the underlying scale. In order to
explore this idea, we have developed methods for perspective
analysis built upon previous findings of the scaling research.
In this paper, our primary contribution is the development
of automated perspective discovery techniques which would
contribute to the understanding of features (i.e. social, political, cultural, religious beliefs, goals, and practices) shared by
one side of a debate, and by those opposing them. Secondly,
we show that, our perspective discovery algorithms not only
identify larger number of relevant features - compared to the
semi-automated process, but also yield a higher accuracy scale
of radicalism vs. counter-radicalism.
II. P ROBLEM D EFINITION
In this study, we would like to automatically discover the
perspectives of organizations on a given set of topics, and
extract the underlying discourse scale. Here, a scale is a social
science model to measure the positioning of organizations on
a latent dimension. We would like this generated scale to fit
both the theoretical model, and also be able to mimic expert
level deduction in the domain.
For our specific instance, we have utilized Rasch model [13]
as the probabilistic version of a Guttman scale, and web
mining techniques for producing response tables from organizations’ corpora. We have utilized the opinion of our experts

on Indonesian Islamic religious organizations as the target gold
standard of orderings of these organizations.
III. M ETHODS AND T HEORY
A. Overall System Model
We devised an end-to-end pipeline of methods that would
generate two sets of perspectives for each of the polarities of
a scale, starting from a mined web corpus, and a list of topics
provided by experts.
Overall flow of the system consists of data gathering from
experts, and web mining, manual topic selection, perspective
analysis, and scale generation.
B. Scaling
In social science scaling is a process of measuring and
ordering entities such as subjects based on their qualitative
attributes called items. In general, subjects respond to surveys
in form of interviews or questionnaires, where items are
presented to the subjects in form of questions. Some of the
widely followed scaling procedures in social science are Likert
scale [14], Thurstone scale [15], and Guttman scale [16].
Guttman scaling procedure orders both the subjects and the
items simultaneously with respect to some underlying latent
cumulative continuum. In this study, items tend to have a
natural total ordering to partial ordering, since an organization
support for keyword such as “Sharia” will most likely imply
their support for the keyword “Quran”, we used the Guttman
scaling to rank the organizations based on their response on
the radical and counter-radical keywords.
A Guttman [17] scale presents a number of items to which
each subject is requested to provide a dichotomous response,
e.g. agree/disagree, yes/no, or 1/0. This scaling procedure is
based on the premise that the items have strict orders (i.e., the
items are presented to the subjects ranked according to the
level of the item’s difficulty). An item “A” is said to be “more
difficult” than an item “B” if any subject answering “yes”
on item “A” implies that the subject will also answer “yes”
on item “B”. A subject who responds to an item positively
is expected to respond positively to all the items of lesser
difficulty.
Guttman scale is a deterministic process and the score of
a subject depends on the number of affirmative responses he
has made on the items. Scores in Guttman scale can also be
interpreted as the “ability” of a subject in answering questions
sorted in increasing order of “difficulty”. These scores when
presented on an underlying scale, give us an ordering of the
subjects based on their “ability” too.
The objective of our paper is to order the Indonesian
Islamic organizations based on their views on religio-social
keywords which have an inherent ordering. An organization
supporting “Sharia” will also likely to believe in “Quran”. So
it makes sense to use Guttman scaling procedure to rank the
organizations and their beliefs and practices. One drawback of
Guttman scale is that it is deterministic and assumes a strict
ordering of the items. We used Rasch [18] model to overcome
this drawback, by providing a probabilistic framework for

Guttman scales. Specifically, in the simple Rasch model, the
probability of a positive response (yes) is modeled as a logistic
function of the difference between the subject and item’s
parameters. Item parameters pertain to the difficulty of items
while subject parameters pertain to the ability of subjects
who are assessed. A subject of higher ability relative to the
difficulty of an item, has higher probability to respond to a
question affirmatively. In this paper Rasch models are used to
assess the organizations degree of being radical or counterradical based on the religio-social keywords (items) appearing
in their rhetoric.
Rasch model maps the responses of the subjects to the items
in binary or dichotomous format , i.e., 1 or 0. Let Bernoulli
variable Xvi denotes the response of a subject v to the item
i, variable θv denotes the parameter of “ability” of the subject
v and βi denotes the parameter of “difficulty” of an item i.
According to the Rasch model the probability that subject v
responds 1 for item i is given by
P (Xvi = 1|θv , βi ) =

exp(θv − βi )
1 + exp(θv − βi )

The maximum likelihood method is used to provide estimates for subject and item parameters. We can also assess
whether the data fits the model by looking at goodness of
fit indices, such as the Andersen’s likelihood ratio test (LRtest) [19]. A p-value, returned by the test, indicates the
goodness of fit and a p-value higher than 0.05 indicates no
presence of lack of fit. We used the eRm [20] package to run
the Rasch models
C. Implementing Rasch Model in the Text Mining Domain
Other works in text-mining domain such as sentiment
analysis, have used Rasch model in their analysis [21]. In
our application, Rasch model subjects correspond to a group
of religious organizations, and items correspond to a set
of keywords for socio-cultural, political, religious radical
and counter-radical beliefs, and practices. An organization
responding “yes” to a feature means the organization exhibits
that feature in its narrative, while an organization responding
“no” to a feature indicates that the organization does not
exhibit such a feature. Difficulty of an item translates to
strength of the corresponding attitude in defining radical or
counter-radical ideology of any organization. Similarly ability
of a subject in this case means the degree of radicalism or
counter-radicalism exhibited by an organization’s rhetoric.
D. Our Initial Work on Scale Generation
Our initial work [10] depended on more direct interaction
with experts’ opinion to build a model that can capture the
underlying dynamics of the scale. The experts both provided
a set of target organizations, and also directly selected the
items that would make up the scale, from a machine generated
candidate list. The candidate list consisted of the union of top100 n-grams from each organization’s individual corpus, which
were a total of 790 items. The resulting scale has utilized a
total of 55 of keywords selected by experts.

E. Debates and Perspective Analysis
Upon inspecting the keywords selected by our team of experts we observed that, some of these keywords correspond to
differing perspectives on a set of topics that are debated within
these web sites. Definition of debate is “a formal discussion on
a particular topic in a public meeting or legislative assembly, in
which opposing arguments are put forward.”1 . During a debate
on a particular topic, like education, both radical and counterradical organizations discuss different perspectives – such as
“secular multi-cultural education” vs. “sharia based religious
education”.
During the design of an automated perspective detection
algorithm, we made the following simplifying assumptions:
1) Organizations will mostly discuss their own perspective
in a debate;
2) Organizations will occasionally mention others’ perspectives, however, then relate them back to their own
perspective.
In our upcoming work [11], we present a mathematical
formulation of the perspective keyword generation problem
for a given topic, and provide an NP-Completeness proof of
this problem, and design an exact solution through an ILP
(integer linear programming) based solver.
The input to this algorithm also takes the polarity suggestion
from experts into consideration, for automatically identifying
the discriminating perspectives of those organizations from
opposite sides of a debate.
However, due to the algorithmic complexity and the strict
constraints of the exact model, the ILP based solver was not
always able to produce acceptable solutions. Namely, for larger
debated topics, the runtime requirements2 exceeded acceptable
limits of the study, and for more intervened debates, none of
the possible item sets could satisfy strict constraints of the ILP
definition.
In order to resolve this, in our current version of the system,
we have worked with a feature selection framework, SLEP.
The discussion of the implementation of SLEP is discussed in
the next section.
F. SLEP: A Sparse Learning Package
In order to address the scalability problem encountered in
ILP we resorted to SLEP [22], again with the underlined
motivation to select a subset of discriminating features that can
(a) classify and (b) satisfy Guttman scale [16]. The following
steps describe our algorithm:
1) For each topic, calculate the frequency of the words occurring within a fixed size window of the topic keyword
2) Filter the term × document matrix to include only the
most frequent 1000 words from each camp
3) Formulate the problem in a general sparse learning frame
[22]. Logistic formulation fits our application, since it is
a dichotomous classification problem
1 Oxford

Online Dictionary
data volume projections, we have estimated an upper bound of one
hour runtime restriction per topic. For this paper, we have run the cplex ILP
solver several hours for each topic before a timeout.
2 Given

min
x

m
X

wi log(1 + exp(−yi (xT ai + c))

(1)

i=1

+ λ|x|1
ρ
+ ||x||22
2

(2)
(3)

where Di is the document i and Fj is the feature (word) j.
A is the term × document matrix with all Aij ≥ 0, yi ∈ y is
the class of each document Di coded as +1 for Radical (R)
and -1 for Counter-Radical (CR) and xj is the weight for each
feature Fj . Let us explain further the three terms involved in
the convex optimization problem.
Pm
T
•
i=1 wi log(1 + exp(−yi (x ai + c)), this first term is
related to the logistic classification error. We set the
weights wi values to be all 1 so that all documents have
the same weight.
• λ|x|1 , this term involving the L1 norm deals with the
sparsity of the solution vector x. We experienced with
several lambda values which resulted with an x vector of
various sparsity.
ρ
2
• 2 ||x||2 , this last term deals with the ridge regression,
which is an extra level of shrinkage. We set the weight
of this term ρ = 0 as we were mainly driven by sparsity.
• We used the M ATLAB implementation of the SLEP package3 which utilizes gradient descent approach to solve
the aforementioned optimization problem. This package
can handle matrices of 20M entries within a couple of
seconds on a machine with standard configuration.
• The features with non-zero values on the x vector are the
candidate discriminants. Let FR , where xj > 0 be the
discriminant for the R class. Similarly, let FCR , where
xj < 0 be the discriminant for the CR class due to
the coding schema in step 3. Given that the optimized
formulation resulted with a sparse x vector, most of the
words Fj had xj = 0 and hence were not included in
either FR or FCR .
Note that the sets of features FR and FCR may not satisfy the
Guttman pattern. These sets needed to be further filtered such
0
0
⊆ FR and FCR
⊆ FCR would satisfy the Guttman
that FR
pattern.
IV. S YSTEM A RCHITECTURE
A. Data gathering
Initially, social scientists were invited to use their domain
and area expertise to identify a set of organizations, and
hypothesize any number of unipolar or bipolar scales that
could explain the variance among their beliefs and practices.
Next, a set of web crawling scripts were created for extraction
of articles from those organizations’ web sites. For each organization’s corpus we extracted their original text in Indonesian,
and stored the corpus in a ZIP file.
3 http://http://www.public.asu.edu/œjye02/Software/SLEP

B. Feature Extraction

Fig. 1.

After identifying the features for the analysis, we iterate
over the documents in the corpus of each organization for the
matching items. This yields a feature-document matrix.
This feature extraction task was performed in a simple
three step procedure; initially the occurrence frequencies of
particular features were counted within each organization’s
corpus, then a threshold matrix was calculated from these initial values, and finally a binary response matrix was generated
by applying these thresholds to the initial values.
The frequency metric is shown in formula 4, where k is the
keyword, o is the organization, and Do is the document set
pertaining to that particular organization.
fo,k =

|{d | k ∈ d, d ∈ Do }|
|Do |

(4)

A threshold value for each keyword is calculated from the
values in the related column. And then, each element was
converted into a binary value by comparing it to the column’s
threshold.
C. Model Fitting
We fit the Rasch model on two datasets - (1) radical
organizations with radical keywords and (2) counter-radical
organizations with counter-radical keywords. We used the eRm
package in R, an open source statistical software package4 , to
fit a Rasch model to the dataset, and obtain the organizations’
scores on the latent scale, which are the subject parameter
estimates (θv ) discussed in previous section. The eRm package5 fits Rasch models and provides subjects or organizations
parameter estimates.
D. Feature Expansion Algorithm
We have observed that including all of the newly discovered
features in the scale resulted a poor performance. This is
because, they neither provided the desired Guttman pattern,
nor the resulting scale aligned with the expert opinion. However, exhaustively enumerating all possible subsets to find an
optimal one would also be undesirable due to time complexity.
Thus we have devised a greedy expansion based algorithm to
select the items that make up the scale. It chooses a sufficiently
optimal subset of these features by expanding an initial set,
incrementally adding features that offer a higher performance.
One possible implementation is shown in the algorithm in
Fig. 1. This greedy algorithm will start from an initial set of
features I, and iteratively select the features that increase the
performance of the solution. The performance of a solution
is evaluated by the S OLVE function, which takes a candidate
input, and returns the performance value according to expert
agreement.
Each iteration of the loop (lines 3 – 15) tries to iteratively
expand the current set of selected features (lines 12 – 14).
First, it evaluates the performance of the currently selected
4 http://cran.r-project.org/
5 http://r-forge.r-project.org/projects/erm/

Feature set expansion algorithm

1:

procedure G REEDY-S ELECTION(I, C, .)
initial features I
candidate features C
comparison function .

2:
3:
4:

S←I
repeat
m ← S OLVE(S)
N ←∅
for all c ∈ C \ S do
p ← S OLVE(S ∪ {c})
if p . m then
N ← N ∪ {c}
end if
end for
if N 6= ∅ then
S ←S∪N
end if
until N = ∅
return S
end procedure

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

subset (line 4), and then identifies each not yet selected feature
that provides a performance increase (lines 7 – 8), and finally
collects them into the selected feature set for the next iteration
(lines 5, and 8 – 10). When it can no longer include any new
features, the algorithm will terminate.
Another performance trade-off was done using the natural
grouping of the features. Since the features in our problem are
grouped by topic, we decided to keep these natural groupings,
thus making each c in set C a collection of features.
Here the . comparison function will assume greater-thanor-equal-to semantics. This is because, while we want to have
the best possible scoring features as possible, we also want to
be able to have a larger set of perspectives that can be used
to explain the underlying latent scale.
In order to be able to handle the case of an empty initial
feature set (I), we expanded the algorithm as shown in Fig. 2.
This modification (to lines 6 – 11 in the original algorithm)
will choose the best available features in the first iteration that
are within a score difference of δ of each other. This also
assumes S OLVE function will return a sensible upper limit
value when an empty set is given as its input.
E. eRm Iterative Item Elimination Algorithm
While the eRm Rasch analysis package already does trivial
eliminations in the model (for example, ignoring full/empty
1/0 responses), it also provides an algorithm to clean up
a model from features that do not adhere to the Rasch
model/Guttman pattern.
The overall idea of the algorithm is summarized in the algorithm in Fig. 3. While our greedy feature selection algorithm
worked by expanding a set of features, this algorighm works
by going the opposite direction, and reducing the feature set

Fig. 2.
Feature set expansion algorithm modification, enabling special
handling of the empty initial set of features
1: for all c ∈ C do
2:
p ← S OLVE(S ∪ {c})
3:
if p . m then
4:
if S = ∅ ∧ p − m < δ then
5:
N ← {c}
6:
7:
8:
9:
10:
11:

Rasch Model (RM) m
Evaluation Function E VAL

5:
6:
7:
8:
9:
10:
11:
12:

The quadrants model

m←p
else
N ← N ∪ {c}
end if
end if
end for

Fig. 3. Feature elimination algorithm provided by the eRm package
1: procedure S TEP W ISE I T (m, E VAL )

2:
3:
4:

Fig. 4.

r←m
repeat
e ← E VAL(r)
if not F ITS(e) then
i ← L OWEST R ANKED F EATURE(e)
x ← r$x \ {i}
r ← RM(x)
end if
until Max # of Steps, or F ITS(e)
return r
end procedure

in each step. Here $ is the R member access operator, where
r$x is the feature set of rasch model object r, and \ is set
difference. Functions E VAL, and L OWEST R ANKED F EATURE
are references to eRm provided facilities to evaluate, and find
the worst contributing item of Rasch models.
V. E XPERIMENTAL E VALUATION
In order to measure the relation of the generated perspectives to the underlying scale, we have performed a series of
experiments designed to compare their scaling capabilities to
the gold standard ordering done by the experts.
A. Indonesian Corpus
The corpus domain is the online articles published by the
web sites of the 23 religious organizations identified in Indonesia, in the Indonesian language. These sources are the web
sites or blogs of the identified think tanks and organizations.
As discussed in the introduction, each source was classified
as either radical or counter-radical by the area experts. We
downloaded a total of 37,000 Indonesian articles published in
these 23 web sites, dating from 2001 to 2011.
B. The Quadrants Model
Our project leverages the results of our previous work,
which relied on social theory including Durkheim’s research

on collective representations [23], Simmel’s work on conflict
and social differentiation [24], Wallace’s writings on revitalization movements [25], and Tilly and Bayat’s studies on
contemporary social movement theory [26] [27]. Our team
has also developed, and is currently testing a theoretically
based class model comprised of continuous latent scales. The
first pair of scales focus on distinctions between the goals and
methods of counter-radical and radical discourse, and capture
the degree to which individuals, groups, and behaviors aim
to influence the social order (Change Orientation) and the
methods by which they attempt to do so (Change Strategies).
Quadrants model (see Figure 4) captures multiple social
trends in four quadrants A, B, C, and D, and it makes
the significant distinction between violent and not-violent
dimensions of both radicalisms and counter radicalisms. Using
the quadrants model, a researcher can locate organizations,
individuals, and discourses in broader categories while still
considering subtle differences between groups within categories. A researcher can document movement and trends from
category to category, and identify points where movement is
likely.
C. Expert Opinion and Gold Standard of Rankings
We collaborated with three area experts, who collectively
possess 35 years of scholarly expertise on Indonesia and
Islam. We utilized a homegrown graphical drag-and-drop user
interface to collect their opinion to build the gold standard of
the rankings. A screenshot of this tool is shown in Figure 5.
Each expert separately evaluated and ranked the organizations in the dataset according to a two dimensional scale
of radical / counter-radical (R/CR) and violent / non-violent
(V/NV) axis. The consensus among the experts was high; since
per item standard deviations among the experts’ scores along
the R/CR axis over a range of [−10, 10], across all organizations were 2.75. The individual scores for each organization
were combined and averaged to obtain the consensus gold
standard rankings along the hypothesized R/CR scale.
In this paper, we used two measures for evaluating the difference between two separate rankings, based on Spearman’s
footrule and Spearman’s correlation coefficient. The original
work utilized a mean displacement based measure as follows:

Fig. 5. The visual interface of the expert opinion collector for manually
placing the organizations on the two dimensional scale

In order to have a baseline for comparison of the automatically generated items, we have opted to use this scale in our
current study.
E. Candidate Perspectives
We have run both the ILP, and the SLEP based feature
generators on all the 50 topics that has been identified. ILP was
able to identify perspectives for 18 of the topics, while failed
for the rest, due to either finding no viable exact solution, or
timeouts. This resulted in a total of 2869 perspectives, with
159 average on each topic. Since these exact features also
included items with very low support, we have filtered these
results to include only the ones with higher frequency in the
corpus. The final set contained a total of 227 perspectives on
all 18 solved topics. On the other hand, SLEP was able to
successfully generate candidate perspective on every 50 topic,
totaling 1065 perspectives, with an average of 21 on each
topic.

Given two discrete ordering functions G, and R, on the
organization set O, the normalized displacement of a single
organization is given as:
disp(G, R, O, o) =

|G(o) − R(o)|
|O|

(5)

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1, |O|].
Then overall error measure for a given set of rankings was
then defined as:
error(G, R, O) =

X disp(G, R, O, o)
|O|

(6)

o∈O

In addition to this measure, we have also opted to include another measurement to take stability of the items into
consideration. Based on the L2 − N orm of the normalized
displacement function, the msd measure can be defined as
the following:
msd(G, R, O) =

X disp(G, R, O, o)2
|O|

(7)

o∈O

Since our initial work, we have also modified the evaluation
of the missing items. Specifically, for empty/full response
patterns, the Rasch model would not be able to make any
inference. Since we experimented with dynamic features, and
the missing items varied in each test, we have opted to position
them in their neutral places. This change has introduced a
slight difference from the experimental results of our original
study.
D. Baseline Performance
In our previous study [10] we have automatically generated
a Rasch model from the organizational corpus data, and the
expert selected items. We have observed that, against several
baseline algorithms, including score sorting, and principal
component analysis, the Rasch model was able to demonstrate
the best available performance, and was ranked at expert level.

F. Aligning Perspectives with the Scales
In order to identify the perspectives that make up the
theoretical scale we are working on (R/CR bi-polar scales on
Indonesian Islamic religious organizations), we have devised
a set of experiments that measure their relation to the Rasch
model, and the expectation of field experts.
Initially, as a baseline, we have re-run the original scale with
the expert selected features, with the new evaluation metrics.
The mean displacement of the features was 0.1172, while
the mean square displacement score was 0.0287. (The slight
difference with the original paper is due to the handling of the
missing items, discussed in Section V-C).
In order to observe the effect of the S TEP W ISE I T, we have
run the elimination algorithm on the original set of features.
The mean displacement was decreased to 0.1115, while the
mean square displacement stayed the same. The algorithm has
eliminated 15 features to reach this score. The summary of
these experiments can be seen in Table I
TABLE I
E XPERIMENTAL RESULTS FOR THE ORIGINAL EXPERT SELECTED FEATURE
BASED SCALES

Original
Original + S TEP W ISE I T

error
0.1172
0.1115

msd
0.0287
0.0287

run time
14s
53s

G. The Initial Experiments with Feature Expansion
After establishing the baseline, we evaluated the perspective
based features discovered by the ILP solver. First we built a
model including all the candidate features proposed by the
solver. This resulted in an mean displacement of 0.1323 and
mean square displacement of 0.0284. While the performance
was near the expert level, the hand selected features performed
(13%) better than this initial run.
Then the features were refined with S TEP W ISE I T, and our
G REEDY-S ELECTION algorithms. The S TEP W ISE I T failed to
provide better results, and actually performed worse, with

TABLE II
T HE TOPICS CHOSEN BE THE G REEDY-S ELECTION ALGORITHM FROM THE
CANDIDATE PERSPECTIVES OF THE ILP SOLUTION .
Iteration
1

Topics
kufur
disbelief

2

kdrt, kekafiran, kesetaraan, konstitusional,
multikultural, sekularisme, tabligh, toleransi
(domestic violence, infidelity, equality, constitutional,
multicultural, secularism, tabligh, tolerance)

3

bunuh, gender, homoseksual, musyrikin, syirik
(suicide, gender, homosexuals, idolaters, paganism)

mean displacement of 0.1632, and mean square displacement
of 0.0386, while failing the LR−test for Rasch model fitness.
The likely reason for this is that S TEP W ISE I T performs item
eliminated locally based on individual item fitness, but the
sparse nature causes loss of global Guttman pattern.
When we built an optimum item set from scratch using
the G REEDY-S ELECTION algorithm, we were able to identify
14 topics that contributed with better fitting perspectives.
The expanding topic sets can be seen in Table II. The final
solution had a mean displacement of 0.1020, with a mean
square displacement of 0.0189. An additional cleanup using
the S TEP W ISE I T algorithm over this existing solution did not
produce better results.
The summary of these experiments can be seen in Table III.
TABLE III
S CALING EXPERIMENTS WITH THE ILP SOLVER BASED DATA

ILP
ILP + StepWiseIt
Greedy(ILP)
Greedy(ILP) + StepWiseIt

error
0.1323
0.1632
0.1020
0.1122

msd
0.0284
0.0386
0.0182
0.189

run time
3m:38s
56m:47s

H. SLEP Based Features
In addition to the ILP based exact features, we also ran
separate experiments for the SLEP output. These yielded a
total of 449 features on counter radical, and 616 features on
the radical scales. The overall runtime duration was 6 hours
and 4 minutes. The resulting scales had a mean displacement
of 0.1398 and mean square displacement of 0.0312. We opted
not to run the S TEP W ISE I T on this particular case, since the
expected runtime would be in the order of weeks, which would
not be practical for the real life conditions of the project.
TABLE IV
S CALING EXPERIMENTS WITH THE SLEP SOLVER BASED DATA

SLEP
Greedy(SLEP)

error
0.1398
0.0982

msd
0.0312
0.0189

run time
6h:04m

Like the ILP based candidates, we also ran the G REEDYS ELECTION algorithm on the SLEP input (Table IV). Over two
iterations, the algorithm was able to identify 15 topics, whose

Fig. 6. Runtime performance of the Rasch model fitting algorithm in the
eRm package. The x axis corresponds to the number of items, while the y axis
represents the runtime length in seconds. Notice that the scatter plot shows
fitness to the x2 polynomial prediction line.

TABLE V
T HE TOPICS CHOSEN BE THE G REEDY-S ELECTION ALGORITHM FROM THE
CANDIDATE PERSPECTIVES OF THE SLEP SOLUTION .
Iteration
1
2

Topics
manusia
(human)
beragama, bunuh, dakwah, demokrasi, jihad,
kafir, kristen, liberal, multikultural, pluralisme,
politik, sipil, syariat, syirik
(religion, kill, propaganda, democracy, jihad,
infidel, Christian, liberal, multicultural, pluralism,
political, civil, Sharia, polytheism)

perspectives were closely related to the underlying scale. The
expanding topic set can be seen in Table V. The best mean
displacement achieved was 0.0982, with a corresponding mean
square displacement of 0.0189.
The main reason that this table does not share a significant
amount of topics with the ILP based topic set, is that the ILP
solver could not provide results for the great majority of the
topics selected by SLEP. The common ones, like “multikultural”, “syirik” were selected in both, while similar topics (like
“politik”/”konstitusional”) were chosen when available.
I. Sample perspectives
A set of sample perspectives selected by the ILP solver are
displayed in Figure 7. Here the columns represent individual
topics, while two rows correspond to radical, and counterradical perspectives on these topics. The items have been
machine translated from Indonesian into English.
VI. C ONCLUSION
In this paper, we have implemented a perspective identification algorithm, with exact, and approximate solutions, and
also tested the viability of this algorithm, by trying to apply
the discovered perspectives on an underlying bi-polar social
scale.
In relation to our previous work, we have increased the
automation in the scale generation process, by abstracting the
hand selected items to automatically discovered perspectives
from topics, and also increased the overall efficiency of the
system, by producing lower distance to the expert agreement.

Fig. 7. A sample set of perspectives generated by the ILP based solver. Here each row represents a debate topic, while the linear scales represent the
locations of the perspectives. The left side items are the counter-radical, and the right side items are the radical perspectives in each of these topics.

The theorized greedy growth based algorithm demonstrated
the best available performance in our experiments. Additionally both ILP, and SLEP based perspective generation
techniques provided features that fit the underlying bi-polar
scale.
Our future work includes reducing the expert interaction
further by automatically discovering debate topics, and investigating a possible use of these perspective analysis techniques
in the related field of sentiment analysis.
R EFERENCES
[1] Z. Papacharissi, “The virtual sphere: the internet as a public sphere,”
New Media Society, vol. 4, no. 1, pp. 9–27, Mar. 2002. [Online].
Available: http://nms.sagepub.com/cgi/content/abstract/4/1/9
[2] I. Himelboim, “Civil society and online political discourse: The network
structure of unrestricted discussions,” Communication Research, Oct.
2010. [Online]. Available: http://dx.doi.org/10.1177/0093650210384853
[3] L. Dahlberg, “The internet and democratic discourse: Exploring the
prospects of online deliberative forums extending the public sphere,”
pp. 615–633, Dec. [Online]. Available: http://www.ingentaconnect.com/
content/routledg/rics/2001/00000004/00000004/art00007
[4] T. Mullen and R. Malouf, “A preliminary investigation into sentiment
analysis of informal political discourse,” in AAAI symp. on computational approaches to analysing weblogs (AAAI-CAAW), 2006, pp. 159–
162.
[5] R. Malouf and T. Mullen, Graph-based user classification for informal
online political discourse, 2007.
[6] M. Thomas, B. Pang, and L. Lee, “Get out the vote: Determining
support or opposition from congressional floor-debate transcripts,” in
In Proceedings of EMNLP, 2006, pp. 327–335.
[7] M. Bansal, C. Cardie, and L. Lee, “The power of negative thinking:
Exploiting label disagreement in the min-cut classification framework,”
Proceedings of COLING: Companion volume: Posters, pp. 13–16, 2008.
[8] W. Lin and A. Hauptmann, “Are these documents written from different
perspectives?: a test of different perspectives based on statistical distribution divergence,” in Proc. of the 21st Int. Conf. on Computational
Linguistics and the 44th annual meeting of the ACL. ACL, 2006, pp.
1057–1064.
[9] M. Woodward, I. Rohmaniyah, A. Amin, and D. Coleman, “Muslim
education, celebrating islam and having fun as counter-radicalization
strategies in indonesia,” Perspectives on Terrorism, vol. 4, no. 4, 2010.
[10] S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen,
S. Corman, M. Woodward, I. Rochmaniyah, and A. Amin, “A system
for ranking organizations using social scale analysis,” in EISIC. IEEE,
2011, pp. 308–313.

[11] S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen,
S. Corman, M. Woodward, S. Nair, I. Rohmaniyah, and A. Amin, “A
system for ranking organizations using social scale analysis,” Social
Network Analysis and Mining (SNAM), Accepted for publication.
[12] J. Hartigan and M. Wong, “Algorithm as 136: A k-means clustering
algorithm,” Journal of the Royal Statistical Society. Series C (Applied
Statistics), vol. 28, no. 1, pp. 100–108, 1979.
[13] G. Rasch, “On general laws and the meaning of measurement in
psychology,” in Proceedings of the Fourth Berkeley Symposium on
Mathematical Statistics and Psychology, 4, 1961, p. 332.
[14] R. Likert, “A technique for the measurement of attitudes,” Archives of
Psychology, vol. 140, pp. 1–55, 1932.
[15] L. L. Thurstone, “Attitudes can be measured,” American Journal of
Sociology, vol. 33, pp. 529–554, 1928.
[16] J. McIver and E. Carmines, Unidimensional Scaling. Sage Publications,
Inc, 1981, vol. 24.
[17] L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60–90, 1950.
[18] D. Andrich, Rasch models for measurement. Sage, 1988.
[19] D. Hessen, “Likelihood ratio tests for special rasch models,” Journal of
Educational and Behavioral Statistics, vol. 35, no. 6, p. 611, 2010.
[20] P. Mair and R. Hatzinger, “Extended rasch modeling: The erm package
for the application of irt models in r,” 2007.
[21] D. Drehmer, J. Belohlav, and R. Coye, “An exploration of employee
participation using a scaling approach,” Group & Organization Management, vol. 25, no. 4, p. 397, 2000.
[22] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 547–556.
[23] E. Durkheim, “The cultural logic of collective representations,” Social
theory the multicultural and classic readings, Wesleyan University:
Westview Press, 2004.
[24] G. Simmel, Sociological Theory. New York: McGraw-Hill, 2008.
[25] A. Wallace, “Revitalization movements,” American Anthropologist,
vol. 58, pp. 264–281, 1956.
[26] C. Tilly, Social Movements. Boulder, CO, USA: Paradigm Publishers,
2004.
[27] A. Bayat, Making Islam Democratic: Social Movements and the PostIslamist Turn. Stanford University Press, 2007.

Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)

MUVIR: Multi-View Rare Category Detection
Dawei Zhou, Jingrui He, K. Seluk Candan, Hasan Davulcu
Arizona State University
Tempe, Arizona
{dzhou23,jingrui.he,candan,hdavulcu}@asu.edu

Abstract

In many real-world applications, the data consists of multiple views, or features from multiple information sources.
For example, in synthetic ID detection, we aim to distinguish
between the true identities and the fake ones generated for
the purpose of committing fraud. Each identity is associated
with information from various aspects, such as demographic
information, online social behaviors, banking behaviors. Another example is insider threat detection, where the goal is to
detect malicious insiders in a large organization, by collecting various types of information regarding each employee’s
daily behaviors. To detect the rare categories in these applications, simply concatenating all the features from multiple
views may lead to sub-optimal performance in terms of increased number of label requests, as it ignores the relationship
among the multiple views. Furthermore, among the multiple
information sources, some may generate features irrelevant
to the identification of the rare examples, thus deteriorates
the performance of rare category detection.
To address this problem, in this paper, we propose a novel
framework named MUVIR for detecting the initial examples from the minority classes in the presence of multi-view
data. The key idea is to integrate view-specific posterior
probabilities of the example coming from the minority class
given features from each view, in order to obtain the estimate of the overall posterior probability given features from
all the views. In particular, the view-specific posterior probabilities can be inferred from the scores computed using
a variety of existing techniques [He and Carbonell, 2007;
He et al., 2008]. Furthermore, MUVIR can be generalized
to handle problems where the exact priors of the minority
classes are unknown. To the best of our knowledge, this paper is the first principled effort on rare category detection in
the presence of multiple views. Compared with existing techniques, the main advantages of MUVIR can be summarized
as follows.
1. Effectively leveraging the relationship among multiple
views to improve the performance of rare category detection;
2. Robustness to irrelevant views;
3. Flexibility in terms of the base algorithm used for generating view-specific posterior probabilities.
The rest of this paper is organized as follows. After a brief
review of the related work in Section 2, we introduce the pro-

Rare category detection refers to the problem
of identifying the initial examples from underrepresented minority classes in an imbalanced data
set. This problem becomes more challenging in
many real applications where the data comes from
multiple views, and some views may be irrelevant
for distinguishing between majority and minority
classes, such as synthetic ID detection and insider
threat detection. Existing techniques for rare category detection are not best suited for such applications, as they mainly focus on data with a single
view.
To address the problem of multi-view rare category
detection, in this paper, we propose a novel framework named MUVIR. It builds upon existing techniques for rare category detection with each single
view, and exploits the relationship among multiple
views to estimate the overall probability of each example belonging to the minority class. In particular, we study multiple special cases of the framework with respect to their working conditions, and
analyze the performance of MUVIR in the presence
of irrelevant views. For problems where the exact
priors of the minority classes are unknown, we generalize the MUVIR algorithm to work with only an
upper bound on the priors. Experimental results on
both synthetic and real data sets demonstrate the effectiveness of the proposed framework, especially
in the presence of irrelevant views.

1

Introduction

In contrast to the large amount of data being generated and
used everyday in a variety of areas, it is usually the case that
only a small percentage of the data might be of interest to
us, which form the minority class. However, without initial
labeled examples, the minority class might be very difficult
to detect with random sampling due to the imbalance nature
of the data, and the limited budget for requesting labels from
a labeling oracle. Rare category detection has been proposed
to address this problem, so that we are able to identify the
very first examples from the minority class, by issuing a small
number of label requests to the labeling oracle.

4098

Rare Category Detection
Rare category analysis has also been studied for years. Up
to now, many methods have been approached to address this
problem. In this paper, we mainly review the following two
existing works on rare category detection. The first one is
[He and Carbonell, 2007], in which algorithm NNDM is proposed standing on two assumptions: (i) data sets have little
knowledge about labels (ii) there is no separability or nearseparability between majority and minority classes. Both assumptions exactly meet the setting of the problem we want
to figure out. The probability distribution function (pdf) of
the majority class tends to be locally smooth, while the pdf of
minority class tends to be a more compact cluster. In general,
the algorithm measures the changes of local density around
a certain point. NNDM gives a score to each example, and
the score is the maximum difference of local density between
one item and all of its neighboring points. By querying the
examples with the largest score, it is able to hit the region of
minority class with the largest probability.
Another work about rare category detection is [He et al.,
2008], the authors provided an upgraded algorithm GRADE
based on NNDM. In this algorithm, they took the consideration of the manifold structure in minority class. For example,
two examples from the same minority class on the manifold
may be far away in Euclidean distance. In this case, they generate a global similarity matrix embedded all of the examples
from the original feature space. The items of minority class
are made to form a more compact cluster for each minority
class. Based on global similarity matrix, they measure the
changes of local density for each example. The changes of local density, to some extent, has been enlarged, and made the
minority classes easier to be discovered. Furthermore, they
provided an approximating algorithm to manage rare category detection with less information about priors of minority classes. In this paper, our proposed framework MUVIR
is generic in the sense that it can leverage multiple existing
RCD methods, such as GRADE, NNDM and etc., to analyze
the problem in the multi-view version. To the best of our
knowledge, this is the first effort on rare category detection
with multiple views.

posed framework for multi-view rare category detection in
Section 3. In Section 4, we test our model on both synthetic
data sets and real data sets. Finally, we conclude this paper in
Section 5.

2

Related Work

Multi-view Learning
Multi-view learning targets problems where the features naturally come from multiple information sources, or multiple
views. It has been studied extensively in the literature. Cotraining [Blum and Mitchell, 1998] is one of the earliest efforts in this area, where the authors proved that maximizing
the mutual consistency of two independent views could be
used to learn the pattern based on a few labeled and many unlabeled examples. Since then, multi-view learning has been
studied in multiple aspects during these years. A portion of
the researchers focus on the study of independent assumption for co-training, which is essential in the real world application. [Abney, 2002] refined the analysis of co-training
and gave a theoretical justification that their algorithm could
work on a more relax independence scenario rather than cotraining. [Balcan et al., 2004] proposed an independence
expansion and proved that it could guarantee the success of
co-training. Another line of work has been devoted to the
construction of multiple views and how to combine multiple views. In [Ho, 1998], they apply random sampling
algorithm called RSM, which perform bootstrapping in the
feature space to separate the views. [Chen et al., 2011]
transform the feature decomposition task into an optimization problem, which could automatically divide the feature
space into two exclusive subsets. While, in the aspect of how
to combine multiple views and learn models, we can separate
it into the problems of supervised learning, semi-supervised
learning and unsupervised learning. In the category of supervised and semi-supervised learning, [Muslea et al., 2003;
2006] designed a robust semi-supervised algorithm which
combined co-learning with active learning. CoMR [Sindhwani and Rosenberg, 2008] proposed a multi-view learning
algorithm based on a reproducing kernel Hilbert space with a
data-dependent co-regularization norm. In [Yu et al., 2011],
author proposed a co-training Bayesian graph model, which is
more reliable in handling the case of missing views. SMVC
[Günnemann et al., 2014] proposed a Bayesian framework
for modeling multiple clusterings of data by multiple mixture distributions. In the category of unsupervised learning,
[Long et al., 2008] introduced a general model for unsupervised multiple view learning and demonstrate it in various
types of unsupervised learning on various types of multiple
view data. The authors of [Song et al., 2013] developed
a kernel machine for learning in multi-view latent variable
models, which also allows mixture components to be nonparametric and to learn data in an unsupervised fashion.
Different from existing work on multi-view learning, in
this paper, we start de-novo, i.e., we do not have any labeled
examples to start with, but we are able to query the oracle for
the labels of selected examples until at least one example has
been detected from each minority class.

3

The Proposed Framework

In this section, we introduce the proposed framework MUVIR for multi-view rare category detection. Notice that similar as existing techniques designed to address this problem
for single-view data, we target the more challenging setting where the support regions of the majority and minority
classes overlap with each other, which makes MUVIR widely
applicable to a variety of real problems.

3.1

Notation

Suppose that we are given a set of unlabeled examples S =
{x1 , · · · , xn }, which come from m distinct classes, i.e. yi ∈
{1, · · · , m}. Without loss of generality, assume that yi = 1
corresponds to the majority class with prior p1 , and the remaining classes are minority classes with prior pc . Furthermore, each example xi is described by features from V views,
i.e., xi = [(x1i )T , . . . , (xVi )T ]T , where xvi ∈ Rdv , and dv is

4099

the dimensionality of the v th view. In our proposed model, we
repeatedly select examples to be labeled by an oracle, and the
goal is to discover at leaset one example from each minority
class by requesting as few labels as possible.

3.2

Proof. Notice that when the features from multiple views are
conditionally independent given the class label, we have
P (x|y = 2) =

P (xv |y = 2)

v=1

Multi-View Fusion

The rest of the proof follows by changing the inequality in
Equation 2 to equality.

In this section, for the sake of exposition, we focus on the binary case, i.e., m = 2, and the minority class corresponds to
yi = 2, although the analysis can be generalized to multiple
minority classes. As reviewed in Section 2, existing techniques for rare category detection with single-view data essentially compute the score for each example according to the
change in the local density, and select the examples with the
largest scores to be labeled by the oracle. Under mild conditions [He et al., 2008; He and Carbonell, 2007], these scores
reflect P (x, y = 2), thus are in proportion to the conditional
probability P (y = 2|x).
For data with multi-view features, running these algorithms [He et al., 2008; He and Carbonell, 2007] on each
view will generate scores in proportion to P (y = 2|xv ),
v = 1, . . . , V . Next, we establish the relationship between
these probabilities and the overall probability P (y = 2|x).

Based on the above analysis, in MUVIR, we propose to assign the score for each example as follows.
!d
QV
V
v
Y
v
v
v=1 P (x )
s(x) =
(3)
s (x )
P (x)
v=1
where sv (xv ) denotes the score obtained based on the v th
view using existing techniques such as NNDM [He and Carbonell, 2007] or GRADE [He et al., 2008]; and d ≥ 0 is a
parameter that controls the impact of the term related to the
marginal probability of the features. In particular, we would
like to discuss two special cases of Equation 3.
Case 1. If the features from multiple views are conditionally
independent given the class label, and they are marginally inQV
dependent, i.e., P (x) = v=1 P (xv ), then Corollary 1 indicates that d = 0;
Case 2. If the features from multiple views are conditionally
independent given the class label, then Corollary 1 indicates
that d = 1.
In Section 4, we study the impact of the parameter d on
the performance of MUVIR, and show that in general, d ∈
(0, 1.5] will lead to reasonable performance.
Notice that the proposed score in Equation 3 is robust to
irrelevant views in the data, i.e., the views where the examples from the majority and minority classes cannot be effectively distinguished. This is mainly due to the first part
QV
v
v
v=1 s (x ) on the right hand side of Equation 3. For example, assume that view 1 is irrelevant such that the distribution of the majority class (P (x|y = 1)) is the same as
the minority class (P (x|y = 2)). In this case, the viewspecific score s1 (x1 ), which reflects the conditional probability P (y = 2|x), would be the same for all the examples.
Therefore, when integrated with the scores from the other relevant views, view 1 will not impact the relative score of all
the examples, thus it will not degrade the performance of the
proposed framework.

Theorem 1. If the features from multiple views have weak
dependence given the class label yi = 2 [Abney, 2002], i.e.,
QV
P (x|y = 2) ≥ α v=1 P (xv |y = 2), α > 0, then
!
QV
V
v
Y
v
v=1 P (x )
P (y = 2|x) ≥ C(
P (y = 2|x )) ×
P (x)
v=1
(1)
where C = (p2 )αV −1 is a constant.
Proof.
P (y = 2)P (x|y = 2)
P (x)
QV
P (y = 2)α v=1 P (xv |y = 2)
≥
P (x)
QV P (y=2|xv )P (xv )
P (y = 2) v=1
P (y=2)
=α
P (x)
QV
P (y = 2|xv )P (xv )
= α v=1
P (x)(P (y = 2))V −1
QV
V
Y
P (xv )
α
v
= 2 V −1
P (y = 2|x ) v=1
(p )
P (x)
v=1

V
Y

P (y = 2|x) =

(2)

3.3 MUVIR Algorithm
The proposed MUVIR algorithm is described in Algorithm 1.
It takes as input the multi-view data set, the priors of all the
classes (p1 , p2 , . . . , pm ), as well as some parameters, and outputs the set of selected examples together with their labels.
MUVIR works as follows. In Step 2, we compute the viewspecific score for each example, which can be done using
any existing techniques for rare category detection. In Step
3, we estimate the view-specific density using kernel density estimation; whereas in Step 5, we estimate the overall density by pooling the features from all the views together. Finally, Steps 6 to 16 aim to select candidates according to P (y = c|x). To be specific, in Step 7, we skip

As a special case of Theorem 1, when the features from
multiple view are conditionally independent given the class
label, i.e., α = 1, we have the following corollary.
Corollary 1. If the features from multiple views are conditionally independent given the class label, then Inequality 1
becomes equality, and C = (p2 )1V −1 .

4100

class c if examples from this class have already been identified in the previous iterations. Step 10 implements the feedback loop by excluding any examples close to the labeled
ones from being selected in future iterations. Notice that
the threshold  depends on the algorithm used to obtain the
view-specific scores. For example, it is set to the smallest
k-nearest neighbor distance in NNDM [He and Carbonell,
2007], and the largest k-nearest neighbor global similarity in
GRADE [He et al., 2008]. Step 11 updates the view-specific
score for each example with enlarged neighborhood for computing the change in local density [He and Carbonell, 2007;
He et al., 2008]. In Step 13, we compute the overall score
based on Equation 3, and select the example with the maximum overall score to be labeled by the oracle in Step 14. In
Step 15, if the labeled example is from the target class in this
iteration, we proceed to the next class; otherwise, we mark
the class of this examples as labeled.

VIR, MUVIR-LI is more suitable in real world applications.
MUVIR-LI is described in Algorithm 2. It works as follows. Step 2 calculates the specific score sv for each example.
The only difference from MUVIR is that here we use upper
bound p to calculate sv , which is a less accurate measurement of changing local density than in MUVIR. The same as
MUVIR, we estimate the view specific density and the overall
density by applying kernel density estimation in Step 3 and
Step 5. The while loop from Step 6 to Step 16 is the query
processing. We calculate the overall score for each example
and select the examples with the largest overall score to be
labeled by oracle. We end the loop until all the classes has
been discovered.
Algorithm 2 MUVIR-LI Algorithm
Input:
Unlabeled data set S with features from V views, p, d, .
Output:
The set I of selected examples and the set L of their labels.
1: for v = 1 : V do
2:
Compute the view-specific score sv (xvi ) for all the examples using existing techniques for rare category detection, such as GRADE-LI [He et al., 2008];
3:
Estimate P (xvi ) using kernel density estimation;
4: end for;
5: Estimate P (xi ) using kernel density estimation;
6: while not all the classes have been discovered do
7:
for t = 2 : n do
8:
for v = 1 : V do
9:
For each xi that has been labeled by the oracle,
∀i, j = 1, . . . , n, i 6= j,, if kxvi , xvj k2 ≤ , then
sv (xvj ) = −∞;
10:
Update the view-specific score sv (xvi ) using existing techniques such as GRADE-LI [He et al.,
2008];
11:
end for;
12:
Compute the overall score for each example s(xi )
based on Equation 3;
13:
Query the label of the example with the maximum
s(xi )
14:
Mark the class that x belongs to as discovered.
15:
end for;
16: end while

Algorithm 1 MUVIR Algorithm
Input: Unlabeled data set S with features from V views,
p1 , . . . , pm , d, .
Output: The set I of selected examples and the set L of their
labels.
1: for v=1 : V do
2:
Compute the view-specific score sv (xvi ) for all the examples using existing techniques for rare category detection, such as GRADE [He et al., 2008];
3:
Estimate P (xvi ) using kernel density estimation;
4: end for
5: Estimate P (xi ) using kernel density estimation on all the
features combined;
6: for c=2 : m do
7:
If class c has been discovered, continue;
8:
for t = 2 : n do
9:
for v = 1 : V do
10:
For each xi that has been labeled by the oracle,
∀i, j = 1, . . . , n, i 6= j,, if kxvi , xvj k2 ≤ , then
sv (xvj ) = −∞;
11:
Update the view-specific score sv (xvi ) using existing techniques such as GRADE [He et al.,
2008];
12:
end for
13:
Compute the overall score for each example s(xi )
based on Equation 3;
14:
Query the label of the example with the maximum
s(xi )
15:
If the label of xi is from class c, break; otherwise,
mark the class of xi as labeled.
16:
end for
17: end for

4

Experimental Results

In this section, we will present the results of our algorithm on
both synthetic data sets and real data sets in multiple special
scenarios, such as data sets with different number of irrelevant
features, data sets with multiple classes and data sets with
very rare categories, such as class proportion of 0.02%.

3.4 MUVIR with Less Information (MUVIR-LI)
In many real applications, it may be difficult to obtain the priors of all the minority classes. Therefore, In this subsection,
we introduce MUVIR-LI, a modified version of Algorithm 1,
which replaces the requirement for the exact priors with an
upper bound p for all minority classes. Compared with MU-

4.1

Synthetic Data Sets

Binary Class Data Sets
For binary classes, we perform experiment on 3600 synthetic
data sets, and each scenario has independent 100 data sets.

4101

In Majority Class Center

300
250

450

400

# of selected examples

# of selected examples

350

Near Majority Class Center

450

Random Sampling
GRADE
d=0
d=0.5
d=1
d=1.5

400

200
150
100

350
300
250
200
150
100

50

50

0

0

1

2

0

3

Partly Separated from Majority Class

400

# of selected examples

450

350
300
250
200
150
100
50

0

# of irrelevant features

1

2

0

3

0

# of irrelevant features

1

2

3

# of irrelevant features

Figure 1: Prior of minority class is 0.5%
In Majority Class Center

150

100

50

0

0

1

2

250

# of selected examples

200

Near Majority Class Center

250

Random Sampling
GRADE
d=0
d=0.5
d=1
d=1.5

# of selected examples

# of selected examples

250

200

150

100

50

0

3

0

# of irrelevant features

1

2

200

150

100

50

0

3

Partly Separated from Majority Class

0

# of irrelevant features

1

2

3

# of irrelevant features

Figure 2: Prior of minority class is 1%
In Majority Class Center

70
60
50
40
30
20

70
60
50
40
30
20
10

0

0

1

2

# of irrelevant features

3

Partly Separated from Majority Class

90

80

10
0

100

90

# of selected examples

# of selected examples

80

Near Majority Class Center

100

Random Sampling
GRADE
d=0
d=0.5
d=1
d=1.5

90

# of selected examples

100

80
70
60
50
40
30
20
10

0

1

2

0

3

# of irrelevant features

0

1

2

3

# of irrelevant features

Figure 3: Prior of minority class is 2%
We consider the following three special conditions: (i) different number of irrelevant features, i.e. from 0 to 3 irrelevant
features; (ii) different priors for minority class, i.e. 0.5%,
1%, 2%; (iii) different levels of correlation between majority
class and minority class, ie. minority class stays in the center of majority class, minority class stays around the center of
majority class, minority class stays at the boundary of majority class. Besides, as the distribution of majority class tends
to be more scattered and the distribution of minority class is
more compact, we set each data set with 5000 examples and
σmajority : σminority = 40 : 1.
In the experiment, we compare MUVIR with GRADE [He
et al., 2008] and random sampling. Fig. 1 shows the results when the prior of minority class is 0.5%. Using random sampling, we need to label 200 examples on average to
identify the minority class. In most cases, other approaches
outperform random sampling. However, the learning model
generated by GRADE algorithm performs worse with the increasing of irrelevant features. In contrast, MUVIR is more
efficient and stable rather GRADE. The experiment with minority proportions of 1% and 2% are represented in Fig. 2
and Fig. 3. In these two experiment, MUVIR outperforms
GRADE and random sampling in each condition with any
setting of d. Comparing these three figures, we have the following observations for binary class data sets: (i) MUVIR is
more reliable especially when dealing with data sets containing irrelevant features. (ii) In the case of data sets with no
irrelevant features, the performance of MUVIR with different

values of d are roughly the same. (iii) In the case of data
sets with irrelevant features, MUVIR with d = 1 outperforms
other methods.

# of selected examples

Multi-classes Data Sets with Imprecise Prior
300
250
200

Random Sampling
GRADE
GRADE-LI
MUVIR, d=1
MUVIR-LI, d=1

150
100
50
0

0

1

2

3

# of irrelevant features

Figure 4: Multi-class data sets
For multi-class data sets, we compare the performances
among different approaches. In particular, GRADE-LI [He
et al., 2008] and MUVIR-LI are only provided with an upper
bound p on the proportion of all the minority classes. The
multi-class data sets consisting of 9000 examples correspond
to majority class, and the other 1000 examples correspond to
4 minority classes. The proportions of minority classes are
4%, 3%, 2%, 1%. Similar to previous experiments, we will
discuss the scenario data sets contain different number of irrelevant features. Each value we represented in the figure
is the median value of results from 100 same scenario data
sets. From Fig. 4, we can have the following conclusions: (i)
MUVIR outperforms all other algorithms in multi-class data

4102

Views
relevant view 1
relevant view 2
relevant view 3
relevant view 4
irrelevant view 1
irrelevant view 2

sets; (ii) GRADE only performs good when data sets have 1
or 0 irrelevant feature; (iii) MUVIR-LI is more reliable than
GRADE-LI in all scenarios. The reason that our models have
better performance is that both MUVIR and MUVIR-LI are
capable to exploit the relationship among multiple views and
extract useful information to make predictions.

Table 1: Relevant and irrelevant views in Adult Data set.
algorithms, we have preprocessed both data sets in order to
keep each feature component has mean 0 and standard deviation 1. In the following experiments, we will compare MUVIR and MUVIR-LI with the following algorithms: GRADE,
GRADE-LI and random sampling.
70

# of selected examples

# of selected examples

Parameter Analysis
From previous experiments, we found different parameter
settings may result in different outcomes. In this experiment,
we will focus on analyzing the impact from degree d and upper bound prior p. To measure the impact of these parameters,
we generate 400 data sets with minority class proportion 1%.
The number of irrelevant features varies from 0 to 3, and each
case has 100 data sets. In Fig. 5, the X axis represents different values of degree d, and Y axis represents the number of
selected examples on average. From Fig. 5, we can see that
MUVIR performs better when d ∈ (0, 1.5]. In the following
experiments, we will focus on studying the performance of
our algorithm with d in this certain area.
250
200
150

With 0 Irrelevant
With 1 Irrelevant
With 2 Irrelevant
With 3 Irrelevant

Features
Features
Features
Features

Without Irrelevant features
With Irrelevant features

60
50
40
30
20
10
0

GRADE

MUVIR, d=0

MUVIR, d=0.5

MUVIR, d=1

MUVIR, d=1.5

100

Figure 7: Adult

50
0
0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

5.5

6

Value of d

Adult data set contains 48842 instances and 14 features of
each example. It is a binary classes data sets. Considering the
original prior of minority class in data sets is around 24.93%.
To better test the performance of our model, we keep majority class the same and down sample the minority class to 500
examples. In this way, we generate 24 data sets with minority
prior of 1.3%. And we select relevant and irrelevant views
based on correlation analysis. Noticed that all the views are
fed to all the algorithms without information regarding their
relevance. The details about relevant and irrelevant views are
represented in Tab.1. Fig. 7 shows the comparison results on
real data by applying 5 different approaches. In this experiment, we have not included MUVIR-LI, it is because MUVIRLI is mainly developed for multi-class cases and Adult is a
binary class data sets. By using random sampling, the average number of selected examples is 76. With irrelevant views,
GRADE needs 69 requests, MUVIR with d = 0 needs 60 requests, MUVIR with d 6= 0 needs around 30 to 40 requests.
The results totally meet our intuition that when dealing data
sets with irrelevant views, MUVIR with d 6= 0 outperforms
MUVIR with d = 0, and MUVIR with d = 0 outperforms
GRADE. However, when dealing with data sets without irrelevant views, GRADE needs less labeling requests than MUVIR with d = 0, but more labeling requests than MUVIR with
d around 1.
Different from Adult, Statlog contains 58000 examples and
7 classes. Among 7 classes, there are 6 minority classes, with
priors varying from 0.02% to 15%. In this experiment, we
compare the following 4 methods: GRADE, GRADE-LI with
c
upper bound p = maxm
c=2 p , MUVIR with d = 1, MUVIR-LI
m
with d = 1 and p = maxc=2 pc . From Fig. 8, we can see that
MUVIR outperforms all other algorithms for finding all the
minority class. With the same upper bound prior, GRADE-

Figure 5: Learning curves with different degree d

# of selected examples

Features
education, education years, work class
age, hours per week, occupation
martial status, relationship, sex
race, native country
final weight
capital loss, capital gain

500
400

With 1 Irrelevant Features
Without Irrelevant Features
Random Sampling

300
200
100
0

2%

4%

6%

8%

10%

12%

14%

16%

Upper bound p

Figure 6: Learning curves with different prior upper bound
With the same data sets, we studied the learning curves of
labeling requests by applying MUVIR-LI with different upper
bound p. In Fig. 6, the X axis represents different values of
upper bound proportion and Y axis represents the number of
labeling requests. The red line represents the average number of labeling requests by using random sampling. When
data sets without irrelevant features, MUVIR-LI works well
even with upper bound p changing from 1% to 12%. When
data sets with irrelevant features, MUVIR-LI can still outperforms random sampling with upper bound p changing from
1% to 8.5%. However, when the upper bound exceeds a certain level, the algorithm tends to be random sampling. This
might be due to the reason that when the bound is very loose,
e.g. the exact proportion of the minority class is 1% and the
given upper bound is 10%, the performance of our proposed
algorithm may be greatly affected by the introduced noise.

4.2

Real Data Sets

In this subsection, we will demonstrate our algorithm on two
real data sets Statlog and Adult. Noted that, before we run our

4103

Percentage of Classes Discovered

LI needs 272 labeling requests while MUVIR-LI only needs
168 labeling requests to discover all the classes. If we apply random sampling, it may needs around 5000 labeling request to only identify the smallest minority class. Compared
with Adult, we have better results on Statlog. It is because
the distribution of majority class and minority classes are not
meshed together as in Adult. Thus, to identify the minority
classes in Statlog is a much easier case.

Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100. ACM, 1998.
[Chen et al., 2011] Minmin Chen, Yixin Chen, and Kilian Q
Weinberger. Automatic feature decomposition for single
view co-training. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages 953–
960, 2011.
[Günnemann et al., 2014] Stephan Günnemann, Ines Färber,
Matthias Rüdiger, and Thomas Seidl. Smvc: semisupervised multi-view clustering in subspace projections.
In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 253–262. ACM, 2014.

1
GRADE
GRADE-LI
MUVIR
MUVIR-LI

0.8

0.6

0.4

[He and Carbonell, 2007] Jingrui He and Jaime G Carbonell.
Nearest-neighbor-based active learning for rare category
detection. In Advances in neural information processing
systems, pages 633–640, 2007.

0.2

0
0

50

100

150

200

250

300

# of Selected Examples

Figure 8: Statlog

5

[He et al., 2008] Jingrui He, Yan Liu, and Richard
Lawrence. Graph-based rare category detection. In
Data Mining, 2008. ICDM’08. Eighth IEEE International
Conference on, pages 833–838. IEEE, 2008.

Conclusion

In this paper, we have proposed a multi-view based method
for rare category detection named MUVIR. Based on MUVIR,
we also provided a modified version MUVIR-LI for dealing
with real applications with less prior information. Different
from existing methods, our methods exploit the relationship
among multiple views and measure the probability belonging
to target class for all examples. Our algorithm works well
with multiple special cases: data sets with irrelevant features,
data sets with multiple minority class and various correlation
levels between minority class and majority class. The effectiveness of our proposed methods is guaranteed by theoretical
justification and extensive experiments results on both synthetic and real data sets, especially in the presence of irrelevant views.

[Ho, 1998] Tin Kam Ho. The random subspace method for
constructing decision forests. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(8):832–844,
1998.
[Long et al., 2008] Bo Long,
S Yu Philip,
and
Zhongfei (Mark) Zhang. A general model for multiple view unsupervised learning. In SDM, pages 822–833.
SIAM, 2008.
[Muslea et al., 2003] Ion Muslea, Steven Minton, and
Craig A Knoblock. Active learning with strong and weak
views: A case study on wrapper induction. In IJCAI, volume 3, pages 415–420, 2003.

Acknowledgment

[Muslea et al., 2006] Ion Muslea, Steven Minton, and
Craig A Knoblock. Active learning with multiple views.
Journal of Artificial Intelligence Research, pages 203–
233, 2006.

The authors gratefully acknowledge the support from the
National Science Foundation under Grant Numbers IIP1430144. Any opinions, findings, and conclusions expressed
in this material are those of the authors and do not necessarily
reflect the views of the National Science Foundation.

[Sindhwani and Rosenberg, 2008] Vikas Sindhwani and
David S Rosenberg. An rkhs for multi-view learning
and manifold co-regularization. In Proceedings of the
25th international conference on Machine learning, pages
976–983. ACM, 2008.

References
[Abney, 2002] Steven P. Abney. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA., pages 360–367, 2002.
[Balcan et al., 2004] Maria-Florina Balcan, Avrim Blum,
and Ke Yang. Co-training and expansion: Towards bridging theory and practice. In Advances in neural information
processing systems, pages 89–96, 2004.
[Blum and Mitchell, 1998] Avrim Blum and Tom Mitchell.
Combining labeled and unlabeled data with co-training. In

[Song et al., 2013] Le Song, Animashree Anandkumar,
Bo Dai, and Bo Xie. Nonparametric estimation of
multi-view latent variable models.
arXiv preprint
arXiv:1311.3287, 2013.
[Yu et al., 2011] Shipeng Yu, Balaji Krishnapuram, Rómer
Rosales, and R Bharat Rao. Bayesian co-training. The
Journal of Machine Learning Research, 12:2649–2680,
2011.

4104

Rank-One Matrix Pursuit for Matrix Completion

Zheng Wang∗
ZHENGWANG @ ASU . EDU
Ming-Jun Lai†
MJLAI @ MATH . UGA . EDU
Zhaosong Lu‡
ZHAOSONG @ SFU . CA
Wei Fan§
DAVID . FANWEI @ HUAWEI . COM
Hasan Davulcu¶
HASANDAVULCU @ ASU . EDU
Jieping Ye∗¶
JIEPING . YE @ ASU . EDU
∗
The Biodesign Institue, Arizona State University, Tempe, AZ 85287, USA
†
Department of Mathematics, University of Georgia, Athens, GA 30602, USA
‡
Department of Mathematics, Simon Fraser University, Burnaby, BC, V5A 156, Canada
§
Huawei Noah’s Ark Lab, Hong Kong Science Park, Shatin, Hong Kong
¶
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281, USA

Abstract
Low rank matrix completion has been applied
successfully in a wide range of machine learning applications, such as collaborative filtering,
image inpainting and Microarray data imputation. However, many existing algorithms are not
scalable to large-scale problems, as they involve
computing singular value decomposition. In this
paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to
extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In
each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of
the current approximation residual and update
the weights for all rank-one matrices obtained
up to the current iteration. We further propose
a novel weight updating rule to reduce the time
and storage complexity, making the proposed algorithm scalable to large matrices. We establish
the linear convergence of the proposed algorithm.
The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world
large-scale datasets. Results show that our algorithm is much more efficient than state-of-theart matrix completion algorithms while achieving
similar or better prediction performance.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

1. Introduction
Low rank matrix learning has attracted significant attention
in the machine learning community due to its wide range
of applications, such as collaborative filtering (Koren et al.,
2009; Srebro et al., 2005), compressed sensing (Candès
& Recht, 2009), multi-class learning and multi-task learning (Argyriou et al., 2008; Negahban & Wainwright, 2010;
Dudı́k et al., 2012). In this paper, we consider the general
form of low rank matrix completion: given a partially observed real-valued matrix Y ∈ <n×m , the low rank matrix
completion problem is to find a matrix X ∈ <n×m with
minimum rank such that PΩ (X) = PΩ (Y), where Ω includes the index pairs (i, j) of all observed entries, and PΩ
is the orthogonal projector onto the span of matrices vanishing outside of Ω. As it is intractable to minimize the
matrix rank exactly in the general case, the trace norm or
nuclear norm is widely used as a convex relaxation of the
matrix rank (Candès & Recht, 2009). It is defined by the
Schatten p-norm with p = 1. ForP
matrix X with rank r, its
r
Schatten p-norm is defined by ( i=1 σip )1/p , where {σi }
are the singular values of X. Thus, the trace norm
Prof X is
the `1 norm of the matrix spectrum as ||X||∗ = i=1 |σi |.
Solving the standard low rank or trace norm problem is
computationally expensive for large matrices, as it involves
computing singular value decomposition (SVD). How to
solve these problems efficiently and accurately has attracted much attention in recent years (Avron et al., 2012;
Srebro et al., 2005; Cai et al., 2010; Balzano et al., 2010;
Keshavan & Oh, 2009; Toh & Yun, 2010; Ji & Ye, 2009;
Ma et al., 2011; Mazumder et al., 2010; Mishra et al.,
2011; Wen et al., 2010; Lee & Bresler, 2010; Recht &
Ré, 2013). Most of these methods still involve the computation of SVD or truncated SVD iteratively, which is

Rank-One Matrix Pursuit for Matrix Completion

not scalable to large-scale problems (Cai et al., 2010; Keshavan & Oh, 2009; Toh & Yun, 2010; Ma et al., 2011;
Mazumder et al., 2010; Lee & Bresler, 2010). Several
methods approximate the trace norm using its variational
characterizations (Mishra et al., 2011; Srebro et al., 2005;
Wen et al., 2010; Recht & Ré, 2013), and proceed by alternating optimization. The linear convergence rate is established theoretically for properly designed alternating optimization algorithm under appropriate initialization (Jain
et al., 2013). However, its computational complexity depends on the square of the rank of the estimated matrix.
Thus in practical problems, especially for large matrices, it
requires the rank of the estimated matrix to be very small,
which sacrifices the estimation accuracy.
Recently, the coordinate gradient descent method has been
demonstrated to be efficient in solving sparse learning
problems in the vector case (Friedman et al., 2010; ShalevShwartz & Tewari, 2009). The key idea is to solve a very
simple one-dimensional problem (for one coordinate) in
each iteration. One natural question is whether and how
such method can be applied to solve the matrix completion problem. Some progress has been made recently along
this direction (Jaggi & Sulovský, 2010; Tewari et al., 2011;
Shalev-Shwartz et al., 2011; Dudı́k et al., 2012; Zhang
et al., 2012). These algorithms proceed in two main steps
in each iteration. The first step involves computing the
top singular vector pair, and the second step refines the
weights of the rank-one matrices formed by all top singular vector pairs obtained up to the current iteration. The
main differences among these algorithms lie in how they
refine the weights. The Jaggi’s algorithm (JS) (Jaggi &
Sulovský, 2010) directly applies the Hazan’s algorithm,
which adapts the Frank-Wolfe algorithm to the matrix case
(Hazan, 2008). It updates the weights with a small step size
and does not consider further refinement. It does not use
all information in each step, which leads to a slow convergence rate. Similar to JS, Tewari et al. (Tewari et al., 2011)
use a small update step size for a general structure constrained problem. A more efficient Frank-Wolfe type algorithm is to fully refine the weights, which is claimed to be
equivalent to orthogonal matching pursuit (OMP) in a wide
range of l1 ball constrained convex optimization problems
(Jaggi, 2013). The greedy efficient component optimization (GECO) (Shalev-Shwartz et al., 2011) applies a similar approach, which optimizes the weights by solving another time consuming optimization problem. It empirically
reduces the number of iterations without theoretical guarantees. However, the sophisticated weight refinement leads
to a higher total computational cost. The lifted coordinate
gradient descent algorithm (Lifted) (Dudı́k et al., 2012) updates the rank-one matrix basis with a constant weight in
each iteration, and conducts a lasso type algorithm (Tibshirani, 1994) to fully correct the weights. The weights for

the basis update are difficult to tune: a large value leads to
divergence; a small value makes the algorithm slow (Zhang
et al., 2012). The matrix norm boosting approach (Boost)
(Zhang et al., 2012) learns the update weights and designs
a local refinement step by a non-convex optimization problem which is solved by alternating optimization. It has a
sub-linear convergence rate.
In this paper, we present a simple and efficient algorithm
to solve the low rank matrix completion problem. The key
idea is to extend the orthogonal matching pursuit procedure
(Pati et al., 1993) from the vector case to the matrix case. In
each iteration, a rank-one basis matrix is generated by the
left and right top singular vectors of the current approximation residual. In the standard algorithm, we fully update
the weights for all rank-one matrices in the current basis
set at the end of each iteration by performing an orthogonal projection of the observation matrix onto their spanning
subspace. The most time-consuming step of the proposed
algorithm is to calculate the top singular vector pair of a
sparse matrix, which costs O(|Ω|) operations in each iteration. An appealing feature of the proposed algorithm is that
it has a linear convergence rate. This is quite different from
traditional orthogonal matching pursuit or weak orthogonal
greedy algorithms, whose convergence rate for sparse vector recovery is sub-linear as shown in (Liu & Temlyakov,
2012). See also (Tropp, 2004) for an extensive study on
various greedy algorithms. With this rate of convergence,
we only need O(log(1/)) iterations for achieving an accuracy solution. One drawback of the standard algorithm
is that it needs to store all rank-one matrices in the current
basis set for full weight updating, which contains r|Ω| elements in the r-th iteration. This makes the storage complexity of the algorithm dependent on the number of iterations, which restricts the approximation rank especially
for large matrices. To tackle this problem, we propose an
economic weight updating rule for this algorithm. In this
economic algorithm, we only track two matrices in each iteration. One is the current estimated matrix and the other
one is the pursued rank-one matrix. When restricted to the
observations in Ω, each has |Ω| nonzero elements. Thus
the storage requirement, i.e., 2|Ω|, keeps the same in different iterations, which is the same as the greedy algorithms
(Jaggi & Sulovský, 2010; Tewari et al., 2011). Interestingly, we show that using this economic updating rule we
still retain the linear convergence rate. To the best of our
knowledge, our proposed algorithms are the fastest among
all related methods. We verify the efficiency of our algorithms empirically on large-scale matrix completion problems.
The main contributions of our paper are:
• We propose a computationally efficient and scalable
algorithm for matrix completion, which extends the

Rank-One Matrix Pursuit for Matrix Completion

orthogonal matching pursuit from the vector case to
the matrix case.
• We theoretically prove the linear convergence rate of
our algorithm. As a result, we only need O(log(1/))
steps to obtain an -accuracy solution, and in each step
we only need to compute the top singular vector pair,
which can be computed efficiently.
• We further reduce the storage complexity of our algorithm based on an economic weight updating rule
while retaining the linear convergence rate. This algorithm has constant storage complexity which is independent of the approximation rank and is more practical for large-scale problems.
• Our proposed algorithm is free of tuning parameter,
except for the accuracy of the solution. And it is guaranteed to converge, i.e., no risk of divergence.
Notations: Let Y = (y1 , · · · , ym ) ∈ <n×m be an n × m
real matrix, and Ω ⊂ {1, · · · , n} × {1, · · · , m} denote the
indices of the observed entries of Y. PΩ is the projection
operator onto the space spanned by the matrices vanishing outside of Ω so that the (i, j)-th component of PΩ (Y)
equals to Yi,j for (i, j) ∈ Ω and zero otherwise.
The
qP
2
Frobenius norm of Y is defined as ||Y||F =
i,j Yi,j .
T T
) denote a vector reshaped
Let vec(Y) = (y1T , · · · , ym
from matrix Y by concatenating all its column vectors. Let
ẏ = vec(PΩ (Y)) be the vector by concatenating all observed entries in Y. The inner product of two matrices X
and Y is defined as hX, Yi = hvec(X), vec(Y)i. Given a
matrix A ∈ <n×m , we denote PΩ (A) by AΩ . For any two
n×m
matrices A,
, we definephA, BiΩ = hAΩ , BΩ i,
pB ∈ <
kAkΩ = hA, AiΩ and kAk = hA, Ai.

2. Rank-One Matrix Pursuit
It is well-known that any matrix X ∈ <n×m can be written
as a linear combination of rank-one matrices, that is,
X
X = M(θ) =
θi Mi ,
(1)
i∈I

where {Mi : i ∈ I} is the set of all n × m rank-one
matrices with unit Frobenius norm. Clearly, θ is an infinite
dimensional vector. Such a representation can be obtained
from the standard SVD of X.
The original low rank matrix approximation problem aims
to minimize the zero-norm of θ subject to the constraint:
min ||θ||0
θ

s.t.

PΩ (M(θ)) = PΩ (Y),

(2)

where ||θ||0 denotes the cardinality of the number of
nonzero elements of θ.

If we reformulate the problem as
min ||PΩ (M(θ)) − PΩ (Y)||2F
θ

s.t.

||θ||0 ≤ r,

(3)

we could solve it by an orthogonal matching pursuit type
greedy algorithm using rank-one matrices as the basis. If
the dictionary {Mi : i ∈ I} is known and finite, this is
equivalent to the compressed sensing problem. However,
in our formulation, the size of the dictionary is infinite and
the bases are to be constructed during the basis pursuit process. In particular, we are to find a suitable subset with
over-complete rank-one matrix coordinates, and learn the
weight for each coordinate. This is achieved by executing
two steps alternatively: one is to construct the basis, and
the other one is to learn the weights of the basis.
Suppose that after the (k-1)-th iteration, the rank-one basis matrices M1 , . . . , Mk−1 and their current weight θ k−1
are already computed. In the k-th iteration, we are to
pursue a new rank-one basis matrix Mk with unit Frobenius norm, which is mostly correlated with the current observed regression residual Rk = PΩ (Y) − Xk−1 , where
Pk−1
Xk−1 = (M(θ k−1 ))Ω = i=1 θik−1 (Mi )Ω . Therefore,
Mk can be chosen to be an optimal solution of the following problem:
max{hM, Rk i : rank(M) = 1, kMkF = 1}.
M

(4)

Notice that each rank-one matrix M with unit Frobenius
norm can be written as the product of two unit vectors,
namely, M = uvT for some u ∈ <n and v ∈ <m with
kuk = kvk = 1. We then see that problem (4) can be
equivalently reformulated as
max{uT Rk v : kuk = kvk = 1}.
u,v

(5)

Clearly, the optimal solution (u∗ , v∗ ) of problem (5) is
a pair of top left and right singular vectors of Rk . It
can be efficiently computed by the power method (Jaggi
& Sulovský, 2010; Dudı́k et al., 2012). The new rankone basis matrix Mk is then readily available by setting
Mk = u∗ v∗T .
After finding the new rank-one basis matrix Mk , we update the weights θ k for all currently available basis matrices {M1 , · · · , Mk } by solving the following least squares
regression problem:
min ||

θ∈<k

k
X

θi Mi − Y||2Ω .

(6)

i=1

By reshaping the matrices (Y)Ω and (Mi )Ω into vectors ẏ
and ṁi , we can easily see that the optimal solution θ k of
(6) is given by
θ k = (M̄Tk M̄k )−1 M̄Tk ẏ,

(7)

Rank-One Matrix Pursuit for Matrix Completion

where M̄k = [ṁ1 , · · · , ṁk ] is the matrix formed by all
reshaped basis vectors. The row size of matrix M̄k is the
total number of observed entries. It is computationally expensive to directly calculate the matrix multiplication. An
incremental update rule can be applied to solve this step
efficiently (Wang et al., 2014).
We run the above two steps iteratively until some desired stopping condition is satisfied. We can terminate the
method based on the rank of the estimated matrix or the approximation residual. In particular, one can choose a preferred rank of the approximate solution matrix. Alternatively, one can stop the method once the residual kRk k is
less than a tolerance parameter ε. The main steps of RankOne Matrix Pursuit (R1MP) are given in Algorithm 1.
Remark In our algorithm, we adapt orthogonal matching
pursuit on the observed part of the matrix. This is similar to the GECO algorithm. However, GECO constructs
the estimated matrix by projecting the observation matrix
onto a much larger subspace, which is a product of two
subspaces spanned by all left singular vectors and all right
singular vectors obtained up to the current iteration. So
it has much higher computational complexity. Lee et al.
(Lee & Bresler, 2010) recently propose the ADMiRA algorithm, which is also a greedy approach. In each step it first
chooses 2r components by top-2r truncated SVD and then
uses another top-r truncated SVD to obtain a rank-r matrix.
Thus, the ADMiRA algorithm is computationally more expensive than the proposed algorithm. The main difference
between the proposed algorithm and ADMiRA is somewhat similar to the difference between the OMP (Pati et al.,
1993) for learning sparse vectors and CoSaMP (Needell &
Tropp, 2010). In addition, the performance guarantees (including recovery guarantee and convergence property) of
ADMiRA rely on strong assumptions, i.e., the matrix involved in the loss function satisfies a rank-restricted isometry property, which is not satisfied in matrix completion
(Lee & Bresler, 2010). Lee et al. sketch a similar idea as
the standard verion of our algorithm in Remark 2.3 without
any further analysis, and their theoretical results cannot be
easily extended to our algorithm. Another contribution of
our work is that we further propose an economic version of
the algorithm and analyze its convergence property.

3. Convergence Analysis
In this section, we will show that our proposed rank-one
matrix pursuit algorithm achieves a linear convergence rate.
This main result is given in the following theorem.
Theorem 3.1. The rank-one matrix pursuit algorithm satisfies
||Rk || ≤ γ k−1 kYkΩ , ∀k ≥ 1.
γ is a constant in [0, 1).

Algorithm 1 Rank-One Matrix Pursuit (R1MP)
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed residual matrix Rk =
YΩ − Xk−1 and set Mk = uk (vk )T .
Step 2: Compute the weight θ k using the closed form
least squares solution θ k = (M̄Tk M̄k )−1 M̄Tk ẏ.
Pk
Step 3: Set Xk = i=1 θik (Mi )Ω and k ← k + 1.
until stopping criterion is satisfied
Pk
Output: Constructed matrix Ŷ = i=1 θik Mi .

Before proving Theorem 3.1, we need to establish some
useful and preparatory properties of Algorithm 1. The first
property says that Rk+1 is perpendicular to all previously
generated Mi for i = 1, · · · , k.
Property 3.2. hRk+1 , Mi i = 0 for i = 1, · · · , k.
Proof. Recall that θ k is the optimal solution of problem (6). By the first-order optimality condition, one has
Pk
hY − j=1 θjk Mj , Mi iΩ = 0 for i = 1, · · · , k, which toPk
gether with Rk = YΩ − Xk−1 and Xk = j=1 θjk (Mj )Ω
implies that hRk+1 , Mi i = 0 for i = 1, · · · , k.
The following property shows that as the number of rankone basis matrices Mi increases during our learning process, the residual kRk k does not increase.
Property 3.3. kRk+1 k ≤ kRk k for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
Pk
kRk+1 k2 = min {kY − i=1 θi Mi k2Ω }
θ∈<k
Pk−1
≤ min {kY − i=1 θi Mi k2Ω } = kRk k2 ,
θ∈<k−1

and hence the conclusion holds.
We next establish that {(Mi )Ω }ki=1 is linearly independent
unless kRk k = 0. It follows that formula (7) is welldefined and hence θ k is uniquely defined before the algorithm stops.
Property 3.4. Suppose that Rk 6= 0 for some k ≥ 1. Then,
M̄i has a full column rank for all i ≤ k.
Proof. Using Property 3.3 and the assumption Rk 6= 0
for some k ≥ 1, we see that Ri 6= 0 for all i ≤ k.
We now prove this statement by induction on i. Indeed,
since R1 6= 0, we clearly have M̄1 6= 0. Hence the conclusion holds for i = 1. We now assume that it holds
for i − 1 < k and need to show that it also holds for
i ≤ k. By the induction hypothesis, M̄i−1 has a full column rank. Suppose for contradiction that M̄i does not have
a full column rank. Then, there exists α ∈ <i−1 such that

Rank-One Matrix Pursuit for Matrix Completion

Pi−1
(Mi )Ω = j=1 αj (Mj )Ω , which together with Property
3.2 implies that hRi , Mi i = 0. It follows that σmax (Ri ) =
uTi Ri vi = hRi , Mi i = 0, and hence Ri = 0, which contradicts the fact that Rj 6= 0 for all j ≤ k. Therefore, M̄i
has a full column rank and the conclusion holds.

Substituting M̄k = QU into (10), and using QT Q = I
and (11), we obtain that

We next build a relationship between two consecutive
residuals kRk+1 k and kRk k.

= [0, · · · , 0, hMk , Rk i] U−1 U−T [0, · · · , 0, hMk , Rk i]

For convenience, define θkk−1 = 0 and let θ k = θ k−1 +η k .
In view of (6), one can observe that
η k = arg min ||
η

Let
Lk =

k
X

ηi Mi − Rk ||2Ω .

(8)

i=1
k
X

kLk k2
= ṙTk M̄k (UT U)−1 M̄Tk ṙk
T

= hMk , Rk i2 /(Ukk )2 ≥ hMk , Rk i2 .
The last equality follows since U is upper triangular and
the last inequality is due to |Ukk | ≤ 1.
We are now ready to prove Theorem 3.1.
Proof. Using the definition of Mk , we have

ηik (Mi )Ω .

(9)

hMk , Rk i = huk (vk )T , Rk i = σ∗ (Rk ),

i=1

By the definition of Xk , one can also observe that Xk =
Xk−1 + Lk and Rk+1 = Rk − Lk .
Property 3.5. ||Rk+1 ||2 = ||Rk ||2 −||Lk ||2 and ||Lk ||2 ≥
hMk , Rk i2 , where Lk is defined in (9).
P
Proof. Since Lk = i≤k ηik (Mi )Ω , it follows from Property 3.2 that hRk+1 , Lk i = 0. Thus,
||Rk+1 ||2
= ||Rk − Lk ||2 = ||Rk ||2 − 2hRk , Lk i + ||Lk ||2
= ||Rk ||2 − 2hRk+1 + Lk , Lk i + ||Lk ||2
= ||Rk ||2 − 2hLk , Lk i + ||Lk ||2
= ||Rk ||2 − ||Lk ||2
We next bound kLk k2 from below. If Rk = 0, ||Lk ||2 ≥
hMk , Rk i2 clearly holds. We now suppose throughout
the remaining proof that Rk 6= 0. It then follows from
Property 3.4 that M̄k has a full column rank. Using this
−1 T
fact and (8), we have η k = M̄Tk M̄k
M̄k ṙk , where
ṙk is the
reshaped
residual
vector
of
R
.
Invoking that
k
P
Lk = i≤k ηik (Mi )Ω , we then obtain
||Lk ||2 = ṙTk M̄k (M̄Tk M̄k )−1 M̄Tk ṙk .

(10)

Let M̄k = QU be the QR factorization of M̄k , where
QT Q = I and U is a k × k nonsingular upper triangular
matrix. One can observe that (M̄k )k = ṁk , where (M̄k )k
denotes the k-th column of the matrix M̄k and ṁk is the reshaped vector of (Mk )Ω . Recall that kMk k = kuk vkT k =
1. Hence, k(M̄k )k k ≤ 1. Due to QT Q = I, M̄k = QU
and the definition of U, we have
0 < |Ukk | ≤ kUk k = k(M̄k )k k ≤ 1.
In addition, by Property 3.2, we have
T

M̄Tk ṙk = [0, · · · , 0, hMk , Rk i] .

(11)

where σ∗ (Rk ) is the maximum singular value of the residual matrix Rk . Using this inequality and Property 3.5, we
obtain that
||Rk+1 ||2

= ||Rk ||2 − ||Lk ||2
≤ ||Rk ||2 − hMk , Rk i2


σ∗2 (Rk )
= 1 − kR
||Rk ||2 .
2
k
k

In view of this relation and the fact that kR1 k = kYk2Ω ,
we easily conclude that
s
k−1
Y
σ 2 (Ri )
||Rk || ≤ kYkΩ
1− ∗ 2 .
kRi k
i=1
1
∗ (Ri )
As for each step we have 0 < rank(R
≤ σkR
≤ 1,
i)
ik
there must exist 0 ≤ γ < 1 that satisfies ||Rk || ≤
γ k−1 kYkΩ . This completes the proof.
2

ik
Remark In practice, the value of σkR
that controls the
2
∗ (Ri )
convergence speed is much less than min(m, n). We will
emprically verify this in the experiments.

Remark If Ω is the entire set of all indices of {(i, j), i =
1, · · · , m, j = 1, · · · , n}, our rank-one matrix pursuit algorithm equals to standard SVD using the power method.
Remark This convergence is obtained for the optimization
residual in the low rank matrix completion problem. We
further extend our algorithm to solve the more general matrix sensing problem and analyze the corresponding statistical convergence behavior under mild conditions, such as
the rank-restricted isometry property (Lee & Bresler, 2010;
Jain et al., 2013). Details are provided in the longer version
of this paper (Wang et al., 2014).

Rank-One Matrix Pursuit for Matrix Completion

4. Economic Rank-One Matrix Pursuit

5. Experiments

The proposed R1MP algorithm has to track all pursued
bases and save them in the memory. It demands O(r|Ω|)
storage complexity to obtain a rank-r estimated matrix. For
large-scale problems, such storage requirement is not negligible and restricts the rank of the matrix to be estimated.
To adapt our algorithm to large-scale problems with a large
approximation rank, we simplify the orthogonal projection
step by only tracking the estimated matrix Xk−1 and the
rank-one update matrix Mk . In this case, we only need to
estimate the weights for these two matrices in Step 2 of our
algorithm by solving the following least squares problem:

In this section, we compare our rank-one matrix pursuit algorithms R1MP and ER1MP with state-of-the-art matrix
completion algorithms. The competing algorithms include:
singular value projection (SVP) (Jain et al., 2010), singular value thresholding (SVT) (Candès & Recht, 2009),
Jaggi’s fast algorithm for trace norm constraint (JS) (Jaggi
& Sulovský, 2010), spectral regularization algorithm (SoftImpute) (Mazumder et al., 2010), low rank matrix fitting
(LMaFit) (Wen et al., 2010), alternating minimization (AltMin) (Jain et al., 2013), boosting type accelerated matrixnorm penalized solver (Boost) (Zhang et al., 2012) and
greedy efficient component optimization (GECO) (ShalevShwartz et al., 2011). The general greedy method (Tewari
et al., 2011) is not included in our comparison, as it includes JS and GECO (included in our comparison) as special cases for matrix completion. The lifted coordinate descent method (Lifted) (Dudı́k et al., 2012) is not included in
our comparison, as it is similar to Boost proposed in (Zhang
et al., 2012), but more sensitive to the parameters.

αk = arg

min

α={α1 ,α2 }

||α1 Xk−1 + α2 Mk − Y||2Ω . (12)

This still corrects all weights of the existed bases, though
the correction is sub-optimal. If we write the estimated matrix
of the bases, we have Xk =
Pk as ak linear combination
k
θ
(M
)
with
θ
=
α2k and θik = θik−1 α1k , for
i Ω
k
i=1 i
i < k. The detailed procedure of this simplified method
is given in Algorithm 2.

The code for most of these algorithms is available online:
Algorithm 2 Economic Rank-One Matrix Pursuit (ER1MP)
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed residual matrix Rk =
YΩ − Xk−1 and set Mk = uk (vk )T .
Step 2: Compute the optimal weights αk for Xk−1
and Mk by solving:
arg min ||α1 Xk−1 + α2 (Mk )Ω − YΩ ||2 .
α

Step 3: Set Xk = α1k Xk−1 + α2k (Mk )Ω ; θkk = α2k
and θik = θik−1 α1k for i < k; k ← k + 1.
until stopping criterion is satisfied
Pk
Output: Constructed matrix Ŷ = i=1 θik Mi .
The proposed economic rank-one matrix pursuit algorithm
(ER1MP) uses the same amount of storage as the greedy
algorithms (Jaggi & Sulovský, 2010; Tewari et al., 2011),
which is significantly smaller than that required by R1MP
algorithm. Interestingly, we can show that the ER1MP algorithm still retains the linear convergence rate. The main
result is given in the following theorem, and the proof is
provided in the long version of this paper (Wang et al.,
2014).
Theorem 4.1. The economic rank-one matrix pursuit algorithm satisfies
||Rk || ≤ γ̃ k−1 kYkΩ , ∀k ≥ 1.
γ̃ is a constant in [0, 1).

• SVP:
http://www.cs.utexas.edu/∼pjain/svp/
• SVT:
http://svt.stanford.edu/
• SoftImpute:
http://www-stat.stanford.edu/∼rahulm/software.html
• LMaFit:
http://lmafit.blogs.rice.edu/
• Boost:
http://webdocs.cs.ualberta.ca/∼xinhua2/boosting.zip
• GECO:
http://www.cs.huji.ac.il/∼shais/code/geco.zip
We compare these algorithms in two problems, including
image recovery and collaborative filtering. The data size
for image recovery is relatively small, and the recommendation problem is in large-scale. In the experiments, we follow the recommended settings of the parameters for competing algorithms. If no recommended parameter value
is available, we choose the best one from a candidate set
using cross validation. For our R1MP and ER1MP algorithms, we only need a stopping criterion. For simplicity, we stop our algorithms after r iterations. In this
way, we approximate the ground truth using a rank-r matrix. We present the experimental results using root-meansquare error (RMSE) (Jaggi & Sulovský, 2010; ShalevShwartz et al., 2011). The experiments are implemented
in MATLAB1 . They call some external packages for fast
1
GECO is written in C++ and we call its executable file in
MATLAB.

Rank-One Matrix Pursuit for Matrix Completion
Table 1. Image recovery results measured in terms of the RMSE: the value below is the actual value times 100 (mean ± std).
Image

Lenna
Barbara
Clown
Crowd
Girl
Man

SVT

SVP

SoftImpute

LMaFit

AltMin

JS

R1MP

ER1MP

3.86 ± 0.02
4.48 ± 0.02
3.72 ± 0.03
4.48 ± 0.02
3.36 ± 0.02
4.49 ± 0.03

5.31 ± 0.14
5.60 ± 0.08
10.97 ± 0.17
7.62 ± 0.13
4.45 ± 0.16
5.52 ± 0.10

4.60 ± 0.02
5.22 ± 0.01
4.48 ± 0.03
5.35 ± 0.02
4.10 ± 0.01
5.16 ± 0.03

7.45 ± 0.63
5.16 ± 0.28
4.65 ± 0.67
4.91 ± 0.05
4.12 ± 0.48
5.31 ± 0.13

4.47 ± 0.10
5.05 ± 0.06
5.49 ± 0.46
4.87 ± 0.02
5.07 ± 0.50
5.19 ± 0.11

5.48 ± 0.72
6.52 ± 0.88
7.30 ± 2.32
7.38 ± 1.41
4.42 ± 0.46
6.25 ± 0.54

3.90 ± 0.02
4.63 ± 0.01
3.85 ± 0.03
4.89 ± 0.03
3.09 ± 0.02
4.66 ± 0.03

3.97 ± 0.02
4.73 ± 0.02
3.91 ± 0.03
4.96 ± 0.03
3.12 ± 0.02
4.76 ± 0.03

computation of SVD2 and sparse matrix computations. The
experiments are run in a PC with WIN7 system, Intel 4 core
3.4 GHz CPU and 8G RAM.
5.1. Image Recovery
In the image recovery experiments, we use the following
benchmark test images: Lenna, Barbara, Clown, Crowd,
Girl, Man3 . The size of each image is 512 × 512. For each
experiment, we present the average RMSE and the corresponding standard derivation of 10 different runs for each
competing algorithm. In each run, we randomly exclude
50% of the pixels in the image, and the remaining ones
are used as the observations. As the image matrix is not
guaranteed to be low rank, we use the rank 200 for the estimation matrix for each experiment. The JS algorithm does
not explicitly control the rank, thus we fix its number of
iterations to 2000. The numerical results are listed in Table 1. The results show that SVT, our R1MP and ER1MP
achieve the best numerical performance. However, our algorithm is much faster and more stable than SVT. For each
image, ER1MP uses around 3.5 seconds, but SVT consumes around 400 seconds. Image recovery needs a relatively higher approximation rank; GECO and Boost fail
to find a good recovery in some cases, so we do not include
them in the table.

Lens datasets were collected from the MovieLens website4 .
They contain anonymous ratings of the movies on this
web made by its users. For MovieLens100K and MovieLens1M, there are 5 rating scores (1–5), and for MovieLens10M there are 10 levels of scores with a step size 0.5
in the range of 0.5 to 5. In the following experiments, we
randomly split the ratings into training and test sets. Each
set contains 50% of the ratings. We compare the prediction results from different methods. In the experiments, we
use 100 iterations for the JS algorithm, and for other algorithms we use the same rank for the estimated matrices; the
values of the rank are {10, 10, 5, 10, 10, 20} for the six corresponding datasets. The results in terms of the RMSE is
given in Table 3. We also show the running time of different
methods in Table 4. We can observe from the above experiments that our ER1MP algorithm is the fastest among all
competing methods to obtain satisfactory results.
Table 2. Characteristics of the recommendation datasets.
Dataset
Jester1
Jester2
Jester3
MovieLens100k
MovieLens1M
MovieLens10M

# row
24983
23500
24938
943
6040
69878

# column
100
100
100
1682
3706
10677

# rating
106
106
6×105
105
106
107

5.2. Recommendation
In the following experiments, we compare the different
matrix completion algorithms using large recommendation datasets: Jester (Goldberg et al., 2001) and MovieLens (Miller et al., 2003). We use six datasets including:
Jester1, Jester2, Jester3, MovieLens100K, MovieLens1M,
and MovieLens10M. The statistics of these datasets are
given in Table 2. The Jester datasets were collected from
a joke recommendation system. They contain anonymous
ratings of 100 jokes from the users. The ratings are
real values ranging from −10.00 to +10.00. The Movie2

PROPACK is used in SVP, SVT, SoftImpute and Boost. It is
an efficient SVD package, which can be downloaded from http:
//soi.stanford.edu/˜rmunk/PROPACK/
3
Images are downloaded from http://www.utdallas.
edu/˜cxc123730/mh_bcs_spl.html

5.3. Convergence and Efficiency
We present the residual curves on the Lenna image in logarithmic scale for our R1MP and ER1MP algorithms in Figure 1. The results show that our algorithms reduce the approximation error in a linear rate. This is consistent with
our theoretical analysis. The empirical results verify the
linear convergence property of our proposed algorithms.

6. Conclusion
In this paper, we propose an efficient and scalable low rank
matrix completion algorithm. The key idea is to extend orthogonal matching pursuit method from the vector case to
the matrix case. We also propose a novel weight updating
4

http://movielens.umn.edu

Rank-One Matrix Pursuit for Matrix Completion
Table 3. Recommendation results measured in terms of the RMSE. Boost fails on the MovieLens10M.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
4.7311
4.7608
8.6958
0.9683
0.9085
0.8611

SoftImpute
5.1113
5.1646
5.4348
1.0354
0.8989
0.8534

LMaFit
4.7623
4.7500
9.4275
1.2308
0.9232
0.8625

AltMin
4.8572
4.8616
9.7482
1.0042
0.9382
0.9007

Boost
5.1746
5.2319
5.3982
1.1244
1.0850
–

JS
4.4713
4.5102
4.6866
1.0146
1.0439
0.8728

GECO
4.3680
4.3967
5.1790
1.0243
0.9290
0.8668

R1MP
4.3418
4.3649
4.9783
1.0168
0.9595
0.8621

ER1MP
4.3384
4.3546
5.0145
1.0261
0.9462
0.8692

Table 4. The running time (measured in seconds) of all methods on all recommendation datasets.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
18.35
16.85
16.58
1.32
18.90
> 103

SoftImpute
161.49
152.96
10.55
128.07
59.56
> 103

Lenna
−1

RMSE

10

−2

10

AltMin
11.14
10.47
12.23
3.23
68.77
310.82

Boost
93.91
261.70
245.79
2.87
93.91
–

JS
29.68
28.52
12.94
2.86
13.10
130.13

GECO
> 104
> 104
> 103
10.83
> 104
> 105

R1MP
1.83
1.68
0.93
0.04
0.87
23.05

ER1MP
0.99
0.91
0.34
0.04
0.54
13.79

7. Acknowledgments

Lenna

−1

10

RMSE

LMaFit
3.68
2.42
8.45
2.76
30.55
154.38

This work was supported in part by China 973 Fundamental
R&D Program (No.2014CB340304), NIH (LM010730),
and NSF (IIS-0953662, CCF-1025177).

−2

10

References
−3

10

0

−3

50

100

150

rank

200

250

300

10

0

50

100

150

200

250

300

rank

Figure 1. Illustration of the linear convergence of the proposed
rank-one matrix pursuit algorithms on the Lenna image: the xaxis is the iteration, and the y-axis is the RMSE in logarithmic
scale. The curves are the results for R1MP and ER1MP respectively.

rule under this framework to reduce the storage complexity
and make it independent of the approximation rank. Our algorithms are computationally inexpensive for each matrix
pursuit iteration, and find satisfactory results in a few iterations. Another advantage of our proposed algorithms is
they have only one tunable parameter, which is the rank. It
is easy to understand and to use by the user. This becomes
especially important in large-scale learning problems. In
addition, we rigorously show that both algorithms achieve
a linear convergence rate, which is significantly better than
the previous known results (a sub-linear convergence rate).
We also empirically compare the proposed algorithms with
state-of-the-art matrix completion algorithms, and our results show that the proposed algorithms are more efficient
than competing algorithms while achieving similar or better prediction performance. We plan to generalize our theoretical and empirical analysis to other loss functions in the
future.

Argyriou, A., Evgeniou, T., and Pontil, M. Convex multitask feature learning. Machine Learning, 73(3):243–
272, 2008.
Avron, H., Kale, S., Kasiviswanathan, S., and Sindhwani,
V. Efficient and practical stochastic subgradient descent
for nuclear norm regularization. In ICML, 2012.
Balzano, L., Nowak, R., and Recht, B. Online identification and tracking of subspaces from highly incomplete
information. In Allerton, 2010.
Cai, J., Candès, E. J., and Shen, Z. A singular value thresholding algorithm for matrix completion. SIAM Journal
on Optimization, 20(4):1956–1982, 2010.
Candès, E. J. and Recht, B. Exact matrix completion
via convex optimization. Foundations of Computational
Mathematics, 9(6):717–772, 2009.
Dudı́k, M., Harchaoui, Z., and Malick, J. Lifted coordinate
descent for learning with trace-norm regularization. In
AISTATS, 2012.
Friedman, J. H., Hastie, T., and Tibshirani, R. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.

Rank-One Matrix Pursuit for Matrix Completion

Goldberg, K., Roeder, T., Gupta, D., and Perkins, C. Eigentaste: A constant time collaborative filtering algorithm.
Information Retrieval, 4(2):133–151, 2001.

Negahban, S. and Wainwright, M.J. Estimation of (near)
low-rank matrices with noise and high-dimensional scaling. In ICML, 2010.

Hazan, E. Sparse approximate solutions to semidefinite
programs. In LATIN, 2008.

Pati, Y. C., Rezaiifar, R., Rezaiifar, Y. C. Pati R., and Krishnaprasad, P. S. Orthogonal matching pursuit: Recursive
function approximation with applications to wavelet decomposition. In Asilomar SSC, 1993.

Jaggi, M. Revisiting Frank-Wolfe: Projection-free sparse
convex optimization. In ICML, 2013.
Jaggi, M. and Sulovský, M. A simple algorithm for nuclear
norm regularized problems. In ICML, 2010.
Jain, P., Meka, R., and Dhillon, I. S. Guaranteed rank minimization via singular value projection. In NIPS, 2010.
Jain, P., Netrapalli, P., and Sanghavi, S. Low-rank matrix
completion using alternating minimization. In STOC,
2013.
Ji, S. and Ye, J. An accelerated gradient method for trace
norm minimization. In ICML, 2009.
Keshavan, R. and Oh, S. Optspace: A gradient descent
algorithm on grassmann manifold for matrix completion.
http://arxiv.org/abs/0910.5260, 2009.
Koren, Y., Bell, R., and Volinsky, C. Matrix factorization
techniques for recommender systems. Computer, 2009.
Lee, K. and Bresler, Y. Admira: atomic decomposition for
minimum rank approximation. IEEE Transactions on
Information Theory, 56(9):4402–4416, 2010.
Liu, E. and Temlyakov, T. N. The orthogonal super
greedy algorithm and applications in compressed sensing. IEEE Transactions on Information Theory, 58:
2040–2047, 2012.
Ma, S., Goldfarb, D., and Chen, L. Fixed point and
bregman iterative methods for matrix rank minimization.
Mathematical Programming, 128(1-2):321–353, 2011.
Mazumder, R., Hastie, T., and Tibshirani, R. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 99:2287–
2322, August 2010.
Miller, B. N., Albert, I., Lam, S. K., Konstan, J. A., and
Riedl, J. Movielens unplugged: experiences with an
occasionally connected recommender system. In IUI,
2003.
Mishra, B., Meyer, G., Bach, F., and Sepulchre,
R. Low-rank optimization with trace norm penalty.
http://arxiv.org/abs/1112.2318, 2011.
Needell, D. and Tropp, J. A. Cosamp: iterative signal recovery from incomplete and inaccurate samples. Communications of the ACM, 53(12):93–100, 2010.

Recht, B. and Ré, C. Parallel stochastic gradient algorithms
for large-scale matrix completion. Mathematical Programming Computation, 5(2):201–226, 2013.
Shalev-Shwartz, S. and Tewari, A. Stochastic methods for
`1 regularized loss minimization. In ICML, 2009.
Shalev-Shwartz, S., Gonen, A., and Shamir, O. Largescale convex minimization with a low-rank constraint.
In ICML, 2011.
Srebro, N., Rennie, J., and Jaakkola, T. Maximum margin
matrix factorizations. In NIPS, 2005.
Tewari, A., Ravikumar, P., and Dhillon, I. S. Greedy algorithms for structurally constrained high dimensional
problems. In NIPS, 2011.
Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58:267–288, 1994.
Toh, K. and Yun, S. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Pacific Journal of Optimization, 6:615 – 640,
2010.
Tropp, J. A. Greed is good: algorithmic results for sparse
approximation. IEEE Transactions on Information Theory, 50:2231–2242, 2004.
Wang, Z., Lai, M., Lu, Z., Fan, W., Davulcu, H., and Ye, J.
Orthogonal rank-one matrix pursuit for low rank matrix
completion. http://arxiv.org/abs/1404.1377, 2014.
Wen, Z., Yin, W., and Zhang, Y. Low-rank factorization
model for matrix completion by a non-linear successive
over-relaxation algorithm. Rice CAAM Tech Report 1007, University of Rice, 2010.
Zhang, X., Yu, Y., and Schuurmans, D. Accelerated
training for matrix-norm regularization: A boosting approach. In NIPS, 2012.

A Semantic Triplet Based Story Classifier
Betul Ceran∗ , Ravi Karad∗ , Ajay Mandvekar∗ , Steven R. Corman† and Hasan Davulcu∗
∗
School of Computing, Informatics and Decision Systems Engineering
Arizona State University, Tempe, AZ 85287-8809.
Email:{betul, rkarad, amandvek, hdavulcu}@asu.edu
†
Hugh Downs School of Human Communication,
Arizona State University, Tempe, AZ 85287-1205.
Email:steve.corman@asu.edu
Abstract
A story is defined as “an actor(s) taking action(s)
that culminates in a resolution(s).” In this paper, we investigate the utility of standard keyword based features,
statistical features based on shallow-parsing (such as
density of POS tags and named entities), and a new
set of semantic features to develop a story classifier.
This classifier is trained to identify a paragraph as
a “story,” if the paragraph contains mostly story(ies).
Training data is a collection of expert-coded story and
non-story paragraphs from RSS feeds from a list of
extremist web sites. Our proposed semantic features
are based on suitable aggregation and generalization
of <Subject, Verb, Object> triplets that can be extracted using a parser. Experimental results show that
a model of statistical features alongside memory-based
semantic linguistic features achieves the best accuracy
with a Support Vector Machine (SVM) classifier.

1. Introduction
Personal narratives are powerful sources of persuasion, none more so than stories that cultural heroes
tell about their own lives [1]. Whether their account
retells the story of a great athlete or actor or celebrity
or terrorist, fans are drawn to these accounts as moths
to bright lights. In part this is because the stories
themselves can be quite interesting, and in part because
readers often closely want to in some way identify
their own lives with the life stories of their heroes [2].
An investigation of terrorist narrative communication
through an in-depth examination of extremists published autobiographies and interviews can be helpful in
understanding mindsets and motivation behind terrorist
activities. In addition, the analysis of terrorist narratives across geographical regions holds the potential

to illustrate cultural differences, as well as to illustrate
how telling their own stories serves to recruit and
assimilate outsiders into local political groups and
extremist organizations. But the problem with analysis
of extremist text is that it needs many human annotators to extract stories and non stories from different
sources. The main purpose of developing an automated
story classifier is to reduce the human dependency to
annotate story and non-stories.
A story is comprised of three components. First,
there must be an actor or actors telling the story
implicitly or explicitly. This can include politicians,
mujahedeen, everyday people and so on. Second, the
actors must be performing actions. This can include
fighting, preparing for a battle, talking to others and so
on. Third, the actors actions must result in a resolution.
Resolutions can include a new state of affairs, a new
equilibrium created, a previous equilibrium restored,
victory and so on. Besides, stories usually have story
worlds, or worlds where the stories are taking place.
Story worlds are not fictional universes, but rather
environments in which the story takes place.
Story example: “They have planted your remains in
the sands like a flag. To motivate the people morning
and night, woe unto them, they have raised a beacon of
blood To inspire tomorrow’s generation with hate and
dislike”. A non-story paragraph is one, among the categories Exposition, Supplication, Question, Annotation,
Imperative, Verse or Other. Non-Story example: “Let
the soldiers of this Administration go to hell. Petraeus
and Bush are trying to convince the Americans that
their salvation will begin six weeks from next July.
In fact even if Bush keeps all his forces in Iraq until
doomsday and until they go to hell, they will face only
defeat and incur loss, God willing.” This paragraph
is coded as “Non-Story” because there is no explicit
resolution. There are only hypothetical resolutions.

In this paper, we utilize a corpus of 16, 930 paragraphs where 3, 301 paragraphs coded as stories, and
13, 629 paragraphs coded as non-stories by domain
experts to develop a story classifier. Training data
is a collection of Islamist extremist texts, speeches,
video transcripts, forum posts, etc., collected in open
source. We investigate the utility of standard keyword
based features, statistical features that can be extracted
using shallow-parsing (such as density of POS tags and
density of named entities), and a new set of semantic
features in development of a story classifier. Our study
is motivated by the observation [3] that interrelated
stories that work together as a system are fundamental
building blocks of (meta-) narrative analysis.
We focus on discriminating between stories, and
non-stories. The main contribution of this paper is the
introduction of a new set of semantic features based on
related linguistic subject, verb, object categories that
we named as triplet based verb features which are
motivated by the definition of “story” as “actors taking
actions that culminate in resolutions.”. Our proposed
semantic features are based on suitable aggregation
and generalization of <Subject, Verb, Object> triplets
that can be extracted using a shallow-parser. Experimental results (see Table 4) show that a combination
of statistical part-of-speech (POS) and named-entity
(NE) features, with semantic triplet-based features
achieves highest accuracy with a Support Vector Machine (SVM) based classifier. We obtained precision of
73%, recall of 56% and F-measure of 0.63 for minority
class (i.e. stories) which indicates a 161% boost in
recall, and an overall 90% boost in F-measure with
negligible reduction in precision through the utility
of triplet based features over standard keyword based
features.

2. Related Work
Computational models of stories have been studied
for many different purposes. R.E. Hoffman et al.
(2011)[4] modeled stories using an artificial neural network. After the learning stage, they compare the storyrecall performance of the neural network with that of
schizophrenic patients as well as normal controls in
order to derive a computational model which matches
the illness mechanism. The most common form of
classification applied for stories tackles the problem
of mapping a set of stories to predefined categories.
One of the popular applications is the classification of
news stories to their topics [5], [6].
Gordon investigated the problem of detecting stories
in conversational speech [7] and weblogs [8] and [9].
In [7], the authors train a Naive Bayes classifier to

categorize the transcribed text of a speech into story
and non-story categories. Using word-level unigram
and bigram frequency counts as feature vectors, they
reported results for the classification of a speech as
a story with 53.0% precision, 62.9% recall and 0.575
F-measure. For weblogs, in [8], they incorporated techniques for automatically detecting sentence boundaries
to their previously used text features to train a Support
Vector Machine classifier. After smoothing the confidence values with a Gaussian function, they achieved
46.4% precision, 60.6% recall and 0.509 F-measure.
In Gordon and Swanson’s most recent work on story
classification [9], they used a confidence-weighted
linear classifier with a variety of lexical features, and
obtained the best performance with unigrams. They
applied this classifier to classify weblog posts in the
ICWSM 2009 Spinn3r Dataset, and they obtained 66%
precision, 48% recall, and F-measure of 0.55.

3. System Architecture

Figure 1. System Architecture

3.1. Data Collection
Our corpus is comprised of 16, 930 paragraphs from
extremist texts collected in open source. Stories were
drawn from a database of Islamist extremist texts. Texts
were selected by subject matter experts who consulted
open source materials, including opensource.gov, private collection/dissemination groups, and known Islamist extremist web sites and forums. Texts come
from groups including al-Qaeda, its affiliates, and
groups known to sympathize with its cause and methods. The subject matter experts selected texts which
they believe contained or were likely to contain stories,
defined as a sequence of related events, leading to a
resolution or projected resolution.
Extremists’ texts are rarely, if ever, composed of
100% stories, and indeed the purpose of this project
is to enable the detection of portions of texts that are
stories. Accordingly, we developed a coding system
consisting of eight mutually-exclusive and exhaustive
categories: story, exposition, imperative, question, supplication, verse, annotation, and other along with

definitions and examples on which coders could be
trained. After training, coders achieved reliability of
Cohen’s Kappa = 0.824 (average across eleven randomly sampled texts). Once reliability of the coders
and process was established, single coders coded the
remainder of the texts, with spot-check double coding
to ensure reliability was maintained.
The Cohen’s Kappa measure represents how two
observers agrees on sorting items into different categories. The observers can be human or machine. The
range of Cohen’s Kappa varies between 0 and 1. Fleiss
[10] characterizes Kappa range over 0.75 as excellent,
range between 0.40 to 0.75 as fair to good, and less
than 0.40 as poor. Hence coders’ reliability of 0.824
falls into the range of excellent. After training and
testing with ten-fold cross-validation, we calculated the
agreement between classifier algorithm and the human
coders as Cohen’s Kappa = 0.48, which falls into the
range of fair to good.

3.2. Data Preprocessing
3.2.1. Named Entity Recognition Tagger. Named
entity recognition (NER) [11] (also known as entity
identification or entity extraction) is a subtask of
information extraction that seeks to locate and classify
atomic elements in text into predefined categories such
as persons, organizations, locations. Research indicates
that even state-of-the-art NER systems are brittle,
meaning that NER systems developed for one domain
do not typically perform well on other domains [12].
For the purpose of annotating the entities found within
the texts belonging to extremist narratives, we used
the most popular publicly available NER libraries.
We evaluated three libraries: Stanford Named Entity
Recognizer [13], Illinois Named Entity Tagger [14] and
an online web service provided by OpenCalais [15].
A set of six documents belonging to extremist
narratives were manually annotated by a specialist as
a person, a location or an organization. Next, the same
set of documents was annotated using NER libraries in
order to determine their accuracy. F-measure was used
to measure their accuracy as shown in Table 1.
We also developed a consensus analysis based algorithm, which we named as ‘democratic NER tagger’,
using the output of all three taggers as follows:
1) For a particular text document to be annotated
for named-entity tags, invoke each NER tagger
(Stanford, Illinois and Open Calais).
2) For an annotated entity/word determine the category (Person, Organization, Location) assigned
by each NER tagger.

Table 1. NER Tagger F-measures
Text-ID
1
2
3
4
5
6
Average

DNERT
0.592
0.567
0.652
0.837
0.720
0.505
0.644

Stanford
0.355
0.587
0.627
0.867
0.686
0.446
0.594

Illinois
0.463
0.549
0.574
0.867
0.483
0.651
0.597

Open Calasis
0.312
0.164
0.247
0.591
0.459
0.416
0.364

3) If the phrase is classified by only one tagger,
then assign it as the final tag.
4) If all taggers disagree on the category of a
phrase, then we pick the final category according
to the accuracy of the taggers as follows:
• Illinois NER has the highest accuracy for
Locations and Organizations;
• Stanford NER has the highest accuracy for
Persons.
5) If two out of three NER taggers agree on the
predicted category of a phrase, then the final
category is determined by majority agreement.
Within the six documents, specialist annotated 308
organization names, 259 location names, and 127
person names. Table 1 summarizes the accuracies of
the software libraries, as well as the accuracy of our
simple democratic NER tagger (DNERT) which relias
on all of them. Overall, our democratic NERT achieves
the highest performance compared to individual NER
taggers.
3.2.2. Named Entity Standardization. The extremist texts under consideration are collected from various social media sources (i.e. blogs, organizations’
websites and RSS feeds). Since these are all human
generated documents, they are rife with misspellings
and aliases of named entities. For example, the person
entity ‘Osama Bin Laden’ is sometimes referred to
as ‘Bin Laden’, ‘Sheikh Osama’, or it is sometimes
misspelled as ‘Osamma Bin Laden’ or ‘Usama’. In
order to standardize the usage of the entities and
improve the accuracy of classifier dependent on the
entity features we have came up with a two step named
entity standardization process:
• Misspelling Correction Step
The named entity spelling is corrected using a
lookup with the Google’s ‘Did u mean?’ feature.
This process enables us to identify the most
correct spelling of a named entity. E.g. An entity named ‘Osamma bin laden’ is corrected as
‘Osama bin laden’.
• Standardization for Aliases
We query the RDF data stores of DBpedia[16]

Table 2. Named-Entity Correction and
Standardization Results
# of Occurences
Total
Distinct
Standardized
Accuracy

Person
5015
332
72
65 (90.3%)

Organization
2456
200
26
24 (92.3%)

Location
4279
290
30
28 (93.3%)

in order to find a standardized name for all
location, organization and person entities. DBpedia data set has a public SPARQL endpoint available at: http://DBpedia.org/sparql. The
DBpedia OWL ontology for named entity
types have following properties where known
aliases are stored: dbpprop:alternativeNames,
dbpprop:name, foaf:name. By querying the alternative names we are able to obtain the standard
name for each entity.
Table 2 summarizes the results for the named entity
correction and standardization algorithm. First row
shows the total number of occurrences of each type
of named entity in our document corpus. Second row
shows the counts of distinct entities by their type.
Third row shows the number of distinct named entity
corrections made through above spelling correction
and alias standardization procedure. The changes were
manually evaluated by a human annotator to verify
their accuracy. As seen in the final row of the table,
out of 72 person name changes, 65 were accurately
standardized by the algorithm, providing 90% accuracy
for person entities. Similarly, for organization entities
the correction and standardization accuracy is around
92%, and for location entities it is around 93%.

3.3. Human Annotation: Story vs. Non-Story
Coding
Stories are differentiated from non-stories as follows: Because they describe actions, stories will have
a lower proportion of stative verbs than non-stories.
Stories will include more named entities, especially
person names, than non-stories. Stories will use more
personal pronouns than non-stories. Stories may include more past tense verbs (i.e., X resulted in Y, X
succeeded in doing Y, etc.) than non-stories. Stories
may repeat similar nouns. For example, “mujahedeen”
may be mentioned in the beginning of the story and
then again at the end of the story. Paragraphs with
stories in them have different sentence lengths than
paragraphs without stories in them.

3.4. Feature Extraction
Above observations made in Section 3.3 motivates
following standard keyword based features, statistical
features based on shallow-parsing, and a new set
of semantic features for the development of a story
classifier:
• Keywords: TF/IDF measure [17] is calculated
for each word contained in the whole paragraph
set. Then a certain number of terms, in our case
20, 000, with the top TF/IDF values are selected
as features. Then term-document frequency matrix is created out these keyword features.
• Density of POS Tags: Part of Speech (POS) Tag
Ratios [18] for each document is calculated with
respect to numbers of tokens.
• Density of Named Entities: Named Entity (NE)
Tag frequency [13] per document is calculated.
The tags are Person, Location and Organization.
• Density of Stative Verbs: Some other statistical
features are also included in all experiments,
such as the number of valid tokens and the ratio
between observed stative verbs and total number
of verbs in a paragraph.
• Semantic Triplets Extraction: We present our
semantic triplet extraction methods in Section 3.
We also discuss how triplets from stories and nonstories are aggregated and generalized to form
memory-based features for verbs.

3.5. Support Vector Machine (SVM) Classifier
SVM [19] is a supervised learning technique which
makes use of a hyperplane to separate the data into
two categories. SVM is originally proposed as a linear
classifier [20] but later improved by the use of kernel
functions to detect nonlinear patterns underlying the
data [21].There are various types of kernel functions
available [22]. In this study, we use RBF kernel defined
as K(xi , xj ) = ekxi −xj k , where xi,j are data points
[23].

3.6. Training and Testing
The corpus contains 1,256 documents containing
both story and non-story paragraphs. There are a
total of 16,930 paragraphs, where 13,629 paragraphs
classified reliably as non-stories, and 3,301 paragraphs
classified as stories by domain experts. In our evaluations, we performed 10 fold cross validation with the
document files as follows: we break documents into 10
sets of size n/10, where n is total number of documents

resolution module [26], [27] uses a heuristic approach
to identify the noun phrases referred by the pronouns
in a sentence. The heuristic is based on the number of
the pronoun (singular or plural) and the proximity of
the noun phrase. The closest earlier mentioned noun
phrase that matches the number of the pronoun is
considered as the referred phrase.

4.2. Semantic Role Labeler (SRL) Parser

Figure 2. Triplet Extraction Pipeline

(1,256). During the training phase, both story and nonstory paragraphs from 9/10 documents are used as the
training set, their features are extracted, and a classifier
is trained. During the testing phase, the remaining
1/10th of the documents are used; the features for
both stories and non-stories are extracted, and matched
to the features extracted during the training phase.
Doing this evaluation, we are ensuring that training
and test data features are in fact coming from different
documents. We calculate precision, recall for each
iteration of the 10 fold cross validation and we report
mean precision, recall for both both stories and nonstories.

4. Semantic Triplet Extraction
We follow a standard verb-based approach to extract
the simple clauses within a sentence. A sentence is
identified to be complex if it contains more than one
verb. A simple sentence is identified to be one with
a subject, a verb, with objects and their modifying
phrases. A complex sentence involves many verbs. We
define a triplet in a sentence as a relationship between
a verb, its subject and object(s). Extraction of triplets
[24], [25] is the process of finding who (subject),
is doing what (verb) with/to whom (direct objects),
when and where (indirect objects/and prepositions).
Our triplet extraction utilizes the information extraction
pipeline shown in Figure (2).

4.1. Pronoun Resolution
Interactions are often specified through pronominal
references to entities in the discourse, or through co
references where, a number of phrases are used to
refer to the same entity. Hence, a complete approach to
extracting information from text should also take into
account the resolution of these references. Our pronoun

SRL parser [28] is the key component of our triplet
extractor. To extract the subject-predicate-object from
an input sentence, important step is identifying these
elements in a sentence and parse it. SRL parser does
exactly the same. SRL is propriety software developed
by Illinois research group and its shallow semantic
parser. The goal of the semantic role labeling task is
to discover the predicate-argument structure of each
predicate that fill a semantic role and to determine their
role (Agent, Patient, Instrument etc). As shown in the
following example, SRL is robust in identifying verbs,
and their arguments and argument types accurately in
the presence of syntactic variations.
Numbered arguments (A0-A5, AA): Arguments define verb-specific roles. They depend on the verb in a
sentence. The most frequent roles are A0 and A1 and,
commonly, A0 stands for the agent and A1 corresponds
to the patient or theme of the proposition.
Adjuncts (AM-): General arguments that any verb
may take optionally. There are 13 types of adjuncts:
AM-ADV - general-purpose, AM-MOD - modal verb,
AM-CAU - cause, AM-NEG - negation marker, AMDIR - direction, AM-PNC - purpose, AM-DIS - discourse marker, AM-PRD - predication, AM-EXT extent, AM-REC - reciprocal, AM-LOC - location,
AM-TMP - temporal, AM-MNR - manner.
References (R-): Arguments representing arguments
realized in other parts of the sentence. The label is an
R- tag prefixed to the label of the referent, e.g. [A1
The pearls] [R-A1 which] [A0 I] [V left] [A2 to my
daughter-in-law] are fake.
SRL System Architecture: SRL works in fourstages, starting with pruning of irrelevant arguments,
identifying relevant arguments, classifying arguments
and inference of global meaning.
Pruning - Used to filter out simple constituents that
are very unlikely to be arguments.
Argument Identification - Utilizes binary classification to identify whether a candidate is an argument or
not. The classifiers are applied on the output from the
pruning stage. A simple heuristic is employed to filter
out some candidates that are obviously not arguments.
Argument Classification - This stage assigns labels

to the argument candidates identified in the previous
stage.
Inference - In the previous stages, decisions were
always made for each argument independently, ignoring the global information across arguments in the
final output. The purpose of the inference stage is to
incorporate such information, including both linguistic
and structural knowledge. This knowledge is useful to
resolve any inconsistencies of argument classification
in order to generate final legitimate predictions.

4.3. Triplet Extraction
Our triplet extraction algorithm processes SRL output. The SRL output has a specific multi-column
format. Each column represents one verb (predicate)
and its arguments (A0, A1, R-A1, A2, etc) potentially
forming many triplets. For a simple sentence, we can
read one column and extract a triplet. For complex
sentences with many verbs, we developed a bottom-up
extraction algorithm for detecting and tagging nested
events. We will illustrate our approach using the following example.
“America
commissioned
Example Paragraph:
Musharraf with the task of taking revenge on the
border tribes, especially the valiant and lofty Pashtun
tribes, in order to contain this popular support for
jihad against its crusader campaign. So he began
demolishing homes, making arrests, and killing
innocent people. Musharraf, however, pretends to
forget that these tribes, which have defended Islam
throughout its history, will not bow to US”
Our algorithm produces the following triplets for the
example paragraph below:
Table 3. Extracted Triplets
Event

Subject

Verb

Object

E1
E2
E2

America
America
Musharraf
Musharraf
Musharraf
Musharraf
Musharraf
tribes
tribes

commission
take
demolish
make
kill
pretend
forget
defend
not bow

Musharraf
revenge
homes
arrests
innocent people
E1
E2
Islam
to US

4.3.1. Bottom-Up Event Tagging Approach. In the
example above, consider the triplet <Musharraf, pretend, E1>. Here the object column of the verb
pretend has an A1 argument including three other
verbs (forget, defend and bow). That is, argument A1
is itself complex, comprising other triplets. So we tag

argument A1 with a nested event (E1), and recursively
process A1 with our triplet extraction rules. We achieve
this nested processing through a bottom-up algorithm
that (i) detects simple verb occurrences (i.e. verbs
with non-verb arguments) in the SRL parse tree, (ii)
extracts triplets for those simple verb occurrences using
the following Triplet Matching Rules, (iii) replaces
simple verb clauses with an event identifier, thus
turning all complex verb occurrences into simple verb
occurrences with either non-verb or event arguments,
and applies the following Triplet Matching Rules.
4.3.2. Triplet Matching Rules. We list four matching
rules below to turn simple SRL columns into triplets:
• A0, V, A1: <SUBJECT, VERB, DIRECT
OBJECT>
• A0,
V,
A2:
<SUBJECT,
VERB,
PREPOSITION>, if direct object A1 not
present in column.
• A0, V, A1, A2-AM-LOC: <SUBJECT, VERB,
DIRECT OBJECT, location (PREPOSITION)>
• A1,
V, A2: <DIRECT OBJ, VERB,
PREPOSITION>
4.3.3. Triplet Extraction Accuracy. The triplet extraction accuracy is based on SRL accuracy. SRL has
precision of 82.28%, recall of 76.78% and f-measure
79.44% [28].
4.3.4. Triplet Based Feature Extraction. For each
verb (V) mentioned in a story (S), or non-story (NS)
we stemmed and aggregated its arguments corresponding to its SUBJECTs, OBJECTs and PREPOSITIONs
to generate following set-valued “semantic verb features” by using the training data:
• Argument list for S.V.Subjects, S.V.Objects,
S.V.Prepositions for each verb V and story S.
• Argument list for NS.V.Subjects, NS.V.Objects,
NS.V.Prepositions for each verb V and Non-Story
NS.
For each test paragraph P, for each verb V in
P, we extract its typed argument lists P.V.Subjects,
P.V.Objects and P.V.Prepositions. Then, we match them
to the argument lists of the same verb V. A match
succeeds if the overlap between a feature’s argument
list (e.g. S.V.Subjects, or NS.V.Subjects) covers the
majority of the test paragraph’s corresponding verb
argument list (e.g. P.V.Subjects).

5. Generalized Verb Features
Generalization and reduction of features is an important step in classification process. Reduced feature representations not only reduce computing time but they

may also yield to better discriminatory behavior. Owing to the generic nature of the curse of dimensionality
it has to be assumed that feature reduction techniques
are likely to improve classification algorithm.
Our training data had 750 and 1, 754 distinct verbs
in stories and non-stories, yielding 750 ∗ 3 = 2, 250
and, 1, 754 ∗ 3 = 5, 262 verb features for stories and
non-stories respectively, and total of 7, 512 features.
VerbNet (VN) [29] is the largest on-line verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon.
VerbNet index has 5, 879 total verbs represented, and
these verbs are mapped into 270 total VerbNet main
classes. For example, the verbs mingle, meld, blend,
combine, decoct, add, connect all share the same
meaning (i.e. to bring together or combine), and hence
they map to verb class “mix” numbered 22.1. With the
help of VerbNet and SRL argument types of the verbs,
we mapped all occurrences of our verbs in stories and
non-stories to one of these 270 VerbNet main classes.
This mapping enabled us to reduce our verb features to
268 ∗ 6 = 1, 608 verb features. The number 6 is used
in the preceding equation since each verb class can
lead to at most 6 features as V.Subject, V.Object and
V.Preposition for its story and non-story occurrences.
We started with 7, 512 verb features, and after mapping
these verb features to their verb category features we
ended up with 1, 608 features only.
In the generalization process, we faced a problem
of verb sense disambiguation. There are some verbs
which can be mapped to different senses, and each
sense belongs to a different verb class. For example,
the verb “add” can be used with the sense ‘mix’
(22.1) or ‘categorize’ (29.2) or ‘say’ (25.3). To solve
this problem, we used argument types extracted using
SRL for the ambiguous verbs. Then, we performed
a look-up for each verb in the PropBank database
to identify the matching verb sense with same type
of arguments, and its verb class. PropBank [30] is a
corpus that is annotated with verbal propositions, and
their arguments - a “proposition bank”. In the look-up
process, there is a chance that we may encounter more
than one verb sense for the input verb matching the
corresponding argument types. In this case, we picked
the first matching verb sense listed in PropBank.

6. Experimental Evaluations
In this section, we evaluate the the utility of standard
keyword based features, statistical features based on
shallow-parsing (such as density of POS tags and
named entities), and a new set of semantic features
to develop a story classifier. Feature extraction and

matching is implemented using JAVA and classification
is performed using LIBSVM [22] in MATLAB.
Table 4. Classifier Performance for Stories
Feature Set
POS
Keyword
Keyword + POS + NE
Triplet
Triplet + POS + NE

Precision
0.133
0.821
0.750
0.798
0.731

Recall
0.066
0.205
0.214
0.515
0.559

F-measure
0.088
0.329
0.333
0.626
0.634

Table 5. Classifier Performance for Non-Stories
Feature Set
POS
Keyword
Keyword + POS + NE
Triplet
Triplet + POS + NE

Precision
0.887
0.903
0.904
0.886
0.905

Recall
0.944
0.994
0.991
0.998
0.939

F-measure
0.914
0.946
0.945
0.938
0.923

6.1. Effectiveness of Semantic Features
The baseline performance for a dummy classifier
which would assign all instances to the majority class
(non-story) would achieve 80.5% precision and 100%
recall for the non-story category however, its precision
and recall would be null for the stories. Hence, not
useful at all for detecting stories.
Our proposed model makes use of triplets to incorporate both semantic and structural information available in stories and non-stories. In Table (4), we report
the performance of SVM classification with various
feature sets. SVM with POS, NE and generalized
triplet based features outperforms other combinations
of standard categories of features in terms of recall
and F-measure. Table (4) shows 151.2% boost in
recall and 90% boost in F-measure for keyword based
(second row) vs triplet based (fourth row) features.
After adding POS and NE features (Keyword + POS
+ NE based, third row) vs (Triplets + POS + NE, fifth
row), we obtained 161.2% boost in recall and 90%
boost in F-measure.

7. Conclusion
This paper proposes a model with semantic triplet
based features for story classification. The effectiveness of the model is demonstrated against other traditional features used in the literature for text classification tasks. Future work includes more detailed
evaluations, and also experiments with appropriate
generalizations of nouns, adjectives and other types of
keywords found in verb arguments.

Acknowledgment
This research was supported by an Office of Naval
Research grant (N00014-09-1-0872) to the Center for
Strategic Communication at Arizona State University.

References
[1] J. Bruner and S. Weisser, “Autobiography and the
construction of self,” 1992.
[2] C. Joseph, The hero with a thousand faces. Princeton
University Press, 1949.
[3] H. L. Halverson, J. R. Goodall and S. R. Corman,
Master Narratives of Islamist Extremism. New York:
Palgrave Macmillan, 2011.
[4] R. Hoffman, U. Grasemann, R. Gueorguieva, D. Quinlan, D. Lane, and R. Miikkulainen, “Using computational patients to evaluate illness mechanisms in
schizophrenia,” Biological psychiatry, 2011.
[5] B. Masand, G. Linoff, and D. Waltz, “Classifying news
stories using memory based reasoning,” in Proceedings
of the 15th annual international ACM SIGIR conference
on Research and development in information retrieval.
ACM, 1992, pp. 59–65.
[6] D. Billsus and M. Pazzani, “A hybrid user model
for news story classification,” Lectures-International
Centre for Mechanical Sciences, pp. 99–108, 1999.
[7] A. S. Gordon and K. Ganesan, “Automated story
capture from conversational speech,” in K-CAP ’05:
Proceedings of the 3rd international conference on
Knowledge capture, ACM.
Banff, Canada: ACM,
2005, p. 145–152.
[8] A. Gordon, Q. Cao, and R. Swanson, “Automated story
capture from internet weblogs,” in Proceedings of the
4th international conference on Knowledge capture.
ACM, 2007, pp. 167–168.
[9] A. Gordon and R. Swanson, “Identifying personal stories in millions of weblog entries,” in Third International Conference on Weblogs and Social Media, Data
Challenge Workshop, San Jose, CA, 2009.
[10] J. Fleiss, “Measuring nominal scale agreement among
many raters.” Psychological bulletin, vol. 76, no. 5, p.
378, 1971.
[11] E. F. Tjong Kim Sang and F. De Meulder, “Introduction
to the conll-2003 shared task: Language-independent
named entity recognition,” in Proceedings of CoNLL2003, W. Daelemans and M. Osborne, Eds. Edmonton,
Canada, 2003, pp. 142–147.
[12] T. Poibeau and L. Kosseim, “Proper name extraction
from non-journalistic texts,” Language and Computers,
vol. 37, no. 1, pp. 144–157, 2001.
[13] J. Finkel, T. Grenager, and C. Manning, “Incorporating
non-local information into information extraction systems by gibbs sampling,” in Proceedings of the 43rd
Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,
2005, pp. 363–370.
[14] L. Ratinov and D. Roth, “Design challenges and misconceptions in named entity recognition,” in CoNLL
’09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning. Morristown,

[15]
[16]

[17]
[18]

[19]

[20]

[21]
[22]

[23]
[24]

[25]
[26]

[27]

[28]

[29]

[30]

NJ, USA: Association for Computational Linguistics,
2009, pp. 147–155.
“Calais.” [Online]. Available: http://www.opencalais.
com/
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, and
Z. Ives, “Dbpedia: A nucleus for a web of open data,”
in In 6th Intl Semantic Web Conference, Busan, Korea.
Springer, 2007, pp. 11–15.
S. Robertson, “Understanding inverse document frequency: on theoretical arguments for idf,” Journal of
Documentation, vol. 60, no. 5, pp. 503–520, 2004.
E. Brill, “A simple rule-based part of speech tagger,”
in Proceedings of the workshop on Speech and Natural
Language. Association for Computational Linguistics,
1992, pp. 112–116.
T. Joachims, “A statistical learning learning model
of text classification for support vector machines,” in
Proceedings of the 24th annual international ACM
SIGIR conference on Research and development in
information retrieval. ACM, 2001, pp. 128–136.
B. Boser, I. Guyon, and V. Vapnik, “A training algorithm for optimal margin classifiers,” in Proceedings of
the fifth annual workshop on Computational learning
theory. ACM, 1992, pp. 144–152.
C. Cortes and V. Vapnik, “Support-vector networks,”
Machine learning, vol. 20, no. 3, pp. 273–297, 1995.
C. Chang and C. Lin, “Libsvm: a library for support
vector machines,” ACM Transactions on Intelligent
Systems and Technology (TIST), vol. 2, no. 3, p. 27,
2011.
S. Keerthi and C. Lin, “Asymptotic behaviors of support
vector machines with gaussian kernel,” Neural computation, vol. 15, no. 7, pp. 1667–1689, 2003.
D. Rusu, L. Dali, B. Fortuna, M. Grobelnik, and
D. Mladenić, “Triplet extraction from sentences,” Proceedings of the 10th International Multiconference Information Society-IS, pp. 8–12, 2007.
D. Hooge Jr, “Extraction and indexing of triplet-based
knowledge using natural language processing,” Ph.D.
dissertation, University of Georgia, 2007.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and D. Jurafsky, “Stanfords multi-pass sieve
coreference resolution system at the conll-2011 shared
task,” CoNLL 2011, p. 28, 2011.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning, “A multipass sieve for coreference resolution,” in Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing. Association for Computational
Linguistics, 2010, pp. 492–501.
V. Punyakanok, D. Roth, and W. Yih, “The importance
of syntactic parsing and inference in semantic role
labeling,” Computational Linguistics, vol. 34, no. 2, pp.
257–287, 2008.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer, “A
large-scale classification of english verbs,” Language
Resources and Evaluation, vol. 42, no. 1, pp. 21–40,
2008.
M. Palmer, D. Gildea, and P. Kingsbury, “The proposition bank: An annotated corpus of semantic roles,”
Computational Linguistics, vol. 31, no. 1, pp. 71–106,
2005.

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/269200255

LookingGlass
Conference	Paper	·	January	2013
DOI:	10.1145/2492517.2500275

CITATIONS

READS

0

53

4	authors:
Nyunsu	Kim

Sedat	Gokalp

Arizona	State	University

Arizona	State	University

10	PUBLICATIONS			17	CITATIONS			

11	PUBLICATIONS			21	CITATIONS			

SEE	PROFILE

SEE	PROFILE

Hasan	Davulcu

Mark	Woodward

Arizona	State	University

Arizona	State	University

110	PUBLICATIONS			843	CITATIONS			

63	PUBLICATIONS			568	CITATIONS			

SEE	PROFILE

SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Nyunsu	Kim	on	15	January	2015.
The	user	has	requested	enhancement	of	the	downloaded	file.	All	in-text	references	underlined	in	blue	are	added	to	the	original	document
and	are	linked	to	publications	on	ResearchGate,	letting	you	access	and	read	them	immediately.

LookingGlass: A Visual Intelligence Platform for
Tracking Online Social Movements
Nyunsu Kim, Sedat Gokalp, Hasan Davulcu, Mark Woodward
School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University
Tempe, USA
{nkim30, Sedat.Gokalp, hdavulcu, Mark.Woodward}@asu.edu

Abstract— We propose a multi-scale text mining methodology
and develop a visual intelligence platform for tracking the
diffusion of online social movements. The algorithms utilize large
amounts of text collected from a wide variety of organizations’
media outlets to discover their hotly debated topics, and their
discriminative perspectives voiced by opposing camps organized
into multiple scales. We utilize discriminating perspectives to
classify and map individual Tweeter's message content to social
movements based on the perspectives expressed in their weekly
tweets. We developed a visual intelligence platform, named
LookingGlass, to track the geographical footprint, shifting
positions and flows of individuals, topics and perspectives
between groups.
Keywords—Text mining; Multi-scaling; Social movements

I.

INTRODUCTION

We propose a multi-scaling based methodology and a
visual intelligence platform that represents an important step
change in how we might observe and analyze radical social
movements. Rather than placing external forms of analysis that
color and tautological define what is ‘radical’ or not from an
external perspective, we propose a more ontologically oriented
approach. We seek to generate a methodology to allow the
orientations of these movements to define themselves via their
own discourse within their own universe and understanding of
actions, rather than an external and potentially poorly
calibrated analysis of what constitutes radical. Without this
kind of fundamental reorientation to research of religiously or
politically inspired social movements, we get the poor
assumption based analysis that (incorrectly) predicts and
champions ill-defined relationships between certain religious
or political sects and violence, for example. With our
reorientation of approach, we are more fundamentally able to
examine such relationships in a way that should allow analysts
to take other kinds of nuance and understanding into account.
Current technology for monitoring social media tracks
keyword matching documents for names of known groups,
individuals, and places. However, they cannot find the
proverbial “needles in a haystack” corresponding to those
individuals with radical or extremist ideas, connect the dots to
identify their relationships, and their socio-cultural, political,
economic drivers. Raw data in multiple modalities (e.g. tweets,
blogs, and newswires) gushes like an uncapped oil well but

existing technologies fail to provide comprehensive tools for
making-sense of the data and for seeing the bigger picture.
LookingGlass is designed for real-time contextual analysis of
complex socio-political situations that are rife with volatility
and uncertainty. It is able to rapidly recognize radical hot-spots
of networks, narratives and activities, and their socio-cultural
economic, political drivers. Also, it is informed by highly
trained area experts on the ground with social science and
subject matter expertise as well as local cultural knowledge.
II.

PROBLEM DEFINITION

One of the fundamental issues with interpretative and
qualitative data collection and analysis has been the
researchers’ bias while conducting the research. Goertz [1, 2]
makes the crucial point that, in their enthusiasm for reifying
complex sociological or political concepts, theorists and
empiricists often focus too much on what a concept is, rather
than on identifying the concept on a continuum, in order to
assess when a concept is present versus when it is absent. In
the social sciences, scaling is the process of measuring and
ordering actors (subjects) with respect to quantitative attributes
or traits (items). In the context of our project, both social
movements (subjects) and their socio-economic, political, or
religious beliefs, goals and practices (items) are mapped
simultaneously on a set of scales via expert inputs and text
mining algorithms. Recently we developed algorithms [3, 4, 5,
6] that utilize large amounts of multilingual text collected from
a wide variety of organizations’ media outlets (e.g. web sites,
blogs, news, RSS feeds, tweets, leaders’ speeches etc.) to (i)
discover hotly debated topics relevant for a scale, and
discriminative topic-specific perspectives voiced by opposing
camps, (ii) next, we identify a subset of these discriminating
perspectives with a very specific statistical pattern
(probabilistic Guttman pattern [7]) that can be used to classify
and rank any actor’s polarity and neutral-to-extreme position
on a continuous scale at area expert-level accuracy, and finally,
(iii) we utilize spatial-temporal analysis of real-time textual
message feeds, from organizational RSS feeds and individuals’
Tweets to track the geographical footprint, hot spots, shifting
positions and flows of individuals between groups.

III.

MULTI-SCALE MODELING OF SOCIAL MOVEMENTS

Our modeling leverages social theory including
Durkheim’s research on collective representations [8],
Simmel’s work on conflict and social differentiation [9],
Wallace’s writings on revitalization movements [10], and Tilly
and Bayat’s studies on contemporary social movement theory
[11, 12] to understand features shared by violent social and
political movements and by those opposing them. Radicalism
is the ideological conviction that it is acceptable and sometimes
obligatory to use violence to effect profound political, cultural
and religious transformations and to change the existing social
order fundamentally. Radical movements have complex origins
and depend on diverse factors that enable the translation of
their radical ideology into social, political and religious
movements [13, 14]. Crelinsten [15] states, “both violence and
terrorism possess a logic and grammar that must be understood
if we are to prevent or control them.” Binary labeling as
counter-radical vs. radical does not capture the overlaps,
movement and interactivity among these actors. We observe
[6] that both counter-radical and radical movements in Muslim
societies exhibit distinct combinations of discrete states
comprising various social, political, and religious beliefs,
goals, attitudes and practices, and their discriminating
perspectives can be semi-automatically identified and mapped
onto latent linear continuums or scales.
The model described below defines a seven dimensional
possibility space within which diverse organizations, social
movements, and individuals can be located. The variables are
treated as continuous bipolar scales. Each scale is measured
independently of the others. Social movements exhibit distinct
combinations of discrete social, political, and religious beliefs,
attitudes, and practices that can be mapped onto these latent
linear scales. Scaling is a method for measuring and ordering
entities based on their qualitative attributes.
The choice of scales relies on the work of a combination of
American, European, African, and Southeast Asian scholars
and the literature on similar movements in various regions. The
variables are generalizations based on ethnographic research
that involved observation of public events, extended interviews
and informal conversations with leaders and rank and file
members of organizations and movements, and discourse
analysis. Field work findings and conventional discourse
analysis provided input for automated discourse analysis
conducted using methods relying on key words associated with
each of the variables (described elsewhere). These automated
methods analyzed more than fifty thousand local language
documents obtained with the use of web-mining technology.
The scales used in LookingGlass tools for characterizing
Radical-Islamist and Counter-Radical Islamist movements are:
 Epistemology: This refers to the ways in which religious
groups interpret core texts. Foundationalism is at one end
of a continuum. It fixes meaning in invariant, “literal”
readings of core religious texts. Foundationalists claim that
their readings are ahistorical and not influenced by cultural
considerations. Constructivism is at the other end of the
scale. It acknowledges that all variants of a religious
tradition are constructed in historical, social, and cultural
contexts and they can, and indeed must, change over time.

Proponents of this position maintain that to determine the
meaning of a scriptural passage appropriate for a particular
time, place, and culture, both the context of revelation and
the context of exegesis must be considered.
 Religious Diversity Tolerance: Exclusivists, who insist on
universal adherence to their own beliefs and social norms
and who claim exclusive possession of complete truth, are
at one end. Pluralists, who understand difference as a
social and religious good or theological pluralism, are at
the other. An entity at the extreme pluralist end of the
tolerance scale holds the view that all religions should be
tolerated and that all are based on truths that transcend
confessional and sectarian differences.
 Change Orientation: Change orientation aims to capture
the degree to which an entity wishes to effect social,
political, and/or religious change. It is also a measure of
the degree to which an individual or group attempts to
influence others. Revitalization movements that seek to
destroy the world as it is and rebuild it from scratch are at
one end of the scale. Defenders of the social, political, and
religious status quo are at the other.
 Violence: Violence is defined broadly to include more than
killing, inflicting physical injury, and destruction of
property. Symbolic and discursive violence are included in
this scale because they are often steps leading toward
physical violence. They can cause havoc, especially when
the manipulation of symbols and discourse is purposively
articulated to provoke adversaries, demonize opponents,
incite mobs to action, or to provide justifications for the
“necessity of violence.” Unlike physical violence that can
be seen and clearly understood for what it is, symbolic and
discursive violence are not necessarily self-evident; hence
both require knowledge of their contexts to identify them
and assess their real and potential danger.
Dehumanization, demonization, and the desecration of
sacred places and objects are among the most common and
provocative forms of symbolic violence committed in
contexts of ethnic and religious conflict.
 Violence Ideology scale represents the degree to which an
entity supports or rejects violence as a matter of principle.
Though some of the movements scaled rely on reasoned
argumentation appealing to concepts of justice and
oppression in addition to, or in place of narratives. At one
end are those who would support any type of violence; at
the other are pacifists who are ideologically committed to
nonviolence. A lack of violent rhetoric is insufficient to
classify an organization as pacifist if the organization is
silent in the face of others’ violence violent acts and
violent rhetoric.
 Violence Engagement scale measures the degree to which
an entity engages in any type of violence including
symbolic or discursive violence. At one end of the
continuum are those who have explicitly claimed
responsibility for violent acts. At the other end are those
who have never engaged in any type of violence.

Figure 1. Graphical Scaling Tool

IV.

LOOKINGGLASS TOOLS AND METHODS

A. Graphical Scaling Tool
We designed an intuitive, easy-to-use graphical tool for
defining multiple scales, so that area experts may populate
them with polarities and positions of known social movements
from a certain region. A snapshot of such a graphical scaling
tool is shown in Figure 1, which marks the positions and
polarities of 23 groups from Indonesia on the seven scales
described above.
B. Multi-Lingual Topic Detection
We utilized automated on-line topic detection [16] in text
material. We asked area-experts to scan detected topics and
associate them with the scales that they are relevant. For
example, a partial list of relevant topics for a Political Change
scale for Indonesia include: {sharia, family law, corruption,
democracy, election, secularism, state, constitution, justice}.
C. Discriminative Perspective Mining
The more challeging aspect of textual analysis is
discriminating perspective mining in debates between the
opposing camps on a scale. A debate is a formal discussion on
a set of related topics in a forum, in which opposing arguments
are put forward. For example, a debate on education might
comprise opposing perspectives, such as “secular, multicultural education” vs. “religious, sharia based education”. In
this step, our focus is the development of an automated
perspective mining algorithm, which would contribute to the
understanding of features (i.e. social, political, cultural,
religious beliefs, goals, and practices) shared by one side of a
debate, and by those opposing them. We formulate the
perspective mining problem in a general structured sparse
learning framework [17]. In particular, the logistical regression
formulation fits our application, since it is a dichotomous
classification problem:

In the formula below, ai is the vector representation of the ith
document, wi is the weight assigned to the ith document (wi
=1/m by default), and A=[a1, a2, …, am] is the document
keyword matrix, yi is the polarity of each document based upon
the scale polarity of the actor that the document belongs to, and
the unknown xj , the j-th element of x, is the weight for each
keyword, λ>0 is a regularization parameter that controls the
sparsity of the solution, |x|1= Σ|xi| is 1-norm of the x vector. We
use the SLEP sparse learning package that utilizes gradient

descent approach to solve the above convex and non-smooth
optimization problem. The keyword phrases with non-zero
values on the sparse x vector yields the discriminant
perspectives based on their polarity (positive or negative).
Following table displays radical perspectives for five topics.

Figure 2. Radical Perspectives for Topics

D. Response Tables of NSAs
A response table is calculated based on the normalized
frequency with which actors mention various perspectives, as
indicated by keywords. The median frequency of each
perspective is selected as a threshold. Actors and normalized
perspective frequencies are used to build a dichotomous [0/1]
response matrix. A sample partial response table for the
Violence Ideology scale is presented in Figure 3.

Figure 3. Response Table of Radical Actors on Violence Ideology related
perspectives

E. Rasch Modeling
A true Guttman scale is deterministic, i.e. if an actor
subscribes to a certain perspective, then it must also agree
with all lower order perspectives on the scale. Of course,
perfect order is rare in the social world. The Rasch [18] model
provides a probabilistic framework for Guttman scales to
accommodate incomplete observations and measurement
error. We employ the Rasch model to measure the abilities of
actors on a latent scale, alongside the difficulties of
perspectives on the same scale. By definition, the location of
an item (difficulty) on a scale corresponds to the actor location
(ability) at which there is a 0.5 probability of a response. Once
item locations are scaled, then actor locations are measured on
the scale using the EM-method for joint Maximum Likelihood
Estimation [19]. The following figure (Figure 4) is the radical
actor-perspective map displaying the location of
discriminating perspective (items) as well as the distribution
of actors (subjects) along a Violence Ideology scale:

if a message or a collection of messages from a Tweeter maps
to one polarity or another polarity of a scale (e.g. radical or
not-radical), but also if a message or a messenger classifies as
a follower of one of the known social movement’s rhetoric or
ideology. In our 10-fold cross-validation-based experiments
with 37,000 web pages downloaded from web sites of 23
Islamic organizations (10 radical and 13 non-radical) from
Indonesia, we observed that the linear formulation-based
message classifier, with discriminative perspectives as its
features, achieves over 98% accuracy for predicting the
corresponding polarity of documents. Furthermore, we
observed that logistic formulation-based classifier achieves
over 83% accuracy for predicting the corresponding source (a
particular Islamic organization) of a document. Combined
with longitudinal analysis of an individual’s messages (such as
those that can be observed on Twitter, message boards, blogs,
or in chat rooms), we can determine (i) shifts of individuals
from the status of unaffiliated to affiliated of one of the known
social movements (SM), (ii) growth and shrinkage drivers (i.e.
types of events and narratives) of SMs, and (iii) influential
followers of SMs. In the following section we describe the
design of a real-time dashboard, named LookingGlass (see
Figure. 5), to display all types of flows and hot spots of
Tweeters between radical (red) and non-radical (green) SMs,
their popular keywords, hash tags, event mentions, and media
sources driving their weekly shifts.

Figure 4. A Violence Ideology Scale for Radical Actors

F. Micro Level Analysis of Groups and Individuals
The utilization of the logistic regression formulation
presented in Section IV. C provides a classification model [20]
between different polarities of a scale -- by checking the
polarity of (xT.ai) where xT corresponds to the weights of the
discriminant perspectives relevant to a scale, and ai is the
keyword vector of each Tweeter’s weekly message content.
Using the discriminating perspectives, we can not only detect

Figure 5. A Real-Time Dashboard for Visualizing Shifts and their Flows

V.

LOOKINGGLASS DESIGN AND ARCHITECTURE

The following diagram in Figure 6 shows the major
components of the LookingGlass real-time dashboard.

E. Chord Diagram
User-Group mappings are rendered using d3 1 chord
diagram for visualization. All user and group information is
indexed by Apache Solr 2 server, supporting keyword and
parametric search. Google map API 3 was integrated to track
users’ and organizations geographical footprint.
VI.

LOOKINGGLASS DEPLOYMENT

We collected 7 weeks of matching Twitter messages from
Indonesia, Malaysia and Singapore starting on October 10,
2012. Weekly an average of total tweets was 786,484 and by
unique average 348,227 Tweeters. Weekly statistics of number
of tweets and tweeters are shown in Figure 7 and Figure 8.

Figure 6. LookingGlass Design and system architecture.

Initially we download all documents from web sites of
organizations, followed by discriminant perspective mining.
Next, we utilize Twitter Streaming API to collect all tweets
matching topics and perspectives. We collect tweets from each
user and apply the polarity and group-level classifier to map
tweeters to groups. We display shifts and flows among groups
by utilizing a chord diagram and their geographic footprint
extracted from GPS coded tweets and tweeter’s home pages by
utilizing a heat map.

Figure 7. Total number of weekly Indonesian tweets matching topics and
perspectives.

A. Data Collecting
In the beginning, we invite area experts with field and
domain expertise to create a list of radical and non-radical
organizations and mark their polarity and scaling using the
Graphical Scaling Tool. Next, a web crawler downloads the
web sites and RSS feeds of organizations.
B. Perspective Analysis
Next we utilize the multi-lingual topic detection and
mapping tools described in IV. B, and Discriminative
Perspective Mining algorithm described in IV. C to determine
topics and discriminative features (n-grams) to train linear
regression based polarity level (radical or non) classifier and
logistic regression based group level classifier.
C. Twitter Stream
We subscribe to Twitter streaming API and collect all
messages matching topics and topic-specific perspectives.

Figure 8. Total number of weekly Indonesian tweeters.

Based Tweeter classification results (Section V. D), around
10% of Tweeters’ messages were predicted to match the
perspectives of radical groups and rest of users’ messages
matched the non-radical groups. Figure 9 below shows the
percentages of weekly polarization between radical and nonradical.

D. User Classification
Collected raw data is aggregated weekly by user and we
apply previously trained classifiers to map users to groups
based on topics and perspectives mentioned in their tweets.
1

http://d3js.org

2

http://lucene.apache.org/solr/

3

https://developers.google.com/maps/

Figure 9. Weekly radical and non-radical polarities of Tweeters.

In order to detect shifts, we utilized the Rasch Model scaling
of organizations discussed in Section IV. E. Figure 10 below
shows the polarities of the radical and non-radical lists of
organizations we analyzed, and their rankings from neutral-toextreme positions on both sides. For instance, if a tweeters
messages were classified to a radical organization during a
previous week, but next week the Tweeter’s messages were
classified to a non-radical organization, then we consider the
user’s shift as “counter-radicalized”. Similarly, if Tweeter’s
messages were classified to a non-radical group, or a radical
group of a lower order during a previous week, and then that
user’s messages were classifier to a radical group of an higher
order the following week then we labeled the shift as
“radicalized”. The joint polarity and ranking table and two
possible shift directions are shown below in Figure 10.

Figure 11.Weekly radicalized / counter radicalized shifts - percentages.

radical groups by the group level classifier, then we label that
user as “Unaffiliated Radical”. A similar reasoning also
applies to detect “Unaffiliated Counter-Radical”.
a) An event was among the most popular tweets of 370
Unaffiliated Radicals between Oct.10.2012 and Oct.17.2012.
b) The event was a salafi student protest and the violent
reaction by the security forces against them at North Sulawesi,
Indonesia.
c) This “radicalizing” event can be detected by selecting
the “Unaffiliated Radical” segment on the chord diagram and
checking the popular URL’s mentioned in tweets. Clicking on
the corresponding URL pops-up the article titled “Demo
Cagub, Polisi Hajar Mahasiswa” from its original media
source http://sindikasi.inilah.com/. (Figure 12)

Figure 10. Weekly opinion shifts in tweets.

Weekly opinion shifts for 7 weeks are shown in Figure 11.
An average percentage of radicalized opinion shifts was around
15%, average percentage of counter radicalized opinion shifts
was 70%, and approximately 15% of users preserved their
previous weekly position exactly.
Figure 12. Event based users’ opinion tracking

VII. LOOKINGGLASS USE SCENARIOS
In this section, we present three different scenarios for
demonstrating the utility of the LookingGlass platform. All the
scenarios are drawn from real social movement and Tweeter
streams.
1) A Protest Event highlighted by Non-Affiliated Radicals.
If a Tweeter’s weekly message content gets classified as
“Radical”, but the user does not classify to one of the known

2) A Radical Group’s Local Followers.
a) Clicking on one of the radical groups on the chord
diagram shows the geographic footprint of their followers.
b) A group of followers were detected at “The Tun
Dr.Ismail International School of Johor”, in Padang Tengku,
Malaysia.

Figure 13. A Radical Group’s local followers

c) One of tweeters on the corresponding user list, is the
official twitter account of the organization itself highlighing
Khilafah goals. Another one of the tweeters on the user list,
talks about protests, jihad, mujahideen and displays the image
of a militia group on the background image of his Tweeter
page. (Figure 13)
3) A Radical Group’s Transnational Followers.
a) Selecting another radical group on the chord diagram,
reveals a group of followers exchanging messages from
Singapore, Malaysia, and Indonesia.
b) One of the popular URLs shared among these
transnational group of users is an article on a missile attack
into Israeli territory by a terrorist organization.
c) Following figure (Figure 14) shows the selected
organization with a trans-national follower footprnt on the
Chord Diagram, corresponding popular article and its media
source.

ACKNOWLEDGMENT
This research was supported by US DoDs Minerva
Research Initiative Grant N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the
project name is “Finding Allies for the War of Words:
Mapping the Diffusion and Influence of Counter-Radical
Muslim Discourse”.

REFERENCES
[1]
[2]
[3]

[4]

[5]

[6]
Figure 14. A Radical Group’s transnational followers.
[7]
[8]

[9]

G. Goertz, Social science concepts : a user’s guide, Princeton, NJ:
Princeton University Press., 2006.
G. Goertz and J. Mahoney, "Two-Level Theories and Fuzzy-Set
Analysis," Sociological Methods Research, vol. 33, pp. 497-538, 2005.
H. Davulcu, S. Ahmed, S. Gokalp, H. Temkit, T. Taylor, M. Woodward
and A. Amin, “Analyzing Sentiment Markers Describing Radical and
Counter-Radical Elements in Online News,” in IEEE Symposium on
Social Intelligence and Networking (SIN-10), pp. 335-340, 2010.
S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen, S.
Corman, M. Woodward, R. I. and A. Amin, “A System for Ranking
Organizations using Social Scale Analysis,” in Proc. of the International
Symposium on Open Source Intelligence & Web Mining (OSINT-WM),
2011.
S. Tikves, S. Gokalp, M. Temkit, S. Banerjee, J. Ye and H. Davulcu,
"Perspective Analysis for Online Debates," Proceedings of International
Symposium on Foundation of Open Source Intelligence and Security
Informatics (FOISINT-SI), 2012.
S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen, S.
Corman, M. Woodward, I. Rohmaniyah and A. Amin, " A System for
Ranking Organizations Using Social Scale Analysis," Social Network
Analysis and Mining Journal, Vols. ISSN (Print) 1869-5450 - ISSN
(Online) 1869-5469, pp. 1-16, 2012.
L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60-90, 1950.
E. Durkheim, “The cultural logic of collective representations,” in
Social theory the multicultural and classic readings , Wesleyan
University: Westview Press, 2004, pp. 90-99.
G. Simmel, Sociological Theory, New York: McGraw–Hill, 2008.

[10] A. Wallace, “Revitalization Movements,” American Anthropologist, vol.
58, pp. 264-281, 1956.
[11] A. Bayat, Making Islam Democratic: Social Movements and the PostIslamist Turn, Stanford University Press, 2007.
[12] C. Tilly, Social Movements, Boulder, Colorado, USA: Paradigm
Publishers, 2004.
[13] G.-M. J., "The rhetoric and reality: radicalization and political
discourse," International Political Science Review, vol. 33, no. 5, pp.
556-567, 2012.
[14] G.-M. J., "The rhetoric and reality: radicalization and political
discourse," International Political Science Review, vol. 33, no. 5, pp.
556-567, 2012.
[15] R. Crelinsten, "Analysing terrorism and counter-terrorism: A
communication model," Terrorism and Political Violence, vol. 14, p.
77–122, 2002.

View publication stats

[16] L. AlSumait, D. Barbara and C. Domeniconi, "On-line LDA: Adaptive
Topic Models for Mining Text Streams with Applications to Topic
Detection and Tracking," in Eighth IEEE International Conference on
Data Mining (ICDM '08), 2008.
[17] J. Liu, J. Chen and J. Ye, "Large-scale sparse logistic regression," in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge Discovery and Data Mining, 2009.
[18] D. Andrich, Rasch models for measurement, Sage, 1988.
[19] Y. Pawitan, In all likelihood: statistical modelling and inference using
likelihood., Oxford University Press, 2001.
[20] R. Tibshirani, "Regression shrinkage and selection via the lasso,"
Journal of the Royal Statistical Society Series B, vol. 58, no. 1, pp. 267288, 1996.

WWW 2012 – Demos Track

April 16–20, 2012, Lyon, France

Partisan Scale
Sedat Gokalp

Hasan Davulcu

Computer Science
Arizona State University

Computer Science
Arizona State University

Sedat.Gokalp@asu.edu

Hasan.Davulcu@asu.edu

ABSTRACT

ber of ’Yea’ and ’Nay’ votes are recorded, except for the roll
call votes. According to The Library of Congress1 ,

US Senate is the venue of political debates where the federal bills are formed and voted. Senators show their support/opposition along the bills with their votes. This information makes it possible to extract the polarity of the
senators. We use signed bipartite graphs for modeling debates, and we propose an algorithm for partitioning both
the senators, and the bills comprising the debate into binary opposing camps. Simultaneously, our algorithm scales
both the senators and the bills on a univariate scale. Using
this scale, a researcher can identify moderate and partisan
senators within each camp, and polarizing vs. unifying bills.
We applied our algorithm on all the terms of the US Senate
to the date for longitudinal analysis and developed a web
based interactive user interface www.PartisanScale.com
to visualize the analysis.

A roll call vote guarantees that every Member’s
vote is recorded, but only a minority of bills receive a roll call vote.
The current political party system in the United States
is a two-party system, which suggests a bipolar nature for
both the senators and the bills; such that, there exists two
polarized camps of senators that oppose each others views,
and two sets of bills that polarize the senators. It can be
presumed that these camps would purely split according to
the political parties of the senators, or the political parties
of the sponsors of the bills. Although this is true to a certain
extent, our analysis show that the actual behaviours can be
different for a minority.
Senators show their support/opposition along the bills
with their votes. This information makes it possible to extract the polarity of the senators. We use signed bipartite
graphs for modeling the opposition, and we used our previous work ANCO-HITS algorithm for partitioning both the
senators, and the bills into two polarized camps. Simultaneously, our algorithm scales both the senators and the bills
on a univariate scale. Using this scale, a researcher can identify moderate and partisan2 senators within each camp, and
polarizing vs. unifying bills.
Partitioning and scaling help a researcher to better understand the structure of political debates in the Senate.
While partisan ends of a scale may represent senators with
irreconcilable viewpoints, moderate senators may represent
viewpoints that are more amenable to engage in a constructive dialog through a set of unifying issues. Moderates may
sympathize with some of the claims and grievances of the
other side. Longitudinal analysis using our proposed algorithms could reveal interesting dynamics, such as, moderates
from opposing camps could be in the process of forming a
coalition by making the necessary compromises to reach a
consensus.
Major contributions of this paper are: (1) a modification
of our previous algorithm ANCO-HITS, to propagate the
scores on a signed bipartite graph to solve the partitioning and scaling problems described above; (2) applying the
algorithm on 112 terms of the US Senate for longitudinal
analysis; (3) developing a web based interactive user interface to visualize the analysis.

Categories and Subject Descriptors
H.2.8 [Database applications]: Data mining;
H.3.3 [Information Search and Retrieval]: Clustering

General Terms
Algorithms

Keywords
Community discovery, Link Analysis, Partitioning, Ranking,
Scaling, HITS, Signed Bipartite Graphs, Spectral Clustering

1.

INTRODUCTION

The United States has a bicameral legislature that comprises the US Senate as the upper house, and the US House
of Representatives. The terms of the US Senate last for two
years, and the senators serve three terms (six years) each.
The terms are staggered in such a way that approximately
one-third of the seats are up for election every two years.
The Senate meets in the United States Capitol in Washington, D.C. to form and debate on motions, or bills. When
debates conclude, the bill in question is put to a vote, where
senators respond either ’Yea’ (in favor of the bill) or ’Nay’
(against the bill). For most of the bills, only the total numCopyright is held by the International World Wide Web Conference Committee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012 Companion, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1230-1/12/04.

1

http://thomas.loc.gov/home/rollcallvotes.html
Partisanship can be defined as being devoted to or biased
in support of a party.
2

349

WWW 2012 – Demos Track

u1 u2 u3

0

April 16–20, 2012, Lyon, France

um-22um-1 um

X

vn-2 vn-1 vn Y

v 1 v2 v3

Figure 3: Vote matrix for the 111th US Senate after
scaling with ANCO-HITS

Figure 1: Perfectly polarized bipartite graph
u1

0

u2

0

X

Y

X

ui and a bill vj represents support, and a red dashed line
represents opposition.
Figure 2 shows an example of two senators; u1 being extreme and u2 being more moderate. u1 supports the bills
of same polarity, and opposes the vertices of the opposite
polarity. However, u2 has mixed support and opposition.
Same relation holds between polarizing and unifying bills.
Although partitioning algorithms can be utilized to detect
the polarity of senators and bills, it is not possible to distinguish partisans from moderates. Scaling overcomes this
problem and makes it possible to compare two senators of
same polarity. In this paper, we are not only able to compare pairs of senators, but also provide the exact locations
on the scale, therefore providing valuable information about
the shape of the distribution as well.

Y

Figure 2: Partisan vs. Moderate senators

2.

PROBLEM FORMULATION

There are many applications [5, 4, 7, 1, 3] for recognizing
political orientation, and bipartite graphs [2, 6, 8] have been
widely used to represent relationships between two sets of
entities. We use bipartite graphs to model the relationships
between the senators and the bills. We use signed edges to
represent the votes, where positive edges denote support,
and negative edges denote opposition on a bill by a senator.
Given

3.

ANCO-HITS

In this study, we used a modified version of our previous
work ANCO-HITS. Algorithm 1 describes the steps of the
ANCO-HITS algorithm for the co-scaling problem.

• G = (U ∪ V, A) is a bipartite graph consisting of senators U and bills V , and a signed vote matrix A
• U = {u1 , u2 , . . . , um }, a set of m senators

Data: Signed vote matrix A
Result: Scale vectors X and Y
Initiate X <0> = (1, 1, . . . , 1) ;
Initiate Y <0> = (1, 1, . . . , 1) ;
repeat
Update X;
Update Y ;
until X vector converges;
Algorithm 1: Iterative update procedure for ANCO-HITS

• V = {v1 , v2 , . . . , vn }, a set of n bills
• A ∈ Rm×n , where aij represents the vote of senator ui
on bill vj
Find
• X = (x1 , x2 , . . . , xm ), where xi ∈ R is the assigned
value of the senator ui
• Y = (y1 , y2 , . . . , yn ), where yj ∈ R is the assigned value
of the bill vj

This research uses a different normalization scheme than
the original ANCO-HITS algorithm. The update functions
for X and Y are modified such that the vectors X and Y
would converge not only in direction, but also in value.

such that
• xi value for a senator ui should be closer to the yj
values of the bills that he supports, and further away
from the yk values of the bills that he opposes. The
magnitude of xi denotes the partisanship of the senators ui , and the magnitude of yj denote how polarizing
the bill vj is. i.e. magnitudes closer to 0 meaning more
moderate and larger magnitudes meaning more partisan.

n
P

x<k>
i

=

j=1
n
P

|aij yj<k−1> |

j=1

m
P

aij yj<k−1>
yj<k>

=

aij x<k>
i

i=1
m
P

(1)

|aij x<k>
|
i

i=1

The convergence values for X and Y vectors will satisfy
−1 ≤ xi , yj ≤ +1.
Figure 3 represents the bipartite graph of the 111th US
Senate data after scaling both the senate and the bills with
ANCO-HITS. The light green colored edges represent ’Yea’
votes, and dark red represents ’Nay’ votes. Similar to our
motivating Figure 1, this figure also shows partisan behavior
in the 111th US Senate.

Figure 1 depicts a perfectly polarized bipartite graph. The
two axes X and Y represent the univariate scale for the
senators and bills. The vertices to the right of zero have
positive values, and the vertices to the left have negative
values on the scale. A green solid line between a senator

350

WWW 2012 – Demos Track

April 16–20, 2012, Lyon, France

Figure 4: A screenshot from PartisanScale.com showing the partisanship history for a senator

4.

INTERACTIVE USER INTERFACE

The US Congress has been collecting data since the very
first congress of the US history. This data has been encoded as XML files and publicly shared through the govtrack.us project3 . We collected the roll call votes of the
US Senate for the terms 1 through 112, covering the years
1789-2011. We ran the ANCO-HITS algorithm for each individual term. The sign of the ANCO-HITS values are arbitrary; therefore, we aligned consecutive terms by mirroring
the scale if necessary. By analyzing more than 3,000,000
votes, we produced the web based interactive user interface
www.PartisanScale.com that allows the users to navigate
through the history of the US Senate.
Figure 4 shows a screenshot of the user interface. Each
term of the senate is shown as a column in the figure. The
top row shows the terms and the years for each senate with
the incumbent US president shown below. The senators are
represented by boxes which are colored according to their
political parties.
The vertical axis of the scale represents the bipolar nature
of the US Senate. The polarity of each senator is represented
by the location of each box. The dashed line shows the
zero point. Senators around this point are calculated to be
moderate, and the senators away from the dashed line are
calculated to be more polarized. Hovering along these boxes
will show the picture, the political party, and the amount of
partisanship for the senator in focus. Clicking on the scale
will further filter the figure to show the partisanship history.
This filtering can also be done with the quick search tool on
the top right corner. The auto-completion feature will help
the users easily select the senator.
For example, Figure 4 shows a senator that is calculated
to be moderate for the 110th term. It can be seen that this
senator was first elected in 1981 and served for 15 terms until
the year 2010. It also shows us that after 12 terms of service
as a republican, he switches membership to the Democratic
Party for the last 3 terms of his service.
3

Figure 5: Longevity of service
An introductory screencast video that shows the usage of
the system can be found on the website.

5.

STATISTICS

Figure 5 shows the histogram for the number of terms each
senator served. The average number of terms the senators
served is 4.68, and the longest run is 26 terms.
Figure 6 shows the partisanship displacement distribution
for three ∆T values on a semi-log scale. Partisanship displacement is defined as the absolute distance of partisan
scale values for a senator between two terms T1 and T2 .
C∆T (d) is the number of displacements ≥ d between any
two terms T1 and T2 satisfying ∆T = T1 − T2 .
This figure shows three plots of C∆T values for ∆T = 1,
∆T = 2 and ∆T = 3. It can be clearly seen that the plots
on the semi-log scale form a linear function, which suggests
an exponential distribution.
Figure 7 aggregates the party polarities. The mean partisanship values of the senators from each party is shown as a
solid line. The shaded areas show 1 standard deviation along
the mean for each term. This figure is helpful to identify the
times of partisan politics within the US Senate.

http://www.govtrack.us/data

351

WWW 2012 – Demos Track

April 16–20, 2012, Lyon, France

Figure 7: Aggregated Party Partisanship

[3]

[4]

[5]

Figure 6: Partisanship displacement distribution

6.

[6]

CONCLUSIONS

In this paper, we introduced a measure for partisanship,
and applied it on 112 terms of the US Senate for longitudinal
analysis. We further developed an interactive user interface
www.PartisanScale.com to visualize the analysis. The
data set and the algorithm in source code are available online.

7.

[7]

[8]

ACKNOWLEDGMENTS

This research was supported in part by US DOD Minerva
Research Initiative grant N00014-09-1-0815.

8.

REFERENCES

[1] M. Bansal, C. Cardie, and L. Lee. The power of
negative thinking: Exploiting label disagreement in the
min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13–16,
2008.
[2] H. Deng, M. Lyu, and I. King. A generalized co-hits
algorithm and its application to bipartite graphs. In
Proceedings of the 15th ACM SIGKDD international

352

conference on Knowledge discovery and data mining,
pages 239–248. ACM, 2009.
W. Lin and A. Hauptmann. Are these documents
written from different perspectives?: a test of different
perspectives based on statistical distribution
divergence. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 1057–1064. Association for
Computational Linguistics, 2006.
R. Malouf and T. Mullen. Graph-based user
classification for informal online political discourse.
2007.
T. Mullen and R. Malouf. A preliminary investigation
into sentiment analysis of informal political discourse.
In AAAI symposium on computational approaches to
analysing weblogs (AAAI-CAAW), pages 159–162, 2006.
M. Rege, M. Dong, and F. Fotouhi. Co-clustering
documents and words using bipartite isoperimetric
graph partitioning. In Data Mining, 2006. ICDM’06.
Sixth International Conference on, pages 532–541.
IEEE, 2006.
M. Thomas, B. Pang, and L. Lee. Get out the vote:
Determining support or opposition from congressional
floor-debate transcripts. In In Proceedings of EMNLP,
pages 327–335, 2006.
H. Zha, X. He, C. Ding, H. Simon, and M. Gu.
Bipartite graph partitioning and data clustering. In
Proceedings of the tenth international conference on
Information and knowledge management, pages 25–32.
ACM, 2001.

