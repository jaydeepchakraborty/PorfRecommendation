Rank-One Matrix Pursuit for Matrix Completion

Zheng Wang ZHENGWANG @ ASU . EDU Ming-Jun Lai MJLAI @ MATH . UGA . EDU Zhaosong Lu ZHAOSONG @ SFU . CA Wei Fan§ DAVID . FANWEI @ HUAWEI . COM Hasan Davulcu¶ HASANDAVULCU @ ASU . EDU Jieping Ye¶ JIEPING . YE @ ASU . EDU  The Biodesign Institue, Arizona State University, Tempe, AZ 85287, USA  Department of Mathematics, University of Georgia, Athens, GA 30602, USA  Department of Mathematics, Simon Fraser University, Burnaby, BC, V5A 156, Canada § Huawei Noah's Ark Lab, Hong Kong Science Park, Shatin, Hong Kong ¶ School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281, USA

Abstract
Low rank matrix completion has been applied successfully in a wide range of machine learning applications, such as collaborative filtering, image inpainting and Microarray data imputation. However, many existing algorithms are not scalable to large-scale problems, as they involve computing singular value decomposition. In this paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of the current approximation residual and update the weights for all rank-one matrices obtained up to the current iteration. We further propose a novel weight updating rule to reduce the time and storage complexity, making the proposed algorithm scalable to large matrices. We establish the linear convergence of the proposed algorithm. The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world large-scale datasets. Results show that our algorithm is much more efficient than state-of-theart matrix completion algorithms while achieving similar or better prediction performance.
Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

1. Introduction
Low rank matrix learning has attracted significant attention in the machine learning community due to its wide range of applications, such as collaborative filtering (Koren et al., 2009; Srebro et al., 2005), compressed sensing (Cand` es & Recht, 2009), multi-class learning and multi-task learning (Argyriou et al., 2008; Negahban & Wainwright, 2010; Dud´ ik et al., 2012). In this paper, we consider the general form of low rank matrix completion: given a partially observed real-valued matrix Y  n×m , the low rank matrix completion problem is to find a matrix X  n×m with minimum rank such that P (X) = P (Y), where  includes the index pairs (i, j ) of all observed entries, and P is the orthogonal projector onto the span of matrices vanishing outside of . As it is intractable to minimize the matrix rank exactly in the general case, the trace norm or nuclear norm is widely used as a convex relaxation of the matrix rank (Cand` es & Recht, 2009). It is defined by the Schatten p-norm with p = 1. For matrix X with rank r, its r p 1/p Schatten p-norm is defined by ( i=1 i ) , where {i } are the singular values of X. Thus, the trace norm of X is r the 1 norm of the matrix spectrum as ||X|| = i=1 |i |. Solving the standard low rank or trace norm problem is computationally expensive for large matrices, as it involves computing singular value decomposition (SVD). How to solve these problems efficiently and accurately has attracted much attention in recent years (Avron et al., 2012; Srebro et al., 2005; Cai et al., 2010; Balzano et al., 2010; Keshavan & Oh, 2009; Toh & Yun, 2010; Ji & Ye, 2009; Ma et al., 2011; Mazumder et al., 2010; Mishra et al., 2011; Wen et al., 2010; Lee & Bresler, 2010; Recht & R´ e, 2013). Most of these methods still involve the computation of SVD or truncated SVD iteratively, which is

Rank-One Matrix Pursuit for Matrix Completion

not scalable to large-scale problems (Cai et al., 2010; Keshavan & Oh, 2009; Toh & Yun, 2010; Ma et al., 2011; Mazumder et al., 2010; Lee & Bresler, 2010). Several methods approximate the trace norm using its variational characterizations (Mishra et al., 2011; Srebro et al., 2005; Wen et al., 2010; Recht & R´ e, 2013), and proceed by alternating optimization. The linear convergence rate is established theoretically for properly designed alternating optimization algorithm under appropriate initialization (Jain et al., 2013). However, its computational complexity depends on the square of the rank of the estimated matrix. Thus in practical problems, especially for large matrices, it requires the rank of the estimated matrix to be very small, which sacrifices the estimation accuracy. Recently, the coordinate gradient descent method has been demonstrated to be efficient in solving sparse learning problems in the vector case (Friedman et al., 2010; ShalevShwartz & Tewari, 2009). The key idea is to solve a very simple one-dimensional problem (for one coordinate) in each iteration. One natural question is whether and how such method can be applied to solve the matrix completion problem. Some progress has been made recently along this direction (Jaggi & Sulovsk´ y, 2010; Tewari et al., 2011; Shalev-Shwartz et al., 2011; Dud´ ik et al., 2012; Zhang et al., 2012). These algorithms proceed in two main steps in each iteration. The first step involves computing the top singular vector pair, and the second step refines the weights of the rank-one matrices formed by all top singular vector pairs obtained up to the current iteration. The main differences among these algorithms lie in how they refine the weights. The Jaggi's algorithm (JS) (Jaggi & Sulovsk´ y, 2010) directly applies the Hazan's algorithm, which adapts the Frank-Wolfe algorithm to the matrix case (Hazan, 2008). It updates the weights with a small step size and does not consider further refinement. It does not use all information in each step, which leads to a slow convergence rate. Similar to JS, Tewari et al. (Tewari et al., 2011) use a small update step size for a general structure constrained problem. A more efficient Frank-Wolfe type algorithm is to fully refine the weights, which is claimed to be equivalent to orthogonal matching pursuit (OMP) in a wide range of l1 ball constrained convex optimization problems (Jaggi, 2013). The greedy efficient component optimization (GECO) (Shalev-Shwartz et al., 2011) applies a similar approach, which optimizes the weights by solving another time consuming optimization problem. It empirically reduces the number of iterations without theoretical guarantees. However, the sophisticated weight refinement leads to a higher total computational cost. The lifted coordinate gradient descent algorithm (Lifted) (Dud´ ik et al., 2012) updates the rank-one matrix basis with a constant weight in each iteration, and conducts a lasso type algorithm (Tibshirani, 1994) to fully correct the weights. The weights for

the basis update are difficult to tune: a large value leads to divergence; a small value makes the algorithm slow (Zhang et al., 2012). The matrix norm boosting approach (Boost) (Zhang et al., 2012) learns the update weights and designs a local refinement step by a non-convex optimization problem which is solved by alternating optimization. It has a sub-linear convergence rate. In this paper, we present a simple and efficient algorithm to solve the low rank matrix completion problem. The key idea is to extend the orthogonal matching pursuit procedure (Pati et al., 1993) from the vector case to the matrix case. In each iteration, a rank-one basis matrix is generated by the left and right top singular vectors of the current approximation residual. In the standard algorithm, we fully update the weights for all rank-one matrices in the current basis set at the end of each iteration by performing an orthogonal projection of the observation matrix onto their spanning subspace. The most time-consuming step of the proposed algorithm is to calculate the top singular vector pair of a sparse matrix, which costs O(||) operations in each iteration. An appealing feature of the proposed algorithm is that it has a linear convergence rate. This is quite different from traditional orthogonal matching pursuit or weak orthogonal greedy algorithms, whose convergence rate for sparse vector recovery is sub-linear as shown in (Liu & Temlyakov, 2012). See also (Tropp, 2004) for an extensive study on various greedy algorithms. With this rate of convergence, we only need O(log(1/ )) iterations for achieving an accuracy solution. One drawback of the standard algorithm is that it needs to store all rank-one matrices in the current basis set for full weight updating, which contains r|| elements in the r-th iteration. This makes the storage complexity of the algorithm dependent on the number of iterations, which restricts the approximation rank especially for large matrices. To tackle this problem, we propose an economic weight updating rule for this algorithm. In this economic algorithm, we only track two matrices in each iteration. One is the current estimated matrix and the other one is the pursued rank-one matrix. When restricted to the observations in , each has || nonzero elements. Thus the storage requirement, i.e., 2||, keeps the same in different iterations, which is the same as the greedy algorithms (Jaggi & Sulovsk´ y, 2010; Tewari et al., 2011). Interestingly, we show that using this economic updating rule we still retain the linear convergence rate. To the best of our knowledge, our proposed algorithms are the fastest among all related methods. We verify the efficiency of our algorithms empirically on large-scale matrix completion problems. The main contributions of our paper are: · We propose a computationally efficient and scalable algorithm for matrix completion, which extends the

Rank-One Matrix Pursuit for Matrix Completion

orthogonal matching pursuit from the vector case to the matrix case. · We theoretically prove the linear convergence rate of our algorithm. As a result, we only need O(log(1/ )) steps to obtain an -accuracy solution, and in each step we only need to compute the top singular vector pair, which can be computed efficiently. · We further reduce the storage complexity of our algorithm based on an economic weight updating rule while retaining the linear convergence rate. This algorithm has constant storage complexity which is independent of the approximation rank and is more practical for large-scale problems. · Our proposed algorithm is free of tuning parameter, except for the accuracy of the solution. And it is guaranteed to converge, i.e., no risk of divergence. Notations: Let Y = (y1 , · · · , ym )  n×m be an n × m real matrix, and   {1, · · · , n} × {1, · · · , m} denote the indices of the observed entries of Y. P is the projection operator onto the space spanned by the matrices vanishing outside of  so that the (i, j )-th component of P (Y) equals to Yi,j for (i, j )   and zero otherwise. The Frobenius norm of Y is defined as ||Y||F =
i,j 2 . Yi,j T T T ) denote a vector reshaped Let vec(Y) = (y1 , · · · , ym from matrix Y by concatenating all its column vectors. Let  = vec(P (Y)) be the vector by concatenating all oby served entries in Y. The inner product of two matrices X and Y is defined as X, Y = vec(X), vec(Y) . Given a matrix A  n×m , we denote P (A) by A . For any two matrices A, B  n×m , we define A, B  = A , B , A = A, A  and A = A, A .

If we reformulate the problem as min ||P (M( )) - P (Y)||2 F


s.t.

|| ||0  r,

(3)

we could solve it by an orthogonal matching pursuit type greedy algorithm using rank-one matrices as the basis. If the dictionary {Mi : i  I} is known and finite, this is equivalent to the compressed sensing problem. However, in our formulation, the size of the dictionary is infinite and the bases are to be constructed during the basis pursuit process. In particular, we are to find a suitable subset with over-complete rank-one matrix coordinates, and learn the weight for each coordinate. This is achieved by executing two steps alternatively: one is to construct the basis, and the other one is to learn the weights of the basis. Suppose that after the (k -1)-th iteration, the rank-one basis matrices M1 , . . . , Mk-1 and their current weight  k-1 are already computed. In the k -th iteration, we are to pursue a new rank-one basis matrix Mk with unit Frobenius norm, which is mostly correlated with the current observed regression residual Rk = P (Y) - Xk-1 , where k -1 k -1 Xk-1 = (M( k-1 )) = i=1 i (Mi ) . Therefore, Mk can be chosen to be an optimal solution of the following problem: max{ M, Rk : rank(M) = 1, M
M F

= 1}.

(4)

Notice that each rank-one matrix M with unit Frobenius norm can be written as the product of two unit vectors, namely, M = uvT for some u  n and v  m with u = v = 1. We then see that problem (4) can be equivalently reformulated as max{uT Rk v :
u,v

u = v = 1}.

(5)

2. Rank-One Matrix Pursuit
It is well-known that any matrix X  n×m can be written as a linear combination of rank-one matrices, that is, X = M( ) =
iI

i Mi ,

(1)

Clearly, the optimal solution (u , v ) of problem (5) is a pair of top left and right singular vectors of Rk . It can be efficiently computed by the power method (Jaggi & Sulovsk´ y, 2010; Dud´ ik et al., 2012). The new rankone basis matrix Mk is then readily available by setting T Mk = u v . After finding the new rank-one basis matrix Mk , we update the weights  k for all currently available basis matrices {M1 , · · · , Mk } by solving the following least squares regression problem:
k 

where {Mi : i  I} is the set of all n × m rank-one matrices with unit Frobenius norm. Clearly,  is an infinite dimensional vector. Such a representation can be obtained from the standard SVD of X. The original low rank matrix approximation problem aims to minimize the zero-norm of  subject to the constraint: min || ||0


min ||
k

i Mi - Y||2 .
i=1

(6)

s.t.

P (M( )) = P (Y),

(2)

where || ||0 denotes the cardinality of the number of nonzero elements of  .

 By reshaping the matrices (Y) and (Mi ) into vectors y  i , we can easily see that the optimal solution  k of and m (6) is given by ¯T ¯ -1 M ¯T ,  k = (M k Mk ) ky (7)

Rank-One Matrix Pursuit for Matrix Completion

¯ k = [m  1, · · · , m  k ] is the matrix formed by all where M ¯ k is the reshaped basis vectors. The row size of matrix M total number of observed entries. It is computationally expensive to directly calculate the matrix multiplication. An incremental update rule can be applied to solve this step efficiently (Wang et al., 2014). We run the above two steps iteratively until some desired stopping condition is satisfied. We can terminate the method based on the rank of the estimated matrix or the approximation residual. In particular, one can choose a preferred rank of the approximate solution matrix. Alternatively, one can stop the method once the residual Rk is less than a tolerance parameter . The main steps of RankOne Matrix Pursuit (R1MP) are given in Algorithm 1. Remark In our algorithm, we adapt orthogonal matching pursuit on the observed part of the matrix. This is similar to the GECO algorithm. However, GECO constructs the estimated matrix by projecting the observation matrix onto a much larger subspace, which is a product of two subspaces spanned by all left singular vectors and all right singular vectors obtained up to the current iteration. So it has much higher computational complexity. Lee et al. (Lee & Bresler, 2010) recently propose the ADMiRA algorithm, which is also a greedy approach. In each step it first chooses 2r components by top-2r truncated SVD and then uses another top-r truncated SVD to obtain a rank-r matrix. Thus, the ADMiRA algorithm is computationally more expensive than the proposed algorithm. The main difference between the proposed algorithm and ADMiRA is somewhat similar to the difference between the OMP (Pati et al., 1993) for learning sparse vectors and CoSaMP (Needell & Tropp, 2010). In addition, the performance guarantees (including recovery guarantee and convergence property) of ADMiRA rely on strong assumptions, i.e., the matrix involved in the loss function satisfies a rank-restricted isometry property, which is not satisfied in matrix completion (Lee & Bresler, 2010). Lee et al. sketch a similar idea as the standard verion of our algorithm in Remark 2.3 without any further analysis, and their theoretical results cannot be easily extended to our algorithm. Another contribution of our work is that we further propose an economic version of the algorithm and analyze its convergence property.

Algorithm 1 Rank-One Matrix Pursuit (R1MP) Input: Y and stopping criterion. Initialize: Set X0 = 0 and k = 1. repeat Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed residual matrix Rk = Y - Xk-1 and set Mk = uk (vk )T . Step 2: Compute the weight  k using the closed form ¯ TM ¯ k )-1 M ¯ Ty least squares solution  k = (M k k . k k Step 3: Set Xk = i=1 i (Mi ) and k  k + 1. until stopping criterion is satisfied ^ = k k Mi . Output: Constructed matrix Y i=1 i

Before proving Theorem 3.1, we need to establish some useful and preparatory properties of Algorithm 1. The first property says that Rk+1 is perpendicular to all previously generated Mi for i = 1, · · · , k . Property 3.2. Rk+1 , Mi = 0 for i = 1, · · · , k . Proof. Recall that  k is the optimal solution of problem (6). By the first-order optimality condition, one has k k Y - j =1 j Mj , Mi  = 0 for i = 1, · · · , k , which tok gether with Rk = Y - Xk-1 and Xk = j =1 j (Mj ) implies that Rk+1 , Mi = 0 for i = 1, · · · , k . k

The following property shows that as the number of rankone basis matrices Mi increases during our learning process, the residual Rk does not increase. Property 3.3. Rk+1  Rk for all k  1. Proof. We observe that for all k  1, Rk+1
2

= min { Y -

k

k 2 i=1 i Mi  } k-1 2 i=1 i Mi  }

 min { Y -

k-1

= Rk

2

,

and hence the conclusion holds. We next establish that {(Mi ) }k i=1 is linearly independent unless Rk = 0. It follows that formula (7) is welldefined and hence  k is uniquely defined before the algorithm stops. Property 3.4. Suppose that Rk = 0 for some k  1. Then, ¯ i has a full column rank for all i  k . M Proof. Using Property 3.3 and the assumption Rk = 0 for some k  1, we see that Ri = 0 for all i  k . We now prove this statement by induction on i. Indeed, ¯ 1 = 0. Hence the consince R1 = 0, we clearly have M clusion holds for i = 1. We now assume that it holds for i - 1 < k and need to show that it also holds for ¯ i-1 has a full coli  k . By the induction hypothesis, M ¯ i does not have umn rank. Suppose for contradiction that M a full column rank. Then, there exists   i-1 such that

3. Convergence Analysis
In this section, we will show that our proposed rank-one matrix pursuit algorithm achieves a linear convergence rate. This main result is given in the following theorem. Theorem 3.1. The rank-one matrix pursuit algorithm satisfies ||Rk ||   k-1 Y  , k  1.  is a constant in [0, 1).

Rank-One Matrix Pursuit for Matrix Completion

(Mi ) = j =1 j (Mj ) , which together with Property 3.2 implies that Ri , Mi = 0. It follows that max (Ri ) = uT i Ri vi = Ri , Mi = 0, and hence Ri = 0, which con¯i tradicts the fact that Rj = 0 for all j  k . Therefore, M has a full column rank and the conclusion holds. We next build a relationship between two consecutive residuals Rk+1 and Rk .
k-1 For convenience, define k = 0 and let  k =  k-1 +  k . In view of (6), one can observe that k

i -1

¯ k = QU into (10), and using QT Q = I Substituting M and (11), we obtain that Lk
2

T -1 ¯ T  ¯ T =r Mk rk k Mk (U U)

= [0, · · · , 0, Mk , Rk ] U-1 U-T [0, · · · , 0, Mk , Rk ] = Mk , Rk 2 /(Ukk )2  Mk , Rk 2 .

T

The last equality follows since U is upper triangular and the last inequality is due to |Ukk |  1. We are now ready to prove Theorem 3.1. Proof. Using the definition of Mk , we have

 k = arg min ||
 i=1 k

i Mi - Rk ||2 .

(8)

Let Lk =

k i (Mi ) . i=1

(9)

Mk , Rk = uk (vk )T , Rk =  (Rk ), where  (Rk ) is the maximum singular value of the residual matrix Rk . Using this inequality and Property 3.5, we obtain that ||Rk+1 ||2 = ||Rk ||2 - ||Lk ||2  ||Rk ||2 - Mk , Rk = 1-
2  (Rk ) Rk 2

By the definition of Xk , one can also observe that Xk = Xk-1 + Lk and Rk+1 = Rk - Lk . Property 3.5. ||Rk+1 ||2 = ||Rk ||2 -||Lk ||2 and ||Lk ||2  Mk , Rk 2 , where Lk is defined in (9). Proof. Since Lk = it follows from Property 3.2 that Rk+1 , Lk = 0. Thus, ||Rk+1 ||
2 k ik i (Mi ) ,

2

||Rk ||2 .
2 ,

= ||Rk - Lk ||2 = ||Rk ||2 - 2 Rk , Lk + ||Lk ||2 = ||Rk ||2 - 2 Rk+1 + Lk , Lk + ||Lk ||2 = ||Rk ||2 - 2 Lk , Lk + ||Lk ||2 = ||Rk || - ||Lk ||
2 2 2 2

In view of this relation and the fact that R1 = Y we easily conclude that
k -1

||Rk || 

Y

 i=1

1-

2 (R )  i . Ri 2

We next bound Lk from below. If Rk = 0, ||Lk ||  Mk , Rk 2 clearly holds. We now suppose throughout the remaining proof that Rk = 0. It then follows from ¯ k has a full column rank. Using this Property 3.4 that M ¯ TM ¯ k -1 M ¯ Tr fact and (8), we have  k = M k k  k , where  k is the reshaped residual vector of Rk . Invoking that r k Lk = ik i (Mi ) , we then obtain ¯T ¯ ¯ T ¯ -1 M  k. T ||Lk ||2 = r k Mk (Mk Mk ) kr (10)

 (Ri ) 1 As for each step we have 0 < rank  1, (Ri )  Ri there must exist 0   < 1 that satisfies ||Rk ||   k-1 Y  . This completes the proof. Ri Remark In practice, the value of 2 that controls the  (Ri ) convergence speed is much less than min(m, n). We will emprically verify this in the experiments.
2

¯ k = QU be the QR factorization of M ¯ k , where Let M T Q Q = I and U is a k × k nonsingular upper triangular ¯ k )k = m ¯ k )k  k , where (M matrix. One can observe that (M ¯ k and m  k is the redenotes the k -th column of the matrix M T shaped vector of (Mk ) . Recall that Mk = uk vk = ¯ k )k  1. Due to QT Q = I, M ¯ k = QU 1. Hence, (M and the definition of U, we have 0 < |Ukk |  Uk = ¯ k )k (M  1.

Remark If  is the entire set of all indices of {(i, j ), i = 1, · · · , m, j = 1, · · · , n}, our rank-one matrix pursuit algorithm equals to standard SVD using the power method. Remark This convergence is obtained for the optimization residual in the low rank matrix completion problem. We further extend our algorithm to solve the more general matrix sensing problem and analyze the corresponding statistical convergence behavior under mild conditions, such as the rank-restricted isometry property (Lee & Bresler, 2010; Jain et al., 2013). Details are provided in the longer version of this paper (Wang et al., 2014).

In addition, by Property 3.2, we have ¯T  k = [0, · · · , 0, Mk , Rk ]T . M kr (11)

Rank-One Matrix Pursuit for Matrix Completion

4. Economic Rank-One Matrix Pursuit
The proposed R1MP algorithm has to track all pursued bases and save them in the memory. It demands O(r||) storage complexity to obtain a rank-r estimated matrix. For large-scale problems, such storage requirement is not negligible and restricts the rank of the matrix to be estimated. To adapt our algorithm to large-scale problems with a large approximation rank, we simplify the orthogonal projection step by only tracking the estimated matrix Xk-1 and the rank-one update matrix Mk . In this case, we only need to estimate the weights for these two matrices in Step 2 of our algorithm by solving the following least squares problem: k = arg
={1 ,2 }

5. Experiments
In this section, we compare our rank-one matrix pursuit algorithms R1MP and ER1MP with state-of-the-art matrix completion algorithms. The competing algorithms include: singular value projection (SVP) (Jain et al., 2010), singular value thresholding (SVT) (Cand` es & Recht, 2009), Jaggi's fast algorithm for trace norm constraint (JS) (Jaggi & Sulovsk´ y, 2010), spectral regularization algorithm (SoftImpute) (Mazumder et al., 2010), low rank matrix fitting (LMaFit) (Wen et al., 2010), alternating minimization (AltMin) (Jain et al., 2013), boosting type accelerated matrixnorm penalized solver (Boost) (Zhang et al., 2012) and greedy efficient component optimization (GECO) (ShalevShwartz et al., 2011). The general greedy method (Tewari et al., 2011) is not included in our comparison, as it includes JS and GECO (included in our comparison) as special cases for matrix completion. The lifted coordinate descent method (Lifted) (Dud´ ik et al., 2012) is not included in our comparison, as it is similar to Boost proposed in (Zhang et al., 2012), but more sensitive to the parameters. The code for most of these algorithms is available online:

min

||1 Xk-1 + 2 Mk - Y||2  . (12)

This still corrects all weights of the existed bases, though the correction is sub-optimal. If we write the estimated matrix as a linear combination of the bases, we have Xk = k k-1 k k k k k 1 , for i=1 i (Mi ) with k = 2 and i = i i < k . The detailed procedure of this simplified method is given in Algorithm 2. Algorithm 2 Economic Rank-One Matrix Pursuit (ER1MP) Input: Y and stopping criterion. Initialize: Set X0 = 0 and k = 1. repeat Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed residual matrix Rk = Y - Xk-1 and set Mk = uk (vk )T . Step 2: Compute the optimal weights k for Xk-1 and Mk by solving: arg min ||1 Xk-1 + 2 (Mk ) - Y ||2 .
 k k k k Step 3: Set Xk = 1 Xk-1 + 2 (Mk ) ; k = 2 k-1 k k and i = i 1 for i < k ; k  k + 1. until stopping criterion is satisfied ^ = k k Mi . Output: Constructed matrix Y i=1 i

· SVP: http://www.cs.utexas.edu/pjain/svp/ · SVT: http://svt.stanford.edu/ · SoftImpute: http://www-stat.stanford.edu/rahulm/software.html · LMaFit: http://lmafit.blogs.rice.edu/ · Boost: http://webdocs.cs.ualberta.ca/xinhua2/boosting.zip · GECO: http://www.cs.huji.ac.il/shais/code/geco.zip We compare these algorithms in two problems, including image recovery and collaborative filtering. The data size for image recovery is relatively small, and the recommendation problem is in large-scale. In the experiments, we follow the recommended settings of the parameters for competing algorithms. If no recommended parameter value is available, we choose the best one from a candidate set using cross validation. For our R1MP and ER1MP algorithms, we only need a stopping criterion. For simplicity, we stop our algorithms after r iterations. In this way, we approximate the ground truth using a rank-r matrix. We present the experimental results using root-meansquare error (RMSE) (Jaggi & Sulovsk´ y, 2010; ShalevShwartz et al., 2011). The experiments are implemented in MATLAB1 . They call some external packages for fast
1 GECO is written in C++ and we call its executable file in MATLAB.

The proposed economic rank-one matrix pursuit algorithm (ER1MP) uses the same amount of storage as the greedy algorithms (Jaggi & Sulovsk´ y, 2010; Tewari et al., 2011), which is significantly smaller than that required by R1MP algorithm. Interestingly, we can show that the ER1MP algorithm still retains the linear convergence rate. The main result is given in the following theorem, and the proof is provided in the long version of this paper (Wang et al., 2014). Theorem 4.1. The economic rank-one matrix pursuit algorithm satisfies ||Rk ||   ~ k-1 Y  ~ is a constant in [0, 1).
,

k  1.

Rank-One Matrix Pursuit for Matrix Completion Table 1. Image recovery results measured in terms of the RMSE: the value below is the actual value times 100 (mean ± std).
Image

SVT
3.86 ± 0.02 4.48 ± 0.02 3.72 ± 0.03 4.48 ± 0.02 3.36 ± 0.02 4.49 ± 0.03

SVP
5.31 ± 0.14 5.60 ± 0.08 10.97 ± 0.17 7.62 ± 0.13 4.45 ± 0.16 5.52 ± 0.10

SoftImpute
4.60 ± 0.02 5.22 ± 0.01 4.48 ± 0.03 5.35 ± 0.02 4.10 ± 0.01 5.16 ± 0.03

LMaFit
7.45 ± 0.63 5.16 ± 0.28 4.65 ± 0.67 4.91 ± 0.05 4.12 ± 0.48 5.31 ± 0.13

AltMin
4.47 ± 0.10 5.05 ± 0.06 5.49 ± 0.46 4.87 ± 0.02 5.07 ± 0.50 5.19 ± 0.11

JS
5.48 ± 0.72 6.52 ± 0.88 7.30 ± 2.32 7.38 ± 1.41 4.42 ± 0.46 6.25 ± 0.54

R1MP
3.90 ± 0.02 4.63 ± 0.01 3.85 ± 0.03 4.89 ± 0.03 3.09 ± 0.02 4.66 ± 0.03

ER1MP
3.97 ± 0.02 4.73 ± 0.02 3.91 ± 0.03 4.96 ± 0.03 3.12 ± 0.02 4.76 ± 0.03

Lenna Barbara Clown Crowd Girl Man

computation of SVD2 and sparse matrix computations. The experiments are run in a PC with WIN7 system, Intel 4 core 3.4 GHz CPU and 8G RAM. 5.1. Image Recovery In the image recovery experiments, we use the following benchmark test images: Lenna, Barbara, Clown, Crowd, Girl, Man3 . The size of each image is 512 × 512. For each experiment, we present the average RMSE and the corresponding standard derivation of 10 different runs for each competing algorithm. In each run, we randomly exclude 50% of the pixels in the image, and the remaining ones are used as the observations. As the image matrix is not guaranteed to be low rank, we use the rank 200 for the estimation matrix for each experiment. The JS algorithm does not explicitly control the rank, thus we fix its number of iterations to 2000. The numerical results are listed in Table 1. The results show that SVT, our R1MP and ER1MP achieve the best numerical performance. However, our algorithm is much faster and more stable than SVT. For each image, ER1MP uses around 3.5 seconds, but SVT consumes around 400 seconds. Image recovery needs a relatively higher approximation rank; GECO and Boost fail to find a good recovery in some cases, so we do not include them in the table. 5.2. Recommendation In the following experiments, we compare the different matrix completion algorithms using large recommendation datasets: Jester (Goldberg et al., 2001) and MovieLens (Miller et al., 2003). We use six datasets including: Jester1, Jester2, Jester3, MovieLens100K, MovieLens1M, and MovieLens10M. The statistics of these datasets are given in Table 2. The Jester datasets were collected from a joke recommendation system. They contain anonymous ratings of 100 jokes from the users. The ratings are real values ranging from -10.00 to +10.00. The MoviePROPACK is used in SVP, SVT, SoftImpute and Boost. It is an efficient SVD package, which can be downloaded from http: //soi.stanford.edu/~rmunk/PROPACK/ 3 Images are downloaded from http://www.utdallas. edu/~cxc123730/mh_bcs_spl.html
2

Lens datasets were collected from the MovieLens website4 . They contain anonymous ratings of the movies on this web made by its users. For MovieLens100K and MovieLens1M, there are 5 rating scores (1­5), and for MovieLens10M there are 10 levels of scores with a step size 0.5 in the range of 0.5 to 5. In the following experiments, we randomly split the ratings into training and test sets. Each set contains 50% of the ratings. We compare the prediction results from different methods. In the experiments, we use 100 iterations for the JS algorithm, and for other algorithms we use the same rank for the estimated matrices; the values of the rank are {10, 10, 5, 10, 10, 20} for the six corresponding datasets. The results in terms of the RMSE is given in Table 3. We also show the running time of different methods in Table 4. We can observe from the above experiments that our ER1MP algorithm is the fastest among all competing methods to obtain satisfactory results.
Table 2. Characteristics of the recommendation datasets. Dataset Jester1 Jester2 Jester3 MovieLens100k MovieLens1M MovieLens10M # row 24983 23500 24938 943 6040 69878 # column 100 100 100 1682 3706 10677 # rating 106 106 6×105 105 106 107

5.3. Convergence and Efficiency We present the residual curves on the Lenna image in logarithmic scale for our R1MP and ER1MP algorithms in Figure 1. The results show that our algorithms reduce the approximation error in a linear rate. This is consistent with our theoretical analysis. The empirical results verify the linear convergence property of our proposed algorithms.

6. Conclusion
In this paper, we propose an efficient and scalable low rank matrix completion algorithm. The key idea is to extend orthogonal matching pursuit method from the vector case to the matrix case. We also propose a novel weight updating
4

http://movielens.umn.edu

Rank-One Matrix Pursuit for Matrix Completion Table 3. Recommendation results measured in terms of the RMSE. Boost fails on the MovieLens10M. Dataset Jester1 Jester2 Jester3 MovieLens100K MovieLens1M MovieLens10M SVP 4.7311 4.7608 8.6958 0.9683 0.9085 0.8611 SoftImpute 5.1113 5.1646 5.4348 1.0354 0.8989 0.8534 LMaFit 4.7623 4.7500 9.4275 1.2308 0.9232 0.8625 AltMin 4.8572 4.8616 9.7482 1.0042 0.9382 0.9007 Boost 5.1746 5.2319 5.3982 1.1244 1.0850 ­ JS 4.4713 4.5102 4.6866 1.0146 1.0439 0.8728 GECO 4.3680 4.3967 5.1790 1.0243 0.9290 0.8668 R1MP 4.3418 4.3649 4.9783 1.0168 0.9595 0.8621 ER1MP 4.3384 4.3546 5.0145 1.0261 0.9462 0.8692

Table 4. The running time (measured in seconds) of all methods on all recommendation datasets. Dataset Jester1 Jester2 Jester3 MovieLens100K MovieLens1M MovieLens10M SVP 18.35 16.85 16.58 1.32 18.90 > 103 SoftImpute 161.49 152.96 10.55 128.07 59.56 > 103 LMaFit 3.68 2.42 8.45 2.76 30.55 154.38 AltMin 11.14 10.47 12.23 3.23 68.77 310.82 Boost 93.91 261.70 245.79 2.87 93.91 ­ JS 29.68 28.52 12.94 2.86 13.10 130.13 GECO > 104 > 104 > 103 10.83 > 104 > 105 R1MP 1.83 1.68 0.93 0.04 0.87 23.05 ER1MP 0.99 0.91 0.34 0.04 0.54 13.79

Lenna
10
-1

Lenna
10
-1

7. Acknowledgments
This work was supported in part by China 973 Fundamental R&D Program (No.2014CB340304), NIH (LM010730), and NSF (IIS-0953662, CCF-1025177).

RMSE

10

-2

RMSE

10

-2

References
10
-3

0

50

100

150

200

250

300

10

-3

0

50

100

150

200

250

300

rank

rank

Figure 1. Illustration of the linear convergence of the proposed rank-one matrix pursuit algorithms on the Lenna image: the xaxis is the iteration, and the y-axis is the RMSE in logarithmic scale. The curves are the results for R1MP and ER1MP respectively.

Argyriou, A., Evgeniou, T., and Pontil, M. Convex multitask feature learning. Machine Learning, 73(3):243­ 272, 2008. Avron, H., Kale, S., Kasiviswanathan, S., and Sindhwani, V. Efficient and practical stochastic subgradient descent for nuclear norm regularization. In ICML, 2012. Balzano, L., Nowak, R., and Recht, B. Online identification and tracking of subspaces from highly incomplete information. In Allerton, 2010. Cai, J., Cand` es, E. J., and Shen, Z. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956­1982, 2010. Cand` es, E. J. and Recht, B. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717­772, 2009. Dud´ ik, M., Harchaoui, Z., and Malick, J. Lifted coordinate descent for learning with trace-norm regularization. In AISTATS, 2012. Friedman, J. H., Hastie, T., and Tibshirani, R. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1­22, 2010.

rule under this framework to reduce the storage complexity and make it independent of the approximation rank. Our algorithms are computationally inexpensive for each matrix pursuit iteration, and find satisfactory results in a few iterations. Another advantage of our proposed algorithms is they have only one tunable parameter, which is the rank. It is easy to understand and to use by the user. This becomes especially important in large-scale learning problems. In addition, we rigorously show that both algorithms achieve a linear convergence rate, which is significantly better than the previous known results (a sub-linear convergence rate). We also empirically compare the proposed algorithms with state-of-the-art matrix completion algorithms, and our results show that the proposed algorithms are more efficient than competing algorithms while achieving similar or better prediction performance. We plan to generalize our theoretical and empirical analysis to other loss functions in the future.

Rank-One Matrix Pursuit for Matrix Completion

Goldberg, K., Roeder, T., Gupta, D., and Perkins, C. Eigentaste: A constant time collaborative filtering algorithm. Information Retrieval, 4(2):133­151, 2001. Hazan, E. Sparse approximate solutions to semidefinite programs. In LATIN, 2008. Jaggi, M. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML, 2013. Jaggi, M. and Sulovsk´ y, M. A simple algorithm for nuclear norm regularized problems. In ICML, 2010. Jain, P., Meka, R., and Dhillon, I. S. Guaranteed rank minimization via singular value projection. In NIPS, 2010. Jain, P., Netrapalli, P., and Sanghavi, S. Low-rank matrix completion using alternating minimization. In STOC, 2013. Ji, S. and Ye, J. An accelerated gradient method for trace norm minimization. In ICML, 2009. Keshavan, R. and Oh, S. Optspace: A gradient descent algorithm on grassmann manifold for matrix completion. http://arxiv.org/abs/0910.5260, 2009. Koren, Y., Bell, R., and Volinsky, C. Matrix factorization techniques for recommender systems. Computer, 2009. Lee, K. and Bresler, Y. Admira: atomic decomposition for minimum rank approximation. IEEE Transactions on Information Theory, 56(9):4402­4416, 2010. Liu, E. and Temlyakov, T. N. The orthogonal super greedy algorithm and applications in compressed sensing. IEEE Transactions on Information Theory, 58: 2040­2047, 2012. Ma, S., Goldfarb, D., and Chen, L. Fixed point and bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1-2):321­353, 2011. Mazumder, R., Hastie, T., and Tibshirani, R. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 99:2287­ 2322, August 2010. Miller, B. N., Albert, I., Lam, S. K., Konstan, J. A., and Riedl, J. Movielens unplugged: experiences with an occasionally connected recommender system. In IUI, 2003. Mishra, B., Meyer, G., Bach, F., and Sepulchre, R. Low-rank optimization with trace norm penalty. http://arxiv.org/abs/1112.2318, 2011. Needell, D. and Tropp, J. A. Cosamp: iterative signal recovery from incomplete and inaccurate samples. Communications of the ACM, 53(12):93­100, 2010.

Negahban, S. and Wainwright, M.J. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. In ICML, 2010. Pati, Y. C., Rezaiifar, R., Rezaiifar, Y. C. Pati R., and Krishnaprasad, P. S. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In Asilomar SSC, 1993. Recht, B. and R´ e, C. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5(2):201­226, 2013. Shalev-Shwartz, S. and Tewari, A. Stochastic methods for 1 regularized loss minimization. In ICML, 2009. Shalev-Shwartz, S., Gonen, A., and Shamir, O. Largescale convex minimization with a low-rank constraint. In ICML, 2011. Srebro, N., Rennie, J., and Jaakkola, T. Maximum margin matrix factorizations. In NIPS, 2005. Tewari, A., Ravikumar, P., and Dhillon, I. S. Greedy algorithms for structurally constrained high dimensional problems. In NIPS, 2011. Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267­288, 1994. Toh, K. and Yun, S. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Pacific Journal of Optimization, 6:615 ­ 640, 2010. Tropp, J. A. Greed is good: algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50:2231­2242, 2004. Wang, Z., Lai, M., Lu, Z., Fan, W., Davulcu, H., and Ye, J. Orthogonal rank-one matrix pursuit for low rank matrix completion. http://arxiv.org/abs/1404.1377, 2014. Wen, Z., Yin, W., and Zhang, Y. Low-rank factorization model for matrix completion by a non-linear successive over-relaxation algorithm. Rice CAAM Tech Report 1007, University of Rice, 2010. Zhang, X., Yu, Y., and Schuurmans, D. Accelerated training for matrix-norm regularization: A boosting approach. In NIPS, 2012.

