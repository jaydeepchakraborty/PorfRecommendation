c 2015 Society for Industrial and Applied Mathematics


Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SIAM J. SCI. COMPUT.
Vol. 37, No. 1, pp. A488–A514

ORTHOGONAL RANK-ONE MATRIX PURSUIT
FOR LOW RANK MATRIX COMPLETION∗
ZHENG WANG† , MING-JUN LAI‡ , ZHAOSONG LU§ , WEI FAN¶, HASAN DAVULCU ,
AND JIEPING YE#

Abstract. In this paper, we propose an eﬃcient and scalable low rank matrix completion
algorithm. The key idea is to extend the orthogonal matching pursuit method from the vector case
to the matrix case. We further propose an economic version of our algorithm by introducing a novel
weight updating rule to reduce the time and storage complexity. Both versions are computationally
inexpensive for each matrix pursuit iteration and ﬁnd satisfactory results in a few iterations. Another
advantage of our proposed algorithm is that it has only one tunable parameter, which is the rank.
It is easy to understand and to use by the user. This becomes especially important in large-scale
learning problems. In addition, we rigorously show that both versions achieve a linear convergence
rate, which is signiﬁcantly better than the previous known results. We also empirically compare the
proposed algorithms with several state-of-the-art matrix completion algorithms on many real-world
datasets, including the large-scale recommendation dataset Netﬂix as well as the MovieLens datasets.
Numerical results show that our proposed algorithm is more eﬃcient than competing algorithms while
achieving similar or better prediction performance.
Key words. low rank, singular value decomposition, rank minimization, matrix completion,
matching pursuit
AMS subject classifications. 15A83, 68W40, 90C06
DOI. 10.1137/130934271

1. Introduction. Recently, low rank matrix learning has attracted signiﬁcant
attention in machine learning and data mining due to its wide range of applications, such as collaborative ﬁltering, dimensionality reduction, compressed sensing,
multiclass learning, and multitask learning. See [1, 2, 3, 7, 9, 23, 34, 40, 37] and
the references therein. In this paper, we consider the general form of low rank matrix completion: given a partially observed real-valued matrix Y ∈ n×m , the low
rank matrix completion problem is to ﬁnd a matrix X ∈ n×m with minimum rank
that best approximates the matrix Y on the observed elements. The mathematical
formulation is given by
(1.1)

min

rank(X)

s.t.

PΩ (X) = PΩ (Y),

X∈n×m

∗ Submitted to the journal’s Methods and Algorithms for Scientiﬁc Computing section August 26,
2013; accepted for publication (in revised form) November 18, 2014; published electronically February
19, 2015. This research was partially supported by NSF (IIS-0953662, CCF-1025177, IIS-1421057),
NIH (LM010730), China 973 Fundamental R&D Program (2014CB340304), NSERC Discovery Grant
and a collaboration grant from the Simons Foundation.
http://www.siam.org/journals/sisc/37-1/93427.html
† Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor,
MI 48109 (zhengwang@umich.edu).
‡ Department of Mathematics, University of Georgia, Athens, GA 30602 (mjlai@math.uga.edu).
§ Department of Mathematics, Simon Fraser University, Burnaby, BC V5A 156, Canada
(zhaosong@sfu.ca).
¶ Huawei Noah’s Ark Lab, Units 520–530 Core Building 2, Hong Kong Science Park, Hong Kong
(wei.fan@gmail.com).
 School of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, AZ 85287 (hasandavulcu@asu.edu).
# Department of Computational Medicine and Bioinformatics, and Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109 (jpye@umich.edu).

A488

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A489

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

where Ω is the set of all index pairs (i, j) of observed entries, and PΩ is the orthogonal
projector onto the span of matrices vanishing outside of Ω.
1.1. Related works. As it is intractable to minimize the matrix rank exactly
in the general case, many approximate solutions have been proposed to attack the
problem (1.1) (cf., e.g., [7, 24, 28]). A widely used convex relaxation of matrix rank is
the trace norm or nuclear norm [7]. The matrix trace norm is deﬁned by the Schatten
p-norm
with p = 1. For matrix X with rank r, its Schatten p-norm is deﬁned by
r
( i=1 σip )1/p , where {σi } are the singular values of X and without loss of generality
we assume they are sorted in descending order.
r Thus, the trace norm of X is the 1
norm of the matrix spectrum as ||X||∗ = i=1 |σi |. Then the convex relaxation for
problem (1.1) is given by
(1.2)

min

||X||∗

s.t.

PΩ (X) = PΩ (Y).

X∈Rn×m

Cai, Candès, and Shen [6] propose an algorithm to solve (1.2) based on soft singular
value thresholding (SVT). Keshavan and Oh [21] and Jain, Meka, and Dhillon [18]
develop more eﬃcient algorithms by using the top-k singular pairs.
Many other algorithms have been developed to solve the trace norm penalized
problem:
(1.3)

min

X∈Rn×m

||PΩ (X) − PΩ (Y)||2F + λ||X||∗ .

Ji and Ye [20], Liu, Sun, and Toh [27], and Toh and Yun [44] independently propose to
employ the proximal gradient algorithm to improve the algorithm of [6] by signiﬁcantly
√
reducing the number of iterations. They obtain an -accurate solution in O(1/ )
steps. More eﬃcient soft singular vector thresholding algorithms are proposed in
[29, 30] by investigating the factorization property of the estimated matrix. Each step
of the algorithms requires the computation of a partial singular value decomposition
(SVD) for a dense matrix. In addition, several methods approximate the trace norm
using its variational characterizations [32, 40, 46, 37] and proceed by alternating
optimization. However, these methods lack global convergence guarantees.
Solving these low rank or trace norm problems is computationally expensive for
large matrices, as it involves computing SVD. Most of the methods above involve
the computation of SVD or truncated SVD iteratively, which is not scalable to largescale problems. How to solve these problems eﬃciently and accurately for large-scale
problems has attracted much attention in recent years.
Recently, the coordinate gradient descent method has been demonstrated to be
eﬃcient in solving sparse learning problems in the vector case [11, 39, 47, 48]. The key
idea is to solve a very simple one-dimensional problem (for one coordinate) in each
iteration. One natural question is whether and how such a method can be applied to
solve the matrix completion problem. Some progress has been made recently in this
direction. Dudı́k, Harchaoui, and Malick [9] propose a coordinate gradient descent
solution for the trace norm penalized problem. They recast the nonsmooth objective
in problem (1.3) as a smooth one in an inﬁnite dimensional rank-one matrix space,
then apply the coordinate gradient algorithm on the collection of rank-one matrices.
Zhang, Yu, and Schuurmann [49] further improve the eﬃciency using the boosting
method, and the improved algorithm guarantees an -accuracy within O(1/) iterations. Although these algorithms need slightly more iterations than the proximal

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A490

WANG, LAI, LU, FAN, DAVULCU, AND YE

methods, they are more scalable as they only need to compute the top singular vector pair in each iteration. Note that the top singular vector pair can be computed
eﬃciently by the power method or Lanczos iterations [13]. Jaggi and Sulovský [17]
propose an algorithm which achieves the same iteration complexity as the algorithm in
[49] by directly applying Hazan’s algorithm [15]. Tewari, Ravikumar, and Dhillon [42]
solve a more general problem based on a greedy algorithm. Shalev-Shwartz, Gonen,
and Shamir [38] further reduce the number of iterations based on heuristics without
theoretical guarantees.
Most methods based on the top singular vector pair include two main steps in
each iteration. The ﬁrst step involves computing the top singular vector pair, and the
second step reﬁnes the weights of the rank-one matrices formed by all top singular
vector pairs obtained up to the current iteration. The main diﬀerences among these algorithms lie in how they reﬁne the weights. Jaggi’s algorithm (JS) [17] directly applies
Hazan’s algorithm [15], which relies on the Frank–Wolfe algorithm [10]. It updates
the weights with a small step size and does not consider further reﬁnement. It does
not choose the optimal weights in each step, which leads to a slow convergence rate.
Similar to JS, Tewari, Ravikumar, and Dhillon [42] use a small update step size for a
general structure constrained problem. The greedy eﬃcient component optimization
(GECO) [38] optimizes the weights by solving another time-consuming optimization
problem. It involves a smaller number of iterations than the JS algorithm. However,
the sophisticated weight reﬁnement leads to a higher total computational cost. The
lifted coordinate gradient descent algorithm [9] updates the weights with a constant
step size in each iteration and conducts a LASSO-type algorithm [43] to fully correct
the weights. The weights for the basis update are diﬃcult to tune as a large value
leads to divergence and a small value makes the algorithm slow [49]. The matrix
norm boosting approach (Boost) [49] learns the update weights and designs a local
reﬁnement step by a nonconvex optimization problem which is solved by alternating
optimization. It has a sublinear convergence rate.
We summarize their common drawbacks as follows:
• Some weight reﬁnement steps are ineﬃcient, resulting in a slow convergence
rate. The current best convergence rate is O(1/). Some reﬁnement steps
themselves contain computationally expensive iterations [9, 49], which do not
scale to large-scale data.
• They have heuristic-based tunable parameters which are not easy to use.
However, these parameters severely aﬀect their convergence speed and the
approximation result. In some algorithms, an improper parameter even makes
the algorithm diverge [6, 9].
In this paper, we present a simple and eﬃcient algorithm to solve the low rank
matrix completion problem. The key idea is to extend the orthogonal matching pursuit (OMP) procedure [35] from the vector case to the matrix case. In each iteration,
a rank-one basis matrix is generated by the left and right top singular vectors of the
current approximation residual. In the standard version of the proposed algorithm,
we fully update the weights for all rank-one matrices in the current basis set at the
end of each iteration; this is achieved by performing an orthogonal projection of the
observation matrix onto the spanning subspace of those rank-one matrices. The most
time-consuming step of the proposed algorithm is to calculate the top singular vector pair of a sparse matrix, which involves O(|Ω|) operations in each iteration. An
appealing feature of the proposed algorithm is that it has a linear convergence rate.
This is diﬀerent from traditional OMP or weak orthogonal greedy algorithms, whose
convergence rate for sparse vector recovery is sublinear, as shown in [26]. See also

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A491

[8], [41], [45] for an extensive study on various greedy algorithms. With this rate of
convergence, we only need O(log(1/)) iterations for achieving an -accuracy solution.
One drawback of the standard algorithm is that it needs to store all rank-one
matrices in the current basis set for full weight updating, which contains r|Ω| elements
in the rth iteration. This makes the storage complexity of the algorithm dependent on
the number of iterations, which restricts the approximation rank especially for largescale matrices. To tackle this problem, we propose an economic weight updating rule
for this algorithm. In this economic version of the proposed algorithm, we only track
two matrices in each iteration. One is the current estimated matrix and the other
is the pursued rank-one matrix. When restricted to the observations in Ω, each has
|Ω| nonzero elements. Thus the storage requirement, i.e., 2|Ω|, remains the same in
diﬀerent iterations, which is the same as the greedy algorithms [17, 42]. Interestingly,
we show that using this economic updating rule we still retain the linear convergence
rate. Besides the convergence property, we also analyze the recovery guarantee of our
proposed algorithm. Speciﬁcally, we extend our proposed algorithm to a more general
matrix sensing problem and show the recovery guarantee of the proposed algorithm
under the rank-restricted isometry property [25]. We verify the eﬃciency of our
algorithm empirically on large-scale matrix completion problems, such as MovieLens
[31] and Netﬂix [4, 5].
The main contributions of our paper are as follows:
• We propose a computationally eﬃcient and scalable algorithm for matrix
completion, which extends OMP from the vector case to the matrix case.
• We theoretically prove the linear convergence rate of our algorithm. As a
result, we only need O(log(1/)) iterations to obtain an -accuracy solution,
and in each iteration we only need to compute the top singular vector pair,
which can be computed eﬃciently.
• We further reduce the storage complexity of our algorithm based on an economic weight updating rule while retaining the linear convergence rate. This
version of our algorithm has a constant storage complexity which is independent of the approximation rank and is more practical for large-scale matrices.
• We extend our proposed algorithm to a more general matrix sensing problem
and show the recovery guarantee of the proposed algorithm under the rankrestricted isometry property.
• Both versions of our algorithm have only one free parameter, i.e., the rank
of the estimated matrix. The proposed algorithm is guaranteed to converge,
i.e., no risk of divergence.
1.2. Notation and organization. Let Y = (y1 , . . . , ym ) ∈ n×m be an n × m
real matrix, and let Ω ⊂ {1, . . . , n} × {1, . . . , m} denote the indices of the observed
entries of Y. PΩ is the projection operator onto the space spanned by the matrices van(i, j) ∈ Ω
ishing outside of Ω so that the (i, j)th component of PΩ (Y) equals to Yi,j for

2
and zero otherwise. The Frobenius norm of Y is deﬁned as ||Y||F =
i,j Yi,j .
T T
Let vec(Y) = (y1T , . . . , ym
) denote a vector reshaped from matrix Y by concatenating all its column vectors. Let ẏ = vecΩ (Y) = {(yω1 , . . . , yω|Ω| )T ∀ ωi ∈ Ω}
denote a vector generated by concatenating all observed elements of Y indexed by
Ω. The Frobenius inner product of two matrices X and Y is deﬁned as X, Y =
trace(XT Y), which also equals the componentwise inner product of the corresponding vectors as vec(X), vec(Y). Given a matrix A ∈ n×m , we denote PΩ (A)
by AΩ . For any two matrices A, B ∈ n×m , we deﬁne A, BΩ = AΩ , BΩ  and

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A492

WANG, LAI, LU, FAN, DAVULCU, AND YE


	A	Ω =
A, AΩ . Without further declaration,
 the matrix norm refers to the
Frobenius norm, which is also written as 	A	 = A, A.
The rest of the paper is organized as follows. We present the standard version of
our algorithm in section 2. Section 3 analyzes the convergence rate of the standard
version of our algorithm; we further propose an economic version of our algorithm
and prove its linear convergence rate in Section 4. Section 5 extends the proposed
algorithm to a more general matrix sensing case and presents its guarantee of ﬁnding
the optimal solution under the rank-restricted isometry property condition. In section
6 we analyze the stability of both versions of our algorithm; empirical evaluations are
presented in section 7 to verify the eﬃciency and eﬀectiveness of our algorithm. We
ﬁnally conclude our paper in section 8.
2. Orthogonal rank-one matrix pursuit. It is well-known that any matrix
X ∈ n×m can be written as a linear combination of rank-one matrices, that is,

(2.1)
X = M(θ) =
θi M i ,
i∈I

where {Mi : i ∈ I} is the set of all n × m rank-one matrices with unit Frobenius
norm. Clearly, there are inﬁnitely many choices of Mi ’s. Such a representation can
be obtained via the standard SVD of X.
The original low rank matrix approximation problem aims to minimize the zeronorm of θ subject to the constraint
min ||θ||0
θ

(2.2)

s.t.

PΩ (M(θ)) = PΩ (Y),

where ||θ||0 denotes the number of nonzero elements of the vector θ.
If we reformulate the problem as
min

||PΩ (M(θ)) − PΩ (Y)||2F

s.t.

||θ||0 ≤ r,

θ

(2.3)

we could solve it by an OMP type algorithm using rank-one matrices as the basis. In
particular, we are to ﬁnd a suitable subset of overcomplete rank-one matrix coordinates and learn the weight for each selected coordinate. This is achieved by executing
two steps alternatively: one is to pursue the basis, and the other is to learn the weight
of the basis.
Suppose that after the (k − 1)th iteration, the rank-one basis matrices M1 , . . . ,
Mk−1 and their current weight vector θ k−1 are already computed. In the kth iteration,
we are to pursue a new rank-one basis matrix Mk with unit Frobenius norm, which is
mostly correlated with the current observed regression residual Rk = PΩ (Y) − Xk−1 ,
where
Xk−1 = (M(θ k−1 ))Ω =

k−1


θik−1 (Mi )Ω .

i=1

Therefore, Mk can be chosen to be an optimal solution of the following problem:
(2.4)

max {M, Rk  : rank(M) = 1, 	M	F = 1} .
M

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A493

Notice that each rank-one matrix M with unit Frobenius norm can be written as the
product of two unit vectors, namely, M = uvT for some u ∈ n and v ∈ m with
	u	 = 	v	 = 1. We then see that problem (2.4) can be equivalently reformulated as
(2.5)

max{uT Rk v : 	u	 = 	v	 = 1}.
u,v

Clearly, the optimal solution (u∗ , v∗ ) of problem (2.5) is a pair of top left and right
singular vectors of Rk . It can be eﬃciently computed by the power method [17, 9].
The new rank-one basis matrix Mk is then readily available by setting Mk = u∗ v∗T .
After ﬁnding the new rank-one basis matrix Mk , we update the weights θ k for all
currently available basis matrices {M1 , . . . , Mk } by solving the following least squares
regression problem:
(2.6)

min ||

θ∈k

k


θi Mi − Y||2Ω .

i=1

By reshaping the matrices (Y)Ω and (Mi )Ω into vectors ẏ and ṁi , we can easily see
that the optimal solution θ k of (2.6) is given by
(2.7)

θ k = (M̄Tk M̄k )−1 M̄Tk ẏ,

where M̄k = [ṁ1 , . . . , ṁk ] is the matrix formed by all reshaped basis vectors. The
row size of matrix M̄k is the total number of observed entries. It is computationally
expensive to directly calculate the matrix multiplication. We simplify this step by an
incremental process and give the implementation details in the appendix.
We run the above two steps iteratively until some desired stopping condition is
satisﬁed. We can terminate the method based on the rank of the estimated matrix
or the approximation residual. In particular, one can choose a preferred rank of the
solution matrix. Alternatively, one can stop the method once the residual 	Rk 	 is less
than a tolerance parameter ε. The main steps of orthogonal rank-one matrix pursuit
(OR1MP) are given in Algorithm 1.
Algorithm 1. OR1MP.
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk vkT .
Step 2: Compute the weight vector θ k using the closed form least squares solution θ k = (M̄Tk M̄k )−1 M̄Tk ẏ.

Step 3: Set Xk = ki=1 θik (Mi )Ω and k ← k + 1.
until stopping criterion is satisﬁed
k
Output: Constructed matrix Ŷ = i=1 θik Mi .
Remark 2.1. In our algorithm, we adapt OMP on the observed part of the matrix.
This is similar to the GECO algorithm. However, GECO constructs the estimated
matrix by projecting the observation matrix onto a much larger subspace, which is a
product of two subspaces spanned by all left singular vectors and all right singular
vectors obtained up to the current iteration. So it has a much higher computational
complexity. Lee and Bresler [25] recently proposed the ADMiRA algorithm, which is

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A494

WANG, LAI, LU, FAN, DAVULCU, AND YE

also a greedy approach. In each step it ﬁrst chooses 2r components by top-2r truncated SVD and then uses another top-r truncated SVD to obtain a rank-r estimated
matrix. Thus, the ADMiRA algorithm is computationally more expensive than the
proposed algorithm. The diﬀerence between the proposed algorithm and ADMiRA is
somewhat similar to the diﬀerence between OMP [35] for learning sparse vectors and
CoSaMP [33]. In addition, the performance guarantees (including recovery guarantee
and convergence property) of ADMiRA rely on strong assumptions, i.e., the matrix
involved in the loss function satisﬁes a rank-restricted isometry property [25].
3. Convergence analysis of Algorithm 1. In this section, we will show that
Algorithm 1 is convergent and achieves a linear convergence rate. This result is given
in the following theorem.
Theorem 3.1. OR1MP satisfies


1
1−
min(m, n)

||Rk || ≤

k−1
	Y 	Ω

∀k ≥ 1.

Before proving Theorem 3.1, we need to establish some useful and preparatory
properties of Algorithm 1. The ﬁrst property says that Rk+1 is perpendicular to all
previously generated Mi for i = 1, . . . , k.
Property 3.2. Rk+1 , Mi  = 0 for i = 1, . . . , k.
Proof. Recall that θ k is the optimal solution of problem (2.6). By the ﬁrst-order
optimality condition, one has


	
k

k
θi M i , M i
= 0 for i = 1, . . . , k,
Y−
Ω

i=1

k
which together with Rk = YΩ − Xk−1 and Xk = i=1 θik (Mi )Ω implies that Rk+1 ,
Mi  = 0 for i = 1, . . . , k.
The following property shows that as the number of rank-one basis matrices Mi
increases during our learning process, the residual 	Rk 	 does not increase.
Property 3.3. 	Rk+1 	 ≤ 	Rk 	 for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
	Rk+1 	2 = min {	Y −
θ∈k

k


θi Mi 	2Ω }

i=1
k−1


≤ min {	Y −
=

θ∈k−1
	Rk 	2 ,

θi Mi 	2Ω }

i=1

and hence the conclusion holds.
We next establish that {(Mi )Ω }ki=1 is linearly independent unless 	Rk 	 = 0. It
follows that formula (2.7) is well-deﬁned and hence θ k is uniquely deﬁned before the
algorithm stops.
Property 3.4. Suppose that Rk = 0 for some k ≥ 1. Then, M̄i has a full
column rank for all i ≤ k.
Proof. Using Property 3.3 and the assumption Rk = 0 for some k ≥ 1, we see
that Ri = 0 for all i ≤ k. We now prove the statement of this lemma by induction
on i. Indeed, since R1 = 0, we clearly have M̄1 = 0. Hence the conclusion holds for

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A495

i = 1. We now assume that it holds for i − 1 < k and need to show that it also holds
for i ≤ k. By the induction hypothesis, M̄i−1 has a full column rank. Suppose for
contradiction that M̄i does not have a full column rank. Then, there exists α ∈ i−1
such that
(Mi )Ω =

i−1


αj (Mj )Ω ,

j=1

which together with Property 3.2 implies that Ri , Mi  = 0. It follows that
σ1 (Ri ) = uTi Ri vi = Ri , Mi  = 0,
and hence Ri = 0, which contradicts the fact that Rj = 0 for all j ≤ i. Therefore,
M̄i has a full column rank and the conclusion holds for general i.
We next build a relationship between two consecutive residuals 	Rk+1 	 and 	Rk 	.
For convenience, deﬁne θkk−1 = 0 and let
θ k = θ k−1 + η k .
In view of (2.6), one can observe that
(3.1)

η = arg min ||
k

η

k


ηi Mi − Rk ||2Ω .

i=1

Let
(3.2)

Lk =

k


ηik (Mi )Ω .

i=1

By the deﬁnition of Xk , one can also observe that
Xk = Xk−1 + Lk ,
Rk+1 = Rk − Lk .
Property 3.5. ||Rk+1 ||2 = ||Rk ||2 − ||Lk ||2 and ||Lk ||2 ≥ Mk , Rk 2 , where Lk
is defined in (3.2).

Proof. Since Lk = i≤k ηik (Mi )Ω , it follows from Property 3.2 that Rk+1 , Lk  =
0. We then have
||Rk+1 ||2 = ||Rk − Lk ||2
= ||Rk ||2 − 2Rk , Lk  + ||Lk ||2
= ||Rk ||2 − 2Rk+1 + Lk , Lk  + ||Lk ||2
= ||Rk ||2 − 2Lk , Lk  + ||Lk ||2
= ||Rk ||2 − ||Lk ||2 .
We next bound 	Lk 	2 from below. If Rk = 0, ||Lk ||2 ≥ Mk , Rk 2 clearly holds.
We now suppose throughout the remaining proof that Rk = 0. It then follows from
Property 3.4 that M̄k has a full column rank. Using this fact and (3.1), we have
−1 T

M̄k ṙk ,
η k = M̄Tk M̄k

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A496

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

where ṙk is the reshaped residual vector of Rk . Invoking that Lk =
we then obtain


i≤k

ηik (Mi )Ω ,

||Lk ||2 = ṙTk M̄k (M̄Tk M̄k )−1 M̄Tk ṙk .

(3.3)

Let M̄k = QU be the QR factorization of M̄k , where QT Q = I and U is a k × k
nonsingular upper triangular matrix. One can observe that (M̄k )k = ṁk , where
(M̄k )k denotes the kth column of the matrix M̄k and ṁk is the reshaped vector of
(Mk )Ω . Recall that 	Mk 	 = 	uk vkT 	 = 1. Hence, 	(M̄k )k 	 ≤ 1. Due to QT Q = I,
M̄k = QU, and the deﬁnition of U, we have
0 < |Ukk | ≤ 	Uk 	 = 	(M̄k )k 	 ≤ 1.
In addition, by Property 3.2, we have
T

M̄Tk ṙk = [0, . . . , 0, Mk , Rk ] .

(3.4)

Substituting M̄k = QU into (3.3), and using QT Q = I and (3.4), we obtain that
	Lk 	2 = ṙTk M̄k (UT U)−1 M̄Tk ṙk
= [0, . . . , 0, Mk , Rk ] U−1 U−T [0, . . . , 0, Mk , Rk ]

T

= Mk , Rk 2 /(Ukk )2 ≥ Mk , Rk 2 ,
where the last equality follows since U is upper triangular and the last inequality is
due to |Ukk | ≤ 1.
We are now ready to prove Theorem 3.1.
Proof of Theorem 3.1. Using the deﬁnition of Mk , we have
Mk , Rk  = uk vkT , Rk  = σ1 (Rk )

≥

σi2 (Rk )
=
rank(Rk )
i



	Rk 	2
≥
rank(Rk )



	Rk 	2
.
min(m, n)

Using this inequality and Property 3.5, we obtain that
||Rk+1 ||2

= ||Rk ||2 − ||Lk ||2 ≤ ||Rk ||2 − Mk , Rk 2
≤ (1 −

1
2
min(m,n) )||Rk || .

In view of this relation and the fact that 	R1 	 = 	Y	2Ω , we easily conclude that
k−1

1
||Rk || ≤
1−
	Y	Ω .
min(m, n)
This completes the proof.
Remark 3.6. If Ω is the entire set of all indices of {(i, j), i = 1, . . . , n, j =
1, . . . , m}, our OR1MP algorithm equals the standard SVD using the power method.
In particular, when Ω is the set of all indices while the given entries are noisy values
of an exact matrix, our OR1MP algorithm can help remove the noise.
Remark 3.7. In a standard study of the convergence rate of OMP or the orthogonal greedy algorithm, one can only get |Mk , Rk | ≥ 	Rk 	2 , which leads to a
sublinear convergence. Our Mk is a data dependent construction which is based on
the top left and right singular vectors of the residual matrix Rk . It thus has a better
estimate which gives us the linear convergence.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A497

Algorithm 2. EOR1MP.
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk vkT .
Step 2: Compute the optimal weights αk for Xk−1 and Mk by solving
min ||α1 Xk−1 + α2 (Mk )Ω − YΩ ||2 .
α

Step 3: Set Xk = αk1 Xk−1 + αk2 (Mk )Ω ; θkk = αk2 and θik = θik−1 αk1 for i < k;
k ← k + 1.
until stopping criterion is satisﬁed
k
Output: Constructed matrix Ŷ = i=1 θik Mi .

4. An economic OR1MP algorithm. The proposed OR1MP algorithm has
to track all pursued bases and save them in the memory. It demands O(r|Ω|) storage
complexity to obtain a rank-r estimated matrix. For large-scale problems, such storage
requirement is not negligible and restricts the rank of the matrix to be estimated. To
adapt our algorithm to large-scale problems with a large approximation rank, we
simplify the orthogonal projection step by only tracking the estimated matrix Xk−1
and the rank-one update matrix Mk . In this case, we only need to estimate the
weights for these two matrices by solving the following least squares problem:
(4.1)

αk = arg

min

α={α1 ,α2 }

||α1 Xk−1 + α2 Mk − Y||2Ω .

This still fully corrects all weights of the existed bases, though the correction is suboptimal. If 
we write the estimated matrix as a linear combination of the bases, we
have Xk = ki=1 θik (Mi )Ω with θkk = αk2 and θik = θik−1 αk1 , for i < k. The detailed
procedure of this simpliﬁed method is given in Algorithm 2.
The proposed economic orthogonal rank-one matrix pursuit algorithm (EOR1MP)
uses the same amount of storage as the greedy algorithms [17, 42], which is signiﬁcantly
smaller than that required by our OR1MP algorithm, i.e., Algorithm 1. Interestingly,
we can show that the EOR1MP algorithm is still convergent and retains the linear
convergence rate. The main result is given in the following theorem.
Theorem 4.1. Algorithm 2, the EOR1MP algorithm, satisfies

||Rk || ≤

1
1−
min(m, n)

k−1
	Y	Ω

∀k ≥ 1.

Before proving Theorem 4.1, we present several useful properties of our Algorithm 2. The ﬁrst property says that Rk+1 is perpendicular to matrix Xk−1 and
matrix Mk .
Property 4.2. Rk+1 , Xk−1  = 0 and Rk+1 , Mk  = 0.
Proof. Recall that αk is the optimal solution of problem (4.1). By the ﬁrst-order
optimality condition according to Xk−1 and Mk , one has
Y − αk1 Xk−1 − αk2 Mk , Xk−1 Ω = 0
and

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A498

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Y − αk1 Xk−1 − αk2 Mk , Mk Ω = 0,
which together with Rk = YΩ − Xk−1 imply that Rk+1 , Xk−1  = 0 and Rk+1 ,
Mk  = 0.
Property 4.3. 	Rk+1 	2 = 	YΩ 	2 − 	Xk 	2 for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
	YΩ 	2 = 	Rk+1 + Xk 	2
= 	Rk+1 	2 + 	Xk 	2 + 2Rk+1 , Xk 
= 	Rk+1 	2 + 	Xk 	2
as Rk+1 , Xk  = αk1 Rk+1 , Xk−1  + αk2 Rk+1 , Mk  = 0, and hence the conclusion
holds.
The following property shows that as the number of rank-one basis matrices Mi
increases during our iterative process, the residual 	Rk 	 decreases.
Property 4.4. 	Rk+1 	 ≤ 	Rk 	 for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
	Rk 	2 = min2 	Y − α1 Xk−2 − α2 Mk−1 	2Ω
α∈

= 	Y − (αk−1
Xk−2 + αk−1
Mk−1 )	2Ω
1
2

≥ min2 	Y − α1 (αk−1
Xk−2 + αk−1
Mk−1 ) − α2 Mk 	2Ω
1
2
α∈

= min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

= 	Rk+1 	2 ,
and hence the conclusion holds.
Let
Ak =

BTk Bk

=



Xk−1 , Xk−1  Xk−1 , Mk 
Mk , Xk−1 

Mk , Mk Ω

and Bk = [vec(Xk−1 ), vec((Mk )Ω )]. The solution of problem (4.1) is αk = A−1
k
BTk vec(YΩ ). We next establish that vec(Xk−1 ) and vec((Mk )Ω ) are linearly independent unless 	Rk 	 = 0. It follows that Ak is invertible and hence αk is uniquely
deﬁned before the algorithm stops.
Property 4.5. If Xk−1 = β(Mk )Ω for some β = 0, then 	Rk+1 	 = 	Rk 	.
Proof. If Xk−1 = β(Mk )Ω with nonzero β, we get
	Rk+1 	2 = min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

= min2 	Y − (α1 + α2 /β)Xk−1 	2Ω
α∈

= min 	Y − γXk−1 	2Ω
γ∈

= min 	Y − γαk−1
Xk−2 − γαk−1
Mk−1 	2Ω
1
2
γ∈

≥ min2 	Y − γ1 Xk−2 − γ2 Mk−1 	2Ω
γ∈

= 	Y − Xk−1 	2Ω
= 	Rk 	2 ,

and hence the conclusion holds with 	Rk 	2 ≥ 	Rk+1 	2 given in Property 4.4.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A499

Property 4.6. Let σ1 (Rk ) be the maximum singular value of Rk . Mk , Rk  =
for all k ≥ 1.
σ1 (Rk ) ≥ √ Rk 
min(m,n)

Proof. The optimum Mk in our algorithm satisﬁes
Mk , Rk  =

M, Rk  = σ1 (Rk ).

max

rank(M)=1


Using the fact that rank(Rk )σ1 (Rk ) ≥ 	Rk 	 and rank(Rk ) ≤ min(m, n), we get
the conclusion.
Property 4.7. Suppose that Rk = 0 for some k ≥ 1. Then, Xk−1 = β(Mk )Ω
for all β = 0.
Proof. If Xk−1 = β(Mk )Ω with β = 0, we have
	Rk+1 	2 = 	Y − Xk 	2Ω
= min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

= min2 	Y − (α1 + α2 /β)Xk−1 	2Ω
α∈

= min 	Y − γXk−1 	2Ω
γ∈

= 	Y − γ k Xk−1 	2Ω
= 	Rk 	2
= 	Y − Xk−1 	2Ω .
As Rk = 0, we have (Mk )Ω = 0 and Xk−1 = 0. Then from the above equality, we
conclude that γ k = 1 is the unique optimal solution of the minimization in terms of
γ, and thus we obtain its ﬁrst-order optimality condition: Xk−1 , Rk  = 0. However,
this contradicts
Xk−1 , Rk  = βMk , Rk  = βσ1 (Rk ) = 0.
This completes the proof.
We next build a relationship between two consecutive residuals 	Rk+1 	 and 	Rk 	.
σ2 (R )
Property 4.8. 	Rk+1 	2 ≤ 	Rk 	2 − M1k ,Mkk 	Ω .
Proof.
	Rk+1 	2 = min2 	Y − α1 Xk−1 − α2 Mk 	2Ω
α∈

≤ min 	Y − Xk−1 − α2 Mk 	2Ω
α2 ∈

= min 	Rk − α2 Mk 	2Ω .
α2 ∈

This has a closed form solution as α∗2 =
into the formulation, we get

Rk ,Mk 	
Mk ,Mk 	Ω .

	Rk+1 	2 ≤ 	Rk −

Plugging this optimum α∗2 back

Rk , Mk 
Mk 	2Ω
Mk , Mk Ω

= 	Rk 	2 −

Rk , Mk 2
Mk , Mk Ω

= 	Rk 	2 −

σ12 (Rk )
.
Mk , Mk Ω

This completes the proof.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A500

WANG, LAI, LU, FAN, DAVULCU, AND YE

We are now ready to prove Theorem 4.1.
Proof of Theorem 4.1. Using the deﬁnition of Mk with its normalization property
Mk , Mk Ω ≤ 1, Property 4.8, and Property 4.6, we obtain that
||Rk+1 ||2

σ2 (R )

≤ ||Rk ||2 − M1k ,Mkk 	Ω ≤ ||Rk ||2 − σ12 (Rk )


1
≤
1 − min(m,n)
||Rk ||2 .

In view of this relation and the fact that 	R1 	 = 	Y	2Ω , we easily conclude that

||Rk || ≤

1
1−
min(m, n)

k−1
	Y	Ω .

This completes the proof.
5. An extension to the matrix sensing problem and its convergence
analysis. In this section, we extend our algorithm to deal with the following matrix
sensing problem (cf. [36, 25, 18, 19]):
(5.1)

min

X∈n×m

rank(X) : A(X) = A(Y),

where Y is a target low rank matrix and A is a linear operator, e.g., A consists of a
set of measurements Ai , X = Ai , Y for a sequence of matrices {Ai }. A(X) could
be written in a compact form as
⎤
⎡
vec(A1 )T
⎥
⎢
..
A(X) = ⎣
⎦ vec(X)
.
vec(Ad )T
for d measurements. Clearly, the matrix completion studied in the previous sections
is a special case of the above problem by setting the linear operator A to be the
observation operator PΩ .
We ﬁrst explain how to use our algorithm to solve this matrix sensing problem (5.1). Recall a linear operator vec which maps a matrix X of size n × m to a
vector vec(X) of size mn × 1. We now deﬁne an inverse operator matnm which converts a vector v of size mn × 1 to a matrix V = matnm (v) of size n × m. Note that
when X is vectorized into vec(X), the linear operator A can be expressed in terms
T
of matrix A = [vec(A1 ), . . . , vec(Ad )] . That is, A(X) = A(Y) can be rewritten
as Avec(X) = Avec(Y). For convenience, we can write A = Avec. It is clear that
A is a matrix of size d × mn. Certainly, one can ﬁnd its pseudoinverse A† which
is A
 (AA
 )−1 as we have assumed that A is of full row rank. We note that since
d << mn, AA† = Id while A† A = Imn , where Id and Imn are the identity matrices of
size d× d and mn× mn, respectively. For convenience, we let A−1 denote matnm ◦ A† ,
where ◦ is the Hadamard product. The linear operators satisfy
AA−1 b = b
for any vector b of size d × 1, while A−1 A is not an identity operator. We are now
ready to tackle the matrix sensing problem (5.1) as follows: let b = A(Y) = Avec(Y)
and R0 = A−1 (b) be the given matrix. We apply Algorithm 3 to obtain M(θ k ) in
k ≥ r steps.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A501

Algorithm 3. Rank-one matrix pursuit for matrix sensing.
Input: R0 and stopping criterion.
Initialize: Set X0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the residual
matrix Rk by using the power method and set Mk = uk vkT .
Step 2: Compute the weight vector θ k using the closed form least squares approximation of R0 by the best rank-one matrices Mi , i = 1, . . . , k:
θ = arg min 	R0 −
k

θ1 ,...,θk

k


θi A−1 A(Mi )	2F .

i=1

k
Step 3: Set M(θ k ) = i=1 θik Mi , Rk+1 = R0 − A−1 A(M(θ k )) and set k ←
k + 1.
until stopping criterion is satisﬁed
Output: the constructed matrix Ŷ = M(θ k ).

We shall show that M(θ k ) converges to the exact rank-r matrix Y. First of all,
Algorithm 3 can also be proved to be linearly convergent using the same procedure as
in the proof of Theorem 3.1. We thus have the following theorem without presenting
a detailed proof.
Theorem 5.1. Each step in Algorithm 3 satisfies

||Rk || ≤

1
1−
min(m, n)

k−1
	A−1 (b)	

∀k ≥ 1.

This holds for all matrices Y of rank at most r.
We now show M(θ k ) approximates the exact matrix Y for a large k. In the
setting of matrix sensing, we are able to use the rank-RIP condition. Let us recall the
following.
Definition 5.2. Let A be a linear map on linear space of matrices of size n × m
with n ≤ m. For every integer r with 1 ≤ r ≤ n, let the rank-r restricted isometry
constant be the smallest number δr (A) such that
(1 − δr (A))	X	2F ≤ 	A(X)	22 ≤ (1 + δr (A))	X	2F
holds for all matrices X of rank at most r.
It is known that for some random matrices A, A = Avec satisﬁes the rank-RIP
condition with high probability [36]. Armed with the rank-RIP condition, we are able
to establish the following result.
Theorem 5.3. Let Y be a matrix of rank r. Suppose the measurement mapping
A(X) satisfies rank-RIP for rank-r0 with δr0 = δr0 (A) < 1 with r0 ≥ 2r. The output
matrix M(θ k ) from Algorithm 3 approximates the exact matrix Y in the following
sense: there is a positive constant τ < 1 such that
C
τk
	M(θ k ) − Y	F ≤ 
1 − δr 0
for all k = 1, . . . , r0 − r, where C > 0 is a constant dependent on A.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A502

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Proof. Using the deﬁnition of δr0 , for k + r ≤ r0 , we have
(1 − δr0 )	M(θ k ) − Y	2F ≤ 	A(M(θ k )) − A(Y)	22
= 	A(Rk )	22 = 	Avec(Rk )	22
≤ 	A	22 	vec(Rk )	22 = 	A	22 	Rk 	2F
≤ 	A	22 τ 2k 	A−1 (b)	2F ,
where the last inequality follows from Theorem 5.1 with τ =
follows that
	M(θ k ) − Y	2F ≤


1−

1
min{m,n} .

It

	A	22 τ 2k −1
	A (b)	2F .
1 − δr 0

Therefore, we have the desired result.
Similarly we can extend our economic algorithm to the setting of matrix sensing.
We leave it to the interested reader. In the above convergence analysis, we require
k ≤ r0 − r, which guarantees the matrix-RIP condition for all estimated matrices
during the learning process. It will be interesting to explore if a similar result can be
obtained for any k > 0.
6. Eﬀect of inexact top singular vectors. In our rank-one matrix pursuit
algorithms, we need to calculate the top singular vector pair of the residual matrix in
each iteration. We rewrite it here as


(6.1)
max uT Rk v : 	u	 = 	v	 = 1 .
u,v

We solve this problem eﬃciently by the power method, which is an iterative method.
In practice, we obtain a solution with approximation error less than a small tolerance
δk ≥ 0, that is,
(6.2)

ũT Rk ṽ ≥ (1 − δk )

max

{uT Rk v}.

u=v=1

We show that the proposed algorithms still retain the linear convergence rate when
the top singular pair computed at each iteration satisﬁes (6.2) for 0 ≤ δk < 1. This
result is given in the following theorem.
Theorem 6.1. Assume that there is a tolerance parameter 0 ≤ δ < 1 such that
δk ≤ δ for all k. Then the orthogonal rank-one matrix pursuit algorithms achieve a
linear convergence rate

||Rk || ≤

q2
1−
min(m, n)

k−1
	Y	Ω ,

where q = 1 − δ satisfies 0 < q ≤ 1.
Proof. In Step 1 of our algorithms, we iteratively solve the problem (6.1) using
the power method. In this method, we stop the iteration such that
 T

ũTk Rk ṽk ≥ (1 − δk )
u Rk v ≥ 0
max
u=1,v=1

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A503

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

with 0 ≤ δk ≤ δ < 1. Denote M̃k = ũk ṽkT as the generated basis. Next, we show that
the following holds for both OR1MP and EOR1MP:
	Rk+1 	2 ≤ 	Rk 	2 − M̃k , Rk 2 .
For the OR1MP algorithm, we have
	Rk+1 	2 = min 	Y −
θ∈k

k


θi M̃i 	2Ω

i=1

≤ min 	Y − Xk−1 − θk M̃k 	2Ω
θk ∈

= min 	Rk − θk M̃k 	2Ω .
θk ∈

For the EOR1MP algorithm, we have
	Rk+1 	2 = min2 	Y − α1 Xk−1 − α2 M̃k 	2Ω
α∈

≤ min 	Y − Xk−1 − α2 M̃k 	2Ω
α2 ∈

= min 	Rk − α2 M̃k 	2Ω .
α2 ∈

Rk ,M̃k 	
. Plugging the optiIn both cases, we obtain closed form solutions as M̃
k ,M̃k 	Ω
mum solution into the corresponding formulations, we get

	Rk+1 	2 ≤ 	Rk −

Rk , M̃k 
M̃k 	2Ω
M̃k , M̃k Ω

= 	Rk 	2 −

Rk , M̃k 2
M̃k , M̃k Ω
M̃k , M̃k 2Ω

≤ 	Rk 	2 − Rk , M̃k 2 ,

as M̃k , M̃k Ω ≤ 1. It follows from Properties 4.5 and 4.6 that
Rk , M̃k  ≥ (1 − δk )σ1 (Rk ) ≥ (1 − δk ) 

	Rk 	
.
rank(Rk )

Combining the above two results, we get


(1 − δk )2
2
	Rk+1 	 ≤ 1 −
	Rk 	2 .
min(m, n)
In view of this relation and the fact that 	R1 	 = 	Y	2Ω , we conclude that

||Rk || ≤

q2
1−
min(m, n)

k−1
	Y	Ω ,

where q = 1 − δ ≤ inf(1 − δk ) = 1 − sup δk and is a constant between (0, 1]. This
completes the proof.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A504

WANG, LAI, LU, FAN, DAVULCU, AND YE

7. Experiments. In this section, we compare the two versions of our algorithm,
e.g., OR1MP and EOR1MP, with several state-of-the-art matrix completion methods
in the literature. The competing algorithms include SVP [18], SVT [7], Jaggi’s fast
algorithm for trace norm constraint (JS) [17], the spectral regularization algorithm
(SoftImpute) [30], low rank matrix ﬁtting (LMaFit) [46], a boosting-type accelerated
matrix-norm penalized solver (Boost) [49], atomic decomposition for minimum rank
approximation (ADMiRA) [25], and GECO [38]. The ﬁrst three solve trace norm
constrained problems; the next three solve trace norm penalized problems; the last
two directly solve the low rank constrained problem. The general greedy method [42]
is not included in our comparison, as it includes JS and GECO (included in our
comparison) as special cases for matrix completion. The lifted coordinate descent
method [9] is not included in our comparison as it is sensitive to the parameters and
is less eﬃcient than Boost proposed in [49].
The codes for most of these methods are available online:
• SVP, http://www.cs.utexas.edu/∼pjain/svp;
• SVT, http://svt.stanford.edu;
• SoftImpute, http://www-stat.stanford.edu/∼rahulm/software.html;
• LMaFit, http://lmaﬁt.blogs.rice.edu;
• Boost, http://webdocs.cs.ualberta.ca/∼xinhua2/boosting.zip;
• GECO, http://www.cs.huji.ac.il/∼shais/code/geco.zip.
We compare these algorithms in two applications: image recovery and collaborative ﬁltering or recommendation problem. The data size for image recovery is
relatively small, and the recommendation problem is large-scale. All the competing
methods are implemented in MATLAB1 and call some external packages for fast computation of SVD2 and sparse matrix computations. The experiments are run on a PC
with the windows 7 system, Intel 4 core 3.4-GHz CPU, and 8G RAM.
In the following experiments, we follow the recommended settings of the parameters for the competing algorithms. If no recommended parameter value is available,
we choose the best one from a candidate set using cross validation. For our OR1MP
and EOR1MP algorithms, we only need a stopping criterion. For simplicity, we stop
our algorithms after r iterations. In this way, we approximate the ground truth using
a rank-r matrix. We present the experimental results using two metrics, peak signalto-noise ratio (PSNR) [16] and root-mean-square error (RMSE) [22]. PSNR is a test
metric speciﬁc for images. A higher value in PSNR generally indicates better quality
[16]. RMSE is a general metric for prediction. It measures the approximation error
of the corresponding result.
7.1. Convergence and eﬃciency. Before we present the numerical results
from these comparison experiments, we shall include another algorithm called the
forward rank-one matrix pursuit algorithm (FR1MP), which extends the matching
pursuit method from the vector case to the matrix case. The detailed procedure of
this method is given in Algorithm 4.
In FR1MP, we add the pursued rank-one matrix with an optimal weight in each iteration, which is similar to the forward selection rule [14]. This is a standard algorithm
to ﬁnd SVD of any matrix Y if all its entries are given. In this case, the FR1MP
algorithm is more eﬃcient in ﬁnding SVD of the matrix than our two proposed al1 GECO

is written in C++ and we call its executable ﬁle in MATLAB.
is used in SVP, SVT, SoftImpute and Boost. It is an eﬃcient SVD package,
which is implemented in C and Fortran. It can be downloaded from http://soi.stanford.edu/∼
rmunk/PROPACK.
2 PROPACK

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A505

Algorithm 4. FR1MP.
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk vkT .
Step 2: Set θkk = (uTk Rk vk )/	Mk 	Ω , and θik = θik−1 for i ≤ k − 1.
Step 3: Set Xk = Xk−1 + θkk (Mk )Ω ; k ← k + 1.
until stopping criterion is satisﬁed
k
Output: Constructed matrix Ŷ = i=1 θik Mi .

gorithms. However, when only partial entries are known, the FR1MP algorithm will
not be able to ﬁnd the best low rank solution. The computational step to ﬁnd θ k in
our proposed algorithms is necessary.
The empirical results for convergence eﬃciency of our proposed algorithms are
reported in Figures 1 and 2. They are based on an image recovery experiment as
well as an experiment of a movie recommendation dataset, Netﬂix [22, 4, 5]. The
Netﬂix dataset has 108 ratings of 17,770 movies by 480,189 Netﬂix3 customers. This
is a large-scale dataset, and most of the competing methods are not applicable for
this dataset. In Figure 1, we present the convergence characteristics of the proposed
OR1MP algorithm. As the memory demand is increasing w.r.t. the iterations, we can
run it for only about 40 iterations on the Netﬂix dataset. The EOR1MP algorithm
has no such limitation. The results in Figure 2 show that our EOR1MP algorithm
rapidly reduces the approximation error. We also present the same residual curves in
logarithmic scale with a relatively large number of iterations in Figure 3, which verify
the linear convergence property of our algorithms. These results are consistent with
our theoretical analysis.
In the convergence analysis, we derive the upper bound for the convergence
speed of our proposed algorithms. From Theorems 3.1 and 4.1, the convergence
2
, where σk,∗ is the maximum singuspeed is controlled by the value of 	Rk 	2F /σk,∗
lar value of the residual matrix Rk in the kth iteration. A smaller value indicates
a faster convergence of our algorithms. Though it has a worst-case upper bound of
2
≤ rank(Rk ) ≤ min(m, n), in the following experiments, we empirically
	Rk 	2F /σk,∗
verify that its value is much smaller than the theoretical worst case. Thus the convergence speed of our algorithms is much faster than the theoretical worst case. We
2
at diﬀerent iterations on the Lenna image and the
present the values of 	Rk 	2F /σk,∗
MovieLens1M dataset for both of our algorithms in Figure 4. The results show that
2
is much smaller than min(m, n).
the quantity 	Rk 	2F /σk,∗
In the following experiments, we plot the residual curves over iterations for different rank-one matrix pursuit algorithms, including our OR1MP algorithm, our
EOR1MP algorithm, and the FR1MP algorithm. The evaluations are conducted
on the Lenna image and the MovieLens1M dataset, which are given in Figure 5.
The results show that among the three algorithms, EOR1MP and OR1MP perform
better than the forward pursuit algorithm. It is interesting to note that EOR1MP
achieves a similar performance as OR1MP, while it demands much less computational
cost.
3 http://www.netﬂixprize.com.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A506

WANG, LAI, LU, FAN, DAVULCU, AND YE
Lenna

Lenna
7
6

Time (seconds)

0.04
0.035

RMSE

0.03
0.025
0.02
0.015

5
4
3

0.01

2

0.005

1

0
0

100

200

0
0

300

50

100

150

rank

200

250

300

350

rank

Netflix

Netflix
8000

0.02

7000

Time (seconds)

RMSE

0.018
0.016
0.014
0.012

6000
5000
4000
3000
2000
1000

0.01
0

10

20

30

0
0

40

10

20

30

40

rank

rank

Fig. 1. Illustration of convergence of the proposed OR1MP algorithm on the Lenna image and
the Netﬂix dataset: the x-axis is the rank, the y-axis is the RMSE (left column), and the running
time is measured in seconds (right column).
Lenna

Lenna

0.04

1
0.9

0.035

0.8
0.03
Time (seconds)

0.7
RMSE

0.025
0.02
0.015

0.6
0.5
0.4
0.3

0.01
0.2
0.005
0
0

0.1
50

100

150
200
Iteration

250

0
0

300

50

100

150
200
Iteration

0.02

4500

0.019

4000

0.018

3500
Time (seconds)

0.017
0.016
0.015
0.014
0.013

300

3000
2500
2000
1500
1000

0.012

500

0.011
0.01
0

250

Netflix

Netflix

RMSE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

8

20

40

60
Iteration

80

100

0
0

20

40

60
Iteration

80

100

Fig. 2. Illustration of convergence of the proposed EOR1MP algorithm on the Lenna image and
the Netﬂix dataset: the x-axis is the rank, the y-axis is the RMSE (left column), and the running
time is measured in seconds (right column).

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A507

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION
Netflix

Lenna

−1

RMSE

RMSE

−2

10

−2

10

−3

10

0

50

100

150

200

250

300

0

5

10

15

Lenna

−1

20

25

30

35

40

rank

rank

Netflix

RMSE

10

RMSE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

10

−2

10

−3

10

−2

0

50

100

150

200

250

10

300

20

40

rank

60

80

100

rank

Fig. 3. Illustration of the linear convergence of diﬀerent rank-one matrix pursuit algorithms
on the Lenna image and the Netﬂix dataset: the x-axis is the iteration, and the y-axis is the RMSE
in log scale. The curves in the ﬁrst row are the results for OR1MP and the curves in the second
row are the results for EOR1MP.
OR1MP on MovieLens1M

OR1MP on Lenna
5000
2

||R|| /
600

σ2*

min(m,n)

||R||2 / σ2*
min(m,n)

4000

500
3000

400
300

2000

200
1000
100
0
0

20

40
60
Iteration

80

100

0
0

EOR1MP on Lenna

10

20
30
Iteration

40

50

EOR1MP on MovieLens1M
5000
||R||2 / σ2*

600

min(m,n)

||R||2 / σ2*
min(m,n)

4000

500
3000

400
300

2000

200
1000
100
0
0

20

40
60
Iteration

80

100

0
0

10

20
30
Iteration

40

50

Fig. 4. Illustration of the values of R2 /σ∗2 at diﬀerent iterations and the value of min(m, n)
on the Lenna image and MovieLens1M for both R1MP and ER1MP algorithms: the x-axis is the
iteration number; the y-axis is the value.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A508

WANG, LAI, LU, FAN, DAVULCU, AND YE
Lenna

MovieLens1M
OR1MP
EOR1MP
FR1MP

0.035

0.017
0.0165
0.016

0.025

RMSE

RMSE

OR1MP
EOR1MP
FR1MP

0.0175

0.03

0.02

0.0155
0.015
0.0145

0.015

0.014
0.01

0.0135

0.005
0

20

40

60
Iteration

80

0.013
0

100

20

40

60
Iteration

80

100

Fig. 5. Illustration of convergence speed of diﬀerent rank-one matrix pursuit algorithms on the
Lenna image and the MovieLens1M dataset: the x-axis is the iteration; the y-axis is the RMSE.
OR1MP on MovieLens1M

EOR1MP on MovieLens1M
iteration = 1
iteration = 2
iteration = 5
iteration = 10
iteration = 20

0.018

RMSE

0.017

0.016

0.016

0.015

0.015

0.014

0.014

0.013
0

iteration = 1
iteration = 2
iteration = 5
iteration = 10
iteration = 20

0.018

0.017
RMSE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

0.04

20

40

60
Iteration

80

100

0.013
0

20

40

60
Iteration

80

100

Fig. 6. Illustration of convergence property of the proposed algorithms with diﬀerent iteration
numbers in the power method on the MovieLens1M dataset: the x-axis is the outer iteration number;
the y-axis is the RMSE.

7.2. Inexact top singular vectors. We empirically analyze the performance
of our algorithms with inexact singular vector computation. In the experiments, we
control the total number of iterations in the power method for computing the top
singular vector pair. The numbers of iterations are set as {1, 2, 5, 10, 20}. We plot
the learning curves for the OR1MP and EOR1MP algorithms on the MovieLens1M
dataset in Figure 6. The results show that the linear convergence speed is preserved
for diﬀerent iteration numbers. However, the results under the same outer iterations
depend on the accuracy of the power methods. This veriﬁes our theoretical results.
Our empirical results also suggest that in practice we need to run more than 5 iterations in the power method, as the learning curves for 5, 10, and 20 power method
iterations are close to each other but are far away from the other two curves, especially
for the EOR1MP algorithm.
7.3. Recovery on synthetic data. In this experiment, we use synthetic data
to evaluate the recovery performance of diﬀerent matrix completion algorithms. We
generate a square n × n matrix Y of rank r as the ground truth. We construct Y by
ﬁrst generating a random matrix with i.i.d. entries drawn from the standard normal
distribution and then setting its ith singular value to 2r−i+1 . Given this matrix Y, we

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A509

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

Recovery Error

SVT
SVP
LMaFit
ADMiRA
SoftImpute
JS
OR1MP
EOR1MP

0.6
0.4
0.2
0
0

0.1

0.2

0.3

0.4

0.5
0.6
Observation Ratio

0.7

0.8

0.9

1

Recovery Error

1
SVT
SVP
LMaFit
ADMiRA
SoftImpute
JS
OR1MP
EOR1MP

0.8
0.6
0.4
0.2
0
0

0.1

0.2

0.3

0.4

0.5
0.6
Observation Ratio

0.7

0.8

0.9

1

1
Recovery Error

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1
0.8

SVT
SVP
LMaFit
ADMiRA
SoftImpute
JS
OR1MP
EOR1MP

0.8
0.6
0.4
0.2
0
0

0.1

0.2

0.3

0.4

0.5
0.6
Observation Ratio

0.7

0.8

0.9

1

Fig. 7. Comparison of recovery performance of diﬀerent matrix completion algorithms with
diﬀerent percentages of observations: the three ﬁgures correspond to the results on three rank-10
random matrices of size 50 × 50 without noise (top ﬁgure), size 100 × 100 without noise (middle
ﬁgure), and size 100 × 100 with Gaussion noise (bottom ﬁgure); the x-axis is the percentage of
observations; the y-axis is the recovery error.

sample a subset Ω of l entries uniformly at random as the observations. We run the
experiment in two diﬀerent settings—noise-free matrix completion and noisy matrix
completion. We ﬁx the rank of the ground truth matrices as r = 10 in all experiments.
In the noise-free case, we use two diﬀerent matrix sizes in the experiment: n = 50 and
n = 100. In the noisy case, we use n = 100 with 5% Gaussion noise. The entries of
the noise matrix are drawn from the standard normal distribution and are normalized
to make the matrix Frobenius norm equal to 0.05	Y	F . We evaluate the recovery
performance of the algorithms based on the relative reconstruction error calculated as
Y−ŶF
YF , with Ŷ as the reconstructed matrix. In the experiment, we ﬁx the number
of iterations to 200 for the JS algorithm. For OR1MP and EOR1MP, we stop the
algorithm after 50 iterations. For other algorithms, we use the true rank r = 10 for
the estimated matrix.
For each algorithm, we present its average result with 50 runs at diﬀerent percentages of observations in Figure 7. We can observe from the ﬁgure that for most
algorithms the recovery error decreases with an increasing number of observations.
The proposed algorithms are very competitive in most cases, particularly when the
observations are scarce.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A510

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 1
Image recovery results measured in terms of the PSNR.
Dataset
Barbara
Cameraman
Clown
Couple
Crowd
Girl
Goldhill
Lenna
Man
Peppers

SVT
26.9635
25.6273
28.5644
23.1765
26.9644
29.4688
28.3097
28.1832
27.0223
25.7202

SVP
SoftImpute LMaFit ADMiRA
JS
OR1MP EOR1MP
25.2598
25.6073
25.9589 23.3528 23.5322 26.5314
26.4413
25.9444
26.7183
24.8956 26.7645 24.6238 27.8565 27.8283
19.0919
26.9788
27.2748 25.7019 25.2690 28.1963
28.2052
23.7974
26.1033
25.8252 25.6260 24.4100 27.0707 27.0310
22.2959
25.4135
26.0662 24.0555 18.6562 26.0535
26.0510
27.5461
27.7180
27.4164 27.3640 26.1557 30.0878 30.0565
16.1256
27.1516
22.4485 26.5647 25.9706 28.5646 28.5101
25.4586
26.7022
23.2003 26.2371 24.5056 28.0115
27.9643
25.3246
25.7912
25.7417 24.5223 23.3060 26.5829
26.5049
26.0223
26.8475
27.3663 25.8934 24.0979 28.0781 28.0723

7.4. Image recovery. In the image recovery experiments, we use the following
benchmark test images: Barbara, Cameraman, Clown, Couple, Crowd, Girl, Goldhill,
Lenna, Man, and Peppers.4 The size of each image is 512×512. We randomly exclude
50% of the pixels in the image, and the remaining ones are used as the observations.
As the image matrix is not guaranteed to be low rank, we use rank 50 for the estimated
matrix for each experiment. In our OR1MP and EOR1MP algorithms, we stop the
algorithms after 150 iterations. The JS algorithm does not explicitly control the
rank, thus we ﬁx its number of iterations to 2000. The numerical results in terms of
the PSNR are listed in Table 1. We also present the images recovered by diﬀerent
algorithms for Lenna in Figure 8. The results show SVT, our OR1MP, and EOR1MP
achieve the best numerical performance. However, our algorithm is much better than
SVT for Cameraman, Couple, Peppers but only slightly worse than SVT for Lenna,
Barbara, and Clown. Besides, our algorithm is much faster and more stable than SVT
(SVT may diverge). For each image, EOR1MP uses around 3.5 seconds, but SVT
consumes around 400 seconds. Image recovery needs a relatively higher approximation
rank; both GECO and Boost fail to ﬁnd a good recovery in most cases, so we do not
include them in the result tables.
7.5. Recommendation. In the following experiments, we compare diﬀerent
matrix completion algorithms using large recommendation datasets: Jester [12] and
MovieLens [31]. We use six datasets: Jester1, Jester2, Jester3, MovieLens100K,
MovieLens1M, and MovieLens10M. The statistics of these datasets are given in Table 2. The Jester datasets were collected from a joke recommendation system. They
contain anonymous ratings of 100 jokes from the users. The ratings are real values
ranging from −10.00 to +10.00. The MovieLens datasets were collected from the
MovieLens website.5 They contain anonymous ratings of the movies on this web
made by its users. For MovieLens100K and MovieLens1M, there are 5 rating scores
(1–5), and for MovieLens10M there are 10 levels of scores with a step size 0.5 in the
range of 0.5 to 5. In the following experiments, we randomly split the ratings into
training and test sets. Each set contains 50% of the ratings. We compare the running
time and the prediction result from diﬀerent methods. In the experiments, we use
100 iterations for the JS algorithm, and for other algorithms we use the same rank
for the estimated matrices; the values of the rank are {10, 10, 5, 10, 10, 20} for the
six corresponding datasets. We ﬁrst show the running time of diﬀerent methods in
4 Images

are downloaded from http://www.utdallas.edu/∼ cxc123730/mh bcs spl.html.

5 http://movielens.umn.edu.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

Original

SVT

SVP

SoftImpute

LMafit

ADMiRA

JS

OR1MP

EOR1MP

A511

Fig. 8. The original image and images recovered by diﬀerent methods used on the Lenna image.
Table 2
Characteristics of the recommendation datasets.
Dataset

# row

# column

# rating

Jester1
Jester2
Jester3
MovieLens100k
MovieLens1M
MovieLens10M

24983
23500
24983
943
6040
69878

100
100
100
1682
3706
10677

106
106
6×105
105
106
107

Table 3. The reconstruction results in terms of the RMSE are given in Table 4. We
can observe from the above experiments that our EOR1MP algorithm is the fastest
among all competing methods to obtain satisfactory results.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

A512

WANG, LAI, LU, FAN, DAVULCU, AND YE

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 3
The running time (measured in seconds). Boost fails on MovieLens10M.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
18.35
16.85
16.58
1.32
18.90
> 103

SoftImpute
161.49
152.96
10.55
128.07
59.56
> 103

LMaFit
3.68
2.42
8.45
2.76
30.55
154.38

Boost
93.91
261.70
245.79
2.87
93.91
–

JS

GECO

OR1MP

EOR1MP

29.68
28.52
12.94
2.86
13.10
130.13

>
> 104
> 103
10.83
> 104
> 105

1.83
1.68
0.93
0.04
0.87
23.05

0.99
0.91
0.34
0.04
0.54
13.79

OR1MP
4.3418
4.3649
4.9783
1.0168
0.9595
0.8621

EOR1MP
4.3384
4.3546
5.0145
1.0261
0.9462
0.8692

104

Table 4
Recommendation results measured in terms of the RMSE.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
4.7311
4.7608
8.6958
0.9683
0.9085
0.8611

SoftImpute
5.1113
5.1646
5.4348
1.0354
0.8989
0.8534

LMaFit
4.7623
4.7500
9.4275
1.2308
0.9232
0.8625

Boost
5.1746
5.2319
5.3982
1.1244
1.0850
–

JS
4.4713
4.5102
4.6866
1.0146
1.0439
0.8728

GECO
4.3680
4.3967
5.1790
1.0243
0.9290
0.8668

8. Conclusion. In this paper, we propose an eﬃcient and scalable low rank
matrix completion algorithm. The key idea is to extend the OMP method from the
vector case to the matrix case. We also propose a novel weight updating rule under
this framework to reduce the storage complexity and make it independent of the
approximation rank. Our algorithms are computationally inexpensive for each matrix
pursuit iteration and ﬁnd satisfactory results in a few iterations. Another advantage
of our proposed algorithms is they have only one tunable parameter, which is the rank.
It is easy to understand and to use by the user. This becomes especially important in
large-scale learning problems. In addition, we rigorously show that both algorithms
achieve a linear convergence rate, which is signiﬁcantly better than the previous known
results (a sublinear convergence rate). We also extend our proposed algorithm to a
more general matrix sensing case and analyze its recovery guarantee under rankrestricted isometry property. We empirically compare the proposed algorithms with
state-of-the-art matrix completion algorithms, and our results show that the proposed
algorithms are more eﬃcient than competing algorithms while achieving similar or
better prediction performance. We plan to generalize our theoretical and empirical
analysis to other loss functions in the future.
Appendix A. Inverse matrix update. In our OR1MP algorithm, we use
the least squares solution to update the weights for the rank-one basis matrices.
In this step, we need to calculate (M̄Tk M̄k )−1 . To directly compute this inverse is
computationally expensive, as the matrix M̄k has a large row size. We implement
this eﬃciently using an incremental method. As
M̄Tk M̄k = [M̄k−1 , ṁk ]T [M̄k−1 , ṁk ],
its inverse can be written in block matrix form:
(M̄Tk M̄k )−1

 T
M̄k−1 M̄k−1
=
ṁTk M̄Tk−1

M̄Tk−1 ṁk
ṁTk ṁk

−1
.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

RANK-ONE MATRIX PURSUIT FOR MATRIX COMPLETION

A513

Then it is calculated by blockwise inverse as


A + dAbbT A −dAb
,
−dbT A
d
where A = (M̄Tk−1 M̄k−1 )−1 is the corresponding inverse matrix in the last step, b =
M̄Tk−1 ṁk is a vector with |Ω| elements, and d = (bT b− bT Ab)−1 = 1/(bT b− bT Ab)
is a scalar. M̄Tk ẏ is also calculated incrementally by [M̄Tk−1 ẏ, ṁTk ẏ], as ẏ is ﬁxed.
REFERENCES
[1] A. Argyriou, T. Evgeniou, and M. Pontil, Convex multi-task feature learning, Mach.
Learn., 73 (2008), pp. 243–272.
[2] F. Bach, Consistency of trace norm minimization, J. Mach. Learn. Res., 9 (2008), pp. 1019–
1048.
[3] L. Balzano, R. Nowak, and B. Recht, Online identiﬁcation and tracking of subspaces from
highly incomplete information, in Proceedings of the Allerton Conference on Communication, Control and Computing, 2010.
[4] R. Bell and Y. Koren, Lessons from the netﬂix prize challenge, ACM SIGKDD Explorations,
9 (2007), pp. 75–79.
[5] J. Bennett and S. Lanning, The netﬂix prize, in Proceedings of KDD Cup and Workshop,
2007.
[6] J.-F. Cai, E. J. Candès, and Z. Shen, A singular value thresholding algorithm for matrix
completion, SIAM J. Optim., 20 (2010), pp. 1956–1982.
[7] E. J. Candès and B. Recht, Exact matrix completion via convex optimization, Found. Comput. Math., 9 (2009), pp. 717–772.
[8] R. A. DeVore and V. N. Temlyakov, Some remarks on greedy algorithms, Adv. Comput.
Math., 5 (1996), pp. 173–187.
[9] M. Dudı́k, Z. Harchaoui, and J. Malick, Lifted coordinate descent for learning with tracenorm regularization, in Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2012.
[10] M. Frank and P. Wolfe, An algorithm for quadratic programming, Naval Res. Logist. Quart.,
3 (1956), pp. 95–110.
[11] J. H. Friedman, T. Hastie, and R. Tibshirani, Regularization paths for generalized linear
models via coordinate descent, J. Statist. Software, 33 (2010), pp. 1–22.
[12] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, Eigentaste: A constant time collaborative ﬁltering algorithm, Inform. Retrieval, 4 (2001), pp. 133–151.
[13] G. H. Golub and C. F. V. Loan, Matrix Computations, 3rd ed., Johns Hopkins University
Press, Baltimore, MD, 1996.
[14] T. Hastie, R. Tibshirani, and J. H. Friedman, The elements of statistical learning: Data
mining, inference, and prediction, Springer-Verlag, New York, 2009.
[15] E. Hazan, Sparse approximate solutions to semideﬁnite programs, in Proceedings of the 8th
Latin American Conference on Theoretical Informatics, 2008.
[16] Q. Huynh-Thu and M. Ghanbari, Scope of validity of psnr in image/video quality assessment,
Electron. Lett., 44 (2008), pp. 800–801.
[17] M. Jaggi and M. Sulovský, A simple algorithm for nuclear norm regularized problems, in
Proceedings of the 27th International Conference on Machine Learning (ICML), 2010,
pp. 471–478.
[18] P. Jain, R. Meka, and I. S. Dhillon, Guaranteed rank minimization via singular value
projection, Adv. Neural Inf. Process. Syste. 22 (2010), pp. 937–945.
[19] P. Jain, P. Netrapalli, and S. Sanghavi, Low-rank matrix completion using alternating
minimization, in Proceedings of the 45th Annual ACM Symposium on Symposium on
Theory of Computing (STOC), 2013, pp. 665–674.
[20] S. Ji and J. Ye, An accelerated gradient method for trace norm minimization, in Proceedings
of the 26th International Conference on Machine Learning (ICML), 2009, pp. 457–464.
[21] R. Keshavan and S. Oh, Optspace: A Gradient Descent Algorithm on the Grassmann Manifold
for Matrix Completion, http://arxiv.org/abs/0910.5260 (2009).
[22] Y. Koren, Factorization meets the neighborhood: A multifaceted collaborative ﬁltering model,
in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2008.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 01/14/16 to 142.58.205.60. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A514

WANG, LAI, LU, FAN, DAVULCU, AND YE

[23] Y. Koren, R. Bell, and C. Volinsky, Matrix factorization techniques for recommender
systems, Computer, 42 (2009), pp. 30–37.
[24] M.-J. Lai, Y. Xu, and W. Yin, Improved iteratively reweighted least squares for unconstrained
smoothed q minimization, SIAM J. Numer. Anal., 51 (2013), pp. 927–957.
[25] K. Lee and Y. Bresler, Admira: atomic decomposition for minimum rank approximation,
IEEE Trans. Inform. Theory, 56 (2010), pp. 4402–4416.
[26] E. Liu and T. N. Temlyakov, The orthogonal super greedy algorithm and applications in
compressed sensing, IEEE Trans. Inform. Theory, 58 (2012), pp. 2040–2047.
[27] Y.-J. Liu, D. Sun, and K.-C. Toh, An implementable proximal point algorithmic framework
for nuclear norm minimization, Math. Program., 133 (2012), pp. 399–436.
[28] Z. Lu and Y. Zhang, Penalty Decomposition Methods for Rank Minimization,
http://arxiv.org/abs/1008.5373 (2010).
[29] S. Ma, D. Goldfarb, and L. Chen, Fixed point and bregman iterative methods for matrix
rank minimization, Math. Program., 128 (2011), pp. 321–353.
[30] R. Mazumder, T. Hastie, and R. Tibshirani, Spectral regularization algorithms for learning
large incomplete matrices, J. Mach. Learn. Res., 99 (2010), pp. 2287–2322.
[31] B. N. Miller, I. Albert, S. K. Lam, J. A. Konstan, and J. Riedl, MovieLens unplugged:
Experiences with an occasionally connected recommender system, in Proceedings of the 8th
International Conference on Intelligent User Interfaces, 2003, pp. 263–266.
[32] B. Mishra, G. Meyer, F. Bach, and R. Sepulchre, Low-rank optimization with trace norm
penalty, SIAM J. Optim., 23 (2013), pp. 2124–2149.
[33] D. Needell and J. A. Tropp, Cosamp: Iterative signal recovery from incomplete and inaccurate samples, Comm. ACM, 53 (2010), pp. 93–100.
[34] S. Negahban and M. Wainwright, Estimation of (near) low-rank matrices with noise and
high-dimensional scaling, in Proceedings of the 27th International Conference on Machine
Learning (ICML), 2010.
[35] Y. C. Pati, R. Rezaiifar, Y. C. P. R. Rezaiifar, and P. S. Krishnaprasad, Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition, in Proceedings of the 27th Annual Asilomar Conference on Signals, Systems, and
Computers, 1993, pp. 40–44.
[36] B. Recht, M. Fazel, and P. A. Parrilo, Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization, SIAM Rev., 52 (2010), pp. 471–501.
[37] B. Recht and C. Ré, Parallel stochastic gradient algorithms for large-scale matrix completion,
Math. Program. Comput., 5 (2013), pp. 201–226.
[38] S. Shalev-Shwartz, A. Gonen, and O. Shamir, Large-scale convex minimization with a lowrank constraint, in Proceedings of the 28th International Conference on Machine Learning
(ICML), 2011, pp. 329–336.
[39] S. Shalev-Shwartz and A. Tewari, Stochastic methods for l1 regularized loss minimization,
in Proceedings of the 26th International Conference on Machine Learning (ICML), 2009,
pp. 929–936.
[40] N. Srebro, J. Rennie, and T. Jaakkola, Maximum-margin matrix factorizations, Adv. Neural Inf. Process. Syst., 17 (2004), pp. 1329–1336.
[41] V. N. Temlyakov, Greedy approximation, Acta Numer., 17 (2008), pp. 235–409.
[42] A. Tewari, P. Ravikumar, and I. S. Dhillon, Greedy algorithms for structurally constrained
high dimensional problems, Adv. Neural Inf. Process. Syst., 24 (2011), pp. 882–890.
[43] R. Tibshirani, Regression shrinkage and selection via the lasso, J. R. Stat. Soc. Ser. B, 58
(1994), pp. 267–288.
[44] K.-C. Toh and S. Yun, An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems, Paciﬁc J. Optim., 6 (2010), pp. 615–640.
[45] J. A. Tropp, Greed is good: Algorithmic results for sparse approximation, IEEE Trans. Inform.
Theory, 50 (2004), pp. 2231–2242.
[46] Z. Wen, W. Yin, and Y. Zhang, Solving a low-rank factorization model for matrix completion
by a nonlinear successive over-relaxation algorithm, Math. Program. Comput., 4 (2012),
pp. 333–361.
[47] T. T. Wu and K. Lange, Coordinate descent algorithms for lasso penalized regression, Ann.
Appl. Stat., 2 (2008), pp. 224–244.
[48] S. Yun and K.-C. Toh, A coordinate gradient descent method for l1-regularized convex minimization, Comput. Optim. Appl., 48 (2011), pp. 273–307.
[49] X. Zhang, Y. Yu, and D. Schuurmans, Accelerated training for matrix-norm regularization:
A boosting approach, Adv. Neural Inf. Process. Syst., 25 (2012), pp. 2906–2914.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

